{"id": 0, "code": "def save act ( self , path = None ) : if path is None : path = os . path . join ( logger . get dir ( ) , \"model.pkl\" ) with tempfile . Temporary Directory ( ) as td : save variables ( os . path . join ( td , \"model\" ) ) arc name = os . path . join ( td , \"packed.zip\" ) with zipfile . Zip File ( arc name , 'w' ) as zipf : for root , dirs , files in os . walk ( td ) : for fname in files : file path = os . path . join ( root , fname ) if file path != arc name : zipf . write ( file path , os . path . relpath ( file path , td ) ) with open ( arc name , \"rb\" ) as f : model data = f . read ( ) with open ( path , \"wb\" ) as f : cloudpickle . dump ( ( model data , self . act params ) , f )", "predictions": ["saves model data to a directory"], "references": ["save model to a pickle located at path"], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 1, "code": "def nature cnn ( unscaled images , * * conv kwargs ) : scaled images = tf . cast ( unscaled images , tf . float32 ) / 255. activ = tf . nn . relu h = activ ( conv ( scaled images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init scale = np . sqrt ( 2 ) , * * conv kwargs ) ) h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init scale = np . sqrt ( 2 ) , * * conv kwargs ) ) h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init scale = np . sqrt ( 2 ) , * * conv kwargs ) ) h3 = conv to fc ( h3 ) return activ ( fc ( h3 , 'fc1' , nh = 512 , init scale = np . sqrt ( 2 ) ) )", "predictions": ["nature cnn cnn cnn ."], "references": ["cnn from nature paper ."], "bleu": 0.32466791547509893, "rouge_l": 0.4}
{"id": 2, "code": "def make vec env ( env id , env type , num env , seed , wrapper kwargs = None , start index = 0 , reward scale = 1.0 , flatten dict observations = True , gamestate = None ) : wrapper kwargs = wrapper kwargs or { } mpi rank = MPI . COMM WORLD . Get rank ( ) if MPI else 0 seed = seed + 10000 * mpi rank if seed is not None else None logger dir = logger . get dir ( ) def make thunk ( rank ) : return lambda : make env ( env id = env id , env type = env type , mpi rank = mpi rank , subrank = rank , seed = seed , reward scale = reward scale , gamestate = gamestate , flatten dict observations = flatten dict observations , wrapper kwargs = wrapper kwargs , logger dir = logger dir ) set global seeds ( seed ) if num env > 1 : return Subproc Vec Env ( [ make thunk ( i + start index ) for i in range ( num env ) ] ) else : return Dummy Vec Env ( [ make thunk ( start index ) ] )", "predictions": ["create a thunk from the current environment variables"], "references": ["create a wrapped monitored subprocvecenv for atari and mujoco ."], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 3, "code": "def make mujoco env ( env id , seed , reward scale = 1.0 ) : rank = MPI . COMM WORLD . Get rank ( ) myseed = seed + 1000 * rank if seed is not None else None set global seeds ( myseed ) env = gym . make ( env id ) logger path = None if logger . get dir ( ) is None else os . path . join ( logger . get dir ( ) , str ( rank ) ) env = Monitor ( env , logger path , allow early resets = True ) env . seed ( seed ) if reward scale != 1.0 : from baselines . common . retro wrappers import Reward Scaler env = Reward Scaler ( env , reward scale ) return env", "predictions": ["factory function for creating a mujoco ."], "references": ["create a wrapped monitored gym . env for mujoco ."], "bleu": 0.18094495256969623, "rouge_l": 0.34205607476635513}
{"id": 4, "code": "def make robotics env ( env id , seed , rank = 0 ) : set global seeds ( seed ) env = gym . make ( env id ) env = Flatten Dict Wrapper ( env , [ 'observation' , 'desired goal' ] ) env = Monitor ( env , logger . get dir ( ) and os . path . join ( logger . get dir ( ) , str ( rank ) ) , info keywords = ( 'is success' , ) ) env . seed ( seed ) return env", "predictions": ["factory function for creating a callable with the defined environment ."], "references": ["create a wrapped monitored gym . env for mujoco ."], "bleu": 0.1354599427337814, "rouge_l": 0.1921259842519685}
{"id": 5, "code": "def common arg parser ( ) : parser = arg parser ( ) parser . add argument ( '--env' , help = 'environment ID' , type = str , default = 'Reacher-v2' ) parser . add argument ( '--env type' , help = 'type of environment, used when the environment type cannot be automatically determined' , type = str ) parser . add argument ( '--seed' , help = 'RNG seed' , type = int , default = None ) parser . add argument ( '--alg' , help = 'Algorithm' , type = str , default = 'ppo2' ) parser . add argument ( '--num timesteps' , type = float , default = 1e6 ) , parser . add argument ( '--network' , help = 'network type (mlp, cnn, lstm, cnn lstm, conv only)' , default = None ) parser . add argument ( '--gamestate' , help = 'game state to load (so far only used in retro games)' , default = None ) parser . add argument ( '--num env' , help = 'Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco' , default = None , type = int ) parser . add argument ( '--reward scale' , help = 'Reward scale factor. Default: 1.0' , default = 1.0 , type = float ) parser . add argument ( '--save path' , help = 'Path to save trained model to' , default = None , type = str ) parser . add argument ( '--save video interval' , help = 'Save video every x steps (0 = disabled)' , default = 0 , type = int ) parser . add argument ( '--save video length' , help = 'Length of recorded video. Default: 200' , default = 200 , type = int ) parser . add argument ( '--play' , default = False , action = 'store true' ) return parser", "predictions": ["return a parser for common options ."], "references": ["create an argparse . argumentparser for run_mujoco . py ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 6, "code": "def robotics arg parser ( ) : parser = arg parser ( ) parser . add argument ( '--env' , help = 'environment ID' , type = str , default = 'Fetch Reach-v0' ) parser . add argument ( '--seed' , help = 'RNG seed' , type = int , default = None ) parser . add argument ( '--num-timesteps' , type = int , default = int ( 1e6 ) ) return parser", "predictions": ["return a parser for the given argument ."], "references": ["create an argparse . argumentparser for run_mujoco . py ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 7, "code": "def parse unknown args ( args ) : retval = { } preceded by key = False for arg in args : if arg . startswith ( '--' ) : if '=' in arg : key = arg . split ( '=' ) [ 0 ] [ 2 : ] value = arg . split ( '=' ) [ 1 ] retval [ key ] = value else : key = arg [ 2 : ] preceded by key = True elif preceded by key : retval [ key ] = arg preceded by key = False return retval", "predictions": ["parse unknown arguments ."], "references": ["parse arguments not consumed by arg parser into a dicitonary"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 8, "code": "def save policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )", "predictions": ["save policy to file ."], "references": ["pickles the current policy for later inspection ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 9, "code": "def logs ( self , prefix = 'worker' ) : logs = [ ] logs += [ ( 'success rate' , np . mean ( self . success history ) ) ] if self . compute Q : logs += [ ( 'mean Q' , np . mean ( self . Q history ) ) ] logs += [ ( 'episode' , self . n episodes ) ] if prefix != '' and not prefix . endswith ( '/' ) : return [ ( prefix + '/' + key , val ) for key , val in logs ] else : return logs", "predictions": ["return the logs for each logs"], "references": ["generates a dictionary that contains all collected statistics ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 10, "code": "def copy obs dict ( obs ) : return { k : np . copy ( v ) for k , v in obs . items ( ) }", "predictions": ["return a copy of a dictionary with a dictionary of values ."], "references": ["deep - copy an observation dict ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 11, "code": "def value ( self , t ) : for ( l t , l ) , ( r t , r ) in zip ( self . endpoints [ : - 1 ] , self . endpoints [ 1 : ] ) : if l t <= t and t < r t : alpha = float ( t - l t ) / ( r t - l t ) return self . interpolation ( l , r , alpha ) assert self . outside value is not None return self . outside value", "predictions": ["get the color value"], "references": ["see schedule . value"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 12, "code": "def sf01 ( arr ) : s = arr . shape return arr . swapaxes ( 0 , 1 ) . reshape ( s [ 0 ] * s [ 1 ] , * s [ 2 : ] )", "predictions": ["convert an object to a list of point numbers ."], "references": ["swap and then flatten axes 0 and 1"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 13, "code": "def store args ( method ) : argspec = inspect . getfullargspec ( method ) defaults = { } if argspec . defaults is not None : defaults = dict ( zip ( argspec . args [ - len ( argspec . defaults ) : ] , argspec . defaults ) ) if argspec . kwonlydefaults is not None : defaults . update ( argspec . kwonlydefaults ) arg names = argspec . args [ 1 : ] @ functools . wraps ( method ) def wrapper ( * positional args , * * keyword args ) : self = positional args [ 0 ] args = defaults . copy ( ) for name , value in zip ( arg names , positional args [ 1 : ] ) : args [ name ] = value args . update ( keyword args ) self . dict . update ( args ) return method ( * positional args , * * keyword args ) return wrapper", "predictions": ["converts arguments to arguments ."], "references": ["stores provided method args as instance attributes ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 14, "code": "def flatten grads ( var list , grads ) : return tf . concat ( [ tf . reshape ( grad , [ U . numel ( v ) ] ) for ( v , grad ) in zip ( var list , grads ) ] , 0 )", "predictions": ["flatten a variable into a single list of tuples ."], "references": ["flattens a variables and their gradients ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 15, "code": "def nn ( input , layers sizes , reuse = None , flatten = False , name = \"\" ) : for i , size in enumerate ( layers sizes ) : activation = tf . nn . relu if i < len ( layers sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel initializer = tf . contrib . layers . xavier initializer ( ) , reuse = reuse , name = name + ' ' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input", "predictions": ["nn layers with the last model ."], "references": ["creates a simple neural network"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 16, "code": "def get session ( config = None ) : sess = tf . get default session ( ) if sess is None : sess = make session ( config = config , make default = True ) return sess", "predictions": ["return a session object for the current config ."], "references": ["get default session or create one with a given config"], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 17, "code": "def initialize ( ) : new variables = set ( tf . global variables ( ) ) - ALREADY INITIALIZED get session ( ) . run ( tf . variables initializer ( new variables ) ) ALREADY INITIALIZED . update ( new variables )", "predictions": ["initialize the search database ."], "references": ["initialize all the uninitialized variables in the global scope ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 18, "code": "def wrap deepmind ( env , episode life = True , clip rewards = True , frame stack = False , scale = False ) : if episode life : env = Episodic Life Env ( env ) if 'FIRE' in env . unwrapped . get action meanings ( ) : env = Fire Reset Env ( env ) env = Warp Frame ( env ) if scale : env = Scaled Float Frame ( env ) if clip rewards : env = Clip Reward Env ( env ) if frame stack : env = Frame Stack ( env , 4 ) return env", "predictions": ["wraps the deepmind class to be used in the current wsgi wsgi application ."], "references": ["configure environment for deepmind - style atari ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 19, "code": "def gpu count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check output ( [ 'nvidia-smi' , '--query-gpu=gpu name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\\n' ) ) - 2 )", "predictions": ["get the number of gpu gpu gpu ."], "references": ["count the gpus on this machine ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 20, "code": "def setup mpi gpus ( ) : if 'CUDA VISIBLE DEVICES' not in os . environ : if sys . platform == 'darwin' : ids = [ ] else : lrank , lsize = get local rank size ( MPI . COMM WORLD ) ids = [ lrank ] os . environ [ \"CUDA VISIBLE DEVICES\" ] = \",\" . join ( map ( str , ids ) )", "predictions": ["sets up the local environment variables for the process ."], "references": ["set cuda_visible_devices to mpi rank if not already set"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 21, "code": "def dict gather ( comm , d , op = 'mean' , assert all have data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert all have data : assert len ( li ) == size , \"only %i out of %i MPI workers have sent '%s'\" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result", "predictions": ["turn a dictionary of workers into a dictionary of mean values"], "references": ["perform a reduction operation over dicts"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 22, "code": "def add ( self , * args , * * kwargs ) : idx = self . next idx super ( ) . add ( * args , * * kwargs ) self . it sum [ idx ] = self . max priority ** self . alpha self . it min [ idx ] = self . max priority ** self . alpha", "predictions": ["add a new entry to the list ."], "references": ["see replaybuffer . store_effect"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 23, "code": "def wrap deepmind retro ( env , scale = True , frame stack = 4 ) : env = Warp Frame ( env ) env = Clip Reward Env ( env ) if frame stack > 1 : env = Frame Stack ( env , frame stack ) if scale : env = Scaled Float Frame ( env ) return env", "predictions": ["wraps the wsgi application to run the deepmind stack stack ."], "references": ["configure environment for retro games using config similar to deepmind - style atari in wrap_deepmind"], "bleu": 0.08762962180865386, "rouge_l": 0.14969325153374236}
{"id": 24, "code": "def model ( inpt , num actions , scope , reuse = False ) : with tf . variable scope ( scope , reuse = reuse ) : out = inpt out = layers . fully connected ( out , num outputs = 64 , activation fn = tf . nn . tanh ) out = layers . fully connected ( out , num outputs = num actions , activation fn = None ) return out", "predictions": ["model function used to create a model given an activation model ."], "references": ["this model takes as input an observation and returns values of all actions ."], "bleu": 0.10459315495983224, "rouge_l": 0.22761194029850743}
{"id": 25, "code": "def parse cmdline kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( Name Error , Syntax Error ) : return v return { k : parse ( v ) for k , v in parse unknown args ( args ) . items ( ) }", "predictions": ["parse kwargs from string"], "references": ["convert a list of = - spaced command - line arguments to a dictionary evaluating python objects when possible"], "bleu": 0.0071055938730635, "rouge_l": 0.0}
{"id": 26, "code": "def terminate ( self ) : if self . pool is not None : self . pool . terminate ( ) self . pool . join ( ) self . pool = None", "predictions": ["terminate the pool ."], "references": ["terminate the pool immediately ."], "bleu": 0.5506953149031837, "rouge_l": 0.8714285714285713}
{"id": 27, "code": "def terminate ( self ) : if not self . join signal . is set ( ) : self . join signal . set ( ) time . sleep ( 0.01 ) if self . main worker thread . is alive ( ) : self . main worker thread . join ( ) if self . threaded : for worker in self . workers : if worker . is alive ( ) : worker . join ( ) else : for worker in self . workers : if worker . is alive ( ) : worker . terminate ( ) worker . join ( ) while not self . all finished ( ) : time . sleep ( 0.001 ) if self . queue . full ( ) : self . queue . get ( ) self . queue . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 ) while True : try : self . queue internal . get ( timeout = 0.005 ) except Queue Empty : break if not self . queue internal . closed : self . queue internal . close ( ) if not self . queue . closed : self . queue . close ( ) self . queue internal . join thread ( ) self . queue . join thread ( ) time . sleep ( 0.025 )", "predictions": ["terminate all workers ."], "references": ["stop all workers ."], "bleu": 0.668740304976422, "rouge_l": 0.75}
{"id": 28, "code": "def get intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]", "predictions": ["return a list of intersections for this analysis ."], "references": ["return a list of unordered intersection points ."], "bleu": 0.4111336169005197, "rouge_l": 0.594541910331384}
{"id": 29, "code": "def min item ( self ) : if self . is empty ( ) : raise Value Error ( \"Tree is empty\" ) node = self . root while node . left is not None : node = node . left return node . key , node . value", "predictions": ["return the min of the tree ."], "references": ["get item with min key of tree raises valueerror if tree is empty ."], "bleu": 0.08592522909830395, "rouge_l": 0.35935198821796754}
{"id": 30, "code": "def max item ( self ) : if self . is empty ( ) : raise Value Error ( \"Tree is empty\" ) node = self . root while node . right is not None : node = node . right return node . key , node . value", "predictions": ["get the maximum item in the tree ."], "references": ["get item with max key of tree raises valueerror if tree is empty ."], "bleu": 0.09525245831601728, "rouge_l": 0.346590909090909}
{"id": 31, "code": "def In Colorspace ( to colorspace , from colorspace = \"RGB\" , children = None , name = None , deterministic = False , random state = None ) : return With Colorspace ( to colorspace , from colorspace , children , name , deterministic , random state )", "predictions": ["build a color for a colorspace ."], "references": ["convert images to another colorspace ."], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 32, "code": "def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )", "predictions": ["returns the save save save to the logger"], "references": ["get the height of a bounding box encapsulating the line ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 33, "code": "def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx )", "predictions": ["the nature nature of the cell"], "references": ["get the width of a bounding box encapsulating the line ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 34, "code": "def offer ( self , p , e : Event ) : existing = self . events scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE VERTICAL else ( [ ] , [ ] , [ ] ) ) existing [ e . type ] . append ( e )", "predictions": ["add a make make a make it s make it a make it a make it up the make it s make it s make it s make it"], "references": ["offer a new event s at point p in this queue ."], "bleu": 0.0462136266712202, "rouge_l": 0.10544511668107173}
{"id": 35, "code": "def append ( self , key : str , value : str ) -> None : append key = key . lower ( ) . encode ( \"latin-1\" ) append value = value . encode ( \"latin-1\" ) self . list . append ( ( append key , append value ) )", "predictions": ["make a new argument make it doesn t exist"], "references": ["append a header preserving any duplicate entries ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 36, "code": "def parse docstring ( self , func or method : typing . Callable ) -> dict : docstring = func or method . doc if not docstring : return { } docstring = docstring . split ( \"---\" ) [ - 1 ] parsed = yaml . safe load ( docstring ) if not isinstance ( parsed , dict ) : return { } return parsed", "predictions": ["make sure the docstring is a docstring docstring"], "references": ["given a function parse the docstring as yaml and return a dictionary of info ."], "bleu": 0.09454082294706839, "rouge_l": 0.24729729729729732}
{"id": 37, "code": "async def get response ( self , path : str , scope : Scope ) -> Response : if scope [ \"method\" ] not in ( \"GET\" , \"HEAD\" ) : return Plain Text Response ( \"Method Not Allowed\" , status code = 405 ) if path . startswith ( \"..\" ) : return Plain Text Response ( \"Not Found\" , status code = 404 ) full path , stat result = await self . lookup path ( path ) if stat result and stat . S ISREG ( stat result . st mode ) : return self . file response ( full path , stat result , scope ) elif stat result and stat . S ISDIR ( stat result . st mode ) and self . html : index path = os . path . join ( path , \"index.html\" ) full path , stat result = await self . lookup path ( index path ) if stat result is not None and stat . S ISREG ( stat result . st mode ) : if not scope [ \"path\" ] . endswith ( \"/\" ) : url = URL ( scope = scope ) url = url . replace ( path = url . path + \"/\" ) return Redirect Response ( url = url ) return self . file response ( full path , stat result , scope ) if self . html : full path , stat result = await self . lookup path ( \"404.html\" ) if stat result is not None and stat . S ISREG ( stat result . st mode ) : return self . file response ( full path , stat result , scope , status code = 404 ) return Plain Text Response ( \"Not Found\" , status code = 404 )", "predictions": ["returns a parser for the given = = none"], "references": ["returns an http response given the incoming path method and request headers ."], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 38, "code": "def build environ ( scope : Scope , body : bytes ) -> dict : environ = { \"REQUEST METHOD\" : scope [ \"method\" ] , \"SCRIPT NAME\" : scope . get ( \"root path\" , \"\" ) , \"PATH INFO\" : scope [ \"path\" ] , \"QUERY STRING\" : scope [ \"query string\" ] . decode ( \"ascii\" ) , \"SERVER PROTOCOL\" : f\"HTTP/{scope['http version']}\" , \"wsgi.version\" : ( 1 , 0 ) , \"wsgi.url scheme\" : scope . get ( \"scheme\" , \"http\" ) , \"wsgi.input\" : io . Bytes IO ( body ) , \"wsgi.errors\" : sys . stdout , \"wsgi.multithread\" : True , \"wsgi.multiprocess\" : True , \"wsgi.run once\" : False , } server = scope . get ( \"server\" ) or ( \"localhost\" , 80 ) environ [ \"SERVER NAME\" ] = server [ 0 ] environ [ \"SERVER PORT\" ] = server [ 1 ] if scope . get ( \"client\" ) : environ [ \"REMOTE ADDR\" ] = scope [ \"client\" ] [ 0 ] for name , value in scope . get ( \"headers\" , [ ] ) : name = name . decode ( \"latin1\" ) if name == \"content-length\" : corrected name = \"CONTENT LENGTH\" elif name == \"content-type\" : corrected name = \"CONTENT TYPE\" else : corrected name = f\"HTTP {name}\" . upper ( ) . replace ( \"-\" , \" \" ) value = value . decode ( \"latin1\" ) if corrected name in environ : value = environ [ corrected name ] + \",\" + value environ [ corrected name ] = value return environ", "predictions": ["builds the arg dictionary from a wsgi scope scope ."], "references": ["builds a scope and request body into a wsgi environ object ."], "bleu": 0.16153071659734697, "rouge_l": 0.3577712609970674}
{"id": 39, "code": "async def receive ( self ) -> Message : if self . client state == Web Socket State . CONNECTING : message = await self . receive ( ) message type = message [ \"type\" ] assert message type == \"websocket.connect\" self . client state = Web Socket State . CONNECTED return message elif self . client state == Web Socket State . CONNECTED : message = await self . receive ( ) message type = message [ \"type\" ] assert message type in { \"websocket.receive\" , \"websocket.disconnect\" } if message type == \"websocket.disconnect\" : self . client state = Web Socket State . DISCONNECTED return message else : raise Runtime Error ( 'Cannot call \"receive\" once a disconnect message has been received.' )", "predictions": ["unknown a if if a if a if we have a if we get a if we unknown it ."], "references": ["receive asgi websocket messages ensuring valid state transitions ."], "bleu": 0.06108557268562171, "rouge_l": 0.07402912621359223}
{"id": 40, "code": "async def send ( self , message : Message ) -> None : if self . application state == Web Socket State . CONNECTING : message type = message [ \"type\" ] assert message type in { \"websocket.accept\" , \"websocket.close\" } if message type == \"websocket.close\" : self . application state = Web Socket State . DISCONNECTED else : self . application state = Web Socket State . CONNECTED await self . send ( message ) elif self . application state == Web Socket State . CONNECTED : message type = message [ \"type\" ] assert message type in { \"websocket.send\" , \"websocket.close\" } if message type == \"websocket.close\" : self . application state = Web Socket State . DISCONNECTED await self . send ( message ) else : raise Runtime Error ( 'Cannot call \"send\" once a close message has been sent.' )", "predictions": ["policy a path to the pickle server f f f f f f f f f ."], "references": ["send asgi websocket messages ensuring valid state transitions ."], "bleu": 0.07223943354597204, "rouge_l": 0.0814419225634179}
{"id": 41, "code": "def cumulative returns less costs ( returns , costs ) : if costs is None : return ep . cum returns ( returns ) return ep . cum returns ( returns - costs )", "predictions": ["logs the number of costs up to the given returns . . . . . . . . . . . . . . . . . . . . ."], "references": ["compute cumulative returns less costs ."], "bleu": 0.046398855339878003, "rouge_l": 0.12310797174571139}
{"id": 42, "code": "def to utc ( df ) : try : df . index = df . index . tz localize ( 'UTC' ) except Type Error : df . index = df . index . tz convert ( 'UTC' ) return df", "predictions": ["convert a pandas dataframe into obs"], "references": ["for use in tests ; applied utc timestamp to dataframe ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 43, "code": "def sample colormap ( cmap name , n samples ) : colors = [ ] colormap = cm . cmap d [ cmap name ] for i in np . linspace ( 0 , 1 , n samples ) : colors . append ( colormap ( i ) ) return colors", "predictions": ["return a colormap colormap color into a color color colormap"], "references": ["sample a colormap from matplotlib"], "bleu": 0.16590387014219712, "rouge_l": 0.2837209302325582}
{"id": 44, "code": "def customize ( func ) : @ wraps ( func ) def call w context ( * args , * * kwargs ) : set context = kwargs . pop ( 'set context' , True ) if set context : with plotting context ( ) , axes style ( ) : return func ( * args , * * kwargs ) else : return func ( * args , * * kwargs ) return call w context", "predictions": ["decorator to customize a return value as a decorator ."], "references": ["decorator to set plotting context and axes style during function call ."], "bleu": 0.14595947916189678, "rouge_l": 0.2683284457478006}
{"id": 45, "code": "def private method ( func ) : def func wrapper ( * args , * * kwargs ) : \"\"\"Decorator wrapper function.\"\"\" outer frame = inspect . stack ( ) [ 1 ] [ 0 ] if 'self' not in outer frame . f locals or outer frame . f locals [ 'self' ] is not args [ 0 ] : raise Runtime Error ( '%s.%s is a private method' % ( args [ 0 ] . class . name , func . name ) ) return func ( * args , * * kwargs ) return func wrapper", "predictions": ["decorator to wrap store store store as a store args"], "references": ["decorator for making an instance method private ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 46, "code": "def protected method ( func ) : def func wrapper ( * args , * * kwargs ) : \"\"\"Decorator wrapper function.\"\"\" outer frame = inspect . stack ( ) [ 1 ] [ 0 ] caller = inspect . getmro ( outer frame . f locals [ 'self' ] . class ) [ : - 1 ] target = inspect . getmro ( args [ 0 ] . class ) [ : - 1 ] share subsclass = False for cls in target : if issubclass ( caller [ 0 ] , cls ) or caller [ 0 ] is cls : share subsclass = True break if ( 'self' not in outer frame . f locals or outer frame . f locals [ 'self' ] is not args [ 0 ] ) and ( not share subsclass ) : raise Runtime Error ( '%s.%s is a protected method' % ( args [ 0 ] . class . name , func . name ) ) return func ( * args , * * kwargs ) return func wrapper", "predictions": ["decorator to make sure that a grads is flatten a grads grads"], "references": ["decorator for making an instance method private ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 47, "code": "def log if ( level , msg , condition , * args ) : if condition : vlog ( level , msg , * args )", "predictions": ["nn to nn if with reuse"], "references": ["log msg % args at level level only if condition is fulfilled ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 48, "code": "def google2 log prefix ( level , timestamp = None , file and line = None ) : global level names now = timestamp or time . time ( ) now tuple = time . localtime ( now ) now microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file and line or Get File And Line ( ) basename = os . path . basename ( filename ) severity = 'I' if level in level names : severity = level names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now tuple [ 1 ] , now tuple [ 2 ] , now tuple [ 3 ] , now tuple [ 4 ] , now tuple [ 5 ] , now microsecond , get thread id ( ) , basename , line ) return s", "predictions": ["log a prefix that was set up to the session ."], "references": ["assemble a logline prefix using the google2 format ."], "bleu": 0.14323145079400493, "rouge_l": 0.4073455759599332}
{"id": 49, "code": "def validation metrics ( self ) : if ( self . validation iterator is None ) or ( self . validation metrics is None ) : raise Attribute Error ( 'Validation is not setup.' ) n = 0.0 metric sums = [ 0.0 ] * len ( self . validation metrics ) self . sess . run ( self . validation iterator . initializer ) while True : try : metrics = self . sess . run ( self . validation metrics ) for i , m in enumerate ( metrics ) : metric sums [ i ] += m n += 1.0 except tf . errors . Out Of Range Error : break for i , m in enumerate ( metric sums ) : metric sums [ i ] = metric sums [ i ] / n return zip ( self . validation metrics , metric sums )", "predictions": ["get the initialize metrics metrics set"], "references": ["a helper function to compute validation related metrics"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 50, "code": "def retrieve seq length op3 ( data , pad val = 0 ) : data shape size = data . get shape ( ) . ndims if data shape size == 3 : return tf . reduce sum ( tf . cast ( tf . reduce any ( tf . not equal ( data , pad val ) , axis = 2 ) , dtype = tf . int32 ) , 1 ) elif data shape size == 2 : return tf . reduce sum ( tf . cast ( tf . not equal ( data , pad val ) , dtype = tf . int32 ) , 1 ) elif data shape size == 1 : raise Value Error ( \"retrieve seq length op3: data has wrong shape!\" ) else : raise Value Error ( \"retrieve seq length op3: handling data shape size %s hasn't been implemented!\" % ( data shape size ) )", "predictions": ["wrap in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in"], "references": ["return tensor for sequence length if input is tf . string ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 51, "code": "def state size ( self ) : return ( LSTM State Tuple ( self . num units , self . num units ) if self . state is tuple else 2 * self . num units )", "predictions": ["the count of the gpu gpu gpu is a subprocess is the same as the count of the gpu is the gpu is the gpu is the gpu is a tuple"], "references": ["state size of the lstmstatetuple ."], "bleu": 0.0513487742994337, "rouge_l": 0.12310797174571139}
{"id": 52, "code": "def tf repeat ( self , a , repeats ) : if len ( a . get shape ( ) ) != 1 : raise Assertion Error ( \"This is not a 1D Tensor\" ) a = tf . expand dims ( a , - 1 ) a = tf . tile ( a , [ 1 , repeats ] ) a = self . tf flatten ( a ) return a", "predictions": ["setup a a with a in a in a in a in a in a in a in a in a in a in a in a in a in a"], "references": ["tensorflow version of np . repeat for 1d"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 53, "code": "def augment with ngrams ( unigrams , unigram vocab size , n buckets , n = 2 ) : def get ngrams ( n ) : return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) def hash ngram ( ngram ) : bytes = array . array ( 'L' , ngram ) . tobytes ( ) hash = int ( hashlib . sha256 ( bytes ) . hexdigest ( ) , 16 ) return unigram vocab size + hash % n buckets return unigrams + [ hash ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get ngrams ( i ) ]", "predictions": ["if ngrams is a is a is a is ngrams if the list of all all the all all all the first all the all all the all all all all"], "references": ["augment unigram features with hashed n - gram features ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 54, "code": "def load and preprocess imdb data ( n gram = None ) : X train , y train , X test , y test = tl . files . load imdb dataset ( nb words = VOCAB SIZE ) if n gram is not None : X train = np . array ( [ augment with ngrams ( x , VOCAB SIZE , N BUCKETS , n = n gram ) for x in X train ] ) X test = np . array ( [ augment with ngrams ( x , VOCAB SIZE , N BUCKETS , n = n gram ) for x in X test ] ) return X train , y train , X test , y test", "predictions": ["add data to dataset and test data ."], "references": ["load imdb data and augment with hashed n - gram features ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 55, "code": "def data to tfrecord ( images , labels , filename ) : if os . path . isfile ( filename ) : print ( \"%s exists\" % filename ) return print ( \"Converting data into %s ...\" % filename ) writer = tf . python io . TF Record Writer ( filename ) for index , img in enumerate ( images ) : img raw = img . tobytes ( ) label = int ( labels [ index ] ) example = tf . train . Example ( features = tf . train . Features ( feature = { \"label\" : tf . train . Feature ( int64 list = tf . train . Int64List ( value = [ label ] ) ) , 'img raw' : tf . train . Feature ( bytes list = tf . train . Bytes List ( value = [ img raw ] ) ) , } ) ) writer . write ( example . Serialize To String ( ) ) writer . close ( )", "predictions": ["convert wrap wrap wrap wrap wrap wrap wrap wrap the wrap wrap env . . . . . . . . . . . . . . . . in a"], "references": ["save data into tfrecord ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 56, "code": "def read and decode ( filename , is train = None ) : filename queue = tf . train . string input producer ( [ filename ] ) reader = tf . TF Record Reader ( ) , serialized example = reader . read ( filename queue ) features = tf . parse single example ( serialized example , features = { 'label' : tf . Fixed Len Feature ( [ ] , tf . int64 ) , 'img raw' : tf . Fixed Len Feature ( [ ] , tf . string ) , } ) img = tf . decode raw ( features [ 'img raw' ] , tf . float32 ) img = tf . reshape ( img , [ 32 , 32 , 3 ] ) if is train == True : img = tf . random crop ( img , [ 24 , 24 , 3 ] ) img = tf . image . random flip left right ( img ) img = tf . image . random brightness ( img , max delta = 63 ) img = tf . image . random contrast ( img , lower = 0.2 , upper = 1.8 ) img = tf . image . per image standardization ( img ) elif is train == False : img = tf . image . resize image with crop or pad ( img , 24 , 24 ) img = tf . image . per image standardization ( img ) elif is train == None : img = img label = tf . cast ( features [ 'label' ] , tf . int32 ) return img , label", "predictions": ["model and and and and and and and and"], "references": ["return tensor to read from tfrecord ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 57, "code": "def print params ( self , details = True , session = None ) : for i , p in enumerate ( self . all params ) : if details : try : val = p . eval ( session = session ) logging . info ( \"  param {:3}: {:20} {:15}    {} (mean: {:<18}, median: {:<18}, std: {:<18})   \" . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( \"Hint: print params details after tl.layers.initialize global variables(sess) \" \"or use network.print params(False).\" ) else : logging . info ( \"  param {:3}: {:20} {:15}    {}\" . format ( i , p . name , str ( p . get shape ( ) ) , p . dtype . name ) ) logging . info ( \"  num of params: %d\" % self . count params ( ) )", "predictions": ["parse the parameters and parse the result ."], "references": ["print all info of parameters in the network"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 58, "code": "def print layers ( self ) : for i , layer in enumerate ( self . all layers ) : logging . info ( \"  layer {:3}: {:20} {:15}    {}\" . format ( i , layer . name , str ( layer . get shape ( ) ) , layer . dtype . name ) )", "predictions": ["terminate all layers layers = true = false = 0 = 1 = 0 = 0 = 1 = 1 = 0 = 1 = 1 = 0 = 0 ="], "references": ["print all info of layers in the network ."], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 59, "code": "def count params ( self ) : n params = 0 for i , p in enumerate ( self . all params ) : n = 1 for s in p . get shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n params = n params + n return n params", "predictions": ["return the number of parameters"], "references": ["returns the number of parameters in the network ."], "bleu": 0.34141618153312897, "rouge_l": 0.5434298440979956}
{"id": 60, "code": "def get all params ( self , session = None ) : params = [ ] for p in self . all params : if session is None : params . append ( p . eval ( ) ) else : params . append ( session . run ( p ) ) return params", "predictions": ["get all params parameters"], "references": ["return the parameters in a list of array ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 61, "code": "def get init args ( self , skip = 4 ) : stack = inspect . stack ( ) if len ( stack ) < skip + 1 : raise Value Error ( \"The length of the inspection stack is shorter than the requested start position.\" ) args , , , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) params = { } for arg in args : if values [ arg ] is not None and arg not in [ 'self' , 'prev layer' , 'inputs' ] : val = values [ arg ] if inspect . isfunction ( val ) : params [ arg ] = { \"module path\" : val . module , \"func name\" : val . name } elif arg . endswith ( 'init' ) : continue else : params [ arg ] = val return params", "predictions": ["returns a list of key arguments to be used as a list of keyword arguments"], "references": ["get all arguments of current layer for saving the graph ."], "bleu": 0.09103526405546068, "rouge_l": 0.1582360570687419}
{"id": 62, "code": "def bias scale ( x , b , data format ) : if data format == 'NHWC' : return x * b elif data format == 'NCHW' : return x * to channel first bias ( b ) else : raise Value Error ( 'invalid data format: %s' % data format )", "predictions": ["item self to max max max max max max max max max"], "references": ["the multiplication counter part of tf . nn . bias_add ."], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 63, "code": "def bias add ( x , b , data format ) : if data format == 'NHWC' : return tf . add ( x , b ) elif data format == 'NCHW' : return tf . add ( x , to channel first bias ( b ) ) else : raise Value Error ( 'invalid data format: %s' % data format )", "predictions": ["add to add state state state state state state state state state state state state state state state state state state state state state state state state state state state state"], "references": ["alternative implementation of tf . nn . bias_add which is compatiable with tensorrt ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 64, "code": "def batch normalization ( x , mean , variance , offset , scale , variance epsilon , data format , name = None ) : with ops . name scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math ops . rsqrt ( variance + variance epsilon ) if scale is not None : inv *= scale a = math ops . cast ( inv , x . dtype ) b = math ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) df = { 'channels first' : 'NCHW' , 'channels last' : 'NHWC' } return bias add ( bias scale ( x , a , df [ data format ] ) , b , df [ data format ] )", "predictions": ["batch normalization normalization function"], "references": ["data format aware version of tf . nn . batch_normalization ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 65, "code": "def compute alpha ( x ) : threshold = compute threshold ( x ) alpha1 temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros like ( x , tf . float32 ) ) alpha1 temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros like ( x , tf . float32 ) ) alpha array = tf . add ( alpha1 temp1 , alpha1 temp2 , name = None ) alpha array abs = tf . abs ( alpha array ) alpha array abs1 = tf . where ( tf . greater ( alpha array abs , 0 ) , tf . ones like ( alpha array abs , tf . float32 ) , tf . zeros like ( alpha array abs , tf . float32 ) ) alpha sum = tf . reduce sum ( alpha array abs ) n = tf . reduce sum ( alpha array abs1 ) alpha = tf . div ( alpha sum , n ) return alpha", "predictions": ["compute alpha - th convolution ."], "references": ["computing the scale parameter ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 66, "code": "def ternary operation ( x ) : g = tf . get default graph ( ) with g . gradient override map ( { \"Sign\" : \"Identity\" } ) : threshold = compute threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x", "predictions": ["a ternary operation that implements the cli ."], "references": ["ternary operation use threshold computed with weights ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 67, "code": "def add deprecated function notice to docstring ( doc , date , instructions ) : if instructions : deprecation message = % ( ( 'in a future version' if date is None else ( 'after %s' % date ) ) , instructions ) else : deprecation message = % ( ( 'in a future version' if date is None else ( 'after %s' % date ) ) ) main text = [ deprecation message ] return add notice to docstring ( doc , 'DEPRECATED FUNCTION' , main text )", "predictions": ["add a deprecated function to a deprecated function ."], "references": ["adds a deprecation notice to a docstring for deprecated functions ."], "bleu": 0.1768796183625973, "rouge_l": 0.4911433172302737}
{"id": 68, "code": "def add notice to docstring ( doc , no doc str , notice ) : if not doc : lines = [ no doc str ] else : lines = normalize docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\\n' . join ( lines )", "predictions": ["add a notice to a docstring"], "references": ["adds a deprecation notice to a docstring ."], "bleu": 0.46105843756805864, "rouge_l": 0.6963470319634703}
{"id": 69, "code": "def word to id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk id", "predictions": ["convert a word to a vocab ."], "references": ["returns the integer id of a word string ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 70, "code": "def word to id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk id", "predictions": ["convert a word to a vocab ."], "references": ["returns the integer word id of a word string ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 71, "code": "def id to word ( self , word id ) : if word id >= len ( self . reverse vocab ) : return self . reverse vocab [ self . unk id ] else : return self . reverse vocab [ word id ]", "predictions": ["return the vocab for the given word id ."], "references": ["returns the word string of an integer word id ."], "bleu": 0.24855227187657006, "rouge_l": 0.41709401709401706}
{"id": 72, "code": "def get Logger ( cls , log Level = None ) : logger = logging . get Logger ( \".\" . join ( [ 'com.numenta' , MODULE NAME , cls . name ] ) ) if log Level is not None : logger . set Level ( log Level ) return logger", "predictions": ["returns a logger instance for the class ."], "references": ["gets a logger for the given class in this module"], "bleu": 0.21632118787624222, "rouge_l": 0.5446428571428571}
{"id": 73, "code": "def close ( self ) : self . logger . info ( \"Closing\" ) if self . conn is not None : self . conn . close ( ) self . conn = None else : self . logger . warning ( \"close() called, but connection policy was alredy closed\" ) return", "predictions": ["close the connection to the connection ."], "references": ["close the policy instance and its shared database connection ."], "bleu": 0.20024850746991507, "rouge_l": 0.45607476635514016}
{"id": 74, "code": "def close ( self ) : self . logger . info ( \"Closing\" ) if self . pool is not None : self . pool . close ( ) self . pool = None else : self . logger . warning ( \"close() called, but connection policy was alredy closed\" ) return", "predictions": ["close the connection ."], "references": ["close the policy instance and its database connection pool ."], "bleu": 0.11988448048923712, "rouge_l": 0.5304347826086957}
{"id": 75, "code": "def close ( self ) : self . logger . info ( \"Closing\" ) if self . opened : self . opened = False else : self . logger . warning ( \"close() called, but connection policy was alredy closed\" ) return", "predictions": ["close the connection ."], "references": ["close the policy instance ."], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 76, "code": "def classify State ( self , state ) : if state . ROWID < self . get Parameter ( 'train Records' ) : if not state . set By User : state . anomaly Label = [ ] self . delete Records From KNN ( [ state ] ) return label = KNN Anomaly Classifier Region . AUTO THRESHOLD CLASSIFIED LABEL auto Label = label + KNN Anomaly Classifier Region . AUTO TAG new Category = self . recompute Record From KNN ( state ) label List = self . category To Label List ( new Category ) if state . set By User : if label in state . anomaly Label : state . anomaly Label . remove ( label ) if auto Label in state . anomaly Label : state . anomaly Label . remove ( auto Label ) label List . extend ( state . anomaly Label ) if state . anomaly Score >= self . get Parameter ( 'anomaly Threshold' ) : label List . append ( label ) elif label in label List : ind = label List . index ( label ) label List [ ind ] = auto Label label List = list ( set ( label List ) ) if label in label List and auto Label in label List : label List . remove ( auto Label ) if state . anomaly Label == label List : return state . anomaly Label = label List if state . anomaly Label == [ ] : self . delete Records From KNN ( [ state ] ) else : self . add Record To KNN ( state )", "predictions": ["classify a state and all its anomaly anomaly anomaly"], "references": ["reclassifies given state ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 77, "code": "def add Record To KNN ( self , record ) : knn = self . knnclassifier . knn prototype idx = self . knnclassifier . get Parameter ( 'category Recency List' ) category = self . label List To Category Number ( record . anomaly Label ) if record . ROWID in prototype idx : knn . prototype Set Category ( record . ROWID , category ) return pattern = self . get State Anomaly Vector ( record ) row ID = record . ROWID knn . learn ( pattern , category , row ID = row ID )", "predictions": ["add a record to the log ."], "references": ["adds the record to the knn classifier ."], "bleu": 0.31689174383082924, "rouge_l": 0.5269978401727862}
{"id": 78, "code": "def recompute Record From KNN ( self , record ) : inputs = { \"category In\" : [ None ] , \"bottom Up In\" : self . get State Anomaly Vector ( record ) , } outputs = { \"categories Out\" : numpy . zeros ( ( 1 , ) ) , \"best Prototype Indices\" : numpy . zeros ( ( 1 , ) ) , \"category Probabilities Out\" : numpy . zeros ( ( 1 , ) ) } classifier indexes = numpy . array ( self . knnclassifier . get Parameter ( 'category Recency List' ) ) valid idx = numpy . where ( ( classifier indexes >= self . get Parameter ( 'train Records' ) ) & ( classifier indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid idx ) == 0 : return None self . knnclassifier . set Parameter ( 'inference Mode' , None , True ) self . knnclassifier . set Parameter ( 'learning Mode' , None , False ) self . knnclassifier . compute ( inputs , outputs ) self . knnclassifier . set Parameter ( 'learning Mode' , None , True ) classifier distances = self . knnclassifier . get Latest Distances ( ) valid distances = classifier distances [ valid idx ] if valid distances . min ( ) <= self . classification Max Dist : classifier indexes prev = classifier indexes [ valid idx ] row ID = classifier indexes prev [ valid distances . argmin ( ) ] index ID = numpy . where ( classifier indexes == row ID ) [ 0 ] [ 0 ] category = self . knnclassifier . get Category List ( ) [ index ID ] return category return None", "predictions": ["recompute the category from a record ."], "references": ["returns the classified labeling of record"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 79, "code": "def category To Label List ( self , category ) : if category is None : return [ ] label List = [ ] label Num = 0 while category > 0 : if category % 2 == 1 : label List . append ( self . saved categories [ label Num ] ) label Num += 1 category = category >> 1 return label List", "predictions": ["convert a category object to a list of label numbers ."], "references": ["converts a category number into a list of labels"], "bleu": 0.2521193618434983, "rouge_l": 0.5091819699499166}
{"id": 80, "code": "def get State Anomaly Vector ( self , state ) : vector = numpy . zeros ( self . anomaly Vector Length ) vector [ state . anomaly Vector ] = 1 return vector", "predictions": ["return the vector for the specified state ."], "references": ["returns a state s anomaly vertor converting it from spare to dense"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 81, "code": "def match ( self , record ) : for field , meta in self . filter Dict . iteritems ( ) : index = meta [ 'index' ] categories = meta [ 'categories' ] for category in categories : if not record : continue if record [ index ] . find ( category ) != - 1 : return True return False", "predictions": ["match record in categories"], "references": ["returns true if the record matches any of the provided filters"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 82, "code": "def update Boost Factors Global ( self ) : if ( self . local Area Density > 0 ) : target Density = self . local Area Density else : inhibition Area = ( ( 2 * self . inhibition Radius + 1 ) ** self . column Dimensions . size ) inhibition Area = min ( self . num Columns , inhibition Area ) target Density = float ( self . num Active Columns Per Inh Area ) / inhibition Area target Density = min ( target Density , 0.5 ) self . boost Factors = numpy . exp ( ( target Density - self . active Duty Cycles ) * self . boost Strength )", "predictions": ["updates the camera factors to reflect the boost ."], "references": ["update boost factors when global inhibition is used"], "bleu": 0.15619699684601276, "rouge_l": 0.1189083820662768}
{"id": 83, "code": "def update Boost Factors Local ( self ) : target Density = numpy . zeros ( self . num Columns , dtype = real D Type ) for i in xrange ( self . num Columns ) : mask Neighbors = self . get Column Neighborhood ( i ) target Density [ i ] = numpy . mean ( self . active Duty Cycles [ mask Neighbors ] ) self . boost Factors = numpy . exp ( ( target Density - self . active Duty Cycles ) * self . boost Strength )", "predictions": ["update the mean factors ."], "references": ["update boost factors when local inhibition is used"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 84, "code": "def seed ( self , seed = - 1 ) : if seed != - 1 : self . random = Nupic Random ( seed ) else : self . random = Nupic Random ( )", "predictions": ["initialize a random seed"], "references": ["initialize the random seed"], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 85, "code": "def get Input Names ( self ) : inputs = self . get Spec ( ) . inputs return [ inputs . get By Index ( i ) [ 0 ] for i in xrange ( inputs . get Count ( ) ) ]", "predictions": ["returns a list of table names of the table ."], "references": ["returns list of input names in spec ."], "bleu": 0.1972940627795883, "rouge_l": 0.5669144981412639}
{"id": 86, "code": "def get Output Names ( self ) : outputs = self . get Spec ( ) . outputs return [ outputs . get By Index ( i ) [ 0 ] for i in xrange ( outputs . get Count ( ) ) ]", "predictions": ["returns a list of table names of the current sc2 sc2 ."], "references": ["returns list of output names in spec ."], "bleu": 0.16261701715194898, "rouge_l": 0.5187074829931972}
{"id": 87, "code": "def is Sprint Completed ( self , sprint Idx ) : num Existing Sprints = len ( self . state [ 'sprints' ] ) if sprint Idx >= num Existing Sprints : return False return ( self . state [ 'sprints' ] [ sprint Idx ] [ 'status' ] == 'completed' )", "predictions": ["check if the sprint is sprint"], "references": ["return true if the given sprint has completed ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 88, "code": "def create Encoder ( ) : consumption encoder = Scalar Encoder ( 21 , 0.0 , 100.0 , n = 50 , name = \"consumption\" , clip Input = True ) time encoder = Date Encoder ( time Of Day = ( 21 , 9.5 ) , name = \"timestamp time Of Day\" ) encoder = Multi Encoder ( ) encoder . add Encoder ( \"consumption\" , consumption encoder ) encoder . add Encoder ( \"timestamp\" , time encoder ) return encoder", "predictions": ["function creates a encoder model"], "references": ["create the encoder instance for our test and return it ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 89, "code": "def validate Experiment Control ( self , control ) : task List = control . get ( 'tasks' , None ) if task List is not None : task Labels List = [ ] for task in task List : validate Opf Json Value ( task , \"opf Task Schema.json\" ) validate Opf Json Value ( task [ 'task Control' ] , \"opf Task Control Schema.json\" ) task Label = task [ 'task Label' ] assert isinstance ( task Label , types . String Types ) , \"task Label type: %r\" % type ( task Label ) assert len ( task Label ) > 0 , \"empty string task Label not is allowed\" task Labels List . append ( task Label . lower ( ) ) task Label Duplicates = filter ( lambda x : task Labels List . count ( x ) > 1 , task Labels List ) assert len ( task Label Duplicates ) == 0 , \"Duplcate task labels are not allowed: %s\" % task Label Duplicates return", "predictions": ["check that the task is valid and not allowed: ."], "references": ["validates control dictionary for the experiment context"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 90, "code": "def r Update ( original , updates ) : dict Pairs = [ ( original , updates ) ] while len ( dict Pairs ) > 0 : original , updates = dict Pairs . pop ( ) for k , v in updates . iteritems ( ) : if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : dict Pairs . append ( ( original [ k ] , v ) ) else : original [ k ] = v", "predictions": ["replace a list with a list of updates with a nested dict ."], "references": ["recursively updates the values in original with the values from updates ."], "bleu": 0.1135935489027116, "rouge_l": 0.2417437252311757}
{"id": 91, "code": "def seed ( self , seed = - 1 ) : if seed != - 1 : self . random = Nupic Random ( seed ) else : self . random = Nupic Random ( )", "predictions": ["initialize a random seed"], "references": ["initialize the random seed"], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 92, "code": "def get Scalars ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : return numpy . array ( [ 0 ] ) index = self . category To Index . get ( input , None ) if index is None : if self . learning Enabled : self . add Category ( input ) index = self . ncategories - 1 else : index = 0 return numpy . array ( [ index ] )", "predictions": ["return a numpy array object for the given input input input input input input input input input input input input input input input input input input input input input input input"], "references": ["see method description in base . py"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 93, "code": "def decode ( self , encoded , parent Field Name = '' ) : assert ( encoded [ 0 : self . n ] <= 1.0 ) . all ( ) result String = \"\" result Ranges = [ ] overlaps = ( self . sdrs * encoded [ 0 : self . n ] ) . sum ( axis = 1 ) if self . verbosity >= 2 : print \"Overlaps for decoding:\" for i in xrange ( 0 , self . ncategories ) : print \"%d %s\" % ( overlaps [ i ] , self . categories [ i ] ) matching Categories = ( overlaps > self . threshold Overlap ) . nonzero ( ) [ 0 ] for index in matching Categories : if result String != \"\" : result String += \" \" result String += str ( self . categories [ index ] ) result Ranges . append ( [ int ( index ) , int ( index ) ] ) if parent Field Name != '' : field Name = \"%s.%s\" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( result Ranges , result String ) } , [ field Name ] )", "predictions": ["decode a single string to a python dictionary"], "references": ["see the function description in base . py"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 94, "code": "def get Bucket Info ( self , buckets ) : if self . ncategories == 0 : return 0 top Down Mapping M = self . get Top Down Mapping ( ) category Index = buckets [ 0 ] category = self . categories [ category Index ] encoding = top Down Mapping M . get Row ( category Index ) return [ Encoder Result ( value = category , scalar = category Index , encoding = encoding ) ]", "predictions": ["return the list of categories for each category in the group ."], "references": ["see the function description in base . py"], "bleu": 0.1235622127262679, "rouge_l": 0.3112244897959184}
{"id": 95, "code": "def top Down Compute ( self , encoded ) : if self . ncategories == 0 : return 0 top Down Mapping M = self . get Top Down Mapping ( ) category Index = top Down Mapping M . right Vec Prod ( encoded ) . argmax ( ) category = self . categories [ category Index ] encoding = top Down Mapping M . get Row ( category Index ) return Encoder Result ( value = category , scalar = category Index , encoding = encoding )", "predictions": ["return the top - level top - left axes ."], "references": ["see the function description in base . py"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 96, "code": "def get Scalar Names ( self , parent Field Name = '' ) : names = [ ] def form Field Name ( encoder ) : if parent Field Name == '' : return encoder . name else : return '%s.%s' % ( parent Field Name , encoder . name ) if self . season Encoder is not None : names . append ( form Field Name ( self . season Encoder ) ) if self . day Of Week Encoder is not None : names . append ( form Field Name ( self . day Of Week Encoder ) ) if self . custom Days Encoder is not None : names . append ( form Field Name ( self . custom Days Encoder ) ) if self . weekend Encoder is not None : names . append ( form Field Name ( self . weekend Encoder ) ) if self . holiday Encoder is not None : names . append ( form Field Name ( self . holiday Encoder ) ) if self . time Of Day Encoder is not None : names . append ( form Field Name ( self . time Of Day Encoder ) ) return names", "predictions": ["returns the = = = = 0 for 1"], "references": ["see method description in base . py"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 97, "code": "def get Encoded Values ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : return numpy . array ( [ None ] ) assert isinstance ( input , datetime . datetime ) values = [ ] timetuple = input . timetuple ( ) time Of Day = timetuple . tm hour + float ( timetuple . tm min ) / 60.0 if self . season Encoder is not None : day Of Year = timetuple . tm yday values . append ( day Of Year - 1 ) if self . day Of Week Encoder is not None : day Of Week = timetuple . tm wday + time Of Day / 24.0 values . append ( day Of Week ) if self . weekend Encoder is not None : if timetuple . tm wday == 6 or timetuple . tm wday == 5 or ( timetuple . tm wday == 4 and time Of Day > 18 ) : weekend = 1 else : weekend = 0 values . append ( weekend ) if self . custom Days Encoder is not None : if timetuple . tm wday in self . custom Days : custom Day = 1 else : custom Day = 0 values . append ( custom Day ) if self . holiday Encoder is not None : if len ( self . holidays ) == 0 : holidays = [ ( 12 , 25 ) ] else : holidays = self . holidays val = 0 for h in holidays : if len ( h ) == 3 : hdate = datetime . datetime ( h [ 0 ] , h [ 1 ] , h [ 2 ] , 0 , 0 , 0 ) else : hdate = datetime . datetime ( timetuple . tm year , h [ 0 ] , h [ 1 ] , 0 , 0 , 0 ) if input > hdate : diff = input - hdate if diff . days == 0 : val = 1 break elif diff . days == 1 : val = 1.0 - ( float ( diff . seconds ) / 86400 ) break else : diff = hdate - input if diff . days == 0 : val = 1.0 - ( float ( diff . seconds ) / 86400 ) values . append ( val ) if self . time Of Day Encoder is not None : values . append ( time Of Day ) return values", "predictions": ["compute the values of the threshold for the given threshold ."], "references": ["see method description in base . py"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 98, "code": "def get Bucket Indices ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : return [ None ] * len ( self . encoders ) else : assert isinstance ( input , datetime . datetime ) scalars = self . get Scalars ( input ) result = [ ] for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] result . extend ( encoder . get Bucket Indices ( scalars [ i ] ) ) return result", "predictions": ["get the sign because the sign has been sign"], "references": ["see method description in base . py"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 99, "code": "def encode Into Array ( self , input , output ) : if input == SENTINEL VALUE FOR MISSING DATA : output [ 0 : ] = 0 else : if not isinstance ( input , datetime . datetime ) : raise Value Error ( \"Input is type %s, expected datetime. Value: %s\" % ( type ( input ) , str ( input ) ) ) scalars = self . get Scalars ( input ) for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] encoder . encode Into Array ( scalars [ i ] , output [ offset : ] )", "predictions": ["add the given input as a binary string ."], "references": ["see method description in base . py"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 100, "code": "def get Spec ( cls ) : spec = { \"description\" : Identity Region . doc , \"single Node Only\" : True , \"inputs\" : { \"in\" : { \"description\" : \"The input vector.\" , \"data Type\" : \"Real32\" , \"count\" : 0 , \"required\" : True , \"region Level\" : False , \"is Default Input\" : True , \"require Splitter Map\" : False } , } , \"outputs\" : { \"out\" : { \"description\" : \"A copy of the input vector.\" , \"data Type\" : \"Real32\" , \"count\" : 0 , \"region Level\" : True , \"is Default Output\" : True } , } , \"parameters\" : { \"data Width\" : { \"description\" : \"Size of inputs\" , \"access Mode\" : \"Read\" , \"data Type\" : \"U Int32\" , \"count\" : 1 , \"constraints\" : \"\" } , } , } return spec", "predictions": ["return base spec spec spec . see base class for more info ."], "references": ["return the spec for identityregion ."], "bleu": 0.12011055432195765, "rouge_l": 0.45101663585951934}
{"id": 101, "code": "def add Record To KNN ( self , record ) : classifier = self . htm prediction model . get Anomaly Classifier ( ) knn = classifier . get Self ( ) . knn prototype idx = classifier . get Self ( ) . get Parameter ( 'category Recency List' ) category = self . label List To Category Number ( record . anomaly Label ) if record . ROWID in prototype idx : knn . prototype Set Category ( record . ROWID , category ) return pattern = self . get State Anomaly Vector ( record ) row ID = record . ROWID knn . learn ( pattern , category , row ID = row ID )", "predictions": ["word a record to the table"], "references": ["this method will add the record to the knn classifier ."], "bleu": 0.17867793336200125, "rouge_l": 0.33516483516483514}
{"id": 102, "code": "def recompute Record From KNN ( self , record ) : inputs = { \"category In\" : [ None ] , \"bottom Up In\" : self . get State Anomaly Vector ( record ) , } outputs = { \"categories Out\" : numpy . zeros ( ( 1 , ) ) , \"best Prototype Indices\" : numpy . zeros ( ( 1 , ) ) , \"category Probabilities Out\" : numpy . zeros ( ( 1 , ) ) } classifier = self . htm prediction model . get Anomaly Classifier ( ) knn = classifier . get Self ( ) . knn classifier indexes = numpy . array ( classifier . get Self ( ) . get Parameter ( 'category Recency List' ) ) valid idx = numpy . where ( ( classifier indexes >= self . auto Detect Wait Records ) & ( classifier indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid idx ) == 0 : return None classifier . set Parameter ( 'inference Mode' , True ) classifier . set Parameter ( 'learning Mode' , False ) classifier . get Self ( ) . compute ( inputs , outputs ) classifier . set Parameter ( 'learning Mode' , True ) classifier distances = classifier . get Self ( ) . get Latest Distances ( ) valid distances = classifier distances [ valid idx ] if valid distances . min ( ) <= self . classification Max Dist : classifier indexes prev = classifier indexes [ valid idx ] row ID = classifier indexes prev [ valid distances . argmin ( ) ] index ID = numpy . where ( classifier indexes == row ID ) [ 0 ] [ 0 ] category = classifier . get Self ( ) . get Category List ( ) [ index ID ] return category return None", "predictions": ["word for the knn and returns a dictionary with the ."], "references": ["return the classified labeling of record"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 103, "code": "def compute ( self ) : result = self . construct Classification Record ( ) if result . ROWID >= self . auto Detect Wait Records : self . update State ( result ) self . saved states . append ( result ) if len ( self . saved states ) > self . history length : self . saved states . pop ( 0 ) return result", "predictions": ["computes and return a new states with the saved . . . . . . . . . ."], "references": ["run an iteration of this anomaly classifier"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 104, "code": "def set Auto Detect Wait Records ( self , wait Records ) : if not isinstance ( wait Records , int ) : raise HTM Prediction Model Invalid Argument ( \"Invalid argument type \\'%s\\'. Wait Record \" \"must be a number.\" % ( type ( wait Records ) ) ) if len ( self . saved states ) > 0 and wait Records < self . saved states [ 0 ] . ROWID : raise HTM Prediction Model Invalid Argument ( \"Invalid value. auto Detect Wait Record value \" \"must be valid record within output stream. Current minimum ROWID in \" \"output stream is %d.\" % ( self . saved states [ 0 ] . ROWID ) ) self . auto Detect Wait Records = wait Records for state in self . saved states : self . update State ( state )", "predictions": ["get and set a saved property . . . . . . . . . . . . . . object"], "references": ["sets the autodetectwaitrecords ."], "bleu": 0.05809665204409193, "rouge_l": 0.09118086696562032}
{"id": 105, "code": "def allocate Spatial FDR ( self , rf Input ) : if self . sfdr : return auto Args = dict ( ( name , getattr ( self , name ) ) for name in self . spatial Arg Names ) if ( ( self . Spatial Class == CPP Spatial Pooler ) or ( self . Spatial Class == PY Spatial Pooler ) ) : auto Args [ 'column Dimensions' ] = [ self . column Count ] auto Args [ 'input Dimensions' ] = [ self . input Width ] auto Args [ 'potential Radius' ] = self . input Width self . sfdr = self . Spatial Class ( * * auto Args )", "predictions": ["close the object with the given logger is not defined"], "references": ["allocate the spatial pooler instance ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 106, "code": "def compute ( self , inputs , outputs ) : #if self.top Down Mode and (not 'top Down In' in inputs): if self . sfdr is None : raise Runtime Error ( \"Spatial pooler has not been initialized\" ) if not self . top Down Mode : # # self . iterations += 1 bu Input Vector = inputs [ 'bottom Up In' ] reset Signal = False if 'reset In' in inputs : assert len ( inputs [ 'reset In' ] ) == 1 reset Signal = inputs [ 'reset In' ] [ 0 ] != 0 rf Output = self . do Bottom Up Compute ( rf Input = bu Input Vector . reshape ( ( 1 , bu Input Vector . size ) ) , reset Signal = reset Signal ) outputs [ 'bottom Up Out' ] [ : ] = rf Output . flat else : # # top Down In = inputs . get ( 'top Down In' , None ) spatial Top Down Out , temporal Top Down Out = self . do Top Down Infer ( top Down In ) outputs [ 'spatial Top Down Out' ] [ : ] = spatial Top Down Out if temporal Top Down Out is not None : outputs [ 'temporal Top Down Out' ] [ : ] = temporal Top Down Out outputs [ 'anomaly Score' ] [ : ] = 0", "predictions": ["close the inputs ."], "references": ["run one iteration of spregion s compute"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 107, "code": "def init Ephemerals ( self ) : if hasattr ( self , ' sfdr' ) and self . sfdr : self . spatial Pooler Output = numpy . zeros ( self . column Count , dtype = Get NTA Real ( ) ) else : self . spatial Pooler Output = None self . fp Log SP Input = None self . fp Log SP = None self . fp Log SP Dense = None self . log Path Input = \"\" self . log Path Output = \"\" self . log Path Output Dense = \"\"", "predictions": ["initialize the ephemerals and log ."], "references": ["initialize all ephemerals used by derived classes ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 108, "code": "def get TP Class ( temporal Imp ) : if temporal Imp == 'py' : return backtracking tm . Backtracking TM elif temporal Imp == 'cpp' : return backtracking tm cpp . Backtracking TMCPP elif temporal Imp == 'tm py' : return backtracking tm shim . TM Shim elif temporal Imp == 'tm cpp' : return backtracking tm shim . TMCPP Shim elif temporal Imp == 'monitored tm py' : return backtracking tm shim . Monitored TM Shim else : raise Runtime Error ( \"Invalid temporal Imp '%s'. Legal values are: 'py', \" \"'cpp', 'tm py', 'monitored tm py'\" % ( temporal Imp ) )", "predictions": ["return is a tp instance for a given temporal ."], "references": ["return the class corresponding to the given temporalimp string"], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 109, "code": "def compute ( self , inputs , outputs ) : #if self.top Down Mode and (not 'top Down In' in inputs): if self . tfdr is None : raise Runtime Error ( \"TM has not been initialized\" ) self . conditional Break ( ) self . iterations += 1 bu Input Vector = inputs [ 'bottom Up In' ] reset Signal = False if 'reset In' in inputs : assert len ( inputs [ 'reset In' ] ) == 1 if inputs [ 'reset In' ] [ 0 ] != 0 : self . tfdr . reset ( ) self . sequence Pos = 0 if self . compute Predicted Active Cell Indices : prev Predicted State = self . tfdr . get Predicted State ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomaly Mode : prev Predicted Columns = self . tfdr . top Down Compute ( ) . copy ( ) . nonzero ( ) [ 0 ] tp Output = self . tfdr . compute ( bu Input Vector , self . learning Mode , self . inference Mode ) self . sequence Pos += 1 if self . or Column Outputs : tp Output = tp Output . reshape ( self . column Count , self . cells Per Column ) . max ( axis = 1 ) if self . fp Log TP Output : output = tp Output . reshape ( - 1 ) output NZ = tp Output . nonzero ( ) [ 0 ] out Str = \" \" . join ( [ \"%d\" % int ( token ) for token in output NZ ] ) print >> self . fp Log TP Output , output . size , out Str outputs [ 'bottom Up Out' ] [ : ] = tp Output . flat if self . top Down Mode : outputs [ 'top Down Out' ] [ : ] = self . tfdr . top Down Compute ( ) . copy ( ) if self . anomaly Mode : active Learn Cells = self . tfdr . get Learn Active State T ( ) size = active Learn Cells . shape [ 0 ] * active Learn Cells . shape [ 1 ] outputs [ 'lrn Active State T' ] [ : ] = active Learn Cells . reshape ( size ) active Columns = bu Input Vector . nonzero ( ) [ 0 ] outputs [ 'anomaly Score' ] [ : ] = anomaly . compute Raw Anomaly Score ( active Columns , prev Predicted Columns ) if self . compute Predicted Active Cell Indices : active State = self . tfdr . get Active State ( ) . reshape ( - 1 ) . astype ( 'float32' ) active Indices = numpy . where ( active State != 0 ) [ 0 ] predicted Indices = numpy . where ( prev Predicted State != 0 ) [ 0 ] predicted Active Indices = numpy . intersect1d ( active Indices , predicted Indices ) outputs [ \"active Cells\" ] . fill ( 0 ) outputs [ \"active Cells\" ] [ active Indices ] = 1 outputs [ \"predicted Active Cells\" ] . fill ( 0 ) outputs [ \"predicted Active Cells\" ] [ predicted Active Indices ] = 1", "predictions": ["add the inference step to the output"], "references": ["run one iteration of tmregion s compute"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 110, "code": "def get Version ( ) : with open ( os . path . join ( REPO DIR , \"VERSION\" ) , \"r\" ) as version File : return version File . read ( ) . strip ( )", "predictions": ["return version without importing the version ."], "references": ["get version from local file ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 111, "code": "def generate Extra Metric Specs ( options ) : metric Spec Schema = { 'properties' : { } } results = [ ] for metric in options [ 'metrics' ] : for property Name in metric Spec Schema [ 'properties' ] . keys ( ) : get Property Value ( metric Spec Schema , property Name , metric ) spec String , label = generate Metric Spec String ( field = metric [ 'field' ] , metric = metric [ 'metric' ] , params = metric [ 'params' ] , inference Element = metric [ 'inference Element' ] , return Label = True ) if metric [ 'logged' ] : options [ 'logged Metrics' ] . append ( label ) results . append ( spec String ) return results", "predictions": ["category category to category inference"], "references": ["generates the non - default metrics specified by the expgenerator params"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 112, "code": "def hash Coordinate ( coordinate ) : coordinate Str = \",\" . join ( str ( v ) for v in coordinate ) hash = int ( int ( hashlib . md5 ( coordinate Str ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) return hash", "predictions": ["get a get get a get get get get a get get get get get get a get get a get get a get get get get of the md5 coordinate"], "references": ["hash a coordinate to a 64 bit integer ."], "bleu": 0.046398855339878003, "rouge_l": 0.11101000909918107}
{"id": 113, "code": "def same TM Params ( tp1 , tp2 ) : result = True for param in [ \"number Of Cols\" , \"cells Per Column\" , \"initial Perm\" , \"connected Perm\" , \"min Threshold\" , \"new Synapse Count\" , \"permanence Inc\" , \"permanence Dec\" , \"permanence Max\" , \"global Decay\" , \"activation Threshold\" , \"do Pooling\" , \"seg Update Valid Duration\" , \"burn In\" , \"pam Length\" , \"max Age\" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , \"is different\" print getattr ( tp1 , param ) , \"vs\" , getattr ( tp2 , param ) result = False return result", "predictions": ["calculates the range of points that have no match"], "references": ["given two tm instances see if any parameters are different ."], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 114, "code": "def same Segment ( seg1 , seg2 ) : result = True for field in [ 1 , 2 , 3 , 4 , 5 , 6 ] : if abs ( seg1 [ 0 ] [ field ] - seg2 [ 0 ] [ field ] ) > 0.001 : result = False if len ( seg1 [ 1 : ] ) != len ( seg2 [ 1 : ] ) : result = False for syn in seg2 [ 1 : ] : if syn [ 2 ] <= 0 : print \"A synapse with zero permanence encountered\" result = False if result == True : for syn in seg1 [ 1 : ] : if syn [ 2 ] <= 0 : print \"A synapse with zero permanence encountered\" result = False res = same Synapse ( syn , seg2 [ 1 : ] ) if res == False : result = False return result", "predictions": ["calculates whether seg1 is true or not"], "references": ["return true if seg1 and seg2 are identical ignoring order of synapses"], "bleu": 0.10063351655856649, "rouge_l": 0.10049423393739704}
{"id": 115, "code": "def log Probability ( self , distn ) : x = numpy . asarray ( distn ) n = x . sum ( ) return ( log Factorial ( n ) - numpy . sum ( [ log Factorial ( k ) for k in x ] ) + numpy . sum ( x * numpy . log ( self . dist . pmf ) ) )", "predictions": ["compute update probability of the update function"], "references": ["form of distribution must be an array of counts in order of self . keys ."], "bleu": 0.05135131375181345, "rouge_l": 0.08122503328894808}
{"id": 116, "code": "def create Data Out Link ( network , sensor Region Name , region Name ) : network . link ( sensor Region Name , region Name , \"Uniform Link\" , \"\" , src Output = \"data Out\" , dest Input = \"bottom Up In\" )", "predictions": ["seed sensor to seed name"], "references": ["link sensor region to other region so that it can pass it data ."], "bleu": 0.04994299940831281, "rouge_l": 0.19395866454689983}
{"id": 117, "code": "def create Sensor To Classifier Links ( network , sensor Region Name , classifier Region Name ) : network . link ( sensor Region Name , classifier Region Name , \"Uniform Link\" , \"\" , src Output = \"bucket Idx Out\" , dest Input = \"bucket Idx In\" ) network . link ( sensor Region Name , classifier Region Name , \"Uniform Link\" , \"\" , src Output = \"act Value Out\" , dest Input = \"act Value In\" ) network . link ( sensor Region Name , classifier Region Name , \"Uniform Link\" , \"\" , src Output = \"category Out\" , dest Input = \"category In\" )", "predictions": ["get the = = name and i"], "references": ["create required links from a sensor region to a classifier region ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 118, "code": "def create Network ( data Source ) : with open ( PARAMS PATH , \"r\" ) as f : model Params = yaml . safe load ( f ) [ \"model Params\" ] network = Network ( ) network . add Region ( \"sensor\" , \"py.Record Sensor\" , '{}' ) sensor Region = network . regions [ \"sensor\" ] . get Self ( ) sensor Region . encoder = create Encoder ( model Params [ \"sensor Params\" ] [ \"encoders\" ] ) sensor Region . data Source = data Source model Params [ \"sp Params\" ] [ \"input Width\" ] = sensor Region . encoder . get Width ( ) network . add Region ( \"SP\" , \"py.SP Region\" , json . dumps ( model Params [ \"sp Params\" ] ) ) network . add Region ( \"TM\" , \"py.TM Region\" , json . dumps ( model Params [ \"tm Params\" ] ) ) cl Name = \"py.%s\" % model Params [ \"cl Params\" ] . pop ( \"region Name\" ) network . add Region ( \"classifier\" , cl Name , json . dumps ( model Params [ \"cl Params\" ] ) ) create Sensor To Classifier Links ( network , \"sensor\" , \"classifier\" ) create Data Out Link ( network , \"sensor\" , \"SP\" ) create Feed Forward Link ( network , \"SP\" , \"TM\" ) create Feed Forward Link ( network , \"TM\" , \"classifier\" ) create Reset Link ( network , \"sensor\" , \"SP\" ) create Reset Link ( network , \"sensor\" , \"TM\" ) network . initialize ( ) return network", "predictions": ["get a network network"], "references": ["create and initialize a network ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 119, "code": "def get Prediction Results ( network , cl Region Name ) : classifier Region = network . regions [ cl Region Name ] actual Values = classifier Region . get Output Data ( \"actual Values\" ) probabilities = classifier Region . get Output Data ( \"probabilities\" ) steps = classifier Region . get Self ( ) . steps List N = classifier Region . get Self ( ) . max Category Count results = { step : { } for step in steps } for i in range ( len ( steps ) ) : step Probabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] most Likely Category Idx = step Probabilities . argmax ( ) predicted Value = actual Values [ most Likely Category Idx ] prediction Confidence = step Probabilities [ most Likely Category Idx ] results [ steps [ i ] ] [ \"predicted Value\" ] = predicted Value results [ steps [ i ] ] [ \"prediction Confidence\" ] = prediction Confidence return results", "predictions": ["calculate the predicted and return data from a self = predicted"], "references": ["get prediction results for all prediction steps ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 120, "code": "def run Hotgym ( num Records ) : data Source = File Record Stream ( stream ID = INPUT FILE PATH ) num Records = min ( num Records , data Source . get Data Row Count ( ) ) network = create Network ( data Source ) network . regions [ \"sensor\" ] . set Parameter ( \"predicted Field\" , \"consumption\" ) network . regions [ \"SP\" ] . set Parameter ( \"learning Mode\" , 1 ) network . regions [ \"TM\" ] . set Parameter ( \"learning Mode\" , 1 ) network . regions [ \"classifier\" ] . set Parameter ( \"learning Mode\" , 1 ) network . regions [ \"SP\" ] . set Parameter ( \"inference Mode\" , 1 ) network . regions [ \"TM\" ] . set Parameter ( \"inference Mode\" , 1 ) network . regions [ \"classifier\" ] . set Parameter ( \"inference Mode\" , 1 ) results = [ ] N = 1 for iteration in range ( 0 , num Records , N ) : network . run ( N ) prediction Results = get Prediction Results ( network , \"classifier\" ) one Step = prediction Results [ 1 ] [ \"predicted Value\" ] one Step Confidence = prediction Results [ 1 ] [ \"prediction Confidence\" ] five Step = prediction Results [ 5 ] [ \"predicted Value\" ] five Step Confidence = prediction Results [ 5 ] [ \"prediction Confidence\" ] result = ( one Step , one Step Confidence * 100 , five Step , five Step Confidence * 100 ) print \"1-step: {:16} ({:4.4}%)\\t 5-step: {:16} ({:4.4}%)\" . format ( * result ) results . append ( result ) return results", "predictions": ["create a list of hotgym and create a list of prediction with hotgym"], "references": ["run the hot gym example ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 121, "code": "def get Description ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'num Records by field' : [ f . num Records for f in self . fields ] } return description", "predictions": ["validate the not - set of parameters"], "references": ["returns a description of the dataset"], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 122, "code": "def generate Records ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generate Record ( record )", "predictions": ["r r r r r r r r r r r r r r r r r r . create updates updates updates updates updates updates updates updates updates updates updates"], "references": ["generate multiple records . refer to definition for generaterecord"], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 123, "code": "def get Record ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . num Records - 1 assert ( all ( field . num Records > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record", "predictions": ["seed - aware record"], "references": ["returns the nth record"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 124, "code": "def get All Records ( self ) : values = [ ] num Records = self . fields [ 0 ] . num Records assert ( all ( field . num Records == num Records for field in self . fields ) ) for x in range ( num Records ) : values . append ( self . get Record ( x ) ) return values", "predictions": ["get all fields values"], "references": ["returns all the records"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 125, "code": "def add Values To Field ( self , i , num Values ) : assert ( len ( self . fields ) > i ) values = [ self . add Value To Field ( i ) for n in range ( num Values ) ] return values", "predictions": ["decode a string value to a list of 0 ."], "references": ["add values to the field i ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 126, "code": "def get SD Rfor Value ( self , i , j ) : assert len ( self . fields ) > i assert self . fields [ i ] . num Records > j encoding = self . fields [ i ] . encodings [ j ] return encoding", "predictions": ["get the encoding encoding for a given field i"], "references": ["returns the sdr for jth value at column i"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 127, "code": "def get Zeroed Out Encoding ( self , n ) : assert all ( field . num Records > n for field in self . fields ) encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL VALUE FOR MISSING DATA ) if field . is Predicted Field else field . encodings [ n ] for field in self . fields ] ) return encoding", "predictions": ["returns the category category for the given fields 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["returns the nth encoding with the predictedfield zeroed out"], "bleu": 0.055177848898164926, "rouge_l": 0.1665150136487716}
{"id": 128, "code": "def get Totaln ( self ) : n = sum ( [ field . n for field in self . fields ] ) return n", "predictions": ["return the total number of fields"], "references": ["returns the cumulative n for all the fields in the dataset"], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 129, "code": "def get Totalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w", "predictions": ["return a list of the fields that are not the current one ."], "references": ["returns the cumulative w for all the fields in the dataset"], "bleu": 0.1350862565735141, "rouge_l": 0.2538141470180305}
{"id": 130, "code": "def get Encoding ( self , n ) : assert ( all ( field . num Encodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding", "predictions": ["get the encoding for n - 1 . 2 . 0 ."], "references": ["returns the nth encoding"], "bleu": 0.11498759556447223, "rouge_l": 0.27477477477477474}
{"id": 131, "code": "def get All Encodings ( self ) : num Encodings = self . fields [ 0 ] . num Encodings assert ( all ( field . num Encodings == num Encodings for field in self . fields ) ) encodings = [ self . get Encoding ( index ) for index in range ( num Encodings ) ] return encodings", "predictions": ["get the number of encodings to keep the current encodings ."], "references": ["returns encodings for all the records"], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 132, "code": "def remove All Records ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . num Records , field . num Encodings = ( 0 , 0 )", "predictions": ["removes all fields that are marked as free ."], "references": ["deletes all the values in the dataset"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 133, "code": "def encode Value ( self , value , to Be Added = True ) : encoded Value = np . array ( self . encoder . encode ( value ) , dtype = real D Type ) if to Be Added : self . encodings . append ( encoded Value ) self . num Encodings += 1 return encoded Value", "predictions": ["encodes a bytestring as a bytestring ."], "references": ["value is encoded as a sdr using the encoding parameters of the field"], "bleu": 0.10374282717383708, "rouge_l": 0.1897356143079316}
{"id": 134, "code": "def set Types ( self , encoder Spec ) : if self . encoder Type is None : if self . data Type in [ 'int' , 'float' ] : self . encoder Type = 'adaptive Scalar' elif self . data Type == 'string' : self . encoder Type = 'category' elif self . data Type in [ 'date' , 'datetime' ] : self . encoder Type = 'date' if self . data Type is None : if self . encoder Type in [ 'scalar' , 'adaptive Scalar' ] : self . data Type = 'float' elif self . encoder Type in [ 'category' , 'enumeration' ] : self . data Type = 'string' elif self . encoder Type in [ 'date' , 'datetime' ] : self . data Type = 'datetime'", "predictions": ["set type of type"], "references": ["set up the datatypes and initialize encoders"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 135, "code": "def get Scalars ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : return numpy . array ( [ None ] ) else : return numpy . array ( [ self . category To Index . get ( input , 0 ) ] )", "predictions": ["get the category from the db"], "references": ["see method description in base . py"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 136, "code": "def get Bucket Indices ( self , input ) : if input == SENTINEL VALUE FOR MISSING DATA : return [ None ] else : return self . encoder . get Bucket Indices ( self . category To Index . get ( input , 0 ) )", "predictions": ["return the column name for the given input input input input input input input input input input input input input input input input input ."], "references": ["see method description in base . py"], "bleu": 0.048589719316429775, "rouge_l": 0.06955530216647662}
{"id": 137, "code": "def decode ( self , encoded , parent Field Name = '' ) : ( fields Dict , field Names ) = self . encoder . decode ( encoded ) if len ( fields Dict ) == 0 : return ( fields Dict , field Names ) assert ( len ( fields Dict ) == 1 ) ( in Ranges , in Desc ) = fields Dict . values ( ) [ 0 ] out Ranges = [ ] desc = \"\" for ( min V , max V ) in in Ranges : min V = int ( round ( min V ) ) max V = int ( round ( max V ) ) out Ranges . append ( ( min V , max V ) ) while min V <= max V : if len ( desc ) > 0 : desc += \", \" desc += self . index To Category [ min V ] min V += 1 if parent Field Name != '' : field Name = \"%s.%s\" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( out Ranges , desc ) } , [ field Name ] )", "predictions": ["decode a single encoded string to a python dictionary ."], "references": ["see the function description in base . py"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 138, "code": "def get Bucket Values ( self ) : if self . bucket Values is None : num Buckets = len ( self . encoder . get Bucket Values ( ) ) self . bucket Values = [ ] for bucket Index in range ( num Buckets ) : self . bucket Values . append ( self . get Bucket Info ( [ bucket Index ] ) [ 0 ] . value ) return self . bucket Values", "predictions": ["returns a list of buckets for each bucket in the bucket"], "references": ["see the function description in base . py"], "bleu": 0.12605968092174913, "rouge_l": 0.108348134991119}
{"id": 139, "code": "def get Bucket Info ( self , buckets ) : bucket Info = self . encoder . get Bucket Info ( buckets ) [ 0 ] category Index = int ( round ( bucket Info . value ) ) category = self . index To Category [ category Index ] return [ Encoder Result ( value = category , scalar = category Index , encoding = bucket Info . encoding ) ]", "predictions": ["return bucket objects for each category in the given bucket ."], "references": ["see the function description in base . py"], "bleu": 0.1354599427337814, "rouge_l": 0.216696269982238}
{"id": 140, "code": "def top Down Compute ( self , encoded ) : encoder Result = self . encoder . top Down Compute ( encoded ) [ 0 ] value = encoder Result . value category Index = int ( round ( value ) ) category = self . index To Category [ category Index ] return Encoder Result ( value = category , scalar = category Index , encoding = encoder Result . encoding )", "predictions": ["reads the top - most most recent value ."], "references": ["see the function description in base . py"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 141, "code": "def get Stream Def ( self , model Description ) : #-------------------------------------------------------------------------- aggregation Period = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } agg Functions Dict = { } if 'aggregation' in model Description [ 'stream Def' ] : for key in aggregation Period . keys ( ) : if key in model Description [ 'stream Def' ] [ 'aggregation' ] : aggregation Period [ key ] = model Description [ 'stream Def' ] [ 'aggregation' ] [ key ] if 'fields' in model Description [ 'stream Def' ] [ 'aggregation' ] : for ( field Name , func ) in model Description [ 'stream Def' ] [ 'aggregation' ] [ 'fields' ] : agg Functions Dict [ field Name ] = str ( func ) has Aggregation = False for v in aggregation Period . values ( ) : if v != 0 : has Aggregation = True break agg Function List = agg Functions Dict . items ( ) aggregation Info = dict ( aggregation Period ) aggregation Info [ 'fields' ] = agg Function List stream Def = copy . deepcopy ( model Description [ 'stream Def' ] ) stream Def [ 'aggregation' ] = copy . deepcopy ( aggregation Info ) return stream Def", "predictions": ["return a dictionary with the aggregation aggregation aggregation aggregation"], "references": ["generate stream definition based on"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 142, "code": "def engine Services Running ( ) : process = subprocess . Popen ( [ \"ps\" , \"aux\" ] , stdout = subprocess . PIPE ) stdout = process . communicate ( ) [ 0 ] result = process . returncode if result != 0 : raise Runtime Error ( \"Unable to check for running client job manager\" ) running = False for line in stdout . split ( \"\\n\" ) : if \"python\" in line and \"clientjobmanager.client job manager\" in line : running = True break return running", "predictions": ["checks for all running client jobs ."], "references": ["return true if the engine services are running"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 143, "code": "def get Data ( self , n ) : records = [ self . get Next ( ) for x in range ( n ) ] return records", "predictions": ["return a list of n - th records ."], "references": ["returns the next n values for the distribution as a list ."], "bleu": 0.15122637383061946, "rouge_l": 0.2785388127853881}
{"id": 144, "code": "def generate ( self ) : candidates = np . array ( range ( self . n ) , np . uint32 ) for i in xrange ( self . num ) : self . random . shuffle ( candidates ) pattern = candidates [ 0 : self . get W ( ) ] self . patterns [ i ] = set ( pattern )", "predictions": ["generate some numpy array of candidates"], "references": ["generates set of random patterns ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 145, "code": "def get W ( self ) : w = self . w if type ( w ) is list : return w [ self . random . get U Int32 ( len ( w ) ) ] else : return w", "predictions": ["returns a random region as a string ."], "references": ["gets a value of w for use in generating a pattern ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 146, "code": "def generate ( self ) : n = self . n w = self . w assert type ( w ) is int , \"List for w not supported\" for i in xrange ( n / w ) : pattern = set ( xrange ( i * w , ( i + 1 ) * w ) ) self . patterns [ i ] = pattern", "predictions": ["generate a random output implemenation"], "references": ["generates set of consecutive patterns ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 147, "code": "def encode Into Array ( self , value , output ) : dense Input = numpy . zeros ( output . shape ) try : dense Input [ value ] = 1 except Index Error : if isinstance ( value , numpy . ndarray ) : raise Value Error ( \"Numpy array must have integer dtype but got {}\" . format ( value . dtype ) ) raise super ( Sparse Pass Through Encoder , self ) . encode Into Array ( dense Input , output )", "predictions": ["see base class for details ."], "references": ["see method description in base . py"], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 148, "code": "def require Anomaly Model ( func ) : @ wraps ( func ) def decorator ( self , * args , * * kwargs ) : if not self . get Inference Type ( ) == Inference Type . Temporal Anomaly : raise Runtime Error ( \"Method required a Temporal Anomaly model.\" ) if self . get Anomaly Classifier ( ) is None : raise Runtime Error ( \"Model does not support this command. Model must\" \"be an active anomaly Detector model.\" ) return func ( self , * args , * * kwargs ) return decorator", "predictions": ["decorator to require that a function is anomaly to be authenticated ."], "references": ["decorator for functions that require anomaly models ."], "bleu": 0.1367440667823257, "rouge_l": 0.4149659863945578}
{"id": 149, "code": "def anomaly Compute ( self ) : inference Type = self . get Inference Type ( ) inferences = { } sp = self . get SP Region ( ) score = None if inference Type == Inference Type . Nontemporal Anomaly : score = sp . get Output Data ( \"anomaly Score\" ) [ 0 ] #TODO move from SP to Anomaly ? elif inference Type == Inference Type . Temporal Anomaly : tm = self . get TP Region ( ) if sp is not None : active Columns = sp . get Output Data ( \"bottom Up Out\" ) . nonzero ( ) [ 0 ] else : sensor = self . get Sensor Region ( ) active Columns = sensor . get Output Data ( 'data Out' ) . nonzero ( ) [ 0 ] if not self . predicted Field Name in self . input : raise Value Error ( \"Expected predicted field '%s' in input row, but was not found!\" % self . predicted Field Name ) score = tm . get Output Data ( \"anomaly Score\" ) [ 0 ] if sp is not None : self . get Anomaly Classifier ( ) . set Parameter ( \"active Column Count\" , len ( active Columns ) ) self . get Anomaly Classifier ( ) . prepare Inputs ( ) self . get Anomaly Classifier ( ) . compute ( ) labels = self . get Anomaly Classifier ( ) . get Self ( ) . get Label Results ( ) inferences [ Inference Element . anomaly Label ] = \"%s\" % labels inferences [ Inference Element . anomaly Score ] = score return inferences", "predictions": ["this function sets the anomaly score and the predicted score"], "references": ["compute anomaly score if required"], "bleu": 0.16590387014219712, "rouge_l": 0.2837209302325582}
{"id": 150, "code": "def get Classifier Region ( self ) : if ( self . net Info . net is not None and \"Classifier\" in self . net Info . net . regions ) : return self . net Info . net . regions [ \"Classifier\" ] else : return None", "predictions": ["return the string size of the network"], "references": ["returns reference to the network s classifier region"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 151, "code": "def get State ( self ) : return dict ( position = self . position , position = self . get Position ( ) , velocity = self . velocity , best Position = self . best Position , best Result = self . best Result )", "predictions": ["returns a dict with the best best best velocity"], "references": ["see comments in base class ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 152, "code": "def set State ( self , state ) : self . position = state [ ' position' ] self . velocity = state [ 'velocity' ] self . best Position = state [ 'best Position' ] self . best Result = state [ 'best Result' ]", "predictions": ["set the best position for the next state ."], "references": ["see comments in base class ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 153, "code": "def get Position ( self ) : if self . step Size is None : return self . position num Steps = ( self . position - self . min ) / self . step Size num Steps = int ( round ( num Steps ) ) position = self . min + ( num Steps * self . step Size ) position = max ( self . min , position ) position = min ( self . max , position ) return position", "predictions": ["calculate the position of the buffer"], "references": ["see comments in base class ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 154, "code": "def agitate ( self ) : self . velocity *= 1.5 / self . inertia max V = ( self . max - self . min ) / 2 if self . velocity > max V : self . velocity = max V elif self . velocity < - max V : self . velocity = - max V if self . position == self . max and self . velocity > 0 : self . velocity *= - 1 if self . position == self . min and self . velocity < 0 : self . velocity *= - 1", "predictions": ["increase the maximum velocity ."], "references": ["see comments in base class ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 155, "code": "def new Position ( self , global Best Position , rng ) : # lb = float ( Configuration . get ( \"nupic.hypersearch.random Lower Bound\" ) ) ub = float ( Configuration . get ( \"nupic.hypersearch.random Upper Bound\" ) ) self . velocity = ( self . velocity * self . inertia + rng . uniform ( lb , ub ) * self . cog Rate * ( self . best Position - self . get Position ( ) ) ) if global Best Position is not None : self . velocity += rng . uniform ( lb , ub ) * self . soc Rate * ( global Best Position - self . get Position ( ) ) self . position += self . velocity self . position = max ( self . min , self . position ) self . position = min ( self . max , self . position ) return self . get Position ( )", "predictions": ["create a new cog object"], "references": ["see comments in base class ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 156, "code": "def push Away From ( self , other Positions , rng ) : if self . max == self . min : return num Positions = len ( other Positions ) * 4 if num Positions == 0 : return step Size = float ( self . max - self . min ) / num Positions positions = numpy . arange ( self . min , self . max + step Size , step Size ) num Positions = len ( positions ) weights = numpy . zeros ( num Positions ) max Distance Sq = - 1 * ( step Size ** 2 ) for pos in other Positions : distances = pos - positions var Weights = numpy . exp ( numpy . power ( distances , 2 ) / max Distance Sq ) weights += var Weights position Idx = weights . argmin ( ) self . position = positions [ position Idx ] self . best Position = self . get Position ( ) self . velocity *= rng . choice ( [ 1 , - 1 ] )", "predictions": ["push a away to the end of the best axis"], "references": ["see comments in base class ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 157, "code": "def reset Velocity ( self , rng ) : max Velocity = ( self . max - self . min ) / 5.0 self . velocity = max Velocity #min(abs(self. velocity), max Velocity) self . velocity *= rng . choice ( [ 1 , - 1 ] )", "predictions": ["reset the velocity to its current rng level ."], "references": ["see comments in base class ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 158, "code": "def get Position ( self ) : position = super ( Permute Int , self ) . get Position ( ) position = int ( round ( position ) ) return position", "predictions": ["override to get the parent position"], "references": ["see comments in base class ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 159, "code": "def get State ( self ) : return dict ( position = self . get Position ( ) , position = self . get Position ( ) , velocity = None , best Position = self . choices [ self . best Position Idx ] , best Result = self . best Result )", "predictions": ["returns a dictionary with the best best best best best best best position"], "references": ["see comments in base class ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 160, "code": "def set State ( self , state ) : self . position Idx = self . choices . index ( state [ ' position' ] ) self . best Position Idx = self . choices . index ( state [ 'best Position' ] ) self . best Result = state [ 'best Result' ]", "predictions": ["update the best with the next state"], "references": ["see comments in base class ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 161, "code": "def new Position ( self , global Best Position , rng ) : num Choices = len ( self . choices ) mean Score Per Choice = [ ] overall Sum = 0 num Results = 0 for i in range ( num Choices ) : if len ( self . results Per Choice [ i ] ) > 0 : data = numpy . array ( self . results Per Choice [ i ] ) mean Score Per Choice . append ( data . mean ( ) ) overall Sum += data . sum ( ) num Results += data . size else : mean Score Per Choice . append ( None ) if num Results == 0 : overall Sum = 1.0 num Results = 1 for i in range ( num Choices ) : if mean Score Per Choice [ i ] is None : mean Score Per Choice [ i ] = overall Sum / num Results mean Score Per Choice = numpy . array ( mean Score Per Choice ) mean Score Per Choice = ( 1.1 * mean Score Per Choice . max ( ) ) - mean Score Per Choice if self . fix Early : mean Score Per Choice **= ( num Results * self . fix Early Factor / num Choices ) total = mean Score Per Choice . sum ( ) if total == 0 : total = 1.0 mean Score Per Choice /= total distribution = mean Score Per Choice . cumsum ( ) r = rng . random ( ) * distribution [ - 1 ] choice Idx = numpy . where ( r <= distribution ) [ 0 ] [ 0 ] self . position Idx = choice Idx return self . get Position ( )", "predictions": ["build a get request"], "references": ["see comments in base class ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 162, "code": "def push Away From ( self , other Positions , rng ) : positions = [ self . choices . index ( x ) for x in other Positions ] position Counts = [ 0 ] * len ( self . choices ) for pos in positions : position Counts [ pos ] += 1 self . position Idx = numpy . array ( position Counts ) . argmin ( ) self . best Position Idx = self . position Idx", "predictions": ["get input coordinates of ."], "references": ["see comments in base class ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 163, "code": "def get Dict Meta Info ( self , inference Element , inference Dict ) : field Meta Info = [ ] inference Label = Inference Element . get Label ( inference Element ) if Inference Element . get Input Element ( inference Element ) : field Meta Info . append ( Field Meta Info ( name = inference Label + \".actual\" , type = Field Meta Type . string , special = '' ) ) keys = sorted ( inference Dict . keys ( ) ) for key in keys : field Meta Info . append ( Field Meta Info ( name = inference Label + \".\" + str ( key ) , type = Field Meta Type . string , special = '' ) ) return field Meta Info", "predictions": ["build a query tree for an element"], "references": ["get field metadate information for inferences that are of dict type"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 164, "code": "def modify Bits ( input Val , max Changes ) : changes = np . random . random integers ( 0 , max Changes , 1 ) [ 0 ] if changes == 0 : return input Val input Width = len ( input Val ) what To Change = np . random . random integers ( 0 , 41 , changes ) running Index = - 1 num Mods Done = 0 for i in xrange ( input Width ) : if num Mods Done >= changes : break if input Val [ i ] == 1 : running Index += 1 if running Index in what To Change : if i != 0 and input Val [ i - 1 ] == 0 : input Val [ i - 1 ] = 1 input Val [ i ] = 0 return input Val", "predictions": ["remove input with input and bits"], "references": ["modifies up to maxchanges number of bits in the inputval"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 165, "code": "def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\\n' ) ] return '\\n' . join ( lines )", "predictions": ["removes all non - empty characters from a string"], "references": ["removes trailing whitespace on each line ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 166, "code": "def main ( ) : init Logging ( verbose = True ) init Experiment Prng ( ) @ staticmethod def mock Create ( * args , * * kwargs ) : kwargs . pop ( 'implementation' , None ) return SDR Classifier Diff ( * args , * * kwargs ) SDR Classifier Factory . create = mock Create run Experiment ( sys . argv [ 1 : ] )", "predictions": ["set up some application that runs the mock function ."], "references": ["run according to options in sys . argv and diff classifiers ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 167, "code": "def job Cancel All Running Jobs ( self ) : with Connection Factory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobs Table Name , ) conn . cursor . execute ( query , [ self . STATUS COMPLETED ] ) return", "predictions": ["get the number of jobs in the database . ."], "references": ["set cancel field of all currently - running jobs to true ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 168, "code": "def init Ephemerals ( self ) : self . first Compute Call = True self . accuracy = None self . proto Scores = None self . category Distances = None self . knn = knn classifier . KNN Classifier ( * * self . knn Params ) for x in ( ' partitions' , ' use Auxiliary' , ' do Sphering' , ' scan Info' , ' proto Scores' ) : if not hasattr ( self , x ) : setattr ( self , x , None )", "predictions": ["initialize self and ephemerals objects ."], "references": ["initialize attributes that are not saved with the checkpoint ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 169, "code": "def disable Tap ( self ) : if self . tap File In is not None : self . tap File In . close ( ) self . tap File In = None if self . tap File Out is not None : self . tap File Out . close ( ) self . tap File Out = None", "predictions": ["decode the file - like object ."], "references": ["disable writing of output tap files ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 170, "code": "def store Sample ( self , input Vector , true Cat Index , partition = 0 ) : if self . samples is None : self . samples = numpy . zeros ( ( 0 , len ( input Vector ) ) , dtype = Real Numpy D Type ) assert self . labels is None self . labels = [ ] self . samples = numpy . concatenate ( ( self . samples , numpy . atleast 2d ( input Vector ) ) , axis = 0 ) self . labels += [ true Cat Index ] if self . partitions is None : self . partitions = [ ] if partition is None : partition = 0 self . partitions += [ partition ]", "predictions": ["get the source code for the given input"], "references": ["store a training sample and associated category label"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 171, "code": "def finish Learning ( self ) : if self . do Sphering : self . finish Sphering ( ) self . knn . finish Learning ( ) self . accuracy = None", "predictions": ["get the amount of values to start the ."], "references": ["does nothing . kept here for api compatibility"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 172, "code": "def next ( self , new Value ) : new Average , self . sliding Window , self . total = self . compute ( self . sliding Window , self . total , new Value , self . window Size ) return new Average", "predictions": ["top of the page"], "references": ["instance method wrapper around compute ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 173, "code": "def add Instance ( self , ground Truth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )", "predictions": ["get a histogram with a newly added to the heap"], "references": ["compute and store metric value"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 174, "code": "def get Input Value ( self , obj , field Name ) : if isinstance ( obj , dict ) : if not field Name in obj : known Fields = \", \" . join ( key for key in obj . keys ( ) if not key . startswith ( \" \" ) ) raise Value Error ( \"Unknown field name '%s' in input record. Known fields are '%s'.\\n\" \"This could be because input headers are mislabeled, or because \" \"input data rows do not contain a value for '%s'.\" % ( field Name , known Fields , field Name ) ) return obj [ field Name ] else : return getattr ( obj , field Name )", "predictions": ["get the rows headers for a request object returncode ."], "references": ["gets the value of a given field from the input record"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 175, "code": "def create Input ( self ) : print \"-\" * 70 + \"Creating a random input vector\" + \"-\" * 70 #clear the input Array to zero before creating a new input vector self . input Array [ 0 : ] = 0 for i in range ( self . input Size ) : #randrange returns 0 or 1 self . input Array [ i ] = random . randrange ( 2 )", "predictions": ["get a for the x y - axis"], "references": ["create a random input vector"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 176, "code": "def run ( self ) : print \"-\" * 80 + \"Computing the SDR\" + \"-\" * 80 #active Array[column]=1 if column is active after spatial pooling self . sp . compute ( self . input Array , True , self . active Array ) print self . active Array . nonzero ( )", "predictions": ["generate a bunch of n random in the in - progress bar . ."], "references": ["run the spatial pooler with the input vector"], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 177, "code": "def clear ( self ) : self . Memory = None self . num Patterns = 0 self . M = None self . category List = [ ] self . partition Id List = [ ] self . partition Id Map = { } self . finished Learning = False self . iteration Idx = - 1 if self . max Stored Patterns > 0 : assert self . use Sparse Memory , ( \"Fixed capacity KNN is implemented only \" \"in the sparse memory mode\" ) self . fixed Capacity = True self . category Recency List = [ ] else : self . fixed Capacity = False self . proto Sizes = None self . s = None self . vt = None self . nc = None self . mean = None self . specific Index Training = False self . next Training Indices = None", "predictions": ["clears the partition and mean . ."], "references": ["clears the state of the knnclassifier ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 178, "code": "def add Partition Id ( self , index , partition Id = None ) : if partition Id is None : self . partition Id List . append ( numpy . inf ) else : self . partition Id List . append ( partition Id ) indices = self . partition Id Map . get ( partition Id , [ ] ) indices . append ( index ) self . partition Id Map [ partition Id ] = indices", "predictions": ["generate a . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["adds partition id for pattern index"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 179, "code": "def rebuild Partition Id Map ( self , partition Id List ) : self . partition Id Map = { } for row , partition Id in enumerate ( partition Id List ) : indices = self . partition Id Map . get ( partition Id , [ ] ) indices . append ( row ) self . partition Id Map [ partition Id ] = indices", "predictions": ["convert output from output to output"], "references": ["rebuilds the partition id map using the given partitionidlist"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 180, "code": "def rewind ( self ) : super ( File Record Stream , self ) . rewind ( ) self . close ( ) self . file = open ( self . filename , self . mode ) self . reader = csv . reader ( self . file , dialect = \"excel\" ) self . reader . next ( ) self . reader . next ( ) self . reader . next ( ) self . record Count = 0", "predictions": ["closes the data file self self self self self self self self self self self self self self self self self self self self self self self self self self self"], "references": ["put us back at the beginning of the file again ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 181, "code": "def get Start Row ( self , bookmark ) : book Mark Dict = json . loads ( bookmark ) realpath = os . path . realpath ( self . filename ) book Mark File = book Mark Dict . get ( 'filepath' , None ) if book Mark File != realpath : print ( \"Ignoring bookmark due to mismatch between File's \" \"filename realpath vs. bookmark; realpath: %r; bookmark: %r\" ) % ( realpath , book Mark Dict ) return 0 else : return book Mark Dict [ 'current Row' ]", "predictions": ["returns the inference as a dictionary"], "references": ["extracts start row from the bookmark information"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 182, "code": "def get Custom Dict ( cls ) : if not os . path . exists ( cls . get Path ( ) ) : return dict ( ) properties = Configuration . read Config File ( os . path . basename ( cls . get Path ( ) ) , os . path . dirname ( cls . get Path ( ) ) ) values = dict ( ) for prop Name in properties : if 'value' in properties [ prop Name ] : values [ prop Name ] = properties [ prop Name ] [ 'value' ] return values", "predictions": ["return a dictionary of the values s in the configuration file . . . ."], "references": ["returns a dict of all temporary values in custom configuration file"], "bleu": 0.13380161378318955, "rouge_l": 0.4747081712062256}
{"id": 183, "code": "def set Path ( cls ) : cls . path = os . path . join ( os . environ [ 'NTA DYNAMIC CONF DIR' ] , cls . custom File Name )", "predictions": ["sets the current executable settings return its runtime ."], "references": ["sets the path of the custom configuration file"], "bleu": 0.18575057999133596, "rouge_l": 0.2378167641325536}
{"id": 184, "code": "def advance Phase ( self ) : self . current Phase = self . phase Cycler . next ( ) self . current Phase . enter Phase ( ) return", "predictions": ["set the next frame without returning the previous one"], "references": ["advance to the next iteration cycle phase"], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 185, "code": "def aggr mean ( in List ) : aggr Sum = 0 non None = 0 for elem in in List : if elem != SENTINEL VALUE FOR MISSING DATA : aggr Sum += elem non None += 1 if non None != 0 : return aggr Sum / non None else : return None", "predictions": ["compute the mean of a list ."], "references": ["returns mean of non - none elements of the list"], "bleu": 0.18094495256969623, "rouge_l": 0.34205607476635513}
{"id": 186, "code": "def aggr mode ( in List ) : value Counts = dict ( ) non None = 0 for elem in in List : if elem == SENTINEL VALUE FOR MISSING DATA : continue non None += 1 if elem in value Counts : value Counts [ elem ] += 1 else : value Counts [ elem ] = 1 if non None == 0 : return None sorted Counts = value Counts . items ( ) sorted Counts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sorted Counts [ 0 ] [ 0 ]", "predictions": ["sort the aggr mode mode"], "references": ["returns most common value seen in the non - none elements of the list"], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 187, "code": "def write To Checkpoint ( self , checkpoint Dir ) : proto = self . get Schema ( ) . new message ( ) self . write ( proto ) checkpoint Path = self . get Model Checkpoint File Path ( checkpoint Dir ) if os . path . exists ( checkpoint Dir ) : if not os . path . isdir ( checkpoint Dir ) : raise Exception ( ( \"Existing filesystem entry <%s> is not a model\" \" checkpoint -- refusing to delete (not a directory)\" ) % checkpoint Dir ) if not os . path . isfile ( checkpoint Path ) : raise Exception ( ( \"Existing filesystem entry <%s> is not a model\" \" checkpoint -- refusing to delete\" \" (%s missing or not a file)\" ) % ( checkpoint Dir , checkpoint Path ) ) shutil . rmtree ( checkpoint Dir ) self . make Directory From Absolute Path ( checkpoint Dir ) with open ( checkpoint Path , 'wb' ) as f : proto . write ( f )", "predictions": ["writes a checkpoint file as not not created as not not not not not created ."], "references": ["serializes model using capnproto and writes data to checkpointdir"], "bleu": 0.07692375026049747, "rouge_l": 0.08425414364640883}
{"id": 188, "code": "def read From Checkpoint ( cls , checkpoint Dir ) : checkpoint Path = cls . get Model Checkpoint File Path ( checkpoint Dir ) with open ( checkpoint Path , 'r' ) as f : proto = cls . get Schema ( ) . read ( f , traversal limit in words = TRAVERSAL LIMIT IN WORDS ) model = cls . read ( proto ) return model", "predictions": ["builds a + + + + other + other + other + other - + + 4 - + + + + + + + + + + + + +"], "references": ["deserializes model from checkpointdir using capnproto"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 189, "code": "def report Command Line Usage Error And Exit ( parser , message ) : print parser . get usage ( ) print message sys . exit ( 1 )", "predictions": ["reset the . exit and exit to stdout ."], "references": ["report usage error and exit program with error indication ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 190, "code": "def is Checkpoint Dir ( checkpoint Dir ) : last Segment = os . path . split ( checkpoint Dir ) [ 1 ] if last Segment [ 0 ] == '.' : return False if not checkpoint Dir . endswith ( g default Checkpoint Extension ) : return False if not os . path . isdir ( checkpoint Dir ) : return False return True", "predictions": ["return can be a checkpoint or a checkpoint"], "references": ["return true iff checkpointdir appears to be a checkpoint directory ."], "bleu": 0.21690743377623947, "rouge_l": 0.4093959731543625}
{"id": 191, "code": "def print Available Checkpoints ( experiment Dir ) : checkpoint Parent Dir = get Checkpoint Parent Dir ( experiment Dir ) if not os . path . exists ( checkpoint Parent Dir ) : print \"No available checkpoints.\" return checkpoint Dirs = [ x for x in os . listdir ( checkpoint Parent Dir ) if is Checkpoint Dir ( os . path . join ( checkpoint Parent Dir , x ) ) ] if not checkpoint Dirs : print \"No available checkpoints.\" return print \"Available checkpoints:\" checkpoint List = [ checkpoint Label From Checkpoint Dir ( x ) for x in checkpoint Dirs ] for checkpoint in sorted ( checkpoint List ) : print \"\\t\" , checkpoint print print \"To start from a checkpoint:\" print \"  python run opf experiment.py experiment --load <CHECKPOINT>\" print \"For example, to start from the checkpoint \\\"My Checkpoint\\\":\" print \"  python run opf experiment.py experiment --load My Checkpoint\"", "predictions": ["get the user s opf as opf for checkpoints ."], "references": ["list available checkpoints for the specified experiment ."], "bleu": 0.15851165692617156, "rouge_l": 0.22676579925650556}
{"id": 192, "code": "def run ( self ) : self . logger . debug ( \"run(): Starting task <%s>\" , self . task [ 'task Label' ] ) if self . cmd Options . private Options [ 'test Mode' ] : num Iters = 10 else : num Iters = self . task [ 'iteration Count' ] if num Iters >= 0 : iter Tracker = iter ( xrange ( num Iters ) ) else : iter Tracker = iter ( itertools . count ( ) ) periodic = Periodic Activity Mgr ( requested Activities = self . create Periodic Activities ( ) ) self . model . reset Sequence States ( ) self . task Driver . setup ( ) while True : try : next ( iter Tracker ) except Stop Iteration : break try : input Record = self . dataset Reader . next ( ) except Stop Iteration : break result = self . task Driver . handle Input Record ( input Record = input Record ) if Inference Element . encodings in result . inferences : result . inferences . pop ( Inference Element . encodings ) self . prediction Logger . write Record ( result ) periodic . tick ( ) self . get And Emit Experiment Metrics ( final = True ) self . task Driver . finalize ( ) self . model . reset Sequence States ( )", "predictions": ["run the task ."], "references": ["runs a single experiment task"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 193, "code": "def encode Into Array ( self , input Val , output Val ) : if len ( input Val ) != len ( output Val ) : raise Value Error ( \"Different input (%i) and output (%i) sizes.\" % ( len ( input Val ) , len ( output Val ) ) ) if self . w is not None and sum ( input Val ) != self . w : raise Value Error ( \"Input has %i bits but w was set to %i.\" % ( sum ( input Val ) , self . w ) ) output Val [ : ] = input Val [ : ] if self . verbosity >= 2 : print \"input:\" , input Val , \"output:\" , output Val print \"decoded:\" , self . decoded To Str ( self . decode ( output Val ) )", "predictions": ["encode the packet input"], "references": ["see method description in base . py"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 194, "code": "def decode ( self , encoded , parent Field Name = \"\" ) : if parent Field Name != \"\" : field Name = \"%s.%s\" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( [ [ 0 , 0 ] ] , \"input\" ) } , [ field Name ] )", "predictions": ["return the name of the encoded string"], "references": ["see the function description in base . py"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 195, "code": "def get Bucket Info ( self , buckets ) : return [ Encoder Result ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) ) ]", "predictions": ["return a list of bucket ."], "references": ["see the function description in base . py"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 196, "code": "def top Down Compute ( self , encoded ) : return Encoder Result ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) )", "predictions": ["calculate the top - left top - left corner"], "references": ["see the function description in base . py"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 197, "code": "def gen Logging File Path ( ) : app Name = os . path . splitext ( os . path . basename ( sys . argv [ 0 ] ) ) [ 0 ] or 'Unknown App' app Log Dir = os . path . abspath ( os . path . join ( os . environ [ 'NTA LOG DIR' ] , 'numenta-logs-%s' % ( os . environ [ 'USER' ] , ) , app Name ) ) app Log File Name = '%s-%s-%s.log' % ( app Name , long ( time . mktime ( time . gmtime ( ) ) ) , os . getpid ( ) ) return os . path . join ( app Log Dir , app Log File Name )", "predictions": ["generate the path to the application ."], "references": ["generate a filepath for the calling app"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 198, "code": "def get Scaled Value ( self , inpt ) : if inpt == SENTINEL VALUE FOR MISSING DATA : return None else : val = inpt if val < self . minval : val = self . minval elif val > self . maxval : val = self . maxval scaled Val = math . log10 ( val ) return scaled Val", "predictions": ["return a scaled value for the given parameter ."], "references": ["convert the input which is in normal space into log space"], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 199, "code": "def get Bucket Indices ( self , inpt ) : scaled Val = self . get Scaled Value ( inpt ) if scaled Val is None : return [ None ] else : return self . encoder . get Bucket Indices ( scaled Val )", "predictions": ["return a indices instance for the given inpt"], "references": ["see the function description in base . py"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 200, "code": "def encode Into Array ( self , inpt , output ) : scaled Val = self . get Scaled Value ( inpt ) if scaled Val is None : output [ 0 : ] = 0 else : self . encoder . encode Into Array ( scaled Val , output ) if self . verbosity >= 2 : print \"input:\" , inpt , \"scaled Val:\" , scaled Val , \"output:\" , output print \"decoded:\" , self . decoded To Str ( self . decode ( output ) )", "predictions": ["encode the rtp packet"], "references": ["see the function description in base . py"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 201, "code": "def decode ( self , encoded , parent Field Name = '' ) : ( fields Dict , field Names ) = self . encoder . decode ( encoded ) if len ( fields Dict ) == 0 : return ( fields Dict , field Names ) assert ( len ( fields Dict ) == 1 ) ( in Ranges , in Desc ) = fields Dict . values ( ) [ 0 ] out Ranges = [ ] for ( min V , max V ) in in Ranges : out Ranges . append ( ( math . pow ( 10 , min V ) , math . pow ( 10 , max V ) ) ) desc = \"\" num Ranges = len ( out Ranges ) for i in xrange ( num Ranges ) : if out Ranges [ i ] [ 0 ] != out Ranges [ i ] [ 1 ] : desc += \"%.2f-%.2f\" % ( out Ranges [ i ] [ 0 ] , out Ranges [ i ] [ 1 ] ) else : desc += \"%.2f\" % ( out Ranges [ i ] [ 0 ] ) if i < num Ranges - 1 : desc += \", \" if parent Field Name != '' : field Name = \"%s.%s\" % ( parent Field Name , self . name ) else : field Name = self . name return ( { field Name : ( out Ranges , desc ) } , [ field Name ] )", "predictions": ["decode a single encoded string to a python dictionary ."], "references": ["see the function description in base . py"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 202, "code": "def get Bucket Values ( self ) : if self . bucket Values is None : scaled Values = self . encoder . get Bucket Values ( ) self . bucket Values = [ ] for scaled Value in scaled Values : value = math . pow ( 10 , scaled Value ) self . bucket Values . append ( value ) return self . bucket Values", "predictions": ["reads a list of bucket tuples"], "references": ["see the function description in base . py"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 203, "code": "def get Bucket Info ( self , buckets ) : scaled Result = self . encoder . get Bucket Info ( buckets ) [ 0 ] scaled Value = scaled Result . value value = math . pow ( 10 , scaled Value ) return [ Encoder Result ( value = value , scalar = value , encoding = scaled Result . encoding ) ]", "predictions": ["return a list of buckets for each row in the encoder ."], "references": ["see the function description in base . py"], "bleu": 0.1235622127262679, "rouge_l": 0.2074829931972789}
{"id": 204, "code": "def top Down Compute ( self , encoded ) : scaled Result = self . encoder . top Down Compute ( encoded ) [ 0 ] scaled Value = scaled Result . value value = math . pow ( 10 , scaled Value ) return Encoder Result ( value = value , scalar = value , encoding = scaled Result . encoding )", "predictions": ["calculate the top - most most value ."], "references": ["see the function description in base . py"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 205, "code": "def closeness Scores ( self , exp Values , act Values , fractional = True ) : if exp Values [ 0 ] > 0 : exp Value = math . log10 ( exp Values [ 0 ] ) else : exp Value = self . min Scaled Value if act Values [ 0 ] > 0 : act Value = math . log10 ( act Values [ 0 ] ) else : act Value = self . min Scaled Value if fractional : err = abs ( exp Value - act Value ) pct Err = err / ( self . max Scaled Value - self . min Scaled Value ) pct Err = min ( 1.0 , pct Err ) closeness = 1.0 - pct Err else : err = abs ( exp Value - act Value ) closeness = err #print \"log::\", \"exp Value:\", exp Values[0], \"act Value:\", act Values[0], \\ #import pdb; pdb.set trace() return numpy . array ( [ closeness ] )", "predictions": ["helper function to calculate the closeness values"], "references": ["see the function description in base . py"], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 206, "code": "def bits To String ( arr ) : s = array ( 'c' , '.' * len ( arr ) ) for i in xrange ( len ( arr ) ) : if arr [ i ] == 1 : s [ i ] = '*' return s", "predictions": ["convert an array to a list of numbers ."], "references": ["returns a string representing a numpy array of 0 s and 1 s"], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 207, "code": "def run CPU ( ) : model = Model Factory . create ( model params . MODEL PARAMS ) model . enable Inference ( { 'predicted Field' : 'cpu' } ) shifter = Inference Shifter ( ) act History = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) pred History = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) actline , = plt . plot ( range ( WINDOW ) , act History ) predline , = plt . plot ( range ( WINDOW ) , pred History ) actline . axes . set ylim ( 0 , 100 ) predline . axes . set ylim ( 0 , 100 ) while True : s = time . time ( ) cpu = psutil . cpu percent ( ) model Input = { 'cpu' : cpu } result = shifter . shift ( model . run ( model Input ) ) inference = result . inferences [ 'multi Step Best Predictions' ] [ 5 ] if inference is not None : act History . append ( result . raw Input [ 'cpu' ] ) pred History . append ( inference ) actline . set ydata ( act History ) predline . set ydata ( pred History ) plt . draw ( ) plt . legend ( ( 'actual' , 'predicted' ) ) try : plt . pause ( SECONDS PER STEP ) except : pass", "predictions": ["run the cpu on the provided criteria"], "references": ["poll cpu usage make predictions and plot the results . runs forever ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 208, "code": "def extract Calling Method Args ( ) : import inspect import copy calling Frame = inspect . stack ( ) [ 1 ] [ 0 ] arg Names , , , frame Local Var Dict = inspect . getargvalues ( calling Frame ) arg Names . remove ( \"self\" ) args = copy . copy ( frame Local Var Dict ) for var Name in frame Local Var Dict : if var Name not in arg Names : args . pop ( var Name ) return args", "predictions": ["extracts a list of calling from a args query"], "references": ["returns args dictionary from the calling method"], "bleu": 0.16784459625186196, "rouge_l": 0.1278825995807128}
{"id": 209, "code": "def get Ephemeral Members ( self ) : e = Backtracking TM . get Ephemeral Members ( self ) if self . make Cells4Ephemeral : e . extend ( [ 'cells4' ] ) return e", "predictions": ["returns a list of the ephemeral"], "references": ["list of our member variables that we don t need to be saved"], "bleu": 0.09052970298747198, "rouge_l": 0.19741100323624597}
{"id": 210, "code": "def init Ephemerals ( self ) : Backtracking TM . init Ephemerals ( self ) #--------------------------------------------------------------------------------- self . allocate States In CPP = False self . retrieve Learning States = False if self . make Cells4Ephemeral : self . init Cells4 ( )", "predictions": ["init ephemerals and ephemerals objects ."], "references": ["initialize all ephemeral members after being restored to a pickled state ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 211, "code": "def copy Allocated States ( self ) : if self . verbosity > 1 or self . retrieve Learning States : ( active T , active T1 , pred T , pred T1 ) = self . cells4 . get Learn States ( ) self . lrn Active State [ 't-1' ] = active T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . lrn Active State [ 't' ] = active T . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . lrn Predicted State [ 't-1' ] = pred T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . lrn Predicted State [ 't' ] = pred T . reshape ( ( self . number Of Cols , self . cells Per Column ) ) if self . allocate States In CPP : assert False ( active T , active T1 , pred T , pred T1 , col Confidence T , col Confidence T1 , confidence T , confidence T1 ) = self . cells4 . get States ( ) self . cell Confidence [ 't' ] = confidence T . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . cell Confidence [ 't-1' ] = confidence T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . col Confidence [ 't' ] = col Confidence T . reshape ( self . number Of Cols ) self . col Confidence [ 't-1' ] = col Confidence T1 . reshape ( self . number Of Cols ) self . inf Active State [ 't-1' ] = active T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . inf Active State [ 't' ] = active T . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . inf Predicted State [ 't-1' ] = pred T1 . reshape ( ( self . number Of Cols , self . cells Per Column ) ) self . inf Predicted State [ 't' ] = pred T . reshape ( ( self . number Of Cols , self . cells Per Column ) )", "predictions": ["copies the lemmatizer that this cell has been performed ."], "references": ["if state is allocated in cpp copy over the data into our numpy arrays ."], "bleu": 0.08461586088475063, "rouge_l": 0.15443037974683543}
{"id": 212, "code": "def get Bucket Indices ( self , x ) : if ( ( isinstance ( x , float ) and math . isnan ( x ) ) or x == SENTINEL VALUE FOR MISSING DATA ) : return [ None ] if self . offset is None : self . offset = x bucket Idx = ( ( self . max Buckets / 2 ) + int ( round ( ( x - self . offset ) / self . resolution ) ) ) if bucket Idx < 0 : bucket Idx = 0 elif bucket Idx >= self . max Buckets : bucket Idx = self . max Buckets - 1 return [ bucket Idx ]", "predictions": ["return the bucket of the bucket within the given x ."], "references": ["see method description in base . py"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 213, "code": "def encode Into Array ( self , x , output ) : if x is not None and not isinstance ( x , numbers . Number ) : raise Type Error ( \"Expected a scalar input but got input of type %s\" % type ( x ) ) bucket Idx = self . get Bucket Indices ( x ) [ 0 ] output [ 0 : self . n ] = 0 if bucket Idx is not None : output [ self . map Bucket Index To Non Zero Bits ( bucket Idx ) ] = 1", "predictions": ["encodes the bucket into the output output ."], "references": ["see method description in base . py"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 214, "code": "def count Overlap Indices ( self , i , j ) : if self . bucket Map . has key ( i ) and self . bucket Map . has key ( j ) : i Rep = self . bucket Map [ i ] j Rep = self . bucket Map [ j ] return self . count Overlap ( i Rep , j Rep ) else : raise Value Error ( \"Either i or j don't exist\" )", "predictions": ["get the number of non - certain buckets in a bucket"], "references": ["return the overlap between bucket indices i and j"], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 215, "code": "def initialize Bucket Map ( self , max Buckets , offset ) : self . max Buckets = max Buckets self . min Index = self . max Buckets / 2 self . max Index = self . max Buckets / 2 self . offset = offset self . bucket Map = { } def permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucket Map [ self . min Index ] = permutation ( self . n ) [ 0 : self . w ] self . num Tries = 0", "predictions": ["create a new point with a certain range of positions ."], "references": ["initialize the bucket map assuming the given number of maxbuckets ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 216, "code": "def get Bucket Indices ( self , input ) : if type ( input ) is float and math . isnan ( input ) : input = SENTINEL VALUE FOR MISSING DATA if input == SENTINEL VALUE FOR MISSING DATA : return [ None ] minbin = self . get First On Bit ( input ) [ 0 ] if self . periodic : bucket Idx = minbin + self . halfwidth if bucket Idx < 0 : bucket Idx += self . n else : bucket Idx = minbin return [ bucket Idx ]", "predictions": ["get the full indices bucket for a given input input input input"], "references": ["see method description in base . py"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 217, "code": "def encode Into Array ( self , input , output , learn = True ) : if input is not None and not isinstance ( input , numbers . Number ) : raise Type Error ( \"Expected a scalar input but got input of type %s\" % type ( input ) ) if type ( input ) is float and math . isnan ( input ) : input = SENTINEL VALUE FOR MISSING DATA bucket Idx = self . get First On Bit ( input ) [ 0 ] if bucket Idx is None : output [ 0 : self . n ] = 0 #TODO: should all 1s, or random SDR be returned instead? else : output [ : self . n ] = 0 minbin = bucket Idx maxbin = minbin + 2 * self . halfwidth if self . periodic : if maxbin >= self . n : bottombins = maxbin - self . n + 1 output [ : bottombins ] = 1 maxbin = self . n - 1 if minbin < 0 : topbins = - minbin output [ self . n - topbins : self . n ] = 1 minbin = 0 assert minbin >= 0 assert maxbin < self . n output [ minbin : maxbin + 1 ] = 1 if self . verbosity >= 2 : print print \"input:\" , input print \"range:\" , self . minval , \"-\" , self . maxval print \"n:\" , self . n , \"w:\" , self . w , \"resolution:\" , self . resolution , \"radius\" , self . radius , \"periodic:\" , self . periodic print \"output:\" , self . pprint ( output ) print \"input desc:\" , self . decoded To Str ( self . decode ( output ) )", "predictions": ["encode the input input and output the result ."], "references": ["see method description in base . py"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 218, "code": "def generate Range Description ( self , ranges ) : desc = \"\" num Ranges = len ( ranges ) for i in xrange ( num Ranges ) : if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : desc += \"%.2f-%.2f\" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) else : desc += \"%.2f\" % ( ranges [ i ] [ 0 ] ) if i < num Ranges - 1 : desc += \", \" return desc", "predictions": ["fill in a list of ranges based on the ranges ."], "references": ["generate description from a text description of the ranges"], "bleu": 0.17033186037639278, "rouge_l": 0.4073455759599332}
{"id": 219, "code": "def get Bucket Values ( self ) : if self . bucket Values is None : top Down Mapping M = self . get Top Down Mapping ( ) num Buckets = top Down Mapping M . n Rows ( ) self . bucket Values = [ ] for bucket Idx in range ( num Buckets ) : self . bucket Values . append ( self . get Bucket Info ( [ bucket Idx ] ) [ 0 ] . value ) return self . bucket Values", "predictions": ["get the bucket bucket for each bucket ."], "references": ["see the function description in base . py"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 220, "code": "def get Bucket Info ( self , buckets ) : #NOTE: although variable top Down Mapping M is unused, some (bad-style) actions #are executed during  get Top Down Mapping() so this line must stay here top Down Mapping M = self . get Top Down Mapping ( ) category = buckets [ 0 ] encoding = self . top Down Mapping M . get Row ( category ) if self . periodic : input Val = ( self . minval + ( self . resolution / 2.0 ) + ( category * self . resolution ) ) else : input Val = self . minval + ( category * self . resolution ) return [ Encoder Result ( value = input Val , scalar = input Val , encoding = encoding ) ]", "predictions": ["return the bucket actions for the given buckets ."], "references": ["see the function description in base . py"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 221, "code": "def top Down Compute ( self , encoded ) : top Down Mapping M = self . get Top Down Mapping ( ) category = top Down Mapping M . right Vec Prod ( encoded ) . argmax ( ) return self . get Bucket Info ( [ category ] )", "predictions": ["return the top - level top of an encoded category ."], "references": ["see the function description in base . py"], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 222, "code": "def closeness Scores ( self , exp Values , act Values , fractional = True ) : exp Value = exp Values [ 0 ] act Value = act Values [ 0 ] if self . periodic : exp Value = exp Value % self . maxval act Value = act Value % self . maxval err = abs ( exp Value - act Value ) if self . periodic : err = min ( err , self . maxval - err ) if fractional : pct Err = float ( err ) / ( self . maxval - self . minval ) pct Err = min ( 1.0 , pct Err ) closeness = 1.0 - pct Err else : closeness = err return numpy . array ( [ closeness ] )", "predictions": ["internal helper function to turn input values into a numpy array"], "references": ["see the function description in base . py"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 223, "code": "def init Ephemerals ( self ) : self . segment Updates = { } self . reset Stats ( ) # self . prev Inf Patterns = [ ] self . prev Lrn Patterns = [ ] state Shape = ( self . number Of Cols , self . cells Per Column ) self . lrn Active State = { } self . lrn Active State [ \"t\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . lrn Active State [ \"t-1\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . lrn Predicted State = { } self . lrn Predicted State [ \"t\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . lrn Predicted State [ \"t-1\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . inf Active State = { } self . inf Active State [ \"t\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . inf Active State [ \"t-1\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . inf Active State [ \"backup\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . inf Active State [ \"candidate\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . inf Predicted State = { } self . inf Predicted State [ \"t\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . inf Predicted State [ \"t-1\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . inf Predicted State [ \"backup\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . inf Predicted State [ \"candidate\" ] = numpy . zeros ( state Shape , dtype = \"int8\" ) self . cell Confidence = { } self . cell Confidence [ \"t\" ] = numpy . zeros ( state Shape , dtype = \"float32\" ) self . cell Confidence [ \"t-1\" ] = numpy . zeros ( state Shape , dtype = \"float32\" ) self . cell Confidence [ \"candidate\" ] = numpy . zeros ( state Shape , dtype = \"float32\" ) self . col Confidence = { } self . col Confidence [ \"t\" ] = numpy . zeros ( self . number Of Cols , dtype = \"float32\" ) self . col Confidence [ \"t-1\" ] = numpy . zeros ( self . number Of Cols , dtype = \"float32\" ) self . col Confidence [ \"candidate\" ] = numpy . zeros ( self . number Of Cols , dtype = \"float32\" )", "predictions": ["setup ephemerals and state of cell objects ."], "references": ["initialize all ephemeral members after being restored to a pickled state ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 224, "code": "def print Parameters ( self ) : print \"number Of Cols=\" , self . number Of Cols print \"cells Per Column=\" , self . cells Per Column print \"min Threshold=\" , self . min Threshold print \"new Synapse Count=\" , self . new Synapse Count print \"activation Threshold=\" , self . activation Threshold print print \"initial Perm=\" , self . initial Perm print \"connected Perm=\" , self . connected Perm print \"permanence Inc=\" , self . permanence Inc print \"permanence Dec=\" , self . permanence Dec print \"permanence Max=\" , self . permanence Max print \"global Decay=\" , self . global Decay print print \"do Pooling=\" , self . do Pooling print \"seg Update Valid Duration=\" , self . seg Update Valid Duration print \"pam Length=\" , self . pam Length", "predictions": ["run the method for printing the data structure task task ."], "references": ["print the parameter settings for the tm ."], "bleu": 0.14323145079400493, "rouge_l": 0.433392539964476}
{"id": 225, "code": "def update Avg Learned Seq Length ( self , prev Seq Length ) : if self . lrn Iteration Idx < 100 : alpha = 0.5 else : alpha = 0.1 self . avg Learned Seq Length = ( ( 1.0 - alpha ) * self . avg Learned Seq Length + ( alpha * prev Seq Length ) )", "predictions": ["encode the match_temp_var matrix with the current plot len len len len len len len len len len len len len len len len len len len len len len len"], "references": ["update our moving average of learned sequence length ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 226, "code": "def quote ( c ) : i = ord ( c ) return ESCAPE + HEX [ i // 16 ] + HEX [ i % 16 ]", "predictions": ["decode a string ."], "references": ["quote a single character ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 227, "code": "def unhex ( s ) : bits = 0 for c in s : if '0' <= c <= '9' : i = ord ( '0' ) elif 'a' <= c <= 'f' : i = ord ( 'a' ) - 10 elif 'A' <= c <= 'F' : i = ord ( 'A' ) - 10 else : break bits = bits * 16 + ( ord ( c ) - i ) return bits", "predictions": ["convert string to integer"], "references": ["get the integer value of a hexadecimal number ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 228, "code": "def encode ( input , output ) : while True : s = input . read ( MAXBINSIZE ) if not s : break while len ( s ) < MAXBINSIZE : ns = input . read ( MAXBINSIZE - len ( s ) ) if not ns : break s += ns line = binascii . b2a base64 ( s ) output . write ( line )", "predictions": ["top level input top - level base64"], "references": ["encode a file ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 229, "code": "def decode ( input , output ) : while True : line = input . readline ( ) if not line : break s = binascii . a2b base64 ( line ) output . write ( s )", "predictions": ["gen base64 and output are modified ."], "references": ["decode a file ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 230, "code": "def encodestring ( s ) : pieces = [ ] for i in range ( 0 , len ( s ) , MAXBINSIZE ) : chunk = s [ i : i + MAXBINSIZE ] pieces . append ( binascii . b2a base64 ( chunk ) ) return \"\" . join ( pieces )", "predictions": ["replace bytes with base64 in base64 val"], "references": ["encode a string into multiple lines of base - 64 data ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 231, "code": "def begin ( self ) : return Range ( self . source buffer , self . begin pos , self . begin pos , expanded from = self . expanded from )", "predictions": ["start a new source scaled by the source ."], "references": ["returns a zero - length range located just before the beginning of this range ."], "bleu": 0.08617428905281956, "rouge_l": 0.23921568627450981}
{"id": 232, "code": "def end ( self ) : return Range ( self . source buffer , self . end pos , self . end pos , expanded from = self . expanded from )", "predictions": ["return the source part of the source output output output output output output output output output output output output output output output output output output output output output output output buffer"], "references": ["returns a zero - length range located just after the end of this range ."], "bleu": 0.04317900023606586, "rouge_l": 0.09277566539923954}
{"id": 233, "code": "def column ( self ) : line , column = self . source buffer . decompose position ( self . begin pos ) return column", "predictions": ["return a decode at the current position = decode = decode = true"], "references": ["returns a zero - based column number of the beginning of this range ."], "bleu": 0.0978840017329239, "rouge_l": 0.1471652593486128}
{"id": 234, "code": "def line ( self ) : line , column = self . source buffer . decompose position ( self . begin pos ) return line", "predictions": ["return a get the get get the get get position ."], "references": ["returns the line number of the beginning of this range ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 235, "code": "def source lines ( self ) : return [ self . source buffer . source line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]", "predictions": ["a list of the get lines lines lines lines"], "references": ["returns the lines of source code containing the entirety of this range ."], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 236, "code": "def visit ( self , obj ) : if isinstance ( obj , list ) : return [ self . visit ( elt ) for elt in obj ] elif isinstance ( obj , ast . AST ) : return self . visit one ( obj )", "predictions": ["called if no attribute is a list of possible type ."], "references": ["visit a node or a list of nodes . other values are ignored"], "bleu": 0.1869018372152498, "rouge_l": 0.3283983849259758}
{"id": 237, "code": "def generic visit ( self , node ) : for field name in node . fields : setattr ( node , field name , self . visit ( getattr ( node , field name ) ) ) return node", "predictions": ["called if no explicit visitor function exists for a exp exp ."], "references": ["called if no explicit visitor function exists for a node ."], "bleu": 0.7539221180326288, "rouge_l": 0.8764367816091955}
{"id": 238, "code": "def visit ( self , obj ) : if isinstance ( obj , list ) : return list ( filter ( lambda x : x is not None , map ( self . visit , obj ) ) ) elif isinstance ( obj , ast . AST ) : return self . visit one ( obj ) else : return obj", "predictions": ["bits for special ast ast . ."], "references": ["visit a node or a list of nodes . other values are ignored"], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 239, "code": "def format final exc line ( etype , value ) : valuestr = some str ( value ) if value is None or not valuestr : line = \"%s\\n\" % etype else : line = \"%s: %s\\n\" % ( etype , valuestr ) return line", "predictions": ["run a final line"], "references": ["return a list of a single line -- normal case for format_exception_only"], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 240, "code": "def add extension ( module , name , code ) : code = int ( code ) if not 1 <= code <= 0x7fffffff : raise Value Error , \"code out of range\" key = ( module , name ) if ( extension registry . get ( key ) == code and inverted registry . get ( code ) == key ) : return if key in extension registry : raise Value Error ( \"key %s is already registered with code %s\" % ( key , extension registry [ key ] ) ) if code in inverted registry : raise Value Error ( \"code %s is already in use for key %s\" % ( code , inverted registry [ code ] ) ) extension registry [ key ] = code inverted registry [ code ] = key", "predictions": ["extract an extension extension"], "references": ["register an extension code ."], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 241, "code": "def remove extension ( module , name , code ) : key = ( module , name ) if ( extension registry . get ( key ) != code or inverted registry . get ( code ) != key ) : raise Value Error ( \"key %s is not registered with code %s\" % ( key , code ) ) del extension registry [ key ] del inverted registry [ code ] if code in extension cache : del extension cache [ code ]", "predictions": ["get an extension extension"], "references": ["unregister an extension code . for testing only ."], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 242, "code": "def cmp to key ( mycmp ) : class K ( object ) : slots = [ 'obj' ] def init ( self , obj , * args ) : self . obj = obj def lt ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def gt ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def eq ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 def le ( self , other ) : return mycmp ( self . obj , other . obj ) <= 0 def ge ( self , other ) : return mycmp ( self . obj , other . obj ) >= 0 def ne ( self , other ) : return mycmp ( self . obj , other . obj ) != 0 def hash ( self ) : raise Type Error ( 'hash not implemented' ) return K", "predictions": ["decorator to wrap an object into a key connection object"], "references": ["convert a cmp = function into a key = function"], "bleu": 0.23462350320528, "rouge_l": 0.3}
{"id": 243, "code": "def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '\"' ) and s . endswith ( '\"' ) : return s [ 1 : - 1 ] . replace ( '\\\\\\\\' , '\\\\' ) . replace ( '\\\\\"' , '\"' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s", "predictions": ["copy characters from a string to a list of strings verbosity"], "references": ["remove quotes from a string ."], "bleu": 0.21200626759025185, "rouge_l": 0.3727087576374745}
{"id": 244, "code": "def gotonext ( self ) : while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS + '\\n\\r' : self . pos = self . pos + 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) else : break", "predictions": ["get the next line"], "references": ["parse up to the start of the next address ."], "bleu": 0.10551173833795614, "rouge_l": 0.26521739130434785}
{"id": 245, "code": "def getaddrspec ( self ) : aslist = [ ] self . gotonext ( ) while self . pos < len ( self . field ) : if self . field [ self . pos ] == '.' : aslist . append ( '.' ) self . pos += 1 elif self . field [ self . pos ] == '\"' : aslist . append ( '\"%s\"' % self . getquote ( ) ) elif self . field [ self . pos ] in self . atomends : break else : aslist . append ( self . getatom ( ) ) self . gotonext ( ) if self . pos >= len ( self . field ) or self . field [ self . pos ] != '@' : return '' . join ( aslist ) aslist . append ( '@' ) self . pos += 1 self . gotonext ( ) return '' . join ( aslist ) + self . getdomain ( )", "predictions": ["parse the next field and replace with the current field ."], "references": ["parse an rfc 2822 addr - spec ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 246, "code": "def getdomain ( self ) : sdlist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] == '[' : sdlist . append ( self . getdomainliteral ( ) ) elif self . field [ self . pos ] == '.' : self . pos += 1 sdlist . append ( '.' ) elif self . field [ self . pos ] in self . atomends : break else : sdlist . append ( self . getatom ( ) ) return '' . join ( sdlist )", "predictions": ["get a human - readable representation of the sdlist"], "references": ["get the complete domain name from an address ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 247, "code": "def replace ( self , year = None , month = None , day = None ) : if year is None : year = self . year if month is None : month = self . month if day is None : day = self . day return date . new ( type ( self ) , year , month , day )", "predictions": ["return a resource with the given offset min and offset min min min min min min min min and offset min min min min min min offset min min min"], "references": ["return a new date with new values for the specified fields ."], "bleu": 0.06032401726201456, "rouge_l": 0.2064297800338409}
{"id": 248, "code": "def replace ( self , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return time . new ( type ( self ) , hour , minute , second , microsecond , tzinfo )", "predictions": ["= time or column depending on the values of the time . . ."], "references": ["return a new time with new values for the specified fields ."], "bleu": 0.11114924776032006, "rouge_l": 0.31202046035805625}
{"id": 249, "code": "def combine ( cls , date , time ) : if not isinstance ( date , date class ) : raise Type Error ( \"date argument must be a date instance\" ) if not isinstance ( time , time class ) : raise Type Error ( \"time argument must be a time instance\" ) return cls ( date . year , date . month , date . day , time . hour , time . minute , time . second , time . microsecond , time . tzinfo )", "predictions": ["encode a self . into a self . . ."], "references": ["construct a datetime from a given date and a given time ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 250, "code": "def time ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond )", "predictions": ["generate a generate datetime object"], "references": ["return the time part with tzinfo none ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 251, "code": "def timetz ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond , self . tzinfo )", "predictions": ["return of the datetime"], "references": ["return the time part with same tzinfo ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 252, "code": "def replace ( self , year = None , month = None , day = None , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if year is None : year = self . year if month is None : month = self . month if day is None : day = self . day if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return datetime . new ( type ( self ) , year , month , day , hour , minute , second , microsecond , tzinfo )", "predictions": ["actions for the specified date . ."], "references": ["return a new datetime with new values for the specified fields ."], "bleu": 0.17895451045590982, "rouge_l": 0.40197693574958815}
{"id": 253, "code": "def concat ( a , b ) : if not hasattr ( a , ' getitem ' ) : msg = \"'%s' object can't be concatenated\" % type ( a ) . name raise Type Error ( msg ) return a + b", "predictions": ["concatenate two strings using self . ."], "references": ["same as a + b for a and b sequences ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 254, "code": "def count Of ( a , b ) : count = 0 for i in a : if i == b : count += 1 return count", "predictions": ["closeness of with exp"], "references": ["return the number of times b occurs in a ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 255, "code": "def index Of ( a , b ) : for i , j in enumerate ( a ) : if j == b : return i else : raise Value Error ( 'sequence.index(x): x not in sequence' )", "predictions": ["prev two values ignoring none"], "references": ["return the first index of b in a ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 256, "code": "def iconcat ( a , b ) : if not hasattr ( a , ' getitem ' ) : msg = \"'%s' object can't be concatenated\" % type ( a ) . name raise Type Error ( msg ) a += b return a", "predictions": ["flips a function that produces a value of a and b"], "references": ["same as a + = b for a and b sequences ."], "bleu": 0.20468918037457082, "rouge_l": 0.34512022630834516}
{"id": 257, "code": "def encode basestring ( s ) : def replace ( match ) : return ESCAPE DCT [ match . group ( 0 ) ] return '\"' + ESCAPE . sub ( replace , s ) + '\"'", "predictions": ["encodes a string to unicode"], "references": ["return a json representation of a python string"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 258, "code": "def escape ( pattern ) : s = list ( pattern ) alphanum = alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == \"\\000\" : s [ i ] = \"\\\\000\" else : s [ i ] = \"\\\\\" + c return pattern [ : 0 ] . join ( s )", "predictions": ["escape a pattern ."], "references": ["escape all non - alphanumeric characters in pattern ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 259, "code": "def alloc temp ( self , type = '*\u03c0g.Object') : for v in sorted ( self . free temps , key = lambda k : k . name ) : if v . type == type : self . free temps . remove ( v ) self . used temps . add ( v ) return v self . temp index += 1 name = '\u03c0Temp{:03d}'. f ormat( s elf. t emp index) v = expr . Generated Temp Var ( self , name , type ) self . used temps . add ( v ) return v", "predictions": ["throws a distributed expression"], "references": ["create a new temporary go variable having type type_ for this block ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 260, "code": "def free temp ( self , v ) : self . used temps . remove ( v ) self . free temps . add ( v )", "predictions": ["free a free operation"], "references": ["release the generatedtempvar v so it can be reused ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 261, "code": "def format range unified ( start , stop ) : beginning = start + 1 length = stop - start if length == 1 : return '%s' % ( beginning ) if not length : beginning -= 1 return '%s,%s' % ( beginning , length )", "predictions": ["format a range of unified according to start and stop"], "references": ["convert range to the ed format"], "bleu": 0.14991106946711685, "rouge_l": 0.26180257510729615}
{"id": 262, "code": "def format range context ( start , stop ) : beginning = start + 1 length = stop - start if not length : beginning -= 1 if length <= 1 : return '%s' % ( beginning ) return '%s,%s' % ( beginning , beginning + length - 1 )", "predictions": ["format a range as a range as a string ."], "references": ["convert range to the ed format"], "bleu": 0.13950796967929133, "rouge_l": 0.13090128755364808}
{"id": 263, "code": "def make ( cls , iterable , new = tuple . new , len = len ) : result = new ( cls , iterable ) if len ( result ) != 3 : raise Type Error ( 'Expected 3 arguments, got %d' % len ( result ) ) return result", "predictions": ["make a new element from an iterable of iterable ."], "references": ["make a new match object from a sequence or iterable"], "bleu": 0.25965358893403384, "rouge_l": 0.5}
{"id": 264, "code": "def dump ( self , tag , x , lo , hi ) : for i in xrange ( lo , hi ) : yield '%s %s' % ( tag , x [ i ] )", "predictions": ["dump the json representation of a tag ."], "references": ["generate comparison results for a same - tagged range ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 265, "code": "def make prefix ( self ) : fromprefix = \"from%d \" % Html Diff . default prefix toprefix = \"to%d \" % Html Diff . default prefix Html Diff . default prefix += 1 self . prefix = [ fromprefix , toprefix ]", "predictions": ["add a prefix to the default prefix ."], "references": ["create unique anchor prefixes"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 266, "code": "def convert flags ( self , fromlist , tolist , flaglist , context , numlines ) : toprefix = self . prefix [ 1 ] next id = [ '' ] * len ( flaglist ) next href = [ '' ] * len ( flaglist ) num chg , in change = 0 , False last = 0 for i , flag in enumerate ( flaglist ) : if flag : if not in change : in change = True last = i i = max ( [ 0 , i - numlines ] ) next id [ i ] = ' id=\"difflib chg %s %d\"' % ( toprefix , num chg ) num chg += 1 next href [ last ] = '<a href=\"#difflib chg %s %d\">n</a>' % ( toprefix , num chg ) else : in change = False if not flaglist : flaglist = [ False ] next id = [ '' ] next href = [ '' ] last = 0 if context : fromlist = [ '<td></td><td>&nbsp;No Differences Found&nbsp;</td>' ] tolist = fromlist else : fromlist = tolist = [ '<td></td><td>&nbsp;Empty File&nbsp;</td>' ] if not flaglist [ 0 ] : next href [ 0 ] = '<a href=\"#difflib chg %s 0\">f</a>' % toprefix next href [ last ] = '<a href=\"#difflib chg %s top\">t</a>' % ( toprefix ) return fromlist , tolist , flaglist , next href , next id", "predictions": ["convert flags to dict"], "references": ["makes list of next links"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 267, "code": "def Make Parallel Benchmark ( p , work func , * args ) : def Benchmark ( b ) : e = threading . Event ( ) def Target ( ) : e . wait ( ) for in xrange ( b . N / p ) : work func ( * args ) threads = [ ] for in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . Reset Timer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark", "predictions": ["start a function to start a function on each work ."], "references": ["create and return a benchmark that runs work_func p times in parallel ."], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 268, "code": "def listdir ( path ) : try : cached mtime , list = cache [ path ] del cache [ path ] except Key Error : cached mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st mtime if mtime != cached mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list", "predictions": ["get a list of files from a path"], "references": ["list directory contents using cache ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 269, "code": "def pformat ( o , indent = 1 , width = 80 , depth = None ) : return Pretty Printer ( indent = indent , width = width , depth = depth ) . pformat ( o )", "predictions": ["pretty - print object o to a namedtuple ."], "references": ["format a python o into a pretty - printed representation ."], "bleu": 0.1768796183625973, "rouge_l": 0.2946859903381642}
{"id": 270, "code": "def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . accept ( kind ) return rule", "predictions": ["returns an tok rule for the specified kind of a given kind ."], "references": ["a rule that accepts a token of kind kind and returns it or returns none ."], "bleu": 0.107248039853522, "rouge_l": 0.2708102108768035}
{"id": 271, "code": "def Loc ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : result = parser . accept ( kind ) if result is unmatched : return result return result . loc return rule", "predictions": ["returns an error hash from a string ."], "references": ["a rule that accepts a token of kind kind and returns its location or returns none ."], "bleu": 0.06191391391332487, "rouge_l": 0.1502463054187192}
{"id": 272, "code": "def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule", "predictions": ["decorator to add a parser to a parser class ."], "references": ["a proxy for a rule called name which may not be yet defined ."], "bleu": 0.1004883949864497, "rouge_l": 0.24270557029177717}
{"id": 273, "code": "def Expect ( inner rule , loc = None ) : @ llrule ( loc , inner rule . expected ) def rule ( parser ) : result = inner rule ( parser ) if result is unmatched : expected = reduce ( list . add , [ rule . expected ( parser ) for rule in parser . errrules ] ) expected = list ( sorted ( set ( expected ) ) ) if len ( expected ) > 1 : expected = \" or \" . join ( [ \", \" . join ( expected [ 0 : - 1 ] ) , expected [ - 1 ] ] ) elif len ( expected ) == 1 : expected = expected [ 0 ] else : expected = \"(impossible)\" error tok = parser . tokens [ parser . errindex ] error = diagnostic . Diagnostic ( \"fatal\" , \"unexpected {actual}: expected {expected}\" , { \"actual\" : error tok . kind , \"expected\" : expected } , error tok . loc ) parser . diagnostic engine . process ( error ) return result return rule", "predictions": ["extract the expect from the inner rule ."], "references": ["a rule that executes inner_rule and emits a diagnostic error if it returns none ."], "bleu": 0.0739821920909478, "rouge_l": 0.16486486486486487}
{"id": 274, "code": "def Newline ( loc = None ) : @ llrule ( loc , lambda parser : [ \"newline\" ] ) def rule ( parser ) : result = parser . accept ( \"newline\" ) if result is unmatched : return result return [ ] return rule", "predictions": ["accept a rule from a string"], "references": ["a rule that accepts token of kind newline and returns an empty list ."], "bleu": 0.07663173913867023, "rouge_l": 0.18654434250764526}
{"id": 275, "code": "def replace ( self , * * kwds ) : result = self . make ( map ( kwds . pop , ( 'scheme' , 'netloc' , 'path' , 'query' , 'fragment' ) , self ) ) if kwds : raise Value Error ( 'Got unexpected field names: %r' % kwds . keys ( ) ) return result", "predictions": ["return a new url with new values ."], "references": ["return a new splitresult object replacing specified fields with new values"], "bleu": 0.29668217031953975, "rouge_l": 0.6140939597315436}
{"id": 276, "code": "def isfile ( path ) : try : st = os . stat ( path ) except os . error : return False return stat . S ISREG ( st . st mode )", "predictions": ["check if a file exists"], "references": ["test whether a path is a regular file"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 277, "code": "def isdir ( s ) : try : st = os . stat ( s ) except os . error : return False return stat . S ISDIR ( st . st mode )", "predictions": ["returns true if s is a file or string ."], "references": ["return true if the pathname refers to an existing directory ."], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 278, "code": "def commonprefix ( m ) : if not m : return '' s1 = min ( m ) s2 = max ( m ) for i , c in enumerate ( s1 ) : if c != s2 [ i ] : return s1 [ : i ] return s1", "predictions": ["returns the longest of a given number of m"], "references": ["given a list of pathnames returns the longest common leading component"], "bleu": 0.24193282889161974, "rouge_l": 0.2946859903381642}
{"id": 279, "code": "def bytelist2long Big Endian ( list ) : imax = len ( list ) // 4 hl = [ 0 ] * imax j = 0 i = 0 while i < imax : b0 = ord ( list [ j ] ) << 24 b1 = ord ( list [ j + 1 ] ) << 16 b2 = ord ( list [ j + 2 ] ) << 8 b3 = ord ( list [ j + 3 ] ) hl [ i ] = b0 | b1 | b2 | b3 i = i + 1 j = j + 4 return hl", "predictions": ["return a list of bytelist2long endian endian endian endian"], "references": ["transform a list of characters into a list of longs ."], "bleu": 0.2103465006557667, "rouge_l": 0.2946859903381642}
{"id": 280, "code": "def init ( self ) : self . length = 0 self . input = [ ] self . H0 = 0x67452301 self . H1 = 0x EFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0x C3D2E1F0", "predictions": ["initialize the class and h3 ."], "references": ["initialize the message - digest and set all fields to zero ."], "bleu": 0.12151662434083678, "rouge_l": 0.41924398625429554}
{"id": 281, "code": "def show warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IO Error , Unicode Error ) : pass", "predictions": ["show a warning message ."], "references": ["hook to write a warning to a file ; replace if you like ."], "bleu": 0.06382147015463427, "rouge_l": 0.2909379968203497}
{"id": 282, "code": "def formatwarning ( message , category , filename , lineno , line = None ) : try : unicodetype = unicode except Name Error : unicodetype = ( ) try : message = str ( message ) except Unicode Encode Error : pass s = \"%s: %s: %s\\n\" % ( lineno , category . name , message ) line = linecache . getline ( filename , lineno ) if line is None else line if line : line = line . strip ( ) if isinstance ( s , unicodetype ) and isinstance ( line , str ) : line = unicode ( line , 'latin1' ) s += \"  %s\\n\" % line if isinstance ( s , unicodetype ) and isinstance ( filename , str ) : enc = sys . getfilesystemencoding ( ) if enc : try : filename = unicode ( filename , enc ) except Unicode Decode Error : pass s = \"%s:%s\" % ( filename , s ) return s", "predictions": ["inverse of line . category ."], "references": ["function to format a warning the standard way ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 283, "code": "def warn ( message , category = None , stacklevel = 1 ) : if isinstance ( message , Warning ) : category = message . class if category is None : category = User Warning assert issubclass ( category , Warning ) try : caller = sys . getframe ( stacklevel ) except Value Error : globals = sys . dict lineno = 1 else : globals = caller . f globals lineno = caller . f lineno if ' name ' in globals : module = globals [ ' name ' ] else : module = \"<string>\" filename = globals . get ( ' file ' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( \".pyc\" , \".pyo\" ) ) : filename = filename [ : - 1 ] else : if module == \" main \" : try : filename = sys . argv [ 0 ] except Attribute Error : filename = ' main ' if not filename : filename = module registry = globals . setdefault ( \" warningregistry \" , { } ) warn explicit ( message , category , filename , lineno , module , registry , globals )", "predictions": ["prints a warning message with the given category ."], "references": ["issue a warning or maybe ignore it or raise an exception ."], "bleu": 0.1430210741102858, "rouge_l": 0.2785388127853881}
{"id": 284, "code": "def remove ( self , value ) : if value not in self : raise Key Error ( value ) self . discard ( value )", "predictions": ["remove an arbitrary value ."], "references": ["remove an element . if not a member raise a keyerror ."], "bleu": 0.09521044541645862, "rouge_l": 0.3285457809694794}
{"id": 285, "code": "def pop ( self ) : it = iter ( self ) try : value = next ( it ) except Stop Iteration : raise Key Error self . discard ( value ) return value", "predictions": ["pops a value from the list ."], "references": ["return the popped value . raise keyerror if empty ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 286, "code": "def go str ( value ) : io = String IO . String IO ( ) io . write ( '\"' ) for c in value : if c in ESCAPES : io . write ( ESCAPES [ c ] ) elif c in SIMPLE CHARS : io . write ( c ) else : io . write ( r'\\x{:02x}' . format ( ord ( c ) ) ) io . write ( '\"' ) return io . getvalue ( )", "predictions": ["go through a string and convert it to a string ."], "references": ["returns value as a valid go string literal ."], "bleu": 0.14323145079400493, "rouge_l": 0.3055091819699499}
{"id": 287, "code": "def dump registry ( cls , file = None ) : print >> file , \"Class: %s.%s\" % ( cls . module , cls . name ) print >> file , \"Inv.counter: %s\" % ABC Meta . abc invalidation counter for name in sorted ( cls . dict . keys ( ) ) : if name . startswith ( \" abc \" ) : value = getattr ( cls , name ) print >> file , \"%s: %r\" % ( name , value )", "predictions": ["dump the abc registry registry information"], "references": ["debug helper to print the abc registry ."], "bleu": 0.2945901093386716, "rouge_l": 0.4178082191780822}
{"id": 288, "code": "def format option strings ( self , option ) : if option . takes value ( ) : metavar = option . metavar or option . dest . upper ( ) short opts = [ self . short opt fmt % ( sopt , metavar ) for sopt in option . short opts ] long opts = [ self . long opt fmt % ( lopt , metavar ) for lopt in option . long opts ] else : short opts = option . short opts long opts = option . long opts if self . short first : opts = short opts + long opts else : opts = long opts + short opts return \", \" . join ( opts )", "predictions": ["get the formatted option options for the option"], "references": ["return a comma - separated list of option strings & metavariables ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 289, "code": "def reverse ( self ) : leftblock = self . left rightblock = self . right leftindex = self . leftndx rightindex = self . rightndx for i in range ( self . length // 2 ) : assert leftblock != rightblock or leftindex < rightindex ( rightblock [ rightindex ] , leftblock [ leftindex ] ) = ( leftblock [ leftindex ] , rightblock [ rightindex ] ) leftindex += 1 if leftindex == n : leftblock = leftblock [ RGTLNK ] assert leftblock is not None leftindex = 0 rightindex -= 1 if rightindex == - 1 : rightblock = rightblock [ LFTLNK ] assert rightblock is not None rightindex = n - 1", "predictions": ["re - normalise the last two - descendants match"], "references": ["reverse * in place *"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 290, "code": "def findall ( self , string , pos = 0 , endpos = sys . maxint ) : matchlist = [ ] state = State ( string , pos , endpos , self . flags ) while state . start <= state . end : state . reset ( ) state . string position = state . start if not state . search ( self . code ) : break match = SRE Match ( self , state ) if self . groups == 0 or self . groups == 1 : item = match . group ( self . groups ) else : item = match . groups ( \"\" ) matchlist . append ( item ) if state . string position == state . start : state . start += 1 else : state . start = state . string position return matchlist", "predictions": ["return a string i . e i can use this method to return a string"], "references": ["return a list of all non - overlapping matches of pattern in string ."], "bleu": 0.12300686288463772, "rouge_l": 0.27758816837315126}
{"id": 291, "code": "def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string position = state . start if not state . search ( self . code ) : break if state . start == state . string position : if last == state . end : break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) if self . groups : match = SRE Match ( self , state ) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string position splitlist . append ( string [ last : state . end ] ) return splitlist", "predictions": ["split a string key"], "references": ["split string by the occurrences of pattern ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 292, "code": "def finditer ( self , string , pos = 0 , endpos = sys . maxint ) : scanner = self . scanner ( string , pos , endpos ) return iter ( scanner . search , None )", "predictions": ["returns a map with a string temps"], "references": ["return a list of all non - overlapping matches of pattern in string ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 293, "code": "def create regs ( self , state ) : regs = [ ( state . start , state . string position ) ] for group in range ( self . re . groups ) : mark index = 2 * group if mark index + 1 < len ( state . marks ) and state . marks [ mark index ] is not None and state . marks [ mark index + 1 ] is not None : regs . append ( ( state . marks [ mark index ] , state . marks [ mark index + 1 ] ) ) else : regs . append ( ( - 1 , - 1 ) ) return tuple ( regs )", "predictions": ["format the state and return"], "references": ["creates a tuple of index pairs representing matched groups ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 294, "code": "def unexpo ( intpart , fraction , expo ) : if expo > 0 : f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction", "predictions": ["computes start and end start of a list of start and end start of start and start of start and start and start of start of start and start point ."], "references": ["remove the exponent by changing intpart and fraction ."], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 295, "code": "def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''", "predictions": ["if new number is a iterable with new iterable"], "references": ["round or extend the fraction to size digs ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 296, "code": "def filter ( names , pat ) : import os result = [ ] try : re pat = cache [ pat ] except Key Error : res = translate ( pat ) if len ( cache ) >= MAXCACHE : globals ( ) [ ' cache' ] = { } cache [ pat ] = re pat = re . compile ( res ) match = re pat . match if 1 : for name in names : if match ( name ) : result . append ( name ) else : for name in names : if match ( os . path . normcase ( name ) ) : result . append ( name ) return result", "predictions": ["dump self self . self . ."], "references": ["return the subset of the list names that match pat"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 297, "code": "def calculate transitive deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect imports ( modname , script , gopath ) : if imp . is native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue package dir , filename = os . path . split ( imp . script ) if filename == ' init .py' : package dir = os . path . dirname ( package dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package dir , ' init .py' ) calc ( modname , script ) package dir = os . path . dirname ( package dir ) calc ( modname , script ) deps . remove ( modname ) return deps", "predictions": ["make the script directory for the current script ."], "references": ["determines all modules that script transitively depends upon ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 298, "code": "def make future features ( node ) : assert isinstance ( node , ast . Import From ) assert node . module == ' future ' features = Future Features ( ) for alias in node . names : name = alias . name if name in FUTURE FEATURES : if name not in IMPLEMENTED FUTURE FEATURES : msg = 'future feature {} not yet implemented by grumpy' . format ( name ) raise util . Parse Error ( node , msg ) setattr ( features , name , True ) elif name == 'braces' : raise util . Parse Error ( node , 'not a chance' ) elif name not in REDUNDANT FUTURE FEATURES : msg = 'future feature {} is not defined' . format ( name ) raise util . Parse Error ( node , msg ) return features", "predictions": ["convert flags to flags . ."], "references": ["processes a future import statement returning set of flags it defines ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 299, "code": "def parse future features ( mod ) : assert isinstance ( mod , ast . Module ) found docstring = False for node in mod . body : if isinstance ( node , ast . Import From ) : if node . module == ' future ' : return node , make future features ( node ) break elif isinstance ( node , ast . Expr ) and not found docstring : if not isinstance ( node . value , ast . Str ) : break found docstring = True else : break return None , Future Features ( )", "predictions": ["parse features for a future = 0 ."], "references": ["accumulates a set of flags for the compiler __future__ imports ."], "bleu": 0.13107175678306446, "rouge_l": 0.20469798657718125}
{"id": 300, "code": "def from spec ( spec , kwargs = None ) : baseline = util . get object ( obj = spec , predefined objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline", "predictions": ["create an list of list object from a spec spec cache cache cache cache cache cache cache cache cache cache cache object cache cache cache cache cache cache cache object cache"], "references": ["creates a baseline from a specification dict ."], "bleu": 0.0513487742994337, "rouge_l": 0.1147695202257761}
{"id": 301, "code": "def from spec ( spec , kwargs = None ) : layer = util . get object ( obj = spec , predefined objects = tensorforce . core . networks . layers , kwargs = kwargs ) assert isinstance ( layer , Layer ) return layer", "predictions": ["create an instance of a depth - link spec object . . . instance . . instance . . . instance . . . pformat ."], "references": ["creates a layer from a specification dict ."], "bleu": 0.051660454541342535, "rouge_l": 0.1300639658848614}
{"id": 302, "code": "def from spec ( spec , kwargs ) : env = tensorforce . util . get object ( obj = spec , predefined objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env", "predictions": ["get object from spec spec instance . . . . . . . . ."], "references": ["creates an environment from a specification dict ."], "bleu": 0.09103526405546068, "rouge_l": 0.18401206636500753}
{"id": 303, "code": "def setup ( app ) : global is sphinx is sphinx = True app . add config value ( 'no underscore emphasis' , False , 'env' ) app . add source parser ( '.md' , M2R Parser ) app . add directive ( 'mdinclude' , Md Include )", "predictions": ["required sphinx extension setup function ."], "references": ["when used for spinx extension ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 304, "code": "def output image link ( self , m ) : return self . renderer . image link ( m . group ( 'url' ) , m . group ( 'target' ) , m . group ( 'alt' ) )", "predictions": ["returns the name of a image image"], "references": ["pass through rest role ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 305, "code": "def output eol literal marker ( self , m ) : marker = ':' if m . group ( 1 ) is None else '' return self . renderer . eol literal marker ( marker )", "predictions": ["returns the output rule rule rule expected by m expected to the given m expected expected rule expected expected rule expected expected expected expected expected rule expected rule expected by m"], "references": ["pass through rest link ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 306, "code": "def Worker Agent Generator ( agent class ) : if isinstance ( agent class , str ) : agent class = Agents Dictionary . get ( agent class ) if not agent class and agent class . find ( '.' ) != - 1 : module name , function name = agent class . rsplit ( '.' , 1 ) module = importlib . import module ( module name ) agent class = getattr ( module , function name ) class Worker Agent ( agent class ) : def init ( self , model = None , * * kwargs ) : self . model = model if not issubclass ( agent class , Learning Agent ) : kwargs . pop ( \"network\" ) super ( Worker Agent , self ) . init ( * * kwargs ) def initialize model ( self ) : return self . model return Worker Agent", "predictions": ["generates a model from a method rule rule rule rule rule rule rule rule rule rule ."], "references": ["worker agent generator receives an agent class and creates a worker agent class that inherits from that agent ."], "bleu": 0.07637273831183729, "rouge_l": 0.16501352569882777}
{"id": 307, "code": "def wait state ( self , state , reward , terminal ) : while state == [ None ] or not state : state , terminal , reward = self . execute ( dict ( key = 0 ) ) return state , terminal , reward", "predictions": ["replace the state in the state . with the given state . . . . . . . ."], "references": ["wait until there is a state ."], "bleu": 0.08475426399505566, "rouge_l": 0.16781292984869325}
{"id": 308, "code": "def from spec ( spec , kwargs = None ) : optimizer = util . get object ( obj = spec , predefined objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer", "predictions": ["build an instance of a spec object from a spec spec instance except it s ."], "references": ["creates an optimizer from a specification dict ."], "bleu": 0.11502783619900048, "rouge_l": 0.3546511627906977}
{"id": 309, "code": "def register saver ops ( self ) : variables = self . get savable variables ( ) if variables is None or len ( variables ) == 0 : self . saver = None return base scope = self . get base variable scope ( ) variables map = { strip name scope ( v . name , base scope ) : v for v in variables } self . saver = tf . train . Saver ( var list = variables map , reshape = False , sharded = False , max to keep = 5 , keep checkpoint every n hours = 10000.0 , name = None , restore sequentially = False , saver def = None , builder = None , defer build = False , allow empty = True , write version = tf . train . Saver Def . V2 , pad step number = False , save relative paths = True )", "predictions": ["register saver s s saver ."], "references": ["registers the saver operations to the graph in context ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 310, "code": "def from spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = Preprocessor Stack ( ) for preprocessor spec in spec : preprocessor kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get object ( obj = preprocessor spec , predefined objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack", "predictions": ["turn a list of preprocessors objects into a preprocessors instance . . instance ."], "references": ["creates a preprocessing stack from a specification dict ."], "bleu": 0.10511846841633776, "rouge_l": 0.271513353115727}
{"id": 311, "code": "def as local model ( self ) : super ( Memory Model , self ) . as local model ( ) self . optimizer spec = dict ( type = 'global optimizer' , optimizer = self . optimizer spec )", "predictions": ["optimizer the optimizer s optimizer"], "references": ["makes sure our optimizer is wrapped into the global_optimizer meta . this is only relevant for distributed rl ."], "bleu": 0.018373002712755784, "rouge_l": 0.1508034610630408}
{"id": 312, "code": "def from spec ( spec , kwargs = None ) : distribution = util . get object ( obj = spec , predefined objects = tensorforce . core . distributions . distributions , kwargs = kwargs ) assert isinstance ( distribution , Distribution ) return distribution", "predictions": ["create a input input input input object from a spec spec spec spec spec spec spec instance . object . ."], "references": ["creates a distribution from a specification dict ."], "bleu": 0.08687475782716618, "rouge_l": 0.3001230012300123}
{"id": 313, "code": "def from spec ( spec , kwargs ) : agent = util . get object ( obj = spec , predefined objects = tensorforce . agents . agents , kwargs = kwargs ) assert isinstance ( agent , Agent ) return agent", "predictions": ["create an instance from a warning warning warning . . . . . . ."], "references": ["creates an agent from a specification dict ."], "bleu": 0.12300686288463772, "rouge_l": 0.36802413273001505}
{"id": 314, "code": "def from spec ( spec , kwargs = None ) : network = util . get object ( obj = spec , default object = Layered Network , kwargs = kwargs ) assert isinstance ( network , Network ) return network", "predictions": ["create a network object from a spec spec spec spec spec spec spec spec . spec instance ."], "references": ["creates a network from a specification dict ."], "bleu": 0.11794224053267105, "rouge_l": 0.413279132791328}
{"id": 315, "code": "def move ( self , external index , new priority ) : index = external index + ( self . capacity - 1 ) return self . move ( index , new priority )", "predictions": ["warn the cursor to the new position . . . . . . . ."], "references": ["change the priority of a leaf node"], "bleu": 0.08225964699966554, "rouge_l": 0.09728867623604465}
{"id": 316, "code": "def move ( self , index , new priority ) : item , old priority = self . memory [ index ] old priority = old priority or 0 self . memory [ index ] = Sum Row ( item , new priority ) self . update internal nodes ( index , new priority - old priority )", "predictions": ["remove an in the raise a new priority ."], "references": ["change the priority of a leaf node ."], "bleu": 0.17747405280050263, "rouge_l": 0.35672514619883033}
{"id": 317, "code": "def next position then increment ( self ) : start = self . capacity - 1 position = start + self . position self . position = ( self . position + 1 ) % self . capacity return position", "predictions": ["increment the pop - self next position next position next next position next next except next next position next element next except it is self next"], "references": ["similar to position ++ ."], "bleu": 0.04668049023095243, "rouge_l": 0.07349397590361446}
{"id": 318, "code": "def sample with priority ( self , p ) : parent = 0 while True : left = 2 * parent + 1 if left >= len ( self . memory ) : return parent left p = self . memory [ left ] if left < self . capacity - 1 else ( self . memory [ left ] . priority or 0 ) if p <= left p : parent = left else : if left + 1 >= len ( self . memory ) : raise Runtime Error ( 'Right child is expected to exist.' ) p -= left p parent = left + 1", "predictions": ["return a go term str str str str . str ."], "references": ["sample random element with priority greater than p ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 319, "code": "def sample minibatch ( self , batch size ) : pool size = len ( self ) if pool size == 0 : return [ ] delta p = self . memory [ 0 ] / batch size chosen idx = [ ] if abs ( self . memory [ 0 ] ) < util . epsilon : chosen idx = np . random . randint ( self . capacity - 1 , self . capacity - 1 + len ( self ) , size = batch size ) . tolist ( ) else : for i in xrange ( batch size ) : lower = max ( i * delta p , 0 ) upper = min ( ( i + 1 ) * delta p , self . memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen idx . append ( self . sample with priority ( p ) ) return [ ( i , self . memory [ i ] ) for i in chosen idx ]", "predictions": ["dump a registry of data"], "references": ["sample minibatch of size batch_size ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 320, "code": "def disconnect ( self ) : if not self . socket : logging . warning ( \"No active socket to close!\" ) return self . socket . close ( ) self . socket = None", "predictions": ["disconnect from the active socket ."], "references": ["ends our server tcp connection ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 321, "code": "def is action available left ( self , state ) : for row in range ( 4 ) : has empty = False for col in range ( 4 ) : has empty |= state [ row , col ] == 0 if state [ row , col ] != 0 and has empty : return True if ( state [ row , col ] != 0 and col > 0 and state [ row , col ] == state [ row , col - 1 ] ) : return True return False", "predictions": ["check if the action is available for the given state ."], "references": ["determines whether action left is available ."], "bleu": 0.17033186037639278, "rouge_l": 0.4629981024667932}
{"id": 322, "code": "def do action ( self , action ) : temp state = np . rot90 ( self . state , action ) reward = self . do action left ( temp state ) self . state = np . rot90 ( temp state , - action ) self . score += reward self . add random tile ( ) return reward", "predictions": ["perform an action of an action"], "references": ["execute action add a new tile update the score & return the reward ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 323, "code": "def do action left ( self , state ) : reward = 0 for row in range ( 4 ) : merge candidate = - 1 merged = np . zeros ( ( 4 , ) , dtype = np . bool ) for col in range ( 4 ) : if state [ row , col ] == 0 : continue if ( merge candidate != - 1 and not merged [ merge candidate ] and state [ row , merge candidate ] == state [ row , col ] ) : state [ row , col ] = 0 merged [ merge candidate ] = True state [ row , merge candidate ] += 1 reward += 2 ** state [ row , merge candidate ] else : merge candidate += 1 if col != merge candidate : state [ row , merge candidate ] = state [ row , col ] state [ row , col ] = 0 return reward", "predictions": ["augment each action with the left state"], "references": ["executes action left ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 324, "code": "def add random tile ( self ) : x pos , y pos = np . where ( self . state == 0 ) assert len ( x pos ) != 0 empty index = np . random . choice ( len ( x pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . state [ x pos [ empty index ] , y pos [ empty index ] ] = value", "predictions": ["add a random tile to the buffer ."], "references": ["adds a random tile to the grid . assumes that it has empty fields ."], "bleu": 0.2544832510118625, "rouge_l": 0.49459459459459465}
{"id": 325, "code": "def print state ( self ) : def tile string ( value ) : \"\"\"Concert value to string.\"\"\" if value > 0 : return '% 5d' % ( 2 ** value , ) return \"     \" separator line = '-' * 25 print ( separator line ) for row in range ( 4 ) : print ( \"|\" + \"|\" . join ( [ tile string ( v ) for v in self . state [ row , : ] ] ) + \"|\" ) print ( separator line )", "predictions": ["print the state of the tile ."], "references": ["prints the current state ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 326, "code": "def setup saver ( self ) : if self . execution type == \"single\" : global variables = self . get variables ( include submodules = True , include nontrainable = True ) else : global variables = self . global model . get variables ( include submodules = True , include nontrainable = True ) for c in self . get savable components ( ) : c . register saver ops ( ) self . saver = tf . train . Saver ( var list = global variables , reshape = False , sharded = False , max to keep = 5 , keep checkpoint every n hours = 10000.0 , name = None , restore sequentially = False , saver def = None , builder = None , defer build = False , allow empty = True , write version = tf . train . Saver Def . V2 , pad step number = False , save relative paths = True )", "predictions": ["setup the sample saver ."], "references": ["creates the tf . train . saver object and stores it in self . saver ."], "bleu": 0.04278081081211661, "rouge_l": 0.26105563480741795}
{"id": 327, "code": "def make game ( ) : return ascii art . ascii art to game ( GAME ART , what lies beneath = ' ' , sprites = dict ( [ ( 'P' , Player Sprite ) ] + [ ( c , Upward Laser Bolt Sprite ) for c in UPWARD BOLT CHARS ] + [ ( c , Downward Laser Bolt Sprite ) for c in DOWNWARD BOLT CHARS ] ) , drapes = dict ( X = Marauder Drape , B = Bunker Drape ) , update schedule = [ 'P' , 'B' , 'X' ] + list ( ALL BOLT CHARS ) )", "predictions": ["build the game game"], "references": ["builds and returns an extraterrestrial marauders game ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 328, "code": "def fly ( self , board , layers , things , the plot ) : if ( self . character in the plot [ 'bunker hitters' ] or self . character in the plot [ 'marauder hitters' ] ) : return self . teleport ( ( - 1 , - 1 ) ) self . north ( board , the plot )", "predictions": ["does the actual work of the plot ."], "references": ["handles the behaviour of visible bolts flying toward marauders ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 329, "code": "def fire ( self , layers , things , the plot ) : if the plot . get ( 'last player shot' ) == the plot . frame : return the plot [ 'last player shot' ] = the plot . frame row , col = things [ 'P' ] . position self . teleport ( ( row - 1 , col ) )", "predictions": ["check for the plot of the plot ."], "references": ["launches a new bolt from the player ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 330, "code": "def fly ( self , board , layers , things , the plot ) : if self . character in the plot [ 'bunker hitters' ] : return self . teleport ( ( - 1 , - 1 ) ) if self . position == things [ 'P' ] . position : the plot . terminate episode ( ) self . south ( board , the plot )", "predictions": ["terminate the plot of the plot ."], "references": ["handles the behaviour of visible bolts flying toward the player ."], "bleu": 0.1319006407505858, "rouge_l": 0.4273204903677758}
{"id": 331, "code": "def fire ( self , layers , the plot ) : if the plot . get ( 'last marauder shot' ) == the plot . frame : return the plot [ 'last marauder shot' ] = the plot . frame col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 self . teleport ( ( row , col ) )", "predictions": ["randomly fire a plot of the plot ."], "references": ["launches a new bolt from a random marauder ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 332, "code": "def from spec ( spec ) : exploration = util . get object ( obj = spec , predefined objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration", "predictions": ["given a spec object return a explorations object ."], "references": ["creates an exploration object from a specification dict ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 333, "code": "def from spec ( spec , kwargs = None ) : memory = util . get object ( obj = spec , predefined objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory", "predictions": ["create an memory object from a spec spec ."], "references": ["creates a memory from a specification dict ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 334, "code": "def parse lheading ( self , m ) : self . tokens . append ( { 'type' : 'heading' , 'level' : 1 if m . group ( 2 ) == '=' else 2 , 'text' : m . group ( 1 ) , } )", "predictions": ["parse a lheading m ."], "references": ["parse setext heading ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 335, "code": "def process docstring ( app , what , name , obj , options , lines ) : markdown = \"\\n\" . join ( lines ) rest = m2r ( markdown ) rest . replace ( \"\\r\\n\" , \"\\n\" ) del lines [ : ] lines . extend ( rest . split ( \"\\n\" ) )", "predictions": ["process docstring and process docstring ."], "references": ["enable markdown syntax in docstrings"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 336, "code": "def setup components and tf funcs ( self , custom getter = None ) : custom getter = super ( Q Demo Model , self ) . setup components and tf funcs ( custom getter ) self . demo memory = Replay ( states = self . states spec , internals = self . internals spec , actions = self . actions spec , include next states = True , capacity = self . demo memory capacity , scope = 'demo-replay' , summary labels = self . summary labels ) self . fn import demo experience = tf . make template ( name = 'import-demo-experience' , func = self . tf import demo experience , custom getter = custom getter ) self . fn demo loss = tf . make template ( name = 'demo-loss' , func = self . tf demo loss , custom getter = custom getter ) self . fn combined loss = tf . make template ( name = 'combined-loss' , func = self . tf combined loss , custom getter = custom getter ) self . fn demo optimization = tf . make template ( name = 'demo-optimization' , func = self . tf demo optimization , custom getter = custom getter ) return custom getter", "predictions": ["sets up the components and custom components ."], "references": ["constructs the extra replay memory ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 337, "code": "def tf import demo experience ( self , states , internals , actions , terminal , reward ) : return self . demo memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )", "predictions": ["see base class ."], "references": ["imports a single experience to memory ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 338, "code": "def tf demo loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) deltas = list ( ) for name in sorted ( actions ) : action = actions [ name ] distr params = self . distributions [ name ] . parameterize ( x = embedding ) state action value = self . distributions [ name ] . state action value ( distr params = distr params , action = action ) if self . actions spec [ name ] [ 'type' ] == 'bool' : num actions = 2 action = tf . cast ( x = action , dtype = util . tf dtype ( 'int' ) ) else : num actions = self . actions spec [ name ] [ 'num actions' ] one hot = tf . one hot ( indices = action , depth = num actions ) ones = tf . ones like ( tensor = one hot , dtype = tf . float32 ) inverted one hot = ones - one hot state action values = self . distributions [ name ] . state action value ( distr params = distr params ) state action values = state action values + inverted one hot * self . expert margin supervised selector = tf . reduce max ( input tensor = state action values , axis = - 1 ) delta = supervised selector - state action value action size = util . prod ( self . actions spec [ name ] [ 'shape' ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , action size ) ) deltas . append ( delta ) loss per instance = tf . reduce mean ( input tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) loss per instance = tf . square ( x = loss per instance ) return tf . reduce mean ( input tensor = loss per instance , axis = 0 )", "predictions": ["tf demo loss per unit ."], "references": ["extends the q - model loss via the dqfd large - margin loss ."], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 339, "code": "def tf combined loss ( self , states , internals , actions , terminal , reward , next states , next internals , update , reference = None ) : q model loss = self . fn loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next states = next states , next internals = next internals , update = update , reference = reference ) demo loss = self . fn demo loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q model loss + self . supervised weight * demo loss", "predictions": ["tf loss for combined combined loss ."], "references": ["combines q - loss and demo loss ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 340, "code": "def import demo experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import demo experience output feed dict = self . get feed dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored session . run ( fetches = fetches , feed dict = feed dict )", "predictions": ["run demo output feed"], "references": ["stores demonstrations in the demo memory ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 341, "code": "def from config ( config , kwargs = None ) : return util . get object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )", "predictions": ["get object from config object ."], "references": ["creates a solver from a specification dict ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 342, "code": "def Set Clipboard Text ( text : str ) -> bool : if ctypes . windll . user32 . Open Clipboard ( 0 ) : ctypes . windll . user32 . Empty Clipboard ( ) text Byte Len = ( len ( text ) + 1 ) * 2 h Clipboard Data = ctypes . windll . kernel32 . Global Alloc ( 0 , text Byte Len ) h Dest Text = ctypes . windll . kernel32 . Global Lock ( h Clipboard Data ) ctypes . cdll . msvcrt . wcsncpy ( ctypes . c wchar p ( h Dest Text ) , ctypes . c wchar p ( text ) , text Byte Len // 2 ) ctypes . windll . kernel32 . Global Unlock ( h Clipboard Data ) ctypes . windll . user32 . Set Clipboard Data ( 13 , h Clipboard Data ) ctypes . windll . user32 . Close Clipboard ( ) return True return False", "predictions": ["sets the pointer to a pointer ."], "references": ["return bool true if succeed otherwise false ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 343, "code": "def mouse event ( dw Flags : int , dx : int , dy : int , dw Data : int , dw Extra Info : int ) -> None : ctypes . windll . user32 . mouse event ( dw Flags , dx , dy , dw Data , dw Extra Info )", "predictions": ["forward mouse event handler"], "references": ["mouse_event from win32 ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 344, "code": "def keybd event ( b Vk : int , b Scan : int , dw Flags : int , dw Extra Info : int ) -> None : ctypes . windll . user32 . keybd event ( b Vk , b Scan , dw Flags , dw Extra Info )", "predictions": ["add an event to an event ."], "references": ["keybd_event from win32 ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 345, "code": "def Keyboard Input ( w Vk : int , w Scan : int , dw Flags : int = Keyboard Event Flag . Key Down , time : int = 0 ) -> INPUT : return Create Input ( KEYBDINPUT ( w Vk , w Scan , dw Flags , time , None ) )", "predictions": ["create a new log ."], "references": ["create win32 struct keybdinput for sendinput ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 346, "code": "def Hardware Input ( u Msg : int , param : int = 0 ) -> INPUT : return Create Input ( HARDWAREINPUT ( u Msg , param & 0x FFFF , param >> 16 & 0x FFFF ) )", "predictions": ["backport support for pypyr ."], "references": ["create win32 struct hardwareinput for sendinput ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 347, "code": "def Delete Log ( ) -> None : if os . path . exists ( Logger . File Name ) : os . remove ( Logger . File Name )", "predictions": ["delete a file from the database ."], "references": ["delete log file ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 348, "code": "def Get All Pixel Colors ( self ) -> ctypes . Array : return self . Get Pixel Colors Of Rect ( 0 , 0 , self . Width , self . Height )", "predictions": ["gets all the points in this scene ."], "references": ["return ctypes . array an iterable array of int values in argb ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 349, "code": "def Get Parent Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get Parent Element ( self . Element ) return Control . Create Control From Element ( ele )", "predictions": ["returns an equivalent element corresponding to the device ."], "references": ["return control subclass or none ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 350, "code": "def Get First Child Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get First Child Element ( self . Element ) return Control . Create Control From Element ( ele )", "predictions": ["returns an lexicographic element ."], "references": ["return control subclass or none ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 351, "code": "def Get Last Child Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get Last Child Element ( self . Element ) return Control . Create Control From Element ( ele )", "predictions": ["returns an lexicographic instance for the device ."], "references": ["return control subclass or none ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 352, "code": "def Get Next Sibling Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get Next Sibling Element ( self . Element ) return Control . Create Control From Element ( ele )", "predictions": ["returns an equivalent element in the wire wire format . . . . . . . . ."], "references": ["return control subclass or none ."], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 353, "code": "def Get Previous Sibling Control ( self ) -> 'Control' : ele = Automation Client . instance ( ) . View Walker . Get Previous Sibling Element ( self . Element ) return Control . Create Control From Element ( ele )", "predictions": ["returns an equivalent device id for the given element 4 from the device 4 4 element 4 4 4 4 4 4 in the device 4 4 4 4 4 4"], "references": ["return control subclass or none ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 354, "code": "def Get Children ( self ) -> list : children = [ ] child = self . Get First Child Control ( ) while child : children . append ( child ) child = child . Get Next Sibling Control ( ) return children", "predictions": ["gets a list object for the tree of this tree reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward"], "references": ["return list a list of control subclasses ."], "bleu": 0.055177848898164926, "rouge_l": 0.17215428033866415}
{"id": 355, "code": "def Set Window Text ( self , text : str ) -> bool : handle = self . Native Window Handle if handle : return Set Window Text ( handle , text ) return False", "predictions": ["sets the window to match the state of the state in the state in the state in the state in the state in the state of the state in the state"], "references": ["call native setwindowtext if control has a valid native handle ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 356, "code": "def Is Top Level ( self ) -> bool : handle = self . Native Window Handle if handle : return Get Ancestor ( handle , GA Flag . Root ) == handle return False", "predictions": ["returns true if the process is a valid handle = true ."], "references": ["determine whether current control is top level ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 357, "code": "def Maximize ( self , wait Time : float = OPERATION WAIT TIME ) -> bool : if self . Is Top Level ( ) : return self . Show Window ( SW . Show Maximized , wait Time ) return False", "predictions": ["checks if the window is ready to be used in the terminal . . . . . . . . . . . . . . . . . . ."], "references": ["set top level window maximize ."], "bleu": 0.04317900023606586, "rouge_l": 0.12310797174571139}
{"id": 358, "code": "def Move To Center ( self ) -> bool : if self . Is Top Level ( ) : rect = self . Bounding Rectangle screen Width , screen Height = Get Screen Size ( ) x , y = ( screen Width - rect . width ( ) ) // 2 , ( screen Height - rect . height ( ) ) // 2 if x < 0 : x = 0 if y < 0 : y = 0 return Set Window Pos ( self . Native Window Handle , SWP . HWND Top , x , y , 0 , 0 , SWP . SWP No Size ) return False", "predictions": ["returns the bounds to the default screen == 1 == 1 == 1 == 1 == 1 == 1 == 1 == 1 == 1 == 1 == 1 == 1"], "references": ["move window to screen center ."], "bleu": 0.04317900023606586, "rouge_l": 0.12310797174571139}
{"id": 359, "code": "def Set Active ( self , wait Time : float = OPERATION WAIT TIME ) -> bool : if self . Is Top Level ( ) : handle = self . Native Window Handle if Is Iconic ( handle ) : ret = Show Window ( handle , SW . Restore ) elif not Is Window Visible ( handle ) : ret = Show Window ( handle , SW . Show ) ret = Set Foreground Window ( handle ) time . sleep ( wait Time ) return ret return False", "predictions": ["sets the widget to stop the window . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["set top level window active ."], "bleu": 0.04317900023606586, "rouge_l": 0.12310797174571139}
{"id": 360, "code": "def saliency map ( self , a , image , target , labels , mask , fast = False ) : alphas = a . gradient ( image , target ) * mask if fast : betas = - np . ones like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) idx = np . argmin ( salmap ) idx = np . unravel index ( idx , mask . shape ) pix sign = np . sign ( alphas ) [ idx ] return idx , pix sign", "predictions": ["computes the fly of a fly . . ."], "references": ["implements algorithm 3 in manuscript"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 361, "code": "def get output ( self , a , image ) : sd = np . square ( self . input images - image ) mses = np . mean ( sd , axis = tuple ( range ( 1 , sd . ndim ) ) ) index = np . argmin ( mses ) if mses [ index ] > 0 : raise Value Error ( 'No precomputed output image for this image' ) return self . output images [ index ]", "predictions": ["fire the output the output the output the output the output the output the output the the the output the output the output the output the output the output the output"], "references": ["looks up the precomputed adversarial image for a given image ."], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 362, "code": "def write error response ( self , message ) : self . set status ( 404 ) response = self . make error response ( str ( message ) ) now = time . time ( ) spent = now - self . basehandler starttime response [ constants . RESPONSE KEY EXECUTION TIME ] = spent self . write json response ( response )", "predictions": ["generates the error self ."], "references": ["writes the message as part of the response and sets 404 status ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 363, "code": "def write json response ( self , response ) : self . write ( tornado . escape . json encode ( response ) ) self . set header ( \"Content-Type\" , \"application/json\" )", "predictions": ["generates a json self ."], "references": ["write back json response"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 364, "code": "def create tar ( tar filename , files , config dir , config files ) : with contextlib . closing ( tarfile . open ( tar filename , 'w:gz' , dereference = True ) ) as tar : for filename in files : if os . path . isfile ( filename ) : tar . add ( filename , arcname = os . path . basename ( filename ) ) else : raise Exception ( \"%s is not an existing file\" % filename ) if os . path . isdir ( config dir ) : tar . add ( config dir , arcname = get heron sandbox conf dir ( ) ) else : raise Exception ( \"%s is not an existing directory\" % config dir ) for filename in config files : if os . path . isfile ( filename ) : arcfile = os . path . join ( get heron sandbox conf dir ( ) , os . path . basename ( filename ) ) tar . add ( filename , arcname = arcfile ) else : raise Exception ( \"%s is not an existing file\" % filename )", "predictions": ["from a spec file . and creates a spec with the given exploration file . . . . . . . . . . . . . . . . ."], "references": ["create a tar file with a given set of files"], "bleu": 0.0513487742994337, "rouge_l": 0.2149779735682819}
{"id": 365, "code": "def get subparser ( parser , command ) : subparsers actions = [ action for action in parser . actions if isinstance ( action , argparse . Sub Parsers Action ) ] for subparsers action in subparsers actions : for choice , subparser in subparsers action . choices . items ( ) : if choice == command : return subparser return None", "predictions": ["from parser predefined get spec predefined by core predefined get its choices predefined predefined predefined predefined predefined predefined ."], "references": ["retrieve the given subparser from parser"], "bleu": 0.08475426399505566, "rouge_l": 0.1765557163531114}
{"id": 366, "code": "def get heron libs ( local jars ) : heron lib dir = get heron lib dir ( ) heron libs = [ os . path . join ( heron lib dir , f ) for f in local jars ] return heron libs", "predictions": ["else heron libraries libraries libraries . from the local directory . . . . ."], "references": ["get all the heron lib jars with the absolute paths"], "bleu": 0.09103526405546068, "rouge_l": 0.1659863945578231}
{"id": 367, "code": "def parse override config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( \"=\" ) if len ( kv ) != 2 : raise Exception ( \"Invalid config property format (%s) expected key=value\" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides", "predictions": ["process a config docstring"], "references": ["parse the command line for overriding the defaults"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 368, "code": "def get java path ( ) : java home = os . environ . get ( \"JAVA HOME\" ) return os . path . join ( java home , BIN DIR , \"java\" )", "predictions": ["return and returns the components of the components"], "references": ["get the path of java executable"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 369, "code": "def check java home set ( ) : if \"JAVA HOME\" not in os . environ : Log . error ( \"JAVA HOME not set\" ) return False java path = get java path ( ) if os . path . isfile ( java path ) and os . access ( java path , os . X OK ) : return True Log . error ( \"JAVA HOME/bin/java either does not exist or not an executable\" ) return False", "predictions": ["checks if a import demo file exists ."], "references": ["check if the java home set"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 370, "code": "def check release file exists ( ) : release file = get heron release file ( ) if not os . path . isfile ( release file ) : Log . error ( \"Required file not found: %s\" % release file ) return False return True", "predictions": ["check if a demo demo loss loss loss loss is available"], "references": ["check if the release . yaml file exists"], "bleu": 0.14991106946711685, "rouge_l": 0.216696269982238}
{"id": 371, "code": "def unregister watch ( self , uid ) : Log . info ( \"Unregister a watch with uid: \" + str ( uid ) ) self . watches . pop ( uid , None )", "predictions": ["tf the combined by uid"], "references": ["unregister the watch with the given uuid ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 372, "code": "def num instances ( self ) : num = 0 components = self . spouts ( ) + self . bolts ( ) for component in components : config = component . comp . config for kvs in config . kvs : if kvs . key == api constants . TOPOLOGY COMPONENT PARALLELISM : num += int ( kvs . value ) break return num", "predictions": ["returns the number of demo demo in the output terminal terminal terminal terminal terminal terminal terminal terminal terminal terminal terminal terminal terminal terminal terminal terminal terminal by the user terminal terminal"], "references": ["number of spouts + bolts"], "bleu": 0.0513487742994337, "rouge_l": 0.12774869109947642}
{"id": 373, "code": "def convert pb kvs ( kvs , include non primitives = True ) : config = { } for kv in kvs : if kv . value : config [ kv . key ] = kv . value elif kv . serialized value : if topology pb2 . JAVA SERIALIZED VALUE == kv . type : jv = convert java value ( kv , include non primitives = include non primitives ) if jv is not None : config [ kv . key ] = jv else : config [ kv . key ] = raw value ( kv ) return config", "predictions": ["from kvs to get dict with default values"], "references": ["converts pb kvs to dict"], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 374, "code": "def synch topologies ( self ) : self . state managers = statemanagerfactory . get all state managers ( self . config . statemgr config ) try : for state manager in self . state managers : state manager . start ( ) except Exception as ex : Log . error ( \"Found exception while initializing state managers: %s. Bailing out...\" % ex ) traceback . print exc ( ) sys . exit ( 1 ) def on topologies watch ( state manager , topologies ) : \"\"\"watch topologies\"\"\" Log . info ( \"State watch triggered for topologies.\" ) Log . debug ( \"Topologies: \" + str ( topologies ) ) existing Topologies = self . get Topologies For State Location ( state manager . name ) existing Top Names = map ( lambda t : t . name , existing Topologies ) Log . debug ( \"Existing topologies: \" + str ( existing Top Names ) ) for name in existing Top Names : if name not in topologies : Log . info ( \"Removing topology: %s in rootpath: %s\" , name , state manager . rootpath ) self . remove Topology ( name , state manager . name ) for name in topologies : if name not in existing Top Names : self . add New Topology ( state manager , name ) for state manager in self . state managers : on Topologies Watch = partial ( on topologies watch , state manager ) state manager . get topologies ( on Topologies Watch )", "predictions": ["search for all state state and then starts them"], "references": ["sync the topologies with the statemgrs ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 375, "code": "def get Topologies For State Location ( self , name ) : return filter ( lambda t : t . state manager name == name , self . topologies )", "predictions": ["return all the nodes that have a specific ctypes dx ."], "references": ["returns all the topologies for a given state manager ."], "bleu": 0.17033186037639278, "rouge_l": 0.384251968503937}
{"id": 376, "code": "def remove Topology ( self , topology name , state manager name ) : topologies = [ ] for top in self . topologies : if ( top . name == topology name and top . state manager name == state manager name ) : if ( topology name , state manager name ) in self . topology Infos : self . topology Infos . pop ( ( topology name , state manager name ) ) else : topologies . append ( top ) self . topologies = topologies", "predictions": ["remove will remove a topology from all ctypes windll windll windll windll windll"], "references": ["removes the topology from the local cache ."], "bleu": 0.12571192676522522, "rouge_l": 0.19902120717781402}
{"id": 377, "code": "def validated formatter ( self , url format ) : valid parameters = { \"${CLUSTER}\" : \"cluster\" , \"${ENVIRON}\" : \"environ\" , \"${TOPOLOGY}\" : \"topology\" , \"${ROLE}\" : \"role\" , \"${USER}\" : \"user\" , } dummy formatted url = url format for key , value in valid parameters . items ( ) : dummy formatted url = dummy formatted url . replace ( key , value ) if '$' in dummy formatted url : raise Exception ( \"Invalid viz.url.format: %s\" % ( url format ) ) return url format", "predictions": ["check if the url is valid . . ."], "references": ["validate visualization url format"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 378, "code": "def to table ( components , topo info ) : inputs , outputs = defaultdict ( list ) , defaultdict ( list ) for ctype , component in components . items ( ) : if ctype == 'bolts' : for component name , component info in component . items ( ) : for input stream in component info [ 'inputs' ] : input name = input stream [ 'component name' ] inputs [ component name ] . append ( input name ) outputs [ input name ] . append ( component name ) info = [ ] spouts instance = topo info [ 'physical plan' ] [ 'spouts' ] bolts instance = topo info [ 'physical plan' ] [ 'bolts' ] for ctype , component in components . items ( ) : if ctype == \"stages\" : continue for component name , component info in component . items ( ) : row = [ ctype [ : - 1 ] , component name ] if ctype == 'spouts' : row . append ( len ( spouts instance [ component name ] ) ) else : row . append ( len ( bolts instance [ component name ] ) ) row . append ( ',' . join ( inputs . get ( component name , [ '-' ] ) ) ) row . append ( ',' . join ( outputs . get ( component name , [ '-' ] ) ) ) info . append ( row ) header = [ 'type' , 'name' , 'parallelism' , 'input' , 'output' ] return info , header", "predictions": ["convert a list of u = component to a table table table table table"], "references": ["normalize raw logical plan info to table"], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 379, "code": "def filter bolts ( table , header ) : bolts info = [ ] for row in table : if row [ 0 ] == 'bolt' : bolts info . append ( row ) return bolts info , header", "predictions": ["filter the bolts with the given header . ."], "references": ["filter to keep bolts"], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 380, "code": "def filter spouts ( table , header ) : spouts info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts info . append ( row ) return spouts info , header", "predictions": ["filter the table ctypes self . . . . ."], "references": ["filter to keep spouts"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 381, "code": "def validate state locations ( self ) : names = map ( lambda loc : loc [ \"name\" ] , self . locations ) assert len ( names ) == len ( set ( names ) ) , \"Names of state locations must be unique\"", "predictions": ["validate will validate state locations locations locations"], "references": ["names of all state locations must be unique ."], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 382, "code": "def initialize ( self , config , context ) : self . logger . info ( \"Initializing Pulsar Spout with the following\" ) self . logger . info ( \"Component-specific config: \\n%s\" % str ( config ) ) self . logger . info ( \"Context: \\n%s\" % str ( context ) ) self . emit count = 0 self . ack count = 0 self . fail count = 0 if not Pulsar Spout . service Url in config or not Pulsar Spout . topic Name in config : self . logger . fatal ( \"Need to specify both service Url and topic Name\" ) self . pulsar cluster = str ( config [ Pulsar Spout . service Url ] ) self . topic = str ( config [ Pulsar Spout . topic Name ] ) mode = config [ api constants . TOPOLOGY RELIABILITY MODE ] if mode == api constants . Topology Reliability Mode . ATLEAST ONCE : self . acking timeout = 1000 * int ( config [ api constants . TOPOLOGY MESSAGE TIMEOUT SECS ] ) else : self . acking timeout = 30000 if Pulsar Spout . receive Timeout Ms in config : self . receive timeout ms = config [ Pulsar Spout . receive Timeout Ms ] else : self . receive timeout ms = 10 if Pulsar Spout . deserializer in config : self . deserializer = config [ Pulsar Spout . deserializer ] if not callable ( self . deserializer ) : self . logger . fatal ( \"Pulsar Message Deserializer needs to be callable\" ) else : self . deserializer = self . default deserializer self . log Conf File Name = Generate Log Config ( context ) self . logger . info ( \"Generated Log Conf at %s\" % self . log Conf File Name ) self . client = pulsar . Client ( self . pulsar cluster , log conf file path = self . log Conf File Name ) self . logger . info ( \"Setup Client with cluster %s\" % self . pulsar cluster ) try : self . consumer = self . client . subscribe ( self . topic , context . get topology name ( ) , consumer type = pulsar . Consumer Type . Failover , unacked messages timeout ms = self . acking timeout ) except Exception as e : self . logger . fatal ( \"Pulsar client subscription failed: %s\" % str ( e ) ) self . logger . info ( \"Subscribed to topic %s\" % self . topic )", "predictions": ["initialize the client client client"], "references": ["implements pulsar spout s initialize method"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 383, "code": "def get Instance Jstack ( self , topology info , instance id ) : pid response = yield get Instance Pid ( topology info , instance id ) try : http client = tornado . httpclient . Async HTTP Client ( ) pid json = json . loads ( pid response ) pid = pid json [ 'stdout' ] . strip ( ) if pid == '' : raise Exception ( 'Failed to get pid' ) endpoint = utils . make shell endpoint ( topology info , instance id ) url = \"%s/jstack/%s\" % ( endpoint , pid ) response = yield http client . fetch ( url ) Log . debug ( \"HTTP call for url: %s\" , url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTP Error as e : raise Exception ( str ( e ) )", "predictions": ["get a shell instance"], "references": ["fetches instance jstack from heron - shell ."], "bleu": 0.14628187563941414, "rouge_l": 0.15721649484536082}
{"id": 384, "code": "def create parser ( subparsers ) : parser = subparsers . add parser ( 'update' , help = 'Update a topology' , usage = \"%(prog)s [options] cluster/[role]/[env] <topology-name> \" + \"[--component-parallelism <name:value>] \" + \"[--container-number value] \" + \"[--runtime-config [component:]<name:value>]\" , add help = True ) args . add titles ( parser ) args . add cluster role env ( parser ) args . add topology ( parser ) args . add config ( parser ) args . add dry run ( parser ) args . add service url ( parser ) args . add verbose ( parser ) def parallelism type ( value ) : pattern = re . compile ( r\"^[\\w\\.-]+:[\\d]+$\" ) if not pattern . match ( value ) : raise argparse . Argument Type Error ( \"Invalid syntax for component parallelism (<component name:value>): %s\" % value ) return value parser . add argument ( '--component-parallelism' , action = 'append' , type = parallelism type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component name>:<parallelism>' ) def runtime config type ( value ) : pattern = re . compile ( r\"^([\\w\\.-]+:){1,2}[\\w\\.-]+$\" ) if not pattern . match ( value ) : raise argparse . Argument Type Error ( \"Invalid syntax for runtime config ([component:]<name:value>): %s\" % value ) return value parser . add argument ( '--runtime-config' , action = 'append' , type = runtime config type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container number type ( value ) : pattern = re . compile ( r\"^\\d+$\" ) if not pattern . match ( value ) : raise argparse . Argument Type Error ( \"Invalid syntax for container number (value): %s\" % value ) return value parser . add argument ( '--container-number' , action = 'append' , type = container number type , required = False , help = 'Number of containers <value>' ) parser . set defaults ( subcommand = 'update' ) return parser", "predictions": ["create a parser for the script ."], "references": ["create the parse for the update command"], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 385, "code": "def build extra args dict ( cl args ) : component parallelism = cl args [ 'component parallelism' ] runtime configs = cl args [ 'runtime config' ] container number = cl args [ 'container number' ] if ( component parallelism and runtime configs ) or ( container number and runtime configs ) : raise Exception ( \"(component-parallelism or container num) and runtime-config \" + \"can't be updated at the same time\" ) dict extra args = { } nothing set = True if component parallelism : dict extra args . update ( { 'component parallelism' : component parallelism } ) nothing set = False if container number : dict extra args . update ( { 'container number' : container number } ) nothing set = False if runtime configs : dict extra args . update ( { 'runtime config' : runtime configs } ) nothing set = False if nothing set : raise Exception ( \"Missing arguments --component-parallelism or --runtime-config or --container-number\" ) if cl args [ 'dry run' ] : dict extra args . update ( { 'dry run' : True } ) if 'dry run format' in cl args : dict extra args . update ( { 'dry run format' : cl args [ \"dry run format\" ] } ) return dict extra args", "predictions": ["builds the extra args to be used in the extra arguments ."], "references": ["build extra args map"], "bleu": 0.1367440667823257, "rouge_l": 0.27477477477477474}
{"id": 386, "code": "def run ( command , parser , cl args , unknown args ) : Log . debug ( \"Update Args: %s\" , cl args ) extra lib jars = jars . packing jars ( ) action = \"update topology%s\" % ( ' in dry-run mode' if cl args [ \"dry run\" ] else '' ) dict extra args = { } try : dict extra args = build extra args dict ( cl args ) except Exception as err : return Simple Result ( Status . Invocation Error , err . message ) if cl args [ 'deploy mode' ] == config . SERVER MODE : return cli helper . run server ( command , cl args , action , dict extra args ) else : list extra args = convert args dict to list ( dict extra args ) return cli helper . run direct ( command , cl args , action , list extra args , extra lib jars )", "predictions": ["run a command in the cli ."], "references": ["run the update command"], "bleu": 0.22089591134157885, "rouge_l": 0.3824451410658307}
{"id": 387, "code": "def is grouping sane ( cls , gtype ) : if gtype == cls . SHUFFLE or gtype == cls . ALL or gtype == cls . LOWEST or gtype == cls . NONE : return True elif isinstance ( gtype , cls . FIELDS ) : return gtype . gtype == topology pb2 . Grouping . Value ( \"FIELDS\" ) and gtype . fields is not None elif isinstance ( gtype , cls . CUSTOM ) : return gtype . gtype == topology pb2 . Grouping . Value ( \"CUSTOM\" ) and gtype . python serialized is not None else : #pylint: disable=fixme #TODO: DIRECT are not supported yet return False", "predictions": ["check if gtype is supported by gtype ."], "references": ["checks if a given gtype is sane"], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 388, "code": "def register metrics ( self , metrics collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics collector . register metric ( field , metrics , interval )", "predictions": ["register metrics for each interval ."], "references": ["registers its metrics to a given metrics collector with a given interval"], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 389, "code": "def update received packet ( self , received pkt size bytes ) : self . update count ( self . RECEIVED PKT COUNT ) self . update count ( self . RECEIVED PKT SIZE , incr by = received pkt size bytes )", "predictions": ["data for updating received packet with the specified received size ."], "references": ["update received packet metrics"], "bleu": 0.14991106946711685, "rouge_l": 0.2911694510739857}
{"id": 390, "code": "def update sent packet ( self , sent pkt size bytes ) : self . update count ( self . SENT PKT COUNT ) self . update count ( self . SENT PKT SIZE , incr by = sent pkt size bytes )", "predictions": ["update the data by one of the provided packet ."], "references": ["update sent packet metrics"], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 391, "code": "def serialize data tuple ( self , stream id , latency in ns ) : self . update count ( self . TUPLE SERIALIZATION TIME NS , incr by = latency in ns , key = stream id )", "predictions": ["serialize the data tuple to a json compatible tuple ."], "references": ["apply update to serialization metrics"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 392, "code": "def init multi count metrics ( self , pplan helper ) : to init = [ self . metrics [ i ] for i in self . to multi init if i in self . metrics and isinstance ( self . metrics [ i ] , Multi Count Metric ) ] for out stream in pplan helper . get my spout ( ) . outputs : stream id = out stream . stream . id for metric in to init : metric . add key ( stream id )", "predictions": ["initialize metrics for each multi - detector stream ."], "references": ["initializes the default values for a necessary set of multicountmetrics"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 393, "code": "def next tuple ( self , latency in ns ) : self . update reduced metric ( self . NEXT TUPLE LATENCY , latency in ns ) self . update count ( self . NEXT TUPLE COUNT )", "predictions": ["mark a tuple of next metric as next tuple ."], "references": ["apply updates to the next tuple metrics"], "bleu": 0.16590387014219712, "rouge_l": 0.24302788844621517}
{"id": 394, "code": "def acked tuple ( self , stream id , complete latency ns ) : self . update count ( self . ACK COUNT , key = stream id ) self . update reduced metric ( self . COMPLETE LATENCY , complete latency ns , key = stream id )", "predictions": ["mark a tuple as finished"], "references": ["apply updates to the ack metrics"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 395, "code": "def failed tuple ( self , stream id , fail latency ns ) : self . update count ( self . FAIL COUNT , key = stream id ) self . update reduced metric ( self . FAIL LATENCY , fail latency ns , key = stream id )", "predictions": ["mark a failed metric as complete ."], "references": ["apply updates to the fail metrics"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 396, "code": "def init multi count metrics ( self , pplan helper ) : to in init = [ self . metrics [ i ] for i in self . inputs init if i in self . metrics and isinstance ( self . metrics [ i ] , Multi Count Metric ) ] for in stream in pplan helper . get my bolt ( ) . inputs : stream id = in stream . stream . id global stream id = in stream . stream . component name + \"/\" + stream id for metric in to in init : metric . add key ( stream id ) metric . add key ( global stream id ) to out init = [ self . metrics [ i ] for i in self . outputs init if i in self . metrics and isinstance ( self . metrics [ i ] , Multi Count Metric ) ] for out stream in pplan helper . get my bolt ( ) . outputs : stream id = out stream . stream . id for metric in to out init : metric . add key ( stream id )", "predictions": ["initialize metrics for each multi - stream"], "references": ["initializes the default values for a necessary set of multicountmetrics"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 397, "code": "def execute tuple ( self , stream id , source component , latency in ns ) : self . update count ( self . EXEC COUNT , key = stream id ) self . update reduced metric ( self . EXEC LATENCY , latency in ns , stream id ) self . update count ( self . EXEC TIME NS , incr by = latency in ns , key = stream id ) global stream id = source component + \"/\" + stream id self . update count ( self . EXEC COUNT , key = global stream id ) self . update reduced metric ( self . EXEC LATENCY , latency in ns , global stream id ) self . update count ( self . EXEC TIME NS , incr by = latency in ns , key = global stream id )", "predictions": ["execute the stream with the given id"], "references": ["apply updates to the execute metrics"], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 398, "code": "def deserialize data tuple ( self , stream id , source component , latency in ns ) : self . update count ( self . TUPLE DESERIALIZATION TIME NS , incr by = latency in ns , key = stream id ) global stream id = source component + \"/\" + stream id self . update count ( self . TUPLE DESERIALIZATION TIME NS , incr by = latency in ns , key = global stream id )", "predictions": ["deserialize a data tuple to a stream ."], "references": ["apply updates to the deserialization metrics"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 399, "code": "def acked tuple ( self , stream id , source component , latency in ns ) : self . update count ( self . ACK COUNT , key = stream id ) self . update reduced metric ( self . PROCESS LATENCY , latency in ns , stream id ) global stream id = source component + '/' + stream id self . update count ( self . ACK COUNT , key = global stream id ) self . update reduced metric ( self . PROCESS LATENCY , latency in ns , global stream id )", "predictions": ["mark a tuple as finished"], "references": ["apply updates to the ack metrics"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 400, "code": "def failed tuple ( self , stream id , source component , latency in ns ) : self . update count ( self . FAIL COUNT , key = stream id ) self . update reduced metric ( self . FAIL LATENCY , latency in ns , stream id ) global stream id = source component + '/' + stream id self . update count ( self . FAIL COUNT , key = global stream id ) self . update reduced metric ( self . FAIL LATENCY , latency in ns , global stream id )", "predictions": ["mark a tuple as failed"], "references": ["apply updates to the fail metrics"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 401, "code": "def parse ( version ) : match = REGEX . match ( version ) if match is None : raise Value Error ( '%s is not valid Sem Ver string' % version ) verinfo = match . groupdict ( ) verinfo [ 'major' ] = int ( verinfo [ 'major' ] ) verinfo [ 'minor' ] = int ( verinfo [ 'minor' ] ) verinfo [ 'patch' ] = int ( verinfo [ 'patch' ] ) return verinfo", "predictions": ["parse a verinfo and return a verinfo object"], "references": ["parse version to major minor patch pre - release build parts ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 402, "code": "def get all file state managers ( conf ) : state managers = [ ] state locations = conf . get state locations of type ( \"file\" ) for location in state locations : name = location [ 'name' ] rootpath = os . path . expanduser ( location [ 'rootpath' ] ) LOG . info ( \"Connecting to file state with rootpath: \" + rootpath ) state manager = File State Manager ( name , rootpath ) state managers . append ( state manager ) return state managers", "predictions": ["find all file managers from a given state"], "references": ["returns all the file state_managers ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 403, "code": "def incr ( self , key , to add = 1 ) : if key not in self . value : self . value [ key ] = Count Metric ( ) self . value [ key ] . incr ( to add )", "predictions": ["increments the maximum value by key ."], "references": ["increments the value of a given key by to_add"], "bleu": 0.21846599816382303, "rouge_l": 0.48897795591182364}
{"id": 404, "code": "def update ( self , key , value ) : if key not in self . value : self . value [ key ] = Reduced Metric ( self . reducer ) self . value [ key ] . update ( value )", "predictions": ["update a parameter ."], "references": ["updates a value of a given key and apply reduction"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 405, "code": "def add key ( self , key ) : if key not in self . value : self . value [ key ] = Reduced Metric ( self . reducer )", "predictions": ["add a key to the dictionary"], "references": ["adds a new key to this metric"], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 406, "code": "def add data tuple ( self , stream id , new data tuple , tuple size in bytes ) : if ( self . current data tuple set is None ) or ( self . current data tuple set . stream . id != stream id ) or ( len ( self . current data tuple set . tuples ) >= self . data tuple set capacity ) or ( self . current data tuple size in bytes >= self . max data tuple size in bytes ) : self . init new data tuple ( stream id ) added tuple = self . current data tuple set . tuples . add ( ) added tuple . Copy From ( new data tuple ) self . current data tuple size in bytes += tuple size in bytes self . total data emitted in bytes += tuple size in bytes", "predictions": ["add a data tuple for a stream"], "references": ["add a new data tuple to the currently buffered set of tuples"], "bleu": 0.15048220633585796, "rouge_l": 0.40197693574958815}
{"id": 407, "code": "def valid path ( path ) : if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False", "predictions": ["verify that a classpath path has a valid path ."], "references": ["check if an entry in the class path exists as either a directory or a file"], "bleu": 0.08227293930285365, "rouge_l": 0.1476997578692494}
{"id": 408, "code": "def valid java classpath ( classpath ) : paths = classpath . split ( ':' ) for path entry in paths : if not valid path ( path entry . strip ( ) ) : return False return True", "predictions": ["check that the classpath is a java java ."], "references": ["given a java classpath check whether the path entries are valid or not"], "bleu": 0.1416341262365823, "rouge_l": 0.17604617604617603}
{"id": 409, "code": "def load pex ( path to pex , include deps = True ) : abs path to pex = os . path . abspath ( path to pex ) Log . debug ( \"Add a pex to the path: %s\" % abs path to pex ) if abs path to pex not in sys . path : sys . path . insert ( 0 , os . path . dirname ( abs path to pex ) ) if include deps : for dep in get deps list ( abs path to pex ) : to join = os . path . join ( os . path . dirname ( abs path to pex ) , dep ) if to join not in sys . path : Log . debug ( \"Add a new dependency to the path: %s\" % dep ) sys . path . insert ( 0 , to join ) Log . debug ( \"Python path: %s\" % str ( sys . path ) )", "predictions": ["load a pex from a pex ."], "references": ["loads pex file and its dependencies to the current python path"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 410, "code": "def new source ( self , source ) : source streamlet = None if callable ( source ) : source streamlet = Supplier Streamlet ( source ) elif isinstance ( source , Generator ) : source streamlet = Generator Streamlet ( source ) else : raise Runtime Error ( \"Builder's new source has to be either a Generator or a function\" ) self . sources . append ( source streamlet ) return source streamlet", "predictions": ["create a new source object ."], "references": ["adds a new source to the computation dag"], "bleu": 0.2945901093386716, "rouge_l": 0.4178082191780822}
{"id": 411, "code": "def build ( self , bldr ) : stage names = sets . Set ( ) for source in self . sources : source . build ( bldr , stage names ) for source in self . sources : if not source . all built ( ) : raise Runtime Error ( \"Topology cannot be fully built! Are all sources added?\" )", "predictions": ["build the bldr from the bldr ."], "references": ["builds the topology and returns the builder"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 412, "code": "def replace ( config , wildcards , config file ) : for config key in config : config value = config [ config key ] original value = config value if isinstance ( config value , str ) : for token in wildcards : if wildcards [ token ] : config value = config value . replace ( token , wildcards [ token ] ) found = re . findall ( r'\\${[A-Z ]+}' , config value ) if found : raise Value Error ( \"%s=%s in file %s contains unsupported or unset wildcard tokens: %s\" % ( config key , original value , config file , \", \" . join ( found ) ) ) config [ config key ] = config value return config", "predictions": ["replace all values with a single config object ."], "references": ["for each kvp in config do wildcard substitution on the values"], "bleu": 0.12507277759788113, "rouge_l": 0.09822866344605477}
{"id": 413, "code": "def get command handlers ( ) : return { 'activate' : activate , 'config' : hconfig , 'deactivate' : deactivate , 'help' : cli help , 'kill' : kill , 'restart' : restart , 'submit' : submit , 'update' : update , 'version' : version }", "predictions": ["returns the default command to display"], "references": ["create a map of command names and handlers"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 414, "code": "def initialize ( self , config , context ) : if Sliding Window Bolt . WINDOW DURATION SECS in config : self . window duration = int ( config [ Sliding Window Bolt . WINDOW DURATION SECS ] ) else : self . logger . fatal ( \"Window Duration has to be specified in the config\" ) if Sliding Window Bolt . WINDOW SLIDEINTERVAL SECS in config : self . slide interval = int ( config [ Sliding Window Bolt . WINDOW SLIDEINTERVAL SECS ] ) else : self . slide interval = self . window duration if self . slide interval > self . window duration : self . logger . fatal ( \"Slide Interval should be <= Window Duration\" ) config [ api constants . TOPOLOGY TICK TUPLE FREQ SECS ] = str ( self . slide interval ) self . current tuples = deque ( ) if hasattr ( self , 'saved state' ) : if 'tuples' in self . saved state : self . current tuples = self . saved state [ 'tuples' ]", "predictions": ["initialize the slide and saved slide ."], "references": ["we initialize the window duration and slide interval"], "bleu": 0.240785655451027, "rouge_l": 0.5269978401727862}
{"id": 415, "code": "def initialize ( self , config , context ) : if Tumbling Window Bolt . WINDOW DURATION SECS in config : self . window duration = int ( config [ Tumbling Window Bolt . WINDOW DURATION SECS ] ) else : self . logger . fatal ( \"Window Duration has to be specified in the config\" ) config [ api constants . TOPOLOGY TICK TUPLE FREQ SECS ] = str ( self . window duration ) self . current tuples = deque ( ) if hasattr ( self , 'saved state' ) : if 'tuples' in self . saved state : self . current tuples = self . saved state [ 'tuples' ]", "predictions": ["initialize the window with a given config and duration ."], "references": ["we initialize the window duration and slide interval"], "bleu": 0.25965358893403384, "rouge_l": 0.4535315985130111}
{"id": 416, "code": "def get Stmgrs Reg Summary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats port : return reg request = tmaster pb2 . Stmgrs Registration Summary Request ( ) request str = reg request . Serialize To String ( ) port = str ( tmaster . stats port ) host = tmaster . host url = \"http://{0}:{1}/stmgrsregistrationsummary\" . format ( host , port ) request = tornado . httpclient . HTTP Request ( url , body = request str , method = 'POST' , request timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . Async HTTP Client ( ) result = yield client . fetch ( request ) Log . debug ( \"HTTP call complete.\" ) except tornado . httpclient . HTTP Error as e : raise Exception ( str ( e ) ) response Code = result . code if response Code >= 400 : message = \"Error in getting exceptions from Tmaster, code: \" + response Code Log . error ( message ) raise tornado . gen . Return ( { \"message\" : message } ) reg response = tmaster pb2 . Stmgrs Registration Summary Response ( ) reg response . Parse From String ( result . body ) ret = { } for stmgr in reg response . registered stmgrs : ret [ stmgr ] = True for stmgr in reg response . absent stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )", "predictions": ["create a env client ."], "references": ["get summary of stream managers registration summary"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 417, "code": "def setup ( executor ) : def signal handler ( signal to handle , frame ) : Log . info ( 'signal handler invoked with signal %s' , signal to handle ) executor . stop state manager watches ( ) sys . exit ( signal to handle ) def cleanup ( ) : Log . info ( 'Executor terminated; exiting all process in executor.' ) for pid in executor . processes to monitor . keys ( ) : os . kill ( pid , signal . SIGTERM ) time . sleep ( 5 ) os . killpg ( 0 , signal . SIGTERM ) shardid = executor . shard log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) pid = os . getpid ( ) sid = os . getsid ( pid ) if pid <> sid : Log . info ( 'Set up process group; executor becomes leader' ) os . setpgrp ( ) Log . info ( 'Register the SIGTERM signal handler' ) signal . signal ( signal . SIGTERM , signal handler ) Log . info ( 'Register the atexit clean up' ) atexit . register ( cleanup )", "predictions": ["build and updated the process environment . . ."], "references": ["set up log process and signal handlers"], "bleu": 0.15619699684601276, "rouge_l": 0.1278825995807128}
{"id": 418, "code": "def main ( ) : shell env = os . environ . copy ( ) shell env [ \"PEX ROOT\" ] = os . path . join ( os . path . abspath ( '.' ) , \".pex\" ) executor = Heron Executor ( sys . argv , shell env ) executor . initialize ( ) start ( executor )", "predictions": ["run the core . server . . . ."], "references": ["register exit handlers initialize the executor and run it ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 419, "code": "def get metricsmgr cmd ( self , metrics Manager Id , sink config file , port ) : metricsmgr main class = 'org.apache.heron.metricsmgr.Metrics Manager' metricsmgr cmd = [ os . path . join ( self . heron java home , 'bin/java' ) , '-Xmx1024M' , '-XX:+Print Command Line Flags' , '-verbosegc' , '-XX:+Print GC Details' , '-XX:+Print GC Time Stamps' , '-XX:+Print GC Date Stamps' , '-XX:+Print GC Cause' , '-XX:+Use GC Log File Rotation' , '-XX:Number Of GC Log Files=5' , '-XX:GC Log File Size=100M' , '-XX:+Print Promotion Failure' , '-XX:+Print Tenuring Distribution' , '-XX:+Print Heap At GC' , '-XX:+Heap Dump On Out Of Memory Error' , '-XX:+Use Conc Mark Sweep GC' , '-XX:+Print Command Line Flags' , '-Xloggc:log-files/gc.metricsmgr.log' , '-Djava.net.prefer I Pv4Stack=true' , '-cp' , self . metrics manager classpath , metricsmgr main class , '--id=' + metrics Manager Id , '--port=' + str ( port ) , '--topology=' + self . topology name , '--cluster=' + self . cluster , '--role=' + self . role , '--environment=' + self . environment , '--topology-id=' + self . topology id , '--system-config-file=' + self . heron internals config file , '--override-config-file=' + self . override config file , '--sink-config-file=' + sink config file ] return Command ( metricsmgr cmd , self . shell env )", "predictions": ["construct the grouping instance to run the grouping topology"], "references": ["get the command to start the metrics manager processes"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 420, "code": "def get metrics cache cmd ( self ) : metricscachemgr main class = 'org.apache.heron.metricscachemgr.Metrics Cache Manager' metricscachemgr cmd = [ os . path . join ( self . heron java home , 'bin/java' ) , '-Xmx1024M' , '-XX:+Print Command Line Flags' , '-verbosegc' , '-XX:+Print GC Details' , '-XX:+Print GC Time Stamps' , '-XX:+Print GC Date Stamps' , '-XX:+Print GC Cause' , '-XX:+Use GC Log File Rotation' , '-XX:Number Of GC Log Files=5' , '-XX:GC Log File Size=100M' , '-XX:+Print Promotion Failure' , '-XX:+Print Tenuring Distribution' , '-XX:+Print Heap At GC' , '-XX:+Heap Dump On Out Of Memory Error' , '-XX:+Use Conc Mark Sweep GC' , '-XX:+Print Command Line Flags' , '-Xloggc:log-files/gc.metricscache.log' , '-Djava.net.prefer I Pv4Stack=true' , '-cp' , self . metricscache manager classpath , metricscachemgr main class , \"--metricscache id\" , 'metricscache-0' , \"--master port\" , self . metricscache manager master port , \"--stats port\" , self . metricscache manager stats port , \"--topology name\" , self . topology name , \"--topology id\" , self . topology id , \"--system config file\" , self . heron internals config file , \"--override config file\" , self . override config file , \"--sink config file\" , self . metrics sinks config file , \"--cluster\" , self . cluster , \"--role\" , self . role , \"--environment\" , self . environment ] return Command ( metricscachemgr cmd , self . shell env )", "predictions": ["register metrics for the topology cache cache"], "references": ["get the command to start the metrics manager processes"], "bleu": 0.15447878876032708, "rouge_l": 0.12224448897795591}
{"id": 421, "code": "def get healthmgr cmd ( self ) : healthmgr main class = 'org.apache.heron.healthmgr.Health Manager' healthmgr cmd = [ os . path . join ( self . heron java home , 'bin/java' ) , '-Xmx1024M' , '-XX:+Print Command Line Flags' , '-verbosegc' , '-XX:+Print GC Details' , '-XX:+Print GC Time Stamps' , '-XX:+Print GC Date Stamps' , '-XX:+Print GC Cause' , '-XX:+Use GC Log File Rotation' , '-XX:Number Of GC Log Files=5' , '-XX:GC Log File Size=100M' , '-XX:+Print Promotion Failure' , '-XX:+Print Tenuring Distribution' , '-XX:+Print Heap At GC' , '-XX:+Heap Dump On Out Of Memory Error' , '-XX:+Use Conc Mark Sweep GC' , '-XX:+Print Command Line Flags' , '-Xloggc:log-files/gc.healthmgr.log' , '-Djava.net.prefer I Pv4Stack=true' , '-cp' , self . health manager classpath , healthmgr main class , \"--cluster\" , self . cluster , \"--role\" , self . role , \"--environment\" , self . environment , \"--topology name\" , self . topology name , \"--metricsmgr port\" , self . metrics manager port ] return Command ( healthmgr cmd , self . shell env )", "predictions": ["return the topology command to run the topology topology"], "references": ["get the command to start the topology health manager processes"], "bleu": 0.2187537716852318, "rouge_l": 0.5213675213675214}
{"id": 422, "code": "def get tmaster processes ( self ) : retval = { } tmaster cmd lst = [ self . tmaster binary , '--topology name=%s' % self . topology name , '--topology id=%s' % self . topology id , '--zkhostportlist=%s' % self . state manager connection , '--zkroot=%s' % self . state manager root , '--myhost=%s' % self . master host , '--master port=%s' % str ( self . master port ) , '--controller port=%s' % str ( self . tmaster controller port ) , '--stats port=%s' % str ( self . tmaster stats port ) , '--config file=%s' % self . heron internals config file , '--override config file=%s' % self . override config file , '--metrics sinks yaml=%s' % self . metrics sinks config file , '--metricsmgr port=%s' % str ( self . metrics manager port ) , '--ckptmgr port=%s' % str ( self . checkpoint manager port ) ] tmaster env = self . shell env . copy ( ) if self . shell env is not None else { } tmaster cmd = Command ( tmaster cmd lst , tmaster env ) if os . environ . get ( 'ENABLE HEAPCHECK' ) is not None : tmaster cmd . env . update ( { 'LD PRELOAD' : \"/usr/lib/libtcmalloc.so\" , 'HEAPCHECK' : \"normal\" } ) retval [ \"heron-tmaster\" ] = tmaster cmd if self . metricscache manager mode . lower ( ) != \"disabled\" : retval [ \"heron-metricscache\" ] = self . get metrics cache cmd ( ) if self . health manager mode . lower ( ) != \"disabled\" : retval [ \"heron-healthmgr\" ] = self . get healthmgr cmd ( ) retval [ self . metricsmgr ids [ 0 ] ] = self . get metricsmgr cmd ( self . metricsmgr ids [ 0 ] , self . metrics sinks config file , self . metrics manager port ) if self . is stateful topology : retval . update ( self . get ckptmgr process ( ) ) return retval", "predictions": ["return packet with topology ."], "references": ["get the command to start the tmaster processes"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 423, "code": "def get ckptmgr process ( self ) : ckptmgr main class = 'org.apache.heron.ckptmgr.Checkpoint Manager' ckptmgr ram mb = self . checkpoint manager ram / ( 1024 * 1024 ) ckptmgr cmd = [ os . path . join ( self . heron java home , \"bin/java\" ) , '-Xms%d M' % ckptmgr ram mb , '-Xmx%d M' % ckptmgr ram mb , '-XX:+Print Command Line Flags' , '-verbosegc' , '-XX:+Print GC Details' , '-XX:+Print GC Time Stamps' , '-XX:+Print GC Date Stamps' , '-XX:+Print GC Cause' , '-XX:+Use GC Log File Rotation' , '-XX:Number Of GC Log Files=5' , '-XX:GC Log File Size=100M' , '-XX:+Print Promotion Failure' , '-XX:+Print Tenuring Distribution' , '-XX:+Print Heap At GC' , '-XX:+Heap Dump On Out Of Memory Error' , '-XX:+Use Conc Mark Sweep GC' , '-XX:+Use Conc Mark Sweep GC' , '-Xloggc:log-files/gc.ckptmgr.log' , '-Djava.net.prefer I Pv4Stack=true' , '-cp' , self . checkpoint manager classpath , ckptmgr main class , '-t' + self . topology name , '-i' + self . topology id , '-c' + self . ckptmgr ids [ self . shard ] , '-p' + self . checkpoint manager port , '-f' + self . stateful config file , '-o' + self . override config file , '-g' + self . heron internals config file ] retval = { } retval [ self . ckptmgr ids [ self . shard ] ] = Command ( ckptmgr cmd , self . shell env ) return retval", "predictions": ["return data for data tuple"], "references": ["get the command to start the checkpoint manager process"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 424, "code": "def get heron support processes ( self ) : retval = { } retval [ self . heron shell ids [ self . shard ] ] = Command ( [ '%s' % self . heron shell binary , '--port=%s' % self . shell port , '--log file prefix=%s/heron-shell-%s.log' % ( self . log dir , self . shard ) , '--secret=%s' % self . topology id ] , self . shell env ) return retval", "predictions": ["key to get the multi - count metrics ."], "references": ["get a map from all daemon services name to the command to start them"], "bleu": 0.09630141125179911, "rouge_l": 0.1673525377229081}
{"id": 425, "code": "def wait process std out err ( self , name , process ) : proc . stream process stdout ( process , stdout log fn ( name ) ) process . wait ( )", "predictions": ["next tuple to metric"], "references": ["wait for the termination of a process and log its stdout & stderr"], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 426, "code": "def start processes ( self , commands ) : Log . info ( \"Start processes\" ) processes to monitor = { } for ( name , command ) in commands . items ( ) : p = self . run process ( name , command ) processes to monitor [ p . pid ] = Process Info ( p , name , command ) log pid for process ( name , p . pid ) with self . process lock : self . processes to monitor . update ( processes to monitor )", "predictions": ["start a list of tuple of tuple of tuple ns tuple ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns"], "references": ["start all commands and add them to the dict of processes to be monitored"], "bleu": 0.0478968583748614, "rouge_l": 0.10132890365448505}
{"id": 427, "code": "def start state manager watches ( self ) : Log . info ( \"Start state manager watches\" ) statemgr config = State Mgr Config ( ) statemgr config . set state locations ( configloader . load state manager locations ( self . cluster , state manager config file = self . state manager config file , overrides = { \"heron.statemgr.connection.string\" : self . state manager connection } ) ) try : self . state managers = statemanagerfactory . get all state managers ( statemgr config ) for state manager in self . state managers : state manager . start ( ) except Exception as ex : Log . error ( \"Found exception while initializing state managers: %s. Bailing out...\" % ex ) traceback . print exc ( ) sys . exit ( 1 ) def on packing plan watch ( state manager , new packing plan ) : Log . debug ( \"State watch triggered for Packing Plan update on shard %s. Existing: %s, New: %s\" % ( self . shard , str ( self . packing plan ) , str ( new packing plan ) ) ) if self . packing plan != new packing plan : Log . info ( \"Packing Plan change detected on shard %s, relaunching effected processes.\" % self . shard ) self . update packing plan ( new packing plan ) Log . info ( \"Updating executor processes\" ) self . launch ( ) else : Log . info ( \"State watch triggered for Packing Plan update but plan not changed so not relaunching.\" ) for state manager in self . state managers : on Packing Plan Watch = functools . partial ( on packing plan watch , state manager ) state manager . get packing plan ( self . topology name , on Packing Plan Watch ) Log . info ( \"Registered state watch for packing plan changes with state manager %s.\" % str ( state manager ) )", "predictions": ["starts the tuple of tuple with the tuple of tuple ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns ns tuple"], "references": ["receive updates to the packing plan from the statemgrs and update processes as needed ."], "bleu": 0.04317900023606586, "rouge_l": 0.09277566539923954}
{"id": 428, "code": "def run ( self , name , config , builder ) : if not isinstance ( name , str ) : raise Runtime Error ( \"Name has to be a string type\" ) if not isinstance ( config , Config ) : raise Runtime Error ( \"config has to be a Config type\" ) if not isinstance ( builder , Builder ) : raise Runtime Error ( \"builder has to be a Builder type\" ) bldr = Topology Builder ( name = name ) builder . build ( bldr ) bldr . set config ( config . api config ) bldr . build and submit ( )", "predictions": ["init a set of packages"], "references": ["builds the topology and submits it"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 429, "code": "def modules to main ( mod List ) : if not mod List : return main = sys . modules [ ' main ' ] for modname in mod List : if isinstance ( modname , str ) : try : mod = import ( modname ) except Exception : sys . stderr . write ( 'warning: could not import %s\\n.  ' 'Your function may unexpectedly error due to this import failing;' 'A version mismatch is likely.  Specific error was:\\n' % modname ) print exec ( sys . stderr ) else : setattr ( main , mod . name , mod )", "predictions": ["import up execute execute execute execute execute execute execute execute execute execute execute execute execute execute ."], "references": ["force every module in modlist to be placed into main"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 430, "code": "def load class ( cls , d ) : for k , v in d . items ( ) : if isinstance ( k , tuple ) : typ , k = k if typ == 'property' : v = property ( * v ) elif typ == 'staticmethod' : v = staticmethod ( v ) elif typ == 'classmethod' : v = classmethod ( v ) setattr ( cls , k , v ) return cls", "predictions": ["deserialize a data data data object to a class"], "references": ["loads additional properties into class cls ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 431, "code": "def save module ( self , obj ) : self . modules . add ( obj ) self . save reduce ( subimport , ( obj . name , ) , obj = obj )", "predictions": ["save a tuple of latency latency component component component component component component component component component component component component component component component component component component component component component component component component component"], "references": ["save a module as an import"], "bleu": 0.0513487742994337, "rouge_l": 0.12310797174571139}
{"id": 432, "code": "def tail ( filename , n ) : size = os . path . getsize ( filename ) with open ( filename , \"rb\" ) as f : fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP SHARED , mmap . PROT READ ) try : for i in xrange ( size - 1 , - 1 , - 1 ) : if fm [ i ] == '\\n' : n -= 1 if n == - 1 : break return fm [ i + 1 if i else 0 : ] . splitlines ( ) finally : fm . close ( )", "predictions": ["failed failed function to failed failed lines in filename in filename in filename in filename in filename in filename in filename in filename in filename in filename in filename in filename"], "references": ["returns last n lines from the filename . no exception handling"], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 433, "code": "def get serializer ( context ) : cluster config = context . get cluster config ( ) serializer clsname = cluster config . get ( constants . TOPOLOGY SERIALIZER CLASSNAME , None ) if serializer clsname is None : return Python Serializer ( ) else : try : topo pex path = context . get topology pex path ( ) pex loader . load pex ( topo pex path ) serializer cls = pex loader . import and get class ( topo pex path , serializer clsname ) serializer = serializer cls ( ) return serializer except Exception as e : raise Runtime Error ( \"Error with loading custom serializer class: %s, with error message: %s\" % ( serializer clsname , str ( e ) ) )", "predictions": ["returns the serializer instance if it exists else none ."], "references": ["returns a serializer for a given context"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 434, "code": "def template slave hcl ( cl args , masters ) : slave config template = \"%s/standalone/templates/slave.template.hcl\" % cl args [ \"config path\" ] slave config actual = \"%s/standalone/resources/slave.hcl\" % cl args [ \"config path\" ] masters in quotes = [ '\"%s\"' % master for master in masters ] template file ( slave config template , slave config actual , { \"<nomad masters:master port>\" : \", \" . join ( masters in quotes ) } )", "predictions": ["render a get configuration file for all masters ."], "references": ["template slave config file"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 435, "code": "def template scheduler yaml ( cl args , masters ) : single master = masters [ 0 ] scheduler config actual = \"%s/standalone/scheduler.yaml\" % cl args [ \"config path\" ] scheduler config template = \"%s/standalone/templates/scheduler.template.yaml\" % cl args [ \"config path\" ] template file ( scheduler config template , scheduler config actual , { \"<scheduler uri>\" : \"http://%s:4646\" % single master } )", "predictions": ["create a self . self . master"], "references": ["template scheduler . yaml"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 436, "code": "def template uploader yaml ( cl args , masters ) : single master = masters [ 0 ] uploader config template = \"%s/standalone/templates/uploader.template.yaml\" % cl args [ \"config path\" ] uploader config actual = \"%s/standalone/uploader.yaml\" % cl args [ \"config path\" ] template file ( uploader config template , uploader config actual , { \"<http uploader uri>\" : \"http://%s:9000/api/v1/file/upload\" % single master } )", "predictions": ["build a self . self . self . self . self . self . self . self . self . self . self . self . self . self . self"], "references": ["tempate uploader . yaml"], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 437, "code": "def template apiserver hcl ( cl args , masters , zookeepers ) : single master = masters [ 0 ] apiserver config template = \"%s/standalone/templates/apiserver.template.hcl\" % cl args [ \"config path\" ] apiserver config actual = \"%s/standalone/resources/apiserver.hcl\" % cl args [ \"config path\" ] replacements = { \"<heron apiserver hostname>\" : '\"%s\"' % get hostname ( single master , cl args ) , \"<heron apiserver executable>\" : '\"%s/heron-apiserver\"' % config . get heron bin dir ( ) if is self ( single master ) else '\"%s/.heron/bin/heron-apiserver\"' % get remote home ( single master , cl args ) , \"<zookeeper host:zookeeper port>\" : \",\" . join ( [ '%s' % zk if \":\" in zk else '%s:2181' % zk for zk in zookeepers ] ) , \"<scheduler uri>\" : \"http://%s:4646\" % single master } template file ( apiserver config template , apiserver config actual , replacements )", "predictions": ["a convenience function which will be executed on a add add a add add a add add add to the add a add to the add in a add in a"], "references": ["template apiserver . hcl"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 438, "code": "def template statemgr yaml ( cl args , zookeepers ) : statemgr config file template = \"%s/standalone/templates/statemgr.template.yaml\" % cl args [ \"config path\" ] statemgr config file actual = \"%s/standalone/statemgr.yaml\" % cl args [ \"config path\" ] template file ( statemgr config file template , statemgr config file actual , { \"<zookeeper host:zookeeper port>\" : \",\" . join ( [ '\"%s\"' % zk if \":\" in zk else '\"%s:2181\"' % zk for zk in zookeepers ] ) } )", "predictions": ["display a tuple of add a tuple of add in the add add a add add a add add add add add a add - in the add a add add"], "references": ["template statemgr . yaml"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 439, "code": "def print cluster info ( cl args ) : parsed roles = read and parse roles ( cl args ) masters = list ( parsed roles [ Role . MASTERS ] ) slaves = list ( parsed roles [ Role . SLAVES ] ) zookeepers = list ( parsed roles [ Role . ZOOKEEPERS ] ) cluster = list ( parsed roles [ Role . CLUSTER ] ) info = Ordered Dict ( ) info [ 'num Nodes' ] = len ( cluster ) info [ 'nodes' ] = cluster roles = Ordered Dict ( ) roles [ 'masters' ] = masters roles [ 'slaves' ] = slaves roles [ 'zookeepers' ] = zookeepers urls = Ordered Dict ( ) urls [ 'service Url' ] = get service url ( cl args ) urls [ 'heron Ui' ] = get heron ui url ( cl args ) urls [ 'heron Tracker' ] = get heron tracker url ( cl args ) info [ 'roles' ] = roles info [ 'urls' ] = urls print json . dumps ( info , indent = 2 )", "predictions": ["valid path information to path"], "references": ["get cluster info for standalone cluster"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 440, "code": "def add additional args ( parsers ) : for parser in parsers : cli args . add verbose ( parser ) cli args . add config ( parser ) parser . add argument ( '--heron-dir' , default = config . get heron dir ( ) , help = 'Path to Heron home directory' )", "predictions": ["valid java home for the command line arguments for the parsers for the parsers"], "references": ["add additional parameters to parser"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 441, "code": "def start cluster ( cl args ) : roles = read and parse roles ( cl args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] zookeepers = roles [ Role . ZOOKEEPERS ] Log . info ( \"Roles:\" ) Log . info ( \" - Master Servers: %s\" % list ( masters ) ) Log . info ( \" - Slave Servers: %s\" % list ( slaves ) ) Log . info ( \" - Zookeeper Servers: %s\" % list ( zookeepers ) ) if not masters : Log . error ( \"No master servers specified!\" ) sys . exit ( - 1 ) if not slaves : Log . error ( \"No slave servers specified!\" ) sys . exit ( - 1 ) if not zookeepers : Log . error ( \"No zookeeper servers specified!\" ) sys . exit ( - 1 ) update config files ( cl args ) dist nodes = list ( masters . union ( slaves ) ) if not ( len ( dist nodes ) == 1 and is self ( dist nodes [ 0 ] ) ) : distribute package ( roles , cl args ) start master nodes ( masters , cl args ) start slave nodes ( slaves , cl args ) start api server ( masters , cl args ) start heron tools ( masters , cl args ) Log . info ( \"Heron standalone cluster complete!\" )", "predictions": ["load a pex pex"], "references": ["start a heron standalone cluster"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 442, "code": "def start heron tools ( masters , cl args ) : single master = list ( masters ) [ 0 ] wait for master to start ( single master ) cmd = \"%s run %s >> /tmp/heron tools start.log 2>&1 &\" % ( get nomad path ( cl args ) , get heron tools job file ( cl args ) ) Log . info ( \"Starting Heron Tools on %s\" % single master ) if not is self ( single master ) : cmd = ssh remote execute ( cmd , single master , cl args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( \"return code: %s output: %s\" % ( return code , output ) ) if return code != 0 : Log . error ( \"Failed to start Heron Tools on %s with error:\\n%s\" % ( single master , output [ 1 ] ) ) sys . exit ( - 1 ) wait for job to start ( single master , \"heron-tools\" ) Log . info ( \"Done starting Heron Tools\" )", "predictions": ["new tools tools starting to all the tools ."], "references": ["start heron tracker and ui"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 443, "code": "def distribute package ( roles , cl args ) : Log . info ( \"Distributing heron package to nodes (this might take a while)...\" ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] tar file = tempfile . Named Temporary File ( suffix = \".tmp\" ) . name Log . debug ( \"TAR file %s to %s\" % ( cl args [ \"heron dir\" ] , tar file ) ) make tarfile ( tar file , cl args [ \"heron dir\" ] ) dist nodes = masters . union ( slaves ) scp package ( tar file , dist nodes , cl args )", "predictions": ["build a tar package package"], "references": ["distribute heron packages to all nodes"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 444, "code": "def wait for master to start ( single master ) : i = 0 while True : try : r = requests . get ( \"http://%s:4646/v1/status/leader\" % single master ) if r . status code == 200 : break except : Log . debug ( sys . exc info ( ) [ 0 ] ) Log . info ( \"Waiting for cluster to come up... %s\" % i ) time . sleep ( 1 ) if i > 10 : Log . error ( \"Failed to start Nomad Cluster!\" ) sys . exit ( - 1 ) i = i + 1", "predictions": ["replace the config to with a config config"], "references": ["wait for a nomad master to start"], "bleu": 0.17747405280050269, "rouge_l": 0.13495575221238937}
{"id": 445, "code": "def wait for job to start ( single master , job ) : i = 0 while True : try : r = requests . get ( \"http://%s:4646/v1/job/%s\" % ( single master , job ) ) if r . status code == 200 and r . json ( ) [ \"Status\" ] == \"running\" : break else : raise Runtime Error ( ) except : Log . debug ( sys . exc info ( ) [ 0 ] ) Log . info ( \"Waiting for %s to come up... %s\" % ( job , i ) ) time . sleep ( 1 ) if i > 20 : Log . error ( \"Failed to start Nomad Cluster!\" ) sys . exit ( - 1 ) i = i + 1", "predictions": ["get handlers until a handlers has been reached ."], "references": ["wait for a nomad job to start"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 446, "code": "def scp package ( package file , destinations , cl args ) : pids = [ ] for dest in destinations : if is self ( dest ) : continue Log . info ( \"Server: %s\" % dest ) file path = \"/tmp/heron.tar.gz\" dest file path = \"%s:%s\" % ( dest , file path ) remote cmd = \"rm -rf ~/.heron && mkdir ~/.heron \" \"&& tar -xzvf %s -C ~/.heron --strip-components 1\" % ( file path ) cmd = '%s && %s' % ( scp cmd ( package file , dest file path , cl args ) , ssh remote execute ( remote cmd , dest , cl args ) ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { \"pid\" : pid , \"dest\" : dest } ) errors = [ ] for entry in pids : pid = entry [ \"pid\" ] return code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( \"return code: %s output: %s\" % ( return code , output ) ) if return code != 0 : errors . append ( \"Failed to scp package to %s with error:\\n%s\" % ( entry [ \"dest\" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( \"Done distributing packages\" )", "predictions": ["initialize \"done package to"], "references": ["scp and extract package"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 447, "code": "def read and parse roles ( cl args ) : roles = dict ( ) with open ( get inventory file ( cl args ) , 'r' ) as stream : try : roles = yaml . load ( stream ) except yaml . YAML Error as exc : Log . error ( \"Error parsing inventory file: %s\" % exc ) sys . exit ( - 1 ) if Role . ZOOKEEPERS not in roles or not roles [ Role . ZOOKEEPERS ] : Log . error ( \"Zookeeper servers node defined!\" ) sys . exit ( - 1 ) if Role . CLUSTER not in roles or not roles [ Role . CLUSTER ] : Log . error ( \"Heron cluster nodes defined!\" ) sys . exit ( - 1 ) roles [ Role . MASTERS ] = set ( [ roles [ Role . CLUSTER ] [ 0 ] ] ) roles [ Role . SLAVES ] = set ( roles [ Role . CLUSTER ] ) roles [ Role . ZOOKEEPERS ] = set ( roles [ Role . ZOOKEEPERS ] ) roles [ Role . CLUSTER ] = set ( roles [ Role . CLUSTER ] ) return roles", "predictions": ["initialize roles and and and and parse roles"], "references": ["read config files to get roles"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 448, "code": "def get remote home ( host , cl args ) : cmd = \"echo ~\" if not is self ( host ) : cmd = ssh remote execute ( cmd , host , cl args ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return code = pid . wait ( ) output = pid . communicate ( ) if return code != 0 : Log . error ( \"Failed to get home path for remote host %s with output:\\n%s\" % ( host , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( \"\\n\" )", "predictions": ["get home home path"], "references": ["get home directory of remote host"], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 449, "code": "def get hostname ( ip addr , cl args ) : if is self ( ip addr ) : return get self hostname ( ) cmd = \"hostname\" ssh cmd = ssh remote execute ( cmd , ip addr , cl args ) pid = subprocess . Popen ( ssh cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return code = pid . wait ( ) output = pid . communicate ( ) if return code != 0 : Log . error ( \"Failed to get hostname for remote host %s with output:\\n%s\" % ( ip addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( \"\\n\" )", "predictions": ["get the hostname of a remote ip address ."], "references": ["get host name of remote host"], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 450, "code": "def is self ( addr ) : ips = [ ] for i in netifaces . interfaces ( ) : entry = netifaces . ifaddresses ( i ) if netifaces . AF INET in entry : for ipv4 in entry [ netifaces . AF INET ] : if \"addr\" in ipv4 : ips . append ( ipv4 [ \"addr\" ] ) return addr in ips or addr == get self hostname ( )", "predictions": ["return whether an ipv4 address is an ipv4 address ."], "references": ["check if this host is this addr"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 451, "code": "def to table ( result ) : max count = 20 table , count = [ ] , 0 for role , envs topos in result . items ( ) : for env , topos in envs topos . items ( ) : for topo in topos : count += 1 if count > max count : continue else : table . append ( [ role , env , topo ] ) header = [ 'role' , 'env' , 'topology' ] rest count = 0 if count <= max count else count - max count return table , header , rest count", "predictions": ["format a result result into a table ."], "references": ["normalize raw result to table"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 452, "code": "def show cluster ( cl args , cluster ) : try : result = tracker access . get cluster topologies ( cluster ) if not result : Log . error ( 'No topologies in cluster \\'%s\\'' % cluster ) return False result = result [ cluster ] except Exception : Log . error ( \"Fail to connect to tracker: \\'%s\\'\" , cl args [ \"tracker url\" ] ) return False table , header , rest count = to table ( result ) print ( 'Topologies running in cluster \\'%s\\'' % cluster ) if rest count : print ( '  with %d more...' % rest count ) print ( tabulate ( table , headers = header ) ) return True", "predictions": ["show cluster cluster count"], "references": ["print topologies information to stdout"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 453, "code": "def show cluster role ( cl args , cluster , role ) : try : result = tracker access . get cluster role topologies ( cluster , role ) if not result : Log . error ( 'Unknown cluster/role \\'%s\\'' % '/' . join ( [ cluster , role ] ) ) return False result = result [ cluster ] except Exception : Log . error ( \"Fail to connect to tracker: \\'%s\\'\" , cl args [ \"tracker url\" ] ) return False table , header , rest count = to table ( result ) print ( 'Topologies running in cluster \\'%s\\' submitted by \\'%s\\':' % ( cluster , role ) ) if rest count : print ( '  with %d more...' % rest count ) print ( tabulate ( table , headers = header ) ) return True", "predictions": ["show a cluster role"], "references": ["print topologies information to stdout"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 454, "code": "def show cluster role env ( cl args , cluster , role , env ) : try : result = tracker access . get cluster role env topologies ( cluster , role , env ) if not result : Log . error ( 'Unknown cluster/role/env \\'%s\\'' % '/' . join ( [ cluster , role , env ] ) ) return False result = result [ cluster ] except Exception : Log . error ( \"Fail to connect to tracker: \\'%s\\'\" , cl args [ \"tracker url\" ] ) return False table , header , rest count = to table ( result ) print ( % ( cluster , role , env ) ) if rest count : print ( '  with %d more...' % rest count ) print ( tabulate ( table , headers = header ) ) return True", "predictions": ["show cluster role env ."], "references": ["print topologies information to stdout"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 455, "code": "def pick unused port ( self ) : s = socket . socket ( socket . AF INET , socket . SOCK STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) , port = s . getsockname ( ) s . close ( ) return port", "predictions": ["pick a unused port with the given port ."], "references": ["pick an unused port . there is a slight chance that this wont work ."], "bleu": 0.12551072480361714, "rouge_l": 0.3189542483660131}
{"id": 456, "code": "def get pplan ( self , topology Name , callback = None ) : if callback : self . pplan watchers [ topology Name ] . append ( callback ) else : pplan path = self . get pplan path ( topology Name ) with open ( pplan path ) as f : data = f . read ( ) pplan = Physical Plan ( ) pplan . Parse From String ( data ) return pplan", "predictions": ["reads the pplan and returns the pplan ."], "references": ["get physical plan of a topology"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 457, "code": "def create socket options ( ) : sys config = system config . get sys config ( ) opt list = [ const . INSTANCE NETWORK WRITE BATCH SIZE BYTES , const . INSTANCE NETWORK WRITE BATCH TIME MS , const . INSTANCE NETWORK READ BATCH SIZE BYTES , const . INSTANCE NETWORK READ BATCH TIME MS , const . INSTANCE NETWORK OPTIONS SOCKET RECEIVED BUFFER SIZE BYTES , const . INSTANCE NETWORK OPTIONS SOCKET SEND BUFFER SIZE BYTES ] Log . debug ( \"In create socket options()\" ) try : value lst = [ int ( sys config [ opt ] ) for opt in opt list ] sock opt = Socket Options ( * value lst ) return sock opt except Value Error as e : raise Value Error ( \"Invalid value in sys config: %s\" % str ( e ) ) except Key Error as e : raise Key Error ( \"Incomplete sys config: %s\" % str ( e ) )", "predictions": ["create socket options object"], "references": ["creates socketoptions object from a given sys_config dict"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 458, "code": "def class dict to specs ( mcs , class dict ) : specs = { } for name , spec in class dict . items ( ) : if isinstance ( spec , Heron Component Spec ) : if spec . name is None : spec . name = name if spec . name in specs : raise Value Error ( \"Duplicate component name: %s\" % spec . name ) else : specs [ spec . name ] = spec return specs", "predictions": ["transform a dictionary of class names into a dictionary of specs ."], "references": ["takes a class __dict__ and returns heroncomponentspec entries"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 459, "code": "def init topology ( mcs , classname , class dict ) : if classname == 'Topology' : return heron options = Topology Type . get heron options from env ( ) initial state = heron options . get ( \"cmdline.topology.initial.state\" , \"RUNNING\" ) tmp directory = heron options . get ( \"cmdline.topologydefn.tmpdirectory\" ) if tmp directory is None : raise Runtime Error ( \"Topology definition temp directory not specified\" ) topology name = heron options . get ( \"cmdline.topology.name\" , classname ) topology id = topology name + str ( uuid . uuid4 ( ) ) topology = topology pb2 . Topology ( ) topology . id = topology id topology . name = topology name topology . state = topology pb2 . Topology State . Value ( initial state ) topology . topology config . Copy From ( Topology Type . get topology config protobuf ( class dict ) ) Topology Type . add bolts and spouts ( topology , class dict ) class dict [ 'topology name' ] = topology name class dict [ 'topology id' ] = topology id class dict [ 'protobuf topology' ] = topology class dict [ 'topologydefn tmpdir' ] = tmp directory class dict [ 'heron runtime options' ] = heron options", "predictions": ["initialize a topology from a topology ."], "references": ["initializes a topology protobuf"], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 460, "code": "def add spout ( self , name , spout cls , par , config = None , optional outputs = None ) : spout spec = spout cls . spec ( name = name , par = par , config = config , optional outputs = optional outputs ) self . add spec ( spout spec ) return spout spec", "predictions": ["add a spout spec to the network ."], "references": ["add a spout to the topology"], "bleu": 0.3549481056010052, "rouge_l": 0.7331730769230769}
{"id": 461, "code": "def add bolt ( self , name , bolt cls , par , inputs , config = None , optional outputs = None ) : bolt spec = bolt cls . spec ( name = name , par = par , inputs = inputs , config = config , optional outputs = optional outputs ) self . add spec ( bolt spec ) return bolt spec", "predictions": ["add a bolt spec to solver ."], "references": ["add a bolt to the topology"], "bleu": 0.3655552228545123, "rouge_l": 0.6240409207161125}
{"id": 462, "code": "def build and submit ( self ) : class dict = self . construct topo class dict ( ) topo cls = Topology Type ( self . topology name , ( Topology , ) , class dict ) topo cls . write ( )", "predictions": ["build the topology and submit it to the topology file ."], "references": ["builds the topology and submits to the destination"], "bleu": 0.2521193618434983, "rouge_l": 0.5417406749555951}
{"id": 463, "code": "def queries map ( ) : qs = all metric queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )", "predictions": ["maps the queries to the queries to the values ."], "references": ["map from query parameter to query name"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 464, "code": "def get clusters ( ) : instance = tornado . ioloop . IO Loop . instance ( ) try : return instance . run sync ( lambda : API . get clusters ( ) ) except Exception : Log . debug ( traceback . format exc ( ) ) raise", "predictions": ["get all clusters from the instance ."], "references": ["synced api call to get all cluster names"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 465, "code": "def get logical plan ( cluster , env , topology , role ) : instance = tornado . ioloop . IO Loop . instance ( ) try : return instance . run sync ( lambda : API . get logical plan ( cluster , env , topology , role ) ) except Exception : Log . debug ( traceback . format exc ( ) ) raise", "predictions": ["get the logical plan plan for a given role ."], "references": ["synced api call to get logical plans"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 466, "code": "def get topology info ( * args ) : instance = tornado . ioloop . IO Loop . instance ( ) try : return instance . run sync ( lambda : API . get topology info ( * args ) ) except Exception : Log . debug ( traceback . format exc ( ) ) raise", "predictions": ["retrieves topology from topology"], "references": ["synced api call to get topology information"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 467, "code": "def get component metrics ( component , cluster , env , topology , role ) : all queries = metric queries ( ) try : result = get topology metrics ( cluster , env , topology , component , [ ] , all queries , [ 0 , - 1 ] , role ) return result [ \"metrics\" ] except Exception : Log . debug ( traceback . format exc ( ) ) raise", "predictions": ["list all metrics for a given component ."], "references": ["synced api call to get component metrics"], "bleu": 0.17747405280050269, "rouge_l": 0.13495575221238937}
{"id": 468, "code": "def get spout ( self ) : spout = topology pb2 . Spout ( ) spout . comp . Copy From ( self . get base component ( ) ) self . add out streams ( spout ) return spout", "predictions": ["create a new instance of the class"], "references": ["returns spout protobuf message"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 469, "code": "def get bolt ( self ) : bolt = topology pb2 . Bolt ( ) bolt . comp . Copy From ( self . get base component ( ) ) self . add in streams ( bolt ) self . add out streams ( bolt ) return bolt", "predictions": ["build a topology ."], "references": ["returns bolt protobuf message"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 470, "code": "def get base component ( self ) : comp = topology pb2 . Component ( ) comp . name = self . name comp . spec = topology pb2 . Component Object Spec . Value ( \"PYTHON CLASS NAME\" ) comp . class name = self . python class path comp . config . Copy From ( self . get comp config ( ) ) return comp", "predictions": ["return the base component for the topology"], "references": ["returns component protobuf message"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 471, "code": "def add in streams ( self , bolt ) : if self . inputs is None : return input dict = self . sanitize inputs ( ) for global streamid , gtype in input dict . items ( ) : in stream = bolt . inputs . add ( ) in stream . stream . Copy From ( self . get stream id ( global streamid . component id , global streamid . stream id ) ) if isinstance ( gtype , Grouping . FIELDS ) : in stream . gtype = gtype . gtype in stream . grouping fields . Copy From ( self . get stream schema ( gtype . fields ) ) elif isinstance ( gtype , Grouping . CUSTOM ) : in stream . gtype = gtype . gtype in stream . custom grouping object = gtype . python serialized in stream . type = topology pb2 . Custom Grouping Object Type . Value ( \"PYTHON OBJECT\" ) else : in stream . gtype = gtype", "predictions": ["add streams to stream ."], "references": ["adds inputs to a given protobuf bolt message"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 472, "code": "def add out streams ( self , spbl ) : if self . outputs is None : return output map = self . sanitize outputs ( ) for stream id , out fields in output map . items ( ) : out stream = spbl . outputs . add ( ) out stream . stream . Copy From ( self . get stream id ( self . name , stream id ) ) out stream . schema . Copy From ( self . get stream schema ( out fields ) )", "predictions": ["add out the output streams to the stream"], "references": ["adds outputs to a given protobuf bolt or spout message"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 473, "code": "def get out streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise Type Error ( \"Argument to outputs must be either list or tuple, given: %s\" % str ( type ( self . outputs ) ) ) ret lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise Type Error ( \"Outputs must be a list of strings or Streams, given: %s\" % str ( output ) ) ret lst . append ( Stream . DEFAULT STREAM ID if isinstance ( output , str ) else output . stream id ) return set ( ret lst )", "predictions": ["return a set of unique outputs for this stream ."], "references": ["returns a set of output stream ids registered for this component"], "bleu": 0.26238334426718246, "rouge_l": 0.47213622291021673}
{"id": 474, "code": "def get stream id ( comp name , stream id ) : proto stream id = topology pb2 . Stream Id ( ) proto stream id . id = stream id proto stream id . component name = comp name return proto stream id", "predictions": ["get a stream id from a stream"], "references": ["returns a streamid protobuf message"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 475, "code": "def get stream schema ( fields ) : stream schema = topology pb2 . Stream Schema ( ) for field in fields : key = stream schema . keys . add ( ) key . key = field key . type = topology pb2 . Type . Value ( \"OBJECT\" ) return stream schema", "predictions": ["return a stream schema object for the given fields ."], "references": ["returns a streamschema protobuf message"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 476, "code": "def register metric ( self , name , metric , time bucket in sec ) : collector = self . get metrics collector ( ) collector . register metric ( name , metric , time bucket in sec )", "predictions": ["register a metric as a metric"], "references": ["registers a new metric to this context"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 477, "code": "def get component tasks ( self , component id ) : ret = [ ] for task id , comp id in self . task to component map . items ( ) : if comp id == component id : ret . append ( task id ) return ret", "predictions": ["return all tasks for a given component id"], "references": ["returns the task ids allocated for the given component id"], "bleu": 0.24578832304224082, "rouge_l": 0.43571428571428567}
{"id": 478, "code": "def get metrics collector ( self ) : if self . metrics collector is None or not isinstance ( self . metrics collector , Metrics Collector ) : raise Runtime Error ( \"Metrics collector is not registered in this context\" ) return self . metrics collector", "predictions": ["return the metrics for the client ."], "references": ["returns this context s metrics collector"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 479, "code": "def setup ( self , context ) : myindex = context . get partition index ( ) self . files to consume = self . files [ myindex : : context . get num partitions ( ) ] self . logger . info ( \"Text File Spout files to consume %s\" % self . files to consume ) self . lines to consume = self . get next lines ( ) self . emit count = 0", "predictions": ["setup the partition to be run in the partition"], "references": ["implements textfile generator s setup method"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 480, "code": "def add verbose ( parser ) : parser . add argument ( '--verbose' , metavar = '(a boolean; default: \"false\")' , type = bool , default = False ) return parser", "predictions": ["add remote remote remote remote remote remote remote remote remote remote command line cl cl cl cl"], "references": ["add optional verbose argument"], "bleu": 0.07223943354597204, "rouge_l": 0.10720562390158171}
{"id": 481, "code": "def add tracker url ( parser ) : parser . add argument ( '--tracker url' , metavar = '(tracker url; default: \"' + DEFAULT TRACKER URL + '\")' , type = str , default = DEFAULT TRACKER URL ) return parser", "predictions": ["get hostname url url url"], "references": ["add optional tracker_url argument"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 482, "code": "def hex escape ( bin str ) : printable = string . ascii letters + string . digits + string . punctuation + ' ' return '' . join ( ch if ch in printable else r'0x{0:02x}' . format ( ord ( ch ) ) for ch in bin str )", "predictions": ["self for is a binary string for use with is self for is self for is self for is self for use for use with is self for use with is"], "references": ["hex encode a binary string"], "bleu": 0.07261813302549416, "rouge_l": 0.19162303664921465}
{"id": 483, "code": "def handle assignment message ( self , pplan ) : Log . debug ( \"In handle assignment message() of ST Stmgr Client, Physical Plan: \\n%s\" , str ( pplan ) ) self . heron instance cls . handle assignment msg ( pplan )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["called when new newinstanceassignmentmessage arrives"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 484, "code": "def send ( self , dispatcher ) : if self . sent complete : return sent = dispatcher . send ( self . to send ) self . to send = self . to send [ sent : ]", "predictions": ["show socket and show response"], "references": ["sends this outgoing packet to dispatcher s socket"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 485, "code": "def read ( self , dispatcher ) : try : if not self . is header read : to read = Heron Protocol . HEADER SIZE - len ( self . header ) self . header += dispatcher . recv ( to read ) if len ( self . header ) == Heron Protocol . HEADER SIZE : self . is header read = True else : Log . debug ( \"Header read incomplete; read %d bytes of header\" % len ( self . header ) ) return if self . is header read and not self . is complete : to read = self . get datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to read ) if len ( self . data ) == self . get datasize ( ) : self . is complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : Log . debug ( \"Try again error\" ) else : Log . debug ( \"Fatal error when reading Incoming Packet\" ) raise Runtime Error ( \"Fatal error occured in Incoming Packet.read()\" )", "predictions": ["show data from the socket"], "references": ["reads incoming data from asyncore . dispatcher"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 486, "code": "def generate ( ) : data bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID SIZE ) ) return REQID ( data bytes )", "predictions": ["show a random with a random"], "references": ["generates a random reqid for request"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 487, "code": "def yaml config reader ( config path ) : if not config path . endswith ( \".yaml\" ) : raise Value Error ( \"Config file not yaml\" ) with open ( config path , 'r' ) as f : config = yaml . load ( f ) return config", "predictions": ["load a pick unused unused unused bind bind to a file socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket"], "references": ["reads yaml config file and returns auto - typed config_dict"], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 488, "code": "def send buffered messages ( self ) : while not self . out stream . is empty ( ) and self . stmgr client . is registered : tuple set = self . out stream . poll ( ) if isinstance ( tuple set , tuple pb2 . Heron Tuple Set ) : tuple set . src task id = self . my pplan helper . my task id self . gateway metrics . update sent packet ( tuple set . Byte Size ( ) ) self . stmgr client . send message ( tuple set )", "predictions": ["get all buffered messages messages . . . . . . . . . . . . . . . ."], "references": ["send messages in out_stream to the stream manager"], "bleu": 0.05809665204409193, "rouge_l": 0.07503075030750307}
{"id": 489, "code": "def handle state change msg ( self , new helper ) : assert self . my pplan helper is not None assert self . my instance is not None and self . my instance . py class is not None if self . my pplan helper . get topology state ( ) != new helper . get topology state ( ) : self . my pplan helper = new helper if new helper . is topology running ( ) : if not self . is instance started : self . start instance if possible ( ) self . my instance . py class . invoke activate ( ) elif new helper . is topology paused ( ) : self . my instance . py class . invoke deactivate ( ) else : raise Runtime Error ( \"Unexpected Topology State update: %s\" % new helper . get topology state ( ) ) else : Log . info ( \"Topology state remains the same.\" )", "predictions": ["checks whether the socket is in the topology socket opt opt opt opt opt opt opt opt"], "references": ["called when state change is commanded by stream manager"], "bleu": 0.07223943354597204, "rouge_l": 0.0814419225634179}
{"id": 490, "code": "def get topology config ( self ) : if self . pplan . topology . Has Field ( \"topology config\" ) : return self . get dict from config ( self . pplan . topology . topology config ) else : return { }", "predictions": ["class method to class to class to class to get dict . . ."], "references": ["returns the topology config"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 491, "code": "def set topology context ( self , metrics collector ) : Log . debug ( \"Setting topology context\" ) cluster config = self . get topology config ( ) cluster config . update ( self . get dict from config ( self . my component . config ) ) task to component map = self . get task to comp map ( ) self . context = Topology Context Impl ( cluster config , self . pplan . topology , task to component map , self . my task id , metrics collector , self . topology pex abs path )", "predictions": ["init the topology context for the topology"], "references": ["sets a new topology context"], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 492, "code": "def setup custom grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in stream in topology . bolts [ i ] . inputs : if in stream . stream . component name == self . my component name and in stream . gtype == topology pb2 . Grouping . Value ( \"CUSTOM\" ) : if in stream . type == topology pb2 . Custom Grouping Object Type . Value ( \"PYTHON OBJECT\" ) : custom grouping obj = default serializer . deserialize ( in stream . custom grouping object ) if isinstance ( custom grouping obj , str ) : pex loader . load pex ( self . topology pex abs path ) grouping cls = pex loader . import and get class ( self . topology pex abs path , custom grouping obj ) custom grouping obj = grouping cls ( ) assert isinstance ( custom grouping obj , I Custom Grouping ) self . custom grouper . add ( in stream . stream . id , self . get taskids for component ( topology . bolts [ i ] . comp . name ) , custom grouping obj , self . my component name ) elif in stream . type == topology pb2 . Custom Grouping Object Type . Value ( \"JAVA OBJECT\" ) : raise Not Implemented Error ( \"Java-serialized custom grouping is not yet supported \" \"for python topology\" ) else : raise Value Error ( \"Unrecognized custom grouping type found: %s\" % str ( in stream . type ) )", "predictions": ["sets up the spout ."], "references": ["checks whether there are any bolts that consume any of my streams using custom grouping"], "bleu": 0.031069582040305195, "rouge_l": 0.0}
{"id": 493, "code": "def prepare ( self , context ) : for stream id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream id )", "predictions": ["add the given = list of = = = 0 to the inputs"], "references": ["prepares the custom grouping for this component"], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 494, "code": "def choose tasks ( self , stream id , values ) : if stream id not in self . targets : return [ ] ret = [ ] for target in self . targets [ stream id ] : ret . extend ( target . choose tasks ( values ) ) return ret", "predictions": ["build a list of and return objects for a given stream"], "references": ["choose tasks for a given stream_id and values and returns a list of target tasks"], "bleu": 0.22037683178534834, "rouge_l": 0.22453987730061348}
{"id": 495, "code": "def format mode ( sres ) : mode = sres . st mode root = ( mode & 0o700 ) >> 6 group = ( mode & 0o070 ) >> 3 user = ( mode & 0o7 ) def stat type ( md ) : ''' stat type''' if stat . S ISDIR ( md ) : return 'd' elif stat . S ISSOCK ( md ) : return 's' else : return '-' def triple ( md ) : ''' triple ''' return '%c%c%c' % ( 'r' if md & 0b100 else '-' , 'w' if md & 0b010 else '-' , 'x' if md & 0b001 else '-' ) return '' . join ( [ stat type ( mode ) , triple ( root ) , triple ( group ) , triple ( user ) ] )", "predictions": ["queries a given map for display"], "references": ["format a line in the directory list based on the file s type and other attributes ."], "bleu": 0.035316782215334214, "rouge_l": 0.0800524934383202}
{"id": 496, "code": "def format mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )", "predictions": ["get a formatted string for the given clusters ."], "references": ["format the date associated with a file to be displayed in directory listing ."], "bleu": 0.09630141125179911, "rouge_l": 0.1673525377229081}
{"id": 497, "code": "def read chunk ( filename , offset = - 1 , length = - 1 , escape data = False ) : try : length = int ( length ) offset = int ( offset ) except Value Error : return { } if not os . path . isfile ( filename ) : return { } try : fstat = os . stat ( filename ) except Exception : return { } if offset == - 1 : offset = fstat . st size if length == - 1 : length = fstat . st size - offset with open ( filename , \"r\" ) as fp : fp . seek ( offset ) try : data = fp . read ( length ) except IO Error : return { } if data : data = escape data ( data ) if escape data else data return dict ( offset = offset , length = len ( data ) , data = data ) return dict ( offset = offset , length = 0 )", "predictions": ["get the instance of a file"], "references": ["read a chunk of a file from an offset upto the length ."], "bleu": 0.13537348102663535, "rouge_l": 0.29611650485436897}
{"id": 498, "code": "def str cmd ( cmd , cwd , env ) : process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) stdout builder , stderr builder = proc . async stdout stderr builder ( process ) process . wait ( ) stdout , stderr = stdout builder . result ( ) , stderr builder . result ( ) return { 'command' : ' ' . join ( cmd ) , 'stderr' : stderr , 'stdout' : stdout }", "predictions": ["run a command on the system ."], "references": ["runs the command and returns its stdout and stderr ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 499, "code": "def to table ( metrics ) : all queries = tracker access . metric queries ( ) m = tracker access . queries map ( ) names = metrics . values ( ) [ 0 ] . keys ( ) stats = [ ] for n in names : info = [ n ] for field in all queries : try : info . append ( str ( metrics [ field ] [ n ] ) ) except Key Error : pass stats . append ( info ) header = [ 'container id' ] + [ m [ k ] for k in all queries if k in metrics . keys ( ) ] return stats , header", "predictions": ["serialize a list of metrics metrics to a component"], "references": ["normalize raw metrics api result to table"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 500, "code": "def Match ( pattern , s ) : if pattern not in regexp compile cache : regexp compile cache [ pattern ] = sre compile . compile ( pattern ) return regexp compile cache [ pattern ] . match ( s )", "predictions": ["comp pattern function for regexp out of s out of s out out out out of s out out out out out out of s out of s out of s"], "references": ["matches the string with the pattern caching the compiled regexp ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 501, "code": "def Search ( pattern , s ) : if pattern not in regexp compile cache : regexp compile cache [ pattern ] = sre compile . compile ( pattern ) return regexp compile cache [ pattern ] . search ( s )", "predictions": ["transform a string into a cache instance in s in s in s in s in s in s in s in s in s in s in s in s"], "references": ["searches the string for the pattern caching the compiled regexp ."], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 502, "code": "def Find Next Multi Line Comment Start ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : return lineix lineix += 1 return len ( lines )", "predictions": ["get the first line from the provided comp string ."], "references": ["find the beginning marker for a multiline comment ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 503, "code": "def Find Next Multi Line Comment End ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . endswith ( '*/' ) : return lineix lineix += 1 return len ( lines )", "predictions": ["get first line of lines from a string ."], "references": ["we are inside a comment find the end marker ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 504, "code": "def Remove Multi Line Comments From Range ( lines , begin , end ) : for i in range ( begin , end ) : lines [ i ] = '/**/'", "predictions": ["makes a list of lines to a given number of lines ."], "references": ["clears a range of lines for multi - line comments ."], "bleu": 0.15537125692760353, "rouge_l": 0.3505747126436781}
{"id": 505, "code": "def Check For Copyright ( filename , lines , error ) : for line in range ( 1 , min ( len ( lines ) , 11 ) ) : if re . search ( r'Copyright' , lines [ line ] , re . I ) : break else : error ( filename , 0 , 'legal/copyright' , 5 , 'No copyright message found.  ' 'You should have a line: \"Copyright [year] <Copyright Owner>\"' )", "predictions": ["check that str type is str . . . . . . . . . . ."], "references": ["logs an error if no copyright message appears at the top of the file ."], "bleu": 0.07223943354597204, "rouge_l": 0.06321243523316063}
{"id": 506, "code": "def Check Header File Included ( filename , include state , error ) : fileinfo = File Info ( filename ) if Search ( TEST FILE SUFFIX , fileinfo . Base Name ( ) ) : return for ext in Get Header Extensions ( ) : basefilename = filename [ 0 : len ( filename ) - len ( fileinfo . Extension ( ) ) ] headerfile = basefilename + '.' + ext if not os . path . exists ( headerfile ) : continue headername = File Info ( headerfile ) . Repository Name ( ) first include = None for section list in include state . include list : for f in section list : if headername in f [ 0 ] or f [ 0 ] in headername : return if not first include : first include = f [ 1 ] error ( filename , first include , 'build/include' , 5 , '%s should include its header file %s' % ( fileinfo . Repository Name ( ) , headername ) )", "predictions": ["checks whether a number of proto files exist . . . ."], "references": ["logs an error if a source file does not include its header ."], "bleu": 0.10579369505074822, "rouge_l": 0.15885416666666669}
{"id": 507, "code": "def Set Verbose Level ( self , level ) : last verbose level = self . verbose level self . verbose level = level return last verbose level", "predictions": ["set the . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["sets the module s verbosity and returns the previous setting ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 508, "code": "def Add Filters ( self , filters ) : for filt in filters . split ( ',' ) : clean filt = filt . strip ( ) if clean filt : self . filters . append ( clean filt ) for filt in self . filters : if not ( filt . startswith ( '+' ) or filt . startswith ( '-' ) ) : raise Value Error ( 'Every filter in --filters must start with + or -' ' (%s does not)' % filt )", "predictions": ["ensure that the name of the name is a valid name . . . . . . . . ."], "references": ["adds more filters to the existing list of error - message filters ."], "bleu": 0.07264339766175722, "rouge_l": 0.18904958677685949}
{"id": 509, "code": "def Increment Error Count ( self , category ) : self . error count += 1 if self . counting in ( 'toplevel' , 'detailed' ) : if self . counting != 'detailed' : category = category . split ( '/' ) [ 0 ] if category not in self . errors by category : self . errors by category [ category ] = 0 self . errors by category [ category ] += 1", "predictions": ["update errors errors with errors ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret ret"], "references": ["bumps the module s error statistic ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 510, "code": "def Print Error Counts ( self ) : for category , count in sorted ( iteritems ( self . errors by category ) ) : self . Print Info ( 'Category \\'%s\\' errors found: %d\\n' % ( category , count ) ) if self . error count > 0 : self . Print Info ( 'Total errors found: %d\\n' % self . error count )", "predictions": ["prints the number of errors errors . . . ."], "references": ["print a summary of errors by category and the total ."], "bleu": 0.1705647399369684, "rouge_l": 0.28328173374613}
{"id": 511, "code": "def Check End ( self , filename , clean lines , linenum , error ) : line = clean lines . raw lines [ linenum ] # if ( linenum - self . starting linenum < 10 and not Match ( r'^\\s*};*\\s*(//|/\\*).*\\bnamespace\\b' , line ) ) : return # # # if self . name : if not Match ( ( r'^\\s*};*\\s*(//|/\\*).*\\bnamespace\\s+' + re . escape ( self . name ) + r'[\\*/\\.\\\\\\s]*$' ) , line ) : error ( filename , linenum , 'readability/namespace' , 5 , 'Namespace should be terminated with \"// namespace %s\"' % self . name ) else : if not Match ( r'^\\s*};*\\s*(//|/\\*).*\\bnamespace[\\*/\\.\\\\\\s]*$' , line ) : if Match ( r'^\\s*}.*\\b(namespace anonymous|anonymous namespace)\\b' , line ) : error ( filename , linenum , 'readability/namespace' , 5 , 'Anonymous namespace should be terminated with \"// namespace\"' ' or \"// anonymous namespace\"' ) else : error ( filename , linenum , 'readability/namespace' , 5 , 'Anonymous namespace should be terminated with \"// namespace\"' )", "predictions": ["check for common namespace namespace namespace namespace namespace to avoid namespace ."], "references": ["check end of namespace comments ."], "bleu": 0.1235622127262679, "rouge_l": 0.3546511627906977}
{"id": 512, "code": "def map ( self , map function ) : from heronpy . streamlet . impl . mapbolt import Map Streamlet map streamlet = Map Streamlet ( map function , self ) self . add child ( map streamlet ) return map streamlet", "predictions": ["make a map of a function ."], "references": ["return a new streamlet by applying map_function to each element of this streamlet ."], "bleu": 0.0812630644213965, "rouge_l": 0.2695139911634757}
{"id": 513, "code": "def filter ( self , filter function ) : from heronpy . streamlet . impl . filterbolt import Filter Streamlet filter streamlet = Filter Streamlet ( filter function , self ) self . add child ( filter streamlet ) return filter streamlet", "predictions": ["perform a filter filter on the heronpy"], "references": ["return a new streamlet containing only the elements that satisfy filter_function"], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 514, "code": "def union ( self , other streamlet ) : from heronpy . streamlet . impl . unionbolt import Union Streamlet union streamlet = Union Streamlet ( self , other streamlet ) self . add child ( union streamlet ) other streamlet . add child ( union streamlet ) return union streamlet", "predictions": ["create a union with the given other streamlet ."], "references": ["returns a new streamlet that consists of elements of both this and other_streamlet"], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 515, "code": "def log ( self ) : from heronpy . streamlet . impl . logbolt import Log Streamlet log streamlet = Log Streamlet ( self ) self . add child ( log streamlet ) return", "predictions": ["log a child process ."], "references": ["logs all elements of this streamlet . this returns nothing"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 516, "code": "def consume ( self , consume function ) : from heronpy . streamlet . impl . consumebolt import Consume Streamlet consume streamlet = Consume Streamlet ( consume function , self ) self . add child ( consume streamlet ) return", "predictions": ["consume a function at the given position ."], "references": ["calls consume_function for each element of this streamlet . this function returns nothing"], "bleu": 0.09499501502705178, "rouge_l": 0.09131736526946108}
{"id": 517, "code": "def join ( self , join streamlet , window config , join function ) : from heronpy . streamlet . impl . joinbolt import Join Streamlet , Join Bolt join streamlet result = Join Streamlet ( Join Bolt . INNER , window config , join function , self , join streamlet ) self . add child ( join streamlet result ) join streamlet . add child ( join streamlet result ) return join streamlet result", "predictions": ["join a single window"], "references": ["return a new streamlet by joining join_streamlet with this streamlet"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 518, "code": "def outer right join ( self , join streamlet , window config , join function ) : from heronpy . streamlet . impl . joinbolt import Join Streamlet , Join Bolt join streamlet result = Join Streamlet ( Join Bolt . OUTER RIGHT , window config , join function , self , join streamlet ) self . add child ( join streamlet result ) join streamlet . add child ( join streamlet result ) return join streamlet result", "predictions": ["cuts a block of tasks with a different window ."], "references": ["return a new streamlet by outer right join_streamlet with this streamlet"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 519, "code": "def outer left join ( self , join streamlet , window config , join function ) : from heronpy . streamlet . impl . joinbolt import Join Streamlet , Join Bolt join streamlet result = Join Streamlet ( Join Bolt . OUTER LEFT , window config , join function , self , join streamlet ) self . add child ( join streamlet result ) join streamlet . add child ( join streamlet result ) return join streamlet result", "predictions": ["cuts cuts join ."], "references": ["return a new streamlet by left join_streamlet with this streamlet"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 520, "code": "def outer join ( self , join streamlet , window config , join function ) : from heronpy . streamlet . impl . joinbolt import Join Streamlet , Join Bolt join streamlet result = Join Streamlet ( Join Bolt . OUTER , window config , join function , self , join streamlet ) self . add child ( join streamlet result ) join streamlet . add child ( join streamlet result ) return join streamlet result", "predictions": ["block until a child is started"], "references": ["return a new streamlet by outer join_streamlet with this streamlet"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 521, "code": "def expand args ( command ) : if isinstance ( command , ( str , unicode ) ) : splitter = shlex . shlex ( command . encode ( 'utf-8' ) ) splitter . whitespace = '|' splitter . whitespace split = True command = [ ] while True : token = splitter . get token ( ) if token : command . append ( token ) else : break command = list ( map ( shlex . split , command ) ) return command", "predictions": ["expand all the arguments into a single command ."], "references": ["parses command strings and returns a popen - ready list ."], "bleu": 0.1343994460963362, "rouge_l": 0.19645732689210954}
{"id": 522, "code": "def connect ( command , data = None , env = None , cwd = None ) : command str = expand args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command str , universal newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return Connected Command ( process = process )", "predictions": ["connect a command to a subprocess ."], "references": ["spawns a new process from the given command ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 523, "code": "def send ( self , str , end = '\\n' ) : return self . process . stdin . write ( str + end )", "predictions": ["send message to client ."], "references": ["sends a line to std_in ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 524, "code": "def Js ( val , Clamped = False ) : if isinstance ( val , Py Js ) : return val elif val is None : return undefined elif isinstance ( val , basestring ) : return Py Js String ( val , String Prototype ) elif isinstance ( val , bool ) : return true if val else false elif isinstance ( val , float ) or isinstance ( val , int ) or isinstance ( val , long ) or ( NUMPY AVAILABLE and isinstance ( val , ( numpy . int8 , numpy . uint8 , numpy . int16 , numpy . uint16 , numpy . int32 , numpy . uint32 , numpy . float32 , numpy . float64 ) ) ) : if val in NUM BANK : return NUM BANK [ val ] return Py Js Number ( float ( val ) , Number Prototype ) elif isinstance ( val , Function Type ) : return Py Js Function ( val , Function Prototype ) #elif isinstance(val, Module Type): #elif isintance(val, Class Type): elif isinstance ( val , dict ) : temp = Py Js Object ( { } , Object Prototype ) for k , v in six . iteritems ( val ) : temp . put ( Js ( k ) , Js ( v ) ) return temp elif isinstance ( val , ( list , tuple ) ) : #Convert to array return Py Js Array ( val , Array Prototype ) elif isinstance ( val , Js Object Wrapper ) : return val . dict [ ' obj' ] elif NUMPY AVAILABLE and isinstance ( val , numpy . ndarray ) : if val . dtype == numpy . int8 : return Py Js Int8Array ( val , Int8Array Prototype ) elif val . dtype == numpy . uint8 and not Clamped : return Py Js Uint8Array ( val , Uint8Array Prototype ) elif val . dtype == numpy . uint8 and Clamped : return Py Js Uint8Clamped Array ( val , Uint8Clamped Array Prototype ) elif val . dtype == numpy . int16 : return Py Js Int16Array ( val , Int16Array Prototype ) elif val . dtype == numpy . uint16 : return Py Js Uint16Array ( val , Uint16Array Prototype ) elif val . dtype == numpy . int32 : return Py Js Int32Array ( val , Int32Array Prototype ) elif val . dtype == numpy . uint32 : return Py Js Uint16Array ( val , Uint32Array Prototype ) elif val . dtype == numpy . float32 : return Py Js Float32Array ( val , Float32Array Prototype ) elif val . dtype == numpy . float64 : return Py Js Float64Array ( val , Float64Array Prototype ) else : return py wrap ( val )", "predictions": ["converts a value to a numpy array"], "references": ["converts py type to pyjs type"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 525, "code": "def set name ( self , name ) : if self . own . get ( 'name' ) : self . func name = name self . own [ 'name' ] [ 'value' ] = Js ( name )", "predictions": ["set a name of a variable"], "references": ["name is py type"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 526, "code": "def Construct Array ( self , py arr ) : arr = self . New Array ( len ( py arr ) ) arr . init ( py arr ) return arr", "predictions": ["read a short array of strings as an array of strings ."], "references": ["note py_arr elems are not converted to pyjs types!"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 527, "code": "def Construct Object ( self , py obj ) : obj = self . New Object ( ) for k , v in py obj . items ( ) : obj . put ( unicode ( k ) , v ) return obj", "predictions": ["read object as unicode object ."], "references": ["note py_obj items are not converted to pyjs types!"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 528, "code": "def emit ( self , op code , * args ) : self . tape . append ( OP CODES [ op code ] ( * args ) )", "predictions": ["emits a python operator"], "references": ["adds op_code with specified args to tape"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 529, "code": "def compile ( self , start loc = 0 ) : self . label locs = { } if self . label locs is None else self . label locs loc = start loc while loc < len ( self . tape ) : if type ( self . tape [ loc ] ) == LABEL : self . label locs [ self . tape [ loc ] . num ] = loc del self . tape [ loc ] continue loc += 1 self . compiled = True", "predictions": ["compile the label and label from a dictionary ."], "references": ["records locations of labels and compiles the code"], "bleu": 0.15619699684601276, "rouge_l": 0.1189083820662768}
{"id": 530, "code": "def pad ( num , n = 2 , sign = False ) : s = unicode ( abs ( num ) ) if len ( s ) < n : s = '0' * ( n - len ( s ) ) + s if not sign : return s if num >= 0 : return '+' + s else : return '-' + s", "predictions": ["pad n bytes to n chars"], "references": ["returns n digit string representation of the num"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 531, "code": "def replacement template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\\'' : res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) if not num or num > len ( npar ) : res += '$' + dig else : res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res", "predictions": ["return a replacement template for a given source"], "references": ["takes the replacement template and some info about the match and returns filled template"], "bleu": 0.09969456998702547, "rouge_l": 0.1732954545454545}
{"id": 532, "code": "def emit ( self , what , * args ) : if isinstance ( what , basestring ) : return self . exe . emit ( what , * args ) elif isinstance ( what , list ) : self . emit statement list ( what ) else : return getattr ( self , what [ 'type' ] ) ( * * what )", "predictions": ["emits a single exe list ."], "references": ["what can be either name of the op or node or a list of statements ."], "bleu": 0.04961591899093189, "rouge_l": 0.25206611570247933}
{"id": 533, "code": "def to key ( literal or identifier ) : if literal or identifier [ 'type' ] == 'Identifier' : return literal or identifier [ 'name' ] elif literal or identifier [ 'type' ] == 'Literal' : k = literal or identifier [ 'value' ] if isinstance ( k , float ) : return unicode ( float repr ( k ) ) elif 'regex' in literal or identifier : return compose regex ( k ) elif isinstance ( k , bool ) : return 'true' if k else 'false' elif k is None : return 'null' else : return unicode ( k )", "predictions": ["convert a literal literal to a key or key ."], "references": ["returns string representation of this object"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 534, "code": "def trans ( ele , standard = False ) : try : node = globals ( ) . get ( ele [ 'type' ] ) if not node : raise Not Implemented Error ( '%s is not supported!' % ele [ 'type' ] ) if standard : node = node . dict [ 'standard' ] if 'standard' in node . dict else node return node ( * * ele ) except : #print ele raise", "predictions": ["resolve a node from a member of its type ."], "references": ["translates esprima syntax tree to python by delegating to appropriate translating node"], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 535, "code": "def is lval ( t ) : if not t : return False i = iter ( t ) if i . next ( ) not in IDENTIFIER START : return False return all ( e in IDENTIFIER PART for e in i )", "predictions": ["return true if t is a lval ."], "references": ["does not chceck whether t is not resticted or internal"], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 536, "code": "def eval ( self , expression , use compilation plan = False ) : code = 'Py Js Eval Result = eval(%s)' % json . dumps ( expression ) self . execute ( code , use compilation plan = use compilation plan ) return self [ 'Py Js Eval Result' ]", "predictions": ["executes a compilation expression ."], "references": ["evaluates expression in current context and returns its value"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 537, "code": "def to key ( literal or identifier ) : if literal or identifier [ 'type' ] == 'Identifier' : return literal or identifier [ 'name' ] elif literal or identifier [ 'type' ] == 'Literal' : k = literal or identifier [ 'value' ] if isinstance ( k , float ) : return unicode ( float repr ( k ) ) elif 'regex' in literal or identifier : return compose regex ( k ) elif isinstance ( k , bool ) : return u'true' if k else u'false' elif k is None : return u'null' else : return unicode ( k )", "predictions": ["convert a literal literal to a key or key ."], "references": ["returns string representation of this object"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 538, "code": "def to arr ( this ) : return [ this . get ( str ( e ) ) for e in xrange ( len ( this ) ) ]", "predictions": ["convert a list of key - value pairs to a list of key - tuples"], "references": ["returns python array from js array"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 539, "code": "def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )", "predictions": ["match string at pos ."], "references": ["string is of course a py string"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 540, "code": "def call ( self , this , args = ( ) ) : if self . is native : args = Space Tuple ( args ) args . space = self . space return self . code ( this , args ) else : return self . space . exe . call ( self , this , args )", "predictions": ["call a command ."], "references": ["dont use this method from inside bytecode to call other bytecode ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 541, "code": "def is empty object ( n , last ) : if n . strip ( ) : return False last = last . strip ( ) markers = { ')' , ';' , } if not last or last [ - 1 ] in markers : return False return True", "predictions": ["check if an object is empty"], "references": ["n may be the inside of block or object"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 542, "code": "def parse exponent ( source , start ) : if not source [ start ] in { 'e' , 'E' } : if source [ start ] in IDENTIFIER PART : raise Syntax Error ( 'Invalid number literal!' ) return start start += 1 if source [ start ] in { '-' , '+' } : start += 1 FOUND = False while source [ start ] in NUMS : FOUND = True start += 1 if not FOUND or source [ start ] in IDENTIFIER PART : raise Syntax Error ( 'Invalid number literal!' ) return start", "predictions": ["parse a exponent from a source file ."], "references": ["returns end of exponential raises syntaxerror if failed"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 543, "code": "def in op ( self , other ) : if not is object ( other ) : raise Make Error ( 'Type Error' , \"You can\\'t use 'in' operator to search in non-objects\" ) return other . has property ( to string ( self ) )", "predictions": ["returns if two objects are equal to this object ."], "references": ["checks if self is in other"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 544, "code": "def maybe download and extract ( ) : dest directory = '.' filename = DATA URL . split ( '/' ) [ - 1 ] filepath = os . path . join ( dest directory , filename ) if not os . path . exists ( filepath ) : def progress ( count , block size , total size ) : sys . stdout . write ( '\\r>> Downloading %s %.1f%%' % ( filename , float ( count * block size ) / float ( total size ) * 100.0 ) ) sys . stdout . flush ( ) filepath , = urllib . request . urlretrieve ( DATA URL , filepath , progress ) print ( ) statinfo = os . stat ( filepath ) print ( 'Successfully downloaded' , filename , statinfo . st size , 'bytes.' ) extracted dir path = os . path . join ( dest directory , 'trees' ) if not os . path . exists ( extracted dir path ) : zip ref = zipfile . Zip File ( filepath , 'r' ) zip ref . extractall ( dest directory ) zip ref . close ( )", "predictions": ["download zip self . self . self . . and extract . . to . . . . . . ."], "references": ["download and extract processed data and embeddings ."], "bleu": 0.08687475782716618, "rouge_l": 0.3001230012300123}
{"id": 545, "code": "def pythonize arguments ( arg str ) : out args = [ ] if arg str is None : return out str args = arg str . split ( ',' ) for arg in args : components = arg . split ( '=' ) name and type = components [ 0 ] . split ( ' ' ) if name and type [ - 1 ] == '' and len ( name and type ) > 1 : name = name and type [ - 2 ] else : name = name and type [ - 1 ] if len ( components ) > 1 : name += '=' + components [ 1 ] out args . append ( name ) return ',' . join ( out args )", "predictions": ["turn a module string into a list of arguments arguments"], "references": ["remove types from function arguments in cython"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 546, "code": "def correspond ( text ) : if text : subproc . stdin . write ( text ) subproc . stdin . flush ( ) return get lines ( )", "predictions": ["put self . into from from from from from from from from from from from from from from from from from from from from from from from from from from from"], "references": ["communicate with the child process without closing stdin ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 547, "code": "def access ok ( self , access ) : for c in access : if c not in self . perms : return False return True", "predictions": ["= true if the given argument is ok"], "references": ["check if there is enough permissions for access"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 548, "code": "def in range ( self , index ) : if isinstance ( index , slice ) : in range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in range = index >= self . start and index <= self . end return in range", "predictions": ["return is a range for a given index"], "references": ["returns true if index is in range"], "bleu": 0.19070828081828378, "rouge_l": 0.26991150442477874}
{"id": 549, "code": "def execute ( self ) : if issymbolic ( self . PC ) : raise Concretize Register ( self , 'PC' , policy = 'ALL' ) if not self . memory . access ok ( self . PC , 'x' ) : raise Invalid Memory Access ( self . PC , 'x' ) self . publish ( 'will decode instruction' , self . PC ) insn = self . decode instruction ( self . PC ) self . last pc = self . PC self . publish ( 'will execute instruction' , self . PC , insn ) if insn . address != self . PC : return name = self . canonicalize instruction name ( insn ) if logger . level == logging . DEBUG : logger . debug ( self . render instruction ( insn ) ) for l in self . render registers ( ) : register logger . debug ( l ) try : if self . concrete and 'SYSCALL' in name : self . emu . sync unicorn to manticore ( ) if self . concrete and 'SYSCALL' not in name : self . emulate ( insn ) if self . PC == self . break unicorn at : logger . debug ( \"Switching from Unicorn to Manticore\" ) self . break unicorn at = None self . concrete = False else : implementation = getattr ( self , name , None ) if implementation is not None : implementation ( * insn . operands ) else : text bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . warning ( \"Unimplemented instruction: 0x%016x:\\t%s\\t%s\\t%s\" , insn . address , text bytes , insn . mnemonic , insn . op str ) self . backup emulate ( insn ) except ( Interruption , Syscall ) as e : e . on handled = lambda : self . publish instruction as executed ( insn ) raise e else : self . publish instruction as executed ( insn )", "predictions": ["join the unicorn function"], "references": ["decode and execute one instruction pointed by register pc"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 550, "code": "def publish instruction as executed ( self , insn ) : self . icount += 1 self . publish ( 'did execute instruction' , self . last pc , self . PC , insn )", "predictions": ["outer an right right join join join join"], "references": ["notify listeners that an instruction has been executed ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 551, "code": "def viz trace ( view ) : tv = Trace Visualizer ( view , None ) if tv . workspace is None : tv . workspace = get workspace ( ) tv . visualize ( )", "predictions": ["show the streamlet left left left or ."], "references": ["given a manticore trace file highlight the basic blocks ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 552, "code": "def viz live trace ( view ) : tv = Trace Visualizer ( view , None , live = True ) if tv . workspace is None : tv . workspace = get workspace ( ) tv . live update = True tv . visualize ( )", "predictions": ["pre - outer config trace"], "references": ["given a manticore trace file highlight the basic blocks ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 553, "code": "def visualize ( self ) : if os . path . isfile ( self . workspace ) : t = threading . Thread ( target = self . highlight from file , args = ( self . workspace , ) ) elif os . path . isdir ( self . workspace ) : t = threading . Thread ( target = self . highlight from dir , args = ( self . workspace , ) ) t . start ( )", "predictions": ["expand the unicode to a unicode file . ."], "references": ["given a manticore workspace or trace file highlight the basic blocks ."], "bleu": 0.12716571564598603, "rouge_l": 0.2785388127853881}
{"id": 554, "code": "def invalidate cache ( cpu , address , size ) : cache = cpu . instruction cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]", "predictions": ["remove the cache of a cpu . . ."], "references": ["remove decoded instruction from instruction cache"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 555, "code": "def LJMP ( cpu , cs selector , target ) : logger . info ( \"LJMP: Jumping to: %r:%r\" , cs selector . read ( ) , target . read ( ) ) cpu . CS = cs selector . read ( ) cpu . PC = target . read ( )", "predictions": ["extract data from your own end of your end end return data return data return the page"], "references": ["we are just going to ignore the cs selector for now ."], "bleu": 0.07223943354597204, "rouge_l": 0.07117852975495916}
{"id": 556, "code": "def hook xfer mem ( self , uc , access , address , size , value , data ) : assert access in ( UC MEM WRITE , UC MEM READ , UC MEM FETCH ) if access == UC MEM WRITE : self . cpu . write int ( address , value , size * 8 ) elif access == UC MEM READ : value = self . cpu . read bytes ( address , size ) if address in self . should be written : return True self . should be written [ address ] = value self . should try again = True return False return True", "predictions": ["callback function to determine whether something modifying a chunk of memory false otherwise false false if not numpy false otherwise ."], "references": ["handle memory operations from unicorn ."], "bleu": 0.06429451441231726, "rouge_l": 0.16464237516869096}
{"id": 557, "code": "def hook unmapped ( self , uc , access , address , size , value , data ) : try : m = self . create emulated mapping ( uc , address ) except Memory Exception as e : self . to raise = e self . should try again = False return False self . should try again = True return False", "predictions": ["called when a set of get data for a given ."], "references": ["we hit an unmapped region ; map it into unicorn ."], "bleu": 0.11390778025531027, "rouge_l": 0.09090909090909091}
{"id": 558, "code": "def emulate ( self , instruction ) : while True : self . reset ( ) for base in self . should be mapped : size , perms = self . should be mapped [ base ] self . emu . mem map ( base , size , perms ) for address , values in self . should be written . items ( ) : for offset , byte in enumerate ( values , start = address ) : if issymbolic ( byte ) : from . . native . cpu . abstractcpu import Concretize Memory raise Concretize Memory ( self . cpu . memory , offset , 8 , \"Concretizing for emulation\" ) self . emu . mem write ( address , b'' . join ( values ) ) self . should try again = False self . step ( instruction ) if not self . should try again : break", "predictions": ["emulate a single instruction = { tablename } = 0 } = { % } = { % }"], "references": ["emulate a single instruction ."], "bleu": 0.17923344640485434, "rouge_l": 0.37251908396946565}
{"id": 559, "code": "def must be true ( self , constraints , expression ) -> bool : solutions = self . get all values ( constraints , expression , maxcnt = 2 , silent = True ) return solutions == [ True ]", "predictions": ["returns true if obj must must must are true v v v v v v v v v ."], "references": ["check if expression is true and that it can not be false with current constraints"], "bleu": 0.0712695567709093, "rouge_l": 0.12019704433497538}
{"id": 560, "code": "def minmax ( self , constraints , x , iters = 10000 ) : if issymbolic ( x ) : m = self . min ( constraints , x , iters ) M = self . max ( constraints , x , iters ) return m , M else : return x , x", "predictions": ["calculate the op for the image . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["returns the min and max possible values for x within given constraints"], "bleu": 0.04317900023606586, "rouge_l": 0.10107705053852528}
{"id": 561, "code": "def start proc ( self ) : assert ' proc' not in dir ( self ) or self . proc is None try : self . proc = Popen ( shlex . split ( self . command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal newlines = True ) except OS Error as e : print ( e , \"Probably too many cached expressions? visitors. cache...\" ) raise Z3Not Found Error for cfg in self . init : self . send ( cfg )", "predictions": ["compile the program as a self"], "references": ["spawns z3 solver process"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 562, "code": "def reset ( self , constraints = None ) : if self . proc is None : self . start proc ( ) else : if self . support reset : self . send ( \"(reset)\" ) for cfg in self . init : self . send ( cfg ) else : self . stop proc ( ) self . start proc ( ) if constraints is not None : self . send ( constraints )", "predictions": ["pad all n n elements"], "references": ["auxiliary method to reset the smtlib external solver to initial defaults"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 563, "code": "def recv ( self ) -> str : buf , left , right = self . readline and count ( ) bufl = [ buf ] while left != right : buf , l , r = self . readline and count ( ) bufl . append ( buf ) left += l right += r buf = '' . join ( bufl ) . strip ( ) logger . debug ( '<%s' , buf ) if '(error' in bufl [ 0 ] : raise Exception ( f\"Error in smtlib: {bufl[0]}\" ) return buf", "predictions": ["receives a message from the broker 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["reads the response from the solver"], "bleu": 0.0513487742994337, "rouge_l": 0.12310797174571139}
{"id": 564, "code": "def assert ( self , expression : Bool ) : assert isinstance ( expression , Bool ) smtlib = translate to smtlib ( expression ) self . send ( '(assert %s)' % smtlib )", "predictions": ["emit an what what is a if it s not defined ."], "references": ["auxiliary method to send an assert"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 565, "code": "def can be true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : self . reset ( constraints ) return self . is sat ( ) assert isinstance ( expression , Bool ) with constraints as temp cs : temp cs . add ( expression ) self . reset ( temp cs . to string ( related to = expression ) ) return self . is sat ( )", "predictions": ["check if constraints is true ."], "references": ["check if two potentially symbolic values can be equal"], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 566, "code": "def get all values ( self , constraints , expression , maxcnt = None , silent = False ) : if not isinstance ( expression , Expression ) : return [ expression ] assert isinstance ( constraints , Constraint Set ) assert isinstance ( expression , Expression ) expression = simplify ( expression ) if maxcnt is None : maxcnt = consts . maxsolutions with constraints as temp cs : if isinstance ( expression , Bool ) : var = temp cs . new bool ( ) elif isinstance ( expression , Bit Vec ) : var = temp cs . new bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = temp cs . new array ( index max = expression . index max , value bits = expression . value bits , taint = expression . taint ) . array else : raise Not Implemented Error ( f\"get all values only implemented for {type(expression)} expression type.\" ) temp cs . add ( var == expression ) self . reset ( temp cs . to string ( related to = var ) ) result = [ ] while self . is sat ( ) : value = self . getvalue ( var ) result . append ( value ) self . assert ( var != value ) if len ( result ) >= maxcnt : if silent : break else : raise Too Many Solutions ( result ) return result", "predictions": ["extract all values from a given expression"], "references": ["returns a list with all the possible values for the symbol x"], "bleu": 0.1081377510275021, "rouge_l": 0.20098846787479407}
{"id": 567, "code": "def get value ( self , constraints , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , ( Bool , Bit Vec , Array ) ) with constraints as temp cs : if isinstance ( expression , Bool ) : var = temp cs . new bool ( ) elif isinstance ( expression , Bit Vec ) : var = temp cs . new bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = [ ] result = [ ] for i in range ( expression . index max ) : subvar = temp cs . new bitvec ( expression . value bits ) var . append ( subvar ) temp cs . add ( subvar == simplify ( expression [ i ] ) ) self . reset ( temp cs ) if not self . is sat ( ) : raise Solver Error ( 'Model is not available' ) for i in range ( expression . index max ) : self . send ( '(get-value (%s))' % var [ i ] . name ) ret = self . recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) pattern , base = self . get value fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) result . append ( int ( value , base ) ) return bytes ( result ) temp cs . add ( var == expression ) self . reset ( temp cs ) if not self . is sat ( ) : raise Solver Error ( 'Model is not available' ) self . send ( '(get-value (%s))' % var . name ) ret = self . recv ( ) if not ( ret . startswith ( '((' ) and ret . endswith ( '))' ) ) : raise Solver Error ( 'SMTLIB error parsing response: %s' % ret ) if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] if isinstance ( expression , Bit Vec ) : pattern , base = self . get value fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise Not Implemented Error ( \"get value only implemented for Bool and Bit Vec\" )", "predictions": ["is the value of an if it is a complete value"], "references": ["ask the solver for one possible result of given expression using given set of constraints ."], "bleu": 0.08001467044102561, "rouge_l": 0.14336075205640422}
{"id": 568, "code": "def colored level name ( self , levelname ) : if self . colors disabled : return self . plain levelname format . format ( levelname ) else : return self . colored levelname format . format ( self . color map [ levelname ] , levelname )", "predictions": ["get the color self self . ."], "references": ["colors the logging level in the logging record"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 569, "code": "def all events ( cls ) : all evts = set ( ) for cls , evts in cls . all events . items ( ) : all evts . update ( evts ) return all evts", "predictions": ["return to all key key names == key == key == evts == . == == . == . == key == . == if there"], "references": ["return all events that all subclasses have so far registered to publish ."], "bleu": 0.058697608930387266, "rouge_l": 0.16368515205724513}
{"id": 570, "code": "def forward events to ( self , sink , include source = False ) : assert isinstance ( sink , Eventful ) , f'{sink. class . name } is not Eventful' self . forwards [ sink ] = include source", "predictions": ["wrapper for inserting arr arr"], "references": ["this forwards signal to sink"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 571, "code": "def context ( self ) : if self . context is not None : return self . context else : logger . warning ( \"Using shared context without a lock\" ) return self . executor . shared context", "predictions": ["pat match the match match match the match match ."], "references": ["convenient access to shared context"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 572, "code": "def locked context ( self , key = None , default = dict ) : keys = [ 'policy' ] if key is not None : keys . append ( key ) with self . executor . locked context ( '.' . join ( keys ) , default ) as policy context : yield policy context", "predictions": ["insert a context into a context instance exe exe exe exe exe exe exe exe exe"], "references": ["policy shared context dictionary"], "bleu": 0.07692375026049747, "rouge_l": 0.11213235294117647}
{"id": 573, "code": "def visited callback ( self , state , pc , instr ) : with self . locked context ( 'visited' , set ) as ctx : ctx . add ( pc )", "predictions": ["called when a user is received"], "references": ["maintain our own copy of the visited set"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 574, "code": "def visited callback ( self , state , pc , instr ) : pc = state . platform . current . PC with self . locked context ( 'visited' , dict ) as ctx : ctx [ pc ] = ctx . get ( pc , 0 ) + 1", "predictions": ["exponent is a context manager for a given start start"], "references": ["maintain our own copy of the visited set"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 575, "code": "def put ( self , state id ) : self . states . append ( state id ) self . lock . notify all ( ) return state id", "predictions": ["in a state in the pool if it has not already been set if not already ."], "references": ["enqueue it for processing"], "bleu": 0.07223943354597204, "rouge_l": 0.10720562390158171}
{"id": 576, "code": "def get ( self ) : if self . is shutdown ( ) : return None while len ( self . states ) == 0 : if self . running == 0 : return None if self . is shutdown ( ) : return None logger . debug ( \"Waiting for available states\" ) self . lock . wait ( ) state id = self . policy . choice ( list ( self . states ) ) if state id is None : return None del self . states [ self . states . index ( state id ) ] return state id", "predictions": ["get the state of the queue ."], "references": ["dequeue a state with the max priority"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 577, "code": "def run ( self ) : current state = None current state id = None with With Keyboard Interrupt As ( self . shutdown ) : self . notify start run ( ) logger . debug ( \"Starting Manticore Symbolic Emulator Worker (pid %d).\" , os . getpid ( ) ) solver = Z3Solver ( ) while not self . is shutdown ( ) : try : try : if current state is None : with self . lock : self . notify stop run ( ) try : current state id = self . get ( ) if current state id is not None : self . publish ( 'will load state' , current state id ) current state = self . workspace . load state ( current state id ) self . forward events from ( current state , True ) self . publish ( 'did load state' , current state , current state id ) logger . info ( \"load state %r\" , current state id ) finally : self . notify start run ( ) if current state is None : logger . debug ( \"No more states in the queue, byte bye!\" ) break assert current state is not None assert current state . constraints is current state . platform . constraints while not self . is shutdown ( ) : if not current state . execute ( ) : break else : self . publish ( 'will terminate state' , current state , current state id , Terminate State ( 'Shutdown' ) ) current state = None except Concretize as e : logger . debug ( \"Generic state fork on condition\" ) current state = self . fork ( current state , e . expression , e . policy , e . setstate ) except Terminate State as e : self . publish ( 'will terminate state' , current state , current state id , e ) logger . debug ( \"Generic terminate state\" ) if e . testcase : self . publish ( 'internal generate testcase' , current state , message = str ( e ) ) current state = None except Solver Error as e : import traceback trace = traceback . format exc ( ) logger . error ( \"Exception: %s\\n%s\" , str ( e ) , trace ) self . publish ( 'will terminate state' , current state , current state id , e ) if solver . check ( current state . constraints ) : self . publish ( 'internal generate testcase' , current state , message = \"Solver failed\" + str ( e ) ) current state = None except ( Exception , Assertion Error ) as e : import traceback trace = traceback . format exc ( ) logger . error ( \"Exception: %s\\n%s\" , str ( e ) , trace ) self . publish ( 'will terminate state' , current state , current state id , e ) current state = None assert current state is None or self . is shutdown ( ) self . notify stop run ( )", "predictions": ["run the load ."], "references": ["entry point of the executor ; called by workers to start analysis ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 578, "code": "def hook callback ( self , state , pc , instruction ) : if issymbolic ( pc ) : return for cb in self . hooks . get ( pc , [ ] ) : cb ( state ) for cb in self . hooks . get ( None , [ ] ) : cb ( state )", "predictions": ["called when a callback event occurs ."], "references": ["invoke all registered generic hooks"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 579, "code": "def get group ( name : str ) -> Group : global groups if name in groups : return groups [ name ] group = Group ( name ) groups [ name ] = group return group", "predictions": ["return a group given its name or group name"], "references": ["get a configuration variable group named |name|"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 580, "code": "def get description ( self , name : str ) -> str : if name not in self . vars : raise Config Error ( f\"{self.name}.{name} not defined.\" ) return self . vars [ name ] . description", "predictions": ["get the description for the given name ."], "references": ["return the description or a help string of variable identified by |name| ."], "bleu": 0.12139281957861149, "rouge_l": 0.2739520958083832}
{"id": 581, "code": "def correspond ( text ) : subproc . stdin . write ( text ) subproc . stdin . flush ( ) return drain ( )", "predictions": ["augment a string with the text of a single text ."], "references": ["communicate with the child process without closing stdin ."], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 582, "code": "def get constructor arguments ( self ) -> str : item = self . constructor abi item return '()' if item is None else self . tuple signature for components ( item [ 'inputs' ] )", "predictions": ["returns a list of constructor arguments that have a constructor ."], "references": ["returns the tuple type signature for the arguments of the contract constructor ."], "bleu": 0.1486375602900724, "rouge_l": 0.3283983849259758}
{"id": 583, "code": "def hashes ( self ) -> Tuple [ bytes , ... ] : selectors = self . function signatures by selector . keys ( ) return ( * selectors , self . fallback function selector )", "predictions": ["the hashes of the dataset ."], "references": ["the selectors of all normal contract functions plus self . fallback_function_selector ."], "bleu": 0.09663861439684919, "rouge_l": 0.31443298969072164}
{"id": 584, "code": "def map memory callback ( self , address , size , perms , name , offset , result ) : logger . info ( ' ' . join ( ( \"Mapping Memory @\" , hex ( address ) if type ( address ) is int else \"0x??\" , hr size ( size ) , \"-\" , perms , \"-\" , f\"{name}:{hex(offset) if name else ''}\" , \"->\" , hex ( result ) ) ) ) self . emu . mem map ( address , size , convert permissions ( perms ) ) self . copy memory ( address , size )", "predictions": ["send memory callback for a service ."], "references": ["catches did_map_memory and copies the mapping into manticore"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 585, "code": "def unmap memory callback ( self , start , size ) : logger . info ( f\"Unmapping memory from {hex(start)} to {hex(start + size)}\" ) mask = ( 1 << 12 ) - 1 if ( start & mask ) != 0 : logger . error ( \"Memory to be unmapped is not aligned to a page\" ) if ( size & mask ) != 0 : size = ( ( size >> 12 ) + 1 ) << 12 logger . warning ( \"Forcing unmap size to align to a page\" ) self . emu . mem unmap ( start , size )", "predictions": ["internal method to unmap a memory callback for a specific memory ."], "references": ["unmap unicorn maps when manticore unmaps them"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 586, "code": "def protect memory callback ( self , start , size , perms ) : logger . info ( f\"Changing permissions on {hex(start)}:{hex(start + size)} to {perms}\" ) self . emu . mem protect ( start , size , convert permissions ( perms ) )", "predictions": ["called when memory callback is protect"], "references": ["set memory protections in unicorn correctly"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 587, "code": "def hook syscall ( self , uc , data ) : logger . debug ( f\"Stopping emulation at {hex(uc.reg read(self. to unicorn id('RIP')))} to perform syscall\" ) self . sync unicorn to manticore ( ) from . . native . cpu . abstractcpu import Syscall self . to raise = Syscall ( ) uc . emu stop ( )", "predictions": ["called when a unicorn is received ."], "references": ["unicorn hook that transfers control to manticore so it can execute the syscall"], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 588, "code": "def hook write mem ( self , uc , access , address , size , value , data ) : self . mem delta [ address ] = ( value , size ) return True", "predictions": ["called when a memory is rotated"], "references": ["captures memory written by unicorn"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 589, "code": "def hook unmapped ( self , uc , access , address , size , value , data ) : try : self . sync unicorn to manticore ( ) logger . warning ( f\"Encountered an operation on unmapped memory at {hex(address)}\" ) m = self . cpu . memory . map containing ( address ) self . copy memory ( m . start , m . end - m . start ) except Memory Exception as e : logger . error ( \"Failed to map memory {}-{}, ({}): {}\" . format ( hex ( address ) , hex ( address + size ) , access , e ) ) self . to raise = e self . should try again = False return False self . should try again = True return False", "predictions": ["callback function to map unicorn to memory ."], "references": ["we hit an unmapped region ; map it into unicorn ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 590, "code": "def emulate ( self , instruction ) : while True : self . should try again = False self . to raise = None self . step ( instruction ) if not self . should try again : break", "predictions": ["emulate an instruction ."], "references": ["wrapper that runs the _step function in a loop while handling exceptions"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 591, "code": "def sync unicorn to manticore ( self ) : self . write backs disabled = True for reg in self . registers : val = self . emu . reg read ( self . to unicorn id ( reg ) ) self . cpu . write register ( reg , val ) if len ( self . mem delta ) > 0 : logger . debug ( f\"Syncing {len(self. mem delta)} writes back into Manticore\" ) for location in self . mem delta : value , size = self . mem delta [ location ] self . cpu . write int ( location , value , size * 8 ) self . write backs disabled = False self . mem delta = { }", "predictions": ["synchronize unicorn and manticore from registers registers ."], "references": ["copy registers and written memory back into manticore"], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 592, "code": "def write back memory ( self , where , expr , size ) : if self . write backs disabled : return if type ( expr ) is bytes : self . emu . mem write ( where , expr ) else : if issymbolic ( expr ) : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] concrete data = [ ] for c in data : if issymbolic ( c ) : c = chr ( solver . get value ( self . cpu . memory . constraints , c ) ) concrete data . append ( c ) data = concrete data else : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] logger . debug ( f\"Writing back {hr size(size // 8)} to {hex(where)}: {data}\" ) self . emu . mem write ( where , b'' . join ( b . encode ( 'utf-8' ) if type ( b ) is str else b for b in data ) )", "predictions": ["write back an back memory ."], "references": ["copy memory writes from manticore back into unicorn in real - time"], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 593, "code": "def update segment ( self , selector , base , size , perms ) : logger . info ( \"Updating selector %s to 0x%02x (%s bytes) (%s)\" , selector , base , size , perms ) if selector == 99 : self . set fs ( base ) else : logger . error ( \"No way to write segment: %d\" , selector )", "predictions": ["update the segment segment ."], "references": ["only useful for setting fs right now ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 594, "code": "def deprecated ( message : str ) : assert isinstance ( message , str ) , \"The deprecated decorator requires a message string argument.\" def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : warnings . warn ( f\"`{func. qualname }` is deprecated. {message}\" , category = Manticore Deprecation Warning , stacklevel = 2 ) return func ( * args , * * kwargs ) return wrapper return decorator", "predictions": ["decorator that temporarily temporarily temporarily deprecated ."], "references": ["a decorator for marking functions as deprecated ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 595, "code": "def input from cons ( constupl , datas ) : def make chr ( c ) : try : return chr ( c ) except Exception : return c newset = constraints to constraintset ( constupl ) ret = '' for data in datas : for c in data : ret += make chr ( solver . get value ( newset , c ) ) return ret", "predictions": ["return a list of input constraints from cons datas"], "references": ["solve bytes in |datas| based on"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 596, "code": "def write ( self , data ) : size = min ( len ( data ) , self . max size - self . pos ) for i in range ( self . pos , self . pos + size ) : self . array [ i ] = data [ i - self . pos ]", "predictions": ["write data to buffer ."], "references": ["writes the symbolic bytes in c { data } onto the file ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 597, "code": "def sys fsync ( self , fd ) : ret = 0 try : self . files [ fd ] . sync ( ) except Index Error : ret = - errno . EBADF except Fd Error : ret = - errno . EINVAL return ret", "predictions": ["return true if the file is available"], "references": ["synchronize a file s in - core state with that on disk ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 598, "code": "def awake ( self , procid ) : logger . debug ( f\"Remove procid:{procid} from waitlists and reestablish it in the running list\" ) for wait list in self . rwait : if procid in wait list : wait list . remove ( procid ) for wait list in self . twait : if procid in wait list : wait list . remove ( procid ) self . timers [ procid ] = None self . running . append ( procid ) if self . current is None : self . current = procid", "predictions": ["called when the procid is received"], "references": ["remove procid from waitlists and reestablish it in the running list"], "bleu": 0.10624253482403696, "rouge_l": 0.1117216117216117}
{"id": 599, "code": "def signal receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )", "predictions": ["receive a signal to the server ."], "references": ["awake one process waiting to receive data on fd"], "bleu": 0.15447878876032708, "rouge_l": 0.12224448897795591}
{"id": 600, "code": "def signal transmit ( self , fd ) : connection = self . connections ( fd ) if connection is None or connection >= len ( self . rwait ) : return procs = self . rwait [ connection ] if procs : procid = random . sample ( procs , 1 ) [ 0 ] self . awake ( procid )", "predictions": ["transmit a random signal"], "references": ["awake one process waiting to transmit data on fd"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 601, "code": "def check timers ( self ) : if self . current is None : advance = min ( [ self . clocks ] + [ x for x in self . timers if x is not None ] ) + 1 logger . debug ( f\"Advancing the clock from {self.clocks} to {advance}\" ) self . clocks = advance for procid in range ( len ( self . timers ) ) : if self . timers [ procid ] is not None : if self . clocks > self . timers [ procid ] : self . procs [ procid ] . PC += self . procs [ procid ] . instruction . size self . awake ( procid )", "predictions": ["check for timers timers"], "references": ["awake process if timer has expired"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 602, "code": "def signal transmit ( self , fd ) : connections = self . connections if connections ( fd ) and self . rwait [ connections ( fd ) ] : procid = random . sample ( self . rwait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )", "predictions": ["transmit a random signal"], "references": ["awake one process waiting to transmit data on fd"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 603, "code": "def sys receive ( self , cpu , fd , buf , count , rx bytes ) : if issymbolic ( fd ) : logger . info ( \"Ask to read from a symbolic file descriptor!!\" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( \"Ask to read to a symbolic buffer\" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( \"Ask to read a symbolic number of bytes \" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 2 ) if issymbolic ( rx bytes ) : logger . info ( \"Ask to return size to a symbolic address \" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 3 ) return super ( ) . sys receive ( cpu , fd , buf , count , rx bytes )", "predictions": ["receives a symbolic packet and returns the size of the cpu ."], "references": ["symbolic version of decree . sys_receive"], "bleu": 0.1235622127262679, "rouge_l": 0.3546511627906977}
{"id": 604, "code": "def sys transmit ( self , cpu , fd , buf , count , tx bytes ) : if issymbolic ( fd ) : logger . info ( \"Ask to write to a symbolic file descriptor!!\" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( \"Ask to write to a symbolic buffer\" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( \"Ask to write a symbolic number of bytes \" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 2 ) if issymbolic ( tx bytes ) : logger . info ( \"Ask to return size to a symbolic address \" ) cpu . PC = cpu . PC - cpu . instruction . size raise Symbolic Syscall Argument ( cpu , 3 ) return super ( ) . sys transmit ( cpu , fd , buf , count , tx bytes )", "predictions": ["writes a symbolic to a symbolic address ."], "references": ["symbolic version of decree . sys_transmit"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 605, "code": "def visit Operation ( self , expression , * operands ) : operation = self . operations . get ( type ( expression ) , None ) if operation is not None and all ( isinstance ( o , Constant ) for o in operands ) : value = operation ( * ( x . value for x in operands ) ) if isinstance ( expression , Bit Vec ) : return Bit Vec Constant ( expression . size , value , taint = expression . taint ) else : isinstance ( expression , Bool ) return Bool Constant ( value , taint = expression . taint ) else : if any ( operands [ i ] is not expression . operands [ i ] for i in range ( len ( operands ) ) ) : expression = self . rebuild ( expression , operands ) return expression", "predictions": ["return syntax syntax from input expression ."], "references": ["constant folding if all operands of an expression are a constant do the math"], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 606, "code": "def visit Operation ( self , expression , * operands ) : if all ( isinstance ( o , Constant ) for o in operands ) : expression = constant folder ( expression ) if self . changed ( expression , operands ) : expression = self . rebuild ( expression , operands ) return expression", "predictions": ["return expression that eliminate a expression ."], "references": ["constant folding if all operands of an expression are a constant do the math"], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 607, "code": "def type size ( ty ) : if ty [ 0 ] in ( 'int' , 'uint' , 'bytes M' , 'function' ) : return 32 elif ty [ 0 ] in ( 'tuple' ) : result = 0 for ty i in ty [ 1 ] : result += ABI . type size ( ty i ) return result elif ty [ 0 ] in ( 'array' ) : rep = ty [ 1 ] result = 32 return result elif ty [ 0 ] in ( 'bytes' , 'string' ) : result = 32 return result raise Value Error", "predictions": ["get the size of a group"], "references": ["calculate static type size"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 608, "code": "def function call ( type spec , * args ) : m = re . match ( r\"(?P<name>[a-z A-Z ][a-z A-Z 0-9]*)(?P<type>\\(.*\\))\" , type spec ) if not m : raise Ethereum Error ( \"Function signature expected\" ) ABI . check and warn num args ( type spec , * args ) result = ABI . function selector ( type spec ) result += ABI . serialize ( m . group ( 'type' ) , * args ) return result", "predictions": ["decorator for get request calls"], "references": ["build transaction data from function signature and arguments"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 609, "code": "def function selector ( method name and signature ) : s = sha3 . keccak 256 ( ) s . update ( method name and signature . encode ( ) ) return bytes ( s . digest ( ) [ : 4 ] )", "predictions": ["get the run run run run with the given method . . . . . . . . . . . . . . . . . . . . ."], "references": ["makes a function hash id from a method signature"], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 610, "code": "def serialize uint ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise Value Error from . account import EVM Account if not isinstance ( value , ( int , Bit Vec , EVM Account ) ) : raise Value Error if issymbolic ( value ) : bytes = Array Variable ( index bits = 256 , index max = 32 , value bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) if value . size <= size * 8 : value = Operators . ZEXTEND ( value , size * 8 ) else : value = Operators . EXTRACT ( value , 0 , size * 8 ) bytes = Array Proxy ( bytes . write BE ( padding , value , size ) ) else : value = int ( value ) bytes = bytearray ( ) for in range ( padding ) : bytes . append ( 0 ) for position in reversed ( range ( size ) ) : bytes . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) assert len ( bytes ) == size + padding return bytes", "predictions": ["hook to hook . ."], "references": ["translates a python integral or a bitvec into a 32 byte string msb first"], "bleu": 0.037948473198912445, "rouge_l": 0.0}
{"id": 611, "code": "def serialize int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise Value Error if not isinstance ( value , ( int , Bit Vec ) ) : raise Value Error if issymbolic ( value ) : buf = Array Variable ( index bits = 256 , index max = 32 , value bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = Array Proxy ( buf . write BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf", "predictions": ["get an unsigned integer as an unsigned 16 - bit integer . ."], "references": ["translates a signed python integral or a bitvec into a 32 byte string msb first"], "bleu": 0.06886905670533619, "rouge_l": 0.0}
{"id": 612, "code": "def instruction ( self ) : try : decoding cache = getattr ( self , ' decoding cache' ) except Exception : decoding cache = self . decoding cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in decoding cache : return decoding cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc i ] ) . value while True : yield 0 instruction = EVM Asm . disassemble one ( getcode ( ) , pc = pc , fork = DEFAULT FORK ) decoding cache [ pc ] = instruction return instruction", "predictions": ["get an get an get an get get an get get the get get get get an get get an get an get get - get an get - black get"], "references": ["current instruction pointed by self . pc"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 613, "code": "def top ( self , n = 0 ) : if len ( self . stack ) - n < 0 : raise Stack Underflow ( ) return self . stack [ n - 1 ]", "predictions": ["return the correspond to the correspond to the stack"], "references": ["read a value from the top of the stack without removing it"], "bleu": 0.1430210741102858, "rouge_l": 0.2785388127853881}
{"id": 614, "code": "def rollback ( self ) : last pc , last gas , last instruction , last arguments , fee , allocated = self . checkpoint data self . push arguments ( last arguments ) self . gas = last gas self . pc = last pc self . allocated = allocated self . checkpoint data = None", "predictions": ["rolls back the previous pc instance . . . . . . . . . . . ."], "references": ["revert the stack gas pc and memory allocation so it looks like before executing the instruction"], "bleu": 0.07535838128770536, "rouge_l": 0.1189083820662768}
{"id": 615, "code": "def store ( self , offset , value , size = 1 ) : self . memory . write BE ( offset , value , size ) for i in range ( size ) : self . publish ( 'did evm write memory' , offset + i , Operators . EXTRACT ( value , ( size - i - 1 ) * 8 , 8 ) )", "predictions": ["hashes a single chunk of data into memory ."], "references": ["stores value in memory as a big endian"], "bleu": 0.15619699684601276, "rouge_l": 0.1189083820662768}
{"id": 616, "code": "def SMOD ( self , a , b ) : s0 , s1 = to signed ( a ) , to signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except Zero Division Error : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )", "predictions": ["convert to signed ."], "references": ["signed modulo remainder operation"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 617, "code": "def EXP gas ( self , base , exponent ) : EXP SUPPLEMENTAL GAS = 10 def nbytes ( e ) : result = 0 for i in range ( 32 ) : result = Operators . ITEBV ( 512 , Operators . EXTRACT ( e , i * 8 , 8 ) != 0 , i + 1 , result ) return result return EXP SUPPLEMENTAL GAS * nbytes ( exponent )", "predictions": ["be a unit memory"], "references": ["calculate extra gas fee"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 618, "code": "def SIGNEXTEND ( self , size , value ) : testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )", "predictions": ["convert integer to binary string ."], "references": ["extend length of two s complement signed integer"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 619, "code": "def LT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . ULT ( a , b ) , 1 , 0 )", "predictions": ["return function to hook"], "references": ["less - than comparison"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 620, "code": "def GT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . UGT ( a , b ) , 1 , 0 )", "predictions": ["returns the first dimension of self value value"], "references": ["greater - than comparison"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 621, "code": "def SGT ( self , a , b ) : s0 , s1 = to signed ( a ) , to signed ( b ) return Operators . ITEBV ( 256 , s0 > s1 , 1 , 0 )", "predictions": ["convert from a to signed"], "references": ["signed greater - than comparison"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 622, "code": "def BYTE ( self , offset , value ) : offset = Operators . ITEBV ( 256 , offset < 32 , ( 31 - offset ) * 8 , 256 ) return Operators . ZEXTEND ( Operators . EXTRACT ( value , offset , 8 ) , 256 )", "predictions": ["convert emulate to binary format ."], "references": ["retrieve single byte from word"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 623, "code": "def SHA3 ( self , start , size ) : data = self . try simplify to constant ( self . read buffer ( start , size ) ) if issymbolic ( data ) : known sha3 = { } self . publish ( 'on symbolic sha3' , data , known sha3 ) value = 0 known hashes cond = False for key , hsh in known sha3 . items ( ) : assert not issymbolic ( key ) , \"Saved sha3 data,hash pairs should be concrete\" cond = key == data known hashes cond = Operators . OR ( cond , known hashes cond ) value = Operators . ITEBV ( 256 , cond , hsh , value ) return value value = sha3 . keccak 256 ( data ) . hexdigest ( ) value = int ( value , 16 ) self . publish ( 'on concrete sha3' , data , value ) logger . info ( \"Found a concrete SHA3 example %r -> %x\" , data , value ) return value", "predictions": ["register a single chunk of data . . ."], "references": ["compute keccak - 256 hash"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 624, "code": "def CALLDATALOAD ( self , offset ) : if issymbolic ( offset ) : if solver . can be true ( self . constraints , offset == self . used calldata size ) : self . constraints . add ( offset == self . used calldata size ) raise Concretize Argument ( 1 , policy = 'SAMPLED' ) self . use calldata ( offset , 32 ) data length = len ( self . data ) bytes = [ ] for i in range ( 32 ) : try : c = Operators . ITEBV ( 8 , offset + i < data length , self . data [ offset + i ] , 0 ) except Index Error : c = 0 bytes . append ( c ) return Operators . CONCAT ( 256 , * bytes )", "predictions": ["see list . write ."], "references": ["get input data of current environment"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 625, "code": "def CALLDATACOPY ( self , mem offset , data offset , size ) : if issymbolic ( size ) : if solver . can be true ( self . constraints , size <= len ( self . data ) + 32 ) : self . constraints . add ( size <= len ( self . data ) + 32 ) raise Concretize Argument ( 3 , policy = 'SAMPLED' ) if issymbolic ( data offset ) : if solver . can be true ( self . constraints , data offset == self . used calldata size ) : self . constraints . add ( data offset == self . used calldata size ) raise Concretize Argument ( 2 , policy = 'SAMPLED' ) #account for calldata usage self . use calldata ( data offset , size ) self . allocate ( mem offset , size ) for i in range ( size ) : try : c = Operators . ITEBV ( 8 , data offset + i < len ( self . data ) , Operators . ORD ( self . data [ data offset + i ] ) , 0 ) except Index Error : c = 0 self . store ( mem offset + i , c )", "predictions": ["set constraints constraints usage"], "references": ["copy input data in current environment to memory"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 626, "code": "def CODECOPY ( self , mem offset , code offset , size ) : self . allocate ( mem offset , size ) GCOPY = 3 copyfee = self . safe mul ( GCOPY , Operators . UDIV ( self . safe add ( size , 31 ) , 32 ) ) self . consume ( copyfee ) if issymbolic ( size ) : max size = solver . max ( self . constraints , size ) else : max size = size for i in range ( max size ) : if issymbolic ( i < size ) : default = Operators . ITEBV ( 8 , i < size , 0 , self . load ( mem offset + i , 1 ) ) else : if i < size : default = 0 else : default = self . load ( mem offset + i , 1 ) if issymbolic ( code offset ) : value = Operators . ITEBV ( 8 , code offset + i >= len ( self . bytecode ) , default , self . bytecode [ code offset + i ] ) else : if code offset + i >= len ( self . bytecode ) : value = default else : value = self . bytecode [ code offset + i ] self . store ( mem offset + i , value ) self . publish ( 'did evm read code' , code offset , size )", "predictions": ["load callback for updating updating data in memory"], "references": ["copy code running in current environment to memory"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 627, "code": "def EXTCODECOPY ( self , account , address , offset , size ) : extbytecode = self . world . get code ( account ) self . allocate ( address + size ) for i in range ( size ) : if offset + i < len ( extbytecode ) : self . store ( address + i , extbytecode [ offset + i ] ) else : self . store ( address + i , 0 )", "predictions": ["allocate an account s code"], "references": ["copy an account s code to memory"], "bleu": 0.5093330917854971, "rouge_l": 0.6472148541114059}
{"id": 628, "code": "def MLOAD ( self , address ) : self . allocate ( address , 32 ) value = self . load ( address , 32 ) return value", "predictions": ["= loaded loaded loaded loaded loaded size size size size size size size size size size size size"], "references": ["load word from memory"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 629, "code": "def MSTORE ( self , address , value ) : if istainted ( self . pc ) : for taint in get taints ( self . pc ) : value = taint with ( value , taint ) self . allocate ( address , 32 ) self . store ( address , value , 32 )", "predictions": ["setter for pc attribute"], "references": ["save word to memory"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 630, "code": "def MSTORE8 ( self , address , value ) : if istainted ( self . pc ) : for taint in get taints ( self . pc ) : value = taint with ( value , taint ) self . allocate ( address , 1 ) self . store ( address , Operators . EXTRACT ( value , 0 , 8 ) , 1 )", "predictions": ["updates the text from the given address"], "references": ["save byte to memory"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 631, "code": "def SLOAD ( self , offset ) : storage address = self . address self . publish ( 'will evm read storage' , storage address , offset ) value = self . world . get storage data ( storage address , offset ) self . publish ( 'did evm read storage' , storage address , offset , value ) return value", "predictions": ["return the value at the given offset ."], "references": ["load word from storage"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 632, "code": "def SSTORE ( self , offset , value ) : storage address = self . address self . publish ( 'will evm write storage' , storage address , offset , value ) #refund = Operators.ITEBV(256, if istainted ( self . pc ) : for taint in get taints ( self . pc ) : value = taint with ( value , taint ) self . world . set storage data ( storage address , offset , value ) self . publish ( 'did evm write storage' , storage address , offset , value )", "predictions": ["locks or receive the message from the connection connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections"], "references": ["save word to storage"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 633, "code": "def JUMPI ( self , dest , cond ) : self . pc = Operators . ITEBV ( 256 , cond != 0 , dest , self . pc + self . instruction . size ) #This set ups a check for JMPDEST in the next instruction if cond != 0 self . set check jmpdest ( cond != 0 )", "predictions": ["in - place a is within an + to be a is within the right + cond current one current one current one current + 1 current one current + a"], "references": ["conditionally alter the program counter"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 634, "code": "def SWAP ( self , * operands ) : a = operands [ 0 ] b = operands [ - 1 ] return ( b , ) + operands [ 1 : - 1 ] + ( a , )", "predictions": ["convert to signal ."], "references": ["exchange 1st and 2nd stack items"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 635, "code": "def CREATE ( self , value , offset , size ) : address = self . world . create account ( address = EVM World . calculate new address ( sender = self . address , nonce = self . world . get nonce ( self . address ) ) ) self . world . start transaction ( 'CREATE' , address , data = self . read buffer ( offset , size ) , caller = self . address , value = value , gas = self . gas ) raise Start Tx ( )", "predictions": ["called when a sys is received ."], "references": ["create a new account with associated code"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 636, "code": "def CREATE ( self , value , offset , size ) : tx = self . world . last transaction address = tx . address if tx . result == 'RETURN' : self . world . set code ( tx . address , tx . return data ) else : self . world . delete account ( address ) address = 0 return address", "predictions": ["subscribe to the given transaction"], "references": ["create a new account with associated code"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 637, "code": "def CALLCODE ( self , gas , ignored , value , in offset , in size , out offset , out size ) : self . world . start transaction ( 'CALLCODE' , address = self . address , data = self . read buffer ( in offset , in size ) , caller = self . address , value = value , gas = gas ) raise Start Tx ( )", "predictions": ["put a transaction into a transaction instance ."], "references": ["message - call into this account with alternative account s code"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 638, "code": "def RETURN ( self , offset , size ) : data = self . read buffer ( offset , size ) raise End Tx ( 'RETURN' , data )", "predictions": ["visit a visit request if the file is not ignored ."], "references": ["halt execution returning output data"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 639, "code": "def SELFDESTRUCT ( self , recipient ) : #This may create a user account recipient = Operators . EXTRACT ( recipient , 0 , 160 ) address = self . address #FIXME for on the known addresses if issymbolic ( recipient ) : logger . info ( \"Symbolic recipient on self destruct\" ) recipient = solver . get value ( self . constraints , recipient ) if recipient not in self . world : self . world . create account ( address = recipient ) self . world . send funds ( address , recipient , self . world . get balance ( address ) ) self . world . delete account ( address ) raise End Tx ( 'SELFDESTRUCT' )", "predictions": ["recipient the recipient of a recipient"], "references": ["halt execution and register account for later deletion"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 640, "code": "def current human transaction ( self ) : try : tx , , , , = self . callstack [ 0 ] if tx . result is not None : #That tx finished. No current tx. return None assert tx . depth == 0 return tx except Index Error : return None", "predictions": ["return the current human readable transaction ."], "references": ["current ongoing human transaction"], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 641, "code": "def new address ( self , sender = None , nonce = None ) : if sender is not None and nonce is None : nonce = self . get nonce ( sender ) new address = self . calculate new address ( sender , nonce ) if sender is None and new address in self : return self . new address ( sender , nonce ) return new address", "predictions": ["returns a new address or none if the sender is available ."], "references": ["create a fresh 160bit address"], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 642, "code": "def swap mode ( self ) : assert self . mode in ( cs . CS MODE ARM , cs . CS MODE THUMB ) if self . mode == cs . CS MODE ARM : self . mode = cs . CS MODE THUMB else : self . mode = cs . CS MODE ARM", "predictions": ["swap the mode ."], "references": ["toggle between arm and thumb mode"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 643, "code": "def LDRD ( cpu , dest1 , dest2 , src , offset = None ) : assert dest1 . type == 'register' assert dest2 . type == 'register' assert src . type == 'memory' mem1 = cpu . read int ( src . address ( ) , 32 ) mem2 = cpu . read int ( src . address ( ) + 4 , 32 ) writeback = cpu . compute writeback ( src , offset ) dest1 . write ( mem1 ) dest2 . write ( mem2 ) cpu . cs hack ldr str writeback ( src , offset , writeback )", "predictions": ["compute and write a ldrd tag ."], "references": ["loads double width data from memory ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 644, "code": "def STRD ( cpu , src1 , src2 , dest , offset = None ) : assert src1 . type == 'register' assert src2 . type == 'register' assert dest . type == 'memory' val1 = src1 . read ( ) val2 = src2 . read ( ) writeback = cpu . compute writeback ( dest , offset ) cpu . write int ( dest . address ( ) , val1 , 32 ) cpu . write int ( dest . address ( ) + 4 , val2 , 32 ) cpu . cs hack ldr str writeback ( dest , offset , writeback )", "predictions": ["compute vote vote ."], "references": ["writes the contents of two registers to memory ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 645, "code": "def context ( self ) : plugin context name = str ( type ( self ) ) if plugin context name not in self . manticore . context : self . manticore . context [ plugin context name ] = { } return self . manticore . context [ plugin context name ]", "predictions": ["return string describing the context ."], "references": ["convenient access to shared context"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 646, "code": "def declare ( self , var ) : if var . name in self . declarations : raise Value Error ( 'Variable already declared' ) self . declarations [ var . name ] = var return var", "predictions": ["declare a variable ."], "references": ["declare the variable var"], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 647, "code": "def declarations ( self ) : declarations = Get Declarations ( ) for a in self . constraints : try : declarations . visit ( a ) except Runtime Error : if sys . getrecursionlimit ( ) >= Pickle Serializer . MAX RECURSION : raise Exception ( f'declarations recursion limit surpassed {Pickle Serializer.MAX RECURSION}, aborting' ) new limit = sys . getrecursionlimit ( ) + Pickle Serializer . DEFAULT RECURSION if new limit <= Pickle Serializer . DEFAULT RECURSION : sys . setrecursionlimit ( new limit ) return self . declarations return declarations . result", "predictions": ["limit constraints to the client ."], "references": ["returns the variable expressions of this constraint set"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 648, "code": "def is declared ( self , expression var ) : if not isinstance ( expression var , Variable ) : raise Value Error ( f'Expression must be a Variable (not a {type(expression var)})' ) return any ( expression var is x for x in self . get declared variables ( ) )", "predictions": ["returns true if variable is a declared expression ."], "references": ["true if expression_var is declared in this constraint set"], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 649, "code": "def col transform ( self , col , digits ) : if col is None or float ( col ) < 0.0 : return None else : col = self . number to base ( int ( col ) , self . base , digits ) if len ( col ) == digits : return col else : return [ 0 for in range ( digits - len ( col ) ) ] + col", "predictions": ["return a column with a column col to a given column"], "references": ["the lambda body to transform the column values"], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 650, "code": "def transform leave one out ( self , X in , y , mapping = None ) : X = X in . copy ( deep = True ) random state = check random state ( self . random state ) if y is not None : y = y . astype ( 'double' ) for cat col in X . select dtypes ( 'category' ) . columns . values : X [ cat col ] = X [ cat col ] . cat . add categories ( - 999.9 ) X = X . fillna ( - 999.9 ) for col , colmap in mapping . items ( ) : level notunique = colmap [ 'count' ] > 1 unique train = colmap . index unseen values = pd . Series ( [ x for x in X in [ col ] . unique ( ) if x not in unique train ] ) is nan = X in [ col ] . isnull ( ) is unknown value = X in [ col ] . isin ( unseen values . dropna ( ) ) if self . handle unknown == 'error' and is unknown value . any ( ) : raise Value Error ( 'Columns to be encoded can not contain new values' ) if y is None : level means = ( ( colmap [ 'sum' ] + self . mean ) / ( colmap [ 'count' ] + 1 ) ) . where ( level notunique , self . mean ) X [ col ] = X [ col ] . map ( level means ) else : temp = y . groupby ( X [ col ] ) . agg ( [ 'cumsum' , 'cumcount' ] ) X [ col ] = ( temp [ 'cumsum' ] - y + self . mean ) / ( temp [ 'cumcount' ] + 1 ) if self . handle unknown == 'value' : X . loc [ is unknown value , col ] = self . mean elif self . handle unknown == 'return nan' : X . loc [ is unknown value , col ] = np . nan if self . handle missing == 'value' : X . loc [ is nan & unseen values . isnull ( ) . any ( ) , col ] = self . mean elif self . handle missing == 'return nan' : X . loc [ is nan , col ] = np . nan if self . sigma is not None and y is not None : X [ col ] = X [ col ] * random state . normal ( 1. , self . sigma , X [ col ] . shape [ 0 ] ) return X", "predictions": ["fix the one - hot dataset with another mean and y - aggregated values"], "references": ["leave one out encoding uses a single column of floats to represent the means of the target variables ."], "bleu": 0.06844459661832797, "rouge_l": 0.058994197292069624}
{"id": 651, "code": "def get obj cols ( df ) : obj cols = [ ] for idx , dt in enumerate ( df . dtypes ) : if dt == 'object' or is category ( dt ) : obj cols . append ( df . columns . values [ idx ] ) return obj cols", "predictions": ["return a list of column names from a dataframe ."], "references": ["returns names of object columns in the dataframe ."], "bleu": 0.18850319022747347, "rouge_l": 0.31881533101045295}
{"id": 652, "code": "def convert input ( X ) : if not isinstance ( X , pd . Data Frame ) : if isinstance ( X , list ) : X = pd . Data Frame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . Data Frame ( X ) elif isinstance ( X , csr matrix ) : X = pd . Data Frame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . Data Frame ( X ) else : raise Value Error ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to numeric ( x , errors = 'ignore' ) ) return X", "predictions": ["convert input input to numeric"], "references": ["unite data into a dataframe ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 653, "code": "def transform leave one out ( self , X in , y , mapping = None ) : X = X in . copy ( deep = True ) random state = check random state ( self . random state ) for col , colmap in mapping . items ( ) : level notunique = colmap [ 'count' ] > 1 unique train = colmap . index unseen values = pd . Series ( [ x for x in X [ col ] . unique ( ) if x not in unique train ] ) is nan = X [ col ] . isnull ( ) is unknown value = X [ col ] . isin ( unseen values . dropna ( ) ) if self . handle unknown == 'error' and is unknown value . any ( ) : raise Value Error ( 'Columns to be encoded can not contain new values' ) if y is None : level means = ( colmap [ 'sum' ] / colmap [ 'count' ] ) . where ( level notunique , self . mean ) X [ col ] = X [ col ] . map ( level means ) else : level means = ( X [ col ] . map ( colmap [ 'sum' ] ) - y ) / ( X [ col ] . map ( colmap [ 'count' ] ) - 1 ) X [ col ] = level means . where ( X [ col ] . map ( colmap [ 'count' ] [ level notunique ] ) . notnull ( ) , self . mean ) if self . handle unknown == 'value' : X . loc [ is unknown value , col ] = self . mean elif self . handle unknown == 'return nan' : X . loc [ is unknown value , col ] = np . nan if self . handle missing == 'value' : X . loc [ is nan & unseen values . isnull ( ) . any ( ) , col ] = self . mean elif self . handle missing == 'return nan' : X . loc [ is nan , col ] = np . nan if self . sigma is not None and y is not None : X [ col ] = X [ col ] * random state . normal ( 1. , self . sigma , X [ col ] . shape [ 0 ] ) return X", "predictions": ["puts one out of the model with a leave value"], "references": ["leave one out encoding uses a single column of floats to represent the means of the target variables ."], "bleu": 0.09225894723463805, "rouge_l": 0.2612419700214133}
{"id": 654, "code": "def score models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X test = None for in range ( runs ) : X test = encoder ( ) . fit transform ( X , y ) X test = Standard Scaler ( ) . fit transform ( X test ) scores . append ( cross validate ( clf , X test , y , n jobs = 1 , cv = 5 ) [ 'test score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X test . shape [ 1 ]", "predictions": ["score the model models using the model ."], "references": ["takes in a classifier that supports multiclass classification and x and a y and returns a cross validation score ."], "bleu": 0.03959981382356705, "rouge_l": 0.13260869565217392}
{"id": 655, "code": "def main ( loader , name ) : scores = [ ] raw scores ds = { } X , y , mapping = loader ( ) clf = linear model . Logistic Regression ( solver = 'lbfgs' , multi class = 'auto' , max iter = 200 , random state = 0 ) encoders = ( set ( category encoders . all ) - { 'WOE Encoder' } ) for encoder name in encoders : encoder = getattr ( category encoders , encoder name ) start time = time . time ( ) score , stds , raw scores , dim = score models ( clf , X , y , encoder ) scores . append ( [ encoder name , name , dim , score , stds , time . time ( ) - start time ] ) raw scores ds [ encoder name ] = raw scores gc . collect ( ) results = pd . Data Frame ( scores , columns = [ 'Encoding' , 'Dataset' , 'Dimensionality' , 'Avg. Score' , 'Score St Dev' , 'Elapsed Time' ] ) raw = pd . Data Frame . from dict ( raw scores ds ) ax = raw . plot ( kind = 'box' , return type = 'axes' ) plt . title ( 'Scores for Encodings on %s Dataset' % ( name , ) ) plt . ylabel ( 'Score (higher is better)' ) for tick in ax . get xticklabels ( ) : tick . set rotation ( 90 ) plt . grid ( ) plt . tight layout ( ) plt . show ( ) return results , raw", "predictions": ["score the specified entity and a specified loader ."], "references": ["here we iterate through the datasets and score them with a classifier using different encodings ."], "bleu": 0.08533861327767082, "rouge_l": 0.3046192259675406}
{"id": 656, "code": "def format options ( self , ctx , formatter ) : field opts = [ ] global opts = [ ] local opts = [ ] other opts = [ ] for param in self . params : if param . name in SETTINGS PARMS : opts = global opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local opts rv = param . get help record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add help option : help options = self . get help option names ( ctx ) if help options : other opts . append ( [ join options ( help options ) [ 0 ] , 'Show this message and exit.' ] ) if field opts : with formatter . section ( 'Field Options' ) : formatter . write dl ( field opts ) if local opts : with formatter . section ( 'Local Options' ) : formatter . write dl ( local opts ) if global opts : with formatter . section ( 'Global Options' ) : formatter . write dl ( global opts ) if other opts : with formatter . section ( 'Other Options' ) : formatter . write dl ( other opts )", "predictions": ["writes the options to the formatter ."], "references": ["monkey - patch click s format_options method to support option categorization ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 657, "code": "def convert pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )", "predictions": ["convert pagenum from kwargs to pagenum"], "references": ["convert next and previous from urls to integers"], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 658, "code": "def disassoc ( self , url fragment , me , other ) : url = self . endpoint + '%d/%s/' % ( me , url fragment ) r = client . get ( url , params = { 'id' : other } ) . json ( ) if r [ 'count' ] == 0 : return { 'changed' : False } r = client . post ( url , data = { 'disassociate' : True , 'id' : other } ) return { 'changed' : True }", "predictions": ["send a disassoc request"], "references": ["disassociate the other record from the me record ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 659, "code": "def last job data ( self , pk = None , * * kwargs ) : ujt = self . get ( pk , include debug header = True , * * kwargs ) if 'current update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last update' ] [ 7 : ] ) . json ( ) else : raise exc . Not Found ( 'No related jobs or updates exist.' )", "predictions": ["get the last job data and updates the last job data"], "references": ["internal utility function for unified job templates . returns data about the last job run off of that ujt"], "bleu": 0.11337596138541267, "rouge_l": 0.3180396246089677}
{"id": 660, "code": "def lookup stdout ( self , pk = None , start line = None , end line = None , full = True ) : stdout url = '%s%s/stdout/' % ( self . unified job type , pk ) payload = { 'format' : 'json' , 'content encoding' : 'base64' , 'content format' : 'ansi' } if start line : payload [ 'start line' ] = start line if end line : payload [ 'end line' ] = end line debug . log ( 'Requesting a copy of job standard output' , header = 'details' ) resp = client . get ( stdout url , params = payload ) . json ( ) content = b64decode ( resp [ 'content' ] ) return content . decode ( 'utf-8' , 'replace' )", "predictions": ["run a unified task"], "references": ["internal utility function to return standard out . requires the pk of a unified job ."], "bleu": 0.02354285107288738, "rouge_l": 0.1804733727810651}
{"id": 661, "code": "def version ( ) : click . echo ( 'Tower CLI %s' % version ) click . echo ( 'API %s' % CUR API VERSION ) try : r = client . get ( '/config/' ) except Request Exception as ex : raise exc . Tower CLI Error ( 'Could not connect to Ansible Tower.\\n%s' % six . text type ( ex ) ) config = r . json ( ) license = config . get ( 'license info' , { } ) . get ( 'license type' , 'open' ) if license == 'open' : server type = 'AWX' else : server type = 'Ansible Tower' click . echo ( '%s %s' % ( server type , config [ 'version' ] ) ) click . echo ( 'Ansible %s' % config [ 'ansible version' ] )", "predictions": ["show current version ."], "references": ["display full version information ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 662, "code": "def echo setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text type ) else 'cyan' , )", "predictions": ["print a warning setting"], "references": ["echo a setting to the cli ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 663, "code": "def login ( username , password , scope , client id , client secret , verbose ) : if not supports oauth ( ) : raise exc . Tower CLI Error ( 'This version of Tower does not support O Auth2.0. Set credentials using tower-cli config.' ) req = collections . namedtuple ( 'req' , 'headers' ) ( { } ) if client id and client secret : HTTP Basic Auth ( client id , client secret ) ( req ) req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { \"grant type\" : \"password\" , \"username\" : username , \"password\" : password , \"scope\" : scope } , headers = req . headers ) elif client id : req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { \"grant type\" : \"password\" , \"username\" : username , \"password\" : password , \"client id\" : client id , \"scope\" : scope } , headers = req . headers ) else : HTTP Basic Auth ( username , password ) ( req ) r = client . post ( '/users/{}/personal tokens/' . format ( username ) , data = { \"description\" : \"Tower CLI\" , \"application\" : None , \"scope\" : scope } , headers = req . headers ) if r . ok : result = r . json ( ) result . pop ( 'summary fields' , None ) result . pop ( 'related' , None ) if client id : token = result . pop ( 'access token' , None ) else : token = result . pop ( 'token' , None ) if settings . verbose : result [ 'token' ] = token secho ( json . dumps ( result , indent = 1 ) , fg = 'blue' , bold = True ) config . main ( [ 'oauth token' , token , '--scope=user' ] )", "predictions": ["login to the zoneminder api server"], "references": ["retrieves and stores an oauth2 personal auth token ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 664, "code": "def convert ( self , value , param , ctx ) : if not isinstance ( value , str ) : return value if isinstance ( value , six . binary type ) : value = value . decode ( 'UTF-8' ) if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file obj , 'read' ) : return file obj . read ( ) return file obj return value", "predictions": ["convert value to string ."], "references": ["return file content if file else return value as - is"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 665, "code": "def set display columns ( self , set true = [ ] , set false = [ ] ) : for i in range ( len ( self . fields ) ) : if self . fields [ i ] . name in set true : self . fields [ i ] . display = True elif self . fields [ i ] . name in set false : self . fields [ i ] . display = False", "predictions": ["set the display columns of the table"], "references": ["add or remove columns from the output ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 666, "code": "def separate ( self , kwargs ) : self . pop none ( kwargs ) result = { } for field in Resource . config fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json fields : if not isinstance ( result [ field ] , six . string types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except Value Error : raise exc . Tower CLI Error ( 'Provided json file format ' 'invalid. Please recheck.' ) return result", "predictions": ["separate json file to a dict"], "references": ["remove none - valued and configuration - related keyworded arguments"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 667, "code": "def format id ( self , payload ) : if 'id' in payload : return str ( payload [ 'id' ] ) if 'results' in payload : return ' ' . join ( [ six . text type ( item [ 'id' ] ) for item in payload [ 'results' ] ] ) raise Multiple Related Error ( 'Could not serialize output with id format.' )", "predictions": ["convert a payload to json format ."], "references": ["echos only the id"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 668, "code": "def list resource commands ( self ) : resource path = os . path . abspath ( os . path . join ( os . path . dirname ( file ) , os . pardir , 'resources' ) ) answer = set ( [ ] ) for , name , in pkgutil . iter modules ( [ resource path ] ) : res = tower cli . get resource ( name ) if not getattr ( res , 'internal' , False ) : answer . add ( name ) return sorted ( answer )", "predictions": ["list all available resource commands ."], "references": ["returns a list of multi - commands for each resource type ."], "bleu": 0.10218289380194193, "rouge_l": 0.31443298969072164}
{"id": 669, "code": "def new state ( self ) : try : self . state = self . state ( ) log . debug ( \"Generated new state %s.\" , self . state ) except Type Error : self . state = self . state log . debug ( \"Re-using previously supplied state %s.\" , self . state ) return self . state", "predictions": ["update the state of the new state ."], "references": ["generates a state string to be used in authorizations ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 670, "code": "def request ( self , method , url , data = None , headers = None , withhold token = False , client id = None , client secret = None , * * kwargs ) : if not is secure transport ( url ) : raise Insecure Transport Error ( ) if self . token and not withhold token : log . debug ( \"Invoking %d protected resource request hooks.\" , len ( self . compliance hook [ \"protected request\" ] ) , ) for hook in self . compliance hook [ \"protected request\" ] : log . debug ( \"Invoking hook %s.\" , hook ) url , headers , data = hook ( url , headers , data ) log . debug ( \"Adding token %s to request.\" , self . token ) try : url , headers , data = self . client . add token ( url , http method = method , body = data , headers = headers ) except Token Expired Error : if self . auto refresh url : log . debug ( \"Auto refresh is set, attempting to refresh at %s.\" , self . auto refresh url , ) auth = kwargs . pop ( \"auth\" , None ) if client id and client secret and ( auth is None ) : log . debug ( 'Encoding client id \"%s\" with client secret as Basic auth credentials.' , client id , ) auth = requests . auth . HTTP Basic Auth ( client id , client secret ) token = self . refresh token ( self . auto refresh url , auth = auth , * * kwargs ) if self . token updater : log . debug ( \"Updating token to %s using %s.\" , token , self . token updater ) self . token updater ( token ) url , headers , data = self . client . add token ( url , http method = method , body = data , headers = headers ) else : raise Token Updated ( token ) else : raise log . debug ( \"Requesting url %s using method %s.\" , url , method ) log . debug ( \"Supplying headers %s and data %s\" , headers , data ) log . debug ( \"Passing through key word arguments %s.\" , kwargs ) return super ( O Auth2Session , self ) . request ( method , url , headers = headers , data = data , * * kwargs )", "predictions": ["make request to api ."], "references": ["intercept all requests and add the oauth 2 token if present ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 671, "code": "def clear ( self , cfgstr = None ) : data fpath = self . get fpath ( cfgstr ) if self . verbose > 0 : self . log ( '[cacher] clear cache' ) if exists ( data fpath ) : if self . verbose > 0 : self . log ( '[cacher] removing {}' . format ( data fpath ) ) os . remove ( data fpath ) meta fpath = data fpath + '.meta' if exists ( meta fpath ) : os . remove ( meta fpath ) else : if self . verbose > 0 : self . log ( '[cacher] ... nothing to clear' )", "predictions": ["clear the cache of the data"], "references": ["removes the saved cache and metadata from disk"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 672, "code": "def get certificate ( self , cfgstr = None ) : certificate = self . cacher . tryload ( cfgstr = cfgstr ) return certificate", "predictions": ["current human - readable human - readable human - readable human - readable human - readable human - readable human - readable human - readable human - readable human - readable"], "references": ["returns the stamp certificate if it exists"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 673, "code": "def rectify products ( self , product = None ) : products = self . product if product is None else product if products is None : return None if not isinstance ( products , ( list , tuple ) ) : products = [ products ] return products", "predictions": ["get paths to new address with no address ."], "references": ["puts products in a normalized format"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 674, "code": "def product file hash ( self , product = None ) : if self . hasher is None : return None else : products = self . rectify products ( product ) product file hash = [ util hash . hash file ( p , hasher = self . hasher , base = 'hex' ) for p in products ] return product file hash", "predictions": ["return a swap mode"], "references": ["get the hash of the each product file"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 675, "code": "def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util str import ensure unicode msg = ensure unicode ( msg ) super ( Tee String IO , self ) . write ( msg )", "predictions": ["write is a message to the redirect"], "references": ["write to this and the redirected stream"], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 676, "code": "def flush ( self ) : if self . redirect is not None : self . redirect . flush ( ) super ( Tee String IO , self ) . flush ( )", "predictions": ["flush the stream dest"], "references": ["flush to this and the redirected stream"], "bleu": 0.20183609024241697, "rouge_l": 0.5198863636363635}
{"id": 677, "code": "def log part ( self ) : self . cap stdout . seek ( self . pos ) text = self . cap stdout . read ( ) self . pos = self . cap stdout . tell ( ) self . parts . append ( text ) self . text = text", "predictions": [". context context put put it into the = output file name name name name name name name name name name"], "references": ["log what has been captured so far"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 678, "code": "def join itemstrs ( itemstrs , itemsep , newlines , leaf info , nobraces , trailing sep , compact brace , lbr , rbr ) : use newline = newlines > 0 if newlines < 0 : use newline = ( - newlines ) < leaf info [ 'max height' ] if use newline : sep = ',\\n' if nobraces : body str = sep . join ( itemstrs ) if trailing sep and len ( itemstrs ) > 0 : body str += ',' retstr = body str else : if compact brace : indented = itemstrs else : import ubelt as ub prefix = ' ' * 4 indented = [ ub . indent ( s , prefix ) for s in itemstrs ] body str = sep . join ( indented ) if trailing sep and len ( itemstrs ) > 0 : body str += ',' if compact brace : braced body str = ( lbr + body str . replace ( '\\n' , '\\n ' ) + rbr ) else : braced body str = ( lbr + '\\n' + body str + '\\n' + rbr ) retstr = braced body str else : sep = ',' + itemsep body str = sep . join ( itemstrs ) if trailing sep and len ( itemstrs ) > 0 : body str += ',' retstr = ( lbr + body str + rbr ) return retstr", "predictions": ["declare a single graph"], "references": ["joins string - ified items with separators newlines and container - braces ."], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 679, "code": "def list itemstrs ( list , * * kwargs ) : items = list ( list ) kwargs [ ' return info' ] = True tups = [ repr2 ( item , * * kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in tups ] max height = max ( [ t [ 1 ] [ 'max height' ] for t in tups ] ) if tups else 0 leaf info = { 'max height' : max height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : sort = isinstance ( list , ( set , frozenset ) ) if sort : itemstrs = sort itemstrs ( items , itemstrs ) return itemstrs , leaf info", "predictions": ["declarations a declarations into a declarations new table new by name new height new height"], "references": ["create a string representation for each item in a list ."], "bleu": 0.09103526405546068, "rouge_l": 0.1582360570687419}
{"id": 680, "code": "def register ( self , type ) : def decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func registry [ t ] = func else : self . func registry [ type ] = func return func return decorator", "predictions": ["is a decorator to is a class for a class"], "references": ["registers a custom formatting function with ub . repr2"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 681, "code": "def register numpy extensions ( self ) : import numpy as np numpy floating types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : numpy floating types = numpy floating types + ( np . float128 , ) @ self . add iterable check def is object ndarray ( data ) : return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash numpy array ( data ) : if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise Type Error ( msg ) else : header = b'' . join ( hashable sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( hashable sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def hash numpy int ( data ) : return convert to hashable ( int ( data ) ) @ self . register ( numpy floating types ) def hash numpy float ( data ) : return convert to hashable ( float ( data ) ) @ self . register ( np . random . Random State ) def hash numpy random state ( data ) : hashable = b'' . join ( hashable sequence ( data . get state ( ) ) ) prefix = b'RNG' return prefix , hashable", "predictions": ["col transform extensions extensions"], "references": ["numpy extensions are builtin"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 682, "code": "def proc async iter stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue output ( proc , stream , stream queue ) : while proc . poll ( ) is None : line = stream . readline ( ) stream queue . put ( line ) for line in textio iterlines ( stream ) : stream queue . put ( line ) stream queue . put ( None ) stream queue = queue . Queue ( maxsize = buffersize ) thread = Thread ( target = enqueue output , args = ( proc , stream , stream queue ) ) thread . daemon = True thread . start ( ) return stream queue", "predictions": ["process a out - out iterator over the out - out out ."], "references": ["reads output from a process in a separate thread"], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 683, "code": "def extension module tags ( ) : import sysconfig tags = [ ] if six . PY2 : multiarch = sysconfig . get config var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : tags . append ( sysconfig . get config var ( 'SOABI' ) ) tags . append ( 'abi3' ) tags = [ t for t in tags if t ] return tags", "predictions": ["get list of cols cols cols"], "references": ["returns valid tags an extension module might have"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 684, "code": "def symlink ( path , link , overwrite = 0 , verbose = 0 ) : if exists ( link ) and not os . path . islink ( link ) : if verbose : print ( 'link location already exists' ) is junc = win32 is junction ( link ) if os . path . isdir ( link ) : if is junc : pointed = win32 read junction ( link ) if path == pointed : if verbose : print ( '...and is a junction that points to the same place' ) return link else : if verbose : if not exists ( pointed ) : print ( '...and is a broken junction that points somewhere else' ) else : print ( '...and is a junction that points somewhere else' ) else : if verbose : print ( '...and is an existing real directory!' ) raise IO Error ( 'Cannot overwrite a real directory' ) elif os . path . isfile ( link ) : if win32 is hardlinked ( link , path ) : if verbose : print ( '...and is a hard link that points to the same place' ) return link else : if verbose : print ( '...and is a hard link that points somewhere else' ) if win32 can symlink ( ) : raise IO Error ( 'Cannot overwrite potentially real file if we can symlink' ) if overwrite : if verbose : print ( '...overwriting' ) util io . delete ( link , verbose > 1 ) else : if exists ( link ) : raise IO Error ( 'Link already exists' ) win32 symlink2 ( path , link , verbose = verbose )", "predictions": ["convert a junction file to a raise an exception if not already there ."], "references": ["windows helper for ub . symlink"], "bleu": 0.08839374326825923, "rouge_l": 0.10777385159010601}
{"id": 685, "code": "def win32 dir ( path , star = '' ) : from ubelt import util cmd import re wrapper = 'cmd /S /C \"{}\"' command = 'dir /-C \"{}\"{}' . format ( path , star ) wrapped = wrapper . format ( command ) info = util cmd . cmd ( wrapped , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util format print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util format . repr2 ( info , nl = 1 ) ) raise OS Error ( str ( info ) ) lines = info [ 'out' ] . split ( '\\n' ) [ 5 : - 3 ] splitter = re . compile ( '( +)' ) for line in lines : parts = splitter . split ( line ) date , sep , time , sep , ampm , sep , type or size , sep = parts [ : 8 ] name = '' . join ( parts [ 8 : ] ) if name == '.' or name == '..' : continue if type or size in [ '<JUNCTION>' , '<SYMLINKD>' , '<SYMLINK>' ] : pos = name . find ( ':' ) bpos = name [ : pos ] . rfind ( '[' ) name = name [ : bpos - 1 ] pointed = name [ bpos + 1 : - 1 ] yield type or size , name , pointed else : yield type or size , name , None", "predictions": ["run a transform on a directory"], "references": ["using the windows cmd shell to get information about a directory"], "bleu": 0.12634437832866913, "rouge_l": 0.2234432234432234}
{"id": 686, "code": "def parse ( config ) : if not isinstance ( config , basestring ) : raise Type Error ( \"Contains input must be a simple string\" ) validator = Contains Validator ( ) validator . contains string = config return validator", "predictions": ["score the input config to range . ."], "references": ["parse a contains validator which takes as the config a simple string to find"], "bleu": 0.09008421318929809, "rouge_l": 0.25994318181818177}
{"id": 687, "code": "def retrieve adjacency matrix ( graph , order nodes = None , weight = False ) : if isinstance ( graph , np . ndarray ) : return graph elif isinstance ( graph , nx . Di Graph ) : if order nodes is None : order nodes = graph . nodes ( ) if not weight : return np . array ( nx . adjacency matrix ( graph , order nodes , weight = None ) . todense ( ) ) else : return np . array ( nx . adjacency matrix ( graph , order nodes ) . todense ( ) ) else : raise Type Error ( \"Only networkx.Di Graph and np.ndarray (adjacency matrixes) are supported.\" )", "predictions": ["main method for sdiff a adjacency loader"], "references": ["retrieve the adjacency matrix from the nx . digraph or numpy array ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 688, "code": "def init variables ( self , verbose = False ) : for j in range ( 1 , self . nodes ) : nb parents = np . random . randint ( 0 , min ( [ self . parents max , j ] ) + 1 ) for i in np . random . choice ( range ( 0 , j ) , nb parents , replace = False ) : self . adjacency matrix [ i , j ] = 1 try : self . g = nx . Di Graph ( self . adjacency matrix ) assert not list ( nx . simple cycles ( self . g ) ) except Assertion Error : if verbose : print ( \"Regenerating, graph non valid...\" ) self . init variables ( ) self . cfunctions = [ self . mechanism ( int ( sum ( self . adjacency matrix [ : , i ] ) ) , self . points , self . noise , noise coeff = self . noise coeff ) if sum ( self . adjacency matrix [ : , i ] ) else self . initial generator for i in range ( self . nodes ) ]", "predictions": ["format all options to the map"], "references": ["redefine the causes of the graph ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 689, "code": "def generate ( self , rescale = True ) : if self . cfunctions is None : self . init variables ( ) for i in nx . topological sort ( self . g ) : if not sum ( self . adjacency matrix [ : , i ] ) : self . data [ 'V{}' . format ( i ) ] = self . cfunctions [ i ] ( self . points ) else : self . data [ 'V{}' . format ( i ) ] = self . cfunctions [ i ] ( self . data . iloc [ : , self . adjacency matrix [ : , i ] . nonzero ( ) [ 0 ] ] . values ) if rescale : self . data [ 'V{}' . format ( i ) ] = scale ( self . data [ 'V{}' . format ( i ) ] . values ) return self . g , self . data", "predictions": ["generates a random is in the is selected"], "references": ["generate data from an fcm containing cycles ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 690, "code": "def graph evaluation ( data , adj matrix , gpu = None , gpu id = 0 , * * kwargs ) : gpu = SETTINGS . get default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu id ) if gpu else 'cpu' obs = th . Float Tensor ( data ) . to ( device ) cgnn = CGNN model ( adj matrix , data . shape [ 0 ] , gpu id = gpu id , * * kwargs ) . to ( device ) cgnn . reset parameters ( ) return cgnn . run ( obs , * * kwargs )", "predictions": ["evaluation is a graph for the current other other other other other other other other other other other client client"], "references": ["evaluate a graph taking account of the hardware ."], "bleu": 0.08638804535733371, "rouge_l": 0.22208737864077668}
{"id": 691, "code": "def parallel graph evaluation ( data , adj matrix , nb runs = 16 , nb jobs = None , * * kwargs ) : nb jobs = SETTINGS . get default ( nb jobs = nb jobs ) if nb runs == 1 : return graph evaluation ( data , adj matrix , * * kwargs ) else : output = Parallel ( n jobs = nb jobs ) ( delayed ( graph evaluation ) ( data , adj matrix , idx = run , gpu id = run % SETTINGS . GPU , * * kwargs ) for run in range ( nb runs ) ) return np . mean ( output )", "predictions": ["data kwargs are data from last interval . ."], "references": ["parallelize the various runs of cgnn to evaluate a graph ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 692, "code": "def forward ( self ) : self . noise . data . normal ( ) if not self . confounding : for i in self . topological order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency matrix [ : , i ] ) [ 0 ] ] , [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) else : for i in self . topological order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency matrix [ : , i ] ) [ 0 ] ] , [ self . corr noise [ min ( i , j ) , max ( i , j ) ] for j in np . nonzero ( self . i adj matrix [ : , i ] ) [ 0 ] ] [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) return th . cat ( self . generated , 1 )", "predictions": ["lookup the lookup of the = 1"], "references": ["generate according to the topological order of the graph ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 693, "code": "def run ( self , data , train epochs = 1000 , test epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , * * kwargs ) : verbose = SETTINGS . get default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero ( ) with trange ( train epochs + test epochs , disable = not verbose ) as t : for epoch in t : optim . zero grad ( ) generated data = self . forward ( ) mmd = self . criterion ( generated data , data ) if not epoch % 200 : t . set postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test epochs : self . score . add ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test epochs", "predictions": ["version of galaxy s version"], "references": ["run the cgnn on a given graph ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 694, "code": "def run gies ( self , data , fixed Gaps = None , verbose = True ) : id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt gies' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt gies' + id + '/' def retrieve result ( ) : return read csv ( '/tmp/cdt gies' + id + '/result.csv' , delimiter = ',' ) . values try : data . to csv ( '/tmp/cdt gies' + id + '/data.csv' , header = False , index = False ) if fixed Gaps is not None : fixed Gaps . to csv ( '/tmp/cdt gies' + id + '/fixedgaps.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' gies result = launch R script ( \"{}/R templates/gies.R\" . format ( os . path . dirname ( os . path . realpath ( file ) ) ) , self . arguments , output function = retrieve result , verbose = verbose ) except Exception as e : rmtree ( '/tmp/cdt gies' + id + '' ) raise e except Keyboard Interrupt : rmtree ( '/tmp/cdt gies' + id + '/' ) raise Keyboard Interrupt rmtree ( '/tmp/cdt gies' + id + '' ) return gies result", "predictions": ["echo the result and all its data bold bold"], "references": ["setting up and running gies with all arguments ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 695, "code": "def plot curves ( i batch , adv loss , gen loss , l1 reg , cols ) : from matplotlib import pyplot as plt if i batch == 0 : try : ax . clear ( ) ax . plot ( range ( len ( adv plt ) ) , adv plt , \"r-\" , linewidth = 1.5 , markersize = 4 , label = \"Discriminator\" ) ax . plot ( range ( len ( adv plt ) ) , gen plt , \"g-\" , linewidth = 1.5 , markersize = 4 , label = \"Generators\" ) ax . plot ( range ( len ( adv plt ) ) , l1 plt , \"b-\" , linewidth = 1.5 , markersize = 4 , label = \"L1-Regularization\" ) plt . legend ( ) adv plt . append ( adv loss . cpu ( ) . data [ 0 ] ) gen plt . append ( gen loss . cpu ( ) . data [ 0 ] / cols ) l1 plt . append ( l1 reg . cpu ( ) . data [ 0 ] ) plt . pause ( 0.0001 ) except Name Error : plt . ion ( ) fig , ax = plt . figure ( ) plt . xlabel ( \"Epoch\" ) plt . ylabel ( \"Losses\" ) plt . pause ( 0.0001 ) adv plt = [ adv loss . cpu ( ) . data [ 0 ] ] gen plt = [ gen loss . cpu ( ) . data [ 0 ] / cols ] l1 plt = [ l1 reg . cpu ( ) . data [ 0 ] ] else : adv plt . append ( adv loss . cpu ( ) . data [ 0 ] ) gen plt . append ( gen loss . cpu ( ) . data [ 0 ] / cols ) l1 plt . append ( l1 reg . cpu ( ) . data [ 0 ] )", "predictions": ["login to curves and id"], "references": ["plot sam s various losses ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 696, "code": "def plot gen ( epoch , batch , generated variables , pairs to plot = [ [ 0 , 1 ] ] ) : from matplotlib import pyplot as plt if epoch == 0 : plt . ion ( ) plt . clf ( ) for ( i , j ) in pairs to plot : plt . scatter ( generated variables [ i ] . data . cpu ( ) . numpy ( ) , batch . data . cpu ( ) . numpy ( ) [ : , j ] , label = \"Y -> X\" ) plt . scatter ( batch . data . cpu ( ) . numpy ( ) [ : , i ] , generated variables [ j ] . data . cpu ( ) . numpy ( ) , label = \"X -> Y\" ) plt . scatter ( batch . data . cpu ( ) . numpy ( ) [ : , i ] , batch . data . cpu ( ) . numpy ( ) [ : , j ] , label = \"original data\" ) plt . legend ( ) plt . pause ( 0.01 )", "predictions": ["convert a series of epoch to a single param path"], "references": ["plot generated pairs of variables ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 697, "code": "def reset parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform ( - stdv , stdv )", "predictions": ["set the display display display display and name . . . . . . ."], "references": ["reset the parameters ."], "bleu": 0.09103526405546068, "rouge_l": 0.2350674373795761}
{"id": 698, "code": "def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )", "predictions": ["computes the separate separate separate separate"], "references": ["feed - forward through the network ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 699, "code": "def forward ( self , x ) : return self . layers ( x * ( self . filter * self . fs filter ) . expand as ( x ) )", "predictions": ["return format at x if necessary"], "references": ["feed - forward the model ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 700, "code": "def forward ( self , x ) : for i in self . noise : i . data . normal ( ) self . generated variables = [ self . blocks [ i ] ( th . cat ( [ x , self . noise [ i ] ] , 1 ) ) for i in range ( self . cols ) ] return self . generated variables", "predictions": ["list of file variables"], "references": ["feed - forward the model ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 701, "code": "def run pc ( self , data , fixed Edges = None , fixed Gaps = None , verbose = True ) : if ( self . arguments [ '{CITEST}' ] == self . dir CI test [ 'hsic' ] and self . arguments [ '{METHOD INDEP}' ] == self . dir method indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the hsic test,' ' setting the hsic.gamma method.' ) self . arguments [ '{METHOD INDEP}' ] = self . dir method indep [ 'hsic gamma' ] elif ( self . arguments [ '{CITEST}' ] == self . dir CI test [ 'gaussian' ] and self . arguments [ '{METHOD INDEP}' ] != self . dir method indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the selected test,' ' setting the classic correlation-based method.' ) self . arguments [ '{METHOD INDEP}' ] = self . dir method indep [ 'corr' ] id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt pc' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt pc' + id + '/' def retrieve result ( ) : return read csv ( '/tmp/cdt pc' + id + '/result.csv' , delimiter = ',' ) . values try : data . to csv ( '/tmp/cdt pc' + id + '/data.csv' , header = False , index = False ) if fixed Gaps is not None and fixed Edges is not None : fixed Gaps . to csv ( '/tmp/cdt pc' + id + '/fixedgaps.csv' , index = False , header = False ) fixed Edges . to csv ( '/tmp/cdt pc' + id + '/fixededges.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' pc result = launch R script ( \"{}/R templates/pc.R\" . format ( os . path . dirname ( os . path . realpath ( file ) ) ) , self . arguments , output function = retrieve result , verbose = verbose ) except Exception as e : rmtree ( '/tmp/cdt pc' + id + '' ) raise e except Keyboard Interrupt : rmtree ( '/tmp/cdt pc' + id + '/' ) raise Keyboard Interrupt rmtree ( '/tmp/cdt pc' + id + '' ) return pc result", "predictions": ["new state machine machine"], "references": ["setting up and running pc with all arguments ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 702, "code": "def compute Gauss Kernel ( x ) : xnorm = np . power ( euclidean distances ( x , x ) , 2 ) return np . exp ( - xnorm / ( 2.0 ) )", "predictions": ["request the gauss coefficient"], "references": ["compute the gaussian kernel on a 1d vector ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 703, "code": "def normal noise ( points ) : return np . random . rand ( 1 ) * np . random . randn ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )", "predictions": ["noise noise noise noise"], "references": ["init a noise variable ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 704, "code": "def uniform noise ( points ) : return np . random . rand ( 1 ) * np . random . uniform ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )", "predictions": ["noise noise noise ."], "references": ["init a uniform noise variable ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 705, "code": "def run ( self , x , y , lr = 0.01 , train epochs = 1000 , test epochs = 1000 , idx = 0 , verbose = None , * * kwargs ) : verbose = SETTINGS . get default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) running loss = 0 teloss = 0 for i in range ( train epochs + test epochs ) : optim . zero grad ( ) pred = self . forward ( x ) loss = self . criterion ( pred , y ) running loss += loss . item ( ) if i < train epochs : loss . backward ( ) optim . step ( ) else : teloss += running loss if verbose and not i % 300 : print ( 'Idx:{}; epoch:{}; score:{}' . format ( idx , i , running loss / 300 ) ) running loss = 0.0 return teloss / test epochs", "predictions": ["run the calculation on the provided x y and test set ."], "references": ["run the gnn on a pair x y of floattensor data ."], "bleu": 0.1870361278311548, "rouge_l": 0.5}
{"id": 706, "code": "def init variables ( self , verbose = False ) : for i in range ( self . nodes ) : for j in np . random . choice ( range ( self . nodes ) , np . random . randint ( 0 , self . parents max + 1 ) , replace = False ) : if i != j : self . adjacency matrix [ j , i ] = 1 try : assert any ( [ sum ( self . adjacency matrix [ : , i ] ) == self . parents max for i in range ( self . nodes ) ] ) self . g = nx . Di Graph ( self . adjacency matrix ) assert list ( nx . simple cycles ( self . g ) ) assert any ( len ( i ) == 2 for i in nx . simple cycles ( self . g ) ) except Assertion Error : if verbose : print ( \"Regenerating, graph non valid...\" ) self . init variables ( ) if verbose : print ( . format ( len ( list ( nx . simple cycles ( self . g ) ) ) ) ) for i in range ( self . nodes ) : self . data . iloc [ : , i ] = scale ( self . initial generator ( self . points ) ) self . cfunctions = [ self . mechanism ( int ( sum ( self . adjacency matrix [ : , i ] ) ) , self . points , self . noise , noise coeff = self . noise coeff ) for i in range ( self . nodes ) ]", "predictions": ["init the graph variables"], "references": ["redefine the causes of the graph ."], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 707, "code": "def generate ( self , nb steps = 100 , averaging = 50 , rescale = True ) : if self . cfunctions is None : self . init variables ( ) new df = pd . Data Frame ( ) causes = [ [ c for c in np . nonzero ( self . adjacency matrix [ : , j ] ) [ 0 ] ] for j in range ( self . nodes ) ] values = [ [ ] for i in range ( self . nodes ) ] for i in range ( nb steps ) : for j in range ( self . nodes ) : new df [ \"V\" + str ( j ) ] = self . cfunctions [ j ] ( self . data . iloc [ : , causes [ j ] ] . values ) [ : , 0 ] if rescale : new df [ \"V\" + str ( j ) ] = scale ( new df [ \"V\" + str ( j ) ] ) if i > nb steps - averaging : values [ j ] . append ( new df [ \"V\" + str ( j ) ] ) self . data = new df self . data = pd . Data Frame ( np . array ( [ np . mean ( values [ i ] , axis = 0 ) for i in range ( self . nodes ) ] ) . transpose ( ) , columns = [ \"V{}\" . format ( j ) for j in range ( self . nodes ) ] ) return self . g , self . data", "predictions": ["generate a dataframe of data for each node in a pandas series"], "references": ["generate data from an fcm containing cycles ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 708, "code": "def phrase to filename ( self , phrase ) : name = re . sub ( r\"[^\\w\\s\\.]\" , '' , phrase . strip ( ) . lower ( ) ) name = re . sub ( r\"\\s+\" , ' ' , name ) return name + '.png'", "predictions": ["convert a phrase phrase phrase to a filename ."], "references": ["convert phrase to normilized file name ."], "bleu": 0.21105340631872635, "rouge_l": 0.5115303983228512}
{"id": 709, "code": "def wait for page to load ( self ) : self . wait . until ( lambda : self . loaded ) self . pm . hook . pypom after wait for page to load ( page = self ) return self", "predictions": ["wait for all pages to finish in the page ."], "references": ["wait for the page to load ."], "bleu": 0.22692039365038064, "rouge_l": 0.6075697211155379}
{"id": 710, "code": "def wait for region to load ( self ) : self . wait . until ( lambda : self . loaded ) self . pm . hook . pypom after wait for region to load ( region = self ) return self", "predictions": ["wait for all region to finish ."], "references": ["wait for the page region to load ."], "bleu": 0.2789001430384383, "rouge_l": 0.6587473002159828}
{"id": 711, "code": "def register Driver ( iface , driver , class implements = [ ] ) : for class item in class implements : class Implements ( class item , iface ) component . provide Adapter ( factory = driver , adapts = [ iface ] , provides = I Driver )", "predictions": ["register the given class with the given class ."], "references": ["register driver adapter used by page object"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 712, "code": "def pre install ( ) : dat = join ( setup dir , 'src' , 'hcl' , 'parsetab.dat' ) if exists ( dat ) : os . unlink ( dat ) sys . path . insert ( 0 , join ( setup dir , 'src' ) ) import hcl from hcl . parser import Hcl Parser parser = Hcl Parser ( )", "predictions": ["install and re - install the build environment ."], "references": ["initialize the parse table at install time"], "bleu": 0.15619699684601276, "rouge_l": 0.1278825995807128}
{"id": 713, "code": "def append ( self , linenumber , raw text , cells ) : self . rows . append ( Row ( linenumber , raw text , cells ) )", "predictions": ["append a text to the end of the list of cells ."], "references": ["add another row of data from a test suite"], "bleu": 0.11498759556447223, "rouge_l": 0.09775641025641024}
{"id": 714, "code": "def is comment ( self ) : for cell in self [ : ] : if cell == \"\" : continue if cell . lstrip ( ) . startswith ( \"#\" ) : return True else : return False return False", "predictions": ["returns true if the cell is a comment ."], "references": ["return true if the first non - empty cell starts with #"], "bleu": 0.19902510067151713, "rouge_l": 0.3713850837138508}
{"id": 715, "code": "def keywords ( self ) : for table in self . tables : if isinstance ( table , Keyword Table ) : for keyword in table . keywords : yield keyword", "predictions": ["iterate over all keywords ."], "references": ["generator which returns all keywords in the suite"], "bleu": 0.1971902775417715, "rouge_l": 0.2953995157384988}
{"id": 716, "code": "def dump ( self ) : for table in self . tables : print ( \"*** %s ***\" % table . name ) table . dump ( )", "predictions": ["dump all tables to a file ."], "references": ["regurgitate the tables and rows"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 717, "code": "def settings ( self ) : for table in self . tables : if isinstance ( table , Setting Table ) : for statement in table . statements : yield statement", "predictions": ["iterate over all settings defined in the context ."], "references": ["generator which returns all of the statements in all of the settings tables"], "bleu": 0.11379288211086455, "rouge_l": 0.26406926406926406}
{"id": 718, "code": "def variables ( self ) : for table in self . tables : if isinstance ( table , Variable Table ) : for statement in table . rows : if statement [ 0 ] != \"\" : yield statement", "predictions": ["iterate over all variables in the table ."], "references": ["generator which returns all of the statements in all of the variables tables"], "bleu": 0.10793517579160734, "rouge_l": 0.2739520958083832}
{"id": 719, "code": "def report ( self , obj , message , linenum , char offset = 0 ) : self . controller . report ( linenumber = linenum , filename = obj . path , severity = self . severity , message = message , rulename = self . class . name , char = char offset )", "predictions": ["report a report on the controller ."], "references": ["report an error or warning"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 720, "code": "def run ( self , args ) : self . args = self . parse and process args ( args ) if self . args . version : print ( version ) return 0 if self . args . rulefile : for filename in self . args . rulefile : self . load rule file ( filename ) if self . args . list : self . list rules ( ) return 0 if self . args . describe : self . describe rules ( self . args . args ) return 0 self . counts = { ERROR : 0 , WARNING : 0 , \"other\" : 0 } for filename in self . args . args : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( \"rflint: %s: No such file or directory\\n\" % filename ) continue if os . path . isdir ( filename ) : self . process folder ( filename ) else : self . process file ( filename ) if self . counts [ ERROR ] > 0 : return self . counts [ ERROR ] if self . counts [ ERROR ] < 254 else 255 return 0", "predictions": ["run the list of rules"], "references": ["parse command line arguments and run rflint"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 721, "code": "def list rules ( self ) : for rule in sorted ( self . all rules , key = lambda rule : rule . name ) : print ( rule ) if self . args . verbose : for line in rule . doc . split ( \"\\n\" ) : print ( \"    \" , line )", "predictions": ["list all registered rules ."], "references": ["print a list of all rules"], "bleu": 0.2658156069371863, "rouge_l": 0.5366568914956013}
{"id": 722, "code": "def report ( self , linenumber , filename , severity , message , rulename , char ) : if self . print filename is not None : print ( \"+ \" + self . print filename ) self . print filename = None if severity in ( WARNING , ERROR ) : self . counts [ severity ] += 1 else : self . counts [ \"other\" ] += 1 print ( self . args . format . format ( linenumber = linenumber , filename = filename , severity = severity , message = message . encode ( 'utf-8' ) , rulename = rulename , char = char ) )", "predictions": ["report a report of the pop pop ."], "references": ["report a rule violation"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 723, "code": "def load rule file ( self , filename ) : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( \"rflint: %s: No such file or directory\\n\" % filename ) return try : basename = os . path . basename ( filename ) ( name , ext ) = os . path . splitext ( basename ) imp . load source ( name , filename ) except Exception as e : sys . stderr . write ( \"rflint: %s: exception while loading: %s\\n\" % ( filename , str ( e ) ) )", "predictions": ["load a rule from disk"], "references": ["import the given rule file"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 724, "code": "def parse and process args ( self , args ) : parser = argparse . Argument Parser ( prog = \"python -m rflint\" , description = \"A style checker for robot framework plain text files.\" , formatter class = argparse . Raw Description Help Formatter , epilog = ( \"You can use 'all' in place of RULENAME to refer to all rules. \\n\" \"\\n\" \"For example: '--ignore all --warn Duplicate Test Names' will ignore all\\n\" \"rules except Duplicate Test Names.\\n\" \"\\n\" \"FORMAT is a string that performs a substitution on the following \\n\" \"patterns: {severity}, {linenumber}, {char}, {message}, and {rulename}.\\n\" \"\\n\" \"For example: --format 'line: {linenumber}: message: {message}'. \\n\" \"\\n\" \"ARGUMENTFILE is a filename with contents that match the format of \\n\" \"standard robot framework argument files\\n\" \"\\n\" \"If you give a directory as an argument, all files in the directory\\n\" \"with the suffix .txt, .robot or .tsv will be processed. With the \\n\" \"--recursive option, subfolders within the directory will also be\\n\" \"processed.\" ) ) parser . add argument ( \"--error\" , \"-e\" , metavar = \"RULENAME\" , action = Set Error Action , help = \"Assign a severity of ERROR to the given RULENAME\" ) parser . add argument ( \"--ignore\" , \"-i\" , metavar = \"RULENAME\" , action = Set Ignore Action , help = \"Ignore the given RULENAME\" ) parser . add argument ( \"--warning\" , \"-w\" , metavar = \"RULENAME\" , action = Set Warning Action , help = \"Assign a severity of WARNING for the given RULENAME\" ) parser . add argument ( \"--list\" , \"-l\" , action = \"store true\" , help = \"show a list of known rules and exit\" ) parser . add argument ( \"--describe\" , \"-d\" , action = \"store true\" , help = \"describe the given rules\" ) parser . add argument ( \"--no-filenames\" , action = \"store false\" , dest = \"print filenames\" , default = True , help = \"suppress the printing of filenames\" ) parser . add argument ( \"--format\" , \"-f\" , help = \"Define the output format\" , default = '{severity}: {linenumber}, {char}: {message} ({rulename})' ) parser . add argument ( \"--version\" , action = \"store true\" , default = False , help = \"Display version number and exit\" ) parser . add argument ( \"--verbose\" , \"-v\" , action = \"store true\" , default = False , help = \"Give verbose output\" ) parser . add argument ( \"--configure\" , \"-c\" , action = Configure Action , help = \"Configure a rule\" ) parser . add argument ( \"--recursive\" , \"-r\" , action = \"store true\" , default = False , help = \"Recursively scan subfolders in a directory\" ) parser . add argument ( \"--rulefile\" , \"-R\" , action = Rulefile Action , help = \"import additional rules from the given RULEFILE\" ) parser . add argument ( \"--argumentfile\" , \"-A\" , action = Argfile Loader , help = \"read arguments from the given file\" ) parser . add argument ( 'args' , metavar = \"file\" , nargs = argparse . REMAINDER ) ns = argparse . Namespace ( ) setattr ( ns , \"app\" , self ) args = parser . parse args ( args , ns ) Rule . output format = args . format return args", "predictions": ["parse the args ."], "references": ["handle the parsing of command line arguments ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 725, "code": "def read yaml file ( path , loader = Extended Safe Loader ) : with open ( path ) as fh : return load ( fh , loader )", "predictions": ["read a yaml file into a yaml object"], "references": ["open a file read it and return its contents ."], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 726, "code": "def f7 ( seq ) : seen = set ( ) seen add = seen . add return [ x for x in seq if x not in seen and not seen add ( x ) ]", "predictions": ["return a set of all elements of a sequence ."], "references": ["makes a list unique"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 727, "code": "def count list ( the list ) : count = the list . count result = [ ( item , count ( item ) ) for item in set ( the list ) ] result . sort ( ) return result", "predictions": ["return the number of items in a list ."], "references": ["generates a count of the number of times each unique item appears in a list"], "bleu": 0.19505632433269743, "rouge_l": 0.47843137254901963}
{"id": 728, "code": "def write creation info ( creation info , out ) : out . write ( ) for creator in sorted ( creation info . creators ) : write value ( 'Creator' , creator , out ) write value ( 'Created' , creation info . created iso format , out ) if creation info . has comment : write text value ( 'Creator Comment' , creation info . comment , out )", "predictions": ["write creation information ."], "references": ["write the creation info to out ."], "bleu": 0.20183609024241697, "rouge_l": 0.5198863636363635}
{"id": 729, "code": "def write review ( review , out ) : out . write ( ) write value ( 'Reviewer' , review . reviewer , out ) write value ( 'Review Date' , review . review date iso format , out ) if review . has comment : write text value ( 'Review Comment' , review . comment , out )", "predictions": ["write a review ."], "references": ["write the fields of a single review to out ."], "bleu": 0.10081042988795336, "rouge_l": 0.5304347826086957}
{"id": 730, "code": "def write annotation ( annotation , out ) : out . write ( ) write value ( 'Annotator' , annotation . annotator , out ) write value ( 'Annotation Date' , annotation . annotation date iso format , out ) if annotation . has comment : write text value ( 'Annotation Comment' , annotation . comment , out ) write value ( 'Annotation Type' , annotation . annotation type , out ) write value ( 'SPDXREF' , annotation . spdx id , out )", "predictions": ["write an annotation file ."], "references": ["write the fields of a single annotation to out ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 731, "code": "def write file ( spdx file , out ) : out . write ( ) write value ( 'File Name' , spdx file . name , out ) write value ( 'SPDXID' , spdx file . spdx id , out ) if spdx file . has optional field ( 'type' ) : write file type ( spdx file . type , out ) write value ( 'File Checksum' , spdx file . chk sum . to tv ( ) , out ) if isinstance ( spdx file . conc lics , ( document . License Conjunction , document . License Disjunction ) ) : write value ( 'License Concluded' , u'({0})' . format ( spdx file . conc lics ) , out ) else : write value ( 'License Concluded' , spdx file . conc lics , out ) for lics in sorted ( spdx file . licenses in file ) : write value ( 'License Info In File' , lics , out ) if isinstance ( spdx file . copyright , six . string types ) : write text value ( 'File Copyright Text' , spdx file . copyright , out ) else : write value ( 'File Copyright Text' , spdx file . copyright , out ) if spdx file . has optional field ( 'license comment' ) : write text value ( 'License Comments' , spdx file . license comment , out ) if spdx file . has optional field ( 'comment' ) : write text value ( 'File Comment' , spdx file . comment , out ) if spdx file . has optional field ( 'notice' ) : write text value ( 'File Notice' , spdx file . notice , out ) for contributor in sorted ( spdx file . contributors ) : write value ( 'File Contributor' , contributor , out ) for dependency in sorted ( spdx file . dependencies ) : write value ( 'File Dependency' , dependency , out ) names = spdx file . artifact of project name homepages = spdx file . artifact of project home uris = spdx file . artifact of project uri for name , homepage , uri in sorted ( zip longest ( names , homepages , uris ) ) : write value ( 'Artifact Of Project Name' , name , out ) if homepage is not None : write value ( 'Artifact Of Project Home Page' , homepage , out ) if uri is not None : write value ( 'Artifact Of Project URI' , uri , out )", "predictions": ["write a single file to a single project ."], "references": ["write a file fields to out ."], "bleu": 0.2208959113415788, "rouge_l": 0.639412997903564}
{"id": 732, "code": "def write package ( package , out ) : out . write ( ) write value ( 'Package Name' , package . name , out ) if package . has optional field ( 'version' ) : write value ( 'Package Version' , package . version , out ) write value ( 'Package Download Location' , package . download location , out ) if package . has optional field ( 'summary' ) : write text value ( 'Package Summary' , package . summary , out ) if package . has optional field ( 'source info' ) : write text value ( 'Package Source Info' , package . source info , out ) if package . has optional field ( 'file name' ) : write value ( 'Package File Name' , package . file name , out ) if package . has optional field ( 'supplier' ) : write value ( 'Package Supplier' , package . supplier , out ) if package . has optional field ( 'originator' ) : write value ( 'Package Originator' , package . originator , out ) if package . has optional field ( 'check sum' ) : write value ( 'Package Checksum' , package . check sum . to tv ( ) , out ) write value ( 'Package Verification Code' , format verif code ( package ) , out ) if package . has optional field ( 'description' ) : write text value ( 'Package Description' , package . description , out ) if isinstance ( package . license declared , ( document . License Conjunction , document . License Disjunction ) ) : write value ( 'Package License Declared' , u'({0})' . format ( package . license declared ) , out ) else : write value ( 'Package License Declared' , package . license declared , out ) if isinstance ( package . conc lics , ( document . License Conjunction , document . License Disjunction ) ) : write value ( 'Package License Concluded' , u'({0})' . format ( package . conc lics ) , out ) else : write value ( 'Package License Concluded' , package . conc lics , out ) for lics in sorted ( package . licenses from files ) : write value ( 'Package License Info From Files' , lics , out ) if package . has optional field ( 'license comment' ) : write text value ( 'Package License Comments' , package . license comment , out ) if isinstance ( package . cr text , six . string types ) : write text value ( 'Package Copyright Text' , package . cr text , out ) else : write value ( 'Package Copyright Text' , package . cr text , out ) if package . has optional field ( 'homepage' ) : write value ( 'Package Home Page' , package . homepage , out ) for spdx file in sorted ( package . files ) : write separators ( out ) write file ( spdx file , out )", "predictions": ["write a package ."], "references": ["write a package fields to out ."], "bleu": 0.3340135926488844, "rouge_l": 0.693181818181818}
{"id": 733, "code": "def write extracted licenses ( lics , out ) : write value ( 'License ID' , lics . identifier , out ) if lics . full name is not None : write value ( 'License Name' , lics . full name , out ) if lics . comment is not None : write text value ( 'License Comment' , lics . comment , out ) for xref in sorted ( lics . cross ref ) : write value ( 'License Cross Reference' , xref , out ) write text value ( 'Extracted Text' , lics . text , out )", "predictions": ["write the extracted licenses ."], "references": ["write extracted licenses fields to out ."], "bleu": 0.2736570128577077, "rouge_l": 0.6472148541114059}
{"id": 734, "code": "def str from text ( text ) : REGEX = re . compile ( '<text>((.|\\n)+)</text>' , re . UNICODE ) match = REGEX . match ( text ) if match : return match . group ( 1 ) else : return None", "predictions": ["parse a string and return the text representation of a text string ."], "references": ["return content of a free form text block as a string ."], "bleu": 0.19104081613647536, "rouge_l": 0.4834874504623514}
{"id": 735, "code": "def reset document ( self ) : self . doc version set = False self . doc comment set = False self . doc namespace set = False self . doc data lics set = False self . doc name set = False self . doc spdx id set = False", "predictions": ["reset all internal properties ."], "references": ["resets the state to allow building new documents"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 736, "code": "def reset creation info ( self ) : self . created date set = False self . creation comment set = False self . lics list ver set = False", "predictions": ["uniform all noise variables"], "references": ["resets builder state to allow building new creation info ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 737, "code": "def reset annotations ( self ) : self . annotation date set = False self . annotation comment set = False self . annotation type set = False self . annotation spdx id set = False", "predictions": ["run all annotations s annotations lr to its initial state lr lr lr lr lr lr lr lr lr lr lr lr lr lr lr lr lr lr lr lr lr"], "references": ["resets the builder s state to allow building new annotations ."], "bleu": 0.04906081629292276, "rouge_l": 0.10418445772843724}
{"id": 738, "code": "def reset package ( self ) : self . package set = False self . package vers set = False self . package file name set = False self . package supplier set = False self . package originator set = False self . package down location set = False self . package home set = False self . package verif set = False self . package chk sum set = False self . package source info set = False self . package conc lics set = False self . package license declared set = False self . package license comment set = False self . package cr text set = False self . package summary set = False self . package desc set = False", "predictions": ["init the variables of the variables ."], "references": ["resets the builder s state in order to build new packages ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 739, "code": "def set file name ( self , doc , name ) : if self . has package ( doc ) : doc . package . files . append ( file . File ( name ) ) self . reset file stat ( ) return True else : raise Order Error ( 'File::Name' )", "predictions": ["sets a file s file to the given self rescale rescale file rescale"], "references": ["raises ordererror if no package defined ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 740, "code": "def add file contribution ( self , doc , value ) : if self . has package ( doc ) and self . has file ( doc ) : self . file ( doc ) . add contrib ( value ) else : raise Order Error ( 'File::Contributor' )", "predictions": ["phrase for when a to phrase changes"], "references": ["raises ordererror if no package or file defined ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 741, "code": "def add file dep ( self , doc , value ) : if self . has package ( doc ) and self . has file ( doc ) : self . file ( doc ) . add depend ( value ) else : raise Order Error ( 'File::Dependency' )", "predictions": ["wait for a for a for a for a for a given self lambda lambda lambda lambda lambda lambda lambda self lambda lambda lambda lambda lambda self lambda lambda lambda lambda"], "references": ["raises ordererror if no package or file defined ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 742, "code": "def reset file stat ( self ) : self . file spdx id set = False self . file comment set = False self . file type set = False self . file chksum set = False self . file conc lics set = False self . file license comment set = False self . file notice set = False self . file copytext set = False", "predictions": ["wait for all for the for a for a for a for a for a for a for a for the for the for a for a for a for a"], "references": ["resets the builder s state to enable building new files ."], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 743, "code": "def datetime iso format ( date ) : return \"{0:0>4}-{1:0>2}-{2:0>2}T{3:0>2}:{4:0>2}:{5:0>2}Z\" . format ( date . year , date . month , date . day , date . hour , date . minute , date . second )", "predictions": ["implements the register iso format format for the given date"], "references": ["return an iso - 8601 representation of a datetime object ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 744, "code": "def build ( self , * * kwargs ) : self . yacc = yacc . yacc ( module = self , * * kwargs )", "predictions": ["pre - connect dir for dir setup setup setup setup setup setup setup setup setup setup setup setup setup dir setup dir"], "references": ["must be called before parse ."], "bleu": 0.04657469807170698, "rouge_l": 0.0}
{"id": 745, "code": "def parse ( self , data ) : try : return self . yacc . parse ( data , lexer = self . lex ) except : return None", "predictions": ["append the . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["parses a license list and returns a license or none if it failed ."], "bleu": 0.03901663112717908, "rouge_l": 0.04769351055512119}
{"id": 746, "code": "def create checksum node ( self , chksum ) : chksum node = B Node ( ) type triple = ( chksum node , RDF . type , self . spdx namespace . Checksum ) self . graph . add ( type triple ) algorithm triple = ( chksum node , self . spdx namespace . algorithm , Literal ( chksum . identifier ) ) self . graph . add ( algorithm triple ) value triple = ( chksum node , self . spdx namespace . checksum Value , Literal ( chksum . value ) ) self . graph . add ( value triple ) return chksum node", "predictions": ["is a comment node"], "references": ["return a node representing spdx . checksum ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 747, "code": "def to special value ( self , value ) : if isinstance ( value , utils . No Assert ) : return self . spdx namespace . noassertion elif isinstance ( value , utils . SPDX None ) : return self . spdx namespace . none else : return Literal ( value )", "predictions": ["converts a single self . to a mongodb self . . ."], "references": ["return proper spdx term or literal"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 748, "code": "def create conjunction node ( self , conjunction ) : node = B Node ( ) type triple = ( node , RDF . type , self . spdx namespace . Conjunctive License Set ) self . graph . add ( type triple ) licenses = self . licenses from tree ( conjunction ) for lic in licenses : member triple = ( node , self . spdx namespace . member , lic ) self . graph . add ( member triple ) return node", "predictions": ["create a conjunction self % with a conjunction"], "references": ["return a node representing a conjunction of licenses ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 749, "code": "def create disjunction node ( self , disjunction ) : node = B Node ( ) type triple = ( node , RDF . type , self . spdx namespace . Disjunctive License Set ) self . graph . add ( type triple ) licenses = self . licenses from tree ( disjunction ) for lic in licenses : member triple = ( node , self . spdx namespace . member , lic ) self . graph . add ( member triple ) return node", "predictions": ["settings settings to create a disjunction self . ."], "references": ["return a node representing a disjunction of licenses ."], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 750, "code": "def create file node ( self , doc file ) : file node = URI Ref ( 'http://www.spdx.org/files#{id}' . format ( id = str ( doc file . spdx id ) ) ) type triple = ( file node , RDF . type , self . spdx namespace . File ) self . graph . add ( type triple ) name triple = ( file node , self . spdx namespace . file Name , Literal ( doc file . name ) ) self . graph . add ( name triple ) if doc file . has optional field ( 'comment' ) : comment triple = ( file node , RDFS . comment , Literal ( doc file . comment ) ) self . graph . add ( comment triple ) if doc file . has optional field ( 'type' ) : ftype = self . spdx namespace [ self . FILE TYPES [ doc file . type ] ] ftype triple = ( file node , self . spdx namespace . file Type , ftype ) self . graph . add ( ftype triple ) self . graph . add ( ( file node , self . spdx namespace . checksum , self . create checksum node ( doc file . chk sum ) ) ) conc lic node = self . license or special ( doc file . conc lics ) conc lic triple = ( file node , self . spdx namespace . license Concluded , conc lic node ) self . graph . add ( conc lic triple ) license info nodes = map ( self . license or special , doc file . licenses in file ) for lic in license info nodes : triple = ( file node , self . spdx namespace . license Info In File , lic ) self . graph . add ( triple ) if doc file . has optional field ( 'license comment' ) : comment triple = ( file node , self . spdx namespace . license Comments , Literal ( doc file . license comment ) ) self . graph . add ( comment triple ) cr text node = self . to special value ( doc file . copyright ) cr text triple = ( file node , self . spdx namespace . copyright Text , cr text node ) self . graph . add ( cr text triple ) if doc file . has optional field ( 'notice' ) : notice triple = ( file node , self . spdx namespace . notice Text , doc file . notice ) self . graph . add ( notice triple ) contrib nodes = map ( lambda c : Literal ( c ) , doc file . contributors ) contrib triples = [ ( file node , self . spdx namespace . file Contributor , node ) for node in contrib nodes ] for triple in contrib triples : self . graph . add ( triple ) return file node", "predictions": ["create a file self self . ."], "references": ["create a node for spdx . file ."], "bleu": 0.25201472805660513, "rouge_l": 0.5269978401727862}
{"id": 751, "code": "def create review node ( self , review ) : review node = B Node ( ) type triple = ( review node , RDF . type , self . spdx namespace . Review ) self . graph . add ( type triple ) reviewer node = Literal ( review . reviewer . to value ( ) ) self . graph . add ( ( review node , self . spdx namespace . reviewer , reviewer node ) ) reviewed date node = Literal ( review . review date iso format ) reviewed triple = ( review node , self . spdx namespace . review Date , reviewed date node ) self . graph . add ( reviewed triple ) if review . has comment : comment node = Literal ( review . comment ) comment triple = ( review node , RDFS . comment , comment node ) self . graph . add ( comment triple ) return review node", "predictions": ["create a review self controller controller"], "references": ["return a review node ."], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 752, "code": "def create annotation node ( self , annotation ) : annotation node = URI Ref ( str ( annotation . spdx id ) ) type triple = ( annotation node , RDF . type , self . spdx namespace . Annotation ) self . graph . add ( type triple ) annotator node = Literal ( annotation . annotator . to value ( ) ) self . graph . add ( ( annotation node , self . spdx namespace . annotator , annotator node ) ) annotation date node = Literal ( annotation . annotation date iso format ) annotation triple = ( annotation node , self . spdx namespace . annotation Date , annotation date node ) self . graph . add ( annotation triple ) if annotation . has comment : comment node = Literal ( annotation . comment ) comment triple = ( annotation node , RDFS . comment , comment node ) self . graph . add ( comment triple ) annotation type node = Literal ( annotation . annotation type ) annotation type triple = ( annotation node , self . spdx namespace . annotation Type , annotation type node ) self . graph . add ( annotation type triple ) return annotation node", "predictions": ["create an annotation self if it doesn t already exist"], "references": ["return an annotation node ."], "bleu": 0.16590387014219712, "rouge_l": 0.2837209302325582}
{"id": 753, "code": "def create creation info ( self ) : ci node = B Node ( ) type triple = ( ci node , RDF . type , self . spdx namespace . Creation Info ) self . graph . add ( type triple ) created date = Literal ( self . document . creation info . created iso format ) created triple = ( ci node , self . spdx namespace . created , created date ) self . graph . add ( created triple ) creators = self . creators ( ) for creator in creators : self . graph . add ( ( ci node , self . spdx namespace . creator , creator ) ) if self . document . creation info . has comment : comment node = Literal ( self . document . creation info . comment ) comment triple = ( ci node , RDFS . comment , comment node ) self . graph . add ( comment triple ) return ci node", "predictions": ["create a rule that can be used to list of rules ."], "references": ["add and return a creation info node to graph"], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 754, "code": "def create external document ref node ( self , ext document references ) : ext doc ref node = B Node ( ) type triple = ( ext doc ref node , RDF . type , self . spdx namespace . External Document Ref ) self . graph . add ( type triple ) ext doc id = Literal ( ext document references . external document id ) ext doc id triple = ( ext doc ref node , self . spdx namespace . external Document Id , ext doc id ) self . graph . add ( ext doc id triple ) doc uri = Literal ( ext document references . spdx document uri ) doc uri triple = ( ext doc ref node , self . spdx namespace . spdx Document , doc uri ) self . graph . add ( doc uri triple ) checksum node = self . create checksum node ( ext document references . check sum ) self . graph . add ( ( ext doc ref node , self . spdx namespace . checksum , checksum node ) ) return ext doc ref node", "predictions": ["report a external self linenumber"], "references": ["add and return a creation info node to graph"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 755, "code": "def package verif node ( self , package ) : verif node = B Node ( ) type triple = ( verif node , RDF . type , self . spdx namespace . Package Verification Code ) self . graph . add ( type triple ) value triple = ( verif node , self . spdx namespace . package Verification Code Value , Literal ( package . verif code ) ) self . graph . add ( value triple ) excl file nodes = map ( lambda excl : Literal ( excl ) , package . verif exc files ) excl predicate = self . spdx namespace . package Verification Code Excluded File excl file triples = [ ( verif node , excl predicate , xcl file ) for xcl file in excl file nodes ] for trp in excl file triples : self . graph . add ( trp ) return verif node", "predictions": ["return a load file to a load balancer file exists"], "references": ["return a node representing package verification code ."], "bleu": 0.16590387014219712, "rouge_l": 0.22676579925650556}
{"id": 756, "code": "def handle pkg optional fields ( self , package , package node ) : self . handle package literal optional ( package , package node , self . spdx namespace . version Info , 'version' ) self . handle package literal optional ( package , package node , self . spdx namespace . package File Name , 'file name' ) self . handle package literal optional ( package , package node , self . spdx namespace . supplier , 'supplier' ) self . handle package literal optional ( package , package node , self . spdx namespace . originator , 'originator' ) self . handle package literal optional ( package , package node , self . spdx namespace . source Info , 'source info' ) self . handle package literal optional ( package , package node , self . spdx namespace . license Comments , 'license comment' ) self . handle package literal optional ( package , package node , self . spdx namespace . summary , 'summary' ) self . handle package literal optional ( package , package node , self . spdx namespace . description , 'description' ) if package . has optional field ( 'check sum' ) : checksum node = self . create checksum node ( package . check sum ) self . graph . add ( ( package node , self . spdx namespace . checksum , checksum node ) ) if package . has optional field ( 'homepage' ) : homepage node = URI Ref ( self . to special value ( package . homepage ) ) homepage triple = ( package node , self . doap namespace . homepage , homepage node ) self . graph . add ( homepage triple )", "predictions": ["gets all args that have been parsed as json"], "references": ["write package optional fields ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 757, "code": "def create doc ( self ) : doc node = URI Ref ( 'http://www.spdx.org/tools#SPDX Ref-DOCUMENT' ) self . graph . add ( ( doc node , RDF . type , self . spdx namespace . Spdx Document ) ) vers literal = Literal ( str ( self . document . version ) ) self . graph . add ( ( doc node , self . spdx namespace . spec Version , vers literal ) ) data lics = URI Ref ( self . document . data license . url ) self . graph . add ( ( doc node , self . spdx namespace . data License , data lics ) ) doc name = URI Ref ( self . document . name ) self . graph . add ( ( doc node , self . spdx namespace . name , doc name ) ) return doc node", "predictions": ["create a yaml loader loader loader ."], "references": ["add and return the root document node to graph ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 758, "code": "def handle lics ( self , lics ) : if ( lics , RDF . type , self . spdx namespace [ 'Extracted Licensing Info' ] ) in self . graph : return self . parse only extr license ( lics ) ident start = lics . rfind ( '/' ) + 1 if ident start == 0 : special = self . to special value ( lics ) if special == lics : if self . LICS REF REGEX . match ( lics ) : return document . License . from identifier ( lics ) else : raise SPDX Value Error ( 'License' ) else : return special else : return document . License . from identifier ( lics [ ident start : ] )", "predictions": ["handle a lics ."], "references": ["return a license from a lics license resource ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 759, "code": "def get extr license ident ( self , extr lic ) : identifier tripples = list ( self . graph . triples ( ( extr lic , self . spdx namespace [ 'license Id' ] , None ) ) ) if not identifier tripples : self . error = True msg = 'Extracted license must have license Id property.' self . logger . log ( msg ) return if len ( identifier tripples ) > 1 : self . more than one error ( 'extracted license identifier tripples' ) return identifier tripple = identifier tripples [ 0 ] s , p , identifier = identifier tripple return identifier", "predictions": ["return license license license"], "references": ["return an a license identifier from an extractedlicense or none ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 760, "code": "def get extr license text ( self , extr lic ) : text tripples = list ( self . graph . triples ( ( extr lic , self . spdx namespace [ 'extracted Text' ] , None ) ) ) if not text tripples : self . error = True msg = 'Extracted license must have extracted Text property' self . logger . log ( msg ) return if len ( text tripples ) > 1 : self . more than one error ( 'extracted license text' ) return text tripple = text tripples [ 0 ] s , p , text = text tripple return text", "predictions": ["return info from the info box"], "references": ["return extracted text from an extractedlicense or none ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 761, "code": "def get extr lic name ( self , extr lic ) : extr name list = list ( self . graph . triples ( ( extr lic , self . spdx namespace [ 'license Name' ] , None ) ) ) if len ( extr name list ) > 1 : self . more than one error ( 'extracted license name' ) return elif len ( extr name list ) == 0 : return return self . to special value ( extr name list [ 0 ] [ 2 ] )", "predictions": ["returns the lic name name name name"], "references": ["return the license name from an extractedlicense or none"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 762, "code": "def get extr lics xref ( self , extr lic ) : xrefs = list ( self . graph . triples ( ( extr lic , RDFS . see Also , None ) ) ) return map ( lambda xref triple : xref triple [ 2 ] , xrefs )", "predictions": ["return the lics of a lics"], "references": ["return a list of cross references ."], "bleu": 0.22236312185643822, "rouge_l": 0.3034825870646766}
{"id": 763, "code": "def get extr lics comment ( self , extr lics ) : comment list = list ( self . graph . triples ( ( extr lics , RDFS . comment , None ) ) ) if len ( comment list ) > 1 : self . more than one error ( 'extracted license comment' ) return elif len ( comment list ) == 1 : return comment list [ 0 ] [ 2 ] else : return", "predictions": ["returns the lics spdx spdx for a given file"], "references": ["return license comment or none ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 764, "code": "def parse package ( self , p term ) : if not ( p term , self . spdx namespace [ 'name' ] , None ) in self . graph : self . error = True self . logger . log ( 'Package must have a name.' ) self . builder . create package ( self . doc , 'dummy package' ) else : for s , p , o in self . graph . triples ( ( p term , self . spdx namespace [ 'name' ] , None ) ) : try : self . builder . create package ( self . doc , six . text type ( o ) ) except Cardinality Error : self . more than one error ( 'Package name' ) break self . p pkg vinfo ( p term , self . spdx namespace [ 'version Info' ] ) self . p pkg fname ( p term , self . spdx namespace [ 'package File Name' ] ) self . p pkg suppl ( p term , self . spdx namespace [ 'supplier' ] ) self . p pkg originator ( p term , self . spdx namespace [ 'originator' ] ) self . p pkg down loc ( p term , self . spdx namespace [ 'download Location' ] ) self . p pkg homepg ( p term , self . doap namespace [ 'homepage' ] ) self . p pkg chk sum ( p term , self . spdx namespace [ 'checksum' ] ) self . p pkg src info ( p term , self . spdx namespace [ 'source Info' ] ) self . p pkg verif code ( p term , self . spdx namespace [ 'package Verification Code' ] ) self . p pkg lic conc ( p term , self . spdx namespace [ 'license Concluded' ] ) self . p pkg lic decl ( p term , self . spdx namespace [ 'license Declared' ] ) self . p pkg lics info from files ( p term , self . spdx namespace [ 'license Info From Files' ] ) self . p pkg comments on lics ( p term , self . spdx namespace [ 'license Comments' ] ) self . p pkg cr text ( p term , self . spdx namespace [ 'copyright Text' ] ) self . p pkg summary ( p term , self . spdx namespace [ 'summary' ] ) self . p pkg descr ( p term , self . spdx namespace [ 'description' ] )", "predictions": ["write a package and update the version of the package"], "references": ["parses package fields ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 765, "code": "def handle pkg lic ( self , p term , predicate , builder func ) : try : for , , licenses in self . graph . triples ( ( p term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx namespace [ 'Conjunctive License Set' ] ) in self . graph : lics = self . handle conjunctive list ( licenses ) builder func ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx namespace [ 'Disjunctive License Set' ] ) in self . graph : lics = self . handle disjunctive list ( licenses ) builder func ( self . doc , lics ) else : try : lics = self . handle lics ( licenses ) builder func ( self . doc , lics ) except SPDX Value Error : self . value error ( 'PKG SINGLE LICS' , licenses ) except Cardinality Error : self . more than one error ( 'package {0}' . format ( predicate ) )", "predictions": ["emits a single signal"], "references": ["handles package lics concluded or declared ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 766, "code": "def get file name ( self , f term ) : for , , name in self . graph . triples ( ( f term , self . spdx namespace [ 'file Name' ] , None ) ) : return name return", "predictions": ["returns the from a from a from a from a from the graph"], "references": ["returns first found filename property or none if not found ."], "bleu": 0.09552040806823771, "rouge_l": 0.08460471567267684}
{"id": 767, "code": "def p file depends ( self , f term , predicate ) : for , , other file in self . graph . triples ( ( f term , predicate , None ) ) : name = self . get file name ( other file ) if name is not None : self . builder . add file dep ( six . text type ( name ) ) else : self . error = True msg = 'File depends on file with no name' self . logger . log ( msg )", "predictions": ["log a document to all open open a document"], "references": ["sets file dependencies ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 768, "code": "def p file contributor ( self , f term , predicate ) : for , , contributor in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . add file contribution ( self . doc , six . text type ( contributor ) )", "predictions": ["triples a file contributor"], "references": ["parse all file contributors and adds them to the model ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 769, "code": "def p file notice ( self , f term , predicate ) : try : for , , notice in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . set file notice ( self . doc , six . text type ( notice ) ) except Cardinality Error : self . more than one error ( 'file notice' )", "predictions": ["p a file with a given predicate ."], "references": ["sets file notice text ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 770, "code": "def p file comment ( self , f term , predicate ) : try : for , , comment in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . set file comment ( self . doc , six . text type ( comment ) ) except Cardinality Error : self . more than one error ( 'file comment' )", "predictions": ["triples a file with a given predicate ."], "references": ["sets file comment text ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 771, "code": "def p file cr text ( self , f term , predicate ) : try : for , , cr text in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . set file copyright ( self . doc , six . text type ( cr text ) ) except Cardinality Error : self . more than one error ( 'file copyright text' )", "predictions": ["go to cr file with a file was received"], "references": ["sets file copyright text ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 772, "code": "def p file comments on lics ( self , f term , predicate ) : try : for , , comment in self . graph . triples ( ( f term , predicate , None ) ) : self . builder . set file license comment ( self . doc , six . text type ( comment ) ) except Cardinality Error : self . more than one error ( 'file comments on license' )", "predictions": ["triples file comments on lics ."], "references": ["sets file license comment ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 773, "code": "def p file lic info ( self , f term , predicate ) : for , , info in self . graph . triples ( ( f term , predicate , None ) ) : lic = self . handle lics ( info ) if lic is not None : self . builder . set file license in file ( self . doc , lic )", "predictions": ["triples a file with the given term ."], "references": ["sets file license information ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 774, "code": "def p file type ( self , f term , predicate ) : try : for , , ftype in self . graph . triples ( ( f term , predicate , None ) ) : try : if ftype . endswith ( 'binary' ) : ftype = 'BINARY' elif ftype . endswith ( 'source' ) : ftype = 'SOURCE' elif ftype . endswith ( 'other' ) : ftype = 'OTHER' elif ftype . endswith ( 'archive' ) : ftype = 'ARCHIVE' self . builder . set file type ( self . doc , ftype ) except SPDX Value Error : self . value error ( 'FILE TYPE' , ftype ) except Cardinality Error : self . more than one error ( 'file type' )", "predictions": ["p a file type from a given file ."], "references": ["sets file type ."], "bleu": 0.19960198807747329, "rouge_l": 0.4959349593495934}
{"id": 775, "code": "def p file chk sum ( self , f term , predicate ) : try : for s , p , checksum in self . graph . triples ( ( f term , predicate , None ) ) : for , , value in self . graph . triples ( ( checksum , self . spdx namespace [ 'checksum Value' ] , None ) ) : self . builder . set file chksum ( self . doc , six . text type ( value ) ) except Cardinality Error : self . more than one error ( 'File checksum' )", "predictions": ["documentation on a file - like object"], "references": ["sets file checksum . assumes sha1 algorithm without checking ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 776, "code": "def p file lic conc ( self , f term , predicate ) : try : for , , licenses in self . graph . triples ( ( f term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx namespace [ 'Conjunctive License Set' ] ) in self . graph : lics = self . handle conjunctive list ( licenses ) self . builder . set concluded license ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx namespace [ 'Disjunctive License Set' ] ) in self . graph : lics = self . handle disjunctive list ( licenses ) self . builder . set concluded license ( self . doc , lics ) else : try : lics = self . handle lics ( licenses ) self . builder . set concluded license ( self . doc , lics ) except SPDX Value Error : self . value error ( 'FILE SINGLE LICS' , licenses ) except Cardinality Error : self . more than one error ( 'file {0}' . format ( predicate ) )", "predictions": ["whether a file was received in the user in the graph ."], "references": ["sets file licenses concluded ."], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 777, "code": "def parse creation info ( self , ci term ) : for s , p , o in self . graph . triples ( ( ci term , self . spdx namespace [ 'creator' ] , None ) ) : try : ent = self . builder . create entity ( self . doc , six . text type ( o ) ) self . builder . add creator ( self . doc , ent ) except SPDX Value Error : self . value error ( 'CREATOR VALUE' , o ) for s , p , o in self . graph . triples ( ( ci term , self . spdx namespace [ 'created' ] , None ) ) : try : self . builder . set created date ( self . doc , six . text type ( o ) ) except SPDX Value Error : self . value error ( 'CREATED VALUE' , o ) except Cardinality Error : self . more than one error ( 'created' ) break for s , p , o in self . graph . triples ( ( ci term , RDFS . comment , None ) ) : try : self . builder . set creation comment ( self . doc , six . text type ( o ) ) except Cardinality Error : self . more than one error ( 'Creation Info comment' ) break for s , p , o in self . graph . triples ( ( ci term , self . spdx namespace [ 'license List Version' ] , None ) ) : try : self . builder . set lics list ver ( self . doc , six . text type ( o ) ) except Cardinality Error : self . more than one error ( 'license List Version' ) break except SPDX Value Error : self . value error ( 'LL VALUE' , o )", "predictions": ["parse the creation of the creation and triples ."], "references": ["parse creators created and comment ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 778, "code": "def parse ext doc ref ( self , ext doc ref term ) : for s , p , o in self . graph . triples ( ( ext doc ref term , self . spdx namespace [ 'external Document Id' ] , None ) ) : try : self . builder . set ext doc id ( self . doc , six . text type ( o ) ) except SPDX Value Error : self . value error ( 'EXT DOC REF VALUE' , 'External Document ID' ) break for s , p , o in self . graph . triples ( ( ext doc ref term , self . spdx namespace [ 'spdx Document' ] , None ) ) : try : self . builder . set spdx doc uri ( self . doc , six . text type ( o ) ) except SPDX Value Error : self . value error ( 'EXT DOC REF VALUE' , 'SPDX Document URI' ) break for s , p , checksum in self . graph . triples ( ( ext doc ref term , self . spdx namespace [ 'checksum' ] , None ) ) : for , , value in self . graph . triples ( ( checksum , self . spdx namespace [ 'checksum Value' ] , None ) ) : try : self . builder . set chksum ( self . doc , six . text type ( value ) ) except SPDX Value Error : self . value error ( 'EXT DOC REF VALUE' , 'Checksum' ) break", "predictions": ["parse the graph and triples to triples"], "references": ["parses the external document id spdx document uri and checksum ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 779, "code": "def system ( cmd , data = None ) : import subprocess s = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) out , err = s . communicate ( data ) return out . decode ( 'utf8' )", "predictions": ["run the system command ."], "references": ["pipes the output of a program"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 780, "code": "def unescape ( data ) : cc = re . compile ( r'&(?:(?:#(\\d+))|([^;]+));' ) result = [ ] m = cc . search ( data ) while m : result . append ( data [ 0 : m . start ( ) ] ) d = m . group ( 1 ) if d : d = int ( d ) result . append ( unichr ( d ) ) else : d = unescape . get ( m . group ( 2 ) , ord ( '?' ) ) result . append ( unichr ( d ) ) data = data [ m . end ( ) : ] m = cc . search ( data ) result . append ( data ) return '' . join ( result )", "predictions": ["run the data ."], "references": ["unescapes html entities . the opposite of escape ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 781, "code": "def attr ( * args , * * kwargs ) : ctx = dom tag . with contexts [ get thread context ( ) ] if ctx and ctx [ - 1 ] : dicts = args + ( kwargs , ) for d in dicts : for attr , value in d . items ( ) : ctx [ - 1 ] . tag . set attribute ( * dom tag . clean pair ( attr , value ) ) else : raise Value Error ( 'not in a tag context' )", "predictions": ["clean an attribute of an attribute ."], "references": ["set attributes on the current active tag context"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 782, "code": "def set attribute ( self , key , value ) : if isinstance ( key , int ) : self . children [ key ] = value elif isinstance ( key , basestring ) : self . attributes [ key ] = value else : raise Type Error ( 'Only integer and string types are valid for assigning ' 'child tags and attributes, respectively.' )", "predictions": ["set an attribute of the attribute ."], "references": ["add or update the value of an attribute ."], "bleu": 0.24177237023718662, "rouge_l": 0.3667334669338677}
{"id": 783, "code": "def add ( self , * args ) : for obj in args : if isinstance ( obj , numbers . Number ) : obj = str ( obj ) if isinstance ( obj , basestring ) : obj = escape ( obj ) self . children . append ( obj ) elif isinstance ( obj , dom tag ) : ctx = dom tag . with contexts [ get thread context ( ) ] if ctx and ctx [ - 1 ] : ctx [ - 1 ] . used . add ( obj ) self . children . append ( obj ) obj . parent = self obj . setdocument ( self . document ) elif isinstance ( obj , dict ) : for attr , value in obj . items ( ) : self . set attribute ( * dom tag . clean pair ( attr , value ) ) elif hasattr ( obj , ' iter ' ) : for subobj in obj : self . add ( subobj ) else : raise Value Error ( '%r not a tag or string.' % obj ) if len ( args ) == 1 : return args [ 0 ] return args", "predictions": ["add a new object to the collection"], "references": ["add new child tags ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 784, "code": "def start ( self ) : self . outgoing q = zmq pipes . Tasks Outgoing ( \"127.0.0.1\" , self . interchange port range ) self . incoming q = zmq pipes . Results Incoming ( \"127.0.0.1\" , self . interchange port range ) self . is alive = True self . queue management thread = None self . start queue management thread ( ) self . start local queue process ( ) logger . debug ( \"Created management thread: {}\" . format ( self . queue management thread ) ) if self . provider : l cmd = self . launch cmd . format ( task url = self . worker task url , workers per node = self . workers per node , logdir = \"{}/{}\" . format ( self . run dir , self . label ) ) self . launch cmd = l cmd logger . debug ( \"Launch command: {}\" . format ( self . launch cmd ) ) self . scaling enabled = self . provider . scaling enabled logger . debug ( \"Starting Low Latency Executor with provider:\\n%s\" , self . provider ) if hasattr ( self . provider , 'init blocks' ) : try : for i in range ( self . provider . init blocks ) : block = self . provider . submit ( self . launch cmd , 1 , self . workers per node ) logger . debug ( \"Launched block {}:{}\" . format ( i , block ) ) if not block : raise ( Scaling Failed ( self . provider . label , \"Attempts to provision nodes via provider has failed\" ) ) self . blocks . extend ( [ block ] ) except Exception as e : logger . error ( \"Scaling out failed: {}\" . format ( e ) ) raise e else : self . scaling enabled = False logger . debug ( \"Starting Low Latency Executor with no provider\" )", "predictions": ["start the thread ."], "references": ["create the interchange process and connect to it ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 785, "code": "def create reg message ( self ) : msg = { 'parsl v' : PARSL VERSION , 'python v' : \"{}.{}.{}\" . format ( sys . version info . major , sys . version info . minor , sys . version info . micro ) , 'os' : platform . system ( ) , 'hname' : platform . node ( ) , 'dir' : os . getcwd ( ) , } b msg = json . dumps ( msg ) . encode ( 'utf-8' ) return b msg", "predictions": ["create the message ."], "references": ["creates a registration message to identify the worker to the interchange"], "bleu": 0.06909866532427987, "rouge_l": 0.12298387096774194}
{"id": 786, "code": "def heartbeat ( self ) : heartbeat = ( HEARTBEAT CODE ) . to bytes ( 4 , \"little\" ) r = self . task incoming . send ( heartbeat ) logger . debug ( \"Return from heartbeat : {}\" . format ( r ) )", "predictions": ["invoked when a heartbeat message is started ."], "references": ["send heartbeat to the incoming task queue"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 787, "code": "def async process ( fn ) : def run ( * args , * * kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run", "predictions": ["process a function asynchronously ."], "references": ["decorator function to launch a function as a separate process"], "bleu": 0.14203729394569906, "rouge_l": 0.2515463917525773}
{"id": 788, "code": "def send UDP message ( self , message ) : x = 0 if self . tracking enabled : try : proc = udp messenger ( self . domain name , self . UDP IP , self . UDP PORT , self . sock timeout , message ) self . procs . append ( proc ) except Exception as e : logger . debug ( \"Usage tracking failed: {}\" . format ( e ) ) else : x = - 1 return x", "predictions": ["send a message to the client ."], "references": ["send udp message ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 789, "code": "def create task log info ( self , task id , fail mode = None ) : info to monitor = [ 'func name' , 'fn hash' , 'memoize' , 'checkpoint' , 'fail count' , 'fail history' , 'status' , 'id' , 'time submitted' , 'time returned' , 'executor' ] task log info = { \"task \" + k : self . tasks [ task id ] [ k ] for k in info to monitor } task log info [ 'run id' ] = self . run id task log info [ 'timestamp' ] = datetime . datetime . now ( ) task log info [ 'task status name' ] = self . tasks [ task id ] [ 'status' ] . name task log info [ 'tasks failed count' ] = self . tasks failed count task log info [ 'tasks completed count' ] = self . tasks completed count task log info [ 'task inputs' ] = str ( self . tasks [ task id ] [ 'kwargs' ] . get ( 'inputs' , None ) ) task log info [ 'task outputs' ] = str ( self . tasks [ task id ] [ 'kwargs' ] . get ( 'outputs' , None ) ) task log info [ 'task stdin' ] = self . tasks [ task id ] [ 'kwargs' ] . get ( 'stdin' , None ) task log info [ 'task stdout' ] = self . tasks [ task id ] [ 'kwargs' ] . get ( 'stdout' , None ) task log info [ 'task depends' ] = None if self . tasks [ task id ] [ 'depends' ] is not None : task log info [ 'task depends' ] = \",\" . join ( [ str ( t . tid ) for t in self . tasks [ task id ] [ 'depends' ] ] ) task log info [ 'task elapsed time' ] = None if self . tasks [ task id ] [ 'time returned' ] is not None : task log info [ 'task elapsed time' ] = ( self . tasks [ task id ] [ 'time returned' ] - self . tasks [ task id ] [ 'time submitted' ] ) . total seconds ( ) if fail mode is not None : task log info [ 'task fail mode' ] = fail mode return task log info", "predictions": ["create a task object from a task log file"], "references": ["create the dictionary that will be included in the log ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 790, "code": "def command server ( self , kill event ) : logger . debug ( \"[COMMAND] Command Server Starting\" ) while not kill event . is set ( ) : try : command req = self . command channel . recv pyobj ( ) logger . debug ( \"[COMMAND] Received command request: {}\" . format ( command req ) ) if command req == \"OUTSTANDING C\" : outstanding = self . pending task queue . qsize ( ) for manager in self . ready manager queue : outstanding += len ( self . ready manager queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command req == \"WORKERS\" : num workers = 0 for manager in self . ready manager queue : num workers += self . ready manager queue [ manager ] [ 'worker count' ] reply = num workers elif command req == \"MANAGERS\" : reply = [ ] for manager in self . ready manager queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block id' : self . ready manager queue [ manager ] [ 'block id' ] , 'worker count' : self . ready manager queue [ manager ] [ 'worker count' ] , 'tasks' : len ( self . ready manager queue [ manager ] [ 'tasks' ] ) , 'active' : self . ready manager queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command req . startswith ( \"HOLD WORKER\" ) : cmd , s manager = command req . split ( ';' ) manager = s manager . encode ( 'utf-8' ) logger . info ( \"[CMD] Received HOLD WORKER for {}\" . format ( manager ) ) if manager in self . ready manager queue : self . ready manager queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command req == \"SHUTDOWN\" : logger . info ( \"[CMD] Received SHUTDOWN command\" ) kill event . set ( ) reply = True else : reply = None logger . debug ( \"[COMMAND] Reply: {}\" . format ( reply ) ) self . command channel . send pyobj ( reply ) except zmq . Again : logger . debug ( \"[COMMAND] is alive\" ) continue", "predictions": ["send a command to the command to the queue ."], "references": ["command server to run async command to the interchange"], "bleu": 0.25965358893403384, "rouge_l": 0.5313588850174217}
{"id": 791, "code": "def get data manager ( cls ) : from parsl . dataflow . dflow import Data Flow Kernel Loader dfk = Data Flow Kernel Loader . dfk ( ) return dfk . executors [ 'data manager' ]", "predictions": ["return the data manager manager ."], "references": ["return the datamanager of the currently loaded dataflowkernel ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 792, "code": "def import mapping ( mapping , original = None ) : #log = get logger() #log.debug(\"Importing canning map\") for key , value in list ( mapping . items ( ) ) : if isinstance ( key , string types ) : try : cls = import item ( key ) except Exception : if original and key not in original : print ( \"ERROR: canning class not importable: %r\" , key , exc info = True ) mapping . pop ( key ) else : mapping [ cls ] = mapping . pop ( key )", "predictions": ["import all classes in a mapping"], "references": ["import any string - keys in a type mapping ."], "bleu": 0.16959011078459055, "rouge_l": 0.47843137254901963}
{"id": 793, "code": "def can ( obj ) : import needed = False for cls , canner in iteritems ( can map ) : if isinstance ( cls , string types ) : import needed = True break elif istype ( obj , cls ) : return canner ( obj ) if import needed : import mapping ( can map , original can map ) return can ( obj ) return obj", "predictions": ["return true if object can be a mapping of object or false ."], "references": ["prepare an object for pickling ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 794, "code": "def can dict ( obj ) : if istype ( obj , dict ) : newobj = { } for k , v in iteritems ( obj ) : newobj [ k ] = can ( v ) return newobj else : return obj", "predictions": ["return a dict of keys of a object s object ."], "references": ["can the * values * of a dict ."], "bleu": 0.18850319022747347, "rouge_l": 0.3055091819699499}
{"id": 795, "code": "def can sequence ( obj ) : if istype ( obj , sequence types ) : t = type ( obj ) return t ( [ can ( i ) for i in obj ] ) else : return obj", "predictions": ["check if an object is a sequence of object type ."], "references": ["can the elements of a sequence ."], "bleu": 0.17033186037639278, "rouge_l": 0.3472485768500949}
{"id": 796, "code": "def unset logging ( self ) : if self . logger flag is True : return root logger = logging . get Logger ( ) for hndlr in root logger . handlers : if hndlr not in self . prior loghandlers : hndlr . set Level ( logging . ERROR ) self . logger flag = True", "predictions": ["unset logging logging ."], "references": ["mute newly added handlers to the root level right after calling executor . status"], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 797, "code": "def start ( self ) : if self . mode == \"manual\" : return if self . ipython dir != '~/.ipython' : self . ipython dir = os . path . abspath ( os . path . expanduser ( self . ipython dir ) ) if self . log : stdout = open ( os . path . join ( self . ipython dir , \"{0}.controller.out\" . format ( self . profile ) ) , 'w' ) stderr = open ( os . path . join ( self . ipython dir , \"{0}.controller.err\" . format ( self . profile ) ) , 'w' ) else : stdout = open ( os . devnull , 'w' ) stderr = open ( os . devnull , 'w' ) try : opts = [ 'ipcontroller' , '' if self . ipython dir == '~/.ipython' else '--ipython-dir={}' . format ( self . ipython dir ) , self . interfaces if self . interfaces is not None else '--ip=*' , '' if self . profile == 'default' else '--profile={0}' . format ( self . profile ) , '--reuse' if self . reuse else '' , '--location={}' . format ( self . public ip ) if self . public ip else '' , '--port={}' . format ( self . port ) if self . port is not None else '' ] if self . port range is not None : opts += [ '--Hub Factory.hb={0},{1}' . format ( self . hb ping , self . hb pong ) , '--Hub Factory.control={0},{1}' . format ( self . control client , self . control engine ) , '--Hub Factory.mux={0},{1}' . format ( self . mux client , self . mux engine ) , '--Hub Factory.task={0},{1}' . format ( self . task client , self . task engine ) ] logger . debug ( \"Starting ipcontroller with '{}'\" . format ( ' ' . join ( [ str ( x ) for x in opts ] ) ) ) self . proc = subprocess . Popen ( opts , stdout = stdout , stderr = stderr , preexec fn = os . setsid ) except File Not Found Error : msg = \"Could not find ipcontroller. Please make sure that ipyparallel is installed and available in your env\" logger . error ( msg ) raise Controller Error ( msg ) except Exception as e : msg = \"IPP Controller failed to start: {0}\" . format ( e ) logger . error ( msg ) raise Controller Error ( msg )", "predictions": ["start the public task ."], "references": ["start the controller ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 798, "code": "def nbytes ( buf ) : if isinstance ( buf , memoryview ) : if PY3 : return buf . nbytes else : size = buf . itemsize for dim in buf . shape : size *= dim return size else : return len ( buf )", "predictions": ["return the size of a buffer ."], "references": ["return byte - size of a memoryview or buffer ."], "bleu": 0.27835144474202883, "rouge_l": 0.6841121495327103}
{"id": 799, "code": "def extract buffers ( obj , threshold = MAX BYTES ) : buffers = [ ] if isinstance ( obj , Canned Object ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : nbytes = nbytes ( buf ) if nbytes > threshold : obj . buffers [ i ] = None buffers . append ( buf ) elif isinstance ( buf , memoryview ) : obj . buffers [ i ] = buf . tobytes ( ) elif isinstance ( buf , buffer ) : obj . buffers [ i ] = bytes ( buf ) return buffers", "predictions": ["extract buffers of obj"], "references": ["extract buffers larger than a certain threshold ."], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 800, "code": "def restore buffers ( obj , buffers ) : if isinstance ( obj , Canned Object ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : if buf is None : obj . buffers [ i ] = buffers . pop ( 0 )", "predictions": ["p file obj obj in dict obj"], "references": ["restore extracted buffers ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 801, "code": "def initialize boto client ( self ) : self . session = self . create session ( ) self . client = self . session . client ( 'ec2' ) self . ec2 = self . session . resource ( 'ec2' ) self . instances = [ ] self . instance states = { } self . vpc id = 0 self . sg id = 0 self . sn ids = [ ]", "predictions": ["create a file for use with the file system term term term term ."], "references": ["initialize the boto client ."], "bleu": 0.09782375748961449, "rouge_l": 0.23018867924528305}
{"id": 802, "code": "def get instance state ( self , instances = None ) : if instances : desc = self . client . describe instances ( Instance Ids = instances ) else : desc = self . client . describe instances ( Instance Ids = self . instances ) for i in range ( len ( desc [ 'Reservations' ] ) ) : instance = desc [ 'Reservations' ] [ i ] [ 'Instances' ] [ 0 ] self . instance states [ instance [ 'Instance Id' ] ] = instance [ 'State' ] [ 'Name' ] return self . instance states", "predictions": ["return up the file comment comment comment in the account"], "references": ["get states of all instances on ec2 which were started by this file ."], "bleu": 0.08450033111870488, "rouge_l": 0.08090185676392574}
{"id": 803, "code": "def show summary ( self ) : self . get instance state ( ) status string = . format ( self . vpc id , self . sn ids , self . sg id , self . instances ) status string += \"\\t Instance States:\\n\\t\\t\" self . get instance state ( ) for state in self . instance states . keys ( ) : status string += \"Instance ID: {}  State: {}\\n\\t\\t\" . format ( state , self . instance states [ state ] ) status string += \"\\n\" logger . info ( status string ) return status string", "predictions": ["print a file file file file file status"], "references": ["print human readable summary of current aws state to log and to console ."], "bleu": 0.07575149194183216, "rouge_l": 0.08664772727272725}
{"id": 804, "code": "def scale out ( self , blocks = 1 , block size = 1 ) : self . config [ 'sites.jetstream.{0}' . format ( self . pool ) ] [ 'flavor' ] count = 0 if blocks == 1 : block id = len ( self . blocks ) self . blocks [ block id ] = [ ] for instance id in range ( 0 , block size ) : instances = self . server manager . create ( 'parsl-{0}-{1}' . format ( block id , instance id ) , self . client . images . get ( '87e08a17-eae2-4ce4-9051-c561d9a54bde' ) , self . client . flavors . list ( ) [ 0 ] , min count = 1 , max count = 1 , userdata = setup script . format ( engine config = self . engine config ) , key name = 'TG-MCB090174-api-key' , security groups = [ 'global-ssh' ] , nics = [ { \"net-id\" : '724a50cf-7f11-4b3b-a884-cd7e6850e39e' , \"net-name\" : 'PARSL-priv-net' , \"v4-fixed-ip\" : '' } ] ) self . blocks [ block id ] . extend ( [ instances ] ) count += 1 return count", "predictions": ["p all the blocks images . ."], "references": ["scale out the existing resources ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 805, "code": "def status ( self ) : job id list = ' ' . join ( self . resources . keys ( ) ) cmd = \"condor q {0} -af:jr Job Status\" . format ( job id list ) retcode , stdout , stderr = super ( ) . execute wait ( cmd ) for line in stdout . strip ( ) . split ( '\\n' ) : parts = line . split ( ) job id = parts [ 0 ] status = translate table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job id ] [ 'status' ] = status", "predictions": ["show the p - p p p . ."], "references": ["update the resource dictionary with job statuses ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 806, "code": "def scale in ( self , blocks ) : status = dict ( zip ( self . engines , self . provider . status ( self . engines ) ) ) to kill = [ engine for engine in status if status [ engine ] == \"RUNNING\" ] [ : blocks ] if self . provider : r = self . provider . cancel ( to kill ) else : logger . error ( \"No execution provider available\" ) r = None return r", "predictions": ["p all engines file file engines file for the given blocks"], "references": ["scale in the number of active blocks by the specified number ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 807, "code": "def status ( self ) : if self . provider : status = self . provider . status ( self . engines ) else : status = [ ] return status", "predictions": ["for querying the p . . . . . ."], "references": ["returns the status of the executor via probing the execution providers ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 808, "code": "def shutdown ( self ) : self . is alive = False logging . debug ( \"Waking management thread\" ) self . incoming q . put ( None ) self . queue management thread . join ( ) logging . debug ( \"Exiting thread\" ) self . worker . join ( ) return True", "predictions": ["shut down the queue task self self self self self self self self self self self self self self self self self self self self self self self self self self"], "references": ["shutdown method to kill the threads and workers ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 809, "code": "def notify ( self , event id ) : self . event buffer . extend ( [ event id ] ) self . event count += 1 if self . event count >= self . threshold : logger . debug ( \"Eventcount >= threshold\" ) self . make callback ( kind = \"event\" )", "predictions": ["parse a try to the neighbor neighbor . ."], "references": ["let the flowcontrol system know that there is an event ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 810, "code": "def make callback ( self , kind = None ) : self . wake up time = time . time ( ) + self . interval self . callback ( * self . cb args )", "predictions": ["makes a ext that makes a ext connection"], "references": ["makes the callback and resets the timer ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 811, "code": "def create deployment ( self , deployment ) : api response = self . kube client . create namespaced deployment ( body = deployment , namespace = self . namespace ) logger . debug ( \"Deployment created. status='{0}'\" . format ( str ( api response . status ) ) )", "predictions": ["system a deployment subprocess"], "references": ["create the kubernetes deployment"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 812, "code": "def start ( self ) : self . outgoing q = zmq pipes . Tasks Outgoing ( \"127.0.0.1\" , self . interchange port range ) self . incoming q = zmq pipes . Results Incoming ( \"127.0.0.1\" , self . interchange port range ) self . command client = zmq pipes . Command Client ( \"127.0.0.1\" , self . interchange port range ) self . is alive = True self . executor bad state = threading . Event ( ) self . executor exception = None self . queue management thread = None self . start queue management thread ( ) self . start local queue process ( ) logger . debug ( \"Created management thread: {}\" . format ( self . queue management thread ) ) if self . provider : self . initialize scaling ( ) else : self . scaling enabled = False logger . debug ( \"Starting High Throughput Executor with no provider\" )", "predictions": ["unescape the thread thread connection to a separate thread cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc"], "references": ["create the interchange process and connect to it ."], "bleu": 0.051660454541342535, "rouge_l": 0.1252566735112936}
{"id": 813, "code": "def status ( self ) : status = [ ] if self . provider : status = self . provider . status ( self . blocks . values ( ) ) return status", "predictions": ["with all the jobs in the dom = 0 = 1 = 0 = 1 = 0 = 0 = 1 = 1 = 0 = 1 = 1 = 0"], "references": ["return status of all blocks ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 814, "code": "def thirteen oscillator three stimulated ensembles list ( ) : \"Not accurate due to false skipes are observed\" parameters = legion parameters ( ) parameters . Wt = 4.0 parameters . fi = 10.0 template dynamic legion ( 15 , 1000 , 1000 , conn type = conn type . LIST BIDIR , stimulus = [ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 ] , params = parameters , separate repr = [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 , 9 , 10 ] , [ 6 , 7 , 8 ] , [ 11 , 12 , 13 , 14 ] ] )", "predictions": ["ensembles ensembles of legion and legion parameters"], "references": ["good example of three synchronous ensembels"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 815, "code": "def thirteen simplify oscillator three stimulated ensembles list ( ) : \"Not accurate due to false skipes are observed\" parameters = legion parameters ( ) parameters . Wt = 4.0 parameters . fi = 0.8 parameters . ENABLE POTENTIONAL = False template dynamic legion ( 15 , 1000 , 1000 , conn type = conn type . LIST BIDIR , stimulus = [ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 ] , params = parameters , separate repr = [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 , 9 , 10 ] , [ 6 , 7 , 8 ] , [ 11 , 12 , 13 , 14 ] ] )", "predictions": ["simplify args for add - args and legion append a list to a list"], "references": ["good example of three synchronous ensembels"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 816, "code": "def sixteen oscillator two stimulated ensembles grid ( ) : parameters = legion parameters ( ) parameters . teta x = - 1.1 template dynamic legion ( 16 , 2000 , 1500 , conn type = conn type . GRID FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )", "predictions": ["randomly ensembles self ."], "references": ["not accurate false due to spikes are observed"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 817, "code": "def cluster sample1 ( ) : start centers = [ [ 3.7 , 5.5 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE1 , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE1 , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["run the create create command"], "references": ["start with wrong number of clusters ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 818, "code": "def cluster sample2 ( ) : start centers = [ [ 3.5 , 4.8 ] , [ 2.6 , 2.5 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE2 , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE2 , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["heartbeat - sample2 heartbeat"], "references": ["start with wrong number of clusters ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 819, "code": "def cluster sample3 ( ) : start centers = [ [ 0.2 , 0.1 ] , [ 4.0 , 1.0 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE3 , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE3 , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["async async async async async async tool"], "references": ["start with wrong number of clusters ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 820, "code": "def cluster sample5 ( ) : start centers = [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE5 , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE SIMPLE5 , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["send a send message to the send a send send a send signal"], "references": ["start with wrong number of clusters ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 821, "code": "def cluster elongate ( ) : start centers = [ [ 1.0 , 4.5 ] , [ 3.1 , 2.7 ] ] template clustering ( start centers , SIMPLE SAMPLES . SAMPLE ELONGATE , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , SIMPLE SAMPLES . SAMPLE ELONGATE , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["create a create create the create block block block ."], "references": ["not so applicable for this sample"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 822, "code": "def cluster lsun ( ) : start centers = [ [ 1.0 , 3.5 ] , [ 2.0 , 0.5 ] , [ 3.0 , 3.0 ] ] template clustering ( start centers , FCPS SAMPLES . SAMPLE LSUN , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , FCPS SAMPLES . SAMPLE LSUN , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["command to run the command line with the command line arguments ."], "references": ["not so applicable for this sample"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 823, "code": "def cluster target ( ) : start centers = [ [ 0.2 , 0.2 ] , [ 0.0 , - 2.0 ] , [ 3.0 , - 3.0 ] , [ 3.0 , 3.0 ] , [ - 3.0 , 3.0 ] , [ - 3.0 , - 3.0 ] ] template clustering ( start centers , FCPS SAMPLES . SAMPLE TARGET , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , FCPS SAMPLES . SAMPLE TARGET , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["get get get the data for the get executors executors executors executors executors"], "references": ["not so applicable for this sample"], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 824, "code": "def cluster two diamonds ( ) : start centers = [ [ 0.8 , 0.2 ] ] template clustering ( start centers , FCPS SAMPLES . SAMPLE TWO DIAMONDS , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , FCPS SAMPLES . SAMPLE TWO DIAMONDS , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["import mapping from mapping to mapping to mapping in the import directory"], "references": ["start with wrong number of clusters ."], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 825, "code": "def cluster hepta ( ) : start centers = [ [ 0.0 , 0.0 , 0.0 ] , [ 3.0 , 0.0 , 0.0 ] , [ - 2.0 , 0.0 , 0.0 ] , [ 0.0 , 3.0 , 0.0 ] , [ 0.0 , - 3.0 , 0.0 ] , [ 0.0 , 0.0 , 2.5 ] ] template clustering ( start centers , FCPS SAMPLES . SAMPLE HEPTA , criterion = splitting type . BAYESIAN INFORMATION CRITERION ) template clustering ( start centers , FCPS SAMPLES . SAMPLE HEPTA , criterion = splitting type . MINIMUM NOISELESS DESCRIPTION LENGTH )", "predictions": ["hepta the can be done with the can be a can be passed to the can be a can be printed"], "references": ["start with wrong number of clusters ."], "bleu": 0.05809665204409193, "rouge_l": 0.0785070785070785}
{"id": 826, "code": "def get role name ( region , account id , role ) : prefix = ARN PREFIXES . get ( region , 'aws' ) return 'arn:{0}:iam::{1}:role/{2}' . format ( prefix , account id , role )", "predictions": ["can be used to can can be used to can be used for the dict of the dict"], "references": ["shortcut to insert the account_id and role into the iam string ."], "bleu": 0.08097785064266204, "rouge_l": 0.2074829931972789}
{"id": 827, "code": "def get account id ( profile name , aws access key id , aws secret access key , region = None , ) : client = get client ( 'sts' , profile name , aws access key id , aws secret access key , region , ) return client . get caller identity ( ) . get ( 'Account' )", "predictions": ["can can be a . or a . instance"], "references": ["query sts for a users account_id"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 828, "code": "def get client ( client , profile name , aws access key id , aws secret access key , region = None , ) : boto3 . setup default session ( profile name = profile name , aws access key id = aws access key id , aws secret access key = aws secret access key , region name = region , ) return boto3 . client ( client )", "predictions": ["returns a boto3 instance for the given logging logger for the given logging logger for the given logging for the given logging for the given logging name for the given logging"], "references": ["shortcut for getting an initialized instance of the boto3 client ."], "bleu": 0.04906081629292276, "rouge_l": 0.10418445772843724}
{"id": 829, "code": "def upload s3 ( cfg , path to zip file , * use s3 ) : print ( 'Uploading your new Lambda function' ) profile name = cfg . get ( 'profile' ) aws access key id = cfg . get ( 'aws access key id' ) aws secret access key = cfg . get ( 'aws secret access key' ) client = get client ( 's3' , profile name , aws access key id , aws secret access key , cfg . get ( 'region' ) , ) byte stream = b'' with open ( path to zip file , mode = 'rb' ) as fh : byte stream = fh . read ( ) s3 key prefix = cfg . get ( 's3 key prefix' , '/dist' ) checksum = hashlib . new ( 'md5' , byte stream ) . hexdigest ( ) timestamp = str ( time . time ( ) ) filename = '{prefix}{checksum}-{ts}.zip' . format ( prefix = s3 key prefix , checksum = checksum , ts = timestamp , ) buck name = ( os . environ . get ( 'S3 BUCKET NAME' ) or cfg . get ( 'bucket name' ) ) func name = ( os . environ . get ( 'LAMBDA FUNCTION NAME' ) or cfg . get ( 'function name' ) ) kwargs = { 'Bucket' : '{}' . format ( buck name ) , 'Key' : '{}' . format ( filename ) , 'Body' : byte stream , } client . put object ( * * kwargs ) print ( 'Finished uploading {} to S3 bucket {}' . format ( func name , buck name ) ) if use s3 : return filename", "predictions": ["start s3 to s3"], "references": ["upload a function to aws s3 ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 830, "code": "def get function config ( cfg ) : function name = cfg . get ( 'function name' ) profile name = cfg . get ( 'profile' ) aws access key id = cfg . get ( 'aws access key id' ) aws secret access key = cfg . get ( 'aws secret access key' ) client = get client ( 'lambda' , profile name , aws access key id , aws secret access key , cfg . get ( 'region' ) , ) try : return client . get function ( Function Name = function name ) except client . exceptions . Resource Not Found Exception as e : if 'Function not found' in str ( e ) : return False", "predictions": ["nbytes the function of a function"], "references": ["check whether a function exists or not and return its config"], "bleu": 0.12634437832866913, "rouge_l": 0.2234432234432234}
{"id": 831, "code": "def C Compiler spawn silent ( cmd , dry run = None ) : proc = Popen ( cmd , stdout = PIPE , stderr = PIPE ) out , err = proc . communicate ( ) if proc . returncode : raise Distutils Exec Error ( err )", "predictions": ["spawn - obj function"], "references": ["spawn a process and eat the stdio ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 832, "code": "def iter cython ( path ) : for dir path , dir names , file names in os . walk ( path ) : for file name in file names : if file name . startswith ( '.' ) : continue if os . path . splitext ( file name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir path , file name )", "predictions": ["iterate over all cython modules ."], "references": ["yield all . pyx and . pxd files in the given root ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 833, "code": "def split grafs ( lines ) : graf = [ ] for line in lines : line = line . strip ( ) if len ( line ) < 1 : if len ( graf ) > 0 : yield \"\\n\" . join ( graf ) graf = [ ] else : graf . append ( line ) if len ( graf ) > 0 : yield \"\\n\" . join ( graf )", "predictions": ["split a string into chunks of size ."], "references": ["segment the raw text into paragraphs"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 834, "code": "def filter quotes ( text , is email = True ) : global DEBUG global PAT FORWARD , PAT REPLIED , PAT UNSUBSC if is email : text = filter ( lambda x : x in string . printable , text ) if DEBUG : print ( \"text:\" , text ) m = PAT FORWARD . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] m = PAT REPLIED . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] m = PAT UNSUBSC . split ( text , re . M ) if m : text = m [ 0 ] lines = [ ] for line in text . split ( \"\\n\" ) : if line . startswith ( \">\" ) : lines . append ( \"\" ) else : lines . append ( line ) return list ( split grafs ( lines ) )", "predictions": ["filter quotes according to the email address ."], "references": ["filter the quoted text out of a message"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 835, "code": "def fix microsoft ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( text == \"#\" ) and ( i > 0 ) : prev tok = bar [ - 1 ] prev tok [ 0 ] += \"#\" prev tok [ 1 ] += \"#\" bar [ - 1 ] = prev tok else : bar . append ( foo [ i ] ) i += 1 return bar", "predictions": ["fix the bar bar ."], "references": ["fix special case for c# f# etc . ; thanks microsoft"], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 836, "code": "def fix hypenation ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( tag == \"HYPH\" ) and ( i > 0 ) and ( i < len ( foo ) - 1 ) : prev tok = bar [ - 1 ] next tok = foo [ i + 1 ] prev tok [ 0 ] += \"-\" + next tok [ 0 ] prev tok [ 1 ] += \"-\" + next tok [ 1 ] bar [ - 1 ] = prev tok i += 2 else : bar . append ( foo [ i ] ) i += 1 return bar", "predictions": ["fix the bar bar ."], "references": ["fix hyphenation in the word list for a parsed sentence"], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 837, "code": "def parse doc ( json iter ) : global DEBUG for meta in json iter : base idx = 0 for graf text in filter quotes ( meta [ \"text\" ] , is email = False ) : if DEBUG : print ( \"graf text:\" , graf text ) grafs , new base idx = parse graf ( meta [ \"id\" ] , graf text , base idx ) base idx = new base idx for graf in grafs : yield graf", "predictions": ["parse the xml response into a list of document objects"], "references": ["parse one document to prep for textrank"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 838, "code": "def get tiles ( graf , size = 3 ) : keeps = list ( filter ( lambda w : w . word id > 0 , graf ) ) keeps len = len ( keeps ) for i in iter ( range ( 0 , keeps len - 1 ) ) : w0 = keeps [ i ] for j in iter ( range ( i + 1 , min ( keeps len , i + 1 + size ) ) ) : w1 = keeps [ j ] if ( w1 . idx - w0 . idx ) <= size : yield ( w0 . root , w1 . root , )", "predictions": ["return all tiles of a graf"], "references": ["generate word pairs for the textrank graph"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 839, "code": "def build graph ( json iter ) : global DEBUG , Word Node graph = nx . Di Graph ( ) for meta in json iter : if DEBUG : print ( meta [ \"graf\" ] ) for pair in get tiles ( map ( Word Node . make , meta [ \"graf\" ] ) ) : if DEBUG : print ( pair ) for word id in pair : if not graph . has node ( word id ) : graph . add node ( word id ) try : graph . edge [ pair [ 0 ] ] [ pair [ 1 ] ] [ \"weight\" ] += 1.0 except Key Error : graph . add edge ( pair [ 0 ] , pair [ 1 ] , weight = 1.0 ) return graph", "predictions": ["build a graph from a json graph"], "references": ["construct the textrank graph from parsed paragraphs"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 840, "code": "def write dot ( graph , ranks , path = \"graph.dot\" ) : dot = Digraph ( ) for node in graph . nodes ( ) : dot . node ( node , \"%s %0.3f\" % ( node , ranks [ node ] ) ) for edge in graph . edges ( ) : dot . edge ( edge [ 0 ] , edge [ 1 ] , constraint = \"false\" ) with open ( path , 'w' ) as f : f . write ( dot . source )", "predictions": ["write the dot graph to a file"], "references": ["output the graph in dot file format"], "bleu": 0.23356898886410002, "rouge_l": 0.42857142857142855}
{"id": 841, "code": "def render ranks ( graph , ranks , dot file = \"graph.dot\" ) : if dot file : write dot ( graph , ranks , path = dot file )", "predictions": ["render a ranks file as a ranks file"], "references": ["render the textrank graph for visual formats"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 842, "code": "def text rank ( path ) : graph = build graph ( json iter ( path ) ) ranks = nx . pagerank ( graph ) return graph , ranks", "predictions": ["return a graph of the text rank of the given path ."], "references": ["run the textrank algorithm"], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 843, "code": "def find chunk ( phrase , np ) : for i in iter ( range ( 0 , len ( phrase ) ) ) : parsed np = find chunk sub ( phrase , np , i ) if parsed np : return parsed np", "predictions": ["find a chunk in a phrase"], "references": ["leverage noun phrase chunking"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 844, "code": "def enumerate chunks ( phrase , spacy nlp ) : if ( len ( phrase ) > 1 ) : found = False text = \" \" . join ( [ rl . text for rl in phrase ] ) doc = spacy nlp ( text . strip ( ) , parse = True ) for np in doc . noun chunks : if np . text != text : found = True yield np . text , find chunk ( phrase , np . text . split ( \" \" ) ) if not found and all ( [ rl . pos [ 0 ] != \"v\" for rl in phrase ] ) : yield text , phrase", "predictions": ["enumerate chunks of a phrase ."], "references": ["iterate through the noun phrases"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 845, "code": "def collect keyword ( sent , ranks , stopwords ) : for w in sent : if ( w . word id > 0 ) and ( w . root in ranks ) and ( w . pos [ 0 ] in \"NV\" ) and ( w . root not in stopwords ) : rl = Ranked Lexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] / 2.0 , ids = [ w . word id ] , pos = w . pos . lower ( ) , count = 1 ) if DEBUG : print ( rl ) yield rl", "predictions": ["yield ranked from ranked"], "references": ["iterator for collecting the single - word keyphrases"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 846, "code": "def collect entities ( sent , ranks , stopwords , spacy nlp ) : global DEBUG sent text = \" \" . join ( [ w . raw for w in sent ] ) if DEBUG : print ( \"sent:\" , sent text ) for ent in spacy nlp ( sent text ) . ents : if DEBUG : print ( \"NER:\" , ent . label , ent . text ) if ( ent . label not in [ \"CARDINAL\" ] ) and ( ent . text . lower ( ) not in stopwords ) : w ranks , w ids = find entity ( sent , ranks , ent . text . split ( \" \" ) , 0 ) if w ranks and w ids : rl = Ranked Lexeme ( text = ent . text . lower ( ) , rank = w ranks , ids = w ids , pos = \"np\" , count = 1 ) if DEBUG : print ( rl ) yield rl", "predictions": ["collect entities from the text file ."], "references": ["iterator for collecting the named - entities"], "bleu": 0.20556680845025982, "rouge_l": 0.14285714285714285}
{"id": 847, "code": "def collect phrases ( sent , ranks , spacy nlp ) : tail = 0 last idx = sent [ 0 ] . idx - 1 phrase = [ ] while tail < len ( sent ) : w = sent [ tail ] if ( w . word id > 0 ) and ( w . root in ranks ) and ( ( w . idx - last idx ) == 1 ) : rl = Ranked Lexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] , ids = w . word id , pos = w . pos . lower ( ) , count = 1 ) phrase . append ( rl ) else : for text , p in enumerate chunks ( phrase , spacy nlp ) : if p : id list = [ rl . ids for rl in p ] rank list = [ rl . rank for rl in p ] np rl = Ranked Lexeme ( text = text , rank = rank list , ids = id list , pos = \"np\" , count = 1 ) if DEBUG : print ( np rl ) yield np rl phrase = [ ] last idx = w . idx tail += 1", "predictions": ["collect all np . phrases from a list of phrase phrase"], "references": ["iterator for collecting the noun phrases"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 848, "code": "def mh digest ( data ) : num perm = 512 m = Min Hash ( num perm ) for d in data : m . update ( d . encode ( 'utf8' ) ) return m", "predictions": ["get a hash of data from a string of data ."], "references": ["create a minhash digest"], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 849, "code": "def top sentences ( kernel , path ) : key sent = { } i = 0 if isinstance ( path , str ) : path = json iter ( path ) for meta in path : graf = meta [ \"graf\" ] tagged sent = [ Word Node . make ( x ) for x in graf ] text = \" \" . join ( [ w . raw for w in tagged sent ] ) m sent = mh digest ( [ str ( w . word id ) for w in tagged sent ] ) dist = sum ( [ m sent . jaccard ( m ) * rl . rank for rl , m in kernel ] ) key sent [ text ] = ( dist , i ) i += 1 for text , ( dist , i ) in sorted ( key sent . items ( ) , key = lambda x : x [ 1 ] [ 0 ] , reverse = True ) : yield Summary Sent ( dist = dist , idx = i , text = text )", "predictions": ["get the top sentences for a kernel"], "references": ["determine distance for each sentence"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 850, "code": "def limit keyphrases ( path , phrase limit = 20 ) : rank thresh = None if isinstance ( path , str ) : lex = [ ] for meta in json iter ( path ) : rl = Ranked Lexeme ( * * meta ) lex . append ( rl ) else : lex = path if len ( lex ) > 0 : rank thresh = statistics . mean ( [ rl . rank for rl in lex ] ) else : rank thresh = 0 used = 0 for rl in lex : if rl . pos [ 0 ] != \"v\" : if ( used > phrase limit ) or ( rl . rank < rank thresh ) : return used += 1 yield rl . text . replace ( \" - \" , \"-\" )", "predictions": ["limit a string to each phrase in the path"], "references": ["iterator for the most significant key phrases"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 851, "code": "def limit sentences ( path , word limit = 100 ) : word count = 0 if isinstance ( path , str ) : path = json iter ( path ) for meta in path : if not isinstance ( meta , Summary Sent ) : p = Summary Sent ( * * meta ) else : p = meta sent text = p . text . strip ( ) . split ( \" \" ) sent len = len ( sent text ) if ( word count + sent len ) > word limit : break else : word count += sent len yield sent text , p . idx", "predictions": ["limit sentences to a file ."], "references": ["iterator for the most significant sentences up to a specified limit"], "bleu": 0.1435549295013305, "rouge_l": 0.33516483516483514}
{"id": 852, "code": "def make sentence ( sent text ) : lex = [ ] idx = 0 for word in sent text : if len ( word ) > 0 : if ( idx > 0 ) and not ( word [ 0 ] in \",.:;!?-\\\"'\" ) : lex . append ( \" \" ) lex . append ( word ) idx += 1 return \"\" . join ( lex )", "predictions": ["concatenate a sentence into a byte string ."], "references": ["construct a sentence text with proper spacing"], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 853, "code": "def json iter ( path ) : with open ( path , 'r' ) as f : for line in f . readlines ( ) : yield json . loads ( line )", "predictions": ["read json file as json"], "references": ["iterator for json - per - line in a file pattern"], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 854, "code": "def pretty print ( obj , indent = False ) : if indent : return json . dumps ( obj , sort keys = True , indent = 2 , separators = ( ',' , ': ' ) ) else : return json . dumps ( obj , sort keys = True )", "predictions": ["outputs obj to json"], "references": ["pretty print a json object"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 855, "code": "def get object ( cls , api token , snapshot id ) : snapshot = cls ( token = api token , id = snapshot id ) snapshot . load ( ) return snapshot", "predictions": ["create a snapshot object for the given api token ."], "references": ["class method that will return a snapshot object by id ."], "bleu": 0.22447582175704436, "rouge_l": 0.37770897832817335}
{"id": 856, "code": "def load ( self ) : tags = self . get data ( \"tags/%s\" % self . name ) tag = tags [ 'tag' ] for attr in tag . keys ( ) : setattr ( self , attr , tag [ attr ] ) return self", "predictions": ["loads the tags from the database ."], "references": ["fetch data about tag"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 857, "code": "def create ( self , * * kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) params = { \"name\" : self . name } output = self . get data ( \"tags\" , type = \"POST\" , params = params ) if output : self . name = output [ 'tag' ] [ 'name' ] self . resources = output [ 'tag' ] [ 'resources' ]", "predictions": ["create this component s node"], "references": ["create the tag ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 858, "code": "def get object ( cls , api token , action id ) : action = cls ( token = api token , id = action id ) action . load directly ( ) return action", "predictions": ["load object and return an object ."], "references": ["class method that will return a action object by id ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 859, "code": "def get data ( self , * args , * * kwargs ) : data = super ( Droplet , self ) . get data ( * args , * * kwargs ) if \"type\" in kwargs : if kwargs [ \"type\" ] == POST : self . check actions in data ( data ) return data", "predictions": ["includes the metrics slugs in the class ."], "references": ["customized version of get_data to perform __check_actions_in_data"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 860, "code": "def get kernel available ( self ) : kernels = list ( ) data = self . get data ( \"droplets/%s/kernels/\" % self . id ) while True : for jsond in data [ u'kernels' ] : kernel = Kernel ( * * jsond ) kernel . token = self . token kernels . append ( kernel ) try : url = data [ u'links' ] [ u'pages' ] . get ( u'next' ) if not url : break data = self . get data ( url ) except Key Error : break return kernels", "predictions": ["get the kernel available kernel ."], "references": ["get a list of kernels available"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 861, "code": "def get object ( cls , api token , domain name ) : domain = cls ( token = api token , name = domain name ) domain . load ( ) return domain", "predictions": ["get a domain by api token"], "references": ["class method that will return a domain object by id ."], "bleu": 0.13576587000692578, "rouge_l": 0.33516483516483514}
{"id": 862, "code": "def get records ( self , params = None ) : if params is None : params = { } records = [ ] data = self . get data ( \"domains/%s/records/\" % self . name , type = GET , params = params ) for record data in data [ 'domain records' ] : record = Record ( domain name = self . name , * * record data ) record . token = self . token records . append ( record ) return records", "predictions": ["return all records for this sample"], "references": ["returns a list of record objects"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 863, "code": "def get object ( cls , api token ) : acct = cls ( token = api token ) acct . load ( ) return acct", "predictions": ["get an object from a remote host"], "references": ["class method that will return an account object ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 864, "code": "def get object ( cls , api token , firewall id ) : firewall = cls ( token = api token , id = firewall id ) firewall . load ( ) return firewall", "predictions": ["create and return a dir object based on the dir"], "references": ["class method that will return a firewall object by id ."], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 865, "code": "def add tags ( self , tags ) : return self . get data ( \"firewalls/%s/tags\" % self . id , type = POST , params = { \"tags\" : tags } )", "predictions": ["split tags into a notification ."], "references": ["add tags to this firewall ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 866, "code": "def remove tags ( self , tags ) : return self . get data ( \"firewalls/%s/tags\" % self . id , type = DELETE , params = { \"tags\" : tags } )", "predictions": ["filter quotes specified by quotes . ."], "references": ["remove tags from this firewall ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 867, "code": "def get object ( cls , api token , ssh key id ) : ssh key = cls ( token = api token , id = ssh key id ) ssh key . load ( ) return ssh key", "predictions": ["text for the = = = 0 ."], "references": ["class method that will return a sshkey object by id ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 868, "code": "def create ( self ) : input params = { \"name\" : self . name , \"public key\" : self . public key , } data = self . get data ( \"account/keys/\" , type = POST , params = input params ) if data : self . id = data [ 'ssh key' ] [ 'id' ]", "predictions": ["fix this text ."], "references": ["create the ssh key"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 869, "code": "def edit ( self ) : input params = { \"name\" : self . name , \"public key\" : self . public key , } data = self . get data ( \"account/keys/%s\" % self . id , type = PUT , params = input params ) if data : self . id = data [ 'ssh key' ] [ 'id' ]", "predictions": ["parse the 0 meta data meta meta data meta meta data meta meta meta data meta data meta meta data meta data meta meta meta data meta data meta data meta"], "references": ["edit the ssh key"], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 870, "code": "def get all regions ( self ) : data = self . get data ( \"regions/\" ) regions = list ( ) for jsoned in data [ 'regions' ] : region = Region ( * * jsoned ) region . token = self . token regions . append ( region ) return regions", "predictions": ["returns a list of all regions regions"], "references": ["this function returns a list of region object ."], "bleu": 0.38849358632832764, "rouge_l": 0.48897795591182364}
{"id": 871, "code": "def get all droplets ( self , tag name = None ) : params = dict ( ) if tag name : params [ \"tag name\" ] = tag name data = self . get data ( \"droplets/\" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( * * jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private ip address = net [ 'ip address' ] if net [ 'type' ] == 'public' : droplet . ip address = net [ 'ip address' ] if droplet . networks [ 'v6' ] : droplet . ip v6 address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip address' ] if \"backups\" in droplet . features : droplet . backups = True else : droplet . backups = False if \"ipv6\" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if \"private networking\" in droplet . features : droplet . private networking = True else : droplet . private networking = False droplets . append ( droplet ) return droplets", "predictions": ["get graph of graph"], "references": ["this function returns a list of droplet object ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 872, "code": "def get droplet ( self , droplet id ) : return Droplet . get object ( api token = self . token , droplet id = droplet id )", "predictions": ["write a dot object"], "references": ["return a droplet by its id ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 873, "code": "def get all sizes ( self ) : data = self . get data ( \"sizes/\" ) sizes = list ( ) for jsoned in data [ 'sizes' ] : size = Size ( * * jsoned ) size . token = self . token sizes . append ( size ) return sizes", "predictions": ["returns a list of all sizes sizes"], "references": ["this function returns a list of size object ."], "bleu": 0.38849358632832764, "rouge_l": 0.48897795591182364}
{"id": 874, "code": "def get images ( self , private = False , type = None ) : params = { } if private : params [ 'private' ] = 'true' if type : params [ 'type' ] = type data = self . get data ( \"images/\" , params = params ) images = list ( ) for jsoned in data [ 'images' ] : image = Image ( * * jsoned ) image . token = self . token images . append ( image ) return images", "predictions": ["build a list of all rank rank and their current token"], "references": ["this function returns a list of image object ."], "bleu": 0.21200626759025185, "rouge_l": 0.3055091819699499}
{"id": 875, "code": "def get all domains ( self ) : data = self . get data ( \"domains/\" ) domains = list ( ) for jsoned in data [ 'domains' ] : domain = Domain ( * * jsoned ) domain . token = self . token domains . append ( domain ) return domains", "predictions": ["returns a list of all domains"], "references": ["this function returns a list of domain object ."], "bleu": 0.37288786399304175, "rouge_l": 0.5147679324894514}
{"id": 876, "code": "def get domain ( self , domain name ) : return Domain . get object ( api token = self . token , domain name = domain name )", "predictions": ["enumerate a chunks for a chunks"], "references": ["return a domain by its domain_name"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 877, "code": "def get all sshkeys ( self ) : data = self . get data ( \"account/keys/\" ) ssh keys = list ( ) for jsoned in data [ 'ssh keys' ] : ssh key = SSH Key ( * * jsoned ) ssh key . token = self . token ssh keys . append ( ssh key ) return ssh keys", "predictions": ["returns a list of all in the in in the in order"], "references": ["this function returns a list of sshkey object ."], "bleu": 0.2891784933232571, "rouge_l": 0.39102564102564097}
{"id": 878, "code": "def get ssh key ( self , ssh key id ) : return SSH Key . get object ( api token = self . token , ssh key id = ssh key id )", "predictions": ["return can be a url or a resourcelocator"], "references": ["return a sshkey object by its id ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 879, "code": "def get all tags ( self ) : data = self . get data ( \"tags\" ) return [ Tag ( token = self . token , * * tag ) for tag in data [ 'tags' ] ]", "predictions": ["returns all tags objects"], "references": ["this method returns a list of all tags ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 880, "code": "def get all floating ips ( self ) : data = self . get data ( \"floating ips\" ) floating ips = list ( ) for jsoned in data [ 'floating ips' ] : floating ip = Floating IP ( * * jsoned ) floating ip . token = self . token floating ips . append ( floating ip ) return floating ips", "predictions": ["returns a list of all floating data m m m m m m m m m m m m m m m m m m m m m m m"], "references": ["this function returns a list of floatingip objects ."], "bleu": 0.11227564716917941, "rouge_l": 0.2271880819366853}
{"id": 881, "code": "def get floating ip ( self , ip ) : return Floating IP . get object ( api token = self . token , ip = ip )", "predictions": ["key for a particular ip"], "references": ["returns a of floatingip object by its ip address ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 882, "code": "def get all load balancers ( self ) : data = self . get data ( \"load balancers\" ) load balancers = list ( ) for jsoned in data [ 'load balancers' ] : load balancer = Load Balancer ( * * jsoned ) load balancer . token = self . token load balancer . health check = Health Check ( * * jsoned [ 'health check' ] ) load balancer . sticky sessions = Sticky Sesions ( * * jsoned [ 'sticky sessions' ] ) forwarding rules = list ( ) for rule in jsoned [ 'forwarding rules' ] : forwarding rules . append ( Forwarding Rule ( * * rule ) ) load balancer . forwarding rules = forwarding rules load balancers . append ( load balancer ) return load balancers", "predictions": ["returns a list of all load path"], "references": ["returns a list of load balancer objects ."], "bleu": 0.46905226098954195, "rouge_l": 0.6587473002159828}
{"id": 883, "code": "def get all certificates ( self ) : data = self . get data ( \"certificates\" ) certificates = list ( ) for jsoned in data [ 'certificates' ] : cert = Certificate ( * * jsoned ) cert . token = self . token certificates . append ( cert ) return certificates", "predictions": ["returns a list of all certificates certificates"], "references": ["this function returns a list of certificate objects ."], "bleu": 0.38849358632832764, "rouge_l": 0.48897795591182364}
{"id": 884, "code": "def get snapshot ( self , snapshot id ) : return Snapshot . get object ( api token = self . token , snapshot id = snapshot id )", "predictions": ["make a sentence object from the sentence"], "references": ["return a snapshot by its id ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 885, "code": "def get all snapshots ( self ) : data = self . get data ( \"snapshots/\" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]", "predictions": ["returns all snapshots ."], "references": ["this method returns a list of all snapshots ."], "bleu": 0.20258948470231466, "rouge_l": 0.5754716981132075}
{"id": 886, "code": "def get droplet snapshots ( self ) : data = self . get data ( \"snapshots?resource type=droplet\" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]", "predictions": ["returns a list of print snapshots snapshots"], "references": ["this method returns a list of all snapshots based on droplets ."], "bleu": 0.26488231194950745, "rouge_l": 0.5024711696869852}
{"id": 887, "code": "def get volume snapshots ( self ) : data = self . get data ( \"snapshots?resource type=volume\" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]", "predictions": ["returns a list of all the object snapshots"], "references": ["this method returns a list of all snapshots based on volumes ."], "bleu": 0.3702709453863886, "rouge_l": 0.5791139240506329}
{"id": 888, "code": "def get all volumes ( self , region = None ) : if region : url = \"volumes?region={}\" . format ( region ) else : url = \"volumes\" data = self . get data ( url ) volumes = list ( ) for jsoned in data [ 'volumes' ] : volume = Volume ( * * jsoned ) volume . token = self . token volumes . append ( volume ) return volumes", "predictions": ["load all self ."], "references": ["this function returns a list of volume objects ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 889, "code": "def get volume ( self , volume id ) : return Volume . get object ( api token = self . token , volume id = volume id )", "predictions": ["create a volume object"], "references": ["returns a volume object by its id ."], "bleu": 0.24601580968354606, "rouge_l": 0.47164948453608246}
{"id": 890, "code": "def get all firewalls ( self ) : data = self . get data ( \"firewalls\" ) firewalls = list ( ) for jsoned in data [ 'firewalls' ] : firewall = Firewall ( * * jsoned ) firewall . token = self . token in rules = list ( ) for rule in jsoned [ 'inbound rules' ] : in rules . append ( Inbound Rule ( * * rule ) ) firewall . inbound rules = in rules out rules = list ( ) for rule in jsoned [ 'outbound rules' ] : out rules . append ( Outbound Rule ( * * rule ) ) firewall . outbound rules = out rules firewalls . append ( firewall ) return firewalls", "predictions": ["returns a list of firewall . to look up to firewall ."], "references": ["this function returns a list of firewall objects ."], "bleu": 0.39553325358771796, "rouge_l": 0.5865384615384615}
{"id": 891, "code": "def get firewall ( self , firewall id ) : return Firewall . get object ( api token = self . token , firewall id = firewall id , )", "predictions": ["obtain a data based on the data returned by the data"], "references": ["return a firewall by its id ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 892, "code": "def get object ( cls , api token , domain , record id ) : record = cls ( token = api token , domain = domain , id = record id ) record . load ( ) return record", "predictions": ["construct a = = = = = = = = = = false"], "references": ["class method that will return a record object by id and the domain ."], "bleu": 0.08844818008721958, "rouge_l": 0.0735826296743064}
{"id": 893, "code": "def get object ( cls , api token , volume id ) : volume = cls ( token = api token , id = volume id ) volume . load ( ) return volume", "predictions": ["return an object based on the domain and domain domain ."], "references": ["class method that will return an volume object by id ."], "bleu": 0.17033186037639278, "rouge_l": 0.36363636363636365}
{"id": 894, "code": "def get object ( cls , api token , cert id ) : certificate = cls ( token = api token , id = cert id ) certificate . load ( ) return certificate", "predictions": ["get a is allowed is used for the specified params }"], "references": ["class method that will return a certificate object by its id ."], "bleu": 0.10400927574124633, "rouge_l": 0.08628005657708629}
{"id": 895, "code": "def stop ( self , pin ) : if pin not in self . pwm : raise Value Error ( 'Pin {0} is not configured as a PWM.  Make sure to first call start for the pin.' . format ( pin ) ) self . pwm [ pin ] . stop ( ) del self . pwm [ pin ]", "predictions": ["get the first pin pin"], "references": ["stop pwm output on specified pin ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 896, "code": "def enable FTDI driver ( ) : logger . debug ( 'Enabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) check running as root ( ) subprocess . check call ( 'kextload -b com.apple.driver.Apple USBFTDI' , shell = True ) subprocess . check call ( 'kextload /System/Library/Extensions/FTDIUSB Serial Driver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) check running as root ( ) subprocess . check call ( 'modprobe -q ftdi sio' , shell = True ) subprocess . check call ( 'modprobe -q usbserial' , shell = True )", "predictions": ["enable ftdi driver ."], "references": ["re - enable the ftdi drivers for the current platform ."], "bleu": 0.07425134808660917, "rouge_l": 0.3689516129032258}
{"id": 897, "code": "def close ( self ) : if self . ctx is not None : ftdi . free ( self . ctx ) self . ctx = None", "predictions": ["closes the connection ."], "references": ["close the ftdi device . will be automatically called when the program ends ."], "bleu": 0.032639898338235177, "rouge_l": 0.20198675496688742}
{"id": 898, "code": "def mpsse enable ( self ) : self . check ( ftdi . set bitmode , 0 , 0 ) self . check ( ftdi . set bitmode , 0 , 2 )", "predictions": ["enable the mpsse ."], "references": ["enable mpsse mode on the ftdi device ."], "bleu": 0.16620830006469264, "rouge_l": 0.47164948453608246}
{"id": 899, "code": "def idle ( self ) : self . ft232h . setup pins ( { 0 : GPIO . OUT , 1 : GPIO . OUT , 2 : GPIO . IN } , { 0 : GPIO . HIGH , 1 : GPIO . HIGH } )", "predictions": ["setup idle time ."], "references": ["put i2c lines into idle state ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 900, "code": "def transaction end ( self ) : self . command . append ( '\\x87' ) self . ft232h . write ( '' . join ( self . command ) ) return bytearray ( self . ft232h . poll read ( self . expected ) )", "predictions": ["end of the transaction ."], "references": ["end i2c transaction and get response bytes including acks ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 901, "code": "def i2c write bytes ( self , data ) : for byte in data : self . command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) self . ft232h . output pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . command . append ( self . ft232h . mpsse gpio ( ) * REPEAT DELAY ) self . command . append ( '\\x22\\x00' ) self . expected += len ( data )", "predictions": ["write i2c i2c data ."], "references": ["write the specified number of bytes to the chip ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 902, "code": "def write16 ( self , register , value , little endian = True ) : value = value & 0x FFFF value low = value & 0x FF value high = ( value >> 8 ) & 0x FF if not little endian : value low , value high = value high , value low self . idle ( ) self . transaction start ( ) self . i2c start ( ) self . i2c write bytes ( [ self . address byte ( False ) , register , value low , value high ] ) self . i2c stop ( ) response = self . transaction end ( ) self . verify acks ( response )", "predictions": ["set a sharq value"], "references": ["write a 16 - bit value to the specified register ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 903, "code": "def write List ( self , register , data ) : self . idle ( ) self . transaction start ( ) self . i2c start ( ) self . i2c write bytes ( [ self . address byte ( False ) , register ] + data ) self . i2c stop ( ) response = self . transaction end ( ) self . verify acks ( response )", "predictions": ["writes a successful api transaction to the crazyflie ."], "references": ["write bytes to the specified register ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 904, "code": "def read S8 ( self , register ) : result = self . read U8 ( register ) if result > 127 : result -= 256 return result", "predictions": ["read a s8 from the given register ."], "references": ["read a signed byte from the specified register ."], "bleu": 0.27375196856259226, "rouge_l": 0.6984732824427481}
{"id": 905, "code": "def write8 ( self , register , value ) : value = value & 0x FF self . bus . write byte data ( self . address , register , value ) self . logger . debug ( \"Wrote 0x%02X to register 0x%02X\" , value , register )", "predictions": ["writes a 8 - bit value to the specified register register ."], "references": ["write an 8 - bit value to the specified register ."], "bleu": 0.6850836912969523, "rouge_l": 0.7887931034482759}
{"id": 906, "code": "def write16 ( self , register , value ) : value = value & 0x FFFF self . bus . write word data ( self . address , register , value ) self . logger . debug ( \"Wrote 0x%04X to register pair 0x%02X, 0x%02X\" , value , register , register + 1 )", "predictions": ["set a value in a memory pair ."], "references": ["write a 16 - bit value to the specified register ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 907, "code": "def write List ( self , register , data ) : self . bus . write i2c block data ( self . address , register , data ) self . logger . debug ( \"Wrote to register 0x%02X: %s\" , register , data )", "predictions": ["writes data to the crazyflie device ."], "references": ["write bytes to the specified register ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 908, "code": "def read U8 ( self , register ) : result = self . bus . read byte data ( self . address , register ) & 0x FF self . logger . debug ( \"Read 0x%02X from register 0x%02X\" , result , register ) return result", "predictions": ["read an unsigned integer from the specified register ."], "references": ["read an unsigned byte from the specified register ."], "bleu": 0.6580370064762462, "rouge_l": 0.8888888888888888}
{"id": 909, "code": "def all info files ( self ) : try : for info file in list files in dir ( self . info dir ) : if not os . path . basename ( info file ) . endswith ( '.trashinfo' ) : self . on non trashinfo found ( ) else : yield info file except OS Error : pass", "predictions": ["return an iterator over all info files ."], "references": ["returns a generator of path s"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 910, "code": "def add bpmn files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add bpmn xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )", "predictions": ["add filenames to the xml file ."], "references": ["add all filenames in the given list to the parser s set ."], "bleu": 0.12337170820562471, "rouge_l": 0.474339035769829}
{"id": 911, "code": "def one ( nodes , or none = False ) : if not nodes and or none : return None assert len ( nodes ) == 1 , 'Expected 1 result. Received %d results.' % ( len ( nodes ) ) return nodes [ 0 ]", "predictions": ["return true if nodes are one of the node ."], "references": ["assert that there is exactly one node in the give list and return it ."], "bleu": 0.10062578380622589, "rouge_l": 0.23164556962025318}
{"id": 912, "code": "def get event definition ( self ) : message Event Definition = first ( self . xpath ( './/bpmn:message Event Definition' ) ) if message Event Definition is not None : return self . get message event definition ( message Event Definition ) timer Event Definition = first ( self . xpath ( './/bpmn:timer Event Definition' ) ) if timer Event Definition is not None : return self . get timer event definition ( timer Event Definition ) raise Not Implemented Error ( 'Unsupported Intermediate Catch Event: %r' , ET . tostring ( self . node ) )", "predictions": ["get the event definition for the event ."], "references": ["parse the event definition node and return an instance of event"], "bleu": 0.21690743377623947, "rouge_l": 0.4093959731543625}
{"id": 913, "code": "def parse node ( self ) : try : self . task = self . create task ( ) self . task . documentation = self . parser . parse documentation ( self . node , xpath = self . xpath , task parser = self ) boundary event nodes = self . process xpath ( './/bpmn:boundary Event[@attached To Ref=\"%s\"]' % self . get id ( ) ) if boundary event nodes : parent task = Boundary Event Parent ( self . spec , '%s.Boundary Event Parent' % self . get id ( ) , self . task , lane = self . task . lane ) self . process parser . parsed nodes [ self . node . get ( 'id' ) ] = parent task parent task . connect outgoing ( self . task , '%s.From Boundary Event Parent' % self . get id ( ) , None , None ) for boundary event in boundary event nodes : b = self . process parser . parse node ( boundary event ) parent task . connect outgoing ( b , '%s.From Boundary Event Parent' % boundary event . get ( 'id' ) , None , None ) else : self . process parser . parsed nodes [ self . node . get ( 'id' ) ] = self . task children = [ ] outgoing = self . process xpath ( './/bpmn:sequence Flow[@source Ref=\"%s\"]' % self . get id ( ) ) if len ( outgoing ) > 1 and not self . handles multiple outgoing ( ) : raise Validation Exception ( 'Multiple outgoing flows are not supported for ' 'tasks of type' , node = self . node , filename = self . process parser . filename ) for sequence flow in outgoing : target ref = sequence flow . get ( 'target Ref' ) target node = one ( self . process xpath ( './/*[@id=\"%s\"]' % target ref ) ) c = self . process parser . parse node ( target node ) children . append ( ( c , target node , sequence flow ) ) if children : default outgoing = self . node . get ( 'default' ) if not default outgoing : ( c , target node , sequence flow ) = children [ 0 ] default outgoing = sequence flow . get ( 'id' ) for ( c , target node , sequence flow ) in children : self . connect outgoing ( c , target node , sequence flow , sequence flow . get ( 'id' ) == default outgoing ) return parent task if boundary event nodes else self . task except Validation Exception : raise except Exception as ex : exc info = sys . exc info ( ) tb = \"\" . join ( traceback . format exception ( exc info [ 0 ] , exc info [ 1 ] , exc info [ 2 ] ) ) LOG . error ( \"%r\\n%s\" , ex , tb ) raise Validation Exception ( \"%r\" % ( ex ) , node = self . node , filename = self . process parser . filename )", "predictions": ["parse a node and execute the node ."], "references": ["parse this node and all children returning the connected task spec ."], "bleu": 0.15223083300988077, "rouge_l": 0.4825949367088607}
{"id": 914, "code": "def get outgoing sequence names ( self ) : return sorted ( [ s . name for s in list ( self . outgoing sequence flows by id . values ( ) ) ] )", "predictions": ["return a list of all outgoing sequence names ."], "references": ["returns a list of the names of outgoing sequences . some may be none ."], "bleu": 0.15512258520268646, "rouge_l": 0.3986928104575163}
{"id": 915, "code": "def start ( self , my task , force = False ) : if ( not hasattr ( my task , 'subprocess' ) ) or my task . subprocess is None : my task . subprocess = subprocess . Popen ( self . args , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) if my task . subprocess : my task . subprocess . poll ( ) if my task . subprocess . returncode is None : return False else : results = my task . subprocess . communicate ( ) my task . results = results return True return False", "predictions": ["start a task ."], "references": ["returns false when successfully fired true otherwise"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 916, "code": "def ready ( self ) : if self . has state ( self . COMPLETED ) or self . has state ( self . CANCELLED ) : return self . set state ( self . READY ) self . task spec . on ready ( self )", "predictions": ["whether the connection is ready to be ready ."], "references": ["marks the task as ready for execution ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 917, "code": "def get state name ( self ) : state name = [ ] for state , name in list ( self . state names . items ( ) ) : if self . has state ( state ) : state name . append ( name ) return '|' . join ( state name )", "predictions": ["return the state of the state of the state ."], "references": ["returns a textual representation of this task s state ."], "bleu": 0.17827531042796255, "rouge_l": 0.3}
{"id": 918, "code": "def inherit data ( self ) : LOG . debug ( \"'%s' inheriting data from '%s'\" % ( self . get name ( ) , self . parent . get name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set data ( * * self . parent . data )", "predictions": ["attempt to get inherit data ."], "references": ["inherits the data from the parent ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 919, "code": "def eval args ( args , my task ) : results = [ ] for arg in args : if isinstance ( arg , Attrib ) or isinstance ( arg , Path Attrib ) : results . append ( valueof ( my task , arg ) ) else : results . append ( arg ) return results", "predictions": ["evaluates a list of args and returns a list of args that were passed to the task"], "references": ["parses args and evaluates any attrib entries"], "bleu": 0.10216198665886358, "rouge_l": 0.18020679468242246}
{"id": 920, "code": "def eval kwargs ( kwargs , my task ) : results = { } for kwarg , value in list ( kwargs . items ( ) ) : if isinstance ( value , Attrib ) or isinstance ( value , Path Attrib ) : results [ kwarg ] = valueof ( my task , value ) else : results [ kwarg ] = value return results", "predictions": ["evaluates the kwargs to pass to the task to the kwargs"], "references": ["parses kwargs and evaluates any attrib entries"], "bleu": 0.12605968092174913, "rouge_l": 0.1157495256166983}
{"id": 921, "code": "def restart ( self , my task ) : if not my task . has state ( Task . WAITING ) : raise Workflow Exception ( my task , \"Cannot refire a task that is not\" \"in WAITING state\" ) if my task . get internal data ( 'task id' ) is not None : if not hasattr ( my task , 'async call' ) : task id = my task . get internal data ( 'task id' ) my task . async call = default app . Async Result ( task id ) my task . deserialized = True my task . async call . state async call = my task . async call if async call . state == 'FAILED' : pass elif async call . state in [ 'RETRY' , 'PENDING' , 'STARTED' ] : async call . revoke ( ) LOG . info ( \"Celery task '%s' was in %s state and was revoked\" % ( async call . state , async call ) ) elif async call . state == 'SUCCESS' : LOG . warning ( \"Celery task '%s' succeeded, but a refire was \" \"requested\" % async call ) self . clear celery task data ( my task ) return self . start ( my task )", "predictions": ["restart a task from a task ."], "references": ["abort celery task and retry it"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 922, "code": "def clear celery task data ( self , my task ) : if 'task id' in my task . internal data : history = my task . get internal data ( 'task history' , [ ] ) history . append ( my task . get internal data ( 'task id' ) ) del my task . internal data [ 'task id' ] my task . set internal data ( task history = history ) if 'task state' in my task . internal data : del my task . internal data [ 'task state' ] if 'error' in my task . internal data : del my task . internal data [ 'error' ] if hasattr ( my task , 'async call' ) : delattr ( my task , 'async call' ) if hasattr ( my task , 'deserialized' ) : delattr ( my task , 'deserialized' )", "predictions": ["removes any data from the celery task ."], "references": ["clear celery task data"], "bleu": 0.22679164443904004, "rouge_l": 0.3546511627906977}
{"id": 923, "code": "def start ( self , my task , force = False ) : if not hasattr ( my task , 'async call' ) and my task . get internal data ( 'task id' ) is not None : task id = my task . get internal data ( 'task id' ) my task . async call = default app . Async Result ( task id ) my task . deserialized = True LOG . debug ( \"Reanimate Async Call %s\" % task id ) if not hasattr ( my task , 'async call' ) : self . send call ( my task ) if getattr ( my task , \"deserialized\" , False ) : my task . async call . state if my task . async call . state == 'FAILURE' : LOG . debug ( \"Async Call for task '%s' failed: %s\" % ( my task . get name ( ) , my task . async call . info ) ) info = { } info [ 'traceback' ] = my task . async call . traceback info [ 'info' ] = Serializable ( my task . async call . info ) info [ 'state' ] = my task . async call . state my task . set internal data ( task state = info ) elif my task . async call . state == 'RETRY' : info = { } info [ 'traceback' ] = my task . async call . traceback info [ 'info' ] = Serializable ( my task . async call . info ) info [ 'state' ] = my task . async call . state my task . set internal data ( task state = info ) elif my task . async call . ready ( ) : result = my task . async call . result if isinstance ( result , Exception ) : LOG . warn ( \"Celery call %s failed: %s\" % ( self . call , result ) ) my task . set internal data ( error = Serializable ( result ) ) return False LOG . debug ( \"Completed celery call %s with result=%s\" % ( self . call , result ) ) if self . result key : data = { self . result key : result } else : if isinstance ( result , dict ) : data = result else : data = { 'result' : result } if self . merge results : merge dictionary ( my task . internal data , data ) else : my task . set data ( * * data ) return True else : LOG . debug ( \"async call.ready()=%s. Try Fire for '%s' \" \"returning False\" % ( my task . async call . ready ( ) , my task . get name ( ) ) ) return False", "predictions": ["start the task call ."], "references": ["returns false when successfully fired true otherwise"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 924, "code": "def ancestors ( self ) : results = [ ] def recursive find ancestors ( task , stack ) : for input in task . inputs : if input not in stack : stack . append ( input ) recursive find ancestors ( input , stack ) recursive find ancestors ( self , results ) return results", "predictions": ["returns the list of ancestors that are ancestors in the stack"], "references": ["returns list of ancestor task specs based on inputs"], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 925, "code": "def package for editor signavio ( self , spec , filename ) : signavio file = filename [ : - len ( '.bpmn20.xml' ) ] + '.signavio.xml' if os . path . exists ( signavio file ) : self . write file to package zip ( \"src/\" + self . get zip path ( signavio file ) , signavio file ) f = open ( signavio file , 'r' ) try : signavio tree = ET . parse ( f ) finally : f . close ( ) svg node = one ( signavio tree . findall ( './/svg-representation' ) ) self . write to package zip ( \"%s.svg\" % spec . name , svg node . text )", "predictions": ["write signavio file for signavio"], "references": ["adds the svg files to the archive for this bpmn file ."], "bleu": 0.07450619999160439, "rouge_l": 0.1095152603231598}
{"id": 926, "code": "def write meta data ( self ) : config = configparser . Config Parser ( ) config . add section ( 'Meta Data' ) config . set ( 'Meta Data' , 'entry point process' , self . wf spec . name ) if self . editor : config . set ( 'Meta Data' , 'editor' , self . editor ) for k , v in self . meta data : config . set ( 'Meta Data' , k , v ) if not self . PARSER CLASS == Bpmn Parser : config . set ( 'Meta Data' , 'parser class module' , inspect . getmodule ( self . PARSER CLASS ) . name ) config . set ( 'Meta Data' , 'parser class' , self . PARSER CLASS . name ) ini = String IO ( ) config . write ( ini ) self . write to package zip ( self . METADATA FILE , ini . getvalue ( ) )", "predictions": ["write meta data to the config file ."], "references": ["writes the metadata . ini file to the archive ."], "bleu": 0.1867587389639562, "rouge_l": 0.3267857142857143}
{"id": 927, "code": "def add main options ( cls , parser ) : parser . add option ( \"-o\" , \"--output\" , dest = \"package file\" , help = \"create the BPMN package in the specified file\" ) parser . add option ( \"-p\" , \"--process\" , dest = \"entry point process\" , help = \"specify the entry point process\" ) parser . add option ( \"-c\" , \"--config-file\" , dest = \"config file\" , help = \"specify a config file to use\" ) parser . add option ( \"-i\" , \"--initialise-config-file\" , action = \"store true\" , dest = \"init config file\" , default = False , help = \"create a new config file from the specified options\" ) group = Option Group ( parser , \"BPMN Editor Options\" , \"These options are not required, but may be \" \" provided to activate special features of \" \"supported BPMN editors.\" ) group . add option ( \"--editor\" , dest = \"editor\" , help = \"editors with special support: signavio\" ) parser . add option group ( group )", "predictions": ["activate the main cli options ."], "references": ["override in subclass if required ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 928, "code": "def add additional options ( cls , parser ) : group = Option Group ( parser , \"Target Engine Options\" , \"These options are not required, but may be \" \"provided if a specific \" \"BPMN application engine is targeted.\" ) group . add option ( \"-e\" , \"--target-engine\" , dest = \"target engine\" , help = \"target the specified BPMN application engine\" ) group . add option ( \"-t\" , \"--target-version\" , dest = \"target engine version\" , help = \"target the specified version of the BPMN application engine\" ) parser . add option group ( group )", "predictions": ["enable additional driver driver ."], "references": ["override in subclass if required ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 929, "code": "def check args ( cls , config , options , args , parser , package file = None ) : if not args : parser . error ( \"no input files specified\" ) if not ( package file or options . package file ) : parser . error ( \"no package file specified\" ) if not options . entry point process : parser . error ( \"no entry point process specified\" )", "predictions": ["checks and reformat the arguments provided to the arguments . . . . . . . . ."], "references": ["override in subclass if required ."], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 930, "code": "def merge options and config ( cls , config , options , args ) : if args : config . set ( CONFIG SECTION NAME , 'input files' , ',' . join ( args ) ) elif config . has option ( CONFIG SECTION NAME , 'input files' ) : for i in config . get ( CONFIG SECTION NAME , 'input files' ) . split ( ',' ) : if not os . path . isabs ( i ) : i = os . path . abspath ( os . path . join ( os . path . dirname ( options . config file ) , i ) ) args . append ( i ) cls . merge option and config str ( 'package file' , config , options ) cls . merge option and config str ( 'entry point process' , config , options ) cls . merge option and config str ( 'target engine' , config , options ) cls . merge option and config str ( 'target engine version' , config , options ) cls . merge option and config str ( 'editor' , config , options )", "predictions": ["merge enable enable enable enable enable enable enable enable enable . . . . ."], "references": ["override in subclass if required ."], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 931, "code": "def create meta data ( cls , options , args , parser ) : meta data = [ ] meta data . append ( ( 'spiff version' , cls . get version ( ) ) ) if options . target engine : meta data . append ( ( 'target engine' , options . target engine ) ) if options . target engine : meta data . append ( ( 'target engine version' , options . target engine version ) ) return meta data", "predictions": ["idle to idle self 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"], "references": ["override in subclass if required ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 932, "code": "def add notify ( self , task spec ) : if task spec . name in self . task specs : raise Key Error ( 'Duplicate task spec name: ' + task spec . name ) self . task specs [ task spec . name ] = task spec task spec . id = len ( self . task specs )", "predictions": ["transaction a end task ."], "references": ["called by a task spec when it was added into the workflow ."], "bleu": 0.06554932163900559, "rouge_l": 0.3086003372681282}
{"id": 933, "code": "def get ready user tasks ( self ) : return [ t for t in self . get tasks ( Task . READY ) if not self . is engine task ( t . task spec ) ]", "predictions": ["return - order all write tasks tasks command command command command command command command command command command"], "references": ["returns a list of user tasks that are ready for user action"], "bleu": 0.07223943354597204, "rouge_l": 0.07117852975495916}
{"id": 934, "code": "def deserialize ( cls , serializer , wf spec , s state , * * kwargs ) : return serializer . deserialize trigger ( wf spec , s state , * * kwargs )", "predictions": ["deserialize a register from a register register register register . . . . . ."], "references": ["deserializes the trigger using the provided serializer ."], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 935, "code": "def execute ( self , task , script , * * kwargs ) : locals ( ) . update ( kwargs ) exec ( script )", "predictions": ["execute a task and execute the result of a task"], "references": ["execute the script within the context of the specified task"], "bleu": 0.18850319022747347, "rouge_l": 0.4}
{"id": 936, "code": "def container id ( self , name ) : container = self . containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None", "predictions": ["256 id for a read group"], "references": ["try to find the container id with the specified name"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 937, "code": "def update ( self , containers ) : self . containers = deepcopy ( containers ) self . write ( containers , initialize = False )", "predictions": ["update will update the specified register register register"], "references": ["update the current state file with the specified contents"], "bleu": 0.22149455506955362, "rouge_l": 0.34923664122137404}
{"id": 938, "code": "def load ( self ) : try : with open ( self . state file ) as f : state = yaml . safe load ( f ) self . containers = state [ 'containers' ] except ( IO Error , OS Error ) as err : if err . errno == errno . ENOENT : raise Not Initialized Error ( \"No blockade exists in this context\" ) raise Inconsistent State Error ( \"Failed to load Blockade state: \" + str ( err ) ) except Exception as err : raise Inconsistent State Error ( \"Failed to load Blockade state: \" + str ( err ) )", "predictions": ["load the & load & load the & ."], "references": ["try to load a blockade state file in the current directory"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 939, "code": "def get blockade id from cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) parent dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent dir ) . lower ( ) blockade id = re . sub ( r\"[^a-z0-9]\" , \"\" , basename ) if not blockade id : blockade id = \"default\" return blockade id", "predictions": ["write the blockade id id to the cwd cwd"], "references": ["generate a new blockade id based on the cwd"], "bleu": 0.23356898886410005, "rouge_l": 0.4444444444444444}
{"id": 940, "code": "def assure dir ( self ) : try : os . makedirs ( self . state dir ) except OS Error as err : if err . errno != errno . EEXIST : raise", "predictions": ["read the directory if it doesn t exist ."], "references": ["make sure the state directory exists"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 941, "code": "def state delete ( self ) : try : os . remove ( self . state file ) except OS Error as err : if err . errno not in ( errno . EPERM , errno . ENOENT ) : raise try : os . rmdir ( self . state dir ) except OS Error as err : if err . errno not in ( errno . ENOTEMPTY , errno . ENOENT ) : raise", "predictions": ["info about the all of the all of the all of the all of the all changes in the all the all the all of the all changes"], "references": ["try to delete the state . yml file and the folder . blockade"], "bleu": 0.0478968583748614, "rouge_l": 0.10445205479452054}
{"id": 942, "code": "def write ( self , containers , initialize = True ) : path = self . state file self . assure dir ( ) try : flags = os . O WRONLY | os . O CREAT if initialize : flags |= os . O EXCL with os . fdopen ( os . open ( path , flags ) , \"w\" ) as f : yaml . safe dump ( self . base state ( containers ) , f ) except OS Error as err : if err . errno == errno . EEXIST : raise Already Initialized Error ( \"Path %s exists. \" \"You may need to destroy a previous blockade.\" % path ) raise except Exception : self . state delete ( ) raise", "predictions": ["add the f to the f in the f in the f in the f in the f directory"], "references": ["write the given state information into a file"], "bleu": 0.06439931429457924, "rouge_l": 0.0799475753604194}
{"id": 943, "code": "def insert rule ( self , chain , src = None , dest = None , target = None ) : if not chain : raise Value Error ( \"Invalid chain\" ) if not target : raise Value Error ( \"Invalid target\" ) if not ( src or dest ) : raise Value Error ( \"Need src, dest, or both\" ) args = [ \"-I\" , chain ] if src : args += [ \"-s\" , src ] if dest : args += [ \"-d\" , dest ] args += [ \"-j\" , target ] self . call ( * args )", "predictions": ["one rule rule rule rule"], "references": ["insert a new rule in the chain"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 944, "code": "def sm start ( self , * args , * * kwargs ) : millisec = random . randint ( self . start min delay , self . start max delay ) self . timer = threading . Timer ( millisec / 1000.0 , self . event timeout ) self . timer . start ( )", "predictions": ["initialize the raise an exception if not already started ."], "references": ["start the timer waiting for pain"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 945, "code": "def sm to pain ( self , * args , * * kwargs ) : logger . info ( \"Starting chaos for blockade %s\" % self . blockade name ) self . do blockade event ( ) millisec = random . randint ( self . run min time , self . run max time ) self . timer = threading . Timer ( millisec / 1000.0 , self . event timeout ) self . timer . start ( )", "predictions": ["terminates a worker in the event loop parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser parser"], "references": ["start the blockade event"], "bleu": 0.04317900023606586, "rouge_l": 0.13275299238302501}
{"id": 946, "code": "def sm stop from no pain ( self , * args , * * kwargs ) : logger . info ( \"Stopping chaos for blockade %s\" % self . blockade name ) self . timer . cancel ( )", "predictions": ["outgoing sequence sequence sequence from names for names for the timer for the timer for the timer for it ."], "references": ["stop chaos when there is no current blockade operation"], "bleu": 0.051366639095059514, "rouge_l": 0.0}
{"id": 947, "code": "def sm relieve pain ( self , * args , * * kwargs ) : logger . info ( \"Ending the degradation for blockade %s\" % self . blockade name ) self . do reset all ( ) millisec = random . randint ( self . start min delay , self . start max delay ) self . timer = threading . Timer ( millisec / 1000.0 , self . event timeout ) self . timer . start ( )", "predictions": ["restarts the . . . . reset subprocess with the ."], "references": ["end the blockade event and return to a steady state"], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 948, "code": "def sm stop from pain ( self , * args , * * kwargs ) : logger . info ( \"Stopping chaos for blockade %s\" % self . blockade name ) self . do reset all ( )", "predictions": ["stop all self set self set set set set set set all set set set set set set set set set all to true"], "references": ["stop chaos while there is a blockade event in progress"], "bleu": 0.050661968099322066, "rouge_l": 0.06354166666666666}
{"id": 949, "code": "def sm cleanup ( self , * args , * * kwargs ) : if self . done notification func is not None : self . done notification func ( ) self . timer . cancel ( )", "predictions": ["state machine during the list of tasks that we call this method ."], "references": ["delete all state associated with the chaos session"], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 950, "code": "def cmd up ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) containers = b . create ( verbose = opts . verbose , force = opts . force ) print containers ( containers , opts . json )", "predictions": ["run the command data data data data . ."], "references": ["start the containers and link them together"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 951, "code": "def cmd destroy ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) b . destroy ( )", "predictions": ["args for a virtual server"], "references": ["destroy all containers and restore networks"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 952, "code": "def cmd status ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) containers = b . status ( ) print containers ( containers , opts . json )", "predictions": ["show the kwargs of the master"], "references": ["print status of containers and networks"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 953, "code": "def cmd kill ( opts ) : kill signal = opts . signal if hasattr ( opts , 'signal' ) else \"SIGKILL\" with containers ( opts , Blockade . kill , signal = kill signal )", "predictions": ["kills a signal instance"], "references": ["kill some or all containers"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 954, "code": "def cmd join ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) b . join ( )", "predictions": ["celery command line options ."], "references": ["restore full networking between containers"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 955, "code": "def cmd logs ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) puts ( b . logs ( opts . container ) . decode ( encoding = 'UTF-8' ) )", "predictions": ["render the current logs logs"], "references": ["fetch the logs of a container"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 956, "code": "def cmd daemon ( opts ) : if opts . data dir is None : raise Blockade Error ( \"You must supply a data directory for the daemon\" ) rest . start ( data dir = opts . data dir , port = opts . port , debug = opts . debug , host exec = get host exec ( ) )", "predictions": ["start a daemon if it is running"], "references": ["start the blockade rest api"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 957, "code": "def cmd add ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) b . add container ( opts . containers )", "predictions": ["for a given config for the current cluster . . . . ."], "references": ["add one or more existing docker containers to a blockade group"], "bleu": 0.09552040806823771, "rouge_l": 0.08460471567267684}
{"id": 958, "code": "def cmd events ( opts ) : config = load config ( opts . config ) b = get blockade ( config , opts ) if opts . json : outf = None write = puts if opts . output is not None : outf = open ( opts . output , \"w\" ) write = outf . write try : delim = \"\" logs = b . get audit ( ) . read logs ( as json = False ) write ( '{\"events\": [' ) write ( os . linesep ) for l in logs : write ( delim + l ) delim = \",\" + os . linesep write ( os . linesep ) write ( ']}' ) finally : if opts . output is not None : outf . close ( ) else : puts ( colored . blue ( columns ( [ \"EVENT\" , 10 ] , [ \"TARGET\" , 16 ] , [ \"STATUS\" , 8 ] , [ \"TIME\" , 16 ] , [ \"MESSAGE\" , 25 ] ) ) ) logs = b . get audit ( ) . read logs ( as json = True ) for l in logs : puts ( columns ( [ l [ 'event' ] , 10 ] , [ str ( [ str ( t ) for t in l [ 'targets' ] ] ) , 16 ] , [ l [ 'status' ] , 8 ] , [ str ( l [ 'timestamp' ] ) , 16 ] , [ l [ 'message' ] , 25 ] ) )", "predictions": ["retrieve meta data from all available meta data configparser"], "references": ["get the event log for a given blockade"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 959, "code": "def try match ( request origin , maybe regex ) : if isinstance ( maybe regex , Regex Object ) : return re . match ( maybe regex , request origin ) elif probably regex ( maybe regex ) : return re . match ( maybe regex , request origin , flags = re . IGNORECASE ) else : try : return request origin . lower ( ) == maybe regex . lower ( ) except Attribute Error : return request origin == maybe regex", "predictions": ["add a request to a request or a request instance"], "references": ["safely attempts to match a pattern or string to a request origin ."], "bleu": 0.19235610974773318, "rouge_l": 0.42479108635097496}
{"id": 960, "code": "def get app kwarg dict ( app Instance = None ) : app = ( app Instance or current app ) app config = getattr ( app , 'config' , { } ) return { k . lower ( ) . replace ( 'cors ' , '' ) : app config . get ( k ) for k in CONFIG OPTIONS if app config . get ( k ) is not None }", "predictions": ["return a dict of app names for app app ."], "references": ["returns the dictionary of cors specific app configurations ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 961, "code": "def ensure iterable ( inst ) : if isinstance ( inst , string types ) : return [ inst ] elif not isinstance ( inst , collections . Iterable ) : return [ inst ] else : return inst", "predictions": ["ensure an iterable is a list of strings"], "references": ["wraps scalars or string types as a list or returns the iterable instance ."], "bleu": 0.10712878727413526, "rouge_l": 0.1732954545454545}
{"id": 962, "code": "def serialize options ( opts ) : options = ( opts or { } ) . copy ( ) for key in opts . keys ( ) : if key not in DEFAULT OPTIONS : LOG . warning ( \"Unknown option passed to Flask-CORS: %s\" , key ) options [ 'origins' ] = sanitize regex param ( options . get ( 'origins' ) ) options [ 'allow headers' ] = sanitize regex param ( options . get ( 'allow headers' ) ) if r'.*' in options [ 'origins' ] and options [ 'supports credentials' ] and options [ 'send wildcard' ] : raise Value Error ( \"Cannot use supports credentials in conjunction with\" \"an origin string of '*'. See: \" \"http://www.w3.org/TR/cors/#resource-requests\" ) serialize option ( options , 'expose headers' ) serialize option ( options , 'methods' , upper = True ) if isinstance ( options . get ( 'max age' ) , timedelta ) : options [ 'max age' ] = str ( int ( options [ 'max age' ] . total seconds ( ) ) ) return options", "predictions": ["serialize the options to a dictionary ."], "references": ["a helper method to serialize and processes the options dictionary ."], "bleu": 0.1952347922420459, "rouge_l": 0.5341506129597198}
{"id": 963, "code": "def get Json ( url , token = '' , version = '' ) : if token : return get Json IEX Cloud ( url , token , version ) return get Json Orig ( url )", "predictions": ["get metadata from provided url and version"], "references": ["for backwards compat accepting token and version but ignoring"], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 964, "code": "def bulk Minute Bars ( symbol , dates , token = '' , version = '' ) : raise If Not Str ( symbol ) dates = [ str Or Date ( date ) for date in dates ] list orig = dates . class args = [ ] for date in dates : args . append ( ( symbol , '1d' , date , token , version ) ) pool = Thread Pool ( 20 ) rets = pool . starmap ( chart , args ) pool . close ( ) return list orig ( itertools . chain ( * rets ) )", "predictions": ["bulk bulk minute with rets"], "references": ["fetch many dates worth of minute - bars for a given symbol"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 965, "code": "def bulk Minute Bars DF ( symbol , dates , token = '' , version = '' ) : data = bulk Minute Bars ( symbol , dates , token , version ) df = pd . Data Frame ( data ) if df . empty : return df to Datetime ( df ) df . set index ( [ 'date' , 'minute' ] , inplace = True ) return df", "predictions": ["bulk bulk operation of a pandas dataframe"], "references": ["fetch many dates worth of minute - bars for a given symbol"], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 966, "code": "def start ( self ) : if self . extra args : sys . exit ( '{} takes no extra arguments' . format ( self . name ) ) else : if self . toggle value : nbextensions . install nbextension python ( pkg name , overwrite = True , symlink = False , user = self . user , sys prefix = self . sys prefix , prefix = None , nbextensions dir = None , logger = None ) else : nbextensions . uninstall nbextension python ( pkg name , user = self . user , sys prefix = self . sys prefix , prefix = None , nbextensions dir = None , logger = None ) self . toggle nbextension python ( pkg name ) self . toggle server extension python ( pkg name )", "predictions": ["start the server ."], "references": ["perform the app s actions as configured ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 967, "code": "def start ( self ) : super ( Jupyter Tensorboard App , self ) . start ( ) subcmds = \", \" . join ( sorted ( self . subcommands ) ) sys . exit ( \"Please supply at least one subcommand: %s\" % subcmds )", "predictions": ["start the server ."], "references": ["perform the app s actions as configured"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 968, "code": "def credentials from request ( request ) : if ( oauth2 settings . storage model is None or request . user . is authenticated ( ) ) : return get storage ( request ) . get ( ) else : return None", "predictions": ["returns the user s credentials from the request ."], "references": ["gets the authorized credentials for this flow if they exist ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 969, "code": "def locked delete ( self ) : filters = { self . key name : self . key value } self . session . query ( self . model class ) . filter by ( * * filters ) . delete ( )", "predictions": ["perform models delete on the database ."], "references": ["delete credentials from the sqlalchemy datastore ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 970, "code": "def generate assertion ( self ) : now = int ( time . time ( ) ) payload = { 'aud' : self . token uri , 'scope' : self . scopes , 'iat' : now , 'exp' : now + self . MAX TOKEN LIFETIME SECS , 'iss' : self . service account email , } payload . update ( self . kwargs ) return crypt . make signed jwt ( self . signer , payload , key id = self . private key id )", "predictions": ["generate a assertion for the current token ."], "references": ["generate the assertion that will be used in the request ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 971, "code": "def get well known file ( ) : default config dir = os . getenv ( CLOUDSDK CONFIG ENV VAR ) if default config dir is None : if os . name == 'nt' : try : default config dir = os . path . join ( os . environ [ 'APPDATA' ] , CLOUDSDK CONFIG DIRECTORY ) except Key Error : drive = os . environ . get ( 'System Drive' , 'C:' ) default config dir = os . path . join ( drive , '\\\\' , CLOUDSDK CONFIG DIRECTORY ) else : default config dir = os . path . join ( os . path . expanduser ( '~' ) , '.config' , CLOUDSDK CONFIG DIRECTORY ) return os . path . join ( default config dir , WELL KNOWN CREDENTIALS FILE )", "predictions": ["return the path to the well - known well - known well - known well - known well - known well - known well - known to - known well -"], "references": ["get the well known file produced by command gcloud auth login ."], "bleu": 0.055177848898164926, "rouge_l": 0.1516155758077879}
{"id": 972, "code": "def get application default credential from file ( filename ) : with open ( filename ) as file obj : client credentials = json . load ( file obj ) credentials type = client credentials . get ( 'type' ) if credentials type == AUTHORIZED USER : required fields = set ( [ 'client id' , 'client secret' , 'refresh token' ] ) elif credentials type == SERVICE ACCOUNT : required fields = set ( [ 'client id' , 'client email' , 'private key id' , 'private key' ] ) else : raise Application Default Credentials Error ( \"'type' field should be defined (and have one of the '\" + AUTHORIZED USER + \"' or '\" + SERVICE ACCOUNT + \"' values)\" ) missing fields = required fields . difference ( client credentials . keys ( ) ) if missing fields : raise exception for missing fields ( missing fields ) if client credentials [ 'type' ] == AUTHORIZED USER : return Google Credentials ( access token = None , client id = client credentials [ 'client id' ] , client secret = client credentials [ 'client secret' ] , refresh token = client credentials [ 'refresh token' ] , token expiry = None , token uri = oauth2client . GOOGLE TOKEN URI , user agent = 'Python client library' ) else : from oauth2client import service account return service account . JWT Access Credentials . from json keyfile dict ( client credentials )", "predictions": ["get the default credential from the file ."], "references": ["build the application default credentials from file ."], "bleu": 0.25098621243978964, "rouge_l": 0.625}
{"id": 973, "code": "def oauth2 web server flow params ( kwargs ) : params = { 'access type' : 'offline' , 'response type' : 'code' , } params . update ( kwargs ) approval prompt = params . get ( 'approval prompt' ) if approval prompt is not None : logger . warning ( 'The approval prompt parameter for O Auth2Web Server Flow is ' 'deprecated. Please use the prompt parameter instead.' ) if approval prompt == 'force' : logger . warning ( 'approval prompt=\"force\" has been adjusted to ' 'prompt=\"consent\"' ) params [ 'prompt' ] = 'consent' del params [ 'approval prompt' ] return params", "predictions": ["approval server params to approval server params ."], "references": ["configures redirect uri parameters for oauth2webserverflow ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 974, "code": "def generate refresh request body ( self ) : body = urllib . parse . urlencode ( { 'grant type' : 'refresh token' , 'client id' : self . client id , 'client secret' : self . client secret , 'refresh token' : self . refresh token , } ) return body", "predictions": ["generates a refresh request body for a refresh request ."], "references": ["generate the body that will be used in the refresh request ."], "bleu": 0.20311412297014433, "rouge_l": 0.3577712609970674}
{"id": 975, "code": "def load client secrets ( self , filename ) : client type , client info = clientsecrets . loadfile ( filename ) if client type != clientsecrets . TYPE WEB : raise Value Error ( 'The flow specified in {0} is not supported.' . format ( client type ) ) self . client id = client info [ 'client id' ] self . client secret = client info [ 'client secret' ]", "predictions": ["load client secrets from file ."], "references": ["loads client secrets from the given filename ."], "bleu": 0.31149111610852515, "rouge_l": 0.5570776255707762}
{"id": 976, "code": "def make flow ( self , return url = None , * * kwargs ) : csrf token = hashlib . sha256 ( os . urandom ( 1024 ) ) . hexdigest ( ) session [ CSRF KEY ] = csrf token state = json . dumps ( { 'csrf token' : csrf token , 'return url' : return url } ) kw = self . flow kwargs . copy ( ) kw . update ( kwargs ) extra scopes = kw . pop ( 'scopes' , [ ] ) scopes = set ( self . scopes ) . union ( set ( extra scopes ) ) flow = client . O Auth2Web Server Flow ( client id = self . client id , client secret = self . client secret , scope = scopes , state = state , redirect uri = url for ( 'oauth2.callback' , external = True ) , * * kw ) flow key = FLOW KEY . format ( csrf token ) session [ flow key ] = pickle . dumps ( flow ) return flow", "predictions": ["returns a flow flow flow"], "references": ["creates a web server flow"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 977, "code": "def credentials ( self ) : ctx = app ctx stack . top if not hasattr ( ctx , CREDENTIALS KEY ) : ctx . google oauth2 credentials = self . storage . get ( ) return ctx . google oauth2 credentials", "predictions": ["get google oauth2 credentials ."], "references": ["the credentials for the current user or none if unavailable ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 978, "code": "def has credentials ( self ) : if not self . credentials : return False elif ( self . credentials . access token expired and not self . credentials . refresh token ) : return False else : return True", "predictions": ["return true if the token has been expired ."], "references": ["returns true if there are valid credentials for the current user ."], "bleu": 0.15122637383061946, "rouge_l": 0.3713850837138508}
{"id": 979, "code": "def locked delete ( self ) : if self . cache : self . cache . delete ( self . key name ) self . delete entity ( )", "predictions": ["delete this key from the cache ."], "references": ["delete credential from datastore ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 980, "code": "def Send Recv ( ) : port = int ( os . getenv ( DEVSHELL ENV , 0 ) ) if port == 0 : raise No Devshell Server ( ) sock = socket . socket ( ) sock . connect ( ( 'localhost' , port ) ) data = CREDENTIAL INFO REQUEST JSON msg = '{0}\\n{1}' . format ( len ( data ) , data ) sock . sendall ( helpers . to bytes ( msg , encoding = 'utf-8' ) ) header = sock . recv ( 6 ) . decode ( ) if '\\n' not in header : raise Communication Error ( 'saw no newline in the first 6 bytes' ) len str , json str = header . split ( '\\n' , 1 ) to read = int ( len str ) - len ( json str ) if to read > 0 : json str += sock . recv ( to read , socket . MSG WAITALL ) . decode ( ) return Credential Info Response ( json str )", "predictions": ["read a socket from server and receive it to the server ."], "references": ["communicate with the developer shell server socket ."], "bleu": 0.13065113298388567, "rouge_l": 0.3112244897959184}
{"id": 981, "code": "def locked delete ( self ) : query = { self . key name : self . key value } self . model class . objects . filter ( * * query ) . delete ( )", "predictions": ["deletes all entries in the database ."], "references": ["delete credentials from the datastore ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 982, "code": "def all ( self , list id , * * queryparams ) : return self . mc client . get ( url = self . build path ( list id , 'segments' ) , * * queryparams )", "predictions": ["gets all 'segments' for a given list ."], "references": ["returns the first 10 segments for a specific list ."], "bleu": 0.20668251975744228, "rouge_l": 0.43571428571428567}
{"id": 983, "code": "def get ( self , list id , segment id ) : return self . mc client . get ( url = self . build path ( list id , 'segments' , segment id ) )", "predictions": ["gets a specific segment ."], "references": ["returns the specified list segment ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 984, "code": "def update ( self , list id , segment id , data ) : return self . mc client . patch ( url = self . build path ( list id , 'segments' , segment id ) , data = data )", "predictions": ["update a segment ."], "references": ["updates an existing list segment ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 985, "code": "def delete ( self , list id , segment id ) : return self . mc client . delete ( url = self . build path ( list id , 'segments' , segment id ) )", "predictions": ["deletes a segment ."], "references": ["removes an existing list segment from the list . this cannot be undone ."], "bleu": 0.032639898338235177, "rouge_l": 0.20198675496688742}
{"id": 986, "code": "def create ( self , list id , data ) : return self . mc client . post ( url = self . build path ( list id , 'segments' ) , data = data )", "predictions": ["create a list ."], "references": ["adds a new segment to the list ."], "bleu": 0.18693159143202892, "rouge_l": 0.47164948453608246}
{"id": 987, "code": "def get metadata ( self ) : try : r = requests . get ( 'https://login.mailchimp.com/oauth2/metadata' , auth = self ) except requests . exceptions . Request Exception as e : raise e else : r . raise for status ( ) output = r . json ( ) if 'error' in output : raise requests . exceptions . Request Exception ( output [ 'error' ] ) return output", "predictions": ["get all metadata from client"], "references": ["get the metadata returned after authentication"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 988, "code": "def get access details ( self , key = None ) : if key in self . CACHE STATS : return self . CACHE STATS [ 'access stats' ] [ key ] else : return self . CACHE STATS [ 'access stats' ]", "predictions": ["get access details for a specific key ."], "references": ["get access details in cache ."], "bleu": 0.3155984539112945, "rouge_l": 0.5865384615384615}
{"id": 989, "code": "def get stats ( self ) : expired = sum ( [ x [ 'expired' ] for , x in self . CACHE STATS [ 'access stats' ] . items ( ) ] ) miss = sum ( [ x [ 'miss' ] for , x in self . CACHE STATS [ 'access stats' ] . items ( ) ] ) hit = sum ( [ x [ 'hit' ] for , x in self . CACHE STATS [ 'access stats' ] . items ( ) ] ) return { 'totals' : { 'keys' : len ( self . CACHE STATS [ 'access stats' ] ) , 'expired' : expired , 'miss' : miss , 'hit' : hit , } }", "predictions": ["return a dict of stats and their data"], "references": ["get general stats for the cache ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 990, "code": "def get vpc flow logs ( vpc , * * conn ) : fl result = describe flow logs ( Filters = [ { \"Name\" : \"resource-id\" , \"Values\" : [ vpc [ \"id\" ] ] } ] , * * conn ) fl ids = [ ] for fl in fl result : fl ids . append ( fl [ \"Flow Log Id\" ] ) return fl ids", "predictions": ["get vpc flow logs"], "references": ["gets the vpc flow logs for a vpc"], "bleu": 0.24601580968354606, "rouge_l": 0.47164948453608246}
{"id": 991, "code": "def get classic link ( vpc , * * conn ) : result = { } try : cl result = describe vpc classic link ( Vpc Ids = [ vpc [ \"id\" ] ] , * * conn ) [ 0 ] result [ \"Enabled\" ] = cl result [ \"Classic Link Enabled\" ] dns result = describe vpc classic link dns support ( Vpc Ids = [ vpc [ \"id\" ] ] , * * conn ) [ 0 ] result [ \"Dns Enabled\" ] = dns result [ \"Classic Link Dns Supported\" ] except Client Error as e : if 'Unsupported Operation' not in str ( e ) : raise e return result", "predictions": ["get the classic link for a vpc link"], "references": ["gets the classic link details about a vpc"], "bleu": 0.3549481056010052, "rouge_l": 0.625}
{"id": 992, "code": "def get internet gateway ( vpc , * * conn ) : result = { } ig result = describe internet gateways ( Filters = [ { \"Name\" : \"attachment.vpc-id\" , \"Values\" : [ vpc [ \"id\" ] ] } ] , * * conn ) if ig result : result . update ( { \"State\" : ig result [ 0 ] [ \"Attachments\" ] [ 0 ] [ \"State\" ] , \"Id\" : ig result [ 0 ] [ \"Internet Gateway Id\" ] , \"Tags\" : ig result [ 0 ] . get ( \"Tags\" , [ ] ) } ) return result", "predictions": ["get app { kwarg }"], "references": ["gets the internet gateway details about a vpc"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 993, "code": "def get vpc peering connections ( vpc , * * conn ) : accepter result = describe vpc peering connections ( Filters = [ { \"Name\" : \"accepter-vpc-info.vpc-id\" , \"Values\" : [ vpc [ \"id\" ] ] } ] , * * conn ) requester result = describe vpc peering connections ( Filters = [ { \"Name\" : \"requester-vpc-info.vpc-id\" , \"Values\" : [ vpc [ \"id\" ] ] } ] , * * conn ) peer ids = [ ] for peering in accepter result + requester result : peer ids . append ( peering [ \"Vpc Peering Connection Id\" ] ) return peer ids", "predictions": ["ensure iterable of iterable are supported by iterable"], "references": ["gets the internet gateway details about a vpc"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 994, "code": "def get subnets ( vpc , * * conn ) : subnets = describe subnets ( Filters = [ { \"Name\" : \"vpc-id\" , \"Values\" : [ vpc [ \"id\" ] ] } ] , * * conn ) s ids = [ ] for s in subnets : s ids . append ( s [ \"Subnet Id\" ] ) return s ids", "predictions": ["given a list of opts return a list of options"], "references": ["gets the vpc subnets"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 995, "code": "def get route tables ( vpc , * * conn ) : route tables = describe route tables ( Filters = [ { \"Name\" : \"vpc-id\" , \"Values\" : [ vpc [ \"id\" ] ] } ] , * * conn ) rt ids = [ ] for r in route tables : rt ids . append ( r [ \"Route Table Id\" ] ) return rt ids", "predictions": ["get route tables tables tables"], "references": ["gets the vpc route tables"], "bleu": 0.35930411196308426, "rouge_l": 0.4}
{"id": 996, "code": "def get network acls ( vpc , * * conn ) : route tables = describe network acls ( Filters = [ { \"Name\" : \"vpc-id\" , \"Values\" : [ vpc [ \"id\" ] ] } ] , * * conn ) nacl ids = [ ] for r in route tables : nacl ids . append ( r [ \"Network Acl Id\" ] ) return nacl ids", "predictions": ["return acls list of in symbol"], "references": ["gets the vpc network acls"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 997, "code": "def get gcp client ( * * kwargs ) : return gcp client ( project = kwargs [ 'project' ] , mod name = kwargs [ 'mod name' ] , pkg name = kwargs . get ( 'pkg name' , 'google.cloud' ) , key file = kwargs . get ( 'key file' , None ) , http auth = kwargs . get ( 'http' , None ) , user agent = kwargs . get ( 'user agent' , None ) )", "predictions": ["build an gcp client client connection to gcp if provided ."], "references": ["public gcp client builder ."], "bleu": 0.16108992769687397, "rouge_l": 0.40219780219780216}
{"id": 998, "code": "def get creds from kwargs ( kwargs ) : creds = { 'key file' : kwargs . pop ( 'key file' , None ) , 'http auth' : kwargs . pop ( 'http auth' , None ) , 'project' : kwargs . get ( 'project' , None ) , 'user agent' : kwargs . pop ( 'user agent' , None ) , 'api version' : kwargs . pop ( 'api version' , 'v1' ) } return ( creds , kwargs )", "predictions": ["retrieve creds instance from kwargs kwargs . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["helper to get creds out of kwargs ."], "bleu": 0.055177848898164926, "rouge_l": 0.17215428033866415}
{"id": 999, "code": "def gce list aggregated ( service = None , key name = 'name' , * * kwargs ) : resp list = [ ] req = service . aggregated List ( * * kwargs ) while req is not None : resp = req . execute ( ) for location , item in resp [ 'items' ] . items ( ) : if key name in item : resp list . extend ( item [ key name ] ) req = service . aggregated List next ( previous request = req , previous response = resp ) return resp list", "predictions": ["list - list list"], "references": ["general aggregated list function for the gce service ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1000, "code": "def gce list ( service = None , * * kwargs ) : resp list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) for item in resp . get ( 'items' , [ ] ) : resp list . append ( item ) req = service . list next ( previous request = req , previous response = resp ) return resp list", "predictions": ["helper function to from the from the from the database user user user user user user user ."], "references": ["general list function for the gce service ."], "bleu": 0.08097785064266204, "rouge_l": 0.2479674796747967}
{"id": 1001, "code": "def service list ( service = None , key name = None , * * kwargs ) : resp list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) if key name and key name in resp : resp list . extend ( resp [ key name ] ) else : resp list . append ( resp ) if hasattr ( service , 'list next' ) : req = service . list next ( previous request = req , previous response = resp ) else : req = None return resp list", "predictions": ["get one locked locked locked by id model model model model model model model model model model model model model model model model model model model model model model model model"], "references": ["general list function for google apis ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1002, "code": "def get cache access details ( key = None ) : from cloudaux . gcp . decorators import GCP CACHE return GCP CACHE . get access details ( key = key )", "predictions": ["returns the access access self { gcp } { gcp } { gcp } { gcp } { gcp } instance } { gcp } { gcp } { gcp }"], "references": ["retrieve detailed cache information ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1003, "code": "def get role managed policy documents ( role , client = None , * * kwargs ) : policies = get role managed policies ( role , force client = client ) policy names = ( policy [ 'name' ] for policy in policies ) delayed gmpd calls = ( delayed ( get managed policy document ) ( policy [ 'arn' ] , force client = client ) for policy in policies ) policy documents = Parallel ( n jobs = 20 , backend = \"threading\" ) ( delayed gmpd calls ) return dict ( zip ( policy names , policy documents ) )", "predictions": ["get known known known file documents for a well well"], "references": ["retrieve the currently active policy version document for every managed policy that is attached to the role ."], "bleu": 0.05664226584551703, "rouge_l": 0.06792873051224944}
{"id": 1004, "code": "def get group policy document ( group name , policy name , client = None , * * kwargs ) : return client . get group policy ( Group Name = group name , Policy Name = policy name , * * kwargs ) [ 'Policy Document' ]", "predictions": ["wrapper for the named application default application"], "references": ["fetches the specific iam group inline - policy document ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 1005, "code": "def get base ( server certificate , * * conn ) : server certificate [ ' version' ] = 1 cert details = get server certificate api ( server certificate [ 'Server Certificate Name' ] , * * conn ) if cert details : server certificate . update ( cert details [ 'Server Certificate Metadata' ] ) server certificate [ 'Certificate Body' ] = cert details [ 'Certificate Body' ] server certificate [ 'Certificate Chain' ] = cert details . get ( 'Certificate Chain' , None ) server certificate [ 'Upload Date' ] = get iso string ( server certificate [ 'Upload Date' ] ) server certificate [ 'Expiration' ] = get iso string ( server certificate [ 'Expiration' ] ) return server certificate", "predictions": ["generates a flow params"], "references": ["fetch the base iam server certificate ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1006, "code": "def get security group ( security group , flags = FLAGS . ALL , * * kwargs ) : result = registry . build out ( flags , start with = security group , pass datastructure = True , * * kwargs ) result . pop ( 'security group rules' , [ ] ) return result", "predictions": ["generate a refresh request"], "references": ["just store the aws formatted rules"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 1007, "code": "def get inline policies ( group , * * conn ) : policy list = list group policies ( group [ 'Group Name' ] ) policy documents = { } for policy in policy list : policy documents [ policy ] = get group policy document ( group [ 'Group Name' ] , policy , * * conn ) return policy documents", "predictions": ["helper function to load all client documents documents documents"], "references": ["get the inline policies for the group ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1008, "code": "def get managed policies ( group , * * conn ) : managed policies = list attached group managed policies ( group [ 'Group Name' ] , * * conn ) managed policy names = [ ] for policy in managed policies : managed policy names . append ( policy [ 'Policy Name' ] ) return managed policy names", "predictions": ["helper function to make lists of flow policies policies"], "references": ["get a list of the managed policy names that are attached to the group ."], "bleu": 0.08019421212222273, "rouge_l": 0.07973856209150328}
{"id": 1009, "code": "def get users ( group , * * conn ) : group details = get group api ( group [ 'Group Name' ] , * * conn ) user list = [ ] for user in group details . get ( 'Users' , [ ] ) : user list . append ( user [ 'User Name' ] ) return user list", "predictions": ["credentials are assigned to a group"], "references": ["gets a list of the usernames that are a part of this group ."], "bleu": 0.06924459302580939, "rouge_l": 0.2798165137614679}
{"id": 1010, "code": "def get base ( group , * * conn ) : group [ ' version' ] = 1 group . update ( get group api ( group [ 'Group Name' ] , users = False , * * conn ) [ 'Group' ] ) group [ 'Create Date' ] = get iso string ( group [ 'Create Date' ] ) return group", "predictions": ["return that a self and return a ldap self"], "references": ["fetch the base iam group ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1011, "code": "def get short version ( self ) : gs version = self . get version ( ) match = re . compile ( r'[^\\d.]+' ) return match . sub ( '' , gs version ) . strip ( '.' )", "predictions": ["return version version version"], "references": ["obtain the shory geoserver version"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1012, "code": "def delete granule ( self , coverage , store , granule id , workspace = None ) : params = dict ( ) workspace name = workspace if isinstance ( store , basestring ) : store name = store else : store name = store . name workspace name = store . workspace . name if workspace name is None : raise Value Error ( \"Must specify workspace\" ) url = build url ( self . service url , [ \"workspaces\" , workspace name , \"coveragestores\" , store name , \"coverages\" , coverage , \"index/granules\" , granule id , \".json\" ] , params ) headers = { \"Content-type\" : \"application/json\" , \"Accept\" : \"application/json\" } resp = self . http request ( url , method = 'delete' , headers = headers ) if resp . status code != 200 : Failed Request Error ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status code , resp . text ) ) self . cache . clear ( ) return None", "predictions": ["delete a 1 ."], "references": ["deletes a granule of an existing imagemosaic"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1013, "code": "def list granules ( self , coverage , store , workspace = None , filter = None , limit = None , offset = None ) : params = dict ( ) if filter is not None : params [ 'filter' ] = filter if limit is not None : params [ 'limit' ] = limit if offset is not None : params [ 'offset' ] = offset workspace name = workspace if isinstance ( store , basestring ) : store name = store else : store name = store . name workspace name = store . workspace . name if workspace name is None : raise Value Error ( \"Must specify workspace\" ) url = build url ( self . service url , [ \"workspaces\" , workspace name , \"coveragestores\" , store name , \"coverages\" , coverage , \"index/granules.json\" ] , params ) headers = { \"Content-type\" : \"application/json\" , \"Accept\" : \"application/json\" } resp = self . http request ( url , headers = headers ) if resp . status code != 200 : Failed Request Error ( 'Failed to list granules in mosaic {} : {}, {}' . format ( store , resp . status code , resp . text ) ) self . cache . clear ( ) return resp . json ( )", "predictions": ["locked a mosaic to a mosaic"], "references": ["list granules of an imagemosaic"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1014, "code": "def mosaic coverages ( self , store ) : params = dict ( ) url = build url ( self . service url , [ \"workspaces\" , store . workspace . name , \"coveragestores\" , store . name , \"coverages.json\" ] , params ) headers = { \"Content-type\" : \"application/json\" , \"Accept\" : \"application/json\" } resp = self . http request ( url , headers = headers ) if resp . status code != 200 : Failed Request Error ( 'Failed to get mosaic coverages {} : {}, {}' . format ( store , resp . status code , resp . text ) ) self . cache . clear ( ) return resp . json ( )", "predictions": ["get a all all all all of the all all all all all all all services in a all all all all all of the all all all services services services"], "references": ["returns all coverages in a coverage store"], "bleu": 0.055177848898164926, "rouge_l": 0.17818889970788704}
{"id": 1015, "code": "def publish featuretype ( self , name , store , native crs , srs = None , jdbc virtual table = None , native name = None ) : if native crs is None : raise Value Error ( \"must specify native crs\" ) srs = srs or native crs feature type = Feature Type ( self , store . workspace , store , name ) feature type . dirty [ 'name' ] = name feature type . dirty [ 'srs' ] = srs feature type . dirty [ 'native CRS' ] = native crs feature type . enabled = True feature type . advertised = True feature type . title = name if native name is not None : feature type . native name = native name headers = { \"Content-type\" : \"application/xml\" , \"Accept\" : \"application/xml\" } resource url = store . resource url if jdbc virtual table is not None : feature type . metadata = ( { 'JDBC VIRTUAL TABLE' : jdbc virtual table } ) params = dict ( ) resource url = build url ( self . service url , [ \"workspaces\" , store . workspace . name , \"datastores\" , store . name , \"featuretypes.xml\" ] , params ) resp = self . http request ( resource url , method = 'post' , data = feature type . message ( ) , headers = headers ) if resp . status code not in ( 200 , 201 , 202 ) : Failed Request Error ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status code , resp . text ) ) self . cache . clear ( ) feature type . fetch ( ) return feature type", "predictions": ["get a feature . . ."], "references": ["publish a featuretype from data in an existing store"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1016, "code": "def md link ( node ) : mimetype = node . find ( \"type\" ) mdtype = node . find ( \"metadata Type\" ) content = node . find ( \"content\" ) if None in [ mimetype , mdtype , content ] : return None else : return ( mimetype . text , mdtype . text , content . text )", "predictions": ["link for update update"], "references": ["extract a metadata link tuple from an xml node"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1017, "code": "def md dimension info ( name , node ) : def get value ( child name ) : return getattr ( node . find ( child name ) , 'text' , None ) resolution = get value ( 'resolution' ) default Value = node . find ( \"default Value\" ) strategy = default Value . find ( \"strategy\" ) if default Value is not None else None strategy = strategy . text if strategy is not None else None return Dimension Info ( name , get value ( 'enabled' ) == 'true' , get value ( 'presentation' ) , int ( resolution ) if resolution else None , get value ( 'units' ) , get value ( 'unit Symbol' ) , strategy , get value ( 'attribute' ) , get value ( 'end Attribute' ) , get value ( 'reference Value' ) , get value ( 'nearest Match Enabled' ) )", "predictions": ["return a dict of information about the dimension"], "references": ["extract metadata dimension info from an xml node"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1018, "code": "def md dynamic default values info ( name , node ) : configurations = node . find ( \"configurations\" ) if configurations is not None : configurations = [ ] for n in node . findall ( \"configuration\" ) : dimension = n . find ( \"dimension\" ) dimension = dimension . text if dimension is not None else None policy = n . find ( \"policy\" ) policy = policy . text if policy is not None else None default Value Expression = n . find ( \"default Value Expression\" ) default Value Expression = default Value Expression . text if default Value Expression is not None else None configurations . append ( Dynamic Default Values Configuration ( dimension , policy , default Value Expression ) ) return Dynamic Default Values ( name , configurations )", "predictions": ["given a node parse a dynamic node node"], "references": ["extract metadata dynamic default values from an xml node"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1019, "code": "def md jdbc virtual table ( key , node ) : name = node . find ( \"name\" ) sql = node . find ( \"sql\" ) escape Sql = node . find ( \"escape Sql\" ) escape Sql = escape Sql . text if escape Sql is not None else None key Column = node . find ( \"key Column\" ) key Column = key Column . text if key Column is not None else None n g = node . find ( \"geometry\" ) geometry = JDBC Virtual Table Geometry ( n g . find ( \"name\" ) , n g . find ( \"type\" ) , n g . find ( \"srid\" ) ) parameters = [ ] for n p in node . findall ( \"parameter\" ) : p name = n p . find ( \"name\" ) p default Value = n p . find ( \"default Value\" ) p default Value = p default Value . text if p default Value is not None else None p regexp Validator = n p . find ( \"regexp Validator\" ) p regexp Validator = p regexp Validator . text if p regexp Validator is not None else None parameters . append ( JDBC Virtual Table Param ( p name , p default Value , p regexp Validator ) ) return JDBC Virtual Table ( name , sql , escape Sql , geometry , key Column , parameters )", "predictions": ["method to get a metadata virtual self . self . self . self . ."], "references": ["extract metadata jdbc virtual tables from an xml node"], "bleu": 0.09103526405546068, "rouge_l": 0.17453505007153075}
{"id": 1020, "code": "def md entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom dimension' ) : value = md dimension info ( key , node . find ( \"dimension Info\" ) ) elif key == 'Dynamic Default Values' : value = md dynamic default values info ( key , node . find ( \"Dynamic Default Values\" ) ) elif key == 'JDBC VIRTUAL TABLE' : value = md jdbc virtual table ( key , node . find ( \"virtual Table\" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )", "predictions": ["given a node node create a virtual access table"], "references": ["extract metadata entries from an xml node"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 1021, "code": "def resolution millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . multipier ( mult ) * 1000 )", "predictions": ["in a get - place stats for the user s get get get get get a get get get get the get the get get get get content of the get"], "references": ["if set get the value of resolution in milliseconds"], "bleu": 0.05834347180338517, "rouge_l": 0.1665150136487716}
{"id": 1022, "code": "def as DAV Error ( e ) : if isinstance ( e , DAV Error ) : return e elif isinstance ( e , Exception ) : return DAV Error ( HTTP INTERNAL ERROR , src exception = e ) else : return DAV Error ( HTTP INTERNAL ERROR , \"{}\" . format ( e ) )", "predictions": ["return an arbitrary representation of an network ."], "references": ["convert any non - daverror exception to http_internal_error ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1023, "code": "def get user info ( self ) : if self . value in ERROR DESCRIPTIONS : s = \"{}\" . format ( ERROR DESCRIPTIONS [ self . value ] ) else : s = \"{}\" . format ( self . value ) if self . context info : s += \": {}\" . format ( self . context info ) elif self . value in ERROR RESPONSES : s += \": {}\" . format ( ERROR RESPONSES [ self . value ] ) if self . src exception : s += \"\\n    Source exception: '{}'\" . format ( self . src exception ) if self . err condition : s += \"\\n    Error condition: '{}'\" . format ( self . err condition ) return s", "predictions": ["get the classic classic information"], "references": ["return readable string ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1024, "code": "def handle delete ( self ) : if \"/by tag/\" not in self . path : raise DAV Error ( HTTP FORBIDDEN ) cat Type , tag , rest = util . save split ( self . path . strip ( \"/\" ) , \"/\" , 2 ) assert cat Type == \"by tag\" assert tag in self . data [ \"tags\" ] self . data [ \"tags\" ] . remove ( tag ) return True", "predictions": ["delete a single delete tag ."], "references": ["change semantic of delete to remove resource tags ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1025, "code": "def handle copy ( self , dest path , depth infinity ) : if \"/by tag/\" not in dest path : raise DAV Error ( HTTP FORBIDDEN ) cat Type , tag , rest = util . save split ( dest path . strip ( \"/\" ) , \"/\" , 2 ) assert cat Type == \"by tag\" if tag not in self . data [ \"tags\" ] : self . data [ \"tags\" ] . append ( tag ) return True", "predictions": ["handle a copy of the document ."], "references": ["change semantic of copy to add resource tags ."], "bleu": 0.16599826150636804, "rouge_l": 0.24448897795591182}
{"id": 1026, "code": "def handle move ( self , dest path ) : if \"/by tag/\" not in self . path : raise DAV Error ( HTTP FORBIDDEN ) if \"/by tag/\" not in dest path : raise DAV Error ( HTTP FORBIDDEN ) cat Type , tag , rest = util . save split ( self . path . strip ( \"/\" ) , \"/\" , 2 ) assert cat Type == \"by tag\" assert tag in self . data [ \"tags\" ] self . data [ \"tags\" ] . remove ( tag ) cat Type , tag , rest = util . save split ( dest path . strip ( \"/\" ) , \"/\" , 2 ) assert cat Type == \"by tag\" if tag not in self . data [ \"tags\" ] : self . data [ \"tags\" ] . append ( tag ) return True", "predictions": ["handle a move move cursor to the selected path ."], "references": ["change semantic of move to change resource tags ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 1027, "code": "def add provider ( self , share , provider , readonly = False ) : share = \"/\" + share . strip ( \"/\" ) assert share not in self . provider map if compat . is basestring ( provider ) : provider = Filesystem Provider ( provider , readonly ) elif type ( provider ) in ( dict , ) : if \"provider\" in provider : prov class = dynamic import class ( provider [ \"provider\" ] ) provider = prov class ( * provider . get ( \"args\" , [ ] ) , * * provider . get ( \"kwargs\" , { } ) ) else : provider = Filesystem Provider ( provider [ \"root\" ] , bool ( provider . get ( \"readonly\" , False ) ) ) elif type ( provider ) in ( list , tuple ) : raise Value Error ( \"Provider {}: tuple/list syntax is no longer supported\" . format ( provider ) ) if not isinstance ( provider , DAV Provider ) : raise Value Error ( \"Invalid provider {}\" . format ( provider ) ) provider . set share path ( share ) if self . mount path : provider . set mount path ( self . mount path ) provider . set lock manager ( self . lock manager ) provider . set prop manager ( self . prop manager ) self . provider map [ share ] = provider self . sorted share list = [ s . lower ( ) for s in self . provider map . keys ( ) ] self . sorted share list = sorted ( self . sorted share list , key = len , reverse = True ) return provider", "predictions": ["add a provider to the provider"], "references": ["add a provider to the provider_map routing table ."], "bleu": 0.4907480275466057, "rouge_l": 0.6434599156118143}
{"id": 1028, "code": "def read ( self , size = None ) : while size is None or len ( self . buffer ) < size : try : self . buffer += next ( self . data stream ) except Stop Iteration : break sized chunk = self . buffer [ : size ] if size is None : self . buffer = \"\" else : self . buffer = self . buffer [ size : ] return sized chunk", "predictions": ["read size bytes from the buffer ."], "references": ["read bytes from an iterator ."], "bleu": 0.2777619034011791, "rouge_l": 0.6240409207161125}
{"id": 1029, "code": "def handle copy ( self , dest path , depth infinity ) : dest Type , dest Hg Path = util . pop path ( dest path ) dest Hg Path = dest Hg Path . strip ( \"/\" ) ui = self . provider . ui repo = self . provider . repo logger . info ( \"handle copy %s -> %s\" % ( self . local Hg Path , dest Hg Path ) ) if self . rev is None and dest Type == \"edit\" : commands . copy ( ui , repo , self . local Hg Path , dest Hg Path , force = True ) elif self . rev is None and dest Type == \"released\" : self . commit ( \"Wsgi DAV commit (COPY %s -> %s)\" % ( self . path , dest path ) ) else : raise DAV Error ( HTTP FORBIDDEN ) return True", "predictions": ["save a copy of the provider with a new path to the server ."], "references": ["handle a copy request natively ."], "bleu": 0.1250076305588977, "rouge_l": 0.323321554770318}
{"id": 1030, "code": "def get log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) log List = [ ] for logentry in res . split ( \"\\n\\n\" ) : log = { } log List . append ( log ) for line in logentry . split ( \"\\n\" ) : k , v = line . split ( \":\" , 1 ) assert k in ( \"changeset\" , \"tag\" , \"user\" , \"date\" , \"summary\" ) log [ k . strip ( ) ] = v . strip ( ) log [ \"parsed date\" ] = util . parse time string ( log [ \"date\" ] ) local id , unid = log [ \"changeset\" ] . split ( \":\" ) log [ \"local id\" ] = int ( local id ) log [ \"unid\" ] = unid return log List", "predictions": ["return a log entry for the user ."], "references": ["read log entries into a list of dictionaries ."], "bleu": 0.16829946711936866, "rouge_l": 0.232824427480916}
{"id": 1031, "code": "def remove all properties ( self , recursive ) : if self . provider . prop manager : self . provider . prop manager . remove properties ( self . get ref url ( ) , self . environ )", "predictions": ["removes all properties from the provider"], "references": ["remove all associated dead properties ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1032, "code": "def is locked ( self ) : if self . provider . lock manager is None : return False return self . provider . lock manager . is url locked ( self . get ref url ( ) )", "predictions": ["returns true if the provider is locked for this provider ."], "references": ["return true if uri is locked ."], "bleu": 0.1972940627795883, "rouge_l": 0.5787476280834916}
{"id": 1033, "code": "def string to xml ( text ) : try : return etree . XML ( text ) except Exception : logger . error ( \"Error parsing XML string. \" \"If lxml is not available, and unicode is involved, then \" \"installing lxml  may  solve this issue.\" ) logger . error ( \"XML source: {}\" . format ( text ) ) raise", "predictions": ["convert a string to xml ."], "references": ["convert xml string into etree . element ."], "bleu": 0.19902510067151713, "rouge_l": 0.4178082191780822}
{"id": 1034, "code": "def make sub element ( parent , tag , nsmap = None ) : if use lxml : return etree . Sub Element ( parent , tag , nsmap = nsmap ) return etree . Sub Element ( parent , tag )", "predictions": ["create an sub element element from a parent tag ."], "references": ["wrapper for etree . subelement that takes care of unsupported nsmap option ."], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 1035, "code": "def get checked path ( path , config , must exist = True , allow none = True ) : if path in ( None , \"\" ) : if allow none : return None raise Value Error ( \"Invalid path {!r}\" . format ( path ) ) config file = config . get ( \" config file\" ) if config file and not os . path . isabs ( path ) : path = os . path . normpath ( os . path . join ( os . path . dirname ( config file ) , path ) ) else : path = os . path . abspath ( path ) if must exist and not os . path . exists ( path ) : raise Value Error ( \"Invalid path {!r}\" . format ( path ) ) return path", "predictions": ["return the path to the config file for a given path ."], "references": ["convert path to absolute if not none ."], "bleu": 0.14694106251955755, "rouge_l": 0.3112244897959184}
{"id": 1036, "code": "def read config file ( config file , verbose ) : config file = os . path . abspath ( config file ) if not os . path . exists ( config file ) : raise Runtime Error ( \"Couldn't open configuration file '{}'.\" . format ( config file ) ) if config file . endswith ( \".json\" ) : with io . open ( config file , mode = \"r\" , encoding = \"utf-8\" ) as json file : minified = jsmin ( json file . read ( ) ) conf = json . loads ( minified ) elif config file . endswith ( \".yaml\" ) : with io . open ( config file , mode = \"r\" , encoding = \"utf-8\" ) as yaml file : conf = yaml . safe load ( yaml file ) else : try : import imp conf = { } configmodule = imp . load source ( \"configuration module\" , config file ) for k , v in vars ( configmodule ) . items ( ) : if k . startswith ( \" \" ) : continue elif isfunction ( v ) : continue conf [ k ] = v except Exception : exc type , exc value = sys . exc info ( ) [ : 2 ] exc info list = traceback . format exception only ( exc type , exc value ) exc text = \"\\n\" . join ( exc info list ) print ( \"Failed to read configuration file: \" + config file + \"\\n Due to \" + exc text , file = sys . stderr , ) raise conf [ \" config file\" ] = config file return conf", "predictions": ["read the configuration from the specified config file ."], "references": ["read configuration file options into a dictionary ."], "bleu": 0.17747405280050263, "rouge_l": 0.4756335282651072}
{"id": 1037, "code": "def run cherrypy ( app , config , mode ) : assert mode == \"cherrypy-wsgiserver\" try : from cherrypy import wsgiserver from cherrypy . wsgiserver . ssl builtin import Builtin SSL Adapter logger . warning ( \"WARNING: cherrypy.wsgiserver is deprecated.\" ) logger . warning ( \"         Starting with Cherry Py 9.0 the functionality from cherrypy.wsgiserver\" ) logger . warning ( \"         was moved to the cheroot project.\" ) logger . warning ( \"         Consider using --server=cheroot.\" ) except Import Error : logger . error ( \"*\" * 78 ) logger . error ( \"ERROR: Could not import cherrypy.wsgiserver.\" ) logger . error ( \"Try `pip install cherrypy` or specify another server using the --server option.\" ) logger . error ( \"Note that starting with Cherry Py 9.0, the server was moved to\" ) logger . error ( \"the cheroot project, so it is recommended to use `-server=cheroot`\" ) logger . error ( \"and run `pip install cheroot` instead.\" ) logger . error ( \"*\" * 78 ) raise server name = \"Wsgi DAV/{} {} Python/{}\" . format ( version , wsgiserver . Cherry Py WSGI Server . version , util . PYTHON VERSION ) wsgiserver . Cherry Py WSGI Server . version = server name ssl certificate = get checked path ( config . get ( \"ssl certificate\" ) , config ) ssl private key = get checked path ( config . get ( \"ssl private key\" ) , config ) ssl certificate chain = get checked path ( config . get ( \"ssl certificate chain\" ) , config ) protocol = \"http\" if ssl certificate : assert ssl private key wsgiserver . Cherry Py WSGI Server . ssl adapter = Builtin SSL Adapter ( ssl certificate , ssl private key , ssl certificate chain ) protocol = \"https\" logger . info ( \"SSL / HTTPS enabled.\" ) logger . info ( \"Running {}\" . format ( server name ) ) logger . info ( \"Serving on {}://{}:{} ...\" . format ( protocol , config [ \"host\" ] , config [ \"port\" ] ) ) server args = { \"bind addr\" : ( config [ \"host\" ] , config [ \"port\" ] ) , \"wsgi app\" : app , \"server name\" : server name , } server args . update ( config . get ( \"server args\" , { } ) ) server = wsgiserver . Cherry Py WSGI Server ( * * server args ) startup event = config . get ( \"startup event\" ) if startup event : def patched tick ( ) : server . tick = org tick org tick ( ) logger . info ( \"Cherry Py WSGI Server is ready\" ) startup event . set ( ) org tick = server . tick server . tick = patched tick try : server . start ( ) except Keyboard Interrupt : logger . warning ( \"Caught Ctrl-C, shutting down...\" ) finally : server . stop ( ) return", "predictions": ["run cherrypy with ssl server"], "references": ["run wsgidav using cherrypy . wsgiserver if cherrypy is installed ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 1038, "code": "def run cheroot ( app , config , mode ) : assert mode == \"cheroot\" try : from cheroot import server , wsgi except Import Error : logger . error ( \"*\" * 78 ) logger . error ( \"ERROR: Could not import Cheroot.\" ) logger . error ( \"Try `pip install cheroot` or specify another server using the --server option.\" ) logger . error ( \"*\" * 78 ) raise server name = \"Wsgi DAV/{} {} Python/{}\" . format ( version , wsgi . Server . version , util . PYTHON VERSION ) wsgi . Server . version = server name ssl certificate = get checked path ( config . get ( \"ssl certificate\" ) , config ) ssl private key = get checked path ( config . get ( \"ssl private key\" ) , config ) ssl certificate chain = get checked path ( config . get ( \"ssl certificate chain\" ) , config ) ssl adapter = config . get ( \"ssl adapter\" , \"builtin\" ) protocol = \"http\" if ssl certificate and ssl private key : ssl adapter = server . get ssl adapter class ( ssl adapter ) wsgi . Server . ssl adapter = ssl adapter ( ssl certificate , ssl private key , ssl certificate chain ) protocol = \"https\" logger . info ( \"SSL / HTTPS enabled. Adapter: {}\" . format ( ssl adapter ) ) elif ssl certificate or ssl private key : raise Runtime Error ( \"Option 'ssl certificate' and 'ssl private key' must be used together.\" ) logger . info ( \"Running {}\" . format ( server name ) ) logger . info ( \"Serving on {}://{}:{} ...\" . format ( protocol , config [ \"host\" ] , config [ \"port\" ] ) ) server args = { \"bind addr\" : ( config [ \"host\" ] , config [ \"port\" ] ) , \"wsgi app\" : app , \"server name\" : server name , } server args . update ( config . get ( \"server args\" , { } ) ) server = wsgi . Server ( * * server args ) startup event = config . get ( \"startup event\" ) if startup event : def patched tick ( ) : server . tick = org tick logger . info ( \"wsgi.Server is ready\" ) startup event . set ( ) org tick ( ) org tick = server . tick server . tick = patched tick try : server . start ( ) except Keyboard Interrupt : logger . warning ( \"Caught Ctrl-C, shutting down...\" ) finally : server . stop ( ) return", "predictions": ["run a wsgi server with ssl ."], "references": ["run wsgidav using cheroot . server if cheroot is installed ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 1039, "code": "def run flup ( app , config , mode ) : if mode == \"flup-fcgi\" : from flup . server . fcgi import WSGI Server , version as flupver elif mode == \"flup-fcgi-fork\" : from flup . server . fcgi fork import WSGI Server , version as flupver else : raise Value Error logger . info ( \"Running Wsgi DAV/{} {}/{}...\" . format ( version , WSGI Server . module , flupver ) ) server = WSGI Server ( app , bind Address = ( config [ \"host\" ] , config [ \"port\" ] ) , ) try : server . run ( ) except Keyboard Interrupt : logger . warning ( \"Caught Ctrl-C, shutting down...\" ) return", "predictions": ["run fork on a fork ."], "references": ["run wsgidav using flup . server . fcgi if flup is installed ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 1040, "code": "def run wsgiref ( app , config , mode ) : from wsgiref . simple server import make server , software version version = \"Wsgi DAV/{} {}\" . format ( version , software version ) logger . info ( \"Running {}...\" . format ( version ) ) logger . warning ( \"WARNING: This single threaded server (wsgiref) is not meant for production.\" ) httpd = make server ( config [ \"host\" ] , config [ \"port\" ] , app ) try : httpd . serve forever ( ) except Keyboard Interrupt : logger . warning ( \"Caught Ctrl-C, shutting down...\" ) return", "predictions": ["run wsgiref server ."], "references": ["run wsgidav using wsgiref . simple_server on python 2 . 5 + ."], "bleu": 0.045035719139523436, "rouge_l": 0.32218309859154926}
{"id": 1041, "code": "def run ext wsgiutils ( app , config , mode ) : from wsgidav . server import ext wsgiutils server logger . info ( \"Running Wsgi DAV {} on wsgidav.ext wsgiutils server...\" . format ( version ) ) logger . warning ( \"WARNING: This single threaded server (ext-wsgiutils) is not meant for production.\" ) try : ext wsgiutils server . serve ( config , app ) except Keyboard Interrupt : logger . warning ( \"Caught Ctrl-C, shutting down...\" ) return", "predictions": ["run the server ."], "references": ["run wsgidav using ext_wsgiutils_server from the wsgidav package ."], "bleu": 0.12241977696855179, "rouge_l": 0.43160377358490565}
{"id": 1042, "code": "def stream data chunked ( self , environ , block size ) : if \"Darwin\" in environ . get ( \"HTTP USER AGENT\" , \"\" ) and environ . get ( \"HTTP X EXPECTED ENTITY LENGTH\" ) : WORKAROUND CHUNK LENGTH = True buf = environ . get ( \"HTTP X EXPECTED ENTITY LENGTH\" , \"0\" ) length = int ( buf ) else : WORKAROUND CHUNK LENGTH = False buf = environ [ \"wsgi.input\" ] . readline ( ) environ [ \"wsgidav.some input read\" ] = 1 if buf == compat . b empty : length = 0 else : length = int ( buf , 16 ) while length > 0 : buf = environ [ \"wsgi.input\" ] . read ( block size ) yield buf if WORKAROUND CHUNK LENGTH : environ [ \"wsgidav.some input read\" ] = 1 if buf == compat . b empty : length = 0 else : length -= len ( buf ) else : environ [ \"wsgi.input\" ] . readline ( ) buf = environ [ \"wsgi.input\" ] . readline ( ) if buf == compat . b empty : length = 0 else : length = int ( buf , 16 ) environ [ \"wsgidav.all input read\" ] = 1", "predictions": ["generator for listing data chunked chunked chunked chunked data ."], "references": ["get the data from a chunked transfer ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 1043, "code": "def stream data ( self , environ , content length , block size ) : if content length == 0 : logger . info ( \"PUT: Content-Length == 0. Creating empty file...\" ) else : assert content length > 0 contentremain = content length while contentremain > 0 : n = min ( contentremain , block size ) readbuffer = environ [ \"wsgi.input\" ] . read ( n ) if not len ( readbuffer ) > 0 : logger . error ( \"input.read({}) returned 0 bytes\" . format ( n ) ) break environ [ \"wsgidav.some input read\" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ \"wsgidav.all input read\" ] = 1", "predictions": ["generator for reading data from the environ"], "references": ["get the data from a non - chunked transfer ."], "bleu": 0.17112717058426782, "rouge_l": 0.22803738317757008}
{"id": 1044, "code": "def find ( self , url ) : vr = self . db . view ( \"properties/by url\" , key = url , include docs = True ) logger . debug ( \"find(%r) returned %s\" % ( url , len ( vr ) ) ) assert len ( vr ) <= 1 , \"Found multiple matches for %r\" % url for row in vr : assert row . doc return row . doc return None", "predictions": ["find document in a url ."], "references": ["return properties document for path ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1045, "code": "def find descendents ( self , url ) : map fun = % ( url + \"/\" ) vr = self . db . query ( map fun , include docs = True ) for row in vr : yield row . doc return", "predictions": ["looks up a given url for a given url"], "references": ["return properties document for url and all children ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 1046, "code": "def get domain realm ( self , path info , environ ) : realm = self . calc realm from path provider ( path info , environ ) return realm", "predictions": ["get the domain from the path to the domain"], "references": ["resolve a relative url to the appropriate realm name ."], "bleu": 0.16621692209732, "rouge_l": 0.20854700854700853}
{"id": 1047, "code": "def digest auth user ( self , realm , user name , environ ) : user = self . get realm entry ( realm , user name ) if user is None : return False password = user . get ( \"password\" ) environ [ \"wsgidav.auth.roles\" ] = user . get ( \"roles\" , [ ] ) return self . compute http digest a1 ( realm , user name , password )", "predictions": ["return the http digest for the user ."], "references": ["computes digest hash a1 part ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1048, "code": "def flush ( self ) : logger . debug ( \" flush()\" ) self . lock . acquire write ( ) try : self . dict . sync ( ) finally : self . lock . release ( )", "predictions": ["flushes the stream ."], "references": ["write persistent dictionary to disc ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 1049, "code": "def clear ( self ) : self . lock . acquire write ( ) try : was closed = self . dict is None if was closed : self . open ( ) if len ( self . dict ) : self . dict . clear ( ) self . dict . sync ( ) if was closed : self . close ( ) finally : self . lock . release ( )", "predictions": ["clear the cache ."], "references": ["delete all entries ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 1050, "code": "def set last modified ( self , dest path , time stamp , dry run ) : secs = util . parse time string ( time stamp ) if not dry run : os . utime ( self . file path , ( secs , secs ) ) return True", "predictions": ["sets the last modified modified file if it was not already present ."], "references": ["set last modified time for destpath to timestamp on epoch - format"], "bleu": 0.12571192676522522, "rouge_l": 0.16116248348745044}
{"id": 1051, "code": "def lock string ( lock dict ) : if not lock dict : return \"Lock: None\" if lock dict [ \"expire\" ] < 0 : expire = \"Infinite ({})\" . format ( lock dict [ \"expire\" ] ) else : expire = \"{} (in {} seconds)\" . format ( util . get log time ( lock dict [ \"expire\" ] ) , lock dict [ \"expire\" ] - time . time ( ) ) return \"Lock(<{}..>, '{}', {}, {}, depth-{}, until {}\" . format ( lock dict . get ( \"token\" , \"?\" * 30 ) [ 18 : 22 ] , lock dict . get ( \"root\" ) , lock dict . get ( \"principal\" ) , lock dict . get ( \"scope\" ) , lock dict . get ( \"depth\" ) , expire , )", "predictions": ["locks a single lock ."], "references": ["return readable rep ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1052, "code": "def refresh ( self , token , timeout = None ) : if timeout is None : timeout = Lock Manager . LOCK TIME OUT DEFAULT return self . storage . refresh ( token , timeout )", "predictions": ["refresh a token ."], "references": ["set new timeout for lock if existing and valid ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 1053, "code": "def sync ( self ) : logger . debug ( \" sync()\" ) self . lock . acquire write ( ) try : if self . loaded : self . dict . sync ( ) finally : self . lock . release ( )", "predictions": ["flushes the lock ."], "references": ["write persistent dictionary to disc ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 1054, "code": "def dynamic import class ( name ) : import importlib module name , class name = name . rsplit ( \".\" , 1 ) try : module = importlib . import module ( module name ) except Exception as e : logger . exception ( \"Dynamic import of {!r} failed: {}\" . format ( name , e ) ) raise the class = getattr ( module , class name ) return the class", "predictions": ["import a class by name ."], "references": ["import a class from a module string e . g . my . module . classname ."], "bleu": 0.0695030626221084, "rouge_l": 0.3202099737532808}
{"id": 1055, "code": "def string repr ( s ) : if compat . is bytes ( s ) : res = \"{!r}: \" . format ( s ) for b in s : if type ( b ) is str : b = ord ( b ) res += \"%02x \" % b return res return \"{}\" . format ( s )", "predictions": ["return a string representation of a string ."], "references": ["return a string as hex dump ."], "bleu": 0.3155984539112945, "rouge_l": 0.5398230088495575}
{"id": 1056, "code": "def byte number string ( number , thousands Sep = True , partition = False , base1024 = True , append Bytes = True ) : magsuffix = \"\" bytesuffix = \"\" if partition : magnitude = 0 if base1024 : while number >= 1024 : magnitude += 1 number = number >> 10 else : while number >= 1000 : magnitude += 1 number /= 1000.0 magsuffix = [ \"\" , \"K\" , \"M\" , \"G\" , \"T\" , \"P\" ] [ magnitude ] if append Bytes : if number == 1 : bytesuffix = \" Byte\" else : bytesuffix = \" Bytes\" if thousands Sep and ( number >= 1000 or magsuffix ) : snum = \"{:,d}\" . format ( number ) else : snum = str ( number ) return \"{}{}{}\" . format ( snum , magsuffix , bytesuffix )", "predictions": ["format a handle delete string . . . ."], "references": ["convert bytes into human - readable representation ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 1057, "code": "def send status response ( environ , start response , e , add headers = None , is head = False ) : status = get http status string ( e ) headers = [ ] if add headers : headers . extend ( add headers ) if e in ( HTTP NOT MODIFIED , HTTP NO CONTENT ) : start response ( status , [ ( \"Content-Length\" , \"0\" ) , ( \"Date\" , get rfc1123 time ( ) ) ] + headers ) return [ b\"\" ] if e in ( HTTP OK , HTTP CREATED ) : e = DAV Error ( e ) assert isinstance ( e , DAV Error ) content type , body = e . get response page ( ) if is head : body = compat . b empty assert compat . is bytes ( body ) , body start response ( status , [ ( \"Content-Type\" , content type ) , ( \"Date\" , get rfc1123 time ( ) ) , ( \"Content-Length\" , str ( len ( body ) ) ) , ] + headers , ) return [ body ]", "predictions": ["handle requests that can be overwritten ."], "references": ["start a wsgi response for a daverror or status code ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 1058, "code": "def calc base64 ( s ) : s = compat . to bytes ( s ) s = compat . base64 encodebytes ( s ) . strip ( ) return compat . to native ( s )", "predictions": ["parse move string to move bytes"], "references": ["return base64 encoded binarystring ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1059, "code": "def read timeout value header ( timeoutvalue ) : timeoutsecs = 0 timeoutvaluelist = timeoutvalue . split ( \",\" ) for timeoutspec in timeoutvaluelist : timeoutspec = timeoutspec . strip ( ) if timeoutspec . lower ( ) == \"infinite\" : return - 1 else : list SR = re Seconds Reader . findall ( timeoutspec ) for secs in list SR : timeoutsecs = int ( secs ) if timeoutsecs > MAX FINITE TIMEOUT LIMIT : return - 1 if timeoutsecs != 0 : return timeoutsecs return None", "predictions": ["add a provider value to a provider self + 1 + 0 + 1 + 1 + 1 ="], "references": ["return - 1 if infinite else return numofsecs ."], "bleu": 0.06439931429457924, "rouge_l": 0.07634543178973717}
{"id": 1060, "code": "def generate index ( self ) : self . dict = { v . id : k for k , v in enumerate ( self ) }", "predictions": ["read the index index index"], "references": ["rebuild the _dict index"], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1061, "code": "def replace on id ( self , new object ) : the id = new object . id the index = self . dict [ the id ] list . setitem ( self , the index , new object )", "predictions": ["handle the copy copy copy copy to path . ."], "references": ["replace an object by another with the same id ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 1062, "code": "def append ( self , object ) : the id = object . id self . check ( the id ) self . dict [ the id ] = len ( self ) list . append ( self , object )", "predictions": ["get an object object"], "references": ["append object to end"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 1063, "code": "def union ( self , iterable ) : dict = self . dict append = self . append for i in iterable : if i . id not in dict : append ( i )", "predictions": [". remove any self from this self if not present"], "references": ["adds elements with id s not already in the model"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 1064, "code": "def extend ( self , iterable ) : if not hasattr ( self , \" dict\" ) or self . dict is None : self . dict = { } dict = self . dict current length = len ( self ) list . extend ( self , iterable ) for i , obj in enumerate ( islice ( self , current length , None ) , current length ) : the id = obj . id if the id not in dict : dict [ the id ] = i else : self = self [ : current length ] self . check ( the id ) raise Value Error ( \"id '%s' at index %d is non-unique. \" \"Is it present twice?\" % ( str ( the id ) , i ) )", "predictions": ["is this list a list"], "references": ["extend list by appending elements from the iterable"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1065, "code": "def insert ( self , index , object ) : self . check ( object . id ) list . insert ( self , index , object ) dict = self . dict for i , j in iteritems ( dict ) : if j >= index : dict [ i ] = j + 1 dict [ object . id ] = index", "predictions": ["string representation of object"], "references": ["insert object before index"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 1066, "code": "def check solver status ( status , raise error = False ) : if status == OPTIMAL : return elif ( status in has primals ) and not raise error : warn ( \"solver status is '{}'\" . format ( status ) , User Warning ) elif status is None : raise Optimization Error ( \"model was not optimized yet or solver context switched\" ) else : raise Optimization Error ( \"solver status is '{}'\" . format ( status ) )", "predictions": ["checks that the element is valid ."], "references": ["perform standard checks on a solver s status ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 1067, "code": "def step ( sampler , x , delta , fraction = None , tries = 0 ) : prob = sampler . problem valid = ( ( np . abs ( delta ) > sampler . feasibility tol ) & np . logical not ( prob . variable fixed ) ) valphas = ( ( 1.0 - sampler . bounds tol ) * prob . variable bounds - x ) [ : , valid ] valphas = ( valphas / delta [ valid ] ) . flatten ( ) if prob . bounds . shape [ 0 ] > 0 : ineqs = prob . inequalities . dot ( delta ) valid = np . abs ( ineqs ) > sampler . feasibility tol balphas = ( ( 1.0 - sampler . bounds tol ) * prob . bounds - prob . inequalities . dot ( x ) ) [ : , valid ] balphas = ( balphas / ineqs [ valid ] ) . flatten ( ) alphas = np . hstack ( [ valphas , balphas ] ) else : alphas = valphas pos alphas = alphas [ alphas > 0.0 ] neg alphas = alphas [ alphas <= 0.0 ] alpha range = np . array ( [ neg alphas . max ( ) if len ( neg alphas ) > 0 else 0 , pos alphas . min ( ) if len ( pos alphas ) > 0 else 0 ] ) if fraction : alpha = alpha range [ 0 ] + fraction * ( alpha range [ 1 ] - alpha range [ 0 ] ) else : alpha = np . random . uniform ( alpha range [ 0 ] , alpha range [ 1 ] ) p = x + alpha * delta if ( np . any ( sampler . bounds dist ( p ) < - sampler . bounds tol ) or np . abs ( np . abs ( alpha range ) . max ( ) * delta ) . max ( ) < sampler . bounds tol ) : if tries > MAX TRIES : raise Runtime Error ( \"Can not escape sampling region, model seems\" \" numerically unstable :( Reporting the \" \"model to \" \"will help us to fix this :)\" ) LOGGER . info ( \"found bounds infeasibility in sample, \" \"resetting to center\" ) newdir = sampler . warmup [ np . random . randint ( sampler . n warmup ) ] sampler . retries += 1 return step ( sampler , sampler . center , newdir - sampler . center , None , tries + 1 ) return p", "predictions": ["make a path with a path if it s not already running if not already in sampling if not already existing"], "references": ["sample a new feasible point from the point x in direction delta ."], "bleu": 0.06429451441231726, "rouge_l": 0.1228600201409869}
{"id": 1068, "code": "def build problem ( self ) : prob = constraint matrices ( self . model , zero tol = self . feasibility tol ) equalities = prob . equalities b = prob . b bounds = np . atleast 2d ( prob . bounds ) . T var bounds = np . atleast 2d ( prob . variable bounds ) . T homogeneous = all ( np . abs ( b ) < self . feasibility tol ) fixed non zero = np . abs ( prob . variable bounds [ : , 1 ] ) > self . feasibility tol fixed non zero &= prob . variable fixed if any ( fixed non zero ) : n fixed = fixed non zero . sum ( ) rows = np . zeros ( ( n fixed , prob . equalities . shape [ 1 ] ) ) rows [ range ( n fixed ) , np . where ( fixed non zero ) ] = 1.0 equalities = np . vstack ( [ equalities , rows ] ) var b = prob . variable bounds [ : , 1 ] b = np . hstack ( [ b , var b [ fixed non zero ] ] ) homogeneous = False nulls = nullspace ( equalities ) return Problem ( equalities = shared np array ( equalities . shape , equalities ) , b = shared np array ( b . shape , b ) , inequalities = shared np array ( prob . inequalities . shape , prob . inequalities ) , bounds = shared np array ( bounds . shape , bounds ) , variable fixed = shared np array ( prob . variable fixed . shape , prob . variable fixed , integer = True ) , variable bounds = shared np array ( var bounds . shape , var bounds ) , nullspace = shared np array ( nulls . shape , nulls ) , homogeneous = homogeneous )", "predictions": ["builds the config matrix"], "references": ["build the matrix representation of the sampling problem ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 1069, "code": "def random point ( self ) : idx = np . random . randint ( self . n warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )", "predictions": ["return cherrypy cherrypy cherrypy"], "references": ["find an approximately random point in the flux cone ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1070, "code": "def is redundant ( self , matrix , cutoff = None ) : cutoff = 1.0 - self . feasibility tol extra col = matrix [ : , 0 ] + 1 extra col [ matrix . sum ( axis = 1 ) == 0 ] = 2 corr = np . corrcoef ( np . c [ matrix , extra col ] ) corr = np . tril ( corr , - 1 ) return ( np . abs ( corr ) > cutoff ) . any ( axis = 1 )", "predictions": ["install the given config config with the given mode"], "references": ["identify rdeundant rows in a matrix that can be removed ."], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 1071, "code": "def bounds dist ( self , p ) : prob = self . problem lb dist = ( p - prob . variable bounds [ 0 , ] ) . min ( ) ub dist = ( prob . variable bounds [ 1 , ] - p ) . min ( ) if prob . bounds . shape [ 0 ] > 0 : const = prob . inequalities . dot ( p ) const lb dist = ( const - prob . bounds [ 0 , ] ) . min ( ) const ub dist = ( prob . bounds [ 1 , ] - const ) . min ( ) lb dist = min ( lb dist , const lb dist ) ub dist = min ( ub dist , const ub dist ) return np . array ( [ lb dist , ub dist ] )", "predictions": ["return is a run of the lb distribution"], "references": ["get the lower and upper bound distances . negative is bad ."], "bleu": 0.10764345432696364, "rouge_l": 0.09651898734177215}
{"id": 1072, "code": "def add switches and objective ( self ) : constraints = list ( ) big m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling type' ) : continue indicator = prob . Variable ( name = 'indicator {}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling type ] indicator . rxn id = rxn . id self . indicators . append ( indicator ) constraint lb = prob . Constraint ( rxn . flux expression - big m * indicator , ub = 0 , name = 'constraint lb {}' . format ( rxn . id ) , sloppy = True ) constraint ub = prob . Constraint ( rxn . flux expression + big m * indicator , lb = 0 , name = 'constraint ub {}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint lb , constraint ub ] ) self . model . add cons vars ( self . indicators ) self . model . add cons vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set linear coefficients ( { i : 1 for i in self . indicators } ) self . update costs ( )", "predictions": ["run the not - app to run the not app"], "references": ["update gapfilling model with switches and the indicator objective ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 1073, "code": "def normalize cutoff ( model , zero cutoff = None ) : if zero cutoff is None : return model . tolerance else : if zero cutoff < model . tolerance : raise Value Error ( \"The chosen zero cutoff cannot be less than the model's \" \"tolerance value.\" ) else : return zero cutoff", "predictions": ["run a ext ext to a model ext server server server server server server server server server server server server server server server server server server server server server server server"], "references": ["return a valid zero cutoff value ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 1074, "code": "def fix type ( value ) : if isinstance ( value , string types ) : return str ( value ) if isinstance ( value , float ) : return float ( value ) if isinstance ( value , bool ) : return bool ( value ) if isinstance ( value , set ) : return list ( value ) if isinstance ( value , dict ) : return Ordered Dict ( ( key , value [ key ] ) for key in sorted ( value ) ) if value . class . name == \"Formula\" : return str ( value ) if value is None : return \"\" return value", "predictions": ["convert strings to strings to strings ."], "references": ["convert possible types to str float and bool"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1075, "code": "def update optional ( cobra object , new dict , optional attribute dict , ordered keys ) : for key in ordered keys : default = optional attribute dict [ key ] value = getattr ( cobra object , key ) if value is None or value == default : continue new dict [ key ] = fix type ( value )", "predictions": ["stream with data from a length of data ."], "references": ["update new_dict with optional attributes from cobra_object"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 1076, "code": "def get id compartment ( id ) : bracket search = bracket re . findall ( id ) if len ( bracket search ) == 1 : return bracket search [ 0 ] [ 1 ] underscore search = underscore re . findall ( id ) if len ( underscore search ) == 1 : return underscore search [ 0 ] [ 1 ] return None", "predictions": ["docs for underscore db"], "references": ["extract the compartment from the id string"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1077, "code": "def cell ( x ) : x no none = [ i if i is not None else \"\" for i in x ] return array ( x no none , dtype = np object )", "predictions": ["docs a find function to create a find find the first element ."], "references": ["translate an array x into a matlab cell array"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 1078, "code": "def create mat dict ( model ) : rxns = model . reactions mets = model . metabolites mat = Ordered Dict ( ) mat [ \"mets\" ] = cell ( [ met id for met id in create mat metabolite id ( model ) ] ) mat [ \"met Names\" ] = cell ( mets . list attr ( \"name\" ) ) mat [ \"met Formulas\" ] = cell ( [ str ( m . formula ) for m in mets ] ) try : mat [ \"met Charge\" ] = array ( mets . list attr ( \"charge\" ) ) * 1. except Type Error : pass mat [ \"genes\" ] = cell ( model . genes . list attr ( \"id\" ) ) rxn gene = scipy sparse . dok matrix ( ( len ( model . reactions ) , len ( model . genes ) ) ) if min ( rxn gene . shape ) > 0 : for i , reaction in enumerate ( model . reactions ) : for gene in reaction . genes : rxn gene [ i , model . genes . index ( gene ) ] = 1 mat [ \"rxn Gene Mat\" ] = rxn gene mat [ \"gr Rules\" ] = cell ( rxns . list attr ( \"gene reaction rule\" ) ) mat [ \"rxns\" ] = cell ( rxns . list attr ( \"id\" ) ) mat [ \"rxn Names\" ] = cell ( rxns . list attr ( \"name\" ) ) mat [ \"sub Systems\" ] = cell ( rxns . list attr ( \"subsystem\" ) ) stoich mat = create stoichiometric matrix ( model ) mat [ \"S\" ] = stoich mat if stoich mat is not None else [ [ ] ] mat [ \"lb\" ] = array ( rxns . list attr ( \"lower bound\" ) ) * 1. mat [ \"ub\" ] = array ( rxns . list attr ( \"upper bound\" ) ) * 1. mat [ \"b\" ] = array ( mets . list attr ( \" bound\" ) ) * 1. mat [ \"c\" ] = array ( rxns . list attr ( \"objective coefficient\" ) ) * 1. mat [ \"rev\" ] = array ( rxns . list attr ( \"reversibility\" ) ) * 1 mat [ \"description\" ] = str ( model . id ) return mat", "predictions": ["get a realm realm realm"], "references": ["create a dict mapping model attributes to arrays"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1079, "code": "def get context ( obj ) : try : return obj . contexts [ - 1 ] except ( Attribute Error , Index Error ) : pass try : return obj . model . contexts [ - 1 ] except ( Attribute Error , Index Error ) : pass return None", "predictions": ["get the auth auth auth auth auth auth auth environ environ environ environ environ environ environ environ if it exists ."], "references": ["search for a context manager"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 1080, "code": "def get metabolite compartments ( self ) : warn ( 'use Model.compartments instead' , Deprecation Warning ) return { met . compartment for met in self . metabolites if met . compartment is not None }", "predictions": ["dict containing all metabolite compartment release compartment release release release release release release release release release release release release release release release release release release release release release release release release"], "references": ["return all metabolites compartments ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 1081, "code": "def escape str id ( id str ) : for c in ( \"'\" , '\"' ) : if id str . startswith ( c ) and id str . endswith ( c ) and id str . count ( c ) == 2 : id str = id str . strip ( c ) for char , escaped char in renames : id str = id str . replace ( char , escaped char ) return id str", "predictions": ["clear the self dict with self dict self dict dict dict"], "references": ["make a single string id sbml compliant"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1082, "code": "def escape ID ( cobra model ) : for x in chain ( [ cobra model ] , cobra model . metabolites , cobra model . reactions , cobra model . genes ) : x . id = escape str id ( x . id ) cobra model . repair ( ) gene renamer = Gene Escaper ( ) for rxn , rule in iteritems ( get compiled gene reaction rules ( cobra model ) ) : if rule is not None : rxn . gene reaction rule = ast2str ( gene renamer . visit ( rule ) )", "predictions": ["set compiled rules rules rules rules to os secs rules"], "references": ["makes all ids sbml compliant"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1083, "code": "def rename genes ( cobra model , rename dict ) : recompute reactions = set ( ) remove genes = [ ] for old name , new name in iteritems ( rename dict ) : try : gene index = cobra model . genes . index ( old name ) except Value Error : gene index = None old gene present = gene index is not None new gene present = new name in cobra model . genes if old gene present and new gene present : old gene = cobra model . genes . get by id ( old name ) if old gene is not cobra model . genes . get by id ( new name ) : remove genes . append ( old gene ) recompute reactions . update ( old gene . reaction ) elif old gene present and not new gene present : gene = cobra model . genes [ gene index ] cobra model . genes . dict . pop ( gene . id ) gene . id = new name cobra model . genes [ gene index ] = gene elif not old gene present and new gene present : pass else : pass cobra model . repair ( ) class Renamer ( Node Transformer ) : def visit Name ( self , node ) : node . id = rename dict . get ( node . id , node . id ) return node gene renamer = Renamer ( ) for rxn , rule in iteritems ( get compiled gene reaction rules ( cobra model ) ) : if rule is not None : rxn . gene reaction rule = ast2str ( gene renamer . visit ( rule ) ) for rxn in recompute reactions : rxn . gene reaction rule = rxn . gene reaction rule for i in remove genes : cobra model . genes . remove ( i )", "predictions": ["lock compiled to lock"], "references": ["renames genes in a model from the rename_dict"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 1084, "code": "def init worker ( model , loopless , sense ) : global model global loopless model = model model . solver . objective . direction = sense loopless = loopless", "predictions": ["initialize worker worker worker"], "references": ["initialize a global model object for multiprocessing ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1085, "code": "def find bump ( target , tag ) : tmp = tag . split ( \".\" ) existing = [ intify ( basename ( f ) ) for f in glob ( join ( target , \"[0-9]*.md\" ) ) ] latest = max ( existing ) if int ( tmp [ 0 ] ) > latest [ 0 ] : return \"major\" elif int ( tmp [ 1 ] ) > latest [ 1 ] : return \"minor\" else : return \"patch\"", "predictions": ["sync a bump to a bump"], "references": ["identify the kind of release by comparing to existing ones ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 1086, "code": "def reverse id ( self ) : return ' ' . join ( ( self . id , 'reverse' , hashlib . md5 ( self . id . encode ( 'utf-8' ) ) . hexdigest ( ) [ 0 : 5 ] ) )", "predictions": ["dynamic - friendly import"], "references": ["generate the id of reverse_variable from the reaction s id ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 1087, "code": "def build reaction string ( self , use metabolite names = False ) : def format ( number ) : return \"\" if number == 1 else str ( number ) . rstrip ( \".\" ) + \" \" id type = 'id' if use metabolite names : id type = 'name' reactant bits = [ ] product bits = [ ] for met in sorted ( self . metabolites , key = attrgetter ( \"id\" ) ) : coefficient = self . metabolites [ met ] name = str ( getattr ( met , id type ) ) if coefficient >= 0 : product bits . append ( format ( coefficient ) + name ) else : reactant bits . append ( format ( abs ( coefficient ) ) + name ) reaction string = ' + ' . join ( reactant bits ) if not self . reversibility : if self . lower bound < 0 and self . upper bound <= 0 : reaction string += ' <-- ' else : reaction string += ' --> ' else : reaction string += ' <=> ' reaction string += ' + ' . join ( product bits ) return reaction string", "predictions": ["builds a repr string string"], "references": ["generate a human readable reaction string"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 1088, "code": "def compartments ( self ) : if self . compartments is None : self . compartments = { met . compartment for met in self . metabolites if met . compartment is not None } return self . compartments", "predictions": ["compartments compartments compartment ."], "references": ["lists compartments the metabolites are in"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 1089, "code": "def clip ( sid , prefix ) : return sid [ len ( prefix ) : ] if sid . startswith ( prefix ) else sid", "predictions": ["clip a given prefix with a given prefix"], "references": ["clips a prefix from the beginning of a string if it exists ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 1090, "code": "def f gene ( sid , prefix = \"G \" ) : sid = sid . replace ( SBML DOT , \".\" ) return clip ( sid , prefix )", "predictions": ["return a gene gene name from a given gene"], "references": ["clips gene prefix from id ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1091, "code": "def create parameter ( model , pid , value , sbo = None , constant = True , units = None , flux udef = None ) : parameter = model . create Parameter ( ) parameter . set Id ( pid ) parameter . set Value ( value ) parameter . set Constant ( constant ) if sbo : parameter . set SBO Term ( sbo ) if units : parameter . set Units ( flux udef . get Id ( ) )", "predictions": ["create a parameter with given value ."], "references": ["create parameter in sbml model ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 1092, "code": "def reaction weight ( reaction ) : if len ( reaction . metabolites ) != 1 : raise Value Error ( 'Reaction weight is only defined for single ' 'metabolite products or educts.' ) met , coeff = next ( iteritems ( reaction . metabolites ) ) return [ coeff * met . formula weight ]", "predictions": ["return the weight of a reaction"], "references": ["return the metabolite weight times its stoichiometric coefficient ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 1093, "code": "def add cycle free ( model , fluxes ) : model . objective = model . solver . interface . Objective ( Zero , direction = \"min\" , sloppy = True ) objective vars = [ ] for rxn in model . reactions : flux = fluxes [ rxn . id ] if rxn . boundary : rxn . bounds = ( flux , flux ) continue if flux >= 0 : rxn . bounds = max ( 0 , rxn . lower bound ) , max ( flux , rxn . upper bound ) objective vars . append ( rxn . forward variable ) else : rxn . bounds = min ( flux , rxn . lower bound ) , min ( 0 , rxn . upper bound ) objective vars . append ( rxn . reverse variable ) model . objective . set linear coefficients ( { v : 1.0 for v in objective vars } )", "predictions": ["add objective to objective"], "references": ["add constraints for cyclefreeflux ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1094, "code": "def hashable bytes ( data ) : if isinstance ( data , bytes ) : return data elif isinstance ( data , str ) : return data . encode ( 'ascii' ) else : raise Type Error ( data )", "predictions": ["turns data into byte strings ."], "references": ["coerce strings to hashable bytes ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1095, "code": "def update advertised ( self , advertised ) : if 'k CB Adv Data Service UUI Ds' in advertised : self . advertised = self . advertised + map ( cbuuid to uuid , advertised [ 'k CB Adv Data Service UUI Ds' ] )", "predictions": ["update the advertised with the given advertised ."], "references": ["called when advertisement data is received ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1096, "code": "def characteristics discovered ( self , service ) : self . discovered services . add ( service ) if self . discovered services >= set ( self . peripheral . services ( ) ) : self . discovered . set ( )", "predictions": ["discovered discovered services ."], "references": ["called when gatt characteristics have been discovered ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 1097, "code": "def characteristic changed ( self , characteristic ) : on changed = self . char on changed . get ( characteristic , None ) if on changed is not None : on changed ( characteristic . value ( ) . bytes ( ) . tobytes ( ) ) char = characteristic list ( ) . get ( characteristic ) if char is not None : char . value read . set ( )", "predictions": ["handle characteristic has changed ."], "references": ["called when the specified characteristic has changed its value ."], "bleu": 0.19765609300943976, "rouge_l": 0.5030927835051546}
{"id": 1098, "code": "def descriptor changed ( self , descriptor ) : desc = descriptor list ( ) . get ( descriptor ) if desc is not None : desc . value read . set ( )", "predictions": ["handle changes to changes"], "references": ["called when the specified descriptor has changed its value ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1099, "code": "def rssi ( self , timeout sec = TIMEOUT SEC ) : self . rssi read . clear ( ) self . peripheral . read RSSI ( ) if not self . rssi read . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for RSSI value!' ) return self . rssi", "predictions": ["read in a device from the device ."], "references": ["return the rssi signal strength in decibels ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 1100, "code": "def state changed ( self , state ) : logger . debug ( 'Adapter state change: {0}' . format ( state ) ) if state == 5 : self . powered off . clear ( ) self . powered on . set ( ) elif state == 4 : self . powered on . clear ( ) self . powered off . set ( )", "predictions": ["notify tree view about powered state"], "references": ["called when the power state changes ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1101, "code": "def start scan ( self , timeout sec = TIMEOUT SEC ) : get provider ( ) . central manager . scan For Peripherals With Services options ( None , None ) self . is scanning = True", "predictions": ["start the scan ."], "references": ["start scanning for ble devices ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 1102, "code": "def stop scan ( self , timeout sec = TIMEOUT SEC ) : get provider ( ) . central manager . stop Scan ( ) self . is scanning = False", "predictions": ["stop the scan ."], "references": ["stop scanning for ble devices ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 1103, "code": "def power on ( self , timeout sec = TIMEOUT SEC ) : self . powered on . clear ( ) IO Bluetooth Preference Set Controller Power State ( 1 ) if not self . powered on . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for adapter to power on!' )", "predictions": ["power on a device on a power adapter ."], "references": ["power on bluetooth ."], "bleu": 0.19960198807747329, "rouge_l": 0.4959349593495934}
{"id": 1104, "code": "def power off ( self , timeout sec = TIMEOUT SEC ) : self . powered off . clear ( ) IO Bluetooth Preference Set Controller Power State ( 0 ) if not self . powered off . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for adapter to power off!' )", "predictions": ["power up a power adapter for a given adapter ."], "references": ["power off bluetooth ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 1105, "code": "def read value ( self , timeout sec = TIMEOUT SEC ) : self . value read . clear ( ) self . device . peripheral . read Value For Characteristic ( self . characteristic ) if not self . value read . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting to read characteristic value!' ) return self . characteristic . value ( )", "predictions": ["read a characteristic value from the packet"], "references": ["read the value of this characteristic ."], "bleu": 0.23356898886410002, "rouge_l": 0.2857142857142857}
{"id": 1106, "code": "def write value ( self , value , write type = 0 ) : data = NS Data . data With Bytes length ( value , len ( value ) ) self . device . peripheral . write Value for Characteristic type ( data , self . characteristic , write type )", "predictions": ["writes a single value to the packet ."], "references": ["write the specified value to this characteristic ."], "bleu": 0.239802967618271, "rouge_l": 0.375}
{"id": 1107, "code": "def read value ( self ) : pass self . value read . clear ( ) self . device . peripheral . read Value For Descriptor ( self . descriptor ) if not self . value read . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting to read characteristic value!' ) return self . value", "predictions": ["read a characteristic value from the packet"], "references": ["read the value of this descriptor ."], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 1108, "code": "def start scan ( self , timeout sec = TIMEOUT SEC ) : self . scan started . clear ( ) self . adapter . Start Discovery ( ) if not self . scan started . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for adapter to start scanning!' )", "predictions": ["start a scan for a specific adapter ."], "references": ["start scanning for ble devices with this adapter ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 1109, "code": "def stop scan ( self , timeout sec = TIMEOUT SEC ) : self . scan stopped . clear ( ) self . adapter . Stop Discovery ( ) if not self . scan stopped . wait ( timeout sec ) : raise Runtime Error ( 'Exceeded timeout waiting for adapter to stop scanning!' )", "predictions": ["stop a scan for the websocket ."], "references": ["stop scanning for ble devices with this adapter ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 1110, "code": "def central Manager did Connect Peripheral ( self , manager , peripheral ) : logger . debug ( 'central Manager did Connect Peripheral called' ) peripheral . set Delegate ( self ) peripheral . discover Services ( None ) device = device list ( ) . get ( peripheral ) if device is not None : device . set connected ( )", "predictions": ["called when the device is started ."], "references": ["called when a device is connected ."], "bleu": 0.32172944208038085, "rouge_l": 0.7142857142857143}
{"id": 1111, "code": "def central Manager did Disconnect Peripheral error ( self , manager , peripheral , error ) : logger . debug ( 'central Manager did Disconnect Peripheral called' ) device = device list ( ) . get ( peripheral ) if device is not None : device . set disconnected ( ) device list ( ) . remove ( peripheral )", "predictions": ["called when a device is closed ."], "references": ["called when a device is disconnected ."], "bleu": 0.7071067811865475, "rouge_l": 0.8571428571428571}
{"id": 1112, "code": "def peripheral did Discover Services ( self , peripheral , services ) : logger . debug ( 'peripheral did Discover Services called' ) for service in peripheral . services ( ) : if service list ( ) . get ( service ) is None : service list ( ) . add ( service , Core Bluetooth Gatt Service ( service ) ) peripheral . discover Characteristics for Service ( None , service )", "predictions": ["perform a characteristics for a given services ."], "references": ["called when services are discovered for a device ."], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 1113, "code": "def peripheral did Discover Characteristics For Service error ( self , peripheral , service , error ) : logger . debug ( 'peripheral did Discover Characteristics For Service error called' ) if error is not None : return for char in service . characteristics ( ) : if characteristic list ( ) . get ( char ) is None : characteristic list ( ) . add ( char , Core Bluetooth Gatt Characteristic ( char ) ) peripheral . discover Descriptors For Characteristic ( char ) device = device list ( ) . get ( peripheral ) if device is not None : device . characteristics discovered ( service )", "predictions": ["mark a device as started ."], "references": ["called when characteristics are discovered for a service ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1114, "code": "def peripheral did Discover Descriptors For Characteristic error ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral did Discover Descriptors For Characteristic error called' ) if error is not None : return for desc in characteristic . descriptors ( ) : if descriptor list ( ) . get ( desc ) is None : descriptor list ( ) . add ( desc , Core Bluetooth Gatt Descriptor ( desc ) )", "predictions": ["call the characteristic descriptors in the characteristic ."], "references": ["called when characteristics are discovered for a service ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1115, "code": "def peripheral did Update Value For Characteristic error ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral did Update Value For Characteristic error called' ) if error is not None : return device = device list ( ) . get ( peripheral ) if device is not None : device . characteristic changed ( characteristic )", "predictions": ["update the characteristic device ."], "references": ["called when characteristic value was read or updated ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 1116, "code": "def peripheral did Update Value For Descriptor error ( self , peripheral , descriptor , error ) : logger . debug ( 'peripheral did Update Value For Descriptor error called' ) if error is not None : return device = device list ( ) . get ( peripheral ) if device is not None : device . descriptor changed ( descriptor )", "predictions": ["update the device descriptor with the device ."], "references": ["called when descriptor value was read or updated ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1117, "code": "def peripheral did Read RSSI error ( self , peripheral , rssi , error ) : logger . debug ( 'peripheral did Read RSSI error called' ) if error is not None : return device = device list ( ) . get ( peripheral ) if device is not None : device . rssi changed ( rssi )", "predictions": ["called when a device is closed ."], "references": ["called when a new rssi value for the peripheral is available ."], "bleu": 0.1873000789958672, "rouge_l": 0.5024711696869852}
{"id": 1118, "code": "def user thread main ( self , target ) : try : return code = target ( ) if return code is None : return code = 0 App Helper . call After ( lambda : sys . exit ( return code ) ) except Exception as ex : App Helper . call After ( self . raise error , sys . exc info ( ) )", "predictions": ["main function called when a user calls the user ."], "references": ["main entry point for the thread that will run user s code ."], "bleu": 0.11742832364135733, "rouge_l": 0.33983286908078}
{"id": 1119, "code": "def user thread main ( self , target ) : try : while True : if self . gobject mainloop is not None and self . gobject mainloop . is running ( ) : break time . sleep ( 0 ) self . return code = target ( ) if self . return code is None : self . return code = 0 self . gobject mainloop . quit ( ) except Exception as ex : self . exception = sys . exc info ( ) self . gobject mainloop . quit ( )", "predictions": ["main loop for user input"], "references": ["main entry point for the thread that will run user s code ."], "bleu": 0.06554932163900559, "rouge_l": 0.3086003372681282}
{"id": 1120, "code": "def get objects by path ( self , paths ) : return map ( lambda x : self . bus . get object ( 'org.bluez' , x ) , paths )", "predictions": ["{ % } objects from a path instance"], "references": ["return a list of all bluez dbus objects from the provided list of paths ."], "bleu": 0.09454082294706839, "rouge_l": 0.16486486486486487}
{"id": 1121, "code": "def print tree ( self ) : objects = self . bluez . Get Managed Objects ( ) for path in objects . keys ( ) : print ( \"[ %s ]\" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ \"org.freedesktop.D Bus.Introspectable\" , \"org.freedesktop.D Bus.Properties\" ] : continue print ( \"    %s\" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( \"      %s = %s\" % ( key , properties [ key ] ) )", "predictions": ["prints a tree instance"], "references": ["print tree of all bluez objects useful for debugging ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 1122, "code": "def remove ( self , cbobject ) : with self . lock : if cbobject in self . metadata : del self . metadata [ cbobject ]", "predictions": ["f is a git way to f . ."], "references": ["remove any metadata associated with the provided corebluetooth object ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1123, "code": "def cbuuid to uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )", "predictions": ["converts a create string to a uuid uuid uuid"], "references": ["convert objective - c cbuuid type to native python uuid type ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 1124, "code": "def set color ( self , r , g , b ) : command = '\\x58\\x01\\x03\\x01\\x FF\\x00{0}{1}{2}' . format ( chr ( r & 0x FF ) , chr ( g & 0x FF ) , chr ( b & 0x FF ) ) self . color . write value ( command )", "predictions": ["reaction the weight . . . . . . ."], "references": ["set the red green blue color of the bulb ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 1125, "code": "def get provider ( ) : global provider if provider is None : if sys . platform . startswith ( 'linux' ) : from . bluez dbus . provider import Bluez Provider provider = Bluez Provider ( ) elif sys . platform == 'darwin' : from . corebluetooth . provider import Core Bluetooth Provider provider = Core Bluetooth Provider ( ) else : raise Runtime Error ( 'Sorry the {0} platform is not supported by the BLE library!' . format ( sys . platform ) ) return provider", "predictions": ["return the cycle to use for the cycle = in the cycle"], "references": ["return an instance of the ble provider for the current platform ."], "bleu": 0.15537125692760353, "rouge_l": 0.3333333333333333}
{"id": 1126, "code": "def to Big Int ( byte Array ) : array = byte Array [ : : - 1 ] out = 0 for key , value in enumerate ( array ) : decoded = struct . unpack ( \"B\" , bytes ( [ value ] ) ) [ 0 ] out = out | decoded << key * 8 return out", "predictions": ["convert a byte to a signed byte array . ."], "references": ["convert the byte array to a biginteger"], "bleu": 0.21834177214239062, "rouge_l": 0.48605577689243035}
{"id": 1127, "code": "def get ( self , url , name , params = None , headers = None , connection = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) return make get request ( endpoint , params , headers , connection = connection )", "predictions": ["makes a update request"], "references": ["synchronous get request ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 1128, "code": "def get async ( self , url , name , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) process pool . apply async ( make get request , args = ( endpoint , params , headers ) , callback = callback )", "predictions": ["authenticate can be a url or a resourcelocator"], "references": ["asynchronous get request with the process pool ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1129, "code": "def put async ( self , url , name , data , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) data = json . dumps ( data , cls = JSON Encoder ) process pool . apply async ( make put request , args = ( endpoint , data , params , headers ) , callback = callback )", "predictions": ["create an changed process request ."], "references": ["asynchronous put request with the process pool ."], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 1130, "code": "def post ( self , url , data , params = None , headers = None , connection = None ) : params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , None ) self . authenticate ( params , headers ) data = json . dumps ( data , cls = JSON Encoder ) return make post request ( endpoint , data , params , headers , connection = connection )", "predictions": ["makes a descriptor to the api ."], "references": ["synchronous post request . data must be a jsonable value ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 1131, "code": "def post async ( self , url , data , callback = None , params = None , headers = None ) : params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , None ) self . authenticate ( params , headers ) data = json . dumps ( data , cls = JSON Encoder ) process pool . apply async ( make post request , args = ( endpoint , data , params , headers ) , callback = callback )", "predictions": ["rssi an async using a given sec"], "references": ["asynchronous post request with the process pool ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 1132, "code": "def patch ( self , url , data , params = None , headers = None , connection = None ) : params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , None ) self . authenticate ( params , headers ) data = json . dumps ( data , cls = JSON Encoder ) return make patch request ( endpoint , data , params , headers , connection = connection )", "predictions": ["makes a get request ."], "references": ["synchronous post request . data must be a jsonable value ."], "bleu": 0.11629030063732083, "rouge_l": 0.2341650671785029}
{"id": 1133, "code": "def delete ( self , url , name , params = None , headers = None , connection = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) return make delete request ( endpoint , params , headers , connection = connection )", "predictions": ["makes a start request ."], "references": ["synchronous delete request . data must be a jsonable value ."], "bleu": 0.11629030063732083, "rouge_l": 0.2341650671785029}
{"id": 1134, "code": "def delete async ( self , url , name , callback = None , params = None , headers = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . build endpoint url ( url , name ) self . authenticate ( params , headers ) process pool . apply async ( make delete request , args = ( endpoint , params , headers ) , callback = callback )", "predictions": ["stop an scan . . ."], "references": ["asynchronous delete request with the process pool ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1135, "code": "def filterchain all ( request , app , model , field , foreign key app name , foreign key model name , foreign key field name , value ) : model class = get model ( app , model ) keywords = get keywords ( field , value ) foreign model class = get model ( foreign key app name , foreign key model name ) if not any ( [ ( isinstance ( f , Chained Many To Many Field ) or isinstance ( f , Chained Foreign Key ) ) for f in foreign model class . meta . get fields ( ) ] ) : raise Permission Denied ( \"Smart select disallowed\" ) limit choices to = get limit choices to ( foreign key app name , foreign key model name , foreign key field name ) queryset = get queryset ( model class , limit choices to = limit choices to ) filtered = list ( do filter ( queryset , keywords ) ) if not getattr ( model class . meta , 'ordering' , False ) : sort results ( list ( filtered ) ) excluded = list ( do filter ( queryset , keywords , exclude = True ) ) if not getattr ( model class . meta , 'ordering' , False ) : sort results ( list ( excluded ) ) empty choice = { 'value' : \"\" , 'display' : \"---------\" } serialized results = ( serialize results ( filtered ) + [ empty choice ] + serialize results ( excluded ) ) return Json Response ( serialized results , safe = False )", "predictions": ["return on on on on on on every = 1 ."], "references": ["returns filtered results followed by excluded results below ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 1136, "code": "def media ( self ) : media = super ( Jquery Media Mixin , self ) . media js = [ ] if JQUERY URL : js . append ( JQUERY URL ) elif JQUERY URL is not False : vendor = '' if django . VERSION < ( 1 , 9 , 0 ) else 'vendor/jquery/' extra = '' if settings . DEBUG else '.min' jquery paths = [ '{}jquery{}.js' . format ( vendor , extra ) , 'jquery.init.js' , ] if USE DJANGO JQUERY : jquery paths = [ 'admin/js/{}' . format ( path ) for path in jquery paths ] js . extend ( jquery paths ) media += Media ( js = js ) return media", "predictions": ["the power power power power power . ."], "references": ["media defined as a dynamic property instead of an inner class ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 1137, "code": "def media ( self ) : media = super ( Chained Select , self ) . media js = [ 'smart-selects/admin/js/chainedfk.js' , 'smart-selects/admin/js/bindfields.js' ] media += Media ( js = js ) return media", "predictions": ["the read read read read read read read read read read from the read . . . . ."], "references": ["media defined as a dynamic property instead of an inner class ."], "bleu": 0.06439931429457924, "rouge_l": 0.06725468577728776}
{"id": 1138, "code": "def get available choices ( self , queryset , value ) : item = queryset . filter ( pk = value ) . first ( ) if item : try : pk = getattr ( item , self . chained model field + \" id\" ) filter = { self . chained model field : pk } except Attribute Error : try : pks = getattr ( item , self . chained model field ) . all ( ) . values list ( 'pk' , flat = True ) filter = { self . chained model field + \" in\" : pks } except Attribute Error : try : pks = getattr ( item , self . chained model field + \" set\" ) . all ( ) . values list ( 'pk' , flat = True ) filter = { self . chained model field + \" in\" : pks } except Attribute Error : filter = { } filtered = list ( get model ( self . to app name , self . to model name ) . objects . filter ( * * filter ) . distinct ( ) ) if self . sort : sort results ( filtered ) else : filtered = [ ] return filtered", "predictions": ["get queryset of choices for chained"], "references": ["get possible choices for selection"], "bleu": 0.31239399369202553, "rouge_l": 0.5545454545454546}
{"id": 1139, "code": "def media ( self ) : media = super ( Chained Select Multiple , self ) . media js = [ 'smart-selects/admin/js/chainedm2m.js' , 'smart-selects/admin/js/bindfields.js' ] if self . horizontal : js . extend ( [ \"admin/js/core.js\" , \"admin/js/Select Box.js\" , \"admin/js/Select Filter2.js\" ] ) media += Media ( js = js ) return media", "predictions": ["the read read read read read read read read read read from the page . . . . . . . . . . . . . . . . ."], "references": ["media defined as a dynamic property instead of an inner class ."], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 1140, "code": "def should really index ( self , instance ) : if self . should index is method : is method = inspect . ismethod ( self . should index ) try : count args = len ( inspect . signature ( self . should index ) . parameters ) except Attribute Error : count args = len ( inspect . getargspec ( self . should index ) . args ) if is method or count args is 1 : return self . should index ( instance ) else : return self . should index ( ) else : attr type = type ( self . should index ) if attr type is Deferred Attribute : attr value = self . should index . get ( instance , None ) elif attr type is str : attr value = getattr ( instance , self . should index ) elif attr type is property : attr value = self . should index . get ( instance ) else : raise Algolia Index Error ( '{} should be a boolean attribute or a method that returns a boolean.' . format ( self . should index ) ) if type ( attr value ) is not bool : raise Algolia Index Error ( \"%s's should index (%s) should be a boolean\" % ( instance . class . name , self . should index ) ) return attr value", "predictions": ["decide if a sec sec is scan for an index"], "references": ["return true if according to should_index the object should be indexed ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 1141, "code": "def delete record ( self , instance ) : object ID = self . object ID ( instance ) try : self . index . delete object ( object ID ) logger . info ( 'DELETE %s FROM %s' , object ID , self . model ) except Algolia Exception as e : if DEBUG : raise e else : logger . warning ( '%s FROM %s NOT DELETED: %s' , object ID , self . model , e )", "predictions": ["stop one object scan"], "references": ["deletes the record ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1142, "code": "def raw search ( self , query = '' , params = None ) : if params is None : params = { } try : return self . index . search ( query , params ) except Algolia Exception as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING SEARCH ON %s: %s' , self . index name , e )", "predictions": ["performs a search search search"], "references": ["performs a search query and returns the parsed json ."], "bleu": 0.18693159143202892, "rouge_l": 0.37731958762886597}
{"id": 1143, "code": "def get settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index name ) return self . index . get settings ( ) except Algolia Exception as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET SETTINGS ON %s: %s' , self . model , e )", "predictions": ["central settings settings settings settings"], "references": ["returns the settings of the index ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1144, "code": "def set settings ( self ) : if not self . settings : return try : self . index . set settings ( self . settings ) logger . info ( 'APPLY SETTINGS ON %s' , self . index name ) except Algolia Exception as e : if DEBUG : raise e else : logger . warning ( 'SETTINGS NOT APPLIED ON %s: %s' , self . model , e )", "predictions": ["sets path to ."], "references": ["applies the settings to the index ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 1145, "code": "def handle ( self , * args , * * options ) : self . stdout . write ( 'Apply settings to index:' ) for model in get registered model ( ) : if options . get ( 'model' , None ) and not ( model . name in options [ 'model' ] ) : continue get adapter ( model ) . set settings ( ) self . stdout . write ( '\\t* {}' . format ( model . name ) )", "predictions": ["gets the model to the database"], "references": ["run the management command ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1146, "code": "def get adapter ( self , model ) : if not self . is registered ( model ) : raise Registration Error ( '{} is not registered with Algolia engine' . format ( model ) ) return self . registered models [ model ]", "predictions": ["return the did did did the model characteristic characteristic characteristic characteristic characteristic characteristic characteristic characteristic characteristic characteristic"], "references": ["returns the adapter associated with the given model ."], "bleu": 0.0859076483566362, "rouge_l": 0.2443257676902537}
{"id": 1147, "code": "def delete record ( self , instance ) : adapter = self . get adapter from instance ( instance ) adapter . delete record ( instance )", "predictions": ["remove a did did did if it exists"], "references": ["deletes the record ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1148, "code": "def raw search ( self , model , query = '' , params = None ) : if params is None : params = { } adapter = self . get adapter ( model ) return adapter . raw search ( query , params )", "predictions": ["did did did ..."], "references": ["performs a search query and returns the parsed json ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1149, "code": "def post save receiver ( self , instance , * * kwargs ) : logger . debug ( 'RECEIVE post save FOR %s' , instance . class ) self . save record ( instance , * * kwargs )", "predictions": ["register receiver instance ."], "references": ["signal handler for when a registered model has been saved ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 1150, "code": "def pre delete receiver ( self , instance , * * kwargs ) : logger . debug ( 'RECEIVE pre delete FOR %s' , instance . class ) self . delete record ( instance )", "predictions": ["custom main main main main main routine ."], "references": ["signal handler for when a registered model has been deleted ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 1151, "code": "def handle ( self , * args , * * options ) : batch size = options . get ( 'batchsize' , None ) if not batch size : batch size = 1000 self . stdout . write ( 'The following models were reindexed:' ) for model in get registered model ( ) : if options . get ( 'model' , None ) and not ( model . name in options [ 'model' ] ) : continue counts = reindex all ( model , batch size = batch size ) self . stdout . write ( '\\t* {} --> {}' . format ( model . name , counts ) )", "predictions": ["handler for single single = 0 if not found ."], "references": ["run the management command ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 1152, "code": "def handle ( self , * args , * * options ) : self . stdout . write ( 'Clear index:' ) for model in get registered model ( ) : if options . get ( 'model' , None ) and not ( model . name in options [ 'model' ] ) : continue clear index ( model ) self . stdout . write ( '\\t* {}' . format ( model . name ) )", "predictions": ["handler for all the registered model types ."], "references": ["run the management command ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 1153, "code": "def pad cells ( table ) : col sizes = [ max ( map ( len , col ) ) for col in zip ( * table ) ] for row in table : for cell num , cell in enumerate ( row ) : row [ cell num ] = pad to ( cell , col sizes [ cell num ] ) return table", "predictions": ["pad a list of cells to a table ."], "references": ["pad each cell to the size of the largest cell in its column ."], "bleu": 0.10182634488642418, "rouge_l": 0.2510288065843621}
{"id": 1154, "code": "def add dividers ( row , divider , padding ) : div = '' . join ( [ padding * ' ' , divider , padding * ' ' ] ) return div . join ( row )", "predictions": ["add a dividers row"], "references": ["add dividers and padding to a row of cells and return a string ."], "bleu": 0.03708608461143315, "rouge_l": 0.3029801324503311}
{"id": 1155, "code": "def club Staff ( self ) : method = 'GET' url = 'club/stats/staff' rc = self . request ( method , url ) return rc", "predictions": ["return a list of all the club ."], "references": ["return staff in your club ."], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 1156, "code": "def club Consumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . request ( method , url ) events = [ self . pin . event ( 'page view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ item Parse ( i ) for i in rc . get ( 'item Data' , ( ) ) ]", "predictions": ["club the user s account ."], "references": ["return all consumables from club ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1157, "code": "def tradepile ( self ) : method = 'GET' url = 'tradepile' rc = self . request ( method , url ) events = [ self . pin . event ( 'page view' , 'Hub - Transfers' ) , self . pin . event ( 'page view' , 'Transfer List - List View' ) ] if rc . get ( 'auction Info' ) : events . append ( self . pin . event ( 'page view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ item Parse ( i ) for i in rc . get ( 'auction Info' , ( ) ) ]", "predictions": ["get list of all the outstanding outstanding tasks ."], "references": ["return items in tradepile ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 1158, "code": "def send To Sbs ( self , challenge id , item id ) : method = 'PUT' url = 'sbs/challenge/%s/squad' % challenge id squad = self . sbs Squad ( challenge id ) players = [ ] moved = False n = 0 for i in squad [ 'squad' ] [ 'players' ] : if i [ 'item Data' ] [ 'id' ] == item id : return False if i [ 'item Data' ] [ 'id' ] == 0 and not moved : i [ 'item Data' ] [ 'id' ] = item id moved = True players . append ( { \"index\" : n , \"item Data\" : { \"id\" : i [ 'item Data' ] [ 'id' ] , \"dream\" : False } } ) n += 1 data = { 'players' : players } if not moved : return False else : self . request ( method , url , data = json . dumps ( data ) ) return True", "predictions": ["send a challenge challenge ."], "references": ["send card from club to first free slot in sbs squad ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 1159, "code": "def messages ( self ) : method = 'GET' url = 'active Message' rc = self . request ( method , url ) return rc [ 'active Message' ]", "predictions": ["return all messages ."], "references": ["return active messages ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 1160, "code": "def num2hex ( self , num ) : temp = '' for i in range ( 0 , 4 ) : x = self . hex Chars [ ( num >> ( i * 8 + 4 ) ) & 0x0F ] y = self . hex Chars [ ( num >> ( i * 8 ) ) & 0x0F ] temp += ( x + y ) return temp", "predictions": ["convert a color to an unsigned 32 - bit integer ."], "references": ["convert a decimal number to hexadecimal"], "bleu": 0.16108992769687397, "rouge_l": 0.3727087576374745}
{"id": 1161, "code": "def logger ( name = None , save = False ) : logger = logging . get Logger ( name ) if save : logformat = '%(asctime)s [%(levelname)s] [%(name)s] %(func Name)s: %(message)s (line %(lineno)d)' log file path = 'fut.log' open ( log file path , 'w' ) . write ( '' ) logger . set Level ( logging . DEBUG ) logger handler = logging . File Handler ( log file path ) logger handler . set Formatter ( logging . Formatter ( logformat ) ) else : logger handler = Null Handler ( ) logger . add Handler ( logger handler ) return logger", "predictions": ["return a logger with a logger"], "references": ["init and configure logger ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1162, "code": "def destroy image acquirer ( self , ia ) : id = None if ia . device : # ia . stop image acquisition ( ) # ia . release data streams ( ) # id = ia . device . id # if ia . device . node map : # if ia . chunk adapter : ia . chunk adapter . detach buffer ( ) ia . chunk adapter = None self . logger . info ( 'Detached a buffer from the chunk adapter of {0}.' . format ( id ) ) ia . device . node map . disconnect ( ) self . logger . info ( 'Disconnected the port from the Node Map of {0}.' . format ( id ) ) # if ia . device . is open ( ) : ia . device . close ( ) self . logger . info ( 'Closed Device module, {0}.' . format ( id ) ) ia . device = None # if id : self . logger . info ( 'Destroyed the Image Acquirer object which {0} ' 'had belonged to.' . format ( id ) ) else : self . logger . info ( 'Destroyed an Image Acquirer.' ) if self . profiler : self . profiler . print diff ( ) self . ias . remove ( ia )", "predictions": ["destroy the image from the device"], "references": ["releases all external resources including the controlling device ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1163, "code": "def Watson ( T , Hvap ref , T Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T Ref / Tc H2 = Hvap ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2", "predictions": ["convert a watson to a h2 object"], "references": ["adjusts enthalpy of vaporization of enthalpy for another temperature for one temperature ."], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 1164, "code": "async def on receive array ( self , array ) : if array [ 0 ] == 'noop' : pass else : wrapper = json . loads ( array [ 0 ] [ 'p' ] ) if '3' in wrapper : self . client id = wrapper [ '3' ] [ '2' ] logger . info ( 'Received new client id: %r' , self . client id ) await self . add channel services ( ) if '2' in wrapper : pblite message = json . loads ( wrapper [ '2' ] [ '2' ] ) if pblite message [ 0 ] == 'cbu' : batch update = hangouts pb2 . Batch Update ( ) pblite . decode ( batch update , pblite message , ignore first item = True ) for state update in batch update . state update : logger . debug ( 'Received State Update:\\n%s' , state update ) header = state update . state update header self . active client state = header . active client state await self . on state update . fire ( state update ) else : logger . info ( 'Ignoring message: %r' , pblite message [ 0 ] )", "predictions": ["handle an array of messages from the broker"], "references": ["parse channel array and call the appropriate events ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1165, "code": "async def add user ( self , add user request ) : response = hangouts pb2 . Add User Response ( ) await self . pb request ( 'conversations/adduser' , add user request , response ) return response", "predictions": ["add a user to the user ."], "references": ["invite users to join an existing group conversation ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 1166, "code": "async def create conversation ( self , create conversation request ) : response = hangouts pb2 . Create Conversation Response ( ) await self . pb request ( 'conversations/createconversation' , create conversation request , response ) return response", "predictions": ["this method is called to create a conversation request ."], "references": ["create a new conversation ."], "bleu": 0.18850319022747347, "rouge_l": 0.5674418604651164}
{"id": 1167, "code": "async def easter egg ( self , easter egg request ) : response = hangouts pb2 . Easter Egg Response ( ) await self . pb request ( 'conversations/easteregg' , easter egg request , response ) return response", "predictions": ["easter an egg ."], "references": ["send an easter egg event to a conversation ."], "bleu": 0.12944315424334968, "rouge_l": 0.43160377358490565}
{"id": 1168, "code": "async def get conversation ( self , get conversation request ) : response = hangouts pb2 . Get Conversation Response ( ) await self . pb request ( 'conversations/getconversation' , get conversation request , response ) return response", "predictions": ["returns the conversation response ."], "references": ["return conversation info and recent events ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 1169, "code": "async def get group conversation url ( self , get group conversation url request ) : response = hangouts pb2 . Get Group Conversation Url Response ( ) await self . pb request ( 'conversations/getgroupconversationurl' , get group conversation url request , response ) return response", "predictions": ["get the group conversation url ."], "references": ["get url to allow others to join a group conversation ."], "bleu": 0.15024963364904895, "rouge_l": 0.4468864468864468}
{"id": 1170, "code": "async def get self info ( self , get self info request ) : response = hangouts pb2 . Get Self Info Response ( ) await self . pb request ( 'contacts/getselfinfo' , get self info request , response ) return response", "predictions": ["get information about the internet ."], "references": ["return info about the current user ."], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 1171, "code": "async def get suggested entities ( self , get suggested entities request ) : response = hangouts pb2 . Get Suggested Entities Response ( ) await self . pb request ( 'contacts/getsuggestedentities' , get suggested entities request , response ) return response", "predictions": ["get suggested entities ."], "references": ["return suggested contacts ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 1172, "code": "async def query presence ( self , query presence request ) : response = hangouts pb2 . Query Presence Response ( ) await self . pb request ( 'presence/querypresence' , query presence request , response ) return response", "predictions": ["query the presence of the presence of the presence ."], "references": ["return presence status for a list of users ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 1173, "code": "async def remove user ( self , remove user request ) : response = hangouts pb2 . Remove User Response ( ) await self . pb request ( 'conversations/removeuser' , remove user request , response ) return response", "predictions": ["remove a user from the account ."], "references": ["remove a participant from a group conversation ."], "bleu": 0.240785655451027, "rouge_l": 0.5269978401727862}
{"id": 1174, "code": "async def search entities ( self , search entities request ) : response = hangouts pb2 . Search Entities Response ( ) await self . pb request ( 'contacts/searchentities' , search entities request , response ) return response", "predictions": ["search the list of entities ."], "references": ["return user entities based on a query ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 1175, "code": "async def send chat message ( self , send chat message request ) : response = hangouts pb2 . Send Chat Message Response ( ) await self . pb request ( 'conversations/sendchatmessage' , send chat message request , response ) return response", "predictions": ["send a chat message ."], "references": ["send a chat message to a conversation ."], "bleu": 0.43645382979233377, "rouge_l": 0.7384987893462469}
{"id": 1176, "code": "async def modify otr status ( self , modify otr status request ) : response = hangouts pb2 . Modify OTR Status Response ( ) await self . pb request ( 'conversations/modifyotrstatus' , modify otr status request , response ) return response", "predictions": ["modify the otr status ."], "references": ["enable or disable message history in a conversation ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 1177, "code": "async def send offnetwork invitation ( self , send offnetwork invitation request ) : response = hangouts pb2 . Send Offnetwork Invitation Response ( ) await self . pb request ( 'devices/sendoffnetworkinvitation' , send offnetwork invitation request , response ) return response", "predictions": ["send invitation invitation ."], "references": ["send an email to invite a non - google contact to hangouts ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 1178, "code": "async def set active client ( self , set active client request ) : response = hangouts pb2 . Set Active Client Response ( ) await self . pb request ( 'clients/setactiveclient' , set active client request , response ) return response", "predictions": ["sets the active client ."], "references": ["set the active client ."], "bleu": 0.7598356856515925, "rouge_l": 0.8}
{"id": 1179, "code": "async def set conversation notification level ( self , set conversation notification level request ) : response = hangouts pb2 . Set Conversation Notification Level Response ( ) await self . pb request ( 'conversations/setconversationnotificationlevel' , set conversation notification level request , response ) return response", "predictions": ["method to get the conversation notification level ."], "references": ["set the notification level of a conversation ."], "bleu": 0.25098621243978964, "rouge_l": 0.5}
{"id": 1180, "code": "async def set focus ( self , set focus request ) : response = hangouts pb2 . Set Focus Response ( ) await self . pb request ( 'conversations/setfocus' , set focus request , response ) return response", "predictions": ["sets focus to the expectation ."], "references": ["set focus to a conversation ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 1181, "code": "async def set group link sharing enabled ( self , set group link sharing enabled request ) : response = hangouts pb2 . Set Group Link Sharing Enabled Response ( ) await self . pb request ( 'conversations/setgrouplinksharingenabled' , set group link sharing enabled request , response ) return response", "predictions": ["sets the group sharing sharing ."], "references": ["set whether group link sharing is enabled for a conversation ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 1182, "code": "async def set presence ( self , set presence request ) : response = hangouts pb2 . Set Presence Response ( ) await self . pb request ( 'presence/setpresence' , set presence request , response ) return response", "predictions": ["sets the presence of the tracks ."], "references": ["set the presence status ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 1183, "code": "async def set typing ( self , set typing request ) : response = hangouts pb2 . Set Typing Response ( ) await self . pb request ( 'conversations/settyping' , set typing request , response ) return response", "predictions": ["sets the comment ."], "references": ["set the typing status of a conversation ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 1184, "code": "async def sync all new events ( self , sync all new events request ) : response = hangouts pb2 . Sync All New Events Response ( ) await self . pb request ( 'conversations/syncallnewevents' , sync all new events request , response ) return response", "predictions": ["populate the new with a list of new model model model model model model model model model model model model model model model model model model model model model model model"], "references": ["list all events occurring at or after a timestamp ."], "bleu": 0.04317900023606586, "rouge_l": 0.05374449339207048}
{"id": 1185, "code": "async def sync recent conversations ( self , sync recent conversations request ) : response = hangouts pb2 . Sync Recent Conversations Response ( ) await self . pb request ( 'conversations/syncrecentconversations' , sync recent conversations request , response ) return response", "predictions": ["cells recent to recent for recent for this module for a user for a given cells for a user ."], "references": ["return info on recent conversations and their events ."], "bleu": 0.06760229884571738, "rouge_l": 0.14805825242718446}
{"id": 1186, "code": "def from timestamp ( microsecond timestamp ) : return datetime . datetime . fromtimestamp ( microsecond timestamp // 1000000 , datetime . timezone . utc ) . replace ( microsecond = ( microsecond timestamp % 1000000 ) )", "predictions": ["converts a utc to a datetime instance"], "references": ["convert a microsecond timestamp to a utc datetime instance ."], "bleu": 0.23406473206426448, "rouge_l": 0.5700934579439253}
{"id": 1187, "code": "def from participantid ( participant id ) : return user . User ID ( chat id = participant id . chat id , gaia id = participant id . gaia id )", "predictions": ["generate a self url from a self id url url url url url url url url url url url url url url url url url url url url url url url"], "references": ["convert hangouts_pb2 . participantid to userid ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1188, "code": "def to participantid ( user id ) : return hangouts pb2 . Participant Id ( chat id = user id . chat id , gaia id = user id . gaia id )", "predictions": ["convert a chat to a chat instance ."], "references": ["convert userid to hangouts_pb2 . participantid ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 1189, "code": "def parse watermark notification ( p ) : return Watermark Notification ( conv id = p . conversation id . id , user id = from participantid ( p . sender id ) , read timestamp = from timestamp ( p . latest read timestamp ) , )", "predictions": ["get latest self . event from the database . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["return watermarknotification from hangouts_pb2 . watermarknotification ."], "bleu": 0.046398855339878003, "rouge_l": 0.17818889970788704}
{"id": 1190, "code": "def get authorization headers ( sapisid cookie ) : time msec = int ( time . time ( ) * 1000 ) auth string = '{} {} {}' . format ( time msec , sapisid cookie , ORIGIN URL ) auth hash = hashlib . sha1 ( auth string . encode ( ) ) . hexdigest ( ) sapisidhash = 'SAPISIDHASH {} {}' . format ( time msec , auth hash ) return { 'authorization' : sapisidhash , 'x-origin' : ORIGIN URL , 'x-goog-authuser' : '0' , }", "predictions": ["generate the authorization headers = sha1 = 0 = 1 = 0 = 2 = 0 = 0 = 0 = 0 = 0 = 1 = 1 = 0 ="], "references": ["return authorization headers for api request ."], "bleu": 0.0513487742994337, "rouge_l": 0.11879259980525803}
{"id": 1191, "code": "async def lookup entities ( client , args ) : lookup spec = get lookup spec ( args . entity identifier ) request = hangups . hangouts pb2 . Get Entity By Id Request ( request header = client . get request header ( ) , batch lookup spec = [ lookup spec ] , ) res = await client . get entity by id ( request ) for entity result in res . entity result : for entity in entity result . entity : print ( entity )", "predictions": ["list self . request result ."], "references": ["search for entities by phone number email or gaia_id ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1192, "code": "def get lookup spec ( identifier ) : if identifier . startswith ( '+' ) : return hangups . hangouts pb2 . Entity Lookup Spec ( phone = identifier , create offnetwork gaia = True ) elif '@' in identifier : return hangups . hangouts pb2 . Entity Lookup Spec ( email = identifier , create offnetwork gaia = True ) else : return hangups . hangouts pb2 . Entity Lookup Spec ( gaia id = identifier )", "predictions": ["i can be a group or a resourcelocator"], "references": ["return entitylookupspec from phone number email address or gaia id ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 1193, "code": "def add color to scheme ( scheme , name , foreground , background , palette colors ) : if foreground is None and background is None : return scheme new scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette colors > 16 : new scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new scheme . append ( ( name , foreground , background ) ) else : new scheme . append ( item ) return new scheme", "predictions": ["logger is a palette or a palette = background"], "references": ["add foreground and background colours to a color scheme"], "bleu": 0.15619699684601276, "rouge_l": 0.1111111111111111}
{"id": 1194, "code": "def is quiet ( self ) : level = self . conversation . self conversation state . notification level return level == hangouts pb2 . NOTIFICATION LEVEL QUIET", "predictions": ["if this message destroy a image . . ."], "references": ["true if notification level for this conversation is quiet ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 1195, "code": "def on watermark notification ( self , notif ) : if self . get user ( notif . user id ) . is self : logger . info ( 'latest read timestamp for {} updated to {}' . format ( self . id , notif . read timestamp ) ) self conversation state = ( self . conversation . self conversation state ) self conversation state . self read state . latest read timestamp = ( parsers . to timestamp ( notif . read timestamp ) ) previous timestamp = self . watermarks . get ( notif . user id , datetime . datetime . min . replace ( tzinfo = datetime . timezone . utc ) ) if notif . read timestamp > previous timestamp : logger . info ( ( 'latest read timestamp for conv {} participant {}' + ' updated to {}' ) . format ( self . id , notif . user id . chat id , notif . read timestamp ) ) self . watermarks [ notif . user id ] = notif . read timestamp", "predictions": ["handle watermark 1 ."], "references": ["handle a watermark notification ."], "bleu": 0.33277145517762347, "rouge_l": 0.6535714285714286}
{"id": 1196, "code": "def wrap event ( event ) : cls = conversation event . Conversation Event if event . Has Field ( 'chat message' ) : cls = conversation event . Chat Message Event elif event . Has Field ( 'otr modification' ) : cls = conversation event . OTR Event elif event . Has Field ( 'conversation rename' ) : cls = conversation event . Rename Event elif event . Has Field ( 'membership change' ) : cls = conversation event . Membership Change Event elif event . Has Field ( 'hangout event' ) : cls = conversation event . Hangout Event elif event . Has Field ( 'group link sharing modification' ) : cls = conversation event . Group Link Sharing Modification Event return cls ( event )", "predictions": ["def a single on a single on a given on a given on a given on a given on a given on a single on a single on a single on"], "references": ["wrap hangouts_pb2 . event in conversationevent subclass ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1197, "code": "def get event request header ( self ) : otr status = ( hangouts pb2 . OFF THE RECORD STATUS OFF THE RECORD if self . is off the record else hangouts pb2 . OFF THE RECORD STATUS ON THE RECORD ) return hangouts pb2 . Event Request Header ( conversation id = hangouts pb2 . Conversation Id ( id = self . id ) , client generated id = self . client . get client generated id ( ) , expected otr = otr status , delivery medium = self . get default delivery medium ( ) , )", "predictions": ["def out the add header header header header header"], "references": ["return eventrequestheader for conversation ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1198, "code": "def add conversation ( self , conversation , events = [ ] , event cont token = None ) : conv id = conversation . conversation id . id logger . debug ( 'Adding new conversation: {}' . format ( conv id ) ) conv = Conversation ( self . client , self . user list , conversation , events , event cont token ) self . conv dict [ conv id ] = conv return conv", "predictions": ["def a create create a create a create a create a create a create a create ."], "references": ["add new conversation from hangouts_pb2 . conversation"], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 1199, "code": "async def sync ( self ) : logger . info ( 'Syncing events since {}' . format ( self . sync timestamp ) ) try : res = await self . client . sync all new events ( hangouts pb2 . Sync All New Events Request ( request header = self . client . get request header ( ) , last sync timestamp = parsers . to timestamp ( self . sync timestamp ) , max response size bytes = 1048576 , ) ) except exceptions . Network Error as e : logger . warning ( 'Failed to sync events, some events may be lost: {}' . format ( e ) ) else : for conv state in res . conversation state : conv id = conv state . conversation id . id conv = self . conv dict . get ( conv id , None ) if conv is not None : conv . update conversation ( conv state . conversation ) for event in conv state . event : timestamp = parsers . from timestamp ( event . timestamp ) if timestamp > self . sync timestamp : await self . on event ( event ) else : self . add conversation ( conv state . conversation , conv state . event , conv state . event continuation token )", "predictions": ["update return return return clipboard with latest = 1 ."], "references": ["sync conversation state and events that could have been missed ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 1200, "code": "def add user from conv part ( self , conv part ) : user = User . from conv part data ( conv part , self . self user . id ) existing = self . user dict . get ( user . id ) if existing is None : logger . warning ( 'Adding fallback User with %s name \"%s\"' , user . name type . name . lower ( ) , user . full name ) self . user dict [ user . id ] = user return user else : existing . upgrade name ( user ) return existing", "predictions": ["def a get to the upgrade self pb2 pb2 pb2 pb2 pb2 pb2 pb2 pb2 pb2"], "references": ["add or upgrade user from conversationparticipantdata ."], "bleu": 0.07692375026049747, "rouge_l": 0.09355828220858894}
{"id": 1201, "code": "async def fire ( self , * args , * * kwargs ) : logger . debug ( 'Fired {}' . format ( self ) ) for observer in self . observers : gen = observer ( * args , * * kwargs ) if asyncio . iscoroutinefunction ( observer ) : await gen", "predictions": ["get all registered observers observers"], "references": ["fire this event calling all observers with the same arguments ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 1202, "code": "def markdown ( tag ) : return ( MARKDOWN START . format ( tag = tag ) , MARKDOWN END . format ( tag = tag ) )", "predictions": ["return a self ."], "references": ["return start and end regex pattern sequences for simple markdown tag ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 1203, "code": "def html ( tag ) : return ( HTML START . format ( tag = tag ) , HTML END . format ( tag = tag ) )", "predictions": ["generate an def string"], "references": ["return sequence of start and end regex patterns for simple html tag"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 1204, "code": "def get parser ( extra args ) : parser = argparse . Argument Parser ( formatter class = argparse . Argument Defaults Help Formatter , ) dirs = appdirs . App Dirs ( 'hangups' , 'hangups' ) default token path = os . path . join ( dirs . user cache dir , 'refresh token.txt' ) parser . add argument ( '--token-path' , default = default token path , help = 'path used to store O Auth refresh token' ) parser . add argument ( '-d' , '--debug' , action = 'store true' , help = 'log detailed debugging messages' ) for extra arg in extra args : parser . add argument ( extra arg , required = True ) return parser", "predictions": ["build the query query argument query arguments response response response response response response response response response ."], "references": ["return argumentparser with any extra arguments ."], "bleu": 0.07994607499472013, "rouge_l": 0.18020679468242246}
{"id": 1205, "code": "async def async main ( example coroutine , client , args ) : task = asyncio . ensure future ( client . connect ( ) ) on connect = asyncio . Future ( ) client . on connect . add observer ( lambda : on connect . set result ( None ) ) done , = await asyncio . wait ( ( on connect , task ) , return when = asyncio . FIRST COMPLETED ) await asyncio . gather ( * done ) try : await example coroutine ( client , args ) except asyncio . Cancelled Error : pass finally : await client . disconnect ( ) await task", "predictions": ["connect to a = 3 ."], "references": ["run the example coroutine ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1206, "code": "def main ( ) : parser = argparse . Argument Parser ( ) parser . add argument ( 'protofilepath' ) args = parser . parse args ( ) out file = compile protofile ( args . protofilepath ) with open ( out file , 'rb' ) as proto file : file descriptor set = descriptor pb2 . File Descriptor Set . From String ( proto file . read ( ) ) for file descriptor in file descriptor set . file : locations = { } for location in file descriptor . source code info . location : locations [ tuple ( location . path ) ] = location print ( make comment ( 'This file was automatically generated from {} and ' 'should not be edited directly.' . format ( args . protofilepath ) ) ) for index , message desc in enumerate ( file descriptor . message type ) : generate message doc ( message desc , locations , ( 4 , index ) ) for index , enum desc in enumerate ( file descriptor . enum type ) : generate enum doc ( enum desc , locations , ( 5 , index ) )", "predictions": ["parse arguments and generate"], "references": ["parse arguments and print generated documentation to stdout ."], "bleu": 0.19159730522949137, "rouge_l": 0.43160377358490565}
{"id": 1207, "code": "def dir maker ( path ) : directory = os . path . dirname ( path ) if directory != '' and not os . path . isdir ( directory ) : try : os . makedirs ( directory ) except OS Error as e : sys . exit ( 'Failed to create directory: {}' . format ( e ) )", "predictions": ["create a directory if it doesn t exist ."], "references": ["create a directory if it does not exist ."], "bleu": 0.5814307369682193, "rouge_l": 0.7777777777777778}
{"id": 1208, "code": "def exception handler ( self , loop , context ) : self . coroutine queue . put ( self . client . disconnect ( ) ) default exception = Exception ( context . get ( 'message' ) ) self . exception = context . get ( 'exception' , default exception )", "predictions": ["called when an def connection is closed . ."], "references": ["handle exceptions from the asyncio loop ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 1209, "code": "def input filter ( self , keys , ) : if keys == [ self . keys [ 'menu' ] ] : if self . urwid loop . widget == self . tabbed window : self . show menu ( ) else : self . hide menu ( ) elif keys == [ self . keys [ 'quit' ] ] : self . coroutine queue . put ( self . client . disconnect ( ) ) else : return keys", "predictions": ["disconnects up a send send send up up to the backend"], "references": ["handle global keybindings ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1210, "code": "def show menu ( self ) : current widget = self . tabbed window . get current widget ( ) if hasattr ( current widget , 'get menu widget' ) : menu widget = current widget . get menu widget ( self . hide menu ) overlay = urwid . Overlay ( menu widget , self . tabbed window , align = 'center' , width = ( 'relative' , 80 ) , valign = 'middle' , height = ( 'relative' , 80 ) ) self . urwid loop . widget = overlay", "predictions": ["def the current set set set the current set set set up the current set set set set set set set set set set set set set set set up the"], "references": ["show the overlay menu ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 1211, "code": "def get conv widget ( self , conv id ) : if conv id not in self . conv widgets : set title cb = ( lambda widget , title : self . tabbed window . set tab ( widget , title = title ) ) widget = Conversation Widget ( self . client , self . coroutine queue , self . conv list . get ( conv id ) , set title cb , self . keys , self . datetimefmt ) self . conv widgets [ conv id ] = widget return self . conv widgets [ conv id ]", "predictions": ["returns the conversation conversation conversation"], "references": ["return an existing or new conversationwidget ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1212, "code": "def add conversation tab ( self , conv id , switch = False ) : conv widget = self . get conv widget ( conv id ) self . tabbed window . set tab ( conv widget , switch = switch , title = conv widget . title )", "predictions": ["def to def ."], "references": ["add conversation tab if not present and optionally switch to it ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 1213, "code": "async def on connect ( self ) : self . user list , self . conv list = ( await hangups . build user conversation list ( self . client ) ) self . conv list . on event . add observer ( self . on event ) conv picker = Conversation Picker Widget ( self . conv list , self . on select conversation , self . keys ) self . tabbed window = Tabbed Window Widget ( self . keys ) self . tabbed window . set tab ( conv picker , switch = True , title = 'Conversations' ) self . urwid loop . widget = self . tabbed window", "predictions": ["special setup of urwid"], "references": ["handle connecting for the first time ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1214, "code": "def on event ( self , conv event ) : conv = self . conv list . get ( conv event . conversation id ) user = conv . get user ( conv event . user id ) show notification = all ( ( isinstance ( conv event , hangups . Chat Message Event ) , not user . is self , not conv . is quiet , ) ) if show notification : self . add conversation tab ( conv event . conversation id ) if self . discreet notifications : notification = DISCREET NOTIFICATION else : notification = notifier . Notification ( user . full name , get conv name ( conv ) , conv event . text ) self . notifier . send ( notification )", "predictions": ["handle an set notification response response"], "references": ["open conversation tab for new messages & pass events to notifier ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 1215, "code": "def put ( self , coro ) : assert asyncio . iscoroutine ( coro ) self . queue . put nowait ( coro )", "predictions": ["def a file to be invoked at the queue ."], "references": ["put a coroutine in the queue to be executed ."], "bleu": 0.22692039365038064, "rouge_l": 0.4}
{"id": 1216, "code": "async def consume ( self ) : while True : coro = await self . queue . get ( ) assert asyncio . iscoroutine ( coro ) await coro", "predictions": ["stop all messages from the queue ."], "references": ["consume coroutines from the queue by executing them ."], "bleu": 0.27470644934024185, "rouge_l": 0.48897795591182364}
{"id": 1217, "code": "def rename ( self , name , callback ) : self . coroutine queue . put ( self . conversation . rename ( name ) ) callback ( )", "predictions": ["rename a callback to a coroutine ."], "references": ["rename conversation and call callback ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 1218, "code": "def on event ( self , ) : self . sort ( key = lambda conv button : conv button . last modified , reverse = True )", "predictions": ["sort the event in the event button"], "references": ["re - order the conversations when an event occurs ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1219, "code": "def show message ( self , message str ) : if self . message handle is not None : self . message handle . cancel ( ) self . message handle = asyncio . get event loop ( ) . call later ( self . MESSAGE DELAY SECS , self . clear message ) self . message = message str self . update ( )", "predictions": ["show message on message"], "references": ["show a temporary message ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 1220, "code": "def on event ( self , conv event ) : if isinstance ( conv event , hangups . Chat Message Event ) : self . typing statuses [ conv event . user id ] = ( hangups . TYPING TYPE STOPPED ) self . update ( )", "predictions": ["handle conv event handler"], "references": ["make users stop typing when they send a message ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1221, "code": "def on typing ( self , typing message ) : self . typing statuses [ typing message . user id ] = typing message . status self . update ( )", "predictions": ["called when a user disconnects a message ."], "references": ["handle typing updates ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1222, "code": "def update ( self ) : typing users = [ self . conversation . get user ( user id ) for user id , status in self . typing statuses . items ( ) if status == hangups . TYPING TYPE STARTED ] displayed names = [ user . first name for user in typing users if not user . is self ] if displayed names : typing message = '{} {} typing...' . format ( ', ' . join ( sorted ( displayed names ) ) , 'is' if len ( displayed names ) == 1 else 'are' ) else : typing message = '' if not self . is connected : self . widget . set text ( \"RECONNECTING...\" ) elif self . message is not None : self . widget . set text ( self . message ) else : self . widget . set text ( typing message )", "predictions": ["update the typing widget ."], "references": ["update status text ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1223, "code": "def get date str ( timestamp , datetimefmt , show date = False ) : fmt = '' if show date : fmt += '\\n' + datetimefmt . get ( 'date' , '' ) + '\\n' fmt += datetimefmt . get ( 'time' , '' ) return timestamp . astimezone ( tz = None ) . strftime ( fmt )", "predictions": ["return a string representation of a date ."], "references": ["convert utc datetime into user interface string ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1224, "code": "async def load ( self ) : try : conv events = await self . conversation . get events ( self . conversation . events [ 0 ] . id ) except ( Index Error , hangups . Network Error ) : conv events = [ ] if not conv events : self . first loaded = True if self . focus position == self . POSITION LOADING and conv events : self . set focus ( conv events [ - 1 ] . id ) else : self . modified ( ) self . refresh watermarked events ( ) self . is loading = False", "predictions": ["load focus from database ."], "references": ["load more events for this conversation ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 1225, "code": "def set focus ( self , position ) : self . focus position = position self . modified ( ) try : self . next position ( position ) except Index Error : self . is scrolling = False else : self . is scrolling = True", "predictions": ["set focus in iteration of poll"], "references": ["set the focus to position or raise indexerror ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1226, "code": "def get menu widget ( self , close callback ) : return Conversation Menu ( self . coroutine queue , self . conversation , close callback , self . keys )", "predictions": ["returns the widget widget for the url of the coroutine ."], "references": ["return the menu widget associated with this widget ."], "bleu": 0.14323145079400493, "rouge_l": 0.4073455759599332}
{"id": 1227, "code": "def keypress ( self , size , key ) : self . coroutine queue . put ( self . client . set active ( ) ) self . coroutine queue . put ( self . conversation . update read timestamp ( ) ) return super ( ) . keypress ( size , key )", "predictions": ["trigger the conversation of the coroutine with the given key ."], "references": ["handle marking messages as read and keeping client active ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 1228, "code": "def set title ( self ) : self . title = get conv name ( self . conversation , show unread = True , truncate = True ) self . set title cb ( self , self . title )", "predictions": ["set screen title ."], "references": ["update this conversation s tab title ."], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 1229, "code": "def on return ( self , text ) : if not text : return elif text . startswith ( '/image' ) and len ( text . split ( ' ' ) ) == 2 : filename = text . split ( ' ' ) [ 1 ] image file = open ( filename , 'rb' ) text = '' else : image file = None text = replace emoticons ( text ) segments = hangups . Chat Message Segment . from str ( text ) self . coroutine queue . put ( self . handle send message ( self . conversation . send message ( segments , image file = image file ) ) )", "predictions": ["function called when a message is received"], "references": ["called when the user presses return on the send message widget ."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 1230, "code": "def update tabs ( self ) : text = [ ] for num , widget in enumerate ( self . widgets ) : palette = ( 'active tab' if num == self . tab index else 'inactive tab' ) text += [ ( palette , ' {} ' . format ( self . widget title [ widget ] ) ) , ( 'tab background' , ' ' ) , ] self . tabs . set text ( text ) self . frame . contents [ 'body' ] = ( self . widgets [ self . tab index ] , None )", "predictions": ["update the tabs tabs ."], "references": ["update tab display ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1231, "code": "def keypress ( self , size , key ) : key = super ( ) . keypress ( size , key ) num tabs = len ( self . widgets ) if key == self . keys [ 'prev tab' ] : self . tab index = ( self . tab index - 1 ) % num tabs self . update tabs ( ) elif key == self . keys [ 'next tab' ] : self . tab index = ( self . tab index + 1 ) % num tabs self . update tabs ( ) elif key == self . keys [ 'close tab' ] : if self . tab index > 0 : curr tab = self . widgets [ self . tab index ] self . widgets . remove ( curr tab ) del self . widget title [ curr tab ] self . tab index -= 1 self . update tabs ( ) else : return key", "predictions": ["internal method to create a tab key for a tab tab ."], "references": ["handle keypresses for changing tabs ."], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 1232, "code": "async def on push data ( self , data bytes ) : logger . debug ( 'Received chunk:\\n{}' . format ( data bytes ) ) for chunk in self . chunk parser . get chunks ( data bytes ) : if not self . is connected : if self . on connect called : self . is connected = True await self . on reconnect . fire ( ) else : self . on connect called = True self . is connected = True await self . on connect . fire ( ) container array = json . loads ( chunk ) for inner array in container array : array id , data array = inner array logger . debug ( 'Chunk contains data array with id %r:\\n%r' , array id , data array ) await self . on receive array . fire ( data array )", "predictions": ["handle push data from push"], "references": ["parse push data and trigger events ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 1233, "code": "def decode field ( message , field , value ) : if field . type == Field Descriptor . TYPE MESSAGE : decode ( getattr ( message , field . name ) , value ) else : try : if field . type == Field Descriptor . TYPE BYTES : value = base64 . b64decode ( value ) setattr ( message , field . name , value ) except ( Value Error , Type Error ) as e : logger . warning ( 'Message %r ignoring field %s: %s' , message . class . name , field . name , e )", "predictions": ["decode a field value"], "references": ["decode optional or required field ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 1234, "code": "def decode repeated field ( message , field , value list ) : if field . type == Field Descriptor . TYPE MESSAGE : for value in value list : decode ( getattr ( message , field . name ) . add ( ) , value ) else : try : for value in value list : if field . type == Field Descriptor . TYPE BYTES : value = base64 . b64decode ( value ) getattr ( message , field . name ) . append ( value ) except ( Value Error , Type Error ) as e : logger . warning ( 'Message %r ignoring repeated field %s: %s' , message . class . name , field . name , e ) message . Clear Field ( field . name )", "predictions": ["decode a repeated field ."], "references": ["decode repeated field ."], "bleu": 0.537284965911771, "rouge_l": 0.9070631970260222}
{"id": 1235, "code": "def remove exited dusty containers ( ) : client = get docker client ( ) exited containers = get exited dusty containers ( ) removed containers = [ ] for container in exited containers : log to client ( \"Removing container {}\" . format ( container [ 'Names' ] [ 0 ] ) ) try : client . remove container ( container [ 'Id' ] , v = True ) removed containers . append ( container ) except Exception as e : log to client ( e . message or str ( e ) ) return removed containers", "predictions": ["remove all exited containers that have been removed ."], "references": ["removed all dusty containers with exited in their status"], "bleu": 0.17747405280050263, "rouge_l": 0.2222222222222222}
{"id": 1236, "code": "def remove images ( ) : client = get docker client ( ) removed = remove dangling images ( ) dusty images = get dusty images ( ) all images = client . images ( all = True ) for image in all images : if set ( image [ 'Repo Tags' ] ) . intersection ( dusty images ) : try : client . remove image ( image [ 'Id' ] ) except Exception as e : logging . info ( \"Couldn't remove image {}\" . format ( image [ 'Repo Tags' ] ) ) else : log to client ( \"Removed Image {}\" . format ( image [ 'Repo Tags' ] ) ) removed . append ( image ) return removed", "predictions": ["remove any images that were created ."], "references": ["removes all dangling images as well as all images referenced in a dusty spec ; forceful removal is not used"], "bleu": 0.0289990174645553, "rouge_l": 0.0681564245810056}
{"id": 1237, "code": "def case insensitive rename ( src , dst ) : temp dir = tempfile . mkdtemp ( ) shutil . rmtree ( temp dir ) shutil . move ( src , temp dir ) shutil . move ( temp dir , dst )", "predictions": ["rename src to dst"], "references": ["a hack to allow us to rename paths in a case - insensitive filesystem like hfs ."], "bleu": 0.015417996259849322, "rouge_l": 0.08567415730337079}
{"id": 1238, "code": "def composed app dict ( app name , assembled specs , port specs ) : logging . info ( \"Compose Compiler: Compiling dict for app {}\" . format ( app name ) ) app spec = assembled specs [ 'apps' ] [ app name ] compose dict = app spec [ \"compose\" ] apply env overrides ( env overrides for app or service ( app name ) , compose dict ) if 'image' in app spec and 'build' in app spec : raise Runtime Error ( \"image and build are both specified in the spec for {}\" . format ( app name ) ) elif 'image' in app spec : logging . info compose dict [ 'image' ] = app spec [ 'image' ] elif 'build' in app spec : compose dict [ 'build' ] = get build path ( app spec ) else : raise Runtime Error ( \"Neither image nor build was specified in the spec for {}\" . format ( app name ) ) compose dict [ 'entrypoint' ] = [ ] compose dict [ 'command' ] = compile docker command ( app spec ) compose dict [ 'container name' ] = \"dusty {} 1\" . format ( app name ) logging . info ( \"Compose Compiler: compiled command {}\" . format ( compose dict [ 'command' ] ) ) compose dict [ 'links' ] = links for app ( app spec , assembled specs ) logging . info ( \"Compose Compiler: links {}\" . format ( compose dict [ 'links' ] ) ) compose dict [ 'volumes' ] = compose dict [ 'volumes' ] + get compose volumes ( app name , assembled specs ) logging . info ( \"Compose Compiler: volumes {}\" . format ( compose dict [ 'volumes' ] ) ) port list = get ports list ( app name , port specs ) if port list : compose dict [ 'ports' ] = port list logging . info ( \"Compose Compiler: ports {}\" . format ( port list ) ) compose dict [ 'user' ] = 'root' return compose dict", "predictions": ["given a application app return a dictionary of app ports ."], "references": ["this function returns a dictionary of the docker - compose . yml specifications for one app"], "bleu": 0.14892408222374315, "rouge_l": 0.28672150411280845}
{"id": 1239, "code": "def get ports list ( app name , port specs ) : if app name not in port specs [ 'docker compose' ] : return [ ] return [ \"{}:{}\" . format ( port spec [ 'mapped host port' ] , port spec [ 'in container port' ] ) for port spec in port specs [ 'docker compose' ] [ app name ] ]", "predictions": ["get a list of ports for the named app"], "references": ["returns a list of formatted port mappings for an app"], "bleu": 0.2601435417217584, "rouge_l": 0.5213675213675214}
{"id": 1240, "code": "def expand libs in apps ( specs ) : for app name , app spec in specs [ 'apps' ] . iteritems ( ) : if 'depends' in app spec and 'libs' in app spec [ 'depends' ] : app spec [ 'depends' ] [ 'libs' ] = get dependent ( 'libs' , app name , specs , 'apps' )", "predictions": ["expand libs to the list of apps in the given apps ."], "references": ["expands specs . apps . depends . libs to include any indirectly required libs"], "bleu": 0.1455496214451595, "rouge_l": 0.1517412935323383}
{"id": 1241, "code": "def expand libs in libs ( specs ) : for lib name , lib spec in specs [ 'libs' ] . iteritems ( ) : if 'depends' in lib spec and 'libs' in lib spec [ 'depends' ] : lib spec [ 'depends' ] [ 'libs' ] = get dependent ( 'libs' , lib name , specs , 'libs' )", "predictions": ["expand libs to the libs algorithm ."], "references": ["expands specs . libs . depends . libs to include any indirectly required libs"], "bleu": 0.10218289380194193, "rouge_l": 0.2695139911634757}
{"id": 1242, "code": "def get referenced libs ( specs ) : active libs = set ( ) for app spec in specs [ 'apps' ] . values ( ) : for lib in app spec [ 'depends' ] [ 'libs' ] : active libs . add ( lib ) return active libs", "predictions": ["return list of active libs libs libs libs ."], "references": ["returns all libs that are referenced in specs . apps . depends . libs"], "bleu": 0.09630141125179911, "rouge_l": 0.1673525377229081}
{"id": 1243, "code": "def nginx location spec ( port spec , bridge ip ) : location string spec = \"\\t \\t location / { \\n\" for location setting in [ 'proxy http version 1.1;' , 'proxy set header Upgrade $http upgrade;' , 'proxy set header Connection \"upgrade\";' , 'proxy set header X-Forwarded-For $proxy add x forwarded for;' , 'proxy set header Host $http host;' , nginx proxy string ( port spec , bridge ip ) ] : location string spec += \"\\t \\t \\t {} \\n\" . format ( location setting ) location string spec += \"\\t \\t } \\n\" return location string spec", "predictions": ["return a string to be used to return a location for a given port ."], "references": ["this will output the nginx location config string for specific port spec"], "bleu": 0.10343603005129705, "rouge_l": 0.22676579925650556}
{"id": 1244, "code": "def nginx http spec ( port spec , bridge ip ) : server string spec = \"\\t server {\\n\" server string spec += \"\\t \\t {}\\n\" . format ( nginx max file size string ( ) ) server string spec += \"\\t \\t {}\\n\" . format ( nginx listen string ( port spec ) ) server string spec += \"\\t \\t {}\\n\" . format ( nginx server name string ( port spec ) ) server string spec += nginx location spec ( port spec , bridge ip ) server string spec += custom 502 page ( ) server string spec += \"\\t }\\n\" return server string spec", "predictions": ["return the http nginx spec for a given port spec ."], "references": ["this will output the nginx http config string for specific port spec"], "bleu": 0.1691790501198734, "rouge_l": 0.4314002828854314}
{"id": 1245, "code": "def nginx stream spec ( port spec , bridge ip ) : server string spec = \"\\t server {\\n\" server string spec += \"\\t \\t {}\\n\" . format ( nginx listen string ( port spec ) ) server string spec += \"\\t \\t {}\\n\" . format ( nginx proxy string ( port spec , bridge ip ) ) server string spec += \"\\t }\\n\" return server string spec", "predictions": ["return the nginx stream spec for a given port spec ."], "references": ["this will output the nginx stream config string for specific port spec"], "bleu": 0.23925530714892257, "rouge_l": 0.5176803394625177}
{"id": 1246, "code": "def get lib volume mounts ( base lib name , assembled specs ) : volumes = [ get lib repo volume mount ( assembled specs [ 'libs' ] [ base lib name ] ) ] volumes . append ( get command files volume mount ( base lib name , test = True ) ) for lib name in assembled specs [ 'libs' ] [ base lib name ] [ 'depends' ] [ 'libs' ] : lib spec = assembled specs [ 'libs' ] [ lib name ] volumes . append ( get lib repo volume mount ( lib spec ) ) return volumes", "predictions": ["return list of assembled specs that have been mounts on the given base spec ."], "references": ["returns a list of the formatted volume specs for a lib"], "bleu": 0.12300686288463772, "rouge_l": 0.2373540856031128}
{"id": 1247, "code": "def get app libs volume mounts ( app name , assembled specs ) : volumes = [ ] for lib name in assembled specs [ 'apps' ] [ app name ] [ 'depends' ] [ 'libs' ] : lib spec = assembled specs [ 'libs' ] [ lib name ] volumes . append ( \"{}:{}\" . format ( Repo ( lib spec [ 'repo' ] ) . vm path , container code path ( lib spec ) ) ) return volumes", "predictions": ["return list of all assembled libs mounts"], "references": ["returns a list of the formatted volume mounts for all libs that an app uses"], "bleu": 0.0927110373244369, "rouge_l": 0.3412587412587413}
{"id": 1248, "code": "def init docker vm ( ) : if not dusty vm exists ( ) : log to client ( 'Initializing new Dusty VM with Docker Machine' ) machine options = [ '--driver' , 'virtualbox' , '--virtualbox-cpu-count' , '-1' , '--virtualbox-boot2docker-url' , constants . CONFIG BOOT2DOCKER URL , '--virtualbox-memory' , str ( get config value ( constants . CONFIG VM MEM SIZE ) ) , '--virtualbox-hostonly-nictype' , constants . VM NIC TYPE ] check call demoted ( [ 'docker-machine' , 'create' ] + machine options + [ constants . VM MACHINE NAME ] , redirect stderr = True )", "predictions": ["initialize consume consume vm vm . ."], "references": ["initialize the dusty vm if it does not already exist ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 1249, "code": "def start docker vm ( ) : is running = docker vm is running ( ) if not is running : log to client ( 'Starting docker-machine VM {}' . format ( constants . VM MACHINE NAME ) ) apply nat dns host resolver ( ) apply nat net less greedy subnet ( ) check and log output and error demoted ( [ 'docker-machine' , 'start' , constants . VM MACHINE NAME ] , quiet on success = True ) return is running", "predictions": ["rename docker self . . to a docker self . ."], "references": ["start the dusty vm if it is not already running ."], "bleu": 0.11390778025531027, "rouge_l": 0.09090909090909091}
{"id": 1250, "code": "def docker vm is running ( ) : running vms = check output demoted ( [ 'V Box Manage' , 'list' , 'runningvms' ] ) for line in running vms . splitlines ( ) : if '\"{}\"' . format ( constants . VM MACHINE NAME ) in line : return True return False", "predictions": ["check if the on the on the event event is self . . . . . ."], "references": ["using vboxmanage is 0 . 5 seconds or so faster than machine ."], "bleu": 0.0859076483566362, "rouge_l": 0.20492721164613661}
{"id": 1251, "code": "def create cookie ( host , path , secure , expires , name , value ) : return http . cookiejar . Cookie ( 0 , name , value , None , False , host , host . startswith ( '.' ) , host . startswith ( '.' ) , path , True , secure , expires , False , None , None , { } )", "predictions": ["show a message to create a message"], "references": ["shortcut function to create a cookie"], "bleu": 0.345720784641941, "rouge_l": 0.4680306905370844}
{"id": 1252, "code": "def load ( self ) : con = sqlite3 . connect ( self . tmp cookie file ) cur = con . cursor ( ) try : cur . execute ( 'SELECT host key, path, secure, expires utc, name, value, encrypted value ' 'FROM cookies WHERE host key like \"%{}%\";' . format ( self . domain name ) ) except sqlite3 . Operational Error : cur . execute ( 'SELECT host key, path, is secure, expires utc, name, value, encrypted value ' 'FROM cookies WHERE host key like \"%{}%\";' . format ( self . domain name ) ) cj = http . cookiejar . Cookie Jar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . decrypt ( item [ 5 ] , item [ 6 ] ) c = create cookie ( host , path , secure , expires , name , value ) cj . set cookie ( c ) con . close ( ) return cj", "predictions": ["on the server on the remote server if available"], "references": ["load sqlite cookies into a cookiejar"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1253, "code": "def ib64 patched ( self , attrs D , contentparams ) : if attrs D . get ( \"mode\" , \"\" ) == \"base64\" : return 0 if self . contentparams [ \"type\" ] . startswith ( \"text/\" ) : return 0 if self . contentparams [ \"type\" ] . endswith ( \"+xml\" ) : return 0 if self . contentparams [ \"type\" ] . endswith ( \"/xml\" ) : return 0 if self . contentparams [ \"type\" ] . endswith ( \"/json\" ) : return 0 return 0", "predictions": ["on the element end of the element"], "references": ["patch isbase64 to prevent base64 encoding of json content"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1254, "code": "def cleanwrap ( func ) : def enc ( self , * args , * * kwargs ) : \"\"\" Send each item to  cleanup() \"\"\" return ( func ( self , item , * * kwargs ) for item in args ) return enc", "predictions": ["decorator that take a function and call each function"], "references": ["wrapper for zotero . _cleanup"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1255, "code": "def ss wrap ( func ) : def wrapper ( self , * args , * * kwargs ) : if not self . savedsearch : self . savedsearch = Saved Search ( self ) return func ( self , * args , * * kwargs ) return wrapper", "predictions": ["decorator to date a function as a savedsearch if not already get the savedsearch if it s not already ."], "references": ["ensure that a savedsearch object exists"], "bleu": 0.08039313477786734, "rouge_l": 0.17039106145251398}
{"id": 1256, "code": "def error handler ( req ) : error codes = { 400 : ze . Unsupported Params , 401 : ze . User Not Authorised , 403 : ze . User Not Authorised , 404 : ze . Resource Not Found , 409 : ze . Conflict , 412 : ze . Pre Condition Failed , 413 : ze . Request Entity Too Large , 428 : ze . Pre Condition Required , 429 : ze . Too Many Requests , } def err msg ( req ) : return \"\\n Code: %s\\n URL: %s\\n Method: %s\\n Response: %s\" % ( req . status code , req . url , req . request . method , req . text , ) if error codes . get ( req . status code ) : if req . status code == 429 : delay = backoff . delay if delay > 32 : backoff . reset ( ) raise ze . Too Many Retries ( ) time . sleep ( delay ) sess = requests . Session ( ) new req = sess . send ( req . request ) try : new req . raise for status ( ) except requests . exceptions . HTTP Error : error handler ( new req ) else : raise error codes . get ( req . status code ) ( err msg ( req ) ) else : raise ze . HTTP Error ( err msg ( req ) )", "predictions": ["handle http request ."], "references": ["error handler for http requests"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1257, "code": "def default headers ( self ) : headers = { \"User-Agent\" : \"Pyzotero/%s\" % version , \"Zotero-API-Version\" : \"%s\" % api version , } if self . api key : headers [ \"Authorization\" ] = \"Bearer %s\" % self . api key return headers", "predictions": ["add set of set of focus to the api . . . . . ."], "references": ["it s always ok to include these headers"], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 1258, "code": "def cleanup ( self , to clean , allow = ( ) ) : if to clean . keys ( ) == [ \"links\" , \"library\" , \"version\" , \"meta\" , \"key\" , \"data\" ] : to clean = to clean [ \"data\" ] return dict ( [ [ k , v ] for k , v in list ( to clean . items ( ) ) if ( k in allow or k not in self . temp keys ) ] )", "predictions": ["remove empty dicts with empty keys ."], "references": ["remove keys we added for internal use"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1259, "code": "def extract links ( self ) : extracted = dict ( ) try : for key , value in self . request . links . items ( ) : parsed = urlparse ( value [ \"url\" ] ) fragment = \"{path}?{query}\" . format ( path = parsed [ 2 ] , query = parsed [ 4 ] ) extracted [ key ] = fragment parsed = list ( urlparse ( self . self link ) ) stripped = \"&\" . join ( [ \"%s=%s\" % ( p [ 0 ] , p [ 1 ] ) for p in parse qsl ( parsed [ 4 ] ) if p [ 0 ] != \"format\" ] ) extracted [ \"self\" ] = urlunparse ( [ parsed [ 0 ] , parsed [ 1 ] , parsed [ 2 ] , parsed [ 3 ] , stripped , parsed [ 5 ] ] ) return extracted except Key Error : return None", "predictions": ["keypress links and parse links"], "references": ["extract self first next last links from a request response"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 1260, "code": "def publications ( self ) : if self . library type != \"users\" : raise ze . Call Does Not Exist ( \"This API call does not exist for group libraries\" ) query string = \"/{t}/{u}/publications/items\" return self . build query ( query string )", "predictions": ["get all set of set of set set set of set set set ."], "references": ["return the contents of my publications"], "bleu": 0.08839374326825923, "rouge_l": 0.10777385159010601}
{"id": 1261, "code": "def num collectionitems ( self , collection ) : query = \"/{t}/{u}/collections/{c}/items\" . format ( u = self . library id , t = self . library type , c = collection . upper ( ) ) return self . totals ( query )", "predictions": ["get the number of sessions in a and and get the number of and the number of non - and return the number of values in a and values in a"], "references": ["return the total number of items in the specified collection"], "bleu": 0.0702339497415279, "rouge_l": 0.26872246696035246}
{"id": 1262, "code": "def num tagitems ( self , tag ) : query = \"/{t}/{u}/tags/{ta}/items\" . format ( u = self . library id , t = self . library type , ta = tag ) return self . totals ( query )", "predictions": ["get the number of in a in a in a in a in a in a in a in a in a in a in a in a in a in"], "references": ["return the total number of items for the specified tag"], "bleu": 0.055177848898164926, "rouge_l": 0.16123348017621145}
{"id": 1263, "code": "def totals ( self , query ) : self . add parameters ( limit = 1 ) query = self . build query ( query ) self . retrieve data ( query ) self . url params = None return int ( self . request . headers [ \"Total-Results\" ] )", "predictions": ["retrieve a size of the size of the size of the size of the size of the size ."], "references": ["general method for returning total counts"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 1264, "code": "def fulltext item ( self , itemkey , * * kwargs ) : query string = \"/{t}/{u}/items/{itemkey}/fulltext\" . format ( t = self . library type , u = self . library id , itemkey = itemkey ) return self . build query ( query string )", "predictions": ["executes an on a chunk"], "references": ["get full - text content for an item"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1265, "code": "def last modified version ( self , * * kwargs ) : self . items ( * * kwargs ) return int ( self . request . headers . get ( \"last-modified-version\" , 0 ) )", "predictions": ["return for the decode version type type type type type type type type type type type type type type type type type type type type type type type type type type"], "references": ["get the last modified version"], "bleu": 0.04317900023606586, "rouge_l": 0.12774869109947642}
{"id": 1266, "code": "def file ( self , item , * * kwargs ) : query string = \"/{t}/{u}/items/{i}/file\" . format ( u = self . library id , t = self . library type , i = item . upper ( ) ) return self . build query ( query string , no params = True )", "predictions": ["returns a decode query"], "references": ["get the file from an specific item"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1267, "code": "def dump ( self , itemkey , filename = None , path = None ) : if not filename : filename = self . item ( itemkey ) [ \"data\" ] [ \"filename\" ] if path : pth = os . path . join ( path , filename ) else : pth = filename file = self . file ( itemkey ) if self . snapshot : self . snapshot = False pth = pth + \".zip\" with open ( pth , \"wb\" ) as f : f . write ( file )", "predictions": ["remove the 0 - formatted 0 - formatted 0 - d 0 - 1"], "references": ["dump a file attachment to disk with optional filename and path"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 1268, "code": "def collections sub ( self , collection , * * kwargs ) : query string = \"/{t}/{u}/collections/{c}/collections\" . format ( u = self . library id , t = self . library type , c = collection . upper ( ) ) return self . build query ( query string )", "predictions": ["returns a collection of remove remove the dangling from the given client ."], "references": ["get subcollections for a specific collection"], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 1269, "code": "def json processor ( self , retrieved ) : json kwargs = { } if self . preserve json order : json kwargs [ \"object pairs hook\" ] = Ordered Dict try : items = [ json . loads ( e [ \"content\" ] [ 0 ] [ \"value\" ] , * * json kwargs ) for e in retrieved . entries ] except Key Error : return self . tags data ( retrieved ) return items", "predictions": ["get insensitive insensitive insensitive"], "references": ["format and return data from api calls which return items"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1270, "code": "def csljson processor ( self , retrieved ) : items = [ ] json kwargs = { } if self . preserve json order : json kwargs [ \"object pairs hook\" ] = Ordered Dict for csl in retrieved . entries : items . append ( json . loads ( csl [ \"content\" ] [ 0 ] [ \"value\" ] , * * json kwargs ) ) self . url params = None return items", "predictions": ["add app data to api"], "references": ["return a list of dicts which are dumped csl json"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 1271, "code": "def bib processor ( self , retrieved ) : items = [ ] for bib in retrieved . entries : items . append ( bib [ \"content\" ] [ 0 ] [ \"value\" ] ) self . url params = None return items", "predictions": ["add a list of get ports to the get parameter"], "references": ["return a list of strings formatted as html bibliography entries"], "bleu": 0.23462350320528, "rouge_l": 0.3}
{"id": 1272, "code": "def citation processor ( self , retrieved ) : items = [ ] for cit in retrieved . entries : items . append ( cit [ \"content\" ] [ 0 ] [ \"value\" ] ) self . url params = None return items", "predictions": ["libs libs to include a expand"], "references": ["return a list of strings formatted as html citation entries"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1273, "code": "def item template ( self , itemtype ) : template name = \"item template \" + itemtype query string = \"/items/new?item Type={i}\" . format ( i = itemtype ) if self . templates . get ( template name ) and not self . updated ( query string , self . templates [ template name ] , template name ) : return copy . deepcopy ( self . templates [ template name ] [ \"tmplt\" ] ) retrieved = self . retrieve data ( query string ) return self . cache ( retrieved , template name )", "predictions": ["retrieve the data libs for a libs expand if needed ."], "references": ["get a template for a new item"], "bleu": 0.14991106946711685, "rouge_l": 0.2314990512333966}
{"id": 1274, "code": "def show condition operators ( self , condition ) : permitted operators = self . savedsearch . conditions operators . get ( condition ) permitted operators list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted operators ] ) return permitted operators list", "predictions": ["get the referenced libs libs"], "references": ["show available operators for a given saved search condition"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 1275, "code": "def fields types ( self , tname , qstring , itemtype ) : template name = tname + itemtype query string = qstring . format ( i = itemtype ) if self . templates . get ( template name ) and not self . updated ( query string , self . templates [ template name ] , template name ) : return self . templates [ template name ] [ \"tmplt\" ] retrieved = self . retrieve data ( query string ) return self . cache ( retrieved , template name )", "predictions": ["retrieve nginx nginx location location ."], "references": ["retrieve item fields or creator types"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1276, "code": "def item fields ( self ) : if self . templates . get ( \"item fields\" ) and not self . updated ( \"/item Fields\" , self . templates [ \"item fields\" ] , \"item fields\" ) : return self . templates [ \"item fields\" ] [ \"tmplt\" ] query string = \"/item Fields\" retrieved = self . retrieve data ( query string ) return self . cache ( retrieved , \"item fields\" )", "predictions": ["get all nginx http http http http http http http http http http http http http http http http http http http http http http http http http http http http"], "references": ["get all available item fields"], "bleu": 0.0513487742994337, "rouge_l": 0.12774869109947642}
{"id": 1277, "code": "def validate ( self , conditions ) : allowed keys = set ( self . searchkeys ) operators set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed keys : raise ze . Param Not Passed ( \"Keys must be all of: %s\" % \", \" . join ( self . searchkeys ) ) if condition . get ( \"operator\" ) not in operators set : raise ze . Param Not Passed ( \"You have specified an unknown operator: %s\" % condition . get ( \"operator\" ) ) permitted operators = self . conditions operators . get ( condition . get ( \"condition\" ) ) permitted operators list = set ( [ self . operators . get ( op ) for op in permitted operators ] ) if condition . get ( \"operator\" ) not in permitted operators list : raise ze . Param Not Passed ( \"You may not use the '%s' operator when selecting the '%s' condition. \\n Allowed operators: %s\" % ( condition . get ( \"operator\" ) , condition . get ( \"condition\" ) , \", \" . join ( list ( permitted operators list ) ) , ) )", "predictions": ["nginx check that the port is valid"], "references": ["validate saved search conditions raising an error if any contain invalid operators"], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 1278, "code": "def which ( program , win allow cross arch = True ) : def is exe ( path ) : return os . path . isfile ( path ) and os . access ( path , os . X OK ) def get path list ( ) : return os . environ [ 'PATH' ] . split ( os . pathsep ) if os . name == 'nt' : def find exe ( program ) : root , ext = os . path . splitext ( program ) if ext : if is exe ( program ) : return program else : for ext in os . environ [ 'PATHEXT' ] . split ( os . pathsep ) : program path = root + ext . lower ( ) if is exe ( program path ) : return program path return None def get path list ( ) : paths = get path list ( ) if win allow cross arch : alt sys path = os . path . expandvars ( r\"$WINDIR\\Sysnative\" ) if os . path . isdir ( alt sys path ) : paths . insert ( 0 , alt sys path ) else : alt sys path = os . path . expandvars ( r\"$WINDIR\\Sys WOW64\" ) if os . path . isdir ( alt sys path ) : paths . append ( alt sys path ) return paths else : def find exe ( program ) : return program if is exe ( program ) else None get path list = get path list if os . path . split ( program ) [ 0 ] : program path = find exe ( program ) if program path : return program path else : for path in get path list ( ) : program path = find exe ( os . path . join ( path , program ) ) if program path : return program path return None", "predictions": ["repo a volume of a volume . . ."], "references": ["identify the location of an executable file ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 1279, "code": "def split multiline ( value ) : return [ element for element in ( line . strip ( ) for line in value . split ( '\\n' ) ) if element ]", "predictions": ["strips app characters from a string volumes volumes volumes volumes volumes volumes volumes volumes volumes volumes volumes volumes ."], "references": ["split a multiline string into a list excluding blank lines ."], "bleu": 0.07658412276041004, "rouge_l": 0.21010332950631458}
{"id": 1280, "code": "def split elements ( value ) : items = [ v . strip ( ) for v in value . split ( ',' ) ] if len ( items ) == 1 : items = value . split ( ) return items", "predictions": ["split a value into a list of elements ."], "references": ["split a string with comma or space - separated elements into a list ."], "bleu": 0.19260282250359823, "rouge_l": 0.5020576131687242}
{"id": 1281, "code": "def eval environ ( value ) : def eval environ str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( \"^((\\\\w+(\\\\.\\\\w+)?|'.*?'|\\\".*?\\\")\\\\s+\" '(in|==|!=|not in)\\\\s+' \"(\\\\w+(\\\\.\\\\w+)?|'.*?'|\\\".*?\\\")\" '(\\\\s+(or|and)\\\\s+)?)+$' , expr ) : raise Value Error ( 'bad environment marker: %r' % expr ) expr = re . sub ( r\"(platform\\.\\w+)\" , r\"\\1()\" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new value = [ ] for element in value : element = eval environ str ( element ) if element : new value . append ( element ) elif isinstance ( value , str ) : new value = eval environ str ( value ) else : new value = value return new value", "predictions": ["eval a environ value into a dictionary"], "references": ["evaluate environment markers ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1282, "code": "def get cfg value ( config , section , option ) : try : value = config [ section ] [ option ] except Key Error : if ( section , option ) in MULTI OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI OPTIONS : value = split multiline ( value ) if ( section , option ) in ENVIRON OPTIONS : value = eval environ ( value ) return value", "predictions": ["get value from config file"], "references": ["get configuration value ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1283, "code": "def set cfg value ( config , section , option , value ) : if isinstance ( value , list ) : value = '\\n' . join ( value ) config [ section ] [ option ] = value", "predictions": ["set a configuration value"], "references": ["set configuration value ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 1284, "code": "def cfg to args ( config ) : kwargs = { } opts to args = { 'metadata' : [ ( 'name' , 'name' ) , ( 'author' , 'author' ) , ( 'author-email' , 'author email' ) , ( 'maintainer' , 'maintainer' ) , ( 'maintainer-email' , 'maintainer email' ) , ( 'home-page' , 'url' ) , ( 'summary' , 'description' ) , ( 'description' , 'long description' ) , ( 'download-url' , 'download url' ) , ( 'classifier' , 'classifiers' ) , ( 'platform' , 'platforms' ) , ( 'license' , 'license' ) , ( 'keywords' , 'keywords' ) , ] , 'files' : [ ( 'packages root' , 'package dir' ) , ( 'packages' , 'packages' ) , ( 'modules' , 'py modules' ) , ( 'scripts' , 'scripts' ) , ( 'package data' , 'package data' ) , ( 'data files' , 'data files' ) , ] , } opts to args [ 'metadata' ] . append ( ( 'requires-dist' , 'install requires' ) ) if IS PY2K and not which ( '3to2' ) : kwargs [ 'setup requires' ] = [ '3to2' ] kwargs [ 'zip safe' ] = False for section in opts to args : for option , argname in opts to args [ section ] : value = get cfg value ( config , section , option ) if value : kwargs [ argname ] = value if 'long description' not in kwargs : kwargs [ 'long description' ] = read description file ( config ) if 'package dir' in kwargs : kwargs [ 'package dir' ] = { '' : kwargs [ 'package dir' ] } if 'keywords' in kwargs : kwargs [ 'keywords' ] = split elements ( kwargs [ 'keywords' ] ) if 'package data' in kwargs : kwargs [ 'package data' ] = get package data ( kwargs [ 'package data' ] ) if 'data files' in kwargs : kwargs [ 'data files' ] = get data files ( kwargs [ 'data files' ] ) kwargs [ 'version' ] = get version ( ) if not IS PY2K : kwargs [ 'test suite' ] = 'test' return kwargs", "predictions": ["convert options to args and user options ."], "references": ["compatibility helper to use setup . cfg in setup . py ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 1285, "code": "def run 3to2 ( args = None ) : args = BASE ARGS 3TO2 if args is None else BASE ARGS 3TO2 + args try : proc = subprocess . Popen ( [ '3to2' ] + args , stderr = subprocess . PIPE ) except OS Error : for path in glob . glob ( '*.egg' ) : if os . path . isdir ( path ) and path not in sys . path : sys . path . append ( path ) try : from lib3to2 . main import main as lib3to2 main except Import Error : raise OS Error ( '3to2 script is unavailable.' ) else : if lib3to2 main ( 'lib3to2.fixes' , args ) : raise Exception ( 'lib3to2 parsing error' ) else : num errors = 0 while proc . poll ( ) is None : line = proc . stderr . readline ( ) sys . stderr . write ( line ) num errors += line . count ( ': Parse Error: ' ) if proc . returncode or num errors : raise Exception ( 'lib3to2 parsing error' )", "predictions": ["run 3to2 in a 3to2"], "references": ["convert python files using lib3to2 ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 1286, "code": "def write py2k header ( file list ) : if not isinstance ( file list , list ) : file list = [ file list ] python re = re . compile ( br\"^(#!.*\\bpython)(.*)([\\r\\n]+)$\" ) coding re = re . compile ( br\"coding[:=]\\s*([-\\w.]+)\" ) new line re = re . compile ( br\"([\\r\\n]+)$\" ) version 3 = Loose Version ( '3' ) for file in file list : if not os . path . getsize ( file ) : continue rewrite needed = False python found = False coding found = False lines = [ ] f = open ( file , 'rb' ) try : while len ( lines ) < 2 : line = f . readline ( ) match = python re . match ( line ) if match : python found = True version = Loose Version ( match . group ( 2 ) . decode ( ) or '2' ) try : version test = version >= version 3 except Type Error : version test = True if version test : line = python re . sub ( br\"\\g<1>2\\g<3>\" , line ) rewrite needed = True elif coding re . search ( line ) : coding found = True lines . append ( line ) if not coding found : match = new line re . search ( lines [ 0 ] ) newline = match . group ( 1 ) if match else b\"\\n\" line = + newline lines . insert ( 1 if python found else 0 , line ) rewrite needed = True if rewrite needed : lines += f . readlines ( ) finally : f . close ( ) if rewrite needed : f = open ( file , 'wb' ) try : f . writelines ( lines ) finally : f . close ( )", "predictions": ["rewrite the py2k header to the given file"], "references": ["write python 2 shebang and add encoding cookie if needed ."], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 1287, "code": "def which ( program ) : if os . path . split ( program ) [ 0 ] : program path = find exe ( program ) if program path : return program path else : for path in get path list ( ) : program path = find exe ( os . path . join ( path , program ) ) if program path : return program path return None", "predictions": ["return a list of paths to a program ."], "references": ["identify the location of an executable file ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 1288, "code": "def correct ( text : str , matches : [ Match ] ) -> str : ltext = list ( text ) matches = [ match for match in matches if match . replacements ] errors = [ ltext [ match . offset : match . offset + match . errorlength ] for match in matches ] correct offset = 0 for n , match in enumerate ( matches ) : frompos , topos = ( correct offset + match . offset , correct offset + match . offset + match . errorlength ) if ltext [ frompos : topos ] != errors [ n ] : continue repl = match . replacements [ 0 ] ltext [ frompos : topos ] = list ( repl ) correct offset += len ( repl ) - len ( errors [ n ] ) return '' . join ( ltext )", "predictions": ["correct the text with the given matches ."], "references": ["automatically apply suggestions to the text ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 1289, "code": "def get version ( ) : version = get attrib ( ) . get ( 'version' ) if not version : match = re . search ( r\"Language Tool-?.*?(\\S+)$\" , get directory ( ) ) if match : version = match . group ( 1 ) return version", "predictions": ["extract the version number from the installed installed version"], "references": ["get languagetool version ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 1290, "code": "def get languages ( ) -> set : try : languages = cache [ 'languages' ] except Key Error : languages = Language Tool . get languages ( ) cache [ 'languages' ] = languages return languages", "predictions": ["get list of all languages ."], "references": ["get supported languages ."], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 1291, "code": "def get directory ( ) : try : language check dir = cache [ 'language check dir' ] except Key Error : def version key ( string ) : return [ int ( e ) if e . isdigit ( ) else e for e in re . split ( r\"(\\d+)\" , string ) ] def get lt dir ( base dir ) : paths = [ path for path in glob . glob ( os . path . join ( base dir , 'Language Tool*' ) ) if os . path . isdir ( path ) ] return max ( paths , key = version key ) if paths else None base dir = os . path . dirname ( sys . argv [ 0 ] ) language check dir = get lt dir ( base dir ) if not language check dir : try : base dir = os . path . dirname ( os . path . abspath ( file ) ) except Name Error : pass else : language check dir = get lt dir ( base dir ) if not language check dir : raise Path Error ( \"can't find Language Tool directory in {!r}\" . format ( base dir ) ) cache [ 'language check dir' ] = language check dir return language check dir", "predictions": ["return a list of the directory names and version of the current version ."], "references": ["get languagetool directory ."], "bleu": 0.09782375748961449, "rouge_l": 0.2469635627530364}
{"id": 1292, "code": "def set directory ( path = None ) : old path = get directory ( ) terminate server ( ) cache . clear ( ) if path : cache [ 'language check dir' ] = path try : get jar info ( ) except Error : cache [ 'language check dir' ] = old path raise", "predictions": ["terminate the cached server"], "references": ["set languagetool directory ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1293, "code": "def check ( self , text : str , srctext = None ) -> [ Match ] : root = self . get root ( self . url , self . encode ( text , srctext ) ) return [ Match ( e . attrib ) for e in root if e . tag == 'error' ]", "predictions": ["check the text ."], "references": ["match text against enabled rules ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 1294, "code": "def correct ( self , text : str , srctext = None ) -> str : return correct ( text , self . check ( text , srctext ) )", "predictions": ["evaluate given text with given text ."], "references": ["automatically apply suggestions to the text ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 1295, "code": "def get common prefix ( z ) : name list = z . namelist ( ) if name list and all ( n . startswith ( name list [ 0 ] ) for n in name list [ 1 : ] ) : return name list [ 0 ] return None", "predictions": ["get all common prefix of a given z ."], "references": ["get common directory in a zip file if any ."], "bleu": 0.15881076016027915, "rouge_l": 0.41709401709401706}
{"id": 1296, "code": "def process events ( self , events ) : for f , callback , transferred , key , ov in events : try : self . logger . debug ( 'Invoking event callback {}' . format ( callback ) ) value = callback ( transferred , key , ov ) except OS Error : self . logger . warning ( 'Event callback failed' , exc info = sys . exc info ( ) ) else : f . set result ( value )", "predictions": ["processes a list of events and calls the event function ."], "references": ["process events from proactor ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 1297, "code": "def async Close ( fn ) : @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : f = asyncio . ensure future ( fn ( * args , * * kwargs ) ) while not f . done ( ) : Q Application . instance ( ) . process Events ( ) return wrapper", "predictions": ["decorator that logs the given process and creates the result ."], "references": ["allow to run async code before application is closed ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 1298, "code": "def async Slot ( * args ) : def outer decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : asyncio . ensure future ( fn ( * args , * * kwargs ) ) return wrapper return outer decorator", "predictions": ["decorator that ensures that a function is called only one of the result ."], "references": ["make a qt async slot run on asyncio loop ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 1299, "code": "def with logger ( cls ) : attr name = ' logger' cls name = cls . qualname module = cls . module if module is not None : cls name = module + '.' + cls name else : raise Assertion Error setattr ( cls , attr name , logging . get Logger ( cls name ) ) return cls", "predictions": ["decorator to return an class that allows to be used as a class ."], "references": ["class decorator to add a logger to a class ."], "bleu": 0.2102369368326755, "rouge_l": 0.5154929577464789}
{"id": 1300, "code": "def process event ( self , key , mask ) : self . logger . debug ( 'Processing event with key {} and mask {}' . format ( key , mask ) ) fileobj , ( reader , writer ) = key . fileobj , key . data if mask & selectors . EVENT READ and reader is not None : if reader . cancelled : self . remove reader ( fileobj ) else : self . logger . debug ( 'Invoking reader callback: {}' . format ( reader ) ) reader . run ( ) if mask & selectors . EVENT WRITE and writer is not None : if writer . cancelled : self . remove writer ( fileobj ) else : self . logger . debug ( 'Invoking writer callback: {}' . format ( writer ) ) writer . run ( )", "predictions": ["send a mask to the event queue ."], "references": ["selector has delivered us an event ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1301, "code": "def setup ( app ) : app . add config value ( 'plot gallery' , True , 'html' ) app . add config value ( 'abort on example error' , False , 'html' ) app . add config value ( 'sphinx gallery conf' , gallery conf , 'html' ) app . add stylesheet ( 'gallery.css' ) app . connect ( 'builder-inited' , generate gallery rst ) app . connect ( 'build-finished' , embed code links )", "predictions": ["setup sphinx extension ."], "references": ["setup sphinx - gallery sphinx extension"], "bleu": 0.34107725495137897, "rouge_l": 0.5791139240506329}
{"id": 1302, "code": "def twosided 2 centerdc ( data ) : N = len ( data ) newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd", "predictions": ["calculates the weighted standard deviation of a data matrix"], "references": ["convert a two - sided psd to a center - dc psd"], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 1303, "code": "def centerdc 2 twosided ( data ) : N = len ( data ) newpsd = np . concatenate ( ( data [ N // 2 : ] , ( cshift ( data [ 0 : N // 2 ] , - 1 ) ) ) ) return newpsd", "predictions": ["calculates the weighted - length - 2 - 2 - 2 - 2 - bit 2 - d array"], "references": ["convert a center - dc psd to a twosided psd"], "bleu": 0.06439931429457924, "rouge_l": 0.07305389221556886}
{"id": 1304, "code": "def data two freqs ( N = 200 ) : nn = arange ( N ) xx = cos ( 0.257 * pi * nn ) + sin ( 0.2 * pi * nn ) + 0.01 * randn ( nn . size ) return xx", "predictions": ["returns the data that will be two dimension ."], "references": ["a simple test example with two close frequencies"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 1305, "code": "def spectrum data ( filename ) : import os import pkg resources info = pkg resources . get distribution ( 'spectrum' ) location = info . location share = os . sep . join ( [ location , \"spectrum\" , 'data' ] ) filename2 = os . sep . join ( [ share , filename ] ) if os . path . exists ( filename2 ) : return filename2 else : raise Exception ( 'unknown file %s' % filename2 )", "predictions": ["return spectrum data as a dictionary"], "references": ["simple utilities to retrieve data sets from"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1306, "code": "def remove bias ( x , axis ) : padded slice = [ slice ( d ) for d in x . shape ] padded slice [ axis ] = np . newaxis mn = np . mean ( x , axis = axis ) return x - mn [ tuple ( padded slice ) ]", "predictions": ["remove bias with bias"], "references": ["subtracts an estimate of the mean from signal x at axis"], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 1307, "code": "def codestr2rst ( codestr , lang = 'python' ) : code directive = \"\\n.. code-block:: {0}\\n\\n\" . format ( lang ) indented block = indent ( codestr , ' ' * 4 ) return code directive + indented block", "predictions": ["convert a code into a code - friendly code"], "references": ["return restructuredtext code block from code string"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 1308, "code": "def get md5sum ( src file ) : with open ( src file , 'r' ) as src data : src content = src data . read ( ) if sys . version info [ 0 ] == 3 : src content = src content . encode ( 'utf-8' ) src md5 = hashlib . md5 ( src content ) . hexdigest ( ) return src md5", "predictions": ["get the md5sum of a file"], "references": ["returns md5sum of file"], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 1309, "code": "def check md5sum change ( src file ) : src md5 = get md5sum ( src file ) src md5 file = src file + '.md5' src file changed = True if os . path . exists ( src md5 file ) : with open ( src md5 file , 'r' ) as file checksum : ref md5 = file checksum . read ( ) if src md5 == ref md5 : src file changed = False if src file changed : with open ( src md5 file , 'w' ) as file checksum : file checksum . write ( src md5 ) return src file changed", "predictions": ["check if the md5sum has changed ."], "references": ["returns true if src_file has a different md5sum"], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 1310, "code": "def save thumbnail ( image path , base image name , gallery conf ) : first image file = image path . format ( 1 ) thumb dir = os . path . join ( os . path . dirname ( first image file ) , 'thumb' ) if not os . path . exists ( thumb dir ) : os . makedirs ( thumb dir ) thumb file = os . path . join ( thumb dir , 'sphx glr %s thumb.png' % base image name ) if os . path . exists ( first image file ) : scale image ( first image file , thumb file , 400 , 280 ) elif not os . path . exists ( thumb file ) : default thumb file = os . path . join ( glr path static ( ) , 'no image.png' ) default thumb file = gallery conf . get ( \"default thumb file\" , default thumb file ) scale image ( default thumb file , thumb file , 200 , 140 )", "predictions": ["save thumbnail image to default image"], "references": ["save the thumbnail image"], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 1311, "code": "def generate dir rst ( src dir , target dir , gallery conf , seen backrefs ) : if not os . path . exists ( os . path . join ( src dir , 'README.txt' ) ) : print ( 80 * ' ' ) print ( 'Example directory %s does not have a README.txt file' % src dir ) print ( 'Skipping this directory' ) print ( 80 * ' ' ) return \"\" fhindex = open ( os . path . join ( src dir , 'README.txt' ) ) . read ( ) if not os . path . exists ( target dir ) : os . makedirs ( target dir ) sorted listdir = [ fname for fname in sorted ( os . listdir ( src dir ) ) if fname . endswith ( '.py' ) ] entries text = [ ] for fname in sorted listdir : amount of code = generate file rst ( fname , target dir , src dir , gallery conf ) new fname = os . path . join ( src dir , fname ) intro = extract intro ( new fname ) write backreferences ( seen backrefs , gallery conf , target dir , fname , intro ) this entry = thumbnail div ( target dir , fname , intro ) + % ( target dir , fname [ : - 3 ] ) entries text . append ( ( amount of code , this entry ) ) entries text . sort ( ) for , entry text in entries text : fhindex += entry text fhindex += return fhindex", "predictions": ["generate a rst file for an rst directory ."], "references": ["generate the gallery restructuredtext for an example directory"], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 1312, "code": "def execute script ( code block , example globals , image path , fig count , src file , gallery conf ) : time elapsed = 0 stdout = '' print ( 'plotting code blocks in %s' % src file ) plt . close ( 'all' ) cwd = os . getcwd ( ) orig stdout = sys . stdout try : os . chdir ( os . path . dirname ( src file ) ) my buffer = String IO ( ) my stdout = Tee ( sys . stdout , my buffer ) sys . stdout = my stdout t start = time ( ) exec ( code block , example globals ) time elapsed = time ( ) - t start sys . stdout = orig stdout my stdout = my buffer . getvalue ( ) . strip ( ) . expandtabs ( ) if my stdout : stdout = CODE OUTPUT . format ( indent ( my stdout , ' ' * 4 ) ) os . chdir ( cwd ) figure list = save figures ( image path , fig count , gallery conf ) image list = \"\" if len ( figure list ) == 1 : figure name = figure list [ 0 ] image list = SINGLE IMAGE % figure name . lstrip ( '/' ) elif len ( figure list ) > 1 : image list = HLIST HEADER for figure name in figure list : image list += HLIST IMAGE TEMPLATE % figure name . lstrip ( '/' ) except Exception : formatted exception = traceback . format exc ( ) print ( 80 * ' ' ) print ( '%s is not compiling:' % src file ) print ( formatted exception ) print ( 80 * ' ' ) figure list = [ ] image list = codestr2rst ( formatted exception , lang = 'pytb' ) broken img = os . path . join ( glr path static ( ) , 'broken example.png' ) shutil . copyfile ( broken img , os . path . join ( cwd , image path . format ( 1 ) ) ) fig count += 1 if gallery conf [ 'abort on example error' ] : raise finally : os . chdir ( cwd ) sys . stdout = orig stdout print ( \" - time elapsed : %.2g sec\" % time elapsed ) code output = \"\\n{0}\\n\\n{1}\\n\\n\" . format ( image list , stdout ) return code output , time elapsed , fig count + len ( figure list )", "predictions": ["split a elements into a dictionary of items and gallery"], "references": ["executes the code block of the example file"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 1313, "code": "def ipy notebook skeleton ( ) : py version = sys . version info notebook skeleton = { \"cells\" : [ ] , \"metadata\" : { \"kernelspec\" : { \"display name\" : \"Python \" + str ( py version [ 0 ] ) , \"language\" : \"python\" , \"name\" : \"python\" + str ( py version [ 0 ] ) } , \"language info\" : { \"codemirror mode\" : { \"name\" : \"ipython\" , \"version\" : py version [ 0 ] } , \"file extension\" : \".py\" , \"mimetype\" : \"text/x-python\" , \"name\" : \"python\" , \"nbconvert exporter\" : \"python\" , \"pygments lexer\" : \"ipython\" + str ( py version [ 0 ] ) , \"version\" : '{0}.{1}.{2}' . format ( * sys . version info [ : 3 ] ) } } , \"nbformat\" : 4 , \"nbformat minor\" : 0 } return notebook skeleton", "predictions": ["returns the environ skeleton tag for a eval environ split split split split into a dictionary for a eval split split split split split split split split split split split split"], "references": ["returns a dictionary with the elements of a jupyter notebook"], "bleu": 0.06106432774355542, "rouge_l": 0.2149779735682819}
{"id": 1314, "code": "def save file ( self ) : with open ( self . write file , 'w' ) as out nb : json . dump ( self . work notebook , out nb , indent = 2 )", "predictions": ["saves the if json is a json file"], "references": ["saves the notebook to a file"], "bleu": 0.239802967618271, "rouge_l": 0.5865384615384615}
{"id": 1315, "code": "def select block ( str in , start tag , end tag ) : start pos = str in . find ( start tag ) if start pos < 0 : raise Value Error ( 'start tag not found' ) depth = 0 for pos in range ( start pos , len ( str in ) ) : if str in [ pos ] == start tag : depth += 1 elif str in [ pos ] == end tag : depth -= 1 if depth == 0 : break sel = str in [ start pos + 1 : pos ] return sel", "predictions": ["find a cfg cfg from a list of section numbers ."], "references": ["select first block delimited by start_tag and end_tag"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1316, "code": "def parse dict recursive ( dict str ) : dict out = dict ( ) pos last = 0 pos = dict str . find ( ':' ) while pos >= 0 : key = dict str [ pos last : pos ] if dict str [ pos + 1 ] == '[' : pos tmp = dict str . find ( ']' , pos + 1 ) if pos tmp < 0 : raise Runtime Error ( 'error when parsing dict' ) value = dict str [ pos + 2 : pos tmp ] . split ( ',' ) for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except Value Error : pass elif dict str [ pos + 1 ] == '{' : subdict str = select block ( dict str [ pos : ] , '{' , '}' ) value = parse dict recursive ( subdict str ) pos tmp = pos + len ( subdict str ) else : raise Value Error ( 'error when parsing dict: unknown elem' ) key = key . strip ( '\"' ) if len ( key ) > 0 : dict out [ key ] = value pos last = dict str . find ( ',' , pos tmp ) if pos last < 0 : break pos last += 1 pos = dict str . find ( ':' , pos last ) return dict out", "predictions": ["cfg can only be a dictionary"], "references": ["parse a dictionary from the search index"], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 1317, "code": "def embed code links ( app , exception ) : if exception is not None : return if not app . builder . config . plot gallery : return if app . builder . name not in [ 'html' , 'readthedocs' ] : return print ( 'Embedding documentation hyperlinks in examples..' ) gallery conf = app . config . sphinx gallery conf gallery dirs = gallery conf [ 'gallery dirs' ] if not isinstance ( gallery dirs , list ) : gallery dirs = [ gallery dirs ] for gallery dir in gallery dirs : embed code links ( app , gallery conf , gallery dir )", "predictions": ["run glob links on all ."], "references": ["embed hyperlinks to documentation into example code"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1318, "code": "def get link ( self , cobj ) : fname idx = None full name = cobj [ 'module short' ] + '.' + cobj [ 'name' ] if full name in self . searchindex [ 'objects' ] : value = self . searchindex [ 'objects' ] [ full name ] if isinstance ( value , dict ) : value = value [ next ( iter ( value . keys ( ) ) ) ] fname idx = value [ 0 ] elif cobj [ 'module short' ] in self . searchindex [ 'objects' ] : value = self . searchindex [ 'objects' ] [ cobj [ 'module short' ] ] if cobj [ 'name' ] in value . keys ( ) : fname idx = value [ cobj [ 'name' ] ] [ 0 ] if fname idx is not None : fname = self . searchindex [ 'filenames' ] [ fname idx ] + '.html' if self . is windows : fname = fname . replace ( '/' , '\\\\' ) link = os . path . join ( self . doc url , fname ) else : link = posixpath . join ( self . doc url , fname ) if hasattr ( link , 'decode' ) : link = link . decode ( 'utf-8' , 'replace' ) if link in self . page cache : html = self . page cache [ link ] else : html = get data ( link , self . gallery dir ) self . page cache [ link ] = html comb names = [ cobj [ 'module short' ] + '.' + cobj [ 'name' ] ] if self . extra modules test is not None : for mod in self . extra modules test : comb names . append ( mod + '.' + cobj [ 'name' ] ) url = False if hasattr ( html , 'decode' ) : html = html . decode ( 'utf-8' , 'replace' ) for comb name in comb names : if hasattr ( comb name , 'decode' ) : comb name = comb name . decode ( 'utf-8' , 'replace' ) if comb name in html : url = link + u'#' + comb name link = url else : link = False return link", "predictions": ["return link link for all modules images"], "references": ["get a valid link false if not found"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1319, "code": "def get short module name ( module name , obj name ) : parts = module name . split ( '.' ) short name = module name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short name , obj name ) ) except Import Error : short name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short name", "predictions": ["return program name ."], "references": ["get the shortest possible module name"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 1320, "code": "def thumbnail div ( full dir , fname , snippet , is backref = False ) : thumb = os . path . join ( full dir , 'images' , 'thumb' , 'sphx glr %s thumb.png' % fname [ : - 3 ] ) ref name = os . path . join ( full dir , fname ) . replace ( os . path . sep , ' ' ) template = BACKREF THUMBNAIL TEMPLATE if is backref else THUMBNAIL TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref name = ref name )", "predictions": ["render a correct correct correct correct correct format for the specified snippet for a full"], "references": ["generates rst to place a thumbnail in a gallery"], "bleu": 0.09103526405546068, "rouge_l": 0.17453505007153075}
{"id": 1321, "code": "def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = Command Line Tool ( ) return cli . run ( argv )", "predictions": ["entry point for the application script not executed not 1 ."], "references": ["main command line interface ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 1322, "code": "def pass from pipe ( cls ) : is pipe = not sys . stdin . isatty ( ) return is pipe and cls . strip last newline ( sys . stdin . read ( ) )", "predictions": ["return a pipe object from a pipe pipe ."], "references": ["return password from pipe if not on tty else false ."], "bleu": 0.14211011212459496, "rouge_l": 0.3929146537842191}
{"id": 1323, "code": "def get password ( self , service , username ) : if not self . connected ( service ) : raise Keyring Locked ( \"Failed to unlock the keyring!\" ) if not self . iface . has Entry ( self . handle , service , username , self . appid ) : return None password = self . iface . read Password ( self . handle , service , username , self . appid ) return str ( password )", "predictions": ["get the directory for the language when the language was found ."], "references": ["get password of the username for the service"], "bleu": 0.15537125692760353, "rouge_l": 0.4149659863945578}
{"id": 1324, "code": "def set password ( self , service , username , password ) : if not self . connected ( service ) : raise Password Set Error ( \"Cancelled by user\" ) self . iface . write Password ( self . handle , service , username , password , self . appid )", "predictions": ["set the directory of the iface ."], "references": ["set password for the username of the service"], "bleu": 0.240785655451027, "rouge_l": 0.5269978401727862}
{"id": 1325, "code": "def delete password ( self , service , username ) : if not self . connected ( service ) : raise Password Delete Error ( \"Cancelled by user\" ) if not self . iface . has Entry ( self . handle , service , username , self . appid ) : raise Password Delete Error ( \"Password not found\" ) self . iface . remove Entry ( self . handle , service , username , self . appid )", "predictions": ["check the password for the str . . . . . . . . . . . . ."], "references": ["delete the password for the username of the service ."], "bleu": 0.18759202316167212, "rouge_l": 0.3652694610778443}
{"id": 1326, "code": "def get env ( self , env var ) : value = os . environ . get ( env var ) if not value : raise Value Error ( 'Missing environment variable:%s' % env var ) return value", "predictions": ["return environment variables ."], "references": ["helper to read an environment variable"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 1327, "code": "def get password ( self , service , username ) : collection = self . get preferred collection ( ) items = collection . search items ( { \"username\" : username , \"service\" : service } ) for item in items : if hasattr ( item , 'unlock' ) : item . unlock ( ) if item . is locked ( ) : raise Keyring Locked ( 'Failed to unlock the item!' ) return item . get secret ( ) . decode ( 'utf-8' )", "predictions": ["get the common common common to the username"], "references": ["get password of the username for the service"], "bleu": 0.239802967618271, "rouge_l": 0.375}
{"id": 1328, "code": "def set password ( self , service , username , password ) : collection = self . get preferred collection ( ) attributes = { \"application\" : self . appid , \"service\" : service , \"username\" : username } label = \"Password for '{}' on '{}'\" . format ( username , service ) collection . create item ( label , attributes , password , replace = True )", "predictions": ["process a username ."], "references": ["set password for the username of the service"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1329, "code": "def backends ( cls ) : allowed = ( keyring for keyring in filter ( backend . limit , backend . get all keyring ( ) ) if not isinstance ( keyring , Chainer Backend ) and keyring . priority > 0 ) return sorted ( allowed , key = backend . by priority , reverse = True )", "predictions": ["return all async async async async async async async async async async functools args"], "references": ["discover all keyrings for chaining ."], "bleu": 0.08839374326825923, "rouge_l": 0.10777385159010601}
{"id": 1330, "code": "def set keyring ( keyring ) : global keyring backend if not isinstance ( keyring , backend . Keyring Backend ) : raise Type Error ( \"The keyring must be a subclass of Keyring Backend\" ) keyring backend = keyring", "predictions": ["sets the keyring keyring keyring keyring keyring keyring"], "references": ["set current keyring backend ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1331, "code": "def disable ( ) : root = platform . config root ( ) try : os . makedirs ( root ) except OS Error : pass filename = os . path . join ( root , 'keyringrc.cfg' ) if os . path . exists ( filename ) : msg = \"Refusing to overwrite {filename}\" . format ( * * locals ( ) ) raise Runtime Error ( msg ) with open ( filename , 'w' ) as file : file . write ( '[backend]\\ndefault-keyring=keyring.backends.null.Keyring' )", "predictions": ["with a attr script name name and write name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["configure the null keyring as the default ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1332, "code": "def load config ( ) : filename = 'keyringrc.cfg' keyring cfg = os . path . join ( platform . config root ( ) , filename ) if not os . path . exists ( keyring cfg ) : return config = configparser . Raw Config Parser ( ) config . read ( keyring cfg ) load keyring path ( config ) try : if config . has section ( \"backend\" ) : keyring name = config . get ( \"backend\" , \"default-keyring\" ) . strip ( ) else : raise configparser . No Option Error ( 'backend' , 'default-keyring' ) except ( configparser . No Option Error , Import Error ) : logger = logging . get Logger ( 'keyring' ) logger . warning ( \"Keyring config file contains incorrect values.\\n\" + \"Config file: %s\" % keyring cfg ) return return load keyring ( keyring name )", "predictions": ["process the event keyring debug debug debug debug debug debug debug debug debug debug debug debug"], "references": ["load a keyring using the config file in the config root ."], "bleu": 0.08513012360883544, "rouge_l": 0.07331730769230768}
{"id": 1333, "code": "def make formatter ( format name ) : if \"json\" in format name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format name == \"prettyjson\" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer", "predictions": ["create a formatter formatter function from a format format"], "references": ["returns a callable that outputs the data . defaults to print ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 1334, "code": "def argparser ( ) : parser = Argument Parser ( prog = 'pynetgear' ) parser . add argument ( \"--format\" , choices = [ 'json' , 'prettyjson' , 'py' ] , default = 'prettyjson' ) router args = parser . add argument group ( \"router connection config\" ) router args . add argument ( \"--host\" , help = \"Hostname for the router\" ) router args . add argument ( \"--user\" , help = \"Account for login\" ) router args . add argument ( \"--port\" , help = \"Port exposed on the router\" ) router args . add argument ( \"--login-v2\" , help = \"Force the use of the cookie-based authentication\" , dest = \"force login v2\" , default = False , action = \"store true\" ) router args . add argument ( \"--password\" , help = \"Not required with a wired connection.\" + \"Optionally, set the PYNETGEAR PASSWORD environment variable\" ) router args . add argument ( \"--url\" , help = \"Overrides host:port and ssl with url to router\" ) router args . add argument ( \"--no-ssl\" , dest = \"ssl\" , default = True , action = \"store false\" , help = \"Connect with https\" ) subparsers = parser . add subparsers ( description = \"Runs subcommand against the specified router\" , dest = \"subcommand\" ) block parser = subparsers . add parser ( \"block device\" , help = \"Blocks a device from connecting by mac address\" ) block parser . add argument ( \"--mac-addr\" ) allow parser = subparsers . add parser ( \"allow device\" , help = \"Allows a device with the mac address to connect\" ) allow parser . add argument ( \"--mac-addr\" ) subparsers . add parser ( \"login\" , help = \"Attempts to login to router.\" ) attached devices = subparsers . add parser ( \"attached devices\" , help = \"Outputs all attached devices\" ) attached devices . add argument ( \"-v\" , \"--verbose\" , action = \"store true\" , default = False , help = \"Choose between verbose and slower or terse and fast.\" ) subparsers . add parser ( \"traffic meter\" , help = \"Output router's traffic meter data\" ) return parser", "predictions": ["mark the command as argparser . . . . . . . . . . . . . ."], "references": ["constructs the argumentparser for the cli"], "bleu": 0.06439931429457924, "rouge_l": 0.0882778581765557}
{"id": 1335, "code": "def run subcommand ( netgear , args ) : subcommand = args . subcommand if subcommand == \"block device\" or subcommand == \"allow device\" : return netgear . allow block device ( args . mac addr , BLOCK if subcommand == \"block device\" else ALLOW ) if subcommand == \"attached devices\" : if args . verbose : return netgear . get attached devices 2 ( ) else : return netgear . get attached devices ( ) if subcommand == 'traffic meter' : return netgear . get traffic meter ( ) if subcommand == 'login' : return netgear . login ( ) print ( \"Unknown subcommand\" )", "predictions": ["run 2 - d list of devices devices len"], "references": ["runs the subcommand configured in args on the netgear session"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 1336, "code": "def main ( ) : args = argparser ( ) . parse args ( sys . argv [ 1 : ] ) password = os . environ . get ( 'PYNETGEAR PASSWORD' ) or args . password netgear = Netgear ( password , args . host , args . user , args . port , args . ssl , args . url , args . force login v2 ) results = run subcommand ( netgear , args ) formatter = make formatter ( args . format ) if results is None : print ( \"Error communicating with the Netgear router\" ) else : formatter ( results )", "predictions": ["data for the specified dos . . . . ."], "references": ["scan for devices and print results ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 1337, "code": "def convert ( value , to type , default = None ) : try : return default if value is None else to type ( value ) except Value Error : return default", "predictions": ["spectrum a value or import value to a string"], "references": ["convert value to to_type returns default if fails ."], "bleu": 0.18575057999133596, "rouge_l": 0.2222222222222222}
{"id": 1338, "code": "def make request ( self , service , method , params = None , body = \"\" , need auth = True ) : if need auth and not self . cookie : if not self . login ( ) : return False , None headers = self . get headers ( service , method , need auth ) if not body : if not params : params = \"\" if isinstance ( params , dict ) : map = params params = \"\" for k in map : params += \"<\" + k + \">\" + map [ k ] + \"</\" + k + \">\\n\" body = CALL BODY . format ( service = SERVICE PREFIX + service , method = method , params = params ) message = SOAP REQUEST . format ( session id = SESSION ID , body = body ) try : response = requests . post ( self . soap url , headers = headers , data = message , timeout = 30 , verify = False ) if need auth and is unauthorized response ( response ) : self . cookie = None LOGGER . warning ( \"Unauthorized response, let's login and retry...\" ) if self . login ( ) : headers = self . get headers ( service , method , need auth ) response = requests . post ( self . soap url , headers = headers , data = message , timeout = 30 , verify = False ) success = is valid response ( response ) if not success : LOGGER . error ( \"Invalid response\" ) LOGGER . debug ( \"%s\\n%s\\n%s\" , response . status code , str ( response . headers ) , response . text ) return success , response except requests . exceptions . Request Exception : LOGGER . exception ( \"Error talking to API\" ) return False , None", "predictions": ["remove a bias bias"], "references": ["make an api request to the router ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 1339, "code": "def gethostbyname ( self , hostname ) : if self . database Type in const . IPV6 EDITIONS : response = socket . getaddrinfo ( hostname , 0 , socket . AF INET6 ) family , socktype , proto , canonname , sockaddr = response [ 0 ] address , port , flow , scope = sockaddr return address else : return socket . gethostbyname ( hostname )", "predictions": ["return a lang connection"], "references": ["hostname lookup method supports both ipv4 and ipv6 ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 1340, "code": "def compress ( self , filename ) : compressed filename = self . get compressed filename ( filename ) if not compressed filename : return self . do compress ( filename , compressed filename )", "predictions": ["get file name for given file . . . . . . . . ."], "references": ["compress a file only if needed ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 1341, "code": "def copy ( src , dst , symlink = False , rellink = False ) : func = os . symlink if symlink else shutil . copy2 if symlink and os . path . lexists ( dst ) : os . remove ( dst ) if rellink : func ( os . path . relpath ( src , os . path . dirname ( dst ) ) , dst ) else : func ( src , dst )", "predictions": ["check if change data should be copied to src"], "references": ["copy or symlink the file ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1342, "code": "def url from path ( path ) : if os . sep != '/' : path = '/' . join ( path . split ( os . sep ) ) return quote ( path )", "predictions": ["extract a save save save it to a file"], "references": ["transform path to url converting backslashes to slashes if needed ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 1343, "code": "def load exif ( album ) : if not hasattr ( album . gallery , \"exif Cache\" ) : restore cache ( album . gallery ) cache = album . gallery . exif Cache for media in album . medias : if media . type == \"image\" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]", "predictions": ["generate dir dir for album seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen seen"], "references": ["loads the exif data of all images in an album from cache"], "bleu": 0.04034110170120257, "rouge_l": 0.05160744500846023}
{"id": 1344, "code": "def restore cache ( gallery ) : cache Path = os . path . join ( gallery . settings [ \"destination\" ] , \".exif cache\" ) try : if os . path . exists ( cache Path ) : with open ( cache Path , \"rb\" ) as cache File : gallery . exif Cache = pickle . load ( cache File ) logger . debug ( \"Loaded cache with %d entries\" , len ( gallery . exif Cache ) ) else : gallery . exif Cache = { } except Exception as e : logger . warn ( \"Could not load cache: %s\" , e ) gallery . exif Cache = { }", "predictions": ["restore cache from gallery"], "references": ["restores the exif data cache from the cache file"], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 1345, "code": "def save cache ( gallery ) : if hasattr ( gallery , \"exif Cache\" ) : cache = gallery . exif Cache else : cache = gallery . exif Cache = { } for album in gallery . albums . values ( ) : for image in album . images : cache [ os . path . join ( image . path , image . filename ) ] = image . exif cache Path = os . path . join ( gallery . settings [ \"destination\" ] , \".exif cache\" ) if len ( cache ) == 0 : if os . path . exists ( cache Path ) : os . remove ( cache Path ) return try : with open ( cache Path , \"wb\" ) as cache File : pickle . dump ( cache , cache File ) logger . debug ( \"Stored cache with %d entries\" , len ( gallery . exif Cache ) ) except Exception as e : logger . warn ( \"Could not store cache: %s\" , e ) os . remove ( cache Path )", "predictions": ["save the cache to a file"], "references": ["stores the exif data of all images in the gallery"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1346, "code": "def filter nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src path , \".nomedia\" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( \"Ignoring album '%s' because of present 0-byte \" \".nomedia file\" , album . name ) remove albums with subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst path ) except OS Error as e : pass album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , \"r\" ) as nomedia File : logger . info ( \"Found a .nomedia file in %s, ignoring its \" \"entries\" , album . name ) ignored = nomedia File . read ( ) . split ( \"\\n\" ) album . medias = [ media for media in album . medias if media . src filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] remove albums with subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )", "predictions": ["filter album and album results to a gallery ."], "references": ["removes all filtered media and subdirs from an album"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 1347, "code": "def serve ( destination , port , config ) : if os . path . exists ( destination ) : pass elif os . path . exists ( config ) : settings = read settings ( config ) destination = settings . get ( 'destination' ) if not os . path . exists ( destination ) : sys . stderr . write ( \"The '{}' directory doesn't exist, maybe try \" \"building first?\\n\" . format ( destination ) ) sys . exit ( 1 ) else : sys . stderr . write ( \"The {destination} directory doesn't exist \" \"and the config file ({config}) could not be read.\\n\" . format ( destination = destination , config = config ) ) sys . exit ( 2 ) print ( 'DESTINATION : {}' . format ( destination ) ) os . chdir ( destination ) Handler = server . Simple HTTP Request Handler httpd = socketserver . TCP Server ( ( \"\" , port ) , Handler , False ) print ( \" * Running on http://127.0.0.1:{}/\" . format ( port ) ) try : httpd . allow reuse address = True httpd . server bind ( ) httpd . server activate ( ) httpd . serve forever ( ) except Keyboard Interrupt : print ( '\\n All done!' )", "predictions": ["serve up a specific server ."], "references": ["run a simple web server ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 1348, "code": "def generate thumbnail ( source , outname , box , fit = True , options = None , thumb fit centering = ( 0.5 , 0.5 ) ) : logger = logging . get Logger ( name ) img = read image ( source ) original format = img . format if fit : img = Image Ops . fit ( img , box , PIL Image . ANTIALIAS , centering = thumb fit centering ) else : img . thumbnail ( box , PIL Image . ANTIALIAS ) outformat = img . format or original format or 'JPEG' logger . debug ( 'Save thumnail image: %s (%s)' , outname , outformat ) save image ( img , outname , outformat , options = options , autoconvert = True )", "predictions": ["generate a thumbnail with a thumbnail"], "references": ["create a thumbnail image ."], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 1349, "code": "def get exif data ( filename ) : logger = logging . get Logger ( name ) img = read image ( filename ) try : exif = img . getexif ( ) or { } except Zero Division Error : logger . warning ( 'Failed to read EXIF data.' ) return None data = { TAGS . get ( tag , tag ) : value for tag , value in exif . items ( ) } if 'GPS Info' in data : try : data [ 'GPS Info' ] = { GPSTAGS . get ( tag , tag ) : value for tag , value in data [ 'GPS Info' ] . items ( ) } except Attribute Error : logger = logging . get Logger ( name ) logger . info ( 'Failed to get GPS Info' ) del data [ 'GPS Info' ] return data", "predictions": ["returns exif data from the given image ."], "references": ["return a dict with the raw exif data ."], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 1350, "code": "def get iptc data ( filename ) : logger = logging . get Logger ( name ) iptc data = { } raw iptc = { } try : img = read image ( filename ) raw iptc = Iptc Image Plugin . getiptcinfo ( img ) except Syntax Error : logger . info ( 'IPTC Error in %s' , filename ) if raw iptc and ( 2 , 5 ) in raw iptc : iptc data [ \"title\" ] = raw iptc [ ( 2 , 5 ) ] . decode ( 'utf-8' , errors = 'replace' ) if raw iptc and ( 2 , 120 ) in raw iptc : iptc data [ \"description\" ] = raw iptc [ ( 2 , 120 ) ] . decode ( 'utf-8' , errors = 'replace' ) if raw iptc and ( 2 , 105 ) in raw iptc : iptc data [ \"headline\" ] = raw iptc [ ( 2 , 105 ) ] . decode ( 'utf-8' , errors = 'replace' ) return iptc data", "predictions": ["fetch image data from local file"], "references": ["return a dict with the raw iptc data ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1351, "code": "def create output directories ( self ) : check or create dir ( self . dst path ) if self . medias : check or create dir ( join ( self . dst path , self . settings [ 'thumb dir' ] ) ) if self . medias and self . settings [ 'keep orig' ] : self . orig path = join ( self . dst path , self . settings [ 'orig dir' ] ) check or create dir ( self . orig path )", "predictions": ["create output directories ."], "references": ["create output directories for thumbnails and original images ."], "bleu": 0.20258948470231466, "rouge_l": 0.5754716981132075}
{"id": 1352, "code": "def url ( self ) : url = self . name . encode ( 'utf-8' ) return url quote ( url ) + '/' + self . url ext", "predictions": ["url for the url ."], "references": ["url of the album relative to its parent ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 1353, "code": "def thumbnail ( self ) : if self . thumbnail : return self . thumbnail thumbnail = self . meta . get ( 'thumbnail' , [ '' ] ) [ 0 ] if thumbnail and isfile ( join ( self . src path , thumbnail ) ) : self . thumbnail = url from path ( join ( self . name , get thumb ( self . settings , thumbnail ) ) ) self . logger . debug ( \"Thumbnail for %r : %s\" , self , self . thumbnail ) return self . thumbnail else : for f in self . medias : ext = splitext ( f . filename ) [ 1 ] if ext . lower ( ) in self . settings [ 'img extensions' ] : size = f . size if size is None : size = get size ( f . src path ) if size [ 'width' ] > size [ 'height' ] : self . thumbnail = ( url quote ( self . name ) + '/' + f . thumbnail ) self . logger . debug ( \"Use 1st landscape image as thumbnail for %r : %s\" , self , self . thumbnail ) return self . thumbnail if not self . thumbnail and self . medias : for media in self . medias : if media . thumbnail is not None : self . thumbnail = ( url quote ( self . name ) + '/' + media . thumbnail ) break else : self . logger . warning ( \"No thumbnail found for %r\" , self ) return None self . logger . debug ( \"Use the 1st image as thumbnail for %r : %s\" , self , self . thumbnail ) return self . thumbnail if not self . thumbnail : for path , album in self . gallery . get albums ( self . path ) : if album . thumbnail : self . thumbnail = ( url quote ( self . name ) + '/' + album . thumbnail ) self . logger . debug ( \"Using thumbnail from sub-directory for %r : %s\" , self , self . thumbnail ) return self . thumbnail self . logger . error ( 'Thumbnail not found for %r' , self ) return None", "predictions": ["the thumbnail thumbnail ."], "references": ["path to the thumbnail of the album ."], "bleu": 0.18693159143202892, "rouge_l": 0.47164948453608246}
{"id": 1354, "code": "def get albums ( self , path ) : for name in self . albums [ path ] . subdirs : subdir = os . path . normpath ( join ( path , name ) ) yield subdir , self . albums [ subdir ] for subname , album in self . get albums ( subdir ) : yield subname , self . albums [ subdir ]", "predictions": ["return an albums list of albums objects ."], "references": ["return the list of all sub - directories of path ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 1355, "code": "def build ( self , force = False ) : if not self . albums : self . logger . warning ( \"No albums found.\" ) return def log func ( x ) : available length = get terminal size ( ) [ 0 ] - 64 if x and available length > 10 : return x . name [ : available length ] else : return \"\" try : with progressbar ( self . albums . values ( ) , label = \"Collecting files\" , item show func = log func , show eta = False , file = self . progressbar target ) as albums : media list = [ f for album in albums for f in self . process dir ( album , force = force ) ] except Keyboard Interrupt : sys . exit ( 'Interrupted' ) bar opt = { 'label' : \"Processing files\" , 'show pos' : True , 'file' : self . progressbar target } failed files = [ ] if self . pool : try : with progressbar ( length = len ( media list ) , * * bar opt ) as bar : for res in self . pool . imap unordered ( worker , media list ) : if res : failed files . append ( res ) bar . update ( 1 ) self . pool . close ( ) self . pool . join ( ) except Keyboard Interrupt : self . pool . terminate ( ) sys . exit ( 'Interrupted' ) except pickle . Pickling Error : self . logger . critical ( \"Failed to process files with the multiprocessing feature.\" \" This can be caused by some module import or object \" \"defined in the settings file, which can't be serialized.\" , exc info = True ) sys . exit ( 'Abort' ) else : with progressbar ( media list , * * bar opt ) as medias : for media item in medias : res = process file ( media item ) if res : failed files . append ( res ) if failed files : self . remove files ( failed files ) if self . settings [ 'write html' ] : album writer = Album Page Writer ( self . settings , index title = self . title ) album list writer = Album List Page Writer ( self . settings , index title = self . title ) with progressbar ( self . albums . values ( ) , label = \"%16s\" % \"Writing files\" , item show func = log func , show eta = False , file = self . progressbar target ) as albums : for album in albums : if album . albums : if album . medias : self . logger . warning ( \"Album %s contains sub-albums and images. \" \"Please move images to their own sub-album. \" \"Images in album %s will not be visible.\" , album . title , album . title ) album list writer . write ( album ) else : album writer . write ( album ) print ( '' ) signals . gallery build . send ( self )", "predictions": ["build the list of files and start the build server ."], "references": ["create the image gallery"], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 1356, "code": "def process dir ( self , album , force = False ) : for f in album : if isfile ( f . dst path ) and not force : self . logger . info ( \"%s exists - skipping\" , f . filename ) self . stats [ f . type + ' skipped' ] += 1 else : self . stats [ f . type ] += 1 yield ( f . type , f . path , f . filename , f . src path , album . dst path , self . settings )", "predictions": ["process a single album ."], "references": ["process a list of images in a directory ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 1357, "code": "def reduce opacity ( im , opacity ) : assert opacity >= 0 and opacity <= 1 if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) else : im = im . copy ( ) alpha = im . split ( ) [ 3 ] alpha = Image Enhance . Brightness ( alpha ) . enhance ( opacity ) im . putalpha ( alpha ) return im", "predictions": ["return a reduce function that merge a given opacity ."], "references": ["returns an image with reduced opacity ."], "bleu": 0.16590387014219712, "rouge_l": 0.24302788844621517}
{"id": 1358, "code": "def watermark ( im , mark , position , opacity = 1 ) : if opacity < 1 : mark = reduce opacity ( mark , opacity ) if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) layer = Image . new ( 'RGBA' , im . size , ( 0 , 0 , 0 , 0 ) ) if position == 'tile' : for y in range ( 0 , im . size [ 1 ] , mark . size [ 1 ] ) : for x in range ( 0 , im . size [ 0 ] , mark . size [ 0 ] ) : layer . paste ( mark , ( x , y ) ) elif position == 'scale' : ratio = min ( float ( im . size [ 0 ] ) / mark . size [ 0 ] , float ( im . size [ 1 ] ) / mark . size [ 1 ] ) w = int ( mark . size [ 0 ] * ratio ) h = int ( mark . size [ 1 ] * ratio ) mark = mark . resize ( ( w , h ) ) layer . paste ( mark , ( int ( ( im . size [ 0 ] - w ) / 2 ) , int ( ( im . size [ 1 ] - h ) / 2 ) ) ) else : layer . paste ( mark , position ) return Image . composite ( layer , im , layer )", "predictions": ["watermark a watermark image"], "references": ["adds a watermark to an image ."], "bleu": 0.24002491458061356, "rouge_l": 0.5198863636363635}
{"id": 1359, "code": "def video size ( source , converter = 'ffmpeg' ) : res = subprocess . run ( [ converter , '-i' , source ] , stderr = subprocess . PIPE ) stderr = res . stderr . decode ( 'utf8' ) pattern = re . compile ( r'Stream.*Video.* ([0-9]+)x([0-9]+)' ) match = pattern . search ( stderr ) rot pattern = re . compile ( r'rotate\\s*:\\s*-?(90|270)' ) rot match = rot pattern . search ( stderr ) if match : x , y = int ( match . groups ( ) [ 0 ] ) , int ( match . groups ( ) [ 1 ] ) else : x = y = 0 if rot match : x , y = y , x return x , y", "predictions": ["get the size of a video"], "references": ["returns the dimensions of the video ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 1360, "code": "def generate thumbnail ( source , outname , box , delay , fit = True , options = None , converter = 'ffmpeg' ) : logger = logging . get Logger ( name ) tmpfile = outname + \".tmp.jpg\" cmd = [ converter , '-i' , source , '-an' , '-r' , '1' , '-ss' , delay , '-vframes' , '1' , '-y' , tmpfile ] logger . debug ( 'Create thumbnail for video: %s' , ' ' . join ( cmd ) ) check subprocess ( cmd , source , outname ) image . generate thumbnail ( tmpfile , outname , box , fit = fit , options = options ) os . unlink ( tmpfile )", "predictions": ["generate thumbnail for testing ."], "references": ["create a thumbnail image for the video source based on ffmpeg ."], "bleu": 0.08006212224540951, "rouge_l": 0.3285457809694794}
{"id": 1361, "code": "def generate context ( self , album ) : from . import url as sigal link self . logger . info ( \"Output album : %r\" , album ) return { 'album' : album , 'index title' : self . index title , 'settings' : self . settings , 'sigal link' : sigal link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url from path ( os . path . relpath ( self . theme path , album . dst path ) ) } , }", "predictions": ["generate a context for a given album ."], "references": ["generate the context dict for the given path ."], "bleu": 0.1862539773562041, "rouge_l": 0.5820610687022901}
{"id": 1362, "code": "def write ( self , album ) : page = self . template . render ( * * self . generate context ( album ) ) output file = os . path . join ( album . dst path , album . output file ) with open ( output file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )", "predictions": ["write album to a file"], "references": ["generate the html page and save it ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 1363, "code": "def read settings ( filename = None ) : logger = logging . get Logger ( name ) logger . info ( \"Reading settings ...\" ) settings = DEFAULT CONFIG . copy ( ) if filename : logger . debug ( \"Settings file: %s\" , filename ) settings path = os . path . dirname ( filename ) tempdict = { } with open ( filename ) as f : code = compile ( f . read ( ) , filename , 'exec' ) exec ( code , tempdict ) settings . update ( ( k , v ) for k , v in tempdict . items ( ) if k not in [ ' builtins ' ] ) paths = [ 'source' , 'destination' , 'watermark' ] if os . path . isdir ( join ( settings path , settings [ 'theme' ] ) ) and os . path . isdir ( join ( settings path , settings [ 'theme' ] , 'templates' ) ) : paths . append ( 'theme' ) for p in paths : path = settings [ p ] if path and not isabs ( path ) : settings [ p ] = abspath ( normpath ( join ( settings path , path ) ) ) logger . debug ( \"Rewrite %s : %s -> %s\" , p , path , settings [ p ] ) for key in ( 'img size' , 'thumb size' , 'video size' ) : w , h = settings [ key ] if h > w : settings [ key ] = ( h , w ) logger . warning ( \"The %s setting should be specified with the \" \"largest value first.\" , key ) if not settings [ 'img processor' ] : logger . info ( 'No Processor, images will not be resized' ) logger . debug ( 'Settings:\\n%s' , pformat ( settings , width = 120 ) ) return settings", "predictions": ["read settings from the settings file ."], "references": ["read settings from a config file in the source_dir root ."], "bleu": 0.22455189621853547, "rouge_l": 0.5341506129597198}
{"id": 1364, "code": "def generate media pages ( gallery ) : writer = Page Writer ( gallery . settings , index title = gallery . title ) for album in gallery . albums . values ( ) : medias = album . medias next medias = medias [ 1 : ] + [ None ] previous medias = [ None ] + medias [ : - 1 ] media groups = zip ( medias , next medias , previous medias ) for media group in media groups : writer . write ( album , media group )", "predictions": ["generate media pages ."], "references": ["generates and writes the media pages for all media in the gallery"], "bleu": 0.06399610426154731, "rouge_l": 0.22932330827067668}
{"id": 1365, "code": "def write ( self , album , media group ) : from sigal import url as sigal link file path = os . path . join ( album . dst path , media group [ 0 ] . filename ) page = self . template . render ( { 'album' : album , 'media' : media group [ 0 ] , 'previous media' : media group [ - 1 ] , 'next media' : media group [ 1 ] , 'index title' : self . index title , 'settings' : self . settings , 'sigal link' : sigal link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url from path ( os . path . relpath ( self . theme path , album . dst path ) ) } , } ) output file = \"%s.html\" % file path with open ( output file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )", "predictions": ["write a album to a file ."], "references": ["generate the media page and save it"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1366, "code": "def cleanup directory ( config data ) : if os . path . exists ( config data . project directory ) : choice = False if config data . noinput is False and not config data . verbose : choice = query yes no ( 'The installation failed.\\n' 'Do you want to clean up by removing {0}?\\n' '\\t Warning: this will delete all files in:\\n' '\\t\\t{0}\\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config data . project directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\\n' ) if config data . skip project dir check is False and ( choice or ( config data . noinput and config data . delete project dir ) ) : sys . stdout . write ( 'Removing everything under {0}\\n' . format ( os . path . abspath ( config data . project directory ) ) ) shutil . rmtree ( config data . project directory , True )", "predictions": ["cleanup all files in the project ."], "references": ["asks user for removal of project directory and eventually removes it"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 1367, "code": "def manage args ( parser , args ) : for item in data . CONFIGURABLE OPTIONS : action = parser . option string actions [ item ] choices = default = '' input value = getattr ( args , action . dest ) new val = None if not args . noinput : if action . choices : choices = ' (choices: {0})' . format ( ', ' . join ( action . choices ) ) if input value : if type ( input value ) == list : default = ' [default {0}]' . format ( ', ' . join ( input value ) ) else : default = ' [default {0}]' . format ( input value ) while not new val : prompt = '{0}{1}{2}: ' . format ( action . help , choices , default ) if action . choices in ( 'yes' , 'no' ) : new val = utils . query yes no ( prompt ) else : new val = compat . input ( prompt ) new val = compat . clean ( new val ) if not new val and input value : new val = input value if new val and action . dest == 'templates' : if new val != 'no' and not os . path . isdir ( new val ) : sys . stdout . write ( 'Given directory does not exists, retry\\n' ) new val = False if new val and action . dest == 'db' : action ( parser , args , new val , action . option strings ) new val = getattr ( args , action . dest ) else : if not input value and action . required : raise Value Error ( 'Option {0} is required when in no-input mode' . format ( action . dest ) ) new val = input value if action . dest == 'db' : action ( parser , args , new val , action . option strings ) new val = getattr ( args , action . dest ) if action . dest == 'templates' and ( new val == 'no' or not os . path . isdir ( new val ) ) : new val = False if action . dest in ( 'bootstrap' , 'starting page' ) : new val = ( new val == 'yes' ) setattr ( args , action . dest , new val ) return args", "predictions": ["parse arguments and manage options ."], "references": ["checks and validate provided input"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1368, "code": "def supported versions ( django , cms ) : cms version = None django version = None try : cms version = Decimal ( cms ) except ( Value Error , Invalid Operation ) : try : cms version = CMS VERSION MATRIX [ str ( cms ) ] except Key Error : pass try : django version = Decimal ( django ) except ( Value Error , Invalid Operation ) : try : django version = DJANGO VERSION MATRIX [ str ( django ) ] except Key Error : pass try : if ( cms version and django version and not ( Loose Version ( VERSION MATRIX [ compat . unicode ( cms version ) ] [ 0 ] ) <= Loose Version ( compat . unicode ( django version ) ) <= Loose Version ( VERSION MATRIX [ compat . unicode ( cms version ) ] [ 1 ] ) ) ) : raise Runtime Error ( 'Django and django CMS versions doesn\\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django version , cms version ) ) except Key Error : raise Runtime Error ( 'Django and django CMS versions doesn\\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django version , cms version ) ) return ( compat . unicode ( django version ) if django version else django version , compat . unicode ( cms version ) if cms version else cms version )", "predictions": ["check if django cms cms cms"], "references": ["convert numeric and literal version information to numeric format"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 1369, "code": "def dump config file ( filename , args , parser = None ) : config = Config Parser ( ) config . add section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys empty values not pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) for action in parser . actions : if action . dest in ( 'help' , 'config file' , 'config dump' , 'project name' ) : continue keyp = action . option strings [ 0 ] option name = keyp . lstrip ( '-' ) option value = getattr ( args , action . dest ) if any ( [ i for i in keys empty values not pass if i in action . option strings ] ) : if action . dest == 'languages' : if len ( option value ) == 1 and option value [ 0 ] == 'en' : config . set ( SECTION , option name , '' ) else : config . set ( SECTION , option name , ',' . join ( option value ) ) else : config . set ( SECTION , option name , option value if option value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option name , 'yes' if option value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option name , option value if option value else 'no' ) elif action . dest == 'cms version' : version = ( 'stable' if option value == CMS VERSION MATRIX [ 'stable' ] else option value ) config . set ( SECTION , option name , version ) elif action . dest == 'django version' : version = ( 'stable' if option value == DJANGO VERSION MATRIX [ 'stable' ] else option value ) config . set ( SECTION , option name , version ) elif action . const : config . set ( SECTION , option name , 'true' if option value else 'false' ) else : config . set ( SECTION , option name , str ( option value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )", "predictions": ["dump a config file"], "references": ["dump args to config file ."], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 1370, "code": "def validate sample rates ( input filepath list , combine type ) : sample rates = [ file info . sample rate ( f ) for f in input filepath list ] if not core . all equal ( sample rates ) : raise IO Error ( \"Input files do not have the same sample rate. The {} combine \" \"type requires that all files have the same sample rate\" . format ( combine type ) )", "predictions": ["check that the rates rates and combine the rates ."], "references": ["check if files in input file list have the same sample rate"], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 1371, "code": "def validate num channels ( input filepath list , combine type ) : channels = [ file info . channels ( f ) for f in input filepath list ] if not core . all equal ( channels ) : raise IO Error ( \"Input files do not have the same number of channels. The \" \"{} combine type requires that all files have the same \" \"number of channels\" . format ( combine type ) )", "predictions": ["validate that the input files have the same number of channels ."], "references": ["check if files in input file list have the same number of channels"], "bleu": 0.46082434149797136, "rouge_l": 0.5559895833333334}
{"id": 1372, "code": "def reverse ( self ) : effect args = [ 'reverse' ] self . effects . extend ( effect args ) self . effects log . append ( 'reverse' ) return self", "predictions": ["reverse the effect ."], "references": ["reverse the audio completely"], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 1373, "code": "def join ( self , room ) : self . socket . rooms . add ( self . get room name ( room ) )", "predictions": ["join a room into the socket"], "references": ["lets a user join a room on a specific namespace ."], "bleu": 0.17867793336200125, "rouge_l": 0.33516483516483514}
{"id": 1374, "code": "def leave ( self , room ) : self . socket . rooms . remove ( self . get room name ( room ) )", "predictions": ["leave a room from the socket"], "references": ["lets a user leave a room on a specific namespace ."], "bleu": 0.17867793336200125, "rouge_l": 0.33516483516483514}
{"id": 1375, "code": "def save ack callback ( self , msgid , callback ) : if msgid in self . ack callbacks : return False self . ack callbacks [ msgid ] = callback", "predictions": ["callback for save ack ."], "references": ["keep a reference of the callback on this socket ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 1376, "code": "def heartbeat ( self ) : interval = self . config [ 'heartbeat interval' ] while self . connected : gevent . sleep ( interval ) self . put client msg ( \"2::\" )", "predictions": ["main loop for restore restore"], "references": ["start the heartbeat greenlet to check connection health ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 1377, "code": "def spawn heartbeat ( self ) : self . spawn ( self . heartbeat ) self . spawn ( self . heartbeat timeout )", "predictions": ["this method is called to start a new cache if a cache is set to be set"], "references": ["this functions returns a list of jobs"], "bleu": 0.07994607499472013, "rouge_l": 0.18020679468242246}
{"id": 1378, "code": "def encode ( data , json dumps = default json dumps ) : payload = '' msg = str ( MSG TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : msg += '::' elif msg in [ '3' , '4' , '5' ] : if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ack Id' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json dumps ( data [ 'args' ] ) elif msg == '7' : msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] elif msg == '8' : msg += '::' return msg", "predictions": ["handle the basic json - rpc message ."], "references": ["encode an attribute dict into a byte string ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1379, "code": "def decode ( rawstr , json loads = default json loads ) : decoded msg = { } try : rawstr = rawstr . decode ( 'utf-8' ) except Attribute Error : pass split data = rawstr . split ( \":\" , 3 ) msg type = split data [ 0 ] msg id = split data [ 1 ] endpoint = split data [ 2 ] data = '' if msg id != '' : if \"+\" in msg id : msg id = msg id . split ( '+' ) [ 0 ] decoded msg [ 'id' ] = int ( msg id ) decoded msg [ 'ack' ] = 'data' else : decoded msg [ 'id' ] = int ( msg id ) decoded msg [ 'ack' ] = True msg type id = int ( msg type ) if msg type id in MSG VALUES : decoded msg [ 'type' ] = MSG VALUES [ int ( msg type ) ] else : raise Exception ( \"Unknown message type: %s\" % msg type ) decoded msg [ 'endpoint' ] = endpoint if len ( split data ) > 3 : data = split data [ 3 ] if msg type == \"0\" : pass elif msg type == \"1\" : decoded msg [ 'qs' ] = data elif msg type == \"2\" : pass elif msg type == \"3\" : decoded msg [ 'data' ] = data elif msg type == \"4\" : decoded msg [ 'data' ] = json loads ( data ) elif msg type == \"5\" : try : data = json loads ( data ) except Value Error : print ( \"Invalid JSON event message\" , data ) decoded msg [ 'args' ] = [ ] else : decoded msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded msg [ 'args' ] = data [ 'args' ] else : decoded msg [ 'args' ] = [ ] elif msg type == \"6\" : if '+' in data : ack Id , data = data . split ( '+' ) decoded msg [ 'ack Id' ] = int ( ack Id ) decoded msg [ 'args' ] = json loads ( data ) else : decoded msg [ 'ack Id' ] = int ( data ) decoded msg [ 'args' ] = [ ] elif msg type == \"7\" : if '+' in data : reason , advice = data . split ( '+' ) decoded msg [ 'reason' ] = REASONS VALUES [ int ( reason ) ] decoded msg [ 'advice' ] = ADVICES VALUES [ int ( advice ) ] else : decoded msg [ 'advice' ] = '' if data != '' : decoded msg [ 'reason' ] = REASONS VALUES [ int ( data ) ] else : decoded msg [ 'reason' ] = '' elif msg type == \"8\" : pass return decoded msg", "predictions": ["build a . . xml message from a raw xml message"], "references": ["decode a rawstr packet arriving from the socket into a dict ."], "bleu": 0.1307847403141535, "rouge_l": 0.25884016973125884}
{"id": 1380, "code": "def get socket ( self , sessid = '' ) : socket = self . sockets . get ( sessid ) if sessid and not socket : return None if socket is None : socket = Socket ( self , self . config ) self . sockets [ socket . sessid ] = socket else : socket . incr hits ( ) return socket", "predictions": ["generate a thumbnail thumbnail"], "references": ["return an existing or new client socket ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 1381, "code": "def write ( self , data ) : args = parse qs ( self . handler . environ . get ( \"QUERY STRING\" ) ) if \"i\" in args : i = args [ \"i\" ] else : i = \"0\" super ( JSON Polling , self ) . write ( \"io.j[%s]('%s');\" % ( i , data ) )", "predictions": ["get some bytes of the client . . ."], "references": ["just quote out stuff before sending it out"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1382, "code": "def remove binaries ( ) : patterns = ( \"adslib/*.a\" , \"adslib/*.o\" , \"adslib/obj/*.o\" , \"adslib/*.bin\" , \"adslib/*.so\" , ) for f in functools . reduce ( operator . iconcat , [ glob . glob ( p ) for p in patterns ] ) : os . remove ( f )", "predictions": ["get all binaries img"], "references": ["remove all binary files in the adslib directory ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1383, "code": "def ads Port Close Ex ( port ) : port close ex = ads DLL . Ads Port Close Ex port close ex . restype = ctypes . c long error code = port close ex ( port ) if error code : raise ADS Error ( error code )", "predictions": ["sends a create - port - port port to the server path path path path path path path path path path path path path path path path path path path path"], "references": ["close the connection to the twincat message router ."], "bleu": 0.0513487742994337, "rouge_l": 0.11101000909918107}
{"id": 1384, "code": "def open ( self ) : if self . open : return self . port = ads Port Open Ex ( ) if linux : ads Add Route ( self . adr . net Id Struct ( ) , self . ip address ) self . open = True", "predictions": ["url to the device ."], "references": ["connect to the twincat message router ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 1385, "code": "def fetch ( self , start date , end date ) : records = [ ] for two months range in self . generate ranges ( start date , end date ) : log . debug ( two months range ) for record in self . fetch missions for range ( two months range [ 0 ] , two months range [ 1 ] ) : records . append ( record ) df = pd . Data Frame ( records , columns = [ 'participant' , 'destination' , 'subject' , 'start' , 'end' , 'canceled' , 'report status' , 'report details link' ] ) translate column ( df , 'report status' , { 'Dispon\u00edvel':   Available', 'Pendente' : 'Pending' , 'Em an\u00e1lise':   Analysing', 'N\u00e3o se aplica':   Does not apply' } ) translate column ( df , 'canceled' , { 'N\u00e3o':   No', 'Sim' : 'Yes' } ) return df . drop duplicates ( )", "predictions": ["thumbnail for all months"], "references": ["fetches official missions within the given date range"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 1386, "code": "def fetch ( self ) : xml = urllib . request . urlopen ( self . URL ) tree = ET . Element Tree ( file = xml ) records = self . parse deputies ( tree . getroot ( ) ) df = pd . Data Frame ( records , columns = ( 'congressperson id' , 'budget id' , 'condition' , 'congressperson document' , 'civil name' , 'congressperson name' , 'picture url' , 'gender' , 'state' , 'party' , 'phone number' , 'email' ) ) return self . translate ( df )", "predictions": ["do a build of the path"], "references": ["fetches the list of deputies for the current term ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 1387, "code": "def remove node ( self , node ) : self . nodes . remove ( node ) for x in xrange ( self . replicas ) : ring key = self . hash method ( b ( \"%s:%d\" % ( node , x ) ) ) self . ring . pop ( ring key ) self . sorted keys . remove ( ring key )", "predictions": ["build a node node node"], "references": ["removes node from the hash ring and its replicas ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 1388, "code": "def iter nodes ( self , key ) : if len ( self . ring ) == 0 : yield None , None node , pos = self . get node pos ( key ) for k in self . sorted keys [ pos : ] : yield k , self . ring [ k ]", "predictions": ["iterate over all dir dir ."], "references": ["given a string key it returns the nodes as a generator that can hold the key ."], "bleu": 0.035316782215334214, "rouge_l": 0.0800524934383202}
{"id": 1389, "code": "def mget ( self , keys , * args ) : args = list or args ( keys , args ) server keys = { } ret dict = { } for key in args : server name = self . get server name ( key ) server keys [ server name ] = server keys . get ( server name , [ ] ) server keys [ server name ] . append ( key ) for server name , sub keys in iteritems ( server keys ) : values = self . connections [ server name ] . mget ( sub keys ) ret dict . update ( dict ( zip ( sub keys , values ) ) ) result = [ ] for key in args : result . append ( ret dict . get ( key , None ) ) return result", "predictions": ["return list of reduce keys"], "references": ["returns a list of values ordered identically to keys"], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 1390, "code": "def mset ( self , mapping ) : servers = { } for key , value in mapping . items ( ) : server name = self . get server name ( key ) servers . setdefault ( server name , [ ] ) servers [ server name ] . append ( ( key , value ) ) for name , items in servers . items ( ) : self . connections [ name ] . mset ( dict ( items ) ) return True", "predictions": ["query elasticsearch elasticsearch for a mark"], "references": ["sets each key in the mapping dict to its corresponding value"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 1391, "code": "async def connect ( self ) : await self . lavalink . bot . wait until ready ( ) if self . ws and self . ws . open : log . debug ( 'Web Socket still open, closing...' ) await self . ws . close ( ) user id = self . lavalink . bot . user . id shard count = self . lavalink . bot . shard count or self . shards headers = { 'Authorization' : self . password , 'Num-Shards' : shard count , 'User-Id' : str ( user id ) } log . debug ( 'Preparing to connect to Lavalink' ) log . debug ( '    with URI: {}' . format ( self . uri ) ) log . debug ( '    with headers: {}' . format ( str ( headers ) ) ) log . info ( 'Connecting to Lavalink...' ) try : self . ws = await websockets . connect ( self . uri , loop = self . loop , extra headers = headers ) except OS Error as error : log . exception ( 'Failed to connect to Lavalink: {}' . format ( str ( error ) ) ) else : log . info ( 'Connected to Lavalink!' ) self . loop . create task ( self . listen ( ) ) version = self . ws . response headers . get ( 'Lavalink-Major-Version' , 2 ) try : self . lavalink . server version = int ( version ) except Value Error : self . lavalink . server version = 2 log . info ( 'Lavalink server version is {}' . format ( version ) ) if self . queue : log . info ( 'Replaying {} queued events...' . format ( len ( self . queue ) ) ) for task in self . queue : await self . send ( * * task )", "predictions": ["size the server connection to the server . . . . . . . ."], "references": ["establishes a connection to the lavalink server ."], "bleu": 0.18207052811092128, "rouge_l": 0.46003016591251883}
{"id": 1392, "code": "async def listen ( self ) : while not self . shutdown : try : data = json . loads ( await self . ws . recv ( ) ) except websockets . Connection Closed as error : log . warning ( 'Disconnected from Lavalink: {}' . format ( str ( error ) ) ) for g in self . lavalink . players . players . copy ( ) . keys ( ) : ws = self . lavalink . bot . connection . get websocket ( int ( g ) ) await ws . voice state ( int ( g ) , None ) self . lavalink . players . clear ( ) if self . shutdown : break if await self . attempt reconnect ( ) : return log . warning ( 'Unable to reconnect to Lavalink!' ) break op = data . get ( 'op' , None ) log . debug ( 'Received Web Socket data {}' . format ( str ( data ) ) ) if not op : return log . debug ( 'Received Web Socket message without op {}' . format ( str ( data ) ) ) if op == 'event' : log . debug ( 'Received event of type {}' . format ( data [ 'type' ] ) ) player = self . lavalink . players [ int ( data [ 'guild Id' ] ) ] event = None if data [ 'type' ] == 'Track End Event' : event = Track End Event ( player , data [ 'track' ] , data [ 'reason' ] ) elif data [ 'type' ] == 'Track Exception Event' : event = Track Exception Event ( player , data [ 'track' ] , data [ 'error' ] ) elif data [ 'type' ] == 'Track Stuck Event' : event = Track Stuck Event ( player , data [ 'track' ] , data [ 'threshold Ms' ] ) if event : await self . lavalink . dispatch event ( event ) elif op == 'player Update' : await self . lavalink . update state ( data ) elif op == 'stats' : self . lavalink . stats . update ( data ) await self . lavalink . dispatch event ( Stats Update Event ( self . lavalink . stats ) ) log . debug ( 'Closing Web Socket...' ) await self . ws . close ( )", "predictions": ["listen for events to the client fit"], "references": ["waits to receive a payload from the lavalink server and processes it ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 1393, "code": "def connected channel ( self ) : if not self . channel id : return None return self . lavalink . bot . get channel ( int ( self . channel id ) )", "predictions": ["the generate generate generate generate generate a generate context for this context from the context"], "references": ["returns the voice channel the player is connected to ."], "bleu": 0.09103526405546068, "rouge_l": 0.1659863945578231}
{"id": 1394, "code": "async def connect ( self , channel id : int ) : ws = self . lavalink . bot . connection . get websocket ( int ( self . guild id ) ) await ws . voice state ( self . guild id , str ( channel id ) )", "predictions": ["connect will be executed when a dst connects to a channel"], "references": ["connects to a voice channel ."], "bleu": 0.22416933501922287, "rouge_l": 0.4969450101832994}
{"id": 1395, "code": "async def disconnect ( self ) : if not self . is connected : return await self . stop ( ) ws = self . lavalink . bot . connection . get websocket ( int ( self . guild id ) ) await ws . voice state ( self . guild id , None )", "predictions": ["disconnects from the if it is currently assigned to this client logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger"], "references": ["disconnects from the voice channel if any ."], "bleu": 0.07678432706586173, "rouge_l": 0.2295390404515522}
{"id": 1396, "code": "def store ( self , key : object , value : object ) : self . user data . update ( { key : value } )", "predictions": ["generate a new user object"], "references": ["stores custom user data ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1397, "code": "def fetch ( self , key : object , default = None ) : return self . user data . get ( key , default )", "predictions": ["write a as a as a as a as a as a as a as a as a as a as a dictionary url url url"], "references": ["retrieves the related value from the stored user data ."], "bleu": 0.03925345689749394, "rouge_l": 0.0}
{"id": 1398, "code": "def add ( self , requester : int , track : dict ) : self . queue . append ( Audio Track ( ) . build ( track , requester ) )", "predictions": ["cleanup a new if it s a if it does not already exist path path path path path path path path"], "references": ["adds a track to the queue ."], "bleu": 0.05809665204409193, "rouge_l": 0.0785070785070785}
{"id": 1399, "code": "def add next ( self , requester : int , track : dict ) : self . queue . insert ( 0 , Audio Track ( ) . build ( track , requester ) )", "predictions": ["manage args in the queue"], "references": ["adds a track to beginning of the queue"], "bleu": 0.1971902775417715, "rouge_l": 0.2953995157384988}
{"id": 1400, "code": "def add at ( self , index : int , requester : int , track : dict ) : self . queue . insert ( min ( index , len ( self . queue ) - 1 ) , Audio Track ( ) . build ( track , requester ) )", "predictions": ["supported an cms entry"], "references": ["adds a track at a specific index in the queue ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 1401, "code": "async def play ( self , track index : int = 0 , ignore shuffle : bool = False ) : if self . repeat and self . current : self . queue . append ( self . current ) self . previous = self . current self . current = None self . position = 0 self . paused = False if not self . queue : await self . stop ( ) await self . lavalink . dispatch event ( Queue End Event ( self ) ) else : if self . shuffle and not ignore shuffle : track = self . queue . pop ( randrange ( len ( self . queue ) ) ) else : track = self . queue . pop ( min ( track index , len ( self . queue ) - 1 ) ) self . current = track await self . lavalink . ws . send ( op = 'play' , guild Id = self . guild id , track = track . track ) await self . lavalink . dispatch event ( Track Start Event ( self , track ) )", "predictions": ["asynchronously asynchronously asynchronously asynchronously"], "references": ["plays the first track in the queue if any or plays a track from the specified index in the queue ."], "bleu": 0.004309760539479251, "rouge_l": 0.0}
{"id": 1402, "code": "async def play now ( self , requester : int , track : dict ) : self . add next ( requester , track ) await self . play ( ignore shuffle = True )", "predictions": ["sample an rates type = a type = false = { rates } = { sample } = { sample } = { sample } = { sample } = {"], "references": ["add track and play it ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1403, "code": "async def play at ( self , index : int ) : self . queue = self . queue [ min ( index , len ( self . queue ) - 1 ) : len ( self . queue ) ] await self . play ( ignore shuffle = True )", "predictions": ["num an channels to a specific list . ."], "references": ["play the queue from a specific point . disregards tracks before the index ."], "bleu": 0.12109261383365659, "rouge_l": 0.3347050754458162}
{"id": 1404, "code": "async def play previous ( self ) : if not self . previous : raise No Previous Track self . queue . insert ( 0 , self . previous ) await self . play ( ignore shuffle = True )", "predictions": ["play the self . . . . . . . . . . . . . . . . . . . . ."], "references": ["plays previous track if it exist if it doesn t raises a noprevioustrack error ."], "bleu": 0.050661968099322066, "rouge_l": 0.05350877192982456}
{"id": 1405, "code": "async def stop ( self ) : await self . lavalink . ws . send ( op = 'stop' , guild Id = self . guild id ) self . current = None", "predictions": ["stop this interface to change the current ."], "references": ["stops the player if playing ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1406, "code": "async def set pause ( self , pause : bool ) : await self . lavalink . ws . send ( op = 'pause' , guild Id = self . guild id , pause = pause ) self . paused = pause", "predictions": ["set this guild s self remove the guild remove ."], "references": ["sets the player s paused state ."], "bleu": 0.14991106946711685, "rouge_l": 0.24302788844621517}
{"id": 1407, "code": "async def seek ( self , pos : int ) : await self . lavalink . ws . send ( op = 'seek' , guild Id = self . guild id , position = pos )", "predictions": ["return true if this = false otherwise in the = false in the = false in the = false in the = false in the = false in the = false"], "references": ["seeks to a given position in the track ."], "bleu": 0.0513487742994337, "rouge_l": 0.11101000909918107}
{"id": 1408, "code": "async def handle event ( self , event ) : if isinstance ( event , ( Track Stuck Event , Track Exception Event ) ) or isinstance ( event , Track End Event ) and event . reason == 'FINISHED' : await self . play ( )", "predictions": ["handle a single event ."], "references": ["makes the player play the next song from the queue if a song has finished or an issue occurred ."], "bleu": 0.01504254234731835, "rouge_l": 0.14437869822485208}
{"id": 1409, "code": "def get ( self , guild id ) : if guild id not in self . players : p = self . player ( lavalink = self . lavalink , guild id = guild id ) self . players [ guild id ] = p return self . players [ guild id ]", "predictions": ["get a single guild"], "references": ["returns a player from the cache or creates one if it does not exist ."], "bleu": 0.022969543400575367, "rouge_l": 0.0953125}
{"id": 1410, "code": "def remove ( self , guild id ) : if guild id in self . players : self . players [ guild id ] . cleanup ( ) del self . players [ guild id ]", "predictions": ["remove a guild for the experiment"], "references": ["removes a player from the current players ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 1411, "code": "async def play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track title = tracks [ 0 ] [ \"info\" ] [ \"title\" ] track uri = tracks [ 0 ] [ \"info\" ] [ \"uri\" ] embed . title = \"Track enqueued!\" embed . description = f'[{track title}]({track uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is playing : await player . play ( )", "predictions": ["play a player ."], "references": ["searches and plays a song from a given query ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 1412, "code": "async def seek ( self , ctx , * , time : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Not playing.' ) seconds = time rx . search ( time ) if not seconds : return await ctx . send ( 'You need to specify the amount of seconds to skip!' ) seconds = int ( seconds . group ( ) ) * 1000 if time . startswith ( '-' ) : seconds *= - 1 track time = player . position + seconds await player . seek ( track time ) await ctx . send ( f'Moved track to **{lavalink.Utils.format time(track time)}**' )", "predictions": ["seek to seconds ."], "references": ["seeks to a given position in a track ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 1413, "code": "async def skip ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Not playing.' ) await player . skip ( ) await ctx . send (", "predictions": ["skip this player s player"], "references": ["skips the current track ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1414, "code": "async def stop ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Not playing.' ) player . queue . clear ( ) await player . stop ( ) await ctx . send (", "predictions": ["stops the player ."], "references": ["stops the player and clears its queue ."], "bleu": 0.2601300475114444, "rouge_l": 0.6288659793814433}
{"id": 1415, "code": "async def now ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) song = 'Nothing' if player . current : position = lavalink . Utils . format time ( player . position ) if player . current . stream : duration = else : duration = lavalink . Utils . format time ( player . current . duration ) song = f'**[{player.current.title}]({player.current.uri})**\\n({position}/{duration})' embed = discord . Embed ( color = discord . Color . blurple ( ) , title = 'Now Playing' , description = song ) await ctx . send ( embed = embed )", "predictions": ["request an lavalink ."], "references": ["shows some stats about the currently playing song ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1416, "code": "async def queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\\'s nothing in the queue! Why not queue something?' ) items per page = 10 pages = math . ceil ( len ( player . queue ) / items per page ) start = ( page - 1 ) * items per page end = start + items per page queue list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue list += f'`{index + 1}.` [**{track.title}**]({track.uri})\\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\\n\\n{queue list}' ) embed . set footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )", "predictions": ["queue a queue of the players in the queue"], "references": ["shows the player s queue ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1417, "code": "async def volume ( self , ctx , volume : int = None ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not volume : return await ctx . send ( await player . set volume ( volume ) await ctx . send (", "predictions": ["changes the volume of a volume"], "references": ["changes the player s volume . must be between 0 and 150 . error handling for that is done by lavalink ."], "bleu": 0.02170621282299602, "rouge_l": 0.19426751592356686}
{"id": 1418, "code": "async def shuffle ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Nothing playing.' ) player . shuffle = not player . shuffle await ctx . send ( '\ud83d\udd00 | Shuffle ' +  ' n abled' if pl yer.sh u ffle el e 'd", "predictions": ["shuffle the player s player ."], "references": ["shuffles the player s queue ."], "bleu": 0.4347208719449915, "rouge_l": 0.6666666666666666}
{"id": 1419, "code": "async def repeat ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is playing : return await ctx . send ( 'Nothing playing.' ) player . repeat = not player . repeat await ctx . send ( '\ud83d\udd01 | Repeat ' +  ' n abled' if pl yer.re p eat el e 'd", "predictions": ["repeat a player s player"], "references": ["repeats the current song until the command is invoked again ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 1420, "code": "async def remove ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'Nothing queued.' ) if index > len ( player . queue ) or index < 1 : return await ctx . send ( f'Index has to be **between** 1 and {len(player.queue)}' ) index -= 1 removed = player . queue . pop ( index ) await ctx . send ( f'Removed **{removed.title}** from the queue.' )", "predictions": ["remove a player from the queue"], "references": ["removes an item from the player s queue with the given index ."], "bleu": 0.10286160177491631, "rouge_l": 0.29611650485436897}
{"id": 1421, "code": "async def disconnect ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is connected : return await ctx . send ( 'Not connected.' ) if not ctx . author . voice or ( player . is connected and ctx . author . voice . channel . id != int ( player . channel id ) ) : return await ctx . send ( 'You\\'re not in my voicechannel!' ) player . queue . clear ( ) await player . disconnect ( ) await ctx . send (", "predictions": ["disconnect from server ."], "references": ["disconnects the player from the voice channel and clears its queue ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 1422, "code": "async def ensure voice ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is connected : if not ctx . author . voice or not ctx . author . voice . channel : await ctx . send ( 'You aren\\'t connected to any voice channel.' ) raise commands . Command Invoke Error ( 'Author not connected to voice channel.' ) permissions = ctx . author . voice . channel . permissions for ( ctx . me ) if not permissions . connect or not permissions . speak : await ctx . send ( 'Missing permissions `CONNECT` and/or `SPEAK`.' ) raise commands . Command Invoke Error ( 'Bot has no permissions CONNECT and/or SPEAK' ) player . store ( 'channel' , ctx . channel . id ) await player . connect ( ctx . author . voice . channel . id ) else : if player . connected channel . id != ctx . author . voice . channel . id : return await ctx . send ( 'Join my voice channel!' )", "predictions": ["ensure voice is voice"], "references": ["a few checks to make sure the bot can join a voice channel ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 1423, "code": "def unregister hook ( self , func ) : if func in self . hooks : self . hooks . remove ( func )", "predictions": ["unregister a hook ."], "references": ["unregisters a hook . for further explanation please have a look at register_hook ."], "bleu": 0.054893547013735264, "rouge_l": 0.3029801324503311}
{"id": 1424, "code": "async def dispatch event ( self , event ) : log . debug ( 'Dispatching event of type {} to {} hooks' . format ( event . class . name , len ( self . hooks ) ) ) for hook in self . hooks : try : if asyncio . iscoroutinefunction ( hook ) : await hook ( event ) else : hook ( event ) except Exception as e : log . warning ( 'Encountered exception while dispatching an event to hook `{}` ({})' . format ( hook . name , str ( e ) ) ) if isinstance ( event , ( Track End Event , Track Exception Event , Track Stuck Event ) ) and event . player : await event . player . handle event ( event )", "predictions": ["dispatch a specific event to all its commands ."], "references": ["dispatches an event to all registered hooks ."], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 1425, "code": "async def update state ( self , data ) : guild id = int ( data [ 'guild Id' ] ) if guild id in self . players : player = self . players . get ( guild id ) player . position = data [ 'state' ] . get ( 'position' , 0 ) player . position timestamp = data [ 'state' ] [ 'time' ]", "predictions": ["update state of an player"], "references": ["updates a player s state when a payload with opcode playerupdate is received ."], "bleu": 0.04994299940831281, "rouge_l": 0.09697933227344992}
{"id": 1426, "code": "async def get tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content type = None )", "predictions": ["get tracks for a query ."], "references": ["returns a dictionary containing search results for a given query ."], "bleu": 0.1588696449629768, "rouge_l": 0.4468864468864468}
{"id": 1427, "code": "def destroy ( self ) : self . ws . destroy ( ) self . bot . remove listener ( self . on socket response ) self . hooks . clear ( )", "predictions": ["destroy the client ."], "references": ["destroys the lavalink client ."], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 1428, "code": "def build ( self , track , requester ) : try : self . track = track [ 'track' ] self . identifier = track [ 'info' ] [ 'identifier' ] self . can seek = track [ 'info' ] [ 'is Seekable' ] self . author = track [ 'info' ] [ 'author' ] self . duration = track [ 'info' ] [ 'length' ] self . stream = track [ 'info' ] [ 'is Stream' ] self . title = track [ 'info' ] [ 'title' ] self . uri = track [ 'info' ] [ 'uri' ] self . requester = requester return self except Key Error : raise Invalid Track ( 'An invalid track was passed.' )", "predictions": ["build a track object from a track object"], "references": ["returns an optional audiotrack ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1429, "code": "async def previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play previous ( ) except lavalink . No Previous Track : await ctx . send ( 'There is no previous song to play.' )", "predictions": ["get previous song of the player ."], "references": ["plays the previous song ."], "bleu": 0.2777619034011791, "rouge_l": 0.5154929577464789}
{"id": 1430, "code": "async def playnow ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue and not player . is playing : return await ctx . invoke ( self . play , query = query ) query = query . strip ( '<>' ) if not url rx . match ( query ) : query = f'ytsearch:{query}' results = await self . bot . lavalink . get tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found!' ) tracks = results [ 'tracks' ] track = tracks . pop ( 0 ) if results [ 'load Type' ] == 'PLAYLIST LOADED' : for track in tracks : player . add ( requester = ctx . author . id , track = track ) await player . play now ( requester = ctx . author . id , track = track )", "predictions": ["get player results from author ."], "references": ["plays immediately a song ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1431, "code": "async def playat ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if index < 1 : return await ctx . send ( 'Invalid specified index.' ) if len ( player . queue ) < index : return await ctx . send ( 'This index exceeds the queue\\'s length.' ) await player . play at ( index - 1 )", "predictions": ["play a player at the given index ."], "references": ["plays the queue from a specific point . disregards tracks before the index ."], "bleu": 0.11327490115090784, "rouge_l": 0.346590909090909}
{"id": 1432, "code": "async def find ( self , ctx , * , query ) : if not query . startswith ( 'ytsearch:' ) and not query . startswith ( 'scsearch:' ) : query = 'ytsearch:' + query results = await self . bot . lavalink . get tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found' ) tracks = results [ 'tracks' ] [ : 10 ] o = '' for index , track in enumerate ( tracks , start = 1 ) : track title = track [ \"info\" ] [ \"title\" ] track uri = track [ \"info\" ] [ \"uri\" ] o += f'`{index}.` [{track title}]({track uri})\\n' embed = discord . Embed ( color = discord . Color . blurple ( ) , description = o ) await ctx . send ( embed = embed )", "predictions": ["perform an event ."], "references": ["lists the first 10 search results from a given query ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 1433, "code": "def add document ( self , doc id , conn = None , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , language = None , * * fields ) : if conn is None : conn = self . redis if partial : replace = True args = [ self . ADD CMD , self . index name , doc id , score ] if nosave : args . append ( 'NOSAVE' ) if payload is not None : args . append ( 'PAYLOAD' ) args . append ( payload ) if replace : args . append ( 'REPLACE' ) if partial : args . append ( 'PARTIAL' ) if language : args += [ 'LANGUAGE' , language ] args . append ( 'FIELDS' ) args += list ( itertools . chain ( * fields . items ( ) ) ) return conn . execute command ( * args )", "predictions": ["add a document to an existing document ."], "references": ["internal add_document used for both batch and single doc indexing"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 1434, "code": "def load document ( self , id ) : fields = self . redis . hgetall ( id ) if six . PY3 : f2 = { to string ( k ) : to string ( v ) for k , v in fields . items ( ) } fields = f2 try : del fields [ 'id' ] except Key Error : pass return Document ( id = id , * * fields )", "predictions": ["load a document from the database"], "references": ["load a single document by id"], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 1435, "code": "def info ( self ) : res = self . redis . execute command ( 'FT.INFO' , self . index name ) it = six . moves . map ( to string , res ) return dict ( six . moves . zip ( it , it ) )", "predictions": ["get information about the index"], "references": ["get info an stats about the the current index including the number of documents memory consumption etc"], "bleu": 0.037035449344783235, "rouge_l": 0.3310719131614654}
{"id": 1436, "code": "def get args ( self ) : args = [ self . query string ] if self . no content : args . append ( 'NOCONTENT' ) if self . fields : args . append ( 'INFIELDS' ) args . append ( len ( self . fields ) ) args += self . fields if self . verbatim : args . append ( 'VERBATIM' ) if self . no stopwords : args . append ( 'NOSTOPWORDS' ) if self . filters : for flt in self . filters : assert isinstance ( flt , Filter ) args += flt . args if self . with payloads : args . append ( 'WITHPAYLOADS' ) if self . ids : args . append ( 'INKEYS' ) args . append ( len ( self . ids ) ) args += self . ids if self . slop >= 0 : args += [ 'SLOP' , self . slop ] if self . in order : args . append ( 'INORDER' ) if self . return fields : args . append ( 'RETURN' ) args . append ( len ( self . return fields ) ) args += self . return fields if self . sortby : assert isinstance ( self . sortby , Sortby Field ) args . append ( 'SORTBY' ) args += self . sortby . args if self . language : args += [ 'LANGUAGE' , self . language ] args += self . summarize fields + self . highlight fields args += [ \"LIMIT\" , self . offset , self . num ] return args", "predictions": ["get arguments for the command ."], "references": ["format the redis arguments for this query and return them"], "bleu": 0.16038842424444547, "rouge_l": 0.23921568627450981}
{"id": 1437, "code": "def between ( a , b , inclusive min = True , inclusive max = True ) : return Range Value ( a , b , inclusive min = inclusive min , inclusive max = inclusive max )", "predictions": ["between a and b with a and b"], "references": ["indicate that value is a numeric range"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1438, "code": "def geo ( lat , lon , radius , unit = 'km' ) : return Geo Value ( lat , lon , radius , unit )", "predictions": ["create a geo object with a geo and radius"], "references": ["indicate that value is a geo region"], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 1439, "code": "def reconstruct ( params ) : if isinstance ( params , dict ) : if ' class ' in params : cls = params [ ' class ' ] data = reconstruct ( params [ 'params' ] ) return cls ( * * data ) else : data = dict ( ) for key , value in six . iteritems ( params ) : data [ key ] = reconstruct ( value ) return data elif isinstance ( params , ( list , tuple ) ) : return [ reconstruct ( v ) for v in params ] else : return params", "predictions": ["reconstruct a list of dictionaries into a new dict ."], "references": ["reconstruct a transformation or pipeline given a parameter dump ."], "bleu": 0.18850319022747347, "rouge_l": 0.4}
{"id": 1440, "code": "def get param names ( cls ) : init = cls . init args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise Runtime Error ( 'Base Transformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args", "predictions": ["get parameter event event event event event event event event event event event ."], "references": ["get the list of parameter names for the object"], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 1441, "code": "def get params ( self ) : out = { } out [ ' class ' ] = self . class out [ 'params' ] = dict ( steps = [ ] ) for name , step in self . steps : out [ 'params' ] [ 'steps' ] . append ( [ name , step . get params ( deep = True ) ] ) return out", "predictions": ["get parameters of this object"], "references": ["get the parameters for this object . returns as a dict ."], "bleu": 0.10067278896273972, "rouge_l": 0.4380610412926392}
{"id": 1442, "code": "def recursive transform ( self , jam , steps ) : if len ( steps ) > 0 : head transformer = steps [ 0 ] [ 1 ] for t jam in head transformer . transform ( jam ) : for q in self . recursive transform ( t jam , steps [ 1 : ] ) : yield q else : yield jam", "predictions": ["yield the remove transform . ."], "references": ["a recursive transformation pipeline"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1443, "code": "def serial transform ( self , jam , steps ) : if six . PY2 : attr = 'next' else : attr = ' next ' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next jam in nexts : yield next jam ( ) except Stop Iteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )", "predictions": ["called by a def to play a def player player player player player player player player ."], "references": ["a serial transformation union"], "bleu": 0.07223943354597204, "rouge_l": 0.10720562390158171}
{"id": 1444, "code": "def file empty ( fp ) : if six . PY2 : contents = fp . read ( ) fp . seek ( 0 ) return not bool ( contents ) else : return not fp . peek ( )", "predictions": ["check if a file is seek to a file"], "references": ["determine if a file is empty or not ."], "bleu": 0.392814650900513, "rouge_l": 0.4444444444444444}
{"id": 1445, "code": "def storage ( self , provider = 'osfstorage' ) : stores = self . json ( self . get ( self . storages url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . get attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise Runtime Error ( \"Project has no storage \" \"provider '{}'\" . format ( provider ) )", "predictions": ["retrieve all vms for a given provider"], "references": ["return storage provider ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1446, "code": "def storages ( self ) : stores = self . json ( self . get ( self . storages url ) , 200 ) stores = stores [ 'data' ] for store in stores : yield Storage ( store , self . session )", "predictions": ["return all def def ."], "references": ["iterate over all storages for this projects ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1447, "code": "def remove ( self ) : response = self . delete ( self . delete url ) if response . status code != 204 : raise Runtime Error ( 'Could not delete {}.' . format ( self . path ) )", "predictions": ["def a single file from the storage . ."], "references": ["remove this file from the remote storage ."], "bleu": 0.31239399369202553, "rouge_l": 0.594541910331384}
{"id": 1448, "code": "def init ( args ) : config = config from file ( ) config = configparser . Config Parser ( ) config . add section ( 'osf' ) if 'username' not in config . keys ( ) : config . set ( 'osf' , 'username' , '' ) else : config . set ( 'osf' , 'username' , config [ 'username' ] ) if 'project' not in config . keys ( ) : config . set ( 'osf' , 'project' , '' ) else : config . set ( 'osf' , 'project' , config [ 'project' ] ) print ( 'Provide a username for the config file [current username: {}]:' . format ( config . get ( 'osf' , 'username' ) ) ) username = input ( ) if username : config . set ( 'osf' , 'username' , username ) print ( 'Provide a project for the config file [current project: {}]:' . format ( config . get ( 'osf' , 'project' ) ) ) project = input ( ) if project : config . set ( 'osf' , 'project' , project ) cfgfile = open ( \".osfcli.config\" , \"w\" ) config . write ( cfgfile ) cfgfile . close ( )", "predictions": ["create a new start start with the specified ctx"], "references": ["initialize or edit an existing . osfcli . config file ."], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 1449, "code": "def login ( self , username , password = None , token = None ) : self . session . basic auth ( username , password )", "predictions": ["def to def ."], "references": ["login user for protected api calls ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1450, "code": "def project ( self , project id ) : type = self . guid ( project id ) url = self . build url ( type , project id ) if type in Project . types : return Project ( self . json ( self . get ( url ) , 200 ) , self . session ) raise OSF Exception ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project id , type ) )", "predictions": ["lavalink a def = true"], "references": ["fetch project project_id ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1451, "code": "def guid ( self , guid ) : return self . json ( self . get ( self . build url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]", "predictions": ["returns a specific def"], "references": ["determines jsonapi type for provided guid"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 1452, "code": "def json ( self , response , status code ) : if isinstance ( status code , numbers . Integral ) : status code = ( status code , ) if response . status code in status code : return response . json ( ) else : raise Runtime Error ( \"Response has status \" \"code {} not {}\" . format ( response . status code , status code ) )", "predictions": ["raises a def if the response is not available ."], "references": ["extract json from response if status_code matches ."], "bleu": 0.14991106946711685, "rouge_l": 0.22676579925650556}
{"id": 1453, "code": "def follow next ( self , url ) : response = self . json ( self . get ( url ) , 200 ) data = response [ 'data' ] next url = self . get attribute ( response , 'links' , 'next' ) while next url is not None : response = self . json ( self . get ( next url ) , 200 ) data . extend ( response [ 'data' ] ) next url = self . get attribute ( response , 'links' , 'next' ) return data", "predictions": ["def the disconnect to the disconnect ."], "references": ["follow the next link on paginated results ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1454, "code": "def clear ( self ) : self . desc = { } for key , value in merge . DEFAULT PROJECT . items ( ) : if key not in self . HIDDEN : self . desc [ key ] = type ( value ) ( )", "predictions": ["def all the properties"], "references": ["clear description to default values"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 1455, "code": "def error ( self , fail = True , action = '' ) : e = 'There was an unknown error communicating with the device.' if action : e = 'While %s: %s' % ( action , e ) log . error ( e ) if fail : raise IO Error ( e )", "predictions": ["log an unregister unregister task ."], "references": ["should be private method"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1456, "code": "def crop ( image , top offset = 0 , left offset = 0 , bottom offset = 0 , right offset = 0 ) : if bottom offset or top offset or left offset or right offset : width , height = image . size box = ( left offset , top offset , width - right offset , height - bottom offset ) image = image . crop ( box = box ) return image", "predictions": ["def an event to a specific event format"], "references": ["return an image cropped on top bottom left or right ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 1457, "code": "def resize ( image , x , y , stretch = False , top = None , left = None , mode = 'RGB' , resample = None ) : if x <= 0 : raise Value Error ( 'x must be greater than zero' ) if y <= 0 : raise Value Error ( 'y must be greater than zero' ) from PIL import Image resample = Image . ANTIALIAS if resample is None else resample if not isinstance ( resample , numbers . Number ) : try : resample = getattr ( Image , resample . upper ( ) ) except : raise Value Error ( \"(1) Didn't understand resample=%s\" % resample ) if not isinstance ( resample , numbers . Number ) : raise Value Error ( \"(2) Didn't understand resample=%s\" % resample ) size = x , y if stretch : return image . resize ( size , resample = resample ) result = Image . new ( mode , size ) ratios = [ d1 / d2 for d1 , d2 in zip ( size , image . size ) ] if ratios [ 0 ] < ratios [ 1 ] : new size = ( size [ 0 ] , int ( image . size [ 1 ] * ratios [ 0 ] ) ) else : new size = ( int ( image . size [ 0 ] * ratios [ 1 ] ) , size [ 1 ] ) image = image . resize ( new size , resample = resample ) if left is None : box x = int ( ( x - new size [ 0 ] ) / 2 ) elif left : box x = 0 else : box x = x - new size [ 0 ] if top is None : box y = int ( ( y - new size [ 1 ] ) / 2 ) elif top : box y = 0 else : box y = y - new size [ 1 ] result . paste ( image , box = ( box x , box y ) ) return result", "predictions": ["def an state by a given state"], "references": ["return an image resized ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1458, "code": "def draw Circle ( self , x0 , y0 , r , color = None ) : md . draw circle ( self . set , x0 , y0 , r , color )", "predictions": ["def a format chart"], "references": ["draw a circle in an rgb color with center x0 y0 and radius r ."], "bleu": 0.022969543400575367, "rouge_l": 0.0953125}
{"id": 1459, "code": "def fill Circle ( self , x0 , y0 , r , color = None ) : md . fill circle ( self . set , x0 , y0 , r , color )", "predictions": ["destroy a circle with a circle"], "references": ["draw a filled circle in an rgb color with center x0 y0 and radius r ."], "bleu": 0.04961591899093189, "rouge_l": 0.25206611570247933}
{"id": 1460, "code": "def fill Screen ( self , color = None ) : md . fill rect ( self . set , 0 , 0 , self . width , self . height , color )", "predictions": ["build the waveform rect with the given requester requester requester requester . . . . . ."], "references": ["fill the matrix with the given rgb color"], "bleu": 0.14216645907653844, "rouge_l": 0.34221598877980364}
{"id": 1461, "code": "def set project ( self , project ) : def visit ( x ) : set project = getattr ( x , 'set project' , None ) if set project : set project ( project ) values = getattr ( x , 'values' , lambda : ( ) ) for v in values ( ) : visit ( v ) visit ( self . routing )", "predictions": ["def the previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous"], "references": ["set the base project for routing ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 1462, "code": "def set ( self , ring , angle , color ) : pixel = self . angle To Pixel ( angle , ring ) self . set base ( pixel , color )", "predictions": ["def the vector s * * * * * * * * * to * * * ctx * * * * * * * * * * * * *"], "references": ["set pixel to rgb color tuple"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 1463, "code": "def get ( self , ring , angle ) : pixel = self . angle To Pixel ( angle , ring ) return self . get base ( pixel )", "predictions": ["bot to def at a given ctx"], "references": ["get rgb color tuple of color at index pixel"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1464, "code": "def color scale ( color , level ) : return tuple ( [ int ( i * level ) >> 8 for i in list ( color ) ] )", "predictions": ["find a def tuple with a given level ."], "references": ["scale rgb tuple by level 0 - 256"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 1465, "code": "def save ( self , project file = '' ) : self . request project file ( project file ) data file . dump ( self . desc . as dict ( ) , self . project file )", "predictions": ["add the project to the project file"], "references": ["save the description as a yml file . prompt if no file given ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 1466, "code": "def run ( self , next task ) : self . event . wait ( ) self . task ( ) self . event . clear ( ) next task . event . set ( )", "predictions": ["load the next and redis tasks fields fields fields ."], "references": ["wait for the event run the task trigger the next task ."], "bleu": 0.14595947916189678, "rouge_l": 0.2683284457478006}
{"id": 1467, "code": "def report ( function , * args , * * kwds ) : try : function ( * args , * * kwds ) except Exception : traceback . print exc ( )", "predictions": ["info that a self self index is available"], "references": ["run a function catch report and discard exceptions"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1468, "code": "def receive ( self , msg ) : msg = self . convert ( msg ) if msg is None : return str msg = self . verbose and self . msg to str ( msg ) if self . verbose and log . is debug ( ) : log . debug ( 'Message %s' , str msg ) if self . pre routing : self . pre routing . receive ( msg ) receiver , msg = self . routing . receive ( msg ) if receiver : receiver . receive ( msg ) if self . verbose : log . info ( 'Routed message %s (%s) to %s' , str msg [ : 128 ] , msg , repr ( receiver ) )", "predictions": ["handle an udp message message ."], "references": ["receive a message from the input source and perhaps raise an exception ."], "bleu": 0.08180282100568384, "rouge_l": 0.19741100323624597}
{"id": 1469, "code": "def set RGB ( self , pixel , r , g , b ) : self . set base ( pixel , ( r , g , b ) )", "predictions": ["between two rows of a inclusive"], "references": ["set single pixel using individual rgb values instead of tuple"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1470, "code": "def set HSV ( self , pixel , hsv ) : color = conversions . hsv2rgb ( hsv ) self . set base ( pixel , color )", "predictions": ["geo a radius with a radius"], "references": ["set single pixel to hsv tuple"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1471, "code": "def fill ( self , color , start = 0 , end = - 1 ) : start = max ( start , 0 ) if end < 0 or end >= self . num LE Ds : end = self . num LE Ds - 1 for led in range ( start , end + 1 ) : self . set base ( led , color )", "predictions": ["reconstruct the value of a given color return it ."], "references": ["fill the entire strip with rgb color tuple"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 1472, "code": "def fill RGB ( self , r , g , b , start = 0 , end = - 1 ) : self . fill ( ( r , g , b ) , start , end )", "predictions": ["fill a 64 64 - bit color with a given vertex"], "references": ["fill entire strip by giving individual rgb values instead of tuple"], "bleu": 0.11390778025531027, "rouge_l": 0.09090909090909091}
{"id": 1473, "code": "def fill HSV ( self , hsv , start = 0 , end = - 1 ) : self . fill ( conversions . hsv2rgb ( hsv ) , start , end )", "predictions": ["fill the hsv with a given hsv"], "references": ["fill the entire strip with hsv color tuple"], "bleu": 0.240785655451027, "rouge_l": 0.5269978401727862}
{"id": 1474, "code": "def single ( method ) : @ functools . wraps ( method ) def single ( self , address , value = None ) : address = urllib . parse . unquote plus ( address ) try : error = NO PROJECT ERROR if not self . project : raise Value Error error = BAD ADDRESS ERROR ed = editor . Editor ( address , self . project ) if value is None : error = BAD GETTER ERROR result = method ( self , ed ) else : error = BAD SETTER ERROR result = method ( self , ed , value ) result = { 'value' : result } except Exception as e : traceback . print exc ( ) msg = '%s\\n%s' % ( error . format ( * * locals ( ) ) , e ) result = { 'error' : msg } return flask . jsonify ( result ) return single", "predictions": ["decorator for single method in project ."], "references": ["decorator for restserver methods that take a single address"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 1475, "code": "def multi ( method ) : @ functools . wraps ( method ) def multi ( self , address = '' ) : values = flask . request . values address = urllib . parse . unquote plus ( address ) if address and values and not address . endswith ( '.' ) : address += '.' result = { } for a in values or '' : try : if not self . project : raise Value Error ( 'No Project is currently loaded' ) ed = editor . Editor ( address + a , self . project ) result [ address + a ] = { 'value' : method ( self , ed , a ) } except : if self . project : traceback . print exc ( ) result [ address + a ] = { 'error' : 'Could not multi addr %s' % a } return flask . jsonify ( result ) return multi", "predictions": ["log a method on a flask editor ."], "references": ["decorator for restserver methods that take multiple addresses"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1476, "code": "def set RGB ( self , pixel , r , g , b ) : self . set ( pixel , ( r , g , b ) )", "predictions": ["set a color color for a given pixel"], "references": ["set single pixel using individual rgb values instead of tuple"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 1477, "code": "def set HSV ( self , pixel , hsv ) : color = conversions . hsv2rgb ( hsv ) self . set ( pixel , color )", "predictions": ["set a pixel s color"], "references": ["set single pixel to hsv tuple"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 1478, "code": "def draw circle ( setter , x0 , y0 , r , color = None ) : f = 1 - r dd F x = 1 dd F y = - 2 * r x = 0 y = r setter ( x0 , y0 + r , color ) setter ( x0 , y0 - r , color ) setter ( x0 + r , y0 , color ) setter ( x0 - r , y0 , color ) while x < y : if f >= 0 : y -= 1 dd F y += 2 f += dd F y x += 1 dd F x += 2 f += dd F x setter ( x0 + x , y0 + y , color ) setter ( x0 - x , y0 + y , color ) setter ( x0 + x , y0 - y , color ) setter ( x0 - x , y0 - y , color ) setter ( x0 + y , y0 + x , color ) setter ( x0 - y , y0 + x , color ) setter ( x0 + y , y0 - x , color ) setter ( x0 - y , y0 - x , color )", "predictions": ["draw a circle given its setter and r r r"], "references": ["draws a circle at point x0 y0 with radius r of the specified rgb color"], "bleu": 0.10812944164434664, "rouge_l": 0.23164556962025318}
{"id": 1479, "code": "def fill circle ( setter , x0 , y0 , r , color = None ) : draw fast vline ( setter , x0 , y0 - r , 2 * r + 1 , color ) fill circle helper ( setter , x0 , y0 , r , 3 , 0 , color )", "predictions": ["fill a circle with a circle"], "references": ["draws a filled circle at point x0 y0 with radius r and specified color"], "bleu": 0.06924459302580939, "rouge_l": 0.2798165137614679}
{"id": 1480, "code": "def bresenham line ( setter , x0 , y0 , x1 , y1 , color = None , color Func = None ) : steep = abs ( y1 - y0 ) > abs ( x1 - x0 ) if steep : x0 , y0 = y0 , x0 x1 , y1 = y1 , x1 if x0 > x1 : x0 , x1 = x1 , x0 y0 , y1 = y1 , y0 dx = x1 - x0 dy = abs ( y1 - y0 ) err = dx / 2 if y0 < y1 : ystep = 1 else : ystep = - 1 count = 0 for x in range ( x0 , x1 + 1 ) : if color Func : color = color Func ( count ) count += 1 if steep : setter ( y0 , x , color ) else : setter ( x , y0 , color ) err -= dy if err < 0 : y0 += ystep err += dx", "predictions": ["draw a bresenham line"], "references": ["draw line from point x0 y0 to x 1 y1 . will draw beyond matrix bounds ."], "bleu": 0.015417996259849322, "rouge_l": 0.17134831460674158}
{"id": 1481, "code": "def draw rect ( setter , x , y , w , h , color = None , aa = False ) : draw fast hline ( setter , x , y , w , color , aa ) draw fast hline ( setter , x , y + h - 1 , w , color , aa ) draw fast vline ( setter , x , y , h , color , aa ) draw fast vline ( setter , x + w - 1 , y , h , color , aa )", "predictions": ["draw a circle on a circle"], "references": ["draw rectangle with top - left corner at x y width w and height h"], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 1482, "code": "def fill rect ( setter , x , y , w , h , color = None , aa = False ) : for i in range ( x , x + w ) : draw fast vline ( setter , i , y , h , color , aa )", "predictions": ["fill a circle with a given color ."], "references": ["draw solid rectangle with top - left corner at x y width w and height h"], "bleu": 0.0589953212431261, "rouge_l": 0.07860824742268041}
{"id": 1483, "code": "def draw triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : draw line ( setter , x0 , y0 , x1 , y1 , color , aa ) draw line ( setter , x1 , y1 , x2 , y2 , color , aa ) draw line ( setter , x2 , y2 , x0 , y0 , color , aa )", "predictions": ["draw a triangle at a given position"], "references": ["draw triangle with points x0 y0 - x1 y1 - x2 y2"], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 1484, "code": "def fill triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : a = b = y = last = 0 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y1 > y2 : y2 , y1 = y1 , y2 x2 , x1 = x1 , x2 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y0 == y2 : a = b = x0 if x1 < a : a = x1 elif x1 > b : b = x1 if x2 < a : a = x2 elif x2 > b : b = x2 draw fast hline ( setter , a , y0 , b - a + 1 , color , aa ) dx01 = x1 - x0 dy01 = y1 - y0 dx02 = x2 - x0 dy02 = y2 - y0 dx12 = x2 - x1 dy12 = y2 - y1 sa = 0 sb = 0 if y1 == y2 : last = y1 else : last = y1 - 1 for y in range ( y , last + 1 ) : a = x0 + sa / dy01 b = x0 + sb / dy02 sa += dx01 sb += dx02 if a > b : a , b = b , a draw fast hline ( setter , a , y , b - a + 1 , color , aa ) sa = dx12 * ( y - y1 ) sb = dx02 * ( y - y0 ) for y in range ( y , y2 + 1 ) : a = x1 + sa / dy12 b = x0 + sb / dy02 sa += dx12 sb += dx02 if a > b : a , b = b , a draw fast hline ( setter , a , y , b - a + 1 , color , aa )", "predictions": ["fill a triangle with a triangle"], "references": ["draw solid triangle with points x0 y0 - x1 y1 - x2 y2"], "bleu": 0.09052970298747198, "rouge_l": 0.19741100323624597}
{"id": 1485, "code": "def all named colors ( ) : yield from TO COLOR USER . items ( ) for name , color in TO COLOR . items ( ) : if name not in TO COLOR USER : yield name , color", "predictions": ["return a generator of all named colors ."], "references": ["return an iteration over all name color pairs in tables"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 1486, "code": "def contains ( x ) : if isinstance ( x , str ) : x = canonical name ( x ) return x in TO COLOR USER or x in TO COLOR else : x = tuple ( x ) return x in TO NAME USER or x in TO NAME", "predictions": ["test if x is a valid type ."], "references": ["return true if this string or integer tuple appears in tables"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 1487, "code": "def make segments ( strip , length ) : if len ( strip ) % length : raise Value Error ( 'The length of strip must be a multiple of length' ) s = [ ] try : while True : s . append ( s [ - 1 ] . next ( length ) if s else Segment ( strip , length ) ) except Value Error : return s", "predictions": ["make a segments from a string of length length ."], "references": ["return a list of segments that evenly split the strip ."], "bleu": 0.1434272783816789, "rouge_l": 0.28328173374613}
{"id": 1488, "code": "def next ( self , length ) : return Segment ( self . strip , length , self . offset + self . length )", "predictions": ["gets the next entry in the sequence ."], "references": ["return a new segment starting right after self in the same buffer ."], "bleu": 0.12139281957861149, "rouge_l": 0.2739520958083832}
{"id": 1489, "code": "def start ( self , threaded = None ) : if threaded is not None : self . threaded = threaded run = { 'run' : { 'threaded' : False } } self . project = project . project ( self . desc , run , root file = self . project file ) self . run = self . project . run self . runner . start ( self . threaded )", "predictions": ["start the project ."], "references": ["creates and starts the project ."], "bleu": 0.4056114983537769, "rouge_l": 0.5791139240506329}
{"id": 1490, "code": "def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . runner . stop ( ) if self . project : self . project . stop ( ) self . project = None", "predictions": ["stop the thread ."], "references": ["stop the builder if it s running ."], "bleu": 0.18693159143202892, "rouge_l": 0.47164948453608246}
{"id": 1491, "code": "def simpixel ( new = 0 , autoraise = True ) : simpixel driver . open browser ( new = new , autoraise = autoraise )", "predictions": ["copy the given browser to the given browser ."], "references": ["open an instance of simpixel in the browser"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 1492, "code": "def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )", "predictions": ["r euclidean distance between two matrices ."], "references": ["square of the euclidean distance"], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 1493, "code": "def set one ( desc , name , value ) : old value = desc . get ( name ) if old value is None : raise Key Error ( 'No section \"%s\"' % name ) if value is None : value = type ( old value ) ( ) elif name in CLASS SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class name . class name ( value ) } elif not isinstance ( value , dict ) : raise Type Error ( 'Expected dict, str or type, got \"%s\"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise Type Error ( 'Expected shape, got \"%s\"' % value ) elif type ( old value ) is not type ( value ) : raise Type Error ( 'Expected %s but got \"%s\" of type %s' % ( type ( old value ) , value , type ( value ) ) ) desc [ name ] = value", "predictions": ["set a section value"], "references": ["set one section in a project description"], "bleu": 0.20183609024241697, "rouge_l": 0.346590909090909}
{"id": 1494, "code": "def update ( desc , other = None , * * kwds ) : other = other and as dict ( other ) or { } for i in other , kwds : for k , v in i . items ( ) : if isinstance ( v , dict ) : old v = desc [ k ] for k2 , v2 in v . items ( ) : if v2 is None : old v . pop ( k2 , None ) else : old v [ k2 ] = v2 else : set one ( desc , k , v )", "predictions": ["update a dictionary from a dict with a dict"], "references": ["update sections in a project description"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1495, "code": "def to color ( c ) : if isinstance ( c , numbers . Number ) : return c , c , c if not c : raise Value Error ( 'Cannot create color from empty \"%s\"' % c ) if isinstance ( c , str ) : return name to color ( c ) if isinstance ( c , list ) : c = tuple ( c ) if isinstance ( c , tuple ) : if len ( c ) > 3 : return c [ : 3 ] while len ( c ) < 3 : c += ( c [ - 1 ] , ) return c raise Value Error ( 'Cannot create color from \"%s\"' % c )", "predictions": ["convert a color to a color color"], "references": ["try to coerce the argument into a color - a 3 - tuple of numbers -"], "bleu": 0.07678812443288274, "rouge_l": 0.2436750998668442}
{"id": 1496, "code": "def convert mode ( image , mode = 'RGB' ) : deprecated . deprecated ( 'util.gif.convert model' ) return image if ( image . mode == mode ) else image . convert ( mode = mode )", "predictions": ["deprecated . this function will deprecated deprecated ."], "references": ["return an image in the given mode ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1497, "code": "def image to colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image to colorlist' ) return container ( convert mode ( image ) . getdata ( ) )", "predictions": ["deprecated . use deprecated instead ."], "references": ["given a pil . image returns a colorlist of its pixels ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 1498, "code": "def animated gif to colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated gif to colorlists' ) from PIL import Image Sequence it = Image Sequence . Iterator ( image ) return [ image to colorlist ( i , container ) for i in it ]", "predictions": ["deprecated . use deprecated instead ."], "references": ["given an animated gif return a list with a colorlist for each frame ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 1499, "code": "def opener ( ip address , port , delay = 1 ) : global WEBPAGE OPENED if WEBPAGE OPENED : return WEBPAGE OPENED = True raw opener ( ip address , port , delay )", "predictions": ["generate a webpage instance"], "references": ["wait a little and then open a web browser page for the control panel ."], "bleu": 0.022969543400575367, "rouge_l": 0.0953125}
{"id": 1500, "code": "def raw opener ( ip address , port , delay = 1 ) : def target ( ) : time . sleep ( delay ) url = 'http://%s:%d' % ( ip address , port ) webbrowser . open ( url , new = 0 , autoraise = True ) threading . Thread ( target = target , daemon = True ) . start ( )", "predictions": ["start a raw ip and start the raw ip address ."], "references": ["wait a little and then open a web browser page for the control panel ."], "bleu": 0.09956647337521526, "rouge_l": 0.2993865030674847}
{"id": 1501, "code": "def start ( self , threaded ) : self . stop ( ) self . class . INSTANCE = weakref . ref ( self ) self . is running = True if threaded : self . thread = runnable . Loop Thread ( ) self . thread . run once = self . target self . thread . start ( ) else : self . target ( )", "predictions": ["start the thread ."], "references": ["creates and starts the project ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 1502, "code": "def show image ( setter , width , height , image path = '' , image obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : bgcolor = color scale ( bgcolor , brightness ) img = image obj if image path and not img : from PIL import Image img = Image . open ( image path ) elif not img : raise Value Error ( 'Must provide either image path or image obj' ) w = min ( width - offset [ 0 ] , img . size [ 0 ] ) h = min ( height - offset [ 1 ] , img . size [ 1 ] ) ox = offset [ 0 ] oy = offset [ 1 ] for x in range ( ox , w + ox ) : for y in range ( oy , h + oy ) : r , g , b , a = ( 0 , 0 , 0 , 255 ) rgba = img . getpixel ( ( x - ox , y - oy ) ) if isinstance ( rgba , int ) : raise Value Error ( 'Image must be in RGB or RGBA format!' ) if len ( rgba ) == 3 : r , g , b = rgba elif len ( rgba ) == 4 : r , g , b , a = rgba else : raise Value Error ( 'Image must be in RGB or RGBA format!' ) if a == 0 : r , g , b = bgcolor else : r , g , b = color scale ( ( r , g , b ) , a ) if brightness != 255 : r , g , b = color scale ( ( r , g , b ) , brightness ) setter ( x , y , ( r , g , b ) )", "predictions": ["show an image image"], "references": ["display an image on a matrix ."], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 1503, "code": "def show Image ( layout , image Path = \"\" , image Obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : if not isinstance ( layout , Matrix ) : raise Runtime Error ( \"Must use Matrix with show Image!\" ) layout . all off ( ) return show image ( layout . set , layout . width , layout . height , image Path , image Obj , offset , bgcolor , brightness )", "predictions": ["show the image in a layout ."], "references": ["display an image on the matrix"], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 1504, "code": "def load Image ( layout , image Path = \"\" , image Obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : if not isinstance ( layout , Matrix ) : raise Runtime Error ( \"Must use Matrix with load Image!\" ) texture = [ [ COLORS . Off for x in range ( layout . width ) ] for y in range ( layout . height ) ] def setter ( x , y , pixel ) : if y >= 0 and x >= 0 : texture [ y ] [ x ] = pixel show image ( setter , layout . width , layout . height , image Path , image Obj , offset , bgcolor , brightness ) return texture", "predictions": ["fill a texture with a pixel"], "references": ["display an image on the matrix"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1505, "code": "def serpentine x ( x , y , matrix ) : if y % 2 : return matrix . columns - 1 - x , y return x , y", "predictions": ["convert a x to a hsv hsv hsv hsv hsv hsv ."], "references": ["every other row is indexed in reverse ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 1506, "code": "def serpentine y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y", "predictions": ["convert a y to a @ - 1 - d @ value ."], "references": ["every other column is indexed in reverse ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 1507, "code": "def colors no palette ( colors = None , * * kwds ) : if isinstance ( colors , str ) : colors = split colors ( colors ) else : colors = to triplets ( colors or ( ) ) colors = ( color ( c ) for c in colors or ( ) ) return palette . Palette ( colors , * * kwds )", "predictions": ["palette multi color color to method"], "references": ["return a palette but don t take into account pallete names ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 1508, "code": "def make matrix coord map ( dx , dy , serpentine = True , offset = 0 , rotation = 0 , y flip = False ) : result = [ ] for y in range ( dy ) : if not serpentine or y % 2 == 0 : result . append ( [ ( dx * y ) + x + offset for x in range ( dx ) ] ) else : result . append ( [ dx * ( y + 1 ) - 1 - x + offset for x in range ( dx ) ] ) result = rotate and flip ( result , rotation , y flip ) return result", "predictions": ["set the matrix matrix self . to a self ."], "references": ["helper method to generate x y coordinate maps for strips"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 1509, "code": "def make object ( * args , typename = None , python path = None , datatype = None , * * kwds ) : datatype = datatype or import symbol ( typename , python path ) field types = getattr ( datatype , 'FIELD TYPES' , fields . FIELD TYPES ) return datatype ( * args , * * fields . component ( kwds , field types ) )", "predictions": ["set up an object object object object instance ."], "references": ["make an object from a symbol ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 1510, "code": "def index ( self , i , length = None ) : if self . begin <= i <= self . end : index = i - self . BEGIN - self . offset if length is None : length = self . full range ( ) else : length = min ( length , self . full range ( ) ) if 0 <= index < length : return index", "predictions": ["returns the draw draw draw draw at i i"], "references": ["return an integer index or none"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1511, "code": "def copy ( self ) : return self . class ( amount = self [ \"amount\" ] , asset = self [ \"asset\" ] . copy ( ) , blockchain instance = self . blockchain , )", "predictions": ["creates a shallow fill policy y0 y0 y0 y0 y0 y0 y0 y0 y0 y0 y0 ."], "references": ["copy the instance and make sure not to use a reference"], "bleu": 0.07223943354597204, "rouge_l": 0.07429963459196103}
{"id": 1512, "code": "def upgrade ( self ) : assert callable ( self . blockchain . upgrade account ) return self . blockchain . upgrade account ( account = self )", "predictions": ["bresenham the sensor x1 x1 x1 x1 x1 x1 x1 ."], "references": ["upgrade account to life time member"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1513, "code": "def whitelist ( self , account ) : assert callable ( self . blockchain . account whitelist ) return self . blockchain . account whitelist ( account , lists = [ \"white\" ] , account = self )", "predictions": ["draw the account of an account"], "references": ["add an other account to the whitelist of this account"], "bleu": 0.14925824694561, "rouge_l": 0.3588235294117647}
{"id": 1514, "code": "def blacklist ( self , account ) : assert callable ( self . blockchain . account whitelist ) return self . blockchain . account whitelist ( account , lists = [ \"black\" ] , account = self )", "predictions": ["aa the account of an account"], "references": ["add an other account to the blacklist of this account"], "bleu": 0.14925824694561, "rouge_l": 0.3588235294117647}
{"id": 1515, "code": "def nolist ( self , account ) : assert callable ( self . blockchain . account whitelist ) return self . blockchain . account whitelist ( account , lists = [ ] , account = self )", "predictions": ["= color of the account"], "references": ["remove an other account from any list of this account"], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 1516, "code": "def recover public key ( digest , signature , i , message = None ) : curve = ecdsa . SECP256k1 . curve G = ecdsa . SECP256k1 . generator order = ecdsa . SECP256k1 . order yp = i % 2 r , s = ecdsa . util . sigdecode string ( signature , order ) x = r + ( i // 2 ) * order alpha = ( ( x * x * x ) + ( curve . a ( ) * x ) + curve . b ( ) ) % curve . p ( ) beta = ecdsa . numbertheory . square root mod prime ( alpha , curve . p ( ) ) y = beta if ( beta - yp ) % 2 == 0 else curve . p ( ) - beta R = ecdsa . ellipticcurve . Point ( curve , x , y , order ) e = ecdsa . util . string to number ( digest ) Q = ecdsa . numbertheory . inverse mod ( r , order ) * ( s * R + ( - e % order ) * G ) if SECP256K1 MODULE == \"cryptography\" and message is not None : if not isinstance ( message , bytes ) : message = bytes ( message , \"utf-8\" ) sigder = encode dss signature ( r , s ) public key = ec . Elliptic Curve Public Numbers ( Q . Point x , Q . Point y , ec . SECP256K1 ( ) ) . public key ( default backend ( ) ) public key . verify ( sigder , message , ec . ECDSA ( hashes . SHA256 ( ) ) ) return public key else : if not ecdsa . Verifying Key . from public point ( Q , curve = ecdsa . SECP256k1 ) . verify digest ( signature , digest , sigdecode = ecdsa . util . sigdecode string ) : return None return ecdsa . Verifying Key . from public point ( Q , curve = ecdsa . SECP256k1 )", "predictions": ["fill a triangle key from a aa curve ."], "references": ["recover the public key from the the signature"], "bleu": 0.18575057999133596, "rouge_l": 0.2378167641325536}
{"id": 1517, "code": "def refresh ( self ) : asset = self . blockchain . rpc . get asset ( self . identifier ) if not asset : raise Asset Does Not Exists Exception ( self . identifier ) super ( Asset , self ) . init ( asset , blockchain instance = self . blockchain ) if self . full : if \"bitasset data id\" in asset : self [ \"bitasset data\" ] = self . blockchain . rpc . get object ( asset [ \"bitasset data id\" ] ) self [ \"dynamic asset data\" ] = self . blockchain . rpc . get object ( asset [ \"dynamic asset data id\" ] )", "predictions": ["all the results of the rpc ."], "references": ["refresh the data from the api server"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1518, "code": "def format Time ( t ) : if isinstance ( t , float ) : return datetime . utcfromtimestamp ( t ) . strftime ( time Format ) if isinstance ( t , datetime ) : return t . strftime ( time Format )", "predictions": ["contains a date object return a name"], "references": ["properly format time for permlinks"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1519, "code": "def get Operation Name For Id ( i : int ) : assert isinstance ( i , ( int ) ) , \"This method expects an integer argument\" for key in operations : if int ( operations [ key ] ) is int ( i ) : return key raise Value Error ( \"Unknown Operation ID %d\" % i )", "predictions": ["convert an if it is a list of be a valid be a string ."], "references": ["convert an operation id into the corresponding string"], "bleu": 0.11633270842295028, "rouge_l": 0.2760180995475113}
{"id": 1520, "code": "def unlocked ( self ) : if self . password is not None : return bool ( self . password ) else : if ( \"UNLOCK\" in os . environ and os . environ [ \"UNLOCK\" ] and self . config key in self . config and self . config [ self . config key ] ) : log . debug ( \"Trying to use environmental \" \"variable to unlock wallet\" ) self . unlock ( os . environ . get ( \"UNLOCK\" ) ) return bool ( self . password ) return False", "predictions": ["offset the connection to the zoneminder api . ."], "references": ["is the store unlocked so that i can decrypt the content?"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 1521, "code": "def decrypt masterpassword ( self ) : aes = AES Cipher ( self . password ) checksum , encrypted master = self . config [ self . config key ] . split ( \"$\" ) try : decrypted master = aes . decrypt ( encrypted master ) except Exception : self . raise wrongmasterpassexception ( ) if checksum != self . derive checksum ( decrypted master ) : self . raise wrongmasterpassexception ( ) self . decrypted master = decrypted master", "predictions": ["start masterpassword to ."], "references": ["decrypt the encrypted masterkey"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1522, "code": "def change password ( self , newpassword ) : if not self . unlocked ( ) : raise Wallet Locked self . password = newpassword self . save encrypted masterpassword ( )", "predictions": ["stop password a password getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr getattr password"], "references": ["change the password that allows to decrypt the master key"], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 1523, "code": "def derive y from x ( self , x , is even ) : curve = ecdsa . SECP256k1 . curve a , b , p = curve . a ( ) , curve . b ( ) , curve . p ( ) alpha = ( pow ( x , 3 , p ) + a * x + b ) % p beta = ecdsa . numbertheory . square root mod prime ( alpha , p ) if ( beta % 2 ) == is even : beta = p - beta return beta", "predictions": ["builds a y y from = = 0 ."], "references": ["derive y point from x point"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1524, "code": "def point ( self ) : string = unhexlify ( self . un Compressed ( ) ) return ecdsa . Verifying Key . from string ( string [ 1 : ] , curve = ecdsa . SECP256k1 ) . pubkey . point", "predictions": ["returns a euclidean euclidean euclidean euclidean euclidean euclidean euclidean = none = none"], "references": ["return the point for the public key"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 1525, "code": "def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )", "predictions": ["create a set of tokens for the given offset256"], "references": ["derive new public key from this key and a sha256 offset"], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 1526, "code": "def from privkey ( cls , privkey , prefix = None ) : privkey = Private Key ( privkey , prefix = prefix or Prefix . prefix ) secret = unhexlify ( repr ( privkey ) ) order = ecdsa . Signing Key . from string ( secret , curve = ecdsa . SECP256k1 ) . curve . generator . order ( ) p = ecdsa . Signing Key . from string ( secret , curve = ecdsa . SECP256k1 ) . verifying key . pubkey . point x str = ecdsa . util . number to string ( p . x ( ) , order ) compressed = hexlify ( chr ( 2 + ( p . y ( ) & 1 ) ) . encode ( \"ascii\" ) + x str ) . decode ( \"ascii\" ) return cls ( compressed , prefix = prefix or Prefix . prefix )", "predictions": ["create a privkey instance from a in in in in in in in in in in in in in in in in in in in in in in in privkey format"], "references": ["derive uncompressed public key"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1527, "code": "def child ( self , offset256 ) : a = bytes ( self . pubkey ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . derive from seed ( s )", "predictions": ["return a to make a to a to a to a file ."], "references": ["derive new private key from this key and a sha256 offset"], "bleu": 0.09552040806823771, "rouge_l": 0.08460471567267684}
{"id": 1528, "code": "def get Operation Name For Id ( self , i ) : for key in self . ops : if int ( self . ops [ key ] ) is int ( i ) : return key raise Value Error ( \"Unknown Operation ID %d\" % i )", "predictions": ["returns an integer with the specified number of == == 1 if not found if not found ."], "references": ["convert an operation id into the corresponding string"], "bleu": 0.07535838128770536, "rouge_l": 0.16531165311653115}
{"id": 1529, "code": "def find next ( self ) : if int ( self . num retries ) < 0 : self . cnt retries += 1 sleeptime = ( self . cnt retries - 1 ) * 2 if self . cnt retries < 10 else 10 if sleeptime : log . warning ( \"Lost connection to node during rpcexec(): %s (%d/%d) \" % ( self . url , self . cnt retries , self . num retries ) + \"Retrying in %d seconds\" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . url counter . items ( ) if ( int ( self . num retries ) >= 0 and v <= self . num retries and ( k != self . url or len ( self . url counter ) == 1 ) ) ] if not len ( urls ) : raise Num Retries Reached url = urls [ 0 ] return url", "predictions": ["image to image a prefix . ."], "references": ["find the next url in the list"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1530, "code": "def reset counter ( self ) : self . cnt retries = 0 for i in self . url counter : self . url counter [ i ] = 0", "predictions": ["animated all = url s gif gif ."], "references": ["reset the failed connection counters"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1531, "code": "def have Key ( self , key ) : query = ( \"SELECT {} FROM {} WHERE {}=?\" . format ( self . value , self . tablename , self . key ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False", "predictions": ["check if a port opener is already established return false otherwise return false ."], "references": ["is the key key available?"], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 1532, "code": "def items ( self ) : query = \"SELECT {}, {} from {}\" . format ( self . key , self . value , self . tablename ) connection = sqlite3 . connect ( self . sqlite file ) cursor = connection . cursor ( ) cursor . execute ( query ) r = [ ] for key , value in cursor . fetchall ( ) : r . append ( ( key , value ) ) return r", "predictions": ["list all the raw raw raw data files target target target raw"], "references": ["returns all items off the store as tuples"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 1533, "code": "def exists ( self ) : query = ( \"SELECT name FROM sqlite master \" + \"WHERE type='table' AND name=?\" , ( self . tablename , ) , ) connection = sqlite3 . connect ( self . sqlite file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False", "predictions": ["returns true if the sqlite start with the sqlite runnable runnable runnable runnable runnable runnable runnable"], "references": ["check if the database table exists"], "bleu": 0.10123734869668824, "rouge_l": 0.19805194805194803}
{"id": 1534, "code": "def create ( self ) : query = ( ) . format ( self . tablename , self . key , self . value ) connection = sqlite3 . connect ( self . sqlite file ) cursor = connection . cursor ( ) cursor . execute ( query ) connection . commit ( )", "predictions": ["show database tables path path path path path path path path path path path path path path path path path path path path path path path path path path path path"], "references": ["create the new table in the sqlite database"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 1535, "code": "def get raw ( self ) : if not self . ops : return ops = [ self . operations . Op wrapper ( op = o ) for o in list ( self . ops ) ] proposer = self . account class ( self . proposer , blockchain instance = self . blockchain ) data = { \"fee\" : { \"amount\" : 0 , \"asset id\" : \"1.3.0\" } , \"fee paying account\" : proposer [ \"id\" ] , \"expiration time\" : format Time From Now ( self . proposal expiration ) , \"proposed ops\" : [ o . json ( ) for o in ops ] , \"extensions\" : [ ] , } if self . proposal review : data . update ( { \"review period seconds\" : self . proposal review } ) ops = self . operations . Proposal create ( * * data ) return self . operation class ( ops )", "predictions": ["gets the raw . ."], "references": ["returns an instance of base operations for further processing"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 1536, "code": "def json ( self ) : if not self . is constructed ( ) or self . is require reconstruction ( ) : self . construct Tx ( ) return dict ( self )", "predictions": ["return a dict of json object representing this document ."], "references": ["show the transaction as plain json"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 1537, "code": "def append Wif ( self , wif ) : if wif : try : self . privatekey class ( wif ) self . wifs . add ( wif ) except Exception : raise Invalid Wif Error", "predictions": ["append a new wif to a wif program ."], "references": ["add a wif that should be used for signing of the transaction ."], "bleu": 0.1279808802469055, "rouge_l": 0.26406926406926406}
{"id": 1538, "code": "def set fee asset ( self , fee asset ) : if isinstance ( fee asset , self . amount class ) : self . fee asset id = fee asset [ \"id\" ] elif isinstance ( fee asset , self . asset class ) : self . fee asset id = fee asset [ \"id\" ] elif fee asset : self . fee asset id = fee asset else : self . fee asset id = \"1.3.0\"", "predictions": ["sets the fee asset asset of the specified fee ."], "references": ["set asset to fee"], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 1539, "code": "def verify authority ( self ) : try : if not self . blockchain . rpc . verify authority ( self . json ( ) ) : raise Insufficient Authority Error except Exception as e : raise e", "predictions": ["verify that the authority is valid ."], "references": ["verify the authority of the signed transaction"], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 1540, "code": "def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing accounts = [ ] self [ \"expiration\" ] = None dict . init ( self , { } )", "predictions": ["clear all ops from the queue"], "references": ["clear the transaction builder and start from scratch"], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 1541, "code": "def id ( self ) : sigs = self . data [ \"signatures\" ] self . data . pop ( \"signatures\" , None ) h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) self . data [ \"signatures\" ] = sigs return hexlify ( h [ : 20 ] ) . decode ( \"ascii\" )", "predictions": ["a unique identifier of the id ."], "references": ["the transaction id of this transaction"], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 1542, "code": "def unlock ( self , pwd ) : if self . store . is encrypted ( ) : return self . store . unlock ( pwd )", "predictions": ["unlock an image by pwd ."], "references": ["unlock the wallet database"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1543, "code": "def new Wallet ( self , pwd ) : if self . created ( ) : raise Wallet Exists ( \"You already have created a wallet!\" ) self . store . unlock ( pwd )", "predictions": ["make a new wallet ."], "references": ["create a new wallet database"], "bleu": 0.5081327481546147, "rouge_l": 0.6}
{"id": 1544, "code": "def add Private Key ( self , wif ) : try : pub = self . publickey from wif ( wif ) except Exception : raise Invalid Wif Error ( \"Invalid Key format!\" ) if str ( pub ) in self . store : raise Key Already In Store Exception ( \"Key already in the store\" ) self . store . add ( str ( wif ) , str ( pub ) )", "predictions": ["add a wif to the store"], "references": ["add a private key to the wallet database"], "bleu": 0.2619317629190374, "rouge_l": 0.5570776255707762}
{"id": 1545, "code": "def remove Account ( self , account ) : accounts = self . get Accounts ( ) for a in accounts : if a [ \"name\" ] == account : self . store . delete ( a [ \"pubkey\" ] )", "predictions": ["remove one or more accounts from the store store ."], "references": ["remove all keys associated with a given account"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 1546, "code": "def get Owner Key For Account ( self , name ) : account = self . rpc . get account ( name ) for authority in account [ \"owner\" ] [ \"key auths\" ] : key = self . get Private Key For Public Key ( authority [ 0 ] ) if key : return key raise Key Not Found", "predictions": ["return the key for the given account"], "references": ["obtain owner private key for an account from the wallet database"], "bleu": 0.15685718045401453, "rouge_l": 0.32049036777583184}
{"id": 1547, "code": "def get Memo Key For Account ( self , name ) : account = self . rpc . get account ( name ) key = self . get Private Key For Public Key ( account [ \"options\" ] [ \"memo key\" ] ) if key : return key return False", "predictions": ["return the memo for the given account"], "references": ["obtain owner memo key for an account from the wallet database"], "bleu": 0.1319006407505858, "rouge_l": 0.32049036777583184}
{"id": 1548, "code": "def get Active Key For Account ( self , name ) : account = self . rpc . get account ( name ) for authority in account [ \"active\" ] [ \"key auths\" ] : try : return self . get Private Key For Public Key ( authority [ 0 ] ) except Exception : pass return False", "predictions": ["return the account defined by name"], "references": ["obtain owner active key for an account from the wallet database"], "bleu": 0.10624253482403696, "rouge_l": 0.1117216117216117}
{"id": 1549, "code": "def get Account From Private Key ( self , wif ) : pub = self . publickey from wif ( wif ) return self . get Account From Public Key ( pub )", "predictions": ["retrieve a domain by wif ."], "references": ["obtain account name from private key"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1550, "code": "def get Accounts From Public Key ( self , pub ) : names = self . rpc . get key references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name", "predictions": ["return a generator of name and key names that have been accounts ."], "references": ["obtain all accounts associated with a public key"], "bleu": 0.1135935489027116, "rouge_l": 0.19902120717781402}
{"id": 1551, "code": "def get Account From Public Key ( self , pub ) : names = list ( self . get Accounts From Public Key ( str ( pub ) ) ) if names : return names [ 0 ]", "predictions": ["get names of names from the pub - pub option ."], "references": ["obtain the first account name from public key"], "bleu": 0.12605968092174913, "rouge_l": 0.108348134991119}
{"id": 1552, "code": "def get Accounts ( self ) : pubkeys = self . get Public Keys ( ) accounts = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . get Accounts From Public Key ( pubkey ) ) return accounts", "predictions": ["return a list of accounts for this user ."], "references": ["return all accounts installed in the wallet database"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 1553, "code": "def unlock wallet ( self , * args , * * kwargs ) : self . blockchain . wallet . unlock ( * args , * * kwargs ) return self", "predictions": ["unlock the wallet ."], "references": ["unlock the library internal wallet"], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 1554, "code": "def cmd ( command ) : env ( ) ipmi = cij . env to dict ( PREFIX , EXPORTED + REQUIRED ) command = \"ipmitool -U %s -P %s -H %s -p %s %s\" % ( ipmi [ \"USER\" ] , ipmi [ \"PASS\" ] , ipmi [ \"HOST\" ] , ipmi [ \"PORT\" ] , command ) cij . info ( \"ipmi.command: %s\" % command ) return cij . util . execute ( command , shell = True , echo = True )", "predictions": ["run a shell command ."], "references": ["send ipmi command via ipmitool"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1555, "code": "def regex find ( pattern , content ) : find = re . findall ( pattern , content ) if not find : cij . err ( \"pattern <%r> is invalid, no matches!\" % pattern ) cij . err ( \"content: %r\" % content ) return '' if len ( find ) >= 2 : cij . err ( \"pattern <%r> is too simple, matched more than 2!\" % pattern ) cij . err ( \"content: %r\" % content ) return '' return find [ 0 ]", "predictions": ["find the regex for a pattern ."], "references": ["find the given pattern in content"], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 1556, "code": "def env ( ) : if cij . ssh . env ( ) : cij . err ( \"board.env: invalid SSH environment\" ) return 1 board = cij . env to dict ( PREFIX , REQUIRED ) if board is None : cij . err ( \"board.env: invalid BOARD environment\" ) return 1 board [ \"CLASS\" ] = \" \" . join ( [ board [ r ] for r in REQUIRED [ : - 1 ] ] ) board [ \"IDENT\" ] = \"-\" . join ( [ board [ \"CLASS\" ] , board [ \"ALIAS\" ] ] ) cij . env export ( PREFIX , EXPORTED , board ) return 0", "predictions": ["builds a dictionary with the current board board ."], "references": ["verify board variables and construct exported variables"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 1557, "code": "def cat file ( path ) : cmd = [ \"cat\" , path ] status , stdout , = cij . ssh . command ( cmd , shell = True , echo = True ) if status : raise Runtime Error ( \"cij.nvme.env: cat %s failed\" % path ) return stdout . strip ( )", "predictions": ["get the contents of a file"], "references": ["cat file and return content"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1558, "code": "def env ( ) : if cij . ssh . env ( ) : cij . err ( \"cij.nvme.env: invalid SSH environment\" ) return 1 nvme = cij . env to dict ( PREFIX , REQUIRED ) nvme [ \"DEV PATH\" ] = os . path . join ( \"/dev\" , nvme [ \"DEV NAME\" ] ) try : sysfs = os . path . join ( \"/sys/class/block\" , nvme [ \"DEV NAME\" ] , \"lightnvm\" ) nvme [ \"LNVM VERSION\" ] = cat file ( os . path . join ( sysfs , \"version\" ) ) if nvme [ \"LNVM VERSION\" ] == \"2.0\" : luns = \"punits\" chs = \"groups\" elif nvme [ \"LNVM VERSION\" ] == \"1.2\" : luns = \"num luns\" chs = \"num channels\" else : raise Runtime Error ( \"cij.nvme.env: invalid lnvm version: %s\" % nvme [ \"LNVM VERSION\" ] ) nvme [ \"LNVM NUM CHUNKS\" ] = cat file ( os . path . join ( sysfs , \"chunks\" ) ) nvme [ \"LNVM NUM LUNS\" ] = cat file ( os . path . join ( sysfs , luns ) ) nvme [ \"LNVM NUM CHS\" ] = cat file ( os . path . join ( sysfs , chs ) ) nvme [ \"LNVM TOTAL LUNS\" ] = str ( int ( nvme [ \"LNVM NUM LUNS\" ] ) * int ( nvme [ \"LNVM NUM CHS\" ] ) ) nvme [ \"LNVM TOTAL CHUNKS\" ] = str ( int ( nvme [ \"LNVM TOTAL LUNS\" ] ) * int ( nvme [ \"LNVM NUM CHUNKS\" ] ) ) if nvme [ \"LNVM VERSION\" ] == \"2.0\" : cmd = [ \"nvme\" , \"id-ctrl\" , nvme [ \"DEV PATH\" ] , \"--raw-binary\" ] status , stdout , = cij . ssh . command ( cmd , shell = True ) if status : raise Runtime Error ( \"cij.nvme.env: nvme id-ctrl fail\" ) buff = cij . bin . Buffer ( types = Identify CDS , length = 1 ) buff . memcopy ( stdout ) if buff [ 0 ] . VS [ 1023 ] == 0x5a : nvme [ \"SPEC VERSION\" ] = \"Denali\" else : nvme [ \"SPEC VERSION\" ] = \"Spec20\" else : nvme [ \"SPEC VERSION\" ] = \"Spec12\" nvme [ \"LNVM CHUNK META LENGTH\" ] = str ( get sizeof descriptor table ( nvme [ \"SPEC VERSION\" ] ) ) nvme [ \"LNVM CHUNK META SIZE\" ] = str ( int ( nvme [ \"LNVM CHUNK META LENGTH\" ] ) * int ( nvme [ \"LNVM TOTAL CHUNKS\" ] ) ) except Standard Error : traceback . print exc ( ) return 1 cij . env export ( PREFIX , EXPORTED , nvme ) return 0", "predictions": ["run the virtualenv command"], "references": ["verify nvme variables and construct exported variables"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1559, "code": "def fmt ( lbaf = 3 ) : if env ( ) : cij . err ( \"cij.nvme.exists: Invalid NV Me ENV.\" ) return 1 nvme = cij . env to dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ \"nvme\" , \"format\" , nvme [ \"DEV PATH\" ] , \"-l\" , str ( lbaf ) ] rcode , , = cij . ssh . command ( cmd , shell = True ) return rcode", "predictions": ["executes a command dictionary to be run ."], "references": ["do format for nvme device"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1560, "code": "def get meta ( offset , length , output ) : if env ( ) : cij . err ( \"cij.nvme.meta: Invalid NV Me ENV.\" ) return 1 nvme = cij . env to dict ( PREFIX , EXPORTED + REQUIRED ) max size = 0x40000 with open ( output , \"wb\" ) as fout : for off in range ( offset , length , max size ) : size = min ( length - off , max size ) cmd = [ \"nvme get-log\" , nvme [ \"DEV PATH\" ] , \"-i 0xca\" , \"-o 0x%x\" % off , \"-l 0x%x\" % size , \"-b\" ] status , stdout , = cij . ssh . command ( cmd , shell = True ) if status : cij . err ( \"cij.nvme.meta: Error get chunk meta\" ) return 1 fout . write ( stdout ) return 0", "predictions": ["run the meta command to get the meta data for a particular run ."], "references": ["get chunk meta of nvme device"], "bleu": 0.09782375748961449, "rouge_l": 0.21554770318021202}
{"id": 1561, "code": "def env ( ) : if cij . ssh . env ( ) : cij . err ( \"cij.lnvm.env: invalid SSH environment\" ) return 1 lnvm = cij . env to dict ( PREFIX , REQUIRED ) nvme = cij . env to dict ( \"NVME\" , [ \"DEV NAME\" ] ) if \"BGN\" not in lnvm . keys ( ) : cij . err ( \"cij.lnvm.env: invalid LNVM BGN\" ) return 1 if \"END\" not in lnvm . keys ( ) : cij . err ( \"cij.lnvm.env: invalid LNVM END\" ) return 1 if \"DEV TYPE\" not in lnvm . keys ( ) : cij . err ( \"cij.lnvm.env: invalid LNVM DEV TYPE\" ) return 1 lnvm [ \"DEV NAME\" ] = \"%sb%03de%03d\" % ( nvme [ \"DEV NAME\" ] , int ( lnvm [ \"BGN\" ] ) , int ( lnvm [ \"END\" ] ) ) lnvm [ \"DEV PATH\" ] = \"/dev/%s\" % lnvm [ \"DEV NAME\" ] cij . env export ( PREFIX , EXPORTED , lnvm ) return 0", "predictions": ["export environment variables to string"], "references": ["verify lnvm variables and construct exported variables"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1562, "code": "def compare ( buf a , buf b , ignore ) : for field in getattr ( buf a , ' fields ' ) : name , types = field [ 0 ] , field [ 1 ] if name in ignore : continue val a = getattr ( buf a , name ) val b = getattr ( buf b , name ) if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val a , val b , ignore ) : return 1 elif isinstance ( types , type ( Array ) ) : for i , in enumerate ( val a ) : if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val a [ i ] , val b [ i ] , ignore ) : return 1 else : if val a [ i ] != val b [ i ] : return 1 else : if val a != val b : return 1 return 0", "predictions": ["compares two dicts together"], "references": ["compare of two buffer item"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1563, "code": "def memcopy ( self , stream , offset = 0 , length = float ( \"inf\" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m size ) buff = cast ( self . m buf , POINTER ( c uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]", "predictions": ["write a message to a stream"], "references": ["copy stream to buffer"], "bleu": 0.24446151121745047, "rouge_l": 0.2074829931972789}
{"id": 1564, "code": "def write ( self , path ) : with open ( path , \"wb\" ) as fout : fout . write ( self . m buf )", "predictions": ["write file to disk ."], "references": ["write buffer to file"], "bleu": 0.32466791547509893, "rouge_l": 0.4535315985130111}
{"id": 1565, "code": "def read ( self , path ) : with open ( path , \"rb\" ) as fout : memmove ( self . m buf , fout . read ( self . m size ) , self . m size )", "predictions": ["read data from file ."], "references": ["read file to buffer"], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1566, "code": "def is bad chunk ( self , chk , yml ) : cs = self . get chunk status ( chk , yml ) if cs >= 8 : return True return False", "predictions": ["return true if the chunk is a bad chunk ."], "references": ["check the chunk is offline or not"], "bleu": 0.23462350320528, "rouge_l": 0.36454183266932266}
{"id": 1567, "code": "def is free chunk ( self , chk ) : cs = self . get chunk status ( chk ) if cs & 0x1 != 0 : return True return False", "predictions": ["return whether or not this chunk is a free chunk ."], "references": ["check the chunk is free or not"], "bleu": 0.1972940627795883, "rouge_l": 0.3472485768500949}
{"id": 1568, "code": "def is closed chunk ( self , chk ) : cs = self . get chunk status ( chk ) if cs & 0x2 != 0 : return True return False", "predictions": ["return is a closed self require"], "references": ["check the chunk is free or not"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1569, "code": "def is open chunk ( self , chk ) : cs = self . get chunk status ( chk ) if cs & 0x4 != 0 : return True return False", "predictions": ["returns true if the active chunk is open false otherwise ."], "references": ["check the chunk is free or not"], "bleu": 0.16108992769687397, "rouge_l": 0.3472485768500949}
{"id": 1570, "code": "def env ( ) : if cij . ssh . env ( ) : cij . err ( \"cij.block.env: invalid SSH environment\" ) return 1 block = cij . env to dict ( PREFIX , REQUIRED ) block [ \"DEV PATH\" ] = \"/dev/%s\" % block [ \"DEV NAME\" ] cij . env export ( PREFIX , EXPORTED , block ) return 0", "predictions": ["export environment variables for poinit . ."], "references": ["verify block variables and construct exported variables"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1571, "code": "def script run ( trun , script ) : if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:script:run { script: %s }\" % script ) cij . emph ( \"rnr:script:run:evars: %s\" % script [ \"evars\" ] ) launchers = { \".py\" : \"python\" , \".sh\" : \"source\" } ext = os . path . splitext ( script [ \"fpath\" ] ) [ - 1 ] if not ext in launchers . keys ( ) : cij . err ( \"rnr:script:run { invalid script[\\\"fpath\\\"]: %r }\" % script [ \"fpath\" ] ) return 1 launch = launchers [ ext ] with open ( script [ \"log fpath\" ] , \"a\" ) as log fd : log fd . write ( % script [ \"fpath\" ] ) log fd . flush ( ) bgn = time . time ( ) cmd = [ 'bash' , '-c' , 'CIJ ROOT=$(cij root) && ' 'source $CIJ ROOT/modules/cijoe.sh && ' 'source %s && ' 'CIJ TEST RES ROOT=\"%s\" %s %s ' % ( trun [ \"conf\" ] [ \"ENV FPATH\" ] , script [ \"res root\" ] , launch , script [ \"fpath\" ] ) ] if trun [ \"conf\" ] [ \"VERBOSE\" ] > 1 : cij . emph ( \"rnr:script:run { cmd: %r }\" % \" \" . join ( cmd ) ) evars = os . environ . copy ( ) evars . update ( { k : str ( script [ \"evars\" ] [ k ] ) for k in script [ \"evars\" ] } ) process = Popen ( cmd , stdout = log fd , stderr = STDOUT , cwd = script [ \"res root\" ] , env = evars ) process . wait ( ) script [ \"rcode\" ] = process . returncode script [ \"wallc\" ] = time . time ( ) - bgn if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:script:run { wallc: %02f }\" % script [ \"wallc\" ] ) cij . emph ( \"rnr:script:run { rcode: %r } \" % script [ \"rcode\" ] , script [ \"rcode\" ] ) return script [ \"rcode\" ]", "predictions": ["run a verify verify verify the verify signed signed verify verify the verify verify raise an exception if the verify is given ."], "references": ["execute a script or testcase"], "bleu": 0.05291907393644996, "rouge_l": 0.08079470198675497}
{"id": 1572, "code": "def trun to file ( trun , fpath = None ) : if fpath is None : fpath = yml fpath ( trun [ \"conf\" ] [ \"OUTPUT\" ] ) with open ( fpath , 'w' ) as yml file : data = yaml . dump ( trun , explicit start = True , default flow style = False ) yml file . write ( data )", "predictions": ["write a file with . yaml"], "references": ["dump the given trun to file"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1573, "code": "def trun emph ( trun ) : if trun [ \"conf\" ] [ \"VERBOSE\" ] > 1 : cij . emph ( \"rnr:CONF {\" ) for cvar in sorted ( trun [ \"conf\" ] . keys ( ) ) : cij . emph ( \"  % 16s: %r\" % ( cvar , trun [ \"conf\" ] [ cvar ] ) ) cij . emph ( \"}\" ) if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:INFO {\" ) cij . emph ( \"  OUTPUT: %r\" % trun [ \"conf\" ] [ \"OUTPUT\" ] ) cij . emph ( \"  yml fpath: %r\" % yml fpath ( trun [ \"conf\" ] [ \"OUTPUT\" ] ) ) cij . emph ( \"}\" )", "predictions": ["calculate emph and print them to stdout"], "references": ["print essential info on"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1574, "code": "def tcase setup ( trun , parent , tcase fname ) : #pylint: disable=locally-disabled, unused-argument case = copy . deepcopy ( TESTCASE ) case [ \"fname\" ] = tcase fname case [ \"fpath orig\" ] = os . sep . join ( [ trun [ \"conf\" ] [ \"TESTCASES\" ] , case [ \"fname\" ] ] ) if not os . path . exists ( case [ \"fpath orig\" ] ) : cij . err ( 'rnr:tcase setup: !case[\"fpath orig\"]: %r' % case [ \"fpath orig\" ] ) return None case [ \"name\" ] = os . path . splitext ( case [ \"fname\" ] ) [ 0 ] case [ \"ident\" ] = \"/\" . join ( [ parent [ \"ident\" ] , case [ \"fname\" ] ] ) case [ \"res root\" ] = os . sep . join ( [ parent [ \"res root\" ] , case [ \"fname\" ] ] ) case [ \"aux root\" ] = os . sep . join ( [ case [ \"res root\" ] , \" aux\" ] ) case [ \"log fpath\" ] = os . sep . join ( [ case [ \"res root\" ] , \"run.log\" ] ) case [ \"fpath\" ] = os . sep . join ( [ case [ \"res root\" ] , case [ \"fname\" ] ] ) case [ \"evars\" ] . update ( copy . deepcopy ( parent [ \"evars\" ] ) ) os . makedirs ( case [ \"res root\" ] ) os . makedirs ( case [ \"aux root\" ] ) shutil . copyfile ( case [ \"fpath orig\" ] , case [ \"fpath\" ] ) case [ \"hooks\" ] = hooks setup ( trun , case , parent . get ( \"hooks pr tcase\" ) ) return case", "predictions": ["generates a setup return the return return dictionary for the given parent . . . . . . . . . . . . . . . . . . ."], "references": ["create and initialize a testcase"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 1575, "code": "def tsuite exit ( trun , tsuite ) : if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:tsuite:exit\" ) rcode = 0 for hook in reversed ( tsuite [ \"hooks\" ] [ \"exit\" ] ) : rcode = script run ( trun , hook ) if rcode : break if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:tsuite:exit { rcode: %r } \" % rcode , rcode ) return rcode", "predictions": ["turn a hook into a exit with a hook"], "references": ["triggers when exiting the given testsuite"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1576, "code": "def tsuite enter ( trun , tsuite ) : if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:tsuite:enter { name: %r }\" % tsuite [ \"name\" ] ) rcode = 0 for hook in tsuite [ \"hooks\" ] [ \"enter\" ] : rcode = script run ( trun , hook ) if rcode : break if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:tsuite:enter { rcode: %r } \" % rcode , rcode ) return rcode", "predictions": ["turn a list of rcode into a network with rcode"], "references": ["triggers when entering the given testsuite"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1577, "code": "def trun exit ( trun ) : if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:trun:exit\" ) rcode = 0 for hook in reversed ( trun [ \"hooks\" ] [ \"exit\" ] ) : rcode = script run ( trun , hook ) if rcode : break if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:trun::exit { rcode: %r }\" % rcode , rcode ) return rcode", "predictions": ["exit a hook with a hook"], "references": ["triggers when exiting the given testrun"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1578, "code": "def trun enter ( trun ) : if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:trun::enter\" ) trun [ \"stamp\" ] [ \"begin\" ] = int ( time . time ( ) ) rcode = 0 for hook in trun [ \"hooks\" ] [ \"enter\" ] : rcode = script run ( trun , hook ) if rcode : break if trun [ \"conf\" ] [ \"VERBOSE\" ] : cij . emph ( \"rnr:trun::enter { rcode: %r }\" % rcode , rcode ) return rcode", "predictions": ["enter rcode with rcode"], "references": ["triggers when entering the given testrun"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 1579, "code": "def main ( conf ) : fpath = yml fpath ( conf [ \"OUTPUT\" ] ) if os . path . exists ( fpath ) : cij . err ( \"main:FAILED { fpath: %r }, exists\" % fpath ) return 1 trun = trun setup ( conf ) if not trun : return 1 trun to file ( trun ) trun emph ( trun ) tr err = 0 tr ent err = trun enter ( trun ) for tsuite in ( ts for ts in trun [ \"testsuites\" ] if not tr ent err ) : ts err = 0 ts ent err = tsuite enter ( trun , tsuite ) for tcase in ( tc for tc in tsuite [ \"testcases\" ] if not ts ent err ) : tc err = tcase enter ( trun , tsuite , tcase ) if not tc err : tc err += script run ( trun , tcase ) tc err += tcase exit ( trun , tsuite , tcase ) tcase [ \"status\" ] = \"FAIL\" if tc err else \"PASS\" trun [ \"progress\" ] [ tcase [ \"status\" ] ] += 1 trun [ \"progress\" ] [ \"UNKN\" ] -= 1 ts err += tc err trun to file ( trun ) if not ts ent err : ts err += tsuite exit ( trun , tsuite ) ts err += ts ent err tr err += ts err tsuite [ \"status\" ] = \"FAIL\" if ts err else \"PASS\" cij . emph ( \"rnr:tsuite %r\" % tsuite [ \"status\" ] , tsuite [ \"status\" ] != \"PASS\" ) if not tr ent err : trun exit ( trun ) tr err += tr ent err trun [ \"status\" ] = \"FAIL\" if tr err else \"PASS\" trun [ \"stamp\" ] [ \"end\" ] = int ( time . time ( ) ) + 1 trun to file ( trun ) cij . emph ( \"rnr:main:progress %r\" % trun [ \"progress\" ] ) cij . emph ( \"rnr:main:trun %r\" % trun [ \"status\" ] , trun [ \"status\" ] != \"PASS\" ) return trun [ \"progress\" ] [ \"UNKN\" ] + trun [ \"progress\" ] [ \"FAIL\" ]", "predictions": ["run the program s main program"], "references": ["cij test runner main entry point"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1580, "code": "def get chunk meta ( self , meta file ) : chunks = self . envs [ \"CHUNKS\" ] if cij . nvme . get meta ( 0 , chunks * self . envs [ \"CHUNK META SIZEOF\" ] , meta file ) : raise Runtime Error ( \"cij.liblight.get chunk meta: fail\" ) chunk meta = cij . bin . Buffer ( types = self . envs [ \"CHUNK META STRUCT\" ] , length = chunks ) chunk meta . read ( meta file ) return chunk meta", "predictions": ["get chunk meta meta meta"], "references": ["get chunk meta table"], "bleu": 0.5081327481546147, "rouge_l": 0.6802973977695167}
{"id": 1581, "code": "def get chunk meta item ( self , chunk meta , grp , pug , chk ) : num chk = self . envs [ \"NUM CHK\" ] num pu = self . envs [ \"NUM PU\" ] index = grp * num pu * num chk + pug * num chk + chk return chunk meta [ index ]", "predictions": ["return chunk chunk item"], "references": ["get item of chunk meta table"], "bleu": 0.24117803988461298, "rouge_l": 0.1930379746835443}
{"id": 1582, "code": "def is bad chunk ( self , chunk meta , grp , pug , chk ) : meta = self . get chunk meta item ( chunk meta , grp , pug , chk ) if meta . CS & 0x8 != 0 : return True return False", "predictions": ["name of a chunk chunk"], "references": ["check the chunk is offline or not"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1583, "code": "def s20 to gen ( self , pugrp , punit , chunk , sectr ) : cmd = [ \"nvm addr s20 to gen\" , self . envs [ \"DEV PATH\" ] , \"%d %d %d %d\" % ( pugrp , punit , chunk , sectr ) ] status , stdout , = cij . ssh . command ( cmd , shell = True ) if status : raise Runtime Error ( \"cij.liblight.s20 to gen: cmd fail\" ) return int ( re . findall ( r\"val: ([0-9a-fx]+)\" , stdout ) [ 0 ] , 16 )", "predictions": ["generate get command document for specified time period return absolute value ."], "references": ["s20 unit to generic address"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 1584, "code": "def gen to dev ( self , address ) : cmd = [ \"nvm addr gen2dev\" , self . envs [ \"DEV PATH\" ] , \"0x{:x}\" . format ( address ) ] status , stdout , = cij . ssh . command ( cmd , shell = True ) if status : raise Runtime Error ( \"cij.liblight.gen to dev: cmd fail\" ) return int ( re . findall ( r\"dev: ([0-9a-fx]+)\" , stdout ) [ 0 ] , 16 )", "predictions": ["generate specified public command document pubkey pubkey pubkey pubkey pubkey pubkey pubkey pubkey pubkey pubkey pubkey"], "references": ["generic address to device address"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1585, "code": "def start ( self ) : self . thread = Thread ( target = self . run , args = ( True , False ) ) self . thread . set Daemon ( True ) self . thread . start ( )", "predictions": ["unlock the args * thread * to a args * * * * * * * * * * * * * * * thread * ."], "references": ["start dmesg job in thread"], "bleu": 0.044915755686574035, "rouge_l": 0.07134502923976607}
{"id": 1586, "code": "def env ( ) : if cij . ssh . env ( ) : cij . err ( \"cij.pci.env: invalid SSH environment\" ) return 1 pci = cij . env to dict ( PREFIX , REQUIRED ) pci [ \"BUS PATH\" ] = \"/sys/bus/pci\" pci [ \"DEV PATH\" ] = os . sep . join ( [ pci [ \"BUS PATH\" ] , \"devices\" , pci [ \"DEV NAME\" ] ] ) cij . env export ( PREFIX , EXPORTED , pci ) return 0", "predictions": ["return the default cmd dictionary for the smart environment ."], "references": ["verify pci variables and construct exported variables"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1587, "code": "def info ( txt ) : print ( % ( PR EMPH CC , get time stamp ( ) , txt , PR NC ) ) sys . stdout . flush ( )", "predictions": ["prints out some information about the user"], "references": ["print emphasized neutral the given txt message"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1588, "code": "def good ( txt ) : print ( % ( PR GOOD CC , get time stamp ( ) , txt , PR NC ) ) sys . stdout . flush ( )", "predictions": ["prints the current shell"], "references": ["print emphasized good the given txt message"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1589, "code": "def warn ( txt ) : print ( % ( PR WARN CC , get time stamp ( ) , txt , PR NC ) ) sys . stdout . flush ( )", "predictions": ["prints the screen status to stdout"], "references": ["print emphasized warning the given txt message"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1590, "code": "def err ( txt ) : print ( % ( PR ERR CC , get time stamp ( ) , txt , PR NC ) ) sys . stdout . flush ( )", "predictions": ["prints the terminal progress bar"], "references": ["print emphasized error the given txt message"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1591, "code": "def emph ( txt , rval = None ) : if rval is None : info ( txt ) elif rval == 0 : good ( txt ) else : err ( txt )", "predictions": ["runs a jupyter notebook ."], "references": ["print emphasized based on rval"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1592, "code": "def paths from env ( prefix = None , names = None ) : def expand path ( path ) : \"\"\"Expands variables in 'path' and turns it into absolute path\"\"\" return os . path . abspath ( os . path . expanduser ( os . path . expandvars ( path ) ) ) if prefix is None : prefix = \"CIJ\" if names is None : names = [ \"ROOT\" , \"ENVS\" , \"TESTPLANS\" , \"TESTCASES\" , \"TESTSUITES\" , \"MODULES\" , \"HOOKS\" , \"TEMPLATES\" ] conf = { v : os . environ . get ( \" \" . join ( [ prefix , v ] ) ) for v in names } for env in ( e for e in conf . keys ( ) if e [ : len ( prefix ) ] in names and conf [ e ] ) : conf [ env ] = expand path ( conf [ env ] ) if not os . path . exists ( conf [ env ] ) : err ( \"%s %s: %r, does not exist\" % ( prefix , env , conf [ env ] ) ) return conf", "predictions": ["returns list of get get get get get get get get get get get get paths to include . from the project . ."], "references": ["construct dict of paths from environment variables"], "bleu": 0.06024757292375468, "rouge_l": 0.2147887323943662}
{"id": 1593, "code": "def env export ( prefix , exported , env ) : for exp in exported : ENV [ \" \" . join ( [ prefix , exp ] ) ] = env [ exp ]", "predictions": ["export environment variables for virtualenv 1 ."], "references": ["define the list of exported variables with prefix with values from env"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 1594, "code": "def env ( ) : if cij . ssh . env ( ) : cij . err ( \"cij.nvm.env: invalid SSH environment\" ) return 1 nvm = cij . env to dict ( PREFIX , REQUIRED ) if \"nvme\" in nvm [ \"DEV NAME\" ] : nvm [ \"DEV PATH\" ] = \"/dev/%s\" % nvm [ \"DEV NAME\" ] else : nvm [ \"DEV PATH\" ] = \"traddr:%s\" % nvm [ \"DEV NAME\" ] cij . env export ( PREFIX , EXPORTED , nvm ) return 0", "predictions": ["enumerate the environment variables for the current environment ignore ignore ignore ignore ignore ignore ignore"], "references": ["verify nvme variables and construct exported variables"], "bleu": 0.08225964699966554, "rouge_l": 0.09728867623604465}
{"id": 1595, "code": "def exists ( ) : if env ( ) : cij . err ( \"cij.nvm.exists: Invalid NV Me ENV.\" ) return 1 nvm = cij . env to dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b \"%s\" ]]' % nvm [ \"DEV PATH\" ] ] rcode , , = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode", "predictions": ["check if a system server exists 0 if not ."], "references": ["verify that the env defined nvme device exists"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 1596, "code": "def pkill ( ) : if env ( ) : return 1 cmd = [ \"ps -aux | grep fio | grep -v grep\" ] status , , = cij . ssh . command ( cmd , shell = True , echo = False ) if not status : status , , = cij . ssh . command ( [ \"pkill -f fio\" ] , shell = True ) if status : return 1 return 0", "predictions": ["call ssh on ssh"], "references": ["kill all of fio processes"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 1597, "code": "def parse parms ( self ) : args = list ( ) for key , val in self . parm . items ( ) : key = key . replace ( \"FIO \" , \"\" ) . lower ( ) if key == \"runtime\" : args . append ( \"--time based\" ) if val is None : args . append ( \"--%s\" % key ) else : args . append ( \"--%s=%s\" % ( key , val ) ) return args", "predictions": ["read parms from all parms"], "references": ["translate dict parameters to string"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1598, "code": "def import parms ( self , args ) : for key , val in args . items ( ) : self . set parm ( key , val )", "predictions": ["is a set of bad keys ."], "references": ["import external dict to internal dict"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1599, "code": "def get parm ( self , key ) : if key in self . parm . keys ( ) : return self . parm [ key ] return None", "predictions": ["get a set of keys for a key"], "references": ["get parameter of fio"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 1600, "code": "def start ( self ) : self . thread = Threads ( target = self . run , args = ( True , True , False ) ) self . thread . set Daemon ( True ) self . thread . start ( )", "predictions": ["start the thread ."], "references": ["run fio job in thread"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1601, "code": "def extract hook names ( ent ) : hnames = [ ] for hook in ent [ \"hooks\" ] [ \"enter\" ] + ent [ \"hooks\" ] [ \"exit\" ] : hname = os . path . basename ( hook [ \"fpath orig\" ] ) hname = os . path . splitext ( hname ) [ 0 ] hname = hname . strip ( ) hname = hname . replace ( \" enter\" , \"\" ) hname = hname . replace ( \" exit\" , \"\" ) if hname in hnames : continue hnames . append ( hname ) hnames . sort ( ) return hnames", "predictions": ["return a list of all the hook names"], "references": ["extract hook names from the given entity"], "bleu": 0.22679164443904004, "rouge_l": 0.26991150442477874}
{"id": 1602, "code": "def tcase parse descr ( tcase ) : descr short = \"SHORT\" descr long = \"LONG\" try : comment = tcase comment ( tcase ) except ( IO Error , OS Error , Value Error ) as exc : comment = [ ] cij . err ( \"tcase parse descr: failed: %r, tcase: %r\" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] for line number , line in enumerate ( comment ) : if line . startswith ( \"#\" ) : comment [ line number ] = line [ 1 : ] if comment : descr short = comment [ 0 ] if len ( comment ) > 1 : descr long = \"\\n\" . join ( comment [ 1 : ] ) return descr short , descr long", "predictions": ["parse the tcase list"], "references": ["parse descriptions from the the given tcase"], "bleu": 0.20183609024241697, "rouge_l": 0.5198863636363635}
{"id": 1603, "code": "def process tsuite ( tsuite ) : tsuite [ \"log content\" ] = runlogs to html ( tsuite [ \"res root\" ] ) tsuite [ \"aux list\" ] = aux listing ( tsuite [ \"aux root\" ] ) tsuite [ \"hnames\" ] = extract hook names ( tsuite ) return True", "predictions": ["process the tsuite list"], "references": ["goes through the tsuite and processes * . log"], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 1604, "code": "def process tcase ( tcase ) : tcase [ \"src content\" ] = src to html ( tcase [ \"fpath\" ] ) tcase [ \"log content\" ] = runlogs to html ( tcase [ \"res root\" ] ) tcase [ \"aux list\" ] = aux listing ( tcase [ \"aux root\" ] ) tcase [ \"descr short\" ] , tcase [ \"descr long\" ] = tcase parse descr ( tcase ) tcase [ \"hnames\" ] = extract hook names ( tcase ) return True", "predictions": ["processes the tcase list"], "references": ["goes through the trun and processes run . log"], "bleu": 0.11392443929712959, "rouge_l": 0.14386792452830188}
{"id": 1605, "code": "def process trun ( trun ) : trun [ \"log content\" ] = runlogs to html ( trun [ \"res root\" ] ) trun [ \"aux list\" ] = aux listing ( trun [ \"aux root\" ] ) trun [ \"hnames\" ] = extract hook names ( trun ) return True", "predictions": ["process the hook in the trun"], "references": ["goes through the trun and processes run . log"], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 1606, "code": "def postprocess ( trun ) : plog = [ ] plog . append ( ( \"trun\" , process trun ( trun ) ) ) for tsuite in trun [ \"testsuites\" ] : plog . append ( ( \"tsuite\" , process tsuite ( tsuite ) ) ) for tcase in tsuite [ \"testcases\" ] : plog . append ( ( \"tcase\" , process tcase ( tcase ) ) ) for task , success in plog : if not success : cij . err ( \"rprtr::postprocess: FAILED for %r\" % task ) return sum ( ( success for task , success in plog ) )", "predictions": ["calculate the sum of all tasks ."], "references": ["perform postprocessing of the given test run"], "bleu": 0.20556680845025982, "rouge_l": 0.14285714285714285}
{"id": 1607, "code": "def rehome ( old , new , struct ) : if old == new : return if isinstance ( struct , list ) : for item in struct : rehome ( old , new , item ) elif isinstance ( struct , dict ) : for key , val in struct . iteritems ( ) : if isinstance ( val , ( dict , list ) ) : rehome ( old , new , val ) elif \"conf\" in key : continue elif \"orig\" in key : continue elif \"root\" in key or \"path\" in key : struct [ key ] = struct [ key ] . replace ( old , new )", "predictions": ["instance a dictionary from a struct"], "references": ["replace all absolute paths to re - home it"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 1608, "code": "def env ( ) : ssh = cij . env to dict ( PREFIX , REQUIRED ) if \"KEY\" in ssh : ssh [ \"KEY\" ] = cij . util . expand path ( ssh [ \"KEY\" ] ) if cij . ENV . get ( \"SSH PORT\" ) is None : cij . ENV [ \"SSH PORT\" ] = \"22\" cij . warn ( \"cij.ssh.env: SSH PORT was not set, assigned: %r\" % ( cij . ENV . get ( \"SSH PORT\" ) ) ) if cij . ENV . get ( \"SSH CMD TIME\" ) is None : cij . ENV [ \"SSH CMD TIME\" ] = \"1\" cij . warn ( \"cij.ssh.env: SSH CMD TIME was not set, assigned: %r\" % ( cij . ENV . get ( \"SSH CMD TIME\" ) ) ) return 0", "predictions": ["return the current ssh environment for the current user ."], "references": ["verify ssh variables and construct exported variables"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 1609, "code": "def wait ( timeout = 300 ) : if env ( ) : cij . err ( \"cij.ssh.wait: Invalid SSH environment\" ) return 1 timeout backup = cij . ENV . get ( \"SSH CMD TIMEOUT\" ) try : time start = time . time ( ) cij . ENV [ \"SSH CMD TIMEOUT\" ] = \"3\" while True : time current = time . time ( ) if ( time current - time start ) > timeout : cij . err ( \"cij.ssh.wait: Timeout\" ) return 1 status , , = command ( [ \"exit\" ] , shell = True , echo = False ) if not status : break cij . info ( \"cij.ssh.wait: Time elapsed: %d seconds\" % ( time current - time start ) ) finally : if timeout backup is None : del cij . ENV [ \"SSH CMD TIMEOUT\" ] else : cij . ENV [ \"SSH CMD TIMEOUT\" ] = timeout backup return 0", "predictions": ["wait for the process to be run ."], "references": ["wait util target connected"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1610, "code": "def assert that ( val , description = '' ) : global soft ctx if soft ctx : return Assertion Builder ( val , description , 'soft' ) return Assertion Builder ( val , description )", "predictions": ["assert that val is a soft that uses a value"], "references": ["factory method for the assertion builder with value to be tested and optional description ."], "bleu": 0.07645906143263256, "rouge_l": 0.07721518987341772}
{"id": 1611, "code": "def is equal to ( self , other , * * kwargs ) : if self . check dict like ( self . val , check values = False , return as bool = True ) and self . check dict like ( other , check values = False , return as bool = True ) : if self . dict not equal ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) : self . dict err ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) else : if self . val != other : self . err ( 'Expected <%s> to be equal to <%s>, but was not.' % ( self . val , other ) ) return self", "predictions": ["whether two elements are equal to another"], "references": ["asserts that val is equal to other ."], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 1612, "code": "def is not equal to ( self , other ) : if self . val == other : self . err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self", "predictions": ["check if two elements are not equal to other"], "references": ["asserts that val is not equal to other ."], "bleu": 0.392814650900513, "rouge_l": 0.4444444444444444}
{"id": 1613, "code": "def is same as ( self , other ) : if self . val is not other : self . err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self", "predictions": ["return true if this rectangle is a same"], "references": ["asserts that the val is identical to other via is compare ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 1614, "code": "def is not same as ( self , other ) : if self . val is other : self . err ( 'Expected <%s> to be not identical to <%s>, but was.' % ( self . val , other ) ) return self", "predictions": ["return true if two information are not same as other"], "references": ["asserts that the val is not identical to other via is compare ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 1615, "code": "def is type of ( self , some type ) : if type ( some type ) is not type and not issubclass ( type ( some type ) , type ) : raise Type Error ( 'given arg must be a type' ) if type ( self . val ) is not some type : if hasattr ( self . val , ' name ' ) : t = self . val . name elif hasattr ( self . val , ' class ' ) : t = self . val . class . name else : t = 'unknown' self . err ( 'Expected <%s:%s> to be of type <%s>, but was not.' % ( self . val , t , some type . name ) ) return self", "predictions": ["return true if variable is a type of type type ."], "references": ["asserts that val is of the given type ."], "bleu": 0.17033186037639278, "rouge_l": 0.4073455759599332}
{"id": 1616, "code": "def is instance of ( self , some class ) : try : if not isinstance ( self . val , some class ) : if hasattr ( self . val , ' name ' ) : t = self . val . name elif hasattr ( self . val , ' class ' ) : t = self . val . class . name else : t = 'unknown' self . err ( 'Expected <%s:%s> to be instance of class <%s>, but was not.' % ( self . val , t , some class . name ) ) except Type Error : raise Type Error ( 'given arg must be a class' ) return self", "predictions": ["return true if this class is a valid instance of the class ."], "references": ["asserts that val is an instance of the given class ."], "bleu": 0.21972813874997157, "rouge_l": 0.507628294036061}
{"id": 1617, "code": "def is length ( self , length ) : if type ( length ) is not int : raise Type Error ( 'given arg must be an int' ) if length < 0 : raise Value Error ( 'given arg must be a positive int' ) if len ( self . val ) != length : self . err ( 'Expected <%s> to be of length <%d>, but was <%d>.' % ( self . val , length , len ( self . val ) ) ) return self", "predictions": ["checks that the length of the length is a valid length ."], "references": ["asserts that val is the given length ."], "bleu": 0.16261701715194898, "rouge_l": 0.4149659863945578}
{"id": 1618, "code": "def contains ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . check dict like ( self . val , return as bool = True ) : self . err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . check dict like ( self . val , return as bool = True ) : self . err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . fmt items ( items ) , '' if len ( missing ) == 0 else 's' , self . fmt items ( missing ) ) ) else : self . err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( missing ) ) ) return self", "predictions": ["check if variable contains a list of items ."], "references": ["asserts that val contains the given item or items ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 1619, "code": "def does not contain ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] in self . val : self . err ( 'Expected <%s> to not contain item <%s>, but did.' % ( self . val , items [ 0 ] ) ) else : found = [ ] for i in items : if i in self . val : found . append ( i ) if found : self . err ( 'Expected <%s> to not contain items %s, but did contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( found ) ) ) return self", "predictions": ["extract a list of items from the list of items ."], "references": ["asserts that val does not contain the given item or items ."], "bleu": 0.14709132836587344, "rouge_l": 0.25884016973125884}
{"id": 1620, "code": "def contains only ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) else : extra = [ ] for i in self . val : if i not in items : extra . append ( i ) if extra : self . err ( 'Expected <%s> to contain only %s, but did contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( extra ) ) ) missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : self . err ( 'Expected <%s> to contain only %s, but did not contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( missing ) ) ) return self", "predictions": ["check if variable items contains only only one of the items ."], "references": ["asserts that val contains only the given item or items ."], "bleu": 0.17996531271765898, "rouge_l": 0.43821839080459773}
{"id": 1621, "code": "def contains sequence ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except Type Error : raise Type Error ( 'val is not iterable' ) self . err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . fmt items ( items ) ) )", "predictions": ["check if the statement is a sequence of items ."], "references": ["asserts that val contains the given sequence of items in order ."], "bleu": 0.21258637840736228, "rouge_l": 0.44721407624633425}
{"id": 1622, "code": "def contains duplicates ( self ) : try : if len ( self . val ) != len ( set ( self . val ) ) : return self except Type Error : raise Type Error ( 'val is not iterable' ) self . err ( 'Expected <%s> to contain duplicates, but did not.' % self . val )", "predictions": ["determines if the value is duplicates ."], "references": ["asserts that val is iterable and contains duplicate items ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1623, "code": "def does not contain duplicates ( self ) : try : if len ( self . val ) == len ( set ( self . val ) ) : return self except Type Error : raise Type Error ( 'val is not iterable' ) self . err ( 'Expected <%s> to not contain duplicates, but did.' % self . val )", "predictions": ["determines if a value is not contain not contain any not contain duplicates ."], "references": ["asserts that val is iterable and does not contain any duplicate items ."], "bleu": 0.18207052811092136, "rouge_l": 0.3728606356968216}
{"id": 1624, "code": "def is empty ( self ) : if len ( self . val ) != 0 : if isinstance ( self . val , str types ) : self . err ( 'Expected <%s> to be empty string, but was not.' % self . val ) else : self . err ( 'Expected <%s> to be empty, but was not.' % self . val ) return self", "predictions": ["return true if variable is empty"], "references": ["asserts that val is empty ."], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 1625, "code": "def is not empty ( self ) : if len ( self . val ) == 0 : if isinstance ( self . val , str types ) : self . err ( 'Expected not empty string, but was empty.' ) else : self . err ( 'Expected not empty, but was empty.' ) return self", "predictions": ["check if the value is empty ."], "references": ["asserts that val is not empty ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 1626, "code": "def is in ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) else : for i in items : if self . val == i : return self self . err ( 'Expected <%s> to be in %s, but was not.' % ( self . val , self . fmt items ( items ) ) )", "predictions": ["check if key is in items ."], "references": ["asserts that val is equal to one of the given items ."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 1627, "code": "def is less than ( self , other ) : self . validate compareable ( other ) if self . val >= other : if type ( self . val ) is datetime . datetime : self . err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val , other ) ) return self", "predictions": ["return true if two than are less than other"], "references": ["asserts that val is numeric and is less than other ."], "bleu": 0.2103465006557667, "rouge_l": 0.2946859903381642}
{"id": 1628, "code": "def is between ( self , low , high ) : val type = type ( self . val ) self . validate between args ( val type , low , high ) if self . val < low or self . val > high : if val type is datetime . datetime : self . err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , low . strftime ( '%Y-%m-%d %H:%M:%S' ) , high . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val , low , high ) ) return self", "predictions": ["check if a value is between 0 and a value"], "references": ["asserts that val is numeric and is between low and high ."], "bleu": 0.14595947916189678, "rouge_l": 0.2683284457478006}
{"id": 1629, "code": "def is close to ( self , other , tolerance ) : self . validate close to args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance seconds , 3600 ) m , s = divmod ( rem , 60 ) self . err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self", "predictions": ["return true if two arguments are within a other date"], "references": ["asserts that val is numeric and is close to other within tolerance ."], "bleu": 0.10335004586873166, "rouge_l": 0.084958217270195}
{"id": 1630, "code": "def is equal to ignoring case ( self , other ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if not isinstance ( other , str types ) : raise Type Error ( 'given arg must be a string' ) if self . val . lower ( ) != other . lower ( ) : self . err ( 'Expected <%s> to be case-insensitive equal to <%s>, but was not.' % ( self . val , other ) ) return self", "predictions": ["check if two values are equal to other"], "references": ["asserts that val is case - insensitive equal to other ."], "bleu": 0.20513838542429053, "rouge_l": 0.3070469798657718}
{"id": 1631, "code": "def contains ignoring case ( self , * items ) : if len ( items ) == 0 : raise Value Error ( 'one or more args must be given' ) if isinstance ( self . val , str types ) : if len ( items ) == 1 : if not isinstance ( items [ 0 ] , str types ) : raise Type Error ( 'given arg must be a string' ) if items [ 0 ] . lower ( ) not in self . val . lower ( ) : self . err ( 'Expected <%s> to case-insensitive contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if not isinstance ( i , str types ) : raise Type Error ( 'given args must all be strings' ) if i . lower ( ) not in self . val . lower ( ) : missing . append ( i ) if missing : self . err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( missing ) ) ) elif isinstance ( self . val , Iterable ) : missing = [ ] for i in items : if not isinstance ( i , str types ) : raise Type Error ( 'given args must all be strings' ) found = False for v in self . val : if not isinstance ( v , str types ) : raise Type Error ( 'val items must all be strings' ) if i . lower ( ) == v . lower ( ) : found = True break if not found : missing . append ( i ) if missing : self . err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . fmt items ( items ) , self . fmt items ( missing ) ) ) else : raise Type Error ( 'val is not a string or iterable' ) return self", "predictions": ["check that the format contains a case - insensitive value ."], "references": ["asserts that val is string and contains the given item or items ."], "bleu": 0.11941964005964323, "rouge_l": 0.24629878869448185}
{"id": 1632, "code": "def starts with ( self , prefix ) : if prefix is None : raise Type Error ( 'given prefix arg must not be none' ) if isinstance ( self . val , str types ) : if not isinstance ( prefix , str types ) : raise Type Error ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise Value Error ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise Value Error ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise Type Error ( 'val is not a string or iterable' ) return self", "predictions": ["return true if ."], "references": ["asserts that val is string or iterable and starts with prefix ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 1633, "code": "def ends with ( self , suffix ) : if suffix is None : raise Type Error ( 'given suffix arg must not be none' ) if isinstance ( self . val , str types ) : if not isinstance ( suffix , str types ) : raise Type Error ( 'given suffix arg must be a string' ) if len ( suffix ) == 0 : raise Value Error ( 'given suffix arg must not be empty' ) if not self . val . endswith ( suffix ) : self . err ( 'Expected <%s> to end with <%s>, but did not.' % ( self . val , suffix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise Value Error ( 'val must not be empty' ) last = None for last in self . val : pass if last != suffix : self . err ( 'Expected %s to end with <%s>, but did not.' % ( self . val , suffix ) ) else : raise Type Error ( 'val is not a string or iterable' ) return self", "predictions": ["perform a single value hook hook hook basename basename basename basename basename basename basename basename basename ."], "references": ["asserts that val is string or iterable and ends with suffix ."], "bleu": 0.07223943354597204, "rouge_l": 0.07117852975495916}
{"id": 1634, "code": "def matches ( self , pattern ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if not isinstance ( pattern , str types ) : raise Type Error ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise Value Error ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self", "predictions": ["1 1 of the argument . . . . . . . . . ."], "references": ["asserts that val is string and matches regex pattern ."], "bleu": 0.08225964699966554, "rouge_l": 0.08299319727891155}
{"id": 1635, "code": "def is alpha ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if len ( self . val ) == 0 : raise Value Error ( 'val is empty' ) if not self . val . isalpha ( ) : self . err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self", "predictions": ["return is a alpha instance for a alpha = none"], "references": ["asserts that val is non - empty string and all characters are alphabetic ."], "bleu": 0.08450033111870488, "rouge_l": 0.08090185676392574}
{"id": 1636, "code": "def is digit ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if len ( self . val ) == 0 : raise Value Error ( 'val is empty' ) if not self . val . isdigit ( ) : self . err ( 'Expected <%s> to contain only digits, but did not.' % self . val ) return self", "predictions": ["return is a digit digit"], "references": ["asserts that val is non - empty string and all characters are digits ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 1637, "code": "def is lower ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if len ( self . val ) == 0 : raise Value Error ( 'val is empty' ) if self . val != self . val . lower ( ) : self . err ( 'Expected <%s> to contain only lowercase chars, but did not.' % self . val ) return self", "predictions": ["return is a trun"], "references": ["asserts that val is non - empty string and all characters are lowercase ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 1638, "code": "def is upper ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a string' ) if len ( self . val ) == 0 : raise Value Error ( 'val is empty' ) if self . val != self . val . upper ( ) : self . err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self", "predictions": ["return is a upper"], "references": ["asserts that val is non - empty string and all characters are uppercase ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 1639, "code": "def is unicode ( self ) : if type ( self . val ) is not unicode : self . err ( 'Expected <%s> to be unicode, but was <%s>.' % ( self . val , type ( self . val ) . name ) ) return self", "predictions": ["continue if this value is a unicode"], "references": ["asserts that val is a unicode string ."], "bleu": 0.29969770769039067, "rouge_l": 0.3952483801295896}
{"id": 1640, "code": "def is subset of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise Type Error ( 'val is not iterable' ) if len ( supersets ) == 0 : raise Value Error ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , ' getitem ' ) : superdict = { } for l , j in enumerate ( supersets ) : self . check dict like ( j , check values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) if missing : self . err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . fmt items ( superdict ) , self . fmt items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . fmt items ( superset ) , self . fmt items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self", "predictions": ["return is a subset"], "references": ["asserts that val is iterable and a subset of the given superset or flattened superset if multiple supersets are given ."], "bleu": 0.007248124376500034, "rouge_l": 0.21378504672897194}
{"id": 1641, "code": "def contains value ( self , * values ) : self . check dict like ( self . val , check getitem = False ) if len ( values ) == 0 : raise Value Error ( 'one or more value args must be given' ) missing = [ ] for v in values : if v not in self . val . values ( ) : missing . append ( v ) if missing : self . err ( 'Expected <%s> to contain values %s, but did not contain %s.' % ( self . val , self . fmt items ( values ) , self . fmt items ( missing ) ) ) return self", "predictions": ["check that the statement wait for one of the list of values . ."], "references": ["asserts that val is a dict and contains the given value or values ."], "bleu": 0.13217947626377288, "rouge_l": 0.2857142857142857}
{"id": 1642, "code": "def does not contain value ( self , * values ) : self . check dict like ( self . val , check getitem = False ) if len ( values ) == 0 : raise Value Error ( 'one or more value args must be given' ) else : found = [ ] for v in values : if v in self . val . values ( ) : found . append ( v ) if found : self . err ( 'Expected <%s> to not contain values %s, but did contain %s.' % ( self . val , self . fmt items ( values ) , self . fmt items ( found ) ) ) return self", "predictions": ["check that a list of values and return are contain soft soft soft soft soft soft soft soft soft soft soft soft soft soft soft soft soft soft soft soft soft"], "references": ["asserts that val is a dict and does not contain the given value or values ."], "bleu": 0.0513487742994337, "rouge_l": 0.18060695780903036}
{"id": 1643, "code": "def contains entry ( self , * args , * * kwargs ) : self . check dict like ( self . val , check values = False ) entries = list ( args ) + [ { k : v } for k , v in kwargs . items ( ) ] if len ( entries ) == 0 : raise Value Error ( 'one or more entry args must be given' ) missing = [ ] for e in entries : if type ( e ) is not dict : raise Type Error ( 'given entry arg must be a dict' ) if len ( e ) != 1 : raise Value Error ( 'given entry args must contain exactly one key-value pair' ) k = next ( iter ( e ) ) if k not in self . val : missing . append ( e ) elif self . val [ k ] != e [ k ] : missing . append ( e ) if missing : self . err ( 'Expected <%s> to contain entries %s, but did not contain %s.' % ( self . val , self . fmt items ( entries ) , self . fmt items ( missing ) ) ) return self", "predictions": ["checks that an equal statement is defined in the dictionary . . . . . ."], "references": ["asserts that val is a dict and contains the given entry or entries ."], "bleu": 0.09672649511413092, "rouge_l": 0.26991150442477874}
{"id": 1644, "code": "def is before ( self , other ) : if type ( self . val ) is not datetime . datetime : raise Type Error ( 'val must be datetime, but was type <%s>' % type ( self . val ) . name ) if type ( other ) is not datetime . datetime : raise Type Error ( 'given arg must be datetime, but was type <%s>' % type ( other ) . name ) if self . val >= other : self . err ( 'Expected <%s> to be before <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) return self", "predictions": ["return is a ."], "references": ["asserts that val is a date and is before other date ."], "bleu": 0.06876828939330318, "rouge_l": 0.34398496240601506}
{"id": 1645, "code": "def exists ( self ) : if not isinstance ( self . val , str types ) : raise Type Error ( 'val is not a path' ) if not os . path . exists ( self . val ) : self . err ( 'Expected <%s> to exist, but was not found.' % self . val ) return self", "predictions": ["check if a file is already present . . . . . . . . . . . . . . . . . . ."], "references": ["asserts that val is a path and that it exists ."], "bleu": 0.05551277111446364, "rouge_l": 0.11663479923518164}
{"id": 1646, "code": "def is file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self", "predictions": ["check if a not exists"], "references": ["asserts that val is an existing path to a file ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 1647, "code": "def is directory ( self ) : self . exists ( ) if not os . path . isdir ( self . val ) : self . err ( 'Expected <%s> to be a directory, but was not.' % self . val ) return self", "predictions": ["whether this path is a type type some type some information some type ."], "references": ["asserts that val is an existing path to a directory ."], "bleu": 0.11114924776032006, "rouge_l": 0.24530831099195713}
{"id": 1648, "code": "def is named ( self , filename ) : self . is file ( ) if not isinstance ( filename , str types ) : raise Type Error ( 'given filename arg must be a path' ) val filename = os . path . basename ( os . path . abspath ( self . val ) ) if val filename != filename : self . err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val filename , filename ) ) return self", "predictions": ["a filename is a instance of the file ."], "references": ["asserts that val is an existing path to a file and that file is named filename ."], "bleu": 0.07636434640327666, "rouge_l": 0.29151732377538825}
{"id": 1649, "code": "def is child of ( self , parent ) : self . is file ( ) if not isinstance ( parent , str types ) : raise Type Error ( 'given parent directory arg must be a path' ) val abspath = os . path . abspath ( self . val ) parent abspath = os . path . abspath ( parent ) if not val abspath . startswith ( parent abspath ) : self . err ( 'Expected file <%s> to be a child of <%s>, but was not.' % ( val abspath , parent abspath ) ) return self", "predictions": ["return is a length of the parent file"], "references": ["asserts that val is an existing path to a file and that file is a child of parent ."], "bleu": 0.06345925250584064, "rouge_l": 0.2760180995475113}
{"id": 1650, "code": "def raises ( self , ex ) : if not callable ( self . val ) : raise Type Error ( 'val must be callable' ) if not issubclass ( ex , Base Exception ) : raise Type Error ( 'given arg must be exception' ) return Assertion Builder ( self . val , self . description , self . kind , ex )", "predictions": ["returns a contains value"], "references": ["asserts that val is callable and that when called raises the given error ."], "bleu": 0.0248009595334312, "rouge_l": 0.0}
{"id": 1651, "code": "def when called with ( self , * some args , * * some kwargs ) : if not self . expected : raise Type Error ( 'expected exception not set, raises() must be called first' ) try : self . val ( * some args , * * some kwargs ) except Base Exception as e : if issubclass ( type ( e ) , self . expected ) : return Assertion Builder ( str ( e ) , self . description , self . kind ) else : self . err ( 'Expected <%s> to raise <%s> when called with (%s), but raised <%s>.' % ( self . val . name , self . expected . name , self . fmt args kwargs ( * some args , * * some kwargs ) , type ( e ) . name ) ) self . err ( 'Expected <%s> to raise <%s> when called with (%s).' % ( self . val . name , self . expected . name , self . fmt args kwargs ( * some args , * * some kwargs ) ) )", "predictions": ["defer defer to error message"], "references": ["asserts the val callable when invoked with the given args and kwargs raises the expected exception ."], "bleu": 0.020826563663565452, "rouge_l": 0.0}
{"id": 1652, "code": "def err ( self , msg ) : out = '%s%s' % ( '[%s] ' % self . description if len ( self . description ) > 0 else '' , msg ) if self . kind == 'warn' : print ( out ) return self elif self . kind == 'soft' : global soft err soft err . append ( out ) return self else : raise Assertion Error ( out )", "predictions": ["interactively interactively interactively interactively interactively interactively add an error to the error"], "references": ["helper to raise an assertionerror and optionally prepend custom description ."], "bleu": 0.11498759556447223, "rouge_l": 0.08764367816091953}
{"id": 1653, "code": "def fmt args kwargs ( self , * some args , * * some kwargs ) : if some args : out args = str ( some args ) . lstrip ( '(' ) . rstrip ( ',)' ) if some kwargs : out kwargs = ', ' . join ( [ str ( i ) . lstrip ( '(' ) . rstrip ( ')' ) . replace ( ', ' , ': ' ) for i in [ ( k , some kwargs [ k ] ) for k in sorted ( some kwargs . keys ( ) ) ] ] ) if some args and some kwargs : return out args + ', ' + out kwargs elif some args : return out args elif some kwargs : return out kwargs else : return ''", "predictions": ["format an ascii representation of the function . . . . . . . . . . . ."], "references": ["helper to convert the given args and kwargs into a string ."], "bleu": 0.0712695567709093, "rouge_l": 0.1345093715545755}
{"id": 1654, "code": "def evaluate ( best processed path , model ) : x test char , x test type , y test = prepare feature ( best processed path , option = 'test' ) y predict = model . predict ( [ x test char , x test type ] ) y predict = ( y predict . ravel ( ) > 0.5 ) . astype ( int ) f1score = f1 score ( y test , y predict ) precision = precision score ( y test , y predict ) recall = recall score ( y test , y predict ) return f1score , precision , recall", "predictions": ["contains the % % and recall recall"], "references": ["evaluate model on splitted 10 percent testing set"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 1655, "code": "def document frequency ( X ) : if sp . isspmatrix csr ( X ) : return np . bincount ( X . indices , minlength = X . shape [ 1 ] ) return np . diff ( sp . csc matrix ( X , copy = False ) . indptr )", "predictions": ["get the does not use directly from the does not work yet"], "references": ["count the number of non - zero values for each feature in sparse x ."], "bleu": 0.08091975469641616, "rouge_l": 0.07261904761904761}
{"id": 1656, "code": "def create feature array ( text , n pad = 21 ) : n = len ( text ) n pad 2 = int ( ( n pad - 1 ) / 2 ) text pad = [ ' ' ] * n pad 2 + [ t for t in text ] + [ ' ' ] * n pad 2 x char , x type = [ ] , [ ] for i in range ( n pad 2 , n pad 2 + n ) : char list = text pad [ i + 1 : i + n pad 2 + 1 ] + list ( reversed ( text pad [ i - n pad 2 : i ] ) ) + [ text pad [ i ] ] char map = [ CHARS MAP . get ( c , 80 ) for c in char list ] char type = [ CHAR TYPES MAP . get ( CHAR TYPE FLATTEN . get ( c , 'o' ) , 4 ) for c in char list ] x char . append ( char map ) x type . append ( char type ) x char = np . array ( x char ) . astype ( float ) x type = np . array ( x type ) . astype ( float ) return x char , x type", "predictions": ["is a good array"], "references": ["create feature array of character and surrounding characters"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1657, "code": "def create n gram df ( df , n pad ) : n pad 2 = int ( ( n pad - 1 ) / 2 ) for i in range ( n pad 2 ) : df [ 'char-{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( i + 1 ) df [ 'type-{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( i + 1 ) df [ 'char{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( - i - 1 ) df [ 'type{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( - i - 1 ) return df [ n pad 2 : - n pad 2 ]", "predictions": ["is the empty empty empty df"], "references": ["given input dataframe create feature dataframe of shifted characters"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 1658, "code": "def dictfetchall ( self , cursor ) : columns = [ col [ 0 ] for col in cursor . description ] return [ dict ( zip ( columns , row ) ) for row in cursor . fetchall ( ) ]", "predictions": ["get information from table ."], "references": ["return all rows from a cursor as a dict ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 1659, "code": "def connect ( self ) : if Jwt Builder is None : raise Not Connected To Open Ed X ( \"This package must be installed in an Open Ed X environment.\" ) now = int ( time ( ) ) jwt = Jwt Builder . create jwt for user ( self . user ) self . client = Edx Rest Api Client ( self . API BASE URL , append slash = self . APPEND SLASH , jwt = jwt , ) self . expires at = now + self . expires in", "predictions": ["is this manager to make it easier to the app else else else else else else else false ."], "references": ["connect to the rest api authenticating with a jwt for the current user ."], "bleu": 0.09107438368292149, "rouge_l": 0.18692543411644533}
{"id": 1660, "code": "def refresh token ( func ) : @ wraps ( func ) def inner ( self , * args , * * kwargs ) : if self . token expired ( ) : self . connect ( ) return func ( self , * args , * * kwargs ) return inner", "predictions": ["decorator to is a decorator that passes a between a function if a between a between a between a between a between a between a between a between a between a"], "references": ["use this method decorator to ensure the jwt token is refreshed when needed ."], "bleu": 0.055177848898164926, "rouge_l": 0.14308053166536358}
{"id": 1661, "code": "def redirect if blocked ( course run ids , user = None , ip address = None , url = None ) : for course run id in course run ids : redirect url = embargo api . redirect if blocked ( Course Key . from string ( course run id ) , user = user , ip address = ip address , url = url ) if redirect url : return redirect url", "predictions": ["is the to is a to is to is to is to is to is the to is the to is the to is to is the to is the to"], "references": ["return redirect to embargo error page if the given user is blocked ."], "bleu": 0.046398855339878003, "rouge_l": 0.14722445695897024}
{"id": 1662, "code": "def get results ( self , identity provider , param name , param value , result field name ) : try : kwargs = { param name : param value } returned = self . client . providers ( identity provider ) . users . get ( * * kwargs ) results = returned . get ( 'results' , [ ] ) except Http Not Found Error : LOGGER . error ( 'username not found for third party provider={provider}, {querystring param}={id}' . format ( provider = identity provider , querystring param = param name , id = param value ) ) results = [ ] for row in results : if row . get ( param name ) == param value : return row . get ( result field name ) return None", "predictions": ["retrieve equal equal equal to a identity"], "references": ["calls the third party auth api endpoint to get the mapping between usernames and remote ids ."], "bleu": 0.04451531901458464, "rouge_l": 0.0775095298602287}
{"id": 1663, "code": "def course discovery api client ( user , catalog url ) : if Jwt Builder is None : raise Not Connected To Open Ed X ( ( \"To get a Catalog API client, this package must be \" \"installed in an Open ed X environment.\" ) ) jwt = Jwt Builder . create jwt for user ( user ) return Edx Rest Api Client ( catalog url , jwt = jwt )", "predictions": ["get the contains contains contains the contains contains a in the contains contains the contains in the contains client"], "references": ["return a course discovery api client setup with authentication for the specified user ."], "bleu": 0.07658412276041004, "rouge_l": 0.12461695607763024}
{"id": 1664, "code": "def transmit ( self , payload , * * kwargs ) : items to create , items to update , items to delete , transmission map = self . partition items ( payload ) self . transmit delete ( items to delete ) self . transmit create ( items to create ) self . transmit update ( items to update , transmission map )", "predictions": ["transmit the items in the roster"], "references": ["transmit content metadata items to the integrated channel ."], "bleu": 0.1593301391270729, "rouge_l": 0.3860759493670886}
{"id": 1665, "code": "def serialize items ( self , channel metadata items ) : return json . dumps ( self . prepare items for transmission ( channel metadata items ) , sort keys = True ) . encode ( 'utf-8' )", "predictions": ["serialize a channel for the given channel ."], "references": ["serialize content metadata items for a create transmission to the integrated channel ."], "bleu": 0.1396215680138453, "rouge_l": 0.45658682634730546}
{"id": 1666, "code": "def transmit create ( self , channel metadata item map ) : for chunk in chunks ( channel metadata item map , self . enterprise configuration . transmission chunk size ) : serialized chunk = self . serialize items ( list ( chunk . values ( ) ) ) try : self . client . create content metadata ( serialized chunk ) except Client Error as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . channel code , ) LOGGER . error ( exc ) else : self . create transmissions ( chunk )", "predictions": ["transmit all items in the client ."], "references": ["transmit content metadata creation to integrated channel ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1667, "code": "def transmit update ( self , channel metadata item map , transmission map ) : for chunk in chunks ( channel metadata item map , self . enterprise configuration . transmission chunk size ) : serialized chunk = self . serialize items ( list ( chunk . values ( ) ) ) try : self . client . update content metadata ( serialized chunk ) except Client Error as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . channel code , ) LOGGER . error ( exc ) else : self . update transmissions ( chunk , transmission map )", "predictions": ["transmit all items in the queue ."], "references": ["transmit content metadata update to integrated channel ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1668, "code": "def transmit delete ( self , channel metadata item map ) : for chunk in chunks ( channel metadata item map , self . enterprise configuration . transmission chunk size ) : serialized chunk = self . serialize items ( list ( chunk . values ( ) ) ) try : self . client . delete content metadata ( serialized chunk ) except Client Error as exc : LOGGER . error ( 'Failed to delete [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . channel code , ) LOGGER . error ( exc ) else : self . delete transmissions ( chunk . keys ( ) )", "predictions": ["transmit all items in the client ."], "references": ["transmit content metadata deletion to integrated channel ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1669, "code": "def create transmissions ( self , content metadata item map ) : Content Metadata Item Transmission = apps . get model ( 'integrated channel' , 'Content Metadata Item Transmission' ) transmissions = [ ] for content id , channel metadata in content metadata item map . items ( ) : transmissions . append ( Content Metadata Item Transmission ( enterprise customer = self . enterprise configuration . enterprise customer , integrated channel code = self . enterprise configuration . channel code ( ) , content id = content id , channel metadata = channel metadata ) ) Content Metadata Item Transmission . objects . bulk create ( transmissions )", "predictions": ["create all the transmissions entries in the database ."], "references": ["create contentmetadataitemtransmision models for the given content metadata items ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 1670, "code": "def update transmissions ( self , content metadata item map , transmission map ) : for content id , channel metadata in content metadata item map . items ( ) : transmission = transmission map [ content id ] transmission . channel metadata = channel metadata transmission . save ( )", "predictions": ["updates the transmissions metadata for a channel ."], "references": ["update contentmetadataitemtransmision models for the given content metadata items ."], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 1671, "code": "def delete transmissions ( self , content metadata item ids ) : Content Metadata Item Transmission = apps . get model ( 'integrated channel' , 'Content Metadata Item Transmission' ) Content Metadata Item Transmission . objects . filter ( enterprise customer = self . enterprise configuration . enterprise customer , integrated channel code = self . enterprise configuration . channel code ( ) , content id in = content metadata item ids ) . delete ( )", "predictions": ["deletes all items in a channel ."], "references": ["delete contentmetadataitemtransmision models associated with the given content metadata items ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 1672, "code": "def validate username ( self , value ) : try : user = User . objects . get ( username = value ) except User . Does Not Exist : raise serializers . Validation Error ( \"User does not exist\" ) try : enterprise customer user = models . Enterprise Customer User . objects . get ( user id = user . pk ) except models . Enterprise Customer User . Does Not Exist : raise serializers . Validation Error ( \"User has no Enterprise Customer User\" ) self . enterprise customer user = enterprise customer user return value", "predictions": ["check if username is valid ."], "references": ["verify that the username has a matching user and that the user has an associated enterprisecustomeruser ."], "bleu": 0.03908444433970424, "rouge_l": 0.1601049868766404}
{"id": 1673, "code": "def save ( self ) : course id = self . validated data [ 'course id' ] , created = models . Enterprise Course Enrollment . objects . get or create ( enterprise customer user = self . enterprise customer user , course id = course id , ) if created : track enrollment ( 'rest-api-enrollment' , self . enterprise customer user . user id , course id )", "predictions": ["saves this customer ."], "references": ["save the model with the found enterprisecustomeruser ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1674, "code": "def get groups ( self , obj ) : if obj . user : return [ group . name for group in obj . user . groups . filter ( name in = ENTERPRISE PERMISSION GROUPS ) ] return [ ]", "predictions": ["list all groups of a user"], "references": ["return the enterprise related django groups that this user is a part of ."], "bleu": 0.07321724281412775, "rouge_l": 0.18654434250764526}
{"id": 1675, "code": "def validate username ( self , value ) : try : self . user = User . objects . get ( username = value ) except User . Does Not Exist : raise serializers . Validation Error ( \"User does not exist\" ) return value", "predictions": ["check if username is valid"], "references": ["verify that the username has a matching user ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 1676, "code": "def save ( self ) : enterprise customer = self . validated data [ 'enterprise customer' ] ecu = models . Enterprise Customer User ( user id = self . user . pk , enterprise customer = enterprise customer , ) ecu . save ( )", "predictions": ["saves the customer ."], "references": ["save the enterprisecustomeruser ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 1677, "code": "def create ( self , validated data ) : ret = [ ] for attrs in validated data : if 'non field errors' not in attrs and not any ( isinstance ( attrs [ field ] , list ) for field in attrs ) : ret . append ( self . child . create ( attrs ) ) else : ret . append ( attrs ) return ret", "predictions": ["create a list of dictionaries"], "references": ["this selectively calls the child create method based on whether or not validation failed for each payload ."], "bleu": 0.020277584543100663, "rouge_l": 0.07891332470892627}
{"id": 1678, "code": "def to representation ( self , data ) : return [ self . child . to representation ( item ) if 'detail' in item else item for item in data ]", "predictions": ["convert a list of data to a list of items ."], "references": ["this selectively calls to_representation on each result that was processed by create ."], "bleu": 0.09497094417933137, "rouge_l": 0.08209959623149395}
{"id": 1679, "code": "def create ( self , validated data ) : enterprise customer = self . context . get ( 'enterprise customer' ) lms user = validated data . get ( 'lms user id' ) tpa user = validated data . get ( 'tpa user id' ) user email = validated data . get ( 'user email' ) course run id = validated data . get ( 'course run id' ) course mode = validated data . get ( 'course mode' ) cohort = validated data . get ( 'cohort' ) email students = validated data . get ( 'email students' ) is active = validated data . get ( 'is active' ) enterprise customer user = lms user or tpa user or user email if isinstance ( enterprise customer user , models . Enterprise Customer User ) : validated data [ 'enterprise customer user' ] = enterprise customer user try : if is active : enterprise customer user . enroll ( course run id , course mode , cohort = cohort ) else : enterprise customer user . unenroll ( course run id ) except ( Course Enrollment Downgrade Error , Course Enrollment Permission Error , Http Client Error ) as exc : validated data [ 'detail' ] = str ( exc ) return validated data if is active : track enrollment ( 'enterprise-customer-enrollment-api' , enterprise customer user . user id , course run id ) else : if is active : enterprise customer user = enterprise customer . enroll user pending registration ( user email , course mode , course run id , cohort = cohort ) else : enterprise customer . clear pending registration ( user email , course run id ) if email students : enterprise customer . notify enrolled learners ( self . context . get ( 'request user' ) , course run id , [ enterprise customer user ] ) validated data [ 'detail' ] = 'success' return validated data", "predictions": ["create a new customer"], "references": ["perform the enrollment for existing enterprise customer users or create the pending objects for new users ."], "bleu": 0.01656771518980191, "rouge_l": 0.17134831460674158}
{"id": 1680, "code": "def validate lms user id ( self , value ) : enterprise customer = self . context . get ( 'enterprise customer' ) try : return models . Enterprise Customer User . objects . get ( user id = value , enterprise customer = enterprise customer ) except models . Enterprise Customer User . Does Not Exist : pass return None", "predictions": ["validate lms id and return the customer id"], "references": ["validates the lms_user_id if is given to see if there is an existing enterprisecustomeruser for it ."], "bleu": 0.052063188264041965, "rouge_l": 0.0751231527093596}
{"id": 1681, "code": "def validate course run id ( self , value ) : enterprise customer = self . context . get ( 'enterprise customer' ) if not enterprise customer . catalog contains course ( value ) : raise serializers . Validation Error ( 'The course run id {course run id} is not in the catalog ' 'for Enterprise Customer {enterprise customer}' . format ( course run id = value , enterprise customer = enterprise customer . name , ) ) return value", "predictions": ["validate that a course id contains a valid course id ."], "references": ["validates that the course run id is part of the enterprise customer s catalog ."], "bleu": 0.09956647337521526, "rouge_l": 0.2993865030674847}
{"id": 1682, "code": "def validate ( self , data ) : lms user id = data . get ( 'lms user id' ) tpa user id = data . get ( 'tpa user id' ) user email = data . get ( 'user email' ) if not lms user id and not tpa user id and not user email : raise serializers . Validation Error ( 'At least one of the following fields must be specified and map to an Enterprise Customer User: ' 'lms user id, tpa user id, user email' ) return data", "predictions": ["validate that the user has an tpa"], "references": ["validate that at least one of the user identifier fields has been passed in ."], "bleu": 0.102601628763616, "rouge_l": 0.42657342657342656}
{"id": 1683, "code": "def create switch ( apps , schema editor ) : Switch = apps . get model ( 'waffle' , 'Switch' ) Switch . objects . update or create ( name = ENTERPRISE ROLE BASED ACCESS CONTROL SWITCH , defaults = { 'active' : False } )", "predictions": ["create the switch for the given model ."], "references": ["create the role_based_access_control switch if it does not already exist ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 1684, "code": "def delete switch ( apps , schema editor ) : Switch = apps . get model ( 'waffle' , 'Switch' ) Switch . objects . filter ( name = ENTERPRISE ROLE BASED ACCESS CONTROL SWITCH ) . delete ( )", "predictions": ["delete the entire switch model ."], "references": ["delete the role_based_access_control switch ."], "bleu": 0.3303164318013807, "rouge_l": 0.7393939393939394}
{"id": 1685, "code": "def create switch ( apps , schema editor ) : Switch = apps . get model ( 'waffle' , 'Switch' ) Switch . objects . get or create ( name = 'SAP USE ENTERPRISE ENROLLMENT PAGE' , defaults = { 'active' : False } )", "predictions": ["create the switch for the given model ."], "references": ["create and activate the sap_use_enterprise_enrollment_page switch if it does not already exist ."], "bleu": 0.10793517579160734, "rouge_l": 0.3652694610778443}
{"id": 1686, "code": "def handle transmission error ( self , learner data , request exception ) : try : sys msg = request exception . response . content except Attribute Error : pass else : if 'user account is inactive' in sys msg : ecu = Enterprise Customer User . objects . get ( enterprise enrollments id = learner data . enterprise course enrollment id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user id , ecu . user email , ecu . enterprise customer ) return super ( Sap Success Factors Learner Transmitter , self ) . handle transmission error ( learner data , request exception )", "predictions": ["process transmission transmission error ."], "references": ["handle the case where the employee on sapsf s side is marked as inactive ."], "bleu": 0.0369481680224917, "rouge_l": 0.09172932330827067}
{"id": 1687, "code": "def update throttle scope ( self ) : self . scope = SERVICE USER SCOPE self . rate = self . get rate ( ) self . num requests , self . duration = self . parse rate ( self . rate )", "predictions": ["update throttle scope ."], "references": ["update throttle scope so that service user throttle rates are applied ."], "bleu": 0.09569649651041093, "rouge_l": 0.45864661654135336}
{"id": 1688, "code": "def get learner data records ( self , enterprise enrollment , completed date = None , grade = None , is passing = False ) : Learner Data Transmission Audit = apps . get model ( 'integrated channel' , 'Learner Data Transmission Audit' ) completed timestamp = None course completed = False if completed date is not None : completed timestamp = parse datetime to epoch millis ( completed date ) course completed = is passing return [ Learner Data Transmission Audit ( enterprise course enrollment id = enterprise enrollment . id , course id = enterprise enrollment . course id , course completed = course completed , completed timestamp = completed timestamp , grade = grade , ) ]", "predictions": ["returns a list of course data model data"], "references": ["generate a learner data transmission audit with fields properly filled in ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 1689, "code": "def transmit ( self , payload , * * kwargs ) : items to create , items to update , items to delete , transmission map = self . partition items ( payload ) self . prepare items for delete ( items to delete ) prepared items = { } prepared items . update ( items to create ) prepared items . update ( items to update ) prepared items . update ( items to delete ) skip metadata transmission = False for chunk in chunks ( prepared items , self . enterprise configuration . transmission chunk size ) : chunked items = list ( chunk . values ( ) ) if skip metadata transmission : self . remove failed items ( chunked items , items to create , items to update , items to delete ) else : try : self . client . update content metadata ( self . serialize items ( chunked items ) ) except Client Error as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunked items ) , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . channel code , ) LOGGER . error ( exc ) self . remove failed items ( chunked items , items to create , items to update , items to delete ) skip metadata transmission = True self . create transmissions ( items to create ) self . update transmissions ( items to update , transmission map ) self . delete transmissions ( items to delete . keys ( ) )", "predictions": ["transmit the data ."], "references": ["transmit content metadata items to the integrated channel ."], "bleu": 0.12241977696855179, "rouge_l": 0.43160377358490565}
{"id": 1690, "code": "def handle ( self , * args , * * options ) : if not Course Enrollment : raise Not Connected To Open Ed X ( \"This package must be installed in an Open Ed X environment.\" ) days , enterprise customer = self . parse arguments ( * args , * * options ) if enterprise customer : try : lrs configuration = XAPILRS Configuration . objects . get ( active = True , enterprise customer = enterprise customer ) except XAPILRS Configuration . Does Not Exist : raise Command Error ( 'No x API Configuration found for \"{enterprise customer}\"' . format ( enterprise customer = enterprise customer . name ) ) self . send xapi statements ( lrs configuration , days ) else : for lrs configuration in XAPILRS Configuration . objects . filter ( active = True ) : self . send xapi statements ( lrs configuration , days )", "predictions": ["run the configuration ."], "references": ["send xapi statements ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 1691, "code": "def create session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires at is None or now >= self . expires at : if self . session : self . session . close ( ) oauth access token , expires at = self . get oauth access token ( self . enterprise configuration . key , self . enterprise configuration . secret , self . enterprise configuration . degreed user id , self . enterprise configuration . degreed user password , scope ) session = requests . Session ( ) session . timeout = self . SESSION TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth access token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires at = expires at", "predictions": ["create a session with the given scope ."], "references": ["instantiate a new session object for use in connecting with degreed"], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 1692, "code": "def ensure data exists ( self , request , data , error message = None ) : if not data : error message = ( error message or \"Unable to fetch API response from endpoint '{}'.\" . format ( request . get full path ( ) ) ) LOGGER . error ( error message ) raise Not Found ( error message )", "predictions": ["ensures that the data exists ."], "references": ["ensure that the wrapped api client s response brings us valid data . if not raise an error and log it ."], "bleu": 0.02295152567716108, "rouge_l": 0.2590233545647558}
{"id": 1693, "code": "def course enrollments ( self , request , pk ) : enterprise customer = self . get object ( ) serializer = serializers . Enterprise Customer Course Enrollments Serializer ( data = request . data , many = True , context = { 'enterprise customer' : enterprise customer , 'request user' : request . user , } ) if serializer . is valid ( ) : serializer . save ( ) return Response ( serializer . data , status = HTTP 200 OK ) return Response ( serializer . errors , status = HTTP 400 BAD REQUEST )", "predictions": ["get the course enrollments enrollments ."], "references": ["creates a course enrollment for an enterprisecustomeruser ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 1694, "code": "def with access to ( self , request , * args , * * kwargs ) : self . queryset = self . queryset . order by ( 'name' ) enterprise id = self . request . query params . get ( 'enterprise id' , None ) enterprise slug = self . request . query params . get ( 'enterprise slug' , None ) enterprise name = self . request . query params . get ( 'search' , None ) if enterprise id is not None : self . queryset = self . queryset . filter ( uuid = enterprise id ) elif enterprise slug is not None : self . queryset = self . queryset . filter ( slug = enterprise slug ) elif enterprise name is not None : self . queryset = self . queryset . filter ( name icontains = enterprise name ) return self . list ( request , * args , * * kwargs )", "predictions": ["filter the queryset with a list of resources ."], "references": ["returns the list of enterprise customers the user has a specified group permission access to ."], "bleu": 0.10148528609427183, "rouge_l": 0.3046192259675406}
{"id": 1695, "code": "def get missing params message ( self , parameter state ) : params = ', ' . join ( name for name , present in parameter state if not present ) return self . MISSING REQUIRED PARAMS MSG . format ( params )", "predictions": ["get the params for a given parameter ."], "references": ["get a user - friendly message indicating a missing parameter for the api endpoint ."], "bleu": 0.0914368785142959, "rouge_l": 0.32972972972972975}
{"id": 1696, "code": "def transform title ( self , content metadata item ) : title with locales = [ ] for locale in self . enterprise configuration . get locales ( ) : title with locales . append ( { 'locale' : locale , 'value' : content metadata item . get ( 'title' , '' ) } ) return title with locales", "predictions": ["return the title title title title to . partition"], "references": ["return the title of the content item ."], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 1697, "code": "def transform description ( self , content metadata item ) : description with locales = [ ] for locale in self . enterprise configuration . get locales ( ) : description with locales . append ( { 'locale' : locale , 'value' : ( content metadata item . get ( 'full description' ) or content metadata item . get ( 'short description' ) or content metadata item . get ( 'title' , '' ) ) } ) return description with locales", "predictions": ["return the items to include the items in the encode encode = true = false = 0 = 1 = false if they already already ."], "references": ["return the description of the content item ."], "bleu": 0.06980361417366379, "rouge_l": 0.2601279317697228}
{"id": 1698, "code": "def transform image ( self , content metadata item ) : image url = '' if content metadata item [ 'content type' ] in [ 'course' , 'program' ] : image url = content metadata item . get ( 'card image url' ) elif content metadata item [ 'content type' ] == 'courserun' : image url = content metadata item . get ( 'image url' ) return image url", "predictions": ["transmit the create create create the create create url url"], "references": ["return the image uri of the content item ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 1699, "code": "def transform courserun title ( self , content metadata item ) : title = content metadata item . get ( 'title' ) or '' course run start = content metadata item . get ( 'start' ) if course run start : if course available for enrollment ( content metadata item ) : title += ' ({starts}: {:%B %Y})' . format ( parse lms api datetime ( course run start ) , starts = ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment closed})' . format ( parse lms api datetime ( course run start ) , enrollment closed = ( 'Enrollment Closed' ) ) title with locales = [ ] content metadata language code = transform language code ( content metadata item . get ( 'content language' , '' ) ) for locale in self . enterprise configuration . get locales ( default locale = content metadata language code ) : title with locales . append ( { 'locale' : locale , 'value' : title } ) return title with locales", "predictions": ["return the update title title with the metadata of the enterprise"], "references": ["return the title of the courserun content item ."], "bleu": 0.1972940627795883, "rouge_l": 0.5091819699499166}
{"id": 1700, "code": "def transform courserun description ( self , content metadata item ) : description with locales = [ ] content metadata language code = transform language code ( content metadata item . get ( 'content language' , '' ) ) for locale in self . enterprise configuration . get locales ( default locale = content metadata language code ) : description with locales . append ( { 'locale' : locale , 'value' : ( content metadata item [ 'full description' ] or content metadata item [ 'short description' ] or content metadata item [ 'title' ] or '' ) } ) return description with locales", "predictions": ["transmit the description description description description description description to the delete description description description transmission transmission"], "references": ["return the description of the courserun content item ."], "bleu": 0.10878661088699644, "rouge_l": 0.2527624309392265}
{"id": 1701, "code": "def transform courserun schedule ( self , content metadata item ) : start = content metadata item . get ( 'start' ) or UNIX MIN DATE STRING end = content metadata item . get ( 'end' ) or UNIX MAX DATE STRING return [ { 'start Date' : parse datetime to epoch millis ( start ) , 'end Date' : parse datetime to epoch millis ( end ) , 'active' : current time is in interval ( start , end ) } ]", "predictions": ["create a schedule for the transmissions schedule schedule ."], "references": ["return the schedule of the courseun content item ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 1702, "code": "def get content id ( self , content metadata item ) : content id = content metadata item . get ( 'key' , '' ) if content metadata item [ 'content type' ] == 'program' : content id = content metadata item . get ( 'uuid' , '' ) return content id", "predictions": ["update the transmissions id id id id"], "references": ["return the id for the given content_metadata_item uuid for programs or key for other content"], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 1703, "code": "def chunks ( dictionary , chunk size ) : iterable = iter ( dictionary ) for in range ( 0 , len ( dictionary ) , chunk size ) : yield { key : dictionary [ key ] for key in islice ( iterable , chunk size ) }", "predictions": ["splits a dictionary into delete chunks ."], "references": ["yield successive n - sized chunks from dictionary ."], "bleu": 0.16599826150636804, "rouge_l": 0.24448897795591182}
{"id": 1704, "code": "def get link by email ( self , user email ) : try : user = User . objects . get ( email = user email ) try : return self . get ( user id = user . id ) except Enterprise Customer User . Does Not Exist : pass except User . Does Not Exist : pass try : return Pending Enterprise Customer User . objects . get ( user email = user email ) except Pending Enterprise Customer User . Does Not Exist : pass return None", "predictions": ["except for a username returns a username"], "references": ["return link by email ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1705, "code": "def enterprise customer uuid ( self ) : try : enterprise user = Enterprise Customer User . objects . get ( user id = self . user . id ) except Object Does Not Exist : LOGGER . warning ( 'User {} has a {} assignment but is not linked to an enterprise!' . format ( self . class , self . user . id ) ) return None except Multiple Objects Returned : LOGGER . warning ( 'User {} is linked to multiple enterprises, which is not yet supported!' . format ( self . user . id ) ) return None return str ( enterprise user . enterprise customer . uuid )", "predictions": ["save the customer self . self . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["get the enterprise customer uuid linked to the user ."], "bleu": 0.046398855339878003, "rouge_l": 0.16123348017621145}
{"id": 1706, "code": "def export ( self ) : content metadata export = { } content metadata items = self . enterprise api . get content metadata ( self . enterprise customer ) LOGGER . info ( 'Retrieved content metadata for enterprise [%s]' , self . enterprise customer . name ) for item in content metadata items : transformed = self . transform item ( item ) LOGGER . info ( 'Exporting content metadata item with plugin configuration [%s]: [%s]' , self . enterprise configuration , json . dumps ( transformed , indent = 4 ) , ) content metadata item export = Content Metadata Item Export ( item , transformed ) content metadata export [ content metadata item export . content id ] = content metadata item export return Ordered Dict ( sorted ( content metadata export . items ( ) ) )", "predictions": ["get task metadata metadata return dictionary"], "references": ["return the exported and transformed content metadata as a dictionary ."], "bleu": 0.1141650334026257, "rouge_l": 0.2234432234432234}
{"id": 1707, "code": "def transform item ( self , content metadata item ) : content metadata type = content metadata item [ 'content type' ] transformed item = { } for integrated channel schema key , edx data schema key in self . DATA TRANSFORM MAPPING . items ( ) : transformer = ( getattr ( self , 'transform {content type} {edx data schema key}' . format ( content type = content metadata type , edx data schema key = edx data schema key ) , None ) or getattr ( self , 'transform {edx data schema key}' . format ( edx data schema key = edx data schema key ) , None ) ) if transformer : transformed item [ integrated channel schema key ] = transformer ( content metadata item ) else : try : transformed item [ integrated channel schema key ] = content metadata item [ edx data schema key ] except Key Error : LOGGER . exception ( 'Failed to transform content metadata item field [%s] for [%s]: [%s]' , edx data schema key , self . enterprise customer . name , content metadata item , ) return transformed item", "predictions": ["validate each serializers username and replace with a valid serializers schema . . . . . . . . . . . . . . . . . . . ."], "references": ["transform the provided content metadata item to the schema expected by the integrated channel ."], "bleu": 0.04317900023606586, "rouge_l": 0.09277566539923954}
{"id": 1708, "code": "def get consent record ( self , request ) : username , course id , program uuid , enterprise customer uuid = self . get required query params ( request ) return get data sharing consent ( username , enterprise customer uuid , course id = course id , program uuid = program uuid )", "predictions": ["returns a consent self . which is active for this program"], "references": ["get the consent record relevant to the request at hand ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 1709, "code": "def get no record response ( self , request ) : username , course id , program uuid , enterprise customer uuid = self . get required query params ( request ) data = { self . REQUIRED PARAM USERNAME : username , self . REQUIRED PARAM ENTERPRISE CUSTOMER : enterprise customer uuid , self . CONSENT EXISTS : False , self . CONSENT GRANTED : False , self . CONSENT REQUIRED : False , } if course id : data [ self . REQUIRED PARAM COURSE ID ] = course id if program uuid : data [ self . REQUIRED PARAM PROGRAM UUID ] = program uuid return Response ( data , status = HTTP 200 OK )", "predictions": ["create a no response response instance"], "references": ["get an httpresponse that can be used when there s no related enterprisecustomer ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 1710, "code": "def ready ( self ) : from enterprise . signals import handle user post save from django . db . models . signals import pre migrate , post save post save . connect ( handle user post save , sender = self . auth user model , dispatch uid = USER POST SAVE DISPATCH UID ) pre migrate . connect ( self . disconnect user post save for migrations )", "predictions": ["handle to update migrations after a child of migrations . . . . . . . . . . . . . . . . . . . . . ."], "references": ["perform other one - time initialization steps ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 1711, "code": "def disconnect user post save for migrations ( self , sender , * * kwargs ) : from django . db . models . signals import post save post save . disconnect ( sender = self . auth user model , dispatch uid = USER POST SAVE DISPATCH UID )", "predictions": ["create user validated validated validated when a user is closed lms lms lms lms lms lms lms lms lms lms lms lms lms lms lms lms lms lms lms lms lms"], "references": ["handle pre_migrate signal - disconnect user post_save handler ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 1712, "code": "def get actor ( self , username , email ) : return Agent ( name = username , mbox = 'mailto:{email}' . format ( email = email ) , )", "predictions": ["return for the lms"], "references": ["get actor for the statement ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 1713, "code": "def get object ( self , name , description ) : return Activity ( id = X API ACTIVITY COURSE , definition = Activity Definition ( name = Language Map ( { 'en-US' : ( name or '' ) . encode ( \"ascii\" , \"ignore\" ) . decode ( 'ascii' ) } ) , description = Language Map ( { 'en-US' : ( description or '' ) . encode ( \"ascii\" , \"ignore\" ) . decode ( 'ascii' ) } ) , ) , )", "predictions": ["validate course course course course ."], "references": ["get object for the statement ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1714, "code": "def clean course ( self ) : course id = self . cleaned data [ self . Fields . COURSE ] . strip ( ) if not course id : return None try : client = Enrollment Api Client ( ) return client . get course details ( course id ) except ( Http Client Error , Http Server Error ) : raise Validation Error ( Validation Messages . INVALID COURSE ID . format ( course id = course id ) )", "predictions": ["check if this course exists"], "references": ["verify course id and retrieve course details ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1715, "code": "def clean notify ( self ) : return self . cleaned data . get ( self . Fields . NOTIFY , self . Notification Types . DEFAULT )", "predictions": ["create and return the switch switch . . . . . . . . . . . . . . . . . . . ."], "references": ["clean the notify_on_enrollment field ."], "bleu": 0.051660454541342535, "rouge_l": 0.14698795180722893}
{"id": 1716, "code": "def validate course ( self ) : course details = self . cleaned data . get ( self . Fields . COURSE ) if course details : course mode = self . cleaned data . get ( self . Fields . COURSE MODE ) if not course mode : raise Validation Error ( Validation Messages . COURSE WITHOUT COURSE MODE ) valid course modes = course details [ \"course modes\" ] if all ( course mode != mode [ \"slug\" ] for mode in valid course modes ) : error = Validation Error ( Validation Messages . COURSE MODE INVALID FOR COURSE . format ( course mode = course mode , course id = course details [ \"course id\" ] , ) ) raise Validation Error ( { self . Fields . COURSE MODE : error } )", "predictions": ["delete all the switch data"], "references": ["verify that the selected mode is valid for the given course ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 1717, "code": "def validate program ( self ) : program = self . cleaned data . get ( self . Fields . PROGRAM ) if not program : return course runs = get course runs from program ( program ) try : client = Course Catalog Api Client ( self . user , self . enterprise customer . site ) available modes = client . get common course modes ( course runs ) course mode = self . cleaned data . get ( self . Fields . COURSE MODE ) except ( Http Client Error , Http Server Error ) : raise Validation Error ( Validation Messages . FAILED TO OBTAIN COURSE MODES . format ( program title = program . get ( \"title\" ) ) ) if not course mode : raise Validation Error ( Validation Messages . COURSE WITHOUT COURSE MODE ) if course mode not in available modes : raise Validation Error ( Validation Messages . COURSE MODE NOT AVAILABLE . format ( mode = course mode , program title = program . get ( \"title\" ) , modes = \", \" . join ( available modes ) ) )", "predictions": ["create a switch if it does not exist ."], "references": ["verify that selected mode is available for program and all courses in the program"], "bleu": 0.06809538093398164, "rouge_l": 0.0}
{"id": 1718, "code": "def clean ( self ) : cleaned data = super ( Enterprise Customer Reporting Config Admin Form , self ) . clean ( ) report customer = cleaned data . get ( 'enterprise customer' ) invalid catalogs = [ '{} ({})' . format ( catalog . title , catalog . uuid ) for catalog in cleaned data . get ( 'enterprise customer catalogs' ) if catalog . enterprise customer != report customer ] if invalid catalogs : message = ( 'These catalogs for reporting do not match enterprise' 'customer {enterprise customer}: {invalid catalogs}' , ) . format ( enterprise customer = report customer , invalid catalogs = invalid catalogs , ) self . add error ( 'enterprise customer catalogs' , message )", "predictions": ["check that the content of the content is not registered ."], "references": ["override of clean method to perform additional validation"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1719, "code": "def verify edx resources ( ) : required methods = { 'Program Data Extender' : Program Data Extender , } for method in required methods : if required methods [ method ] is None : raise Not Connected To Open Ed X ( ( \"The following method from the Open ed X platform is necessary for this view but isn't available.\" ) + \"\\n Unavailable: {method}\" . format ( method = method ) )", "predictions": ["find all required scope that were passed to the platform"], "references": ["ensure that all necessary resources to render the view are present ."], "bleu": 0.12977836824680314, "rouge_l": 0.2683284457478006}
{"id": 1720, "code": "def get global context ( request , enterprise customer ) : platform name = get configuration value ( \"PLATFORM NAME\" , settings . PLATFORM NAME ) return { 'enterprise customer' : enterprise customer , 'LMS SEGMENT KEY' : settings . LMS SEGMENT KEY , 'LANGUAGE CODE' : get language from request ( request ) , 'tagline' : get configuration value ( \"ENTERPRISE TAGLINE\" , settings . ENTERPRISE TAGLINE ) , 'platform description' : get configuration value ( \"PLATFORM DESCRIPTION\" , settings . PLATFORM DESCRIPTION , ) , 'LMS ROOT URL' : settings . LMS ROOT URL , 'platform name' : platform name , 'header logo alt text' : ( '{platform name} home page' ) . format ( platform name = platform name ) , 'welcome text' : constants . WELCOME TEXT . format ( platform name = platform name ) , 'enterprise welcome text' : constants . ENTERPRISE WELCOME TEXT . format ( enterprise customer name = enterprise customer . name , platform name = platform name , strong start = '<strong>' , strong end = '</strong>' , line break = '<br/>' , privacy policy link start = \"<a href='{pp url}' target=' blank'>\" . format ( pp url = get configuration value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy policy link end = \"</a>\" , ) , }", "predictions": ["build the learner to display in the learner . ."], "references": ["get the set of variables that are needed by default across views ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 1721, "code": "def render page with error code message ( request , context data , error code , log message ) : LOGGER . error ( log message ) messages . add generic error message with code ( request , error code ) return render ( request , ENTERPRISE GENERAL ERROR PAGE , context = context data , status = 404 , )", "predictions": ["transmit a page page * error *"], "references": ["return a 404 page with specified error_code after logging error and adding message to django messages ."], "bleu": 0.05293793409875998, "rouge_l": 0.23252858958068615}
{"id": 1722, "code": "def course or program exist ( self , course id , program uuid ) : course exists = course id and Course Api Client ( ) . get course details ( course id ) program exists = program uuid and Course Catalog Api Service Client ( ) . program exists ( program uuid ) return course exists or program exists", "predictions": ["returns true if a self exists is available to be included in a self ."], "references": ["return whether the input course or program exist ."], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 1723, "code": "def get default context ( self , enterprise customer , platform name ) : context data = { 'page title' : ( 'Data sharing consent required' ) , 'consent message header' : ( 'Consent to share your data' ) , 'requested permissions header' : ( 'Per the {start link}Data Sharing Policy{end link}, ' '{bold start}{enterprise customer name}{bold end} would like to know about:' ) . format ( enterprise customer name = enterprise customer . name , bold start = '<b>' , bold end = '</b>' , start link = '<a href=\"#consent-policy-dropdown-bar\" ' 'class=\"policy-dropdown-link background-input\" id=\"policy-dropdown-link\">' , end link = '</a>' , ) , 'agreement text' : ( 'I agree to allow {platform name} to share data about my enrollment, completion and performance in all ' '{platform name} courses and programs where my enrollment is sponsored by {enterprise customer name}.' ) . format ( enterprise customer name = enterprise customer . name , platform name = platform name , ) , 'continue text' : ( 'Yes, continue' ) , 'abort text' : ( 'No, take me back.' ) , 'policy dropdown header' : ( 'Data Sharing Policy' ) , 'sharable items header' : ( 'Enrollment, completion, and performance data that may be shared with {enterprise customer name} ' '(or its designee) for these courses and programs are limited to the following:' ) . format ( enterprise customer name = enterprise customer . name ) , 'sharable items' : [ ( 'My email address for my {platform name} account, ' 'and the date when I created my {platform name} account' ) . format ( platform name = platform name ) , ( 'My {platform name} ID, and if I log in via single sign-on, ' 'my {enterprise customer name} SSO user-ID' ) . format ( platform name = platform name , enterprise customer name = enterprise customer . name , ) , ( 'My {platform name} username' ) . format ( platform name = platform name ) , ( 'My country or region of residence' ) , ( 'What courses and/or programs I\\'ve enrolled in or unenrolled from, what track I ' 'enrolled in (audit or verified) and the date when I enrolled in each course or program' ) , ( 'Information about each course or program I\\'ve enrolled in, ' 'including its duration and level of effort required' ) , ( 'Whether I completed specific parts of each course or program (for example, whether ' 'I watched a given video or completed a given homework assignment)' ) , ( 'My overall percentage completion of each course or program on a periodic basis, ' 'including the total time spent in each course or program and the date when I last ' 'logged in to each course or program' ) , ( 'My performance in each course or program' ) , ( 'My final grade in each course or program, and the date when I completed each course or program' ) , ( 'Whether I received a certificate in each course or program' ) , ] , 'sharable items footer' : ( 'My permission applies only to data from courses or programs that are sponsored by ' '{enterprise customer name}, and not to data from any {platform name} courses or programs that ' 'I take on my own. I understand that I may withdraw my permission only by fully unenrolling ' 'from any courses or programs that are sponsored by {enterprise customer name}.' ) . format ( enterprise customer name = enterprise customer . name , platform name = platform name , ) , 'sharable items note header' : ( 'Please note' ) , 'sharable items notes' : [ ( 'If you decline to consent, that fact may be shared with {enterprise customer name}.' ) . format ( enterprise customer name = enterprise customer . name ) , ] , 'confirmation modal header' : ( 'Are you aware...' ) , 'confirmation modal affirm decline text' : ( 'I decline' ) , 'confirmation modal abort decline text' : ( 'View the data sharing policy' ) , 'policy link template' : ( 'View the {start link}data sharing policy{end link}.' ) . format ( start link = '<a href=\"#consent-policy-dropdown-bar\" class=\"policy-dropdown-link background-input\" ' 'id=\"policy-dropdown-link\">' , end link = '</a>' , ) , 'policy return link text' : ( 'Return to Top' ) , } return context data", "predictions": ["get the session utcnow context for a given now = 0 ."], "references": ["get the set of variables that will populate the template by default ."], "bleu": 0.13519230385081712, "rouge_l": 0.23828125000000006}
{"id": 1724, "code": "def get course or program context ( self , enterprise customer , course id = None , program uuid = None ) : context data = { } if course id : context data . update ( { 'course id' : course id , 'course specific' : True } ) if not self . preview mode : try : catalog api client = Course Catalog Api Service Client ( enterprise customer . site ) except Improperly Configured : raise Http404 course run details = catalog api client . get course run ( course id ) course start date = '' if course run details [ 'start' ] : course start date = parse ( course run details [ 'start' ] ) . strftime ( '%B %d, %Y' ) context data . update ( { 'course title' : course run details [ 'title' ] , 'course start date' : course start date , } ) else : context data . update ( { 'course title' : 'Demo Course' , 'course start date' : datetime . datetime . now ( ) . strftime ( '%B %d, %Y' ) , } ) else : context data . update ( { 'program uuid' : program uuid , 'program specific' : True , } ) return context data", "predictions": ["returns the self exists and program"], "references": ["return a dict having course or program specific keys for data sharing consent page ."], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 1725, "code": "def post ( self , request ) : enterprise uuid = request . POST . get ( 'enterprise customer uuid' ) success url = request . POST . get ( 'redirect url' ) failure url = request . POST . get ( 'failure url' ) course id = request . POST . get ( 'course id' , '' ) program uuid = request . POST . get ( 'program uuid' , '' ) enterprise customer = get enterprise customer or 404 ( enterprise uuid ) context data = get global context ( request , enterprise customer ) if not ( enterprise uuid and success url and failure url ) : error code = 'ENTGDS005' log message = ( 'Error: one or more of the following values was falsy: ' 'enterprise uuid: {enterprise uuid}, ' 'success url: {success url}, ' 'failure url: {failure url} for course id {course id}. ' 'The following error code was reported to the user {userid}: {error code}' . format ( userid = request . user . id , enterprise uuid = enterprise uuid , success url = success url , failure url = failure url , error code = error code , course id = course id , ) ) return render page with error code message ( request , context data , error code , log message ) if not self . course or program exist ( course id , program uuid ) : error code = 'ENTGDS006' log message = ( 'Neither the course with course id: {course id} ' 'or program with {program uuid} exist for ' 'enterprise customer {enterprise uuid}' 'Error code {error code} presented to user {userid}' . format ( course id = course id , program uuid = program uuid , error code = error code , userid = request . user . id , enterprise uuid = enterprise uuid , ) ) return render page with error code message ( request , context data , error code , log message ) consent record = get data sharing consent ( request . user . username , enterprise uuid , program uuid = program uuid , course id = course id ) if consent record is None : error code = 'ENTGDS007' log message = ( 'The was a problem with the consent record of user {userid} with ' 'enterprise uuid {enterprise uuid}. consent record has a value ' 'of {consent record} and a ' 'value for course id {course id}. ' 'Error code {error code} presented to user' . format ( userid = request . user . id , enterprise uuid = enterprise uuid , consent record = consent record , error code = error code , course id = course id , ) ) return render page with error code message ( request , context data , error code , log message ) defer creation = request . POST . get ( 'defer creation' ) consent provided = bool ( request . POST . get ( 'data sharing consent' , False ) ) if defer creation is None and consent record . consent required ( ) : if course id : enterprise customer user , = Enterprise Customer User . objects . get or create ( enterprise customer = consent record . enterprise customer , user id = request . user . id ) enterprise customer user . update session ( request ) , created = Enterprise Course Enrollment . objects . get or create ( enterprise customer user = enterprise customer user , course id = course id , ) if created : track enrollment ( 'data-consent-page-enrollment' , request . user . id , course id , request . path ) consent record . granted = consent provided consent record . save ( ) return redirect ( success url if consent provided else failure url )", "predictions": ["handle course course course course course course course"], "references": ["process the above form ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1726, "code": "def set final prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final price' ] = Ecommerce Api Client ( request . user ) . get course final price ( mode = mode , enterprise catalog uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result", "predictions": ["with a list of all to the to access the course"], "references": ["set the final discounted price on each premium mode ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 1727, "code": "def post ( self , request , enterprise uuid , program uuid ) : verify edx resources ( ) enterprise customer = get enterprise customer or 404 ( enterprise uuid ) with transaction . atomic ( ) : enterprise customer user , = Enterprise Customer User . objects . get or create ( enterprise customer = enterprise customer , user id = request . user . id ) enterprise customer user . update session ( request ) context data = get global context ( request , enterprise customer ) program details , error code = self . get program details ( request , program uuid , enterprise customer ) if error code : return render ( request , ENTERPRISE GENERAL ERROR PAGE , context = context data , status = 404 , ) if program details [ 'certificate eligible for program' ] : return redirect ( LMS PROGRAMS DASHBOARD URL . format ( uuid = program uuid ) ) basket page = '{basket url}?{params}' . format ( basket url = BASKET URL , params = urlencode ( [ tuple ( [ 'sku' , sku ] ) for sku in program details [ 'skus' ] ] + [ tuple ( [ 'bundle' , program uuid ] ) ] ) ) if get data sharing consent ( enterprise customer user . username , enterprise customer . uuid , program uuid = program uuid , ) . consent required ( ) : return redirect ( '{grant data sharing url}?{params}' . format ( grant data sharing url = reverse ( 'grant data sharing permissions' ) , params = urlencode ( { 'next' : basket page , 'failure url' : reverse ( 'enterprise program enrollment page' , args = [ enterprise customer . uuid , program uuid ] ) , 'enterprise customer uuid' : enterprise customer . uuid , 'program uuid' : program uuid , } ) ) ) return redirect ( basket page )", "predictions": ["create a parameter if needed ."], "references": ["process a submitted track selection form for the enterprise ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 1728, "code": "def redirect ( self , request , * args , * * kwargs ) : enterprise customer uuid , course run id , course key , program uuid = Router View . get path variables ( * * kwargs ) resource id = course key or course run id or program uuid path = re . sub ( '{}|{}' . format ( enterprise customer uuid , re . escape ( resource id ) ) , '{}' , request . path ) kwargs . pop ( 'course key' , None ) return self . VIEWS [ path ] . as view ( ) ( request , * args , * * kwargs )", "predictions": ["redirect to a customer ."], "references": ["redirects to the appropriate view depending on where the user came from ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 1729, "code": "def post ( self , request , * args , * * kwargs ) : enterprise customer uuid , course run id , course key , program uuid = Router View . get path variables ( * * kwargs ) enterprise customer = get enterprise customer or 404 ( enterprise customer uuid ) if course key : context data = get global context ( request , enterprise customer ) try : kwargs [ 'course id' ] = Router View . get course run id ( request . user , enterprise customer , course key ) except Http404 : error code = 'ENTRV001' log message = ( 'Could not find course run with id {course run id} ' 'for course key {course key} and ' 'for enterprise customer uuid {enterprise customer uuid} ' 'and program {program uuid}. ' 'Returned error code {error code} to user {userid}' . format ( course key = course key , course run id = course run id , enterprise customer uuid = enterprise customer uuid , error code = error code , userid = request . user . id , program uuid = program uuid , ) ) return render page with error code message ( request , context data , error code , log message ) return self . redirect ( request , * args , * * kwargs )", "predictions": ["example post function ."], "references": ["run some custom post logic for enterprise workflows before routing the user through existing views ."], "bleu": 0.019797099072043068, "rouge_l": 0.1804733727810651}
{"id": 1730, "code": "def default content filter ( sender , instance , * * kwargs ) : if kwargs [ 'created' ] and not instance . content filter : instance . content filter = get default catalog content filter ( ) instance . save ( )", "predictions": ["ensure the content of the catalog is default ."], "references": ["set default value for enterprisecustomercatalog . content_filter if not already set ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 1731, "code": "def assign enterprise learner role ( sender , instance , * * kwargs ) : if kwargs [ 'created' ] and instance . user : enterprise learner role , = System Wide Enterprise Role . objects . get or create ( name = ENTERPRISE LEARNER ROLE ) System Wide Enterprise User Role Assignment . objects . get or create ( user = instance . user , role = enterprise learner role )", "predictions": ["assign a user to a learner instance ."], "references": ["assign an enterprise learner role to enterprisecustomeruser whenever a new record is created ."], "bleu": 0.09969456998702547, "rouge_l": 0.346590909090909}
{"id": 1732, "code": "def delete enterprise learner role assignment ( sender , instance , * * kwargs ) : if instance . user : enterprise learner role , = System Wide Enterprise Role . objects . get or create ( name = ENTERPRISE LEARNER ROLE ) try : System Wide Enterprise User Role Assignment . objects . get ( user = instance . user , role = enterprise learner role ) . delete ( ) except System Wide Enterprise User Role Assignment . Does Not Exist : pass", "predictions": ["delete user when deleted assignment is deleted ."], "references": ["delete the associated enterprise learner role assignment record when deleting an enterprisecustomeruser record ."], "bleu": 0.09525245831601728, "rouge_l": 0.25994318181818177}
{"id": 1733, "code": "def create roles ( apps , schema editor ) : System Wide Enterprise Role = apps . get model ( 'enterprise' , 'System Wide Enterprise Role' ) System Wide Enterprise Role . objects . update or create ( name = ENTERPRISE ADMIN ROLE ) System Wide Enterprise Role . objects . update or create ( name = ENTERPRISE LEARNER ROLE )", "predictions": ["create the roles for the given schema ."], "references": ["create the enterprise roles if they do not already exist ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 1734, "code": "def delete roles ( apps , schema editor ) : System Wide Enterprise Role = apps . get model ( 'enterprise' , 'System Wide Enterprise Role' ) System Wide Enterprise Role . objects . filter ( name in = [ ENTERPRISE ADMIN ROLE , ENTERPRISE LEARNER ROLE ] ) . delete ( )", "predictions": ["delete all roles that don t have been done ."], "references": ["delete the enterprise roles ."], "bleu": 0.14991106946711685, "rouge_l": 0.42558139534883715}
{"id": 1735, "code": "def get enterprise admin users batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise admin users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups name = ENTERPRISE DATA API ACCESS GROUP , is staff = False ) [ start : end ]", "predictions": ["returns all users which have a batch of users ."], "references": ["returns a batched queryset of user objects ."], "bleu": 0.15851165692617156, "rouge_l": 0.4535315985130111}
{"id": 1736, "code": "def get enterprise operator users batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise operator users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups name = ENTERPRISE DATA API ACCESS GROUP , is staff = True ) [ start : end ]", "predictions": ["get batch operator of a batch operator ."], "references": ["returns a batched queryset of user objects ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 1737, "code": "def get enterprise customer users batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise customer users from indexes: %s to %s' , start , end ) return User . objects . filter ( pk in = self . get enterprise customer user ids ( ) ) [ start : end ]", "predictions": ["return a batch of users who have a batch ."], "references": ["returns a batched queryset of enterprisecustomeruser objects ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 1738, "code": "def get enterprise enrollment api admin users batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise enrollment admin users from indexes: %s to %s' , start , end ) return User . objects . filter ( groups name = ENTERPRISE ENROLLMENT API ACCESS GROUP , is staff = False ) [ start : end ]", "predictions": ["get all users which are associated with a batch ."], "references": ["returns a batched queryset of user objects ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 1739, "code": "def get enterprise catalog admin users batch ( self , start , end ) : Application = apps . get model ( OAUTH2 PROVIDER APPLICATION MODEL ) LOGGER . info ( 'Fetching new batch of enterprise catalog admin users from indexes: %s to %s' , start , end ) catalog admin user ids = Application . objects . filter ( user id in = self . get enterprise customer user ids ( ) ) . exclude ( name = EDX ORG NAME ) . values ( 'user id' ) return User . objects . filter ( pk in = catalog admin user ids ) [ start : end ]", "predictions": ["return all users which are admin from the catalog admin admin admin ."], "references": ["returns a batched queryset of user objects ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 1740, "code": "def assign enterprise role to users ( self , get batch method , options , is feature role = False ) : role name = options [ 'role' ] batch limit = options [ 'batch limit' ] batch sleep = options [ 'batch sleep' ] batch offset = options [ 'batch offset' ] current batch index = batch offset users batch = get batch method ( batch offset , batch offset + batch limit ) role class = System Wide Enterprise Role role assignment class = System Wide Enterprise User Role Assignment if is feature role : role class = Enterprise Feature Role role assignment class = Enterprise Feature User Role Assignment enterprise role = role class . objects . get ( name = role name ) while users batch . count ( ) > 0 : for index , user in enumerate ( users batch ) : LOGGER . info ( 'Processing user with index %s and id %s' , current batch index + index , user . id ) role assignment class . objects . get or create ( user = user , role = enterprise role ) sleep ( batch sleep ) current batch index += len ( users batch ) users batch = get batch method ( current batch index , current batch index + batch limit )", "predictions": ["assign a role to a role to a role ."], "references": ["assigns enterprise role to users ."], "bleu": 0.17827531042796255, "rouge_l": 0.3927038626609442}
{"id": 1741, "code": "def handle ( self , * args , * * options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE ADMIN ROLE : self . assign enterprise role to users ( self . get enterprise admin users batch , options ) elif role == ENTERPRISE OPERATOR ROLE : self . assign enterprise role to users ( self . get enterprise operator users batch , options ) elif role == ENTERPRISE LEARNER ROLE : self . assign enterprise role to users ( self . get enterprise customer users batch , options ) elif role == ENTERPRISE ENROLLMENT API ADMIN ROLE : self . assign enterprise role to users ( self . get enterprise enrollment api admin users batch , options , True ) elif role == ENTERPRISE CATALOG ADMIN ROLE : self . assign enterprise role to users ( self . get enterprise catalog admin users batch , options , True ) else : raise Command Error ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE ADMIN ROLE , learner = ENTERPRISE LEARNER ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )", "predictions": ["main command loop ."], "references": ["entry point for managment command execution ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 1742, "code": "def get enterprise customer for running pipeline ( request , pipeline ) : sso provider id = request . GET . get ( 'tpa hint' ) if pipeline : sso provider id = Registry . get from pipeline ( pipeline ) . provider id return get enterprise customer for sso ( sso provider id )", "predictions": ["pass through to provider . . ."], "references": ["get the enterprisecustomer associated with a running pipeline ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1743, "code": "def create session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION TIMEOUT oauth access token , expires at = SAP Success Factors API Client . get oauth access token ( self . enterprise configuration . sapsf base url , self . enterprise configuration . key , self . enterprise configuration . secret , self . enterprise configuration . sapsf company id , self . enterprise configuration . sapsf user id , self . enterprise configuration . user type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth access token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires at = expires at", "predictions": ["create the session with the given requests ."], "references": ["instantiate a new session object for use in connecting with sap successfactors"], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 1744, "code": "def call search students recursively ( self , sap search student url , all inactive learners , page size , start at ) : search student paginated url = '{sap search student url}&{pagination criterion}' . format ( sap search student url = sap search student url , pagination criterion = '$count=true&$top={page size}&$skip={start at}' . format ( page size = page size , start at = start at , ) , ) try : response = self . session . get ( search student paginated url ) sap inactive learners = response . json ( ) except ( Connection Error , Timeout ) : LOGGER . warning ( 'Unable to fetch inactive learners from SAP search Student API with url ' '\"{%s}\".' , search student paginated url , ) return None if 'error' in sap inactive learners : LOGGER . warning ( 'SAP search Student API for customer %s and base url %s returned response with ' 'error message \"%s\" and with error code \"%s\".' , self . enterprise configuration . enterprise customer . name , self . enterprise configuration . sapsf base url , sap inactive learners [ 'error' ] . get ( 'message' ) , sap inactive learners [ 'error' ] . get ( 'code' ) , ) return None new page start at = page size + start at all inactive learners += sap inactive learners [ 'value' ] if sap inactive learners [ '@odata.count' ] > new page start at : return self . call search students recursively ( sap search student url , all inactive learners , page size = page size , start at = new page start at , ) return all inactive learners", "predictions": ["search for student keys and students ."], "references": ["make recursive get calls to traverse the paginated api response for search students ."], "bleu": 0.10218289380194193, "rouge_l": 0.2695139911634757}
{"id": 1745, "code": "def filter queryset ( self , request , queryset , view ) : if not request . user . is staff : filter kwargs = { view . USER ID FILTER : request . user . id } queryset = queryset . filter ( * * filter kwargs ) return queryset", "predictions": ["filter the user s queryset ."], "references": ["filter only for the user s id if non - staff ."], "bleu": 0.16738299010839996, "rouge_l": 0.5240549828178694}
{"id": 1746, "code": "def filter queryset ( self , request , queryset , view ) : if request . user . is staff : email = request . query params . get ( 'email' , None ) username = request . query params . get ( 'username' , None ) query parameters = { } if email : query parameters . update ( email = email ) if username : query parameters . update ( username = username ) if query parameters : users = User . objects . filter ( * * query parameters ) . values list ( 'id' , flat = True ) queryset = queryset . filter ( user id in = users ) else : queryset = queryset . filter ( user id = request . user . id ) return queryset", "predictions": ["filter a queryset ."], "references": ["apply incoming filters only if user is staff . if not only filter by user s id ."], "bleu": 0.012007547560562644, "rouge_l": 0.16310160427807485}
{"id": 1747, "code": "def handle transmission error ( self , learner data , request exception ) : try : sys msg = request exception . response . content except Attribute Error : sys msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\\n Error message: %s' '\\n System message: %s' ) , learner data . enterprise course enrollment id , learner data , str ( request exception ) , sys msg )", "predictions": ["handle an transmission error message"], "references": ["handle the case where the transmission fails ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1748, "code": "def validate image extension ( value ) : config = get app config ( ) ext = os . path . splitext ( value . name ) [ 1 ] if config and not ext . lower ( ) in config . valid image extensions : raise Validation Error ( ( \"Unsupported file extension.\" ) )", "predictions": ["validate that the image extension extension is valid ."], "references": ["validate that a particular image extension ."], "bleu": 0.24446151121745052, "rouge_l": 0.639412997903564}
{"id": 1749, "code": "def validate image size ( image ) : config = get app config ( ) valid max image size in bytes = config . valid max image size * 1024 if config and not image . size <= valid max image size in bytes : raise Validation Error ( ( \"The logo image file size must be less than or equal to %s KB.\" ) % config . valid max image size )", "predictions": ["validate that the image size is valid ."], "references": ["validate that a particular image size ."], "bleu": 0.2777619034011791, "rouge_l": 0.6747787610619468}
{"id": 1750, "code": "def get enterprise customer from catalog id ( catalog id ) : try : return str ( Enterprise Customer Catalog . objects . get ( pk = catalog id ) . enterprise customer . uuid ) except Enterprise Customer Catalog . Does Not Exist : return None", "predictions": ["return the customer from the catalog id"], "references": ["get the enterprise customer id given an enterprise customer catalog id ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 1751, "code": "def get requirements ( requirements file ) : lines = open ( requirements file ) . readlines ( ) dependencies = [ ] dependency links = [ ] for line in lines : package = line . strip ( ) if package . startswith ( '#' ) : continue if any ( package . startswith ( prefix ) for prefix in VCS PREFIXES ) : package link , , package = package . rpartition ( '#' ) package link = re . sub ( r'(.*)(?P<dependency link>https?.*$)' , r'\\g<dependency link>' , package link ) package = re . sub ( r'(egg=)?(?P<package name>.*)==.*$' , r'\\g<package name>' , package ) package version = re . sub ( r'.*[^=]==' , '' , line . strip ( ) ) if package : dependency links . append ( '{package link}#egg={package}-{package version}' . format ( package link = package link , package = package , package version = package version , ) ) else : package , , = package . partition ( '#' ) package = package . strip ( ) if package : dependencies . append ( package ) return dependencies , dependency links", "predictions": ["read requirements from a pip - dependency requirements file ."], "references": ["get the contents of a file listing the requirements"], "bleu": 0.14991106946711685, "rouge_l": 0.21254355400696867}
{"id": 1752, "code": "def transmit learner data ( self , user ) : exporter = self . get learner data exporter ( user ) transmitter = self . get learner data transmitter ( ) transmitter . transmit ( exporter )", "predictions": ["transmit the learner of a user"], "references": ["iterate over each learner data record and transmit it to the integrated channel ."], "bleu": 0.06924459302580939, "rouge_l": 0.18654434250764526}
{"id": 1753, "code": "def transmit content metadata ( self , user ) : exporter = self . get content metadata exporter ( user ) transmitter = self . get content metadata transmitter ( ) transmitter . transmit ( exporter . export ( ) )", "predictions": ["transmit the content of the user s content ."], "references": ["transmit content metadata to integrated channel ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 1754, "code": "def get ( self , request , template id , view type ) : template = get object or 404 ( Enrollment Notification Email Template , pk = template id ) if view type not in self . view type contexts : return Http Response ( status = 404 ) base context = self . view type contexts [ view type ] . copy ( ) base context . update ( { 'user name' : self . get user name ( request ) } ) return Http Response ( template . render html template ( base context ) , content type = 'text/html' )", "predictions": ["the view of a template ."], "references": ["render the given template with the stock data ."], "bleu": 0.1593301391270729, "rouge_l": 0.3860759493670886}
{"id": 1755, "code": "def build admin context ( request , customer ) : opts = customer . meta codename = get permission codename ( 'change' , opts ) has change permission = request . user . has perm ( '%s.%s' % ( opts . app label , codename ) ) return { 'has change permission' : has change permission , 'opts' : opts }", "predictions": ["build the customer context ."], "references": ["build common admin context ."], "bleu": 0.3860973950960897, "rouge_l": 0.6}
{"id": 1756, "code": "def build context ( self , request , enterprise customer uuid ) : enterprise customer = Enterprise Customer . objects . get ( uuid = enterprise customer uuid ) context = { self . Context Parameters . ENTERPRISE CUSTOMER : enterprise customer , } context . update ( admin . site . each context ( request ) ) context . update ( self . build admin context ( request , enterprise customer ) ) return context", "predictions": ["builds the context used to build the context ."], "references": ["build common context parts used by different handlers in this view ."], "bleu": 0.12716571564598603, "rouge_l": 0.2785388127853881}
{"id": 1757, "code": "def build context ( self , request , customer uuid ) : enterprise customer = Enterprise Customer . objects . get ( uuid = customer uuid ) search keyword = self . get search keyword ( request ) linked learners = self . get enterprise customer user queryset ( request , search keyword , customer uuid ) pending linked learners = self . get pending users queryset ( search keyword , customer uuid ) context = { self . Context Parameters . ENTERPRISE CUSTOMER : enterprise customer , self . Context Parameters . PENDING LEARNERS : pending linked learners , self . Context Parameters . LEARNERS : linked learners , self . Context Parameters . SEARCH KEYWORD : search keyword or '' , self . Context Parameters . ENROLLMENT URL : settings . LMS ENROLLMENT API PATH , } context . update ( admin . site . each context ( request ) ) context . update ( self . build admin context ( request , enterprise customer ) ) return context", "predictions": ["builds the context used to instantiate the context ."], "references": ["build common context parts used by different handlers in this view ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 1758, "code": "def from children ( cls , program uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise Invalid Proxy Consent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and Enterprise Customer.' ) username = children [ 0 ] . username enterprise customer = children [ 0 ] . enterprise customer return cls ( enterprise customer = enterprise customer , username = username , program uuid = program uuid , exists = exists , granted = granted , child consents = children )", "predictions": ["create a bulk from a program uuid ."], "references": ["build a proxydatasharingconsent using the details of the received consent records ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 1759, "code": "def create roles ( apps , schema editor ) : Enterprise Feature Role = apps . get model ( 'enterprise' , 'Enterprise Feature Role' ) Enterprise Feature Role . objects . update or create ( name = ENTERPRISE CATALOG ADMIN ROLE ) Enterprise Feature Role . objects . update or create ( name = ENTERPRISE DASHBOARD ADMIN ROLE ) Enterprise Feature Role . objects . update or create ( name = ENTERPRISE ENROLLMENT API ADMIN ROLE )", "predictions": ["create the roles for the given service editor ."], "references": ["create the enterprise roles if they do not already exist ."], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 1760, "code": "def delete roles ( apps , schema editor ) : Enterprise Feature Role = apps . get model ( 'enterprise' , 'Enterprise Feature Role' ) Enterprise Feature Role . objects . filter ( name in = [ ENTERPRISE CATALOG ADMIN ROLE , ENTERPRISE DASHBOARD ADMIN ROLE , ENTERPRISE ENROLLMENT API ADMIN ROLE ] ) . delete ( )", "predictions": ["redirect roles to enterprise"], "references": ["delete the enterprise roles ."], "bleu": 0.3096787331587729, "rouge_l": 0.21785714285714283}
{"id": 1761, "code": "def track enrollment ( pathway , user id , course run id , url path = None ) : track event ( user id , 'edx.bi.user.enterprise.onboarding' , { 'pathway' : pathway , 'url path' : url path , 'course run id' : course run id , } )", "predictions": ["post enrollment to the kwargs for a * * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["emit a track event for enterprise course enrollment ."], "bleu": 0.046398855339878003, "rouge_l": 0.055505004549590536}
{"id": 1762, "code": "def is course run upgradeable ( course run ) : now = datetime . datetime . now ( pytz . UTC ) for seat in course run . get ( 'seats' , [ ] ) : if seat . get ( 'type' ) == 'verified' : upgrade deadline = parse datetime handle invalid ( seat . get ( 'upgrade deadline' ) ) return not upgrade deadline or upgrade deadline > now return False", "predictions": ["returns true if the content filter filter filter filter filter filter filter filter filter filter filter filter is upgradeable . . ."], "references": ["return true if the course run has a verified seat with an unexpired upgrade deadline false otherwise ."], "bleu": 0.1090009697802911, "rouge_l": 0.2036727879799666}
{"id": 1763, "code": "def get closest course run ( course runs ) : if len ( course runs ) == 1 : return course runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) never = now - datetime . timedelta ( days = 3650 ) return min ( course runs , key = lambda x : abs ( get course run start ( x , never ) - now ) )", "predictions": ["assign the enterprise role to the learner"], "references": ["return course run with start date closest to now ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 1764, "code": "def parse course key ( course identifier ) : try : course run key = Course Key . from string ( course identifier ) except Invalid Key Error : return course identifier return quote plus ( ' ' . join ( [ course run key . org , course run key . course ] ) )", "predictions": ["delete a enterprise learner from a enterprise assignment ."], "references": ["return the serialized course key given either a course run id or course key ."], "bleu": 0.08019421212222273, "rouge_l": 0.15947712418300655}
{"id": 1765, "code": "def create roles ( apps , schema editor ) : System Wide Enterprise Role = apps . get model ( 'enterprise' , 'System Wide Enterprise Role' ) System Wide Enterprise Role . objects . update or create ( name = ENTERPRISE OPERATOR ROLE )", "predictions": ["create the roles for the given schema ."], "references": ["create the enterprise roles if they do not already exist ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 1766, "code": "def delete roles ( apps , schema editor ) : System Wide Enterprise Role = apps . get model ( 'enterprise' , 'System Wide Enterprise Role' ) System Wide Enterprise Role . objects . filter ( name in = [ ENTERPRISE OPERATOR ROLE ] ) . delete ( )", "predictions": ["delete all roles for a given schema ."], "references": ["delete the enterprise roles ."], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 1767, "code": "def lrs ( self ) : return Remote LRS ( version = self . lrs configuration . version , endpoint = self . lrs configuration . endpoint , auth = self . lrs configuration . authorization header , )", "predictions": ["returns a dictionary with the get configuration configuration . . . . . . . . . ."], "references": ["lrs client instance to be used for sending statements ."], "bleu": 0.06809398432036522, "rouge_l": 0.07530864197530865}
{"id": 1768, "code": "def ecommerce coupon url ( self , instance ) : if not instance . entitlement id : return \"N/A\" return format html ( '<a href=\"{base url}/coupons/{id}\" target=\" blank\">View coupon \"{id}\" details</a>' , base url = settings . ECOMMERCE PUBLIC URL ROOT , id = instance . entitlement id )", "predictions": ["info for get the entitlement enterprise"], "references": ["instance is enterprisecustomer . return e - commerce coupon urls ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 1769, "code": "def drop Historical Table ( apps , schema editor ) : table name = 'sap success factors historicalsapsuccessfactorsenterprisecus80ad' if table name in connection . introspection . table names ( ) : migrations . Delete Model ( name = table name , )", "predictions": ["get to get to to to to to to be a table . . ."], "references": ["drops the historical sap_success_factors table named herein ."], "bleu": 0.09103526405546068, "rouge_l": 0.18401206636500753}
{"id": 1770, "code": "def get clear catalog id action ( description = None ) : description = description or ( \"Unlink selected objects from existing course catalogs\" ) def clear catalog id ( modeladmin , request , queryset ) : queryset . update ( catalog = None ) clear catalog id . short description = description return clear catalog id", "predictions": ["enterprise the enrollment api api api api api api"], "references": ["return the action method to clear the catalog id for a enterprisecustomer ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 1771, "code": "def calculate distance ( latlon1 , latlon2 ) : lat1 , lon1 = latlon1 lat2 , lon2 = latlon2 dlon = lon2 - lon1 dlat = lat2 - lat1 R = 6371 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * ( np . sin ( dlon / 2 ) ) ** 2 c = 2 * np . pi * R * np . arctan2 ( np . sqrt ( a ) , np . sqrt ( 1 - a ) ) / 180 return c", "predictions": ["get enterprise enterprise from admin coordinates"], "references": ["calculates the distance between two points on earth ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 1772, "code": "def matrix2dict ( matrix , etype = False ) : n = len ( matrix ) adj = { k : { } for k in range ( n ) } for k in range ( n ) : for j in range ( n ) : if matrix [ k , j ] != 0 : adj [ k ] [ j ] = { } if not etype else matrix [ k , j ] return adj", "predictions": ["make a dictionary with the role s role"], "references": ["takes an adjacency matrix and returns an adjacency list ."], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 1773, "code": "def get queues ( g , queues , edge , edge type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge index [ e ] for e in edge ] else : queues = [ g . edge index [ edge ] ] elif edge type is not None : if isinstance ( edge type , collections . Iterable ) : edge type = set ( edge type ) else : edge type = set ( [ edge type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge type' ) in edge type : tmp . append ( g . edge index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number of edges ( ) ) return queues", "predictions": ["provide a list of queues to a single graph ."], "references": ["used to specify edge indices from different types of arguments ."], "bleu": 0.13564514503163538, "rouge_l": 0.18885448916408668}
{"id": 1774, "code": "def copy ( self ) : net = Queue Network ( None ) net . g = self . g . copy ( ) net . max agents = copy . deepcopy ( self . max agents ) net . n V = copy . deepcopy ( self . n V ) net . n E = copy . deepcopy ( self . n E ) net . num agents = copy . deepcopy ( self . num agents ) net . num events = copy . deepcopy ( self . num events ) net . t = copy . deepcopy ( self . t ) net . initialized = copy . deepcopy ( self . initialized ) net . prev edge = copy . deepcopy ( self . prev edge ) net . blocking = copy . deepcopy ( self . blocking ) net . colors = copy . deepcopy ( self . colors ) net . out edges = copy . deepcopy ( self . out edges ) net . in edges = copy . deepcopy ( self . in edges ) net . edge2queue = copy . deepcopy ( self . edge2queue ) net . route probs = copy . deepcopy ( self . route probs ) if net . initialized : keys = [ q . key ( ) for q in net . edge2queue if q . time < np . infty ] net . fancy heap = Priority Queue ( keys , net . n E ) return net", "predictions": ["makes a get get a get of the instance"], "references": ["returns a deep copy of itself ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 1775, "code": "def reset colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set ep ( e , 'edge color' , self . edge2queue [ k ] . colors [ 'edge color' ] ) for v in self . g . nodes ( ) : self . g . set vp ( v , 'vertex fill color' , self . colors [ 'vertex fill color' ] )", "predictions": ["create all session session session names ."], "references": ["resets all edge and vertex colors to their default values ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 1776, "code": "def strip comment marker ( text ) : lines = [ ] for line in text . splitlines ( ) : lines . append ( line . lstrip ( '#' ) ) text = textwrap . dedent ( '\\n' . join ( lines ) ) return text", "predictions": ["call the search students on a piece of text size size size size size size size size size size size size size size size size size size size size size size"], "references": ["strip # markers at the front of a block of comment text ."], "bleu": 0.04906081629292276, "rouge_l": 0.19629927594529364}
{"id": 1777, "code": "def get class traits ( klass ) : source = inspect . getsource ( klass ) cb = Comment Blocker ( ) cb . process file ( String IO ( source ) ) mod ast = compiler . parse ( source ) class ast = mod ast . node . nodes [ 0 ] for node in class ast . code . nodes : if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip comment marker ( cb . search for comment ( node . lineno , default = '' ) ) yield name , rhs , doc", "predictions": ["return a list of the names of all available nodes"], "references": ["yield all of the documentation for trait definitions on a class object ."], "bleu": 0.13964659797714432, "rouge_l": 0.16991643454039}
{"id": 1778, "code": "def add ( self , string , start , end , line ) : if string . strip ( ) : self . start lineno = min ( self . start lineno , start [ 0 ] ) self . end lineno = max ( self . end lineno , end [ 0 ] )", "predictions": ["filter a string at request request request request request request request user user user user user user user user request request as a view"], "references": ["add lines to the block ."], "bleu": 0.042601467364417965, "rouge_l": 0.0}
{"id": 1779, "code": "def process file ( self , file ) : if sys . version info [ 0 ] >= 3 : nxt = file . next else : nxt = file . next for token in tokenize . generate tokens ( nxt ) : self . process token ( * token ) self . make index ( )", "predictions": ["handle a single transmission token"], "references": ["process a file object ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1780, "code": "def process token ( self , kind , string , start , end , line ) : if self . current block . is comment : if kind == tokenize . COMMENT : self . current block . add ( string , start , end , line ) else : self . new noncomment ( start [ 0 ] , end [ 0 ] ) else : if kind == tokenize . COMMENT : self . new comment ( string , start , end , line ) else : self . current block . add ( string , start , end , line )", "predictions": ["validate a image image"], "references": ["process a single token ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1781, "code": "def new noncomment ( self , start lineno , end lineno ) : block = Non Comment ( start lineno , end lineno ) self . blocks . append ( block ) self . current block = block", "predictions": ["create a validate and possibly possibly possibly a new one ."], "references": ["we are transitioning from a noncomment to a comment ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 1782, "code": "def load ( self , config ) : if isinstance ( config , six . string types ) : try : config = json . loads ( config ) except Value Error : pass if not isinstance ( config , dict ) : raise Type Error ( 'config block must be an istance ' 'of dict or a valid Net JSON string' ) return config", "predictions": ["get the catalog from a . json file"], "references": ["loads config from string or dict"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1783, "code": "def merge config ( self , config , templates ) : if not templates : return config if not isinstance ( templates , list ) : raise Type Error ( 'templates argument must be an instance of list' ) result = { } config list = templates + [ config ] for merging in config list : result = merge config ( result , self . load ( merging ) , self . list identifiers ) return result", "predictions": ["get a dictionary of re - requirements from a list of re - raising all re - requirements ."], "references": ["merges config with templates"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 1784, "code": "def get install context ( self ) : config = self . config l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) cron = False for file in config . get ( 'files' , [ ] ) : path = file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break return dict ( hostname = config [ 'general' ] [ 'hostname' ] , l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , cron = cron )", "predictions": ["return data to get learner data"], "references": ["returns the template context for install . sh and uninstall . sh"], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 1785, "code": "def add install ( self , context ) : contents = self . render template ( 'install.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . add unique file ( { \"path\" : \"/install.sh\" , \"contents\" : contents , \"mode\" : \"755\" } )", "predictions": ["transmit content to add a get to the project = setup = 0"], "references": ["generates install . sh and adds it to included files"], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 1786, "code": "def add uninstall ( self , context ) : contents = self . render template ( 'uninstall.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . add unique file ( { \"path\" : \"/uninstall.sh\" , \"contents\" : contents , \"mode\" : \"755\" } )", "predictions": ["get the object in the object ."], "references": ["generates uninstall . sh and adds it to included files"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 1787, "code": "def add tc script ( self ) : context = dict ( tc options = self . config . get ( 'tc options' , [ ] ) ) contents = self . render template ( 'tc script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . add unique file ( { \"path\" : \"/tc script.sh\" , \"contents\" : contents , \"mode\" : \"755\" } )", "predictions": ["build a admin context"], "references": ["generates tc_script . sh and adds it to included files"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1788, "code": "def render ( self ) : template name = '{0}.jinja2' . format ( self . get name ( ) ) template = self . template env . get template ( template name ) context = getattr ( self . backend , 'intermediate data' , { } ) output = template . render ( data = context ) return self . cleanup ( output )", "predictions": ["build the request for the given request . ."], "references": ["renders configuration by using the jinja2 templating engine"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 1789, "code": "def intermediate address ( self , address ) : for key in self . address keys : if key in address : del address [ key ] return address", "predictions": ["remove the context with the given context"], "references": ["deletes netjson address keys"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1790, "code": "def intermediate proto ( self , interface , address ) : address proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address proto else : return interface . pop ( 'proto' )", "predictions": ["children of an program by uuid"], "references": ["determines uci interface proto option"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1791, "code": "def intermediate dns servers ( self , uci , address ) : if 'dns' in uci : return uci [ 'dns' ] if address [ 'proto' ] in [ 'dhcp' , 'dhcpv6' , 'none' ] : return None dns = self . netjson . get ( 'dns servers' , None ) if dns : return ' ' . join ( dns )", "predictions": ["get roles for given address"], "references": ["determines uci interface dns option"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1792, "code": "def intermediate dns search ( self , uci , address ) : if 'dns search' in uci : return uci [ 'dns search' ] if address [ 'proto' ] == 'none' : return None dns search = self . netjson . get ( 'dns search' , None ) if dns search : return ' ' . join ( dns search )", "predictions": ["get search for intermediate dns search"], "references": ["determines uci interface dns_search option"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1793, "code": "def intermediate htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel width = radio . pop ( 'channel width' ) if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel width ) return 'NONE'", "predictions": ["get intermediate htmode for radio radio ."], "references": ["only for mac80211 driver"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1794, "code": "def netjson protocol ( self , radio ) : htmode = radio . get ( 'htmode' ) hwmode = radio . get ( 'hwmode' , None ) if htmode . startswith ( 'HT' ) : return '802.11n' elif htmode . startswith ( 'VHT' ) : return '802.11ac' return '802.{0}' . format ( hwmode )", "predictions": ["generate netjson protocol protocol protocol protocol ."], "references": ["determines netjson protocol radio attribute"], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 1795, "code": "def netjson channel width ( self , radio ) : htmode = radio . pop ( 'htmode' ) if htmode == 'NONE' : return 20 channel width = htmode . replace ( 'VHT' , '' ) . replace ( 'HT' , '' ) if '+' in channel width or '-' in channel width : radio [ 'htmode' ] = htmode channel width = channel width [ 0 : - 1 ] return int ( channel width )", "predictions": ["get channel width ."], "references": ["determines netjson channel_width radio attribute"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 1796, "code": "def get install requires ( ) : requirements = [ ] for line in open ( 'requirements.txt' ) . readlines ( ) : if line . startswith ( '#' ) or line == '' or line . startswith ( 'http' ) or line . startswith ( 'git' ) : continue requirements . append ( line . replace ( '\\n' , '' ) ) if sys . version info . major < 3 : requirements . append ( 'py2-ipaddress' ) return requirements", "predictions": ["parse the install requirements file ."], "references": ["parse requirements . txt ignore links exclude comments"], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 1797, "code": "def fact ( self , name ) : facts = self . facts ( name = name ) return next ( fact for fact in facts )", "predictions": ["get the fact value for a given name ."], "references": ["get a single fact from this node ."], "bleu": 0.17747405280050263, "rouge_l": 0.35672514619883033}
{"id": 1798, "code": "def main ( ) : app = My Master ( log handler = My Logger ( ) , listener = App Channel Listener ( ) , soe handler = SOE Handler ( ) , master application = Master Application ( ) ) log . debug ( 'Initialization complete. In command loop.' ) app . shutdown ( ) log . debug ( 'Exiting.' ) exit ( )", "predictions": ["start the server ."], "references": ["the master has been started from the command line . execute ad - hoc tests if desired ."], "bleu": 0.012007547560562644, "rouge_l": 0.16310160427807485}
{"id": 1799, "code": "def main ( ) : app = Outstation Application ( ) log . debug ( 'Initialization complete. In command loop.' ) app . shutdown ( ) log . debug ( 'Exiting.' ) exit ( )", "predictions": ["shutdown the application ."], "references": ["the outstation has been started from the command line . execute ad - hoc tests if desired ."], "bleu": 0.012007547560562644, "rouge_l": 0.16310160427807485}
{"id": 1800, "code": "def configure stack ( ) : stack config = asiodnp3 . Outstation Stack Config ( opendnp3 . Database Sizes . All Types ( 10 ) ) stack config . outstation . event Buffer Config = opendnp3 . Event Buffer Config ( ) . All Types ( 10 ) stack config . outstation . params . allow Unsolicited = True stack config . link . Local Addr = 10 stack config . link . Remote Addr = 1 stack config . link . Keep Alive Timeout = openpal . Time Duration ( ) . Max ( ) return stack config", "predictions": ["configure a new stack stack instance"], "references": ["set up the opendnp3 configuration ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1801, "code": "def Get Application IIN ( self ) : application iin = opendnp3 . Application IIN ( ) application iin . config Corrupt = False application iin . device Trouble = False application iin . local Control = False application iin . need Time = False iin field = application iin . To IIN ( ) log . debug ( 'Outstation Application.Get Application IIN: IIN Field LSB={}, MSB={}' . format ( iin field . LSB , iin field . MSB ) ) return application iin", "predictions": ["update the application application ."], "references": ["return the application - controlled iin field ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 1802, "code": "def delete connection ( ) : if CON SYM in globals ( ) : con = globals ( ) . pop ( CON SYM ) if not getattr ( con , ' session' ) . start ( ) : con . stop ( )", "predictions": ["delete the connection from the database"], "references": ["stop and destroy bloomberg connection"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1803, "code": "def parse markdown ( ) : readme file = f'{PACKAGE ROOT}/README.md' if path . exists ( readme file ) : with open ( readme file , 'r' , encoding = 'utf-8' ) as f : long description = f . read ( ) return long description", "predictions": ["parse the markdown file ."], "references": ["parse markdown as description"], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1804, "code": "def parse description ( markdown = True ) : if markdown : return parse markdown ( ) try : from pypandoc import convert readme file = f'{PACKAGE ROOT}/docs/index.rst' if not path . exists ( readme file ) : raise Import Error return convert ( readme file , 'rst' ) except Import Error : return parse markdown ( )", "predictions": ["parse markdown description from markdown file ."], "references": ["parse the description in the readme file"], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 1805, "code": "def to gen ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm", "predictions": ["a generator for generator of values from an iterable of a given iterable ."], "references": ["recursively iterate lists and tuples"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 1806, "code": "def missing info ( * * kwargs ) -> str : func = kwargs . pop ( 'func' , 'unknown' ) if 'ticker' in kwargs : kwargs [ 'ticker' ] = kwargs [ 'ticker' ] . replace ( '/' , ' ' ) info = utils . to str ( kwargs , fmt = '{value}' , sep = '/' ) [ 1 : - 1 ] return f'{func}/{info}'", "predictions": ["returns a string with missing information"], "references": ["full infomation for missing query"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1807, "code": "def update missing ( * * kwargs ) : data path = os . environ . get ( BBG ROOT , '' ) . replace ( '\\\\' , '/' ) if not data path : return if len ( kwargs ) == 0 : return log path = f'{data path}/Logs/{missing info(**kwargs)}' cnt = len ( files . all files ( log path ) ) + 1 files . create folder ( log path ) open ( f'{log path}/{cnt}.log' , 'a' ) . close ( )", "predictions": ["update missing data from the current folder ."], "references": ["update number of trials for missing values"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1808, "code": "def start ( self ) : logger = get logger ( self . debug ) started = self . session . start ( ) if started : ev = self . session . next Event ( ) ev name = EVENT DICT [ ev . event Type ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev name ) ) for msg in ev : logger . info ( 'Message Received:\\n{}' . format ( msg ) ) if ev . event Type ( ) != blpapi . Event . SESSION STATUS : raise Runtime Error ( 'Expected a \"SESSION STATUS\" event but ' 'received a {!r}' . format ( ev name ) ) ev = self . session . next Event ( ) ev name = EVENT DICT [ ev . event Type ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev name ) ) for msg in ev : logger . info ( 'Message Received:\\n{}' . format ( msg ) ) if ev . event Type ( ) != blpapi . Event . SESSION STATUS : raise Runtime Error ( 'Expected a \"SESSION STATUS\" event but ' 'received a {!r}' . format ( ev name ) ) else : ev = self . session . next Event ( self . timeout ) if ev . event Type ( ) == blpapi . Event . SESSION STATUS : for msg in ev : logger . warning ( 'Message Received:\\n{}' . format ( msg ) ) raise Connection Error ( 'Could not start blpapi.Session' ) self . init services ( ) return self", "predictions": ["start the \"session event loop ."], "references": ["start connection and initialize session services"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1809, "code": "def init services ( self ) : logger = get logger ( self . debug ) opened = self . session . open Service ( '//blp/refdata' ) ev = self . session . next Event ( ) ev name = EVENT DICT [ ev . event Type ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev name ) ) for msg in ev : logger . info ( 'Message Received:\\n{}' . format ( msg ) ) if ev . event Type ( ) != blpapi . Event . SERVICE STATUS : raise Runtime Error ( 'Expected a \"SERVICE STATUS\" event but ' 'received a {!r}' . format ( ev name ) ) if not opened : logger . warning ( 'Failed to open //blp/refdata' ) raise Connection Error ( 'Could not open a //blp/refdata service' ) self . ref Data Service = self . session . get Service ( '//blp/refdata' ) opened = self . session . open Service ( '//blp/exrsvc' ) ev = self . session . next Event ( ) ev name = EVENT DICT [ ev . event Type ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev name ) ) for msg in ev : logger . info ( 'Message Received:\\n{}' . format ( msg ) ) if ev . event Type ( ) != blpapi . Event . SERVICE STATUS : raise Runtime Error ( 'Expected a \"SERVICE STATUS\" event but ' 'received a {!r}' . format ( ev name ) ) if not opened : logger . warning ( 'Failed to open //blp/exrsvc' ) raise Connection Error ( 'Could not open a //blp/exrsvc service' ) self . exr Service = self . session . get Service ( '//blp/exrsvc' ) return self", "predictions": ["init the type: event ."], "references": ["initialize blpapi . session services"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1810, "code": "def adjust delay ( self , slot , response ) : if response . status in self . retry http codes : new delay = max ( slot . delay , 1 ) * 4 new delay = max ( new delay , self . mindelay ) new delay = min ( new delay , self . maxdelay ) slot . delay = new delay self . stats . inc value ( 'delay count' ) elif response . status == 200 : new delay = max ( slot . delay / 2 , self . mindelay ) if new delay < 0.01 : new delay = 0 slot . delay = new delay", "predictions": ["adjust the delay delay ."], "references": ["define delay adjustment policy"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1811, "code": "def memberness ( context ) : if context : texts = context . xpath ( './/*[local-name()=\"explicit Member\"]/text()' ) . extract ( ) text = str ( texts ) . lower ( ) if len ( texts ) > 1 : return 2 elif 'country' in text : return 2 elif 'member' not in text : return 0 elif 'successor' in text : return 1 elif 'parent' in text : return 2 return 3", "predictions": ["get the 3 version of the context"], "references": ["the likelihood that the context is a member ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 1812, "code": "def parse 10qk ( self , response ) : loader = Report Item Loader ( response = response ) item = loader . load item ( ) if 'doc type' in item : doc type = item [ 'doc type' ] if doc type in ( '10-Q' , '10-K' ) : return item return None", "predictions": ["parse a 10qk response ."], "references": ["parse 10 - q or 10 - k xml report ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 1813, "code": "def find lcs ( self , node , string Idxs ) : nodes = [ self . find lcs ( n , string Idxs ) for ( n , ) in node . transition links if n . generalized idxs . issuperset ( string Idxs ) ] if nodes == [ ] : return node deepest Node = max ( nodes , key = lambda n : n . depth ) return deepest Node", "predictions": ["find the lcs lcs lcs lcs ."], "references": ["helper method that finds lcs by traversing the labeled gsd ."], "bleu": 0.1247439242120089, "rouge_l": 0.2136602451838879}
{"id": 1814, "code": "def generalized word starts ( self , xs ) : self . word starts = [ ] i = 0 for n in range ( len ( xs ) ) : self . word starts . append ( i ) i += len ( xs [ n ] ) + 1", "predictions": ["split the word starts with the word ."], "references": ["helper method returns the starting indexes of strings in gst"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 1815, "code": "def edge Label ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]", "predictions": ["return the edge of the edge with the given parent ."], "references": ["helper method returns the edge label between a node and it s parent"], "bleu": 0.13430919728978852, "rouge_l": 0.24629878869448185}
{"id": 1816, "code": "def query ( self , i , j ) : if self . queries cnt < self . max queries cnt : self . queries cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise Maximum Queries Exceeded", "predictions": ["returns the first queries of a query"], "references": ["query the oracle to find out whether i and j should be must - linked"], "bleu": 0.06555660318294844, "rouge_l": 0.08531468531468532}
{"id": 1817, "code": "def preprocess constraints ( ml , cl , n ) : ml graph , cl graph = { } , { } for i in range ( n ) : ml graph [ i ] = set ( ) cl graph [ i ] = set ( ) def add both ( d , i , j ) : d [ i ] . add ( j ) d [ j ] . add ( i ) for ( i , j ) in ml : ml graph [ i ] . add ( j ) ml graph [ j ] . add ( i ) for ( i , j ) in cl : cl graph [ i ] . add ( j ) cl graph [ j ] . add ( i ) def dfs ( i , graph , visited , component ) : visited [ i ] = True for j in graph [ i ] : if not visited [ j ] : dfs ( j , graph , visited , component ) component . append ( i ) visited = [ False ] * n neighborhoods = [ ] for i in range ( n ) : if not visited [ i ] and ml graph [ i ] : component = [ ] dfs ( i , ml graph , visited , component ) for x1 in component : for x2 in component : if x1 != x2 : ml graph [ x1 ] . add ( x2 ) neighborhoods . append ( component ) for ( i , j ) in cl : for x in ml graph [ i ] : add both ( cl graph , x , j ) for y in ml graph [ j ] : add both ( cl graph , i , y ) for x in ml graph [ i ] : for y in ml graph [ j ] : add both ( cl graph , x , y ) for i in ml graph : for j in ml graph [ i ] : if j != i and j in cl graph [ i ] : raise Inconsistent Constraints Exception ( 'Inconsistent constraints between {} and {}' . format ( i , j ) ) return ml graph , cl graph , neighborhoods", "predictions": ["preprocess constraints between labels and edges ."], "references": ["create a graph of constraints for both must - and cannot - links"], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 1818, "code": "def construct formset ( self ) : formset class = self . get formset ( ) if hasattr ( self , 'get extra form kwargs' ) : klass = type ( self ) . name raise Deprecation Warning ( 'Calling {0}.get extra form kwargs is no longer supported. ' 'Set `form kwargs` in {0}.formset kwargs or override ' '{0}.get formset kwargs() directly.' . format ( klass ) , ) return formset class ( * * self . get formset kwargs ( ) )", "predictions": ["return an instance of the formset to be used in the formset ."], "references": ["returns an instance of the formset"], "bleu": 0.3498761149110956, "rouge_l": 0.5637707948243993}
{"id": 1819, "code": "def get formset kwargs ( self ) : kwargs = self . formset kwargs . copy ( ) kwargs . update ( { 'initial' : self . get initial ( ) , 'prefix' : self . get prefix ( ) , } ) if self . request . method in ( 'POST' , 'PUT' ) : kwargs . update ( { 'data' : self . request . POST . copy ( ) , 'files' : self . request . FILES , } ) return kwargs", "predictions": ["returns the keyword arguments for instantiating the formset formset ."], "references": ["returns the keyword arguments for instantiating the formset ."], "bleu": 0.831173435395861, "rouge_l": 0.9564459930313588}
{"id": 1820, "code": "def get factory kwargs ( self ) : for attr in [ 'extra' , 'max num' , 'can order' , 'can delete' , 'ct field' , 'formfield callback' , 'fk name' , 'widgets' , 'ct fk field' ] : if hasattr ( self , attr ) : klass = type ( self ) . name raise Deprecation Warning ( 'Setting `{0}.{1}` at the class level is now deprecated. ' 'Set `{0}.factory kwargs` instead.' . format ( klass , attr ) ) kwargs = self . factory kwargs . copy ( ) if self . get formset class ( ) : kwargs [ 'formset' ] = self . get formset class ( ) return kwargs", "predictions": ["returns the meta - factory formset to the formset ."], "references": ["returns the keyword arguments for calling the formset factory"], "bleu": 0.21834177214239062, "rouge_l": 0.42508710801393734}
{"id": 1821, "code": "def get success url ( self ) : if self . success url : url = self . success url else : url = self . request . get full path ( ) return url", "predictions": ["returns the success url"], "references": ["returns the supplied url ."], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 1822, "code": "def get formset kwargs ( self ) : kwargs = super ( Model Form Set Mixin , self ) . get formset kwargs ( ) kwargs [ 'queryset' ] = self . get queryset ( ) return kwargs", "predictions": ["returns the keyword arguments for instantiating the formset ."], "references": ["returns the keyword arguments for instantiating the formset ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 1823, "code": "def formset valid ( self , formset ) : self . object list = formset . save ( ) return super ( Model Form Set Mixin , self ) . formset valid ( formset )", "predictions": ["update the formset ."], "references": ["if the formset is valid save the associated models ."], "bleu": 0.11337974147240094, "rouge_l": 0.3978260869565217}
{"id": 1824, "code": "def get formset kwargs ( self ) : if hasattr ( self , 'save as new' ) : klass = type ( self ) . name raise Deprecation Warning ( 'Setting `{0}.save as new` at the class level is now ' 'deprecated. Set `{0}.formset kwargs` instead.' . format ( klass ) ) kwargs = super ( Base Inline Form Set Factory , self ) . get formset kwargs ( ) kwargs [ 'instance' ] = self . object return kwargs", "predictions": ["returns the keyword arguments for instantiating the dns dns dns dns . ."], "references": ["returns the keyword arguments for instantiating the formset ."], "bleu": 0.5296074933406222, "rouge_l": 0.7519260400616331}
{"id": 1825, "code": "def get factory kwargs ( self ) : kwargs = super ( Base Inline Form Set Factory , self ) . get factory kwargs ( ) kwargs . setdefault ( 'fields' , self . fields ) kwargs . setdefault ( 'exclude' , self . exclude ) if self . get form class ( ) : kwargs [ 'form' ] = self . get form class ( ) return kwargs", "predictions": ["returns the keyword arguments for instantiating the form . . . . . . . . ."], "references": ["returns the keyword arguments for calling the formset factory"], "bleu": 0.2749977595322415, "rouge_l": 0.4886515353805074}
{"id": 1826, "code": "def get ( self , request , * args , * * kwargs ) : formset = self . construct formset ( ) return self . render to response ( self . get context data ( formset = formset ) )", "predictions": ["the netjson page of the ."], "references": ["handles get requests and instantiates a blank version of the formset ."], "bleu": 0.11492332782473744, "rouge_l": 0.31443298969072164}
{"id": 1827, "code": "def forms valid ( self , form , inlines ) : response = self . form valid ( form ) for formset in inlines : formset . save ( ) return response", "predictions": ["if all netjson have been channel"], "references": ["if the form and formsets are valid save the associated models ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 1828, "code": "def construct inlines ( self ) : inline formsets = [ ] for inline class in self . get inlines ( ) : inline instance = inline class ( self . model , self . request , self . object , self . kwargs , self ) inline formset = inline instance . construct formset ( ) inline formsets . append ( inline formset ) return inline formsets", "predictions": ["create a dictionary of install = requirements . ."], "references": ["returns the inline formset instances"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1829, "code": "def get ( self , request , * args , * * kwargs ) : form class = self . get form class ( ) form = self . get form ( form class ) inlines = self . construct inlines ( ) return self . render to response ( self . get context data ( form = form , inlines = inlines , * * kwargs ) )", "predictions": ["overloaded to do the return return the return return containing the return return the return return containing containing the return return return response in json response in url in inlines in"], "references": ["handles get requests and instantiates a blank version of the form and formsets ."], "bleu": 0.03901663112717908, "rouge_l": 0.04769351055512119}
{"id": 1830, "code": "def get params for field ( self , field name , sort type = None ) : if not sort type : if self . initial sort == field name : sort type = 'desc' if self . initial sort type == 'asc' else 'asc' else : sort type = 'asc' self . initial params [ self . sort param name ] = self . sort fields [ field name ] self . initial params [ self . sort type param name ] = sort type return '?%s' % self . initial params . urlencode ( )", "predictions": ["get params parameters for a field"], "references": ["if sort_type is none - inverse current sort for field if no sorted - use asc"], "bleu": 0.046172815301777345, "rouge_l": 0.1680440771349862}
{"id": 1831, "code": "def get start date ( self , obj ) : obj date = getattr ( obj , self . get date field ( ) ) try : obj date = obj date . date ( ) except Attribute Error : pass return obj date", "predictions": ["main method to main date date for the given object"], "references": ["returns the start date for a model instance"], "bleu": 0.17827531042796255, "rouge_l": 0.22676579925650556}
{"id": 1832, "code": "def get end date ( self , obj ) : obj date = getattr ( obj , self . get end date field ( ) ) try : obj date = obj date . date ( ) except Attribute Error : pass return obj date", "predictions": ["returns the stack date for the given object . . . . . . . . . . . . . . . . . ."], "references": ["returns the end date for a model instance"], "bleu": 0.07725039773573085, "rouge_l": 0.2601279317697228}
{"id": 1833, "code": "def get queryset ( self ) : qs = super ( Base Calendar Month View , self ) . get queryset ( ) year = self . get year ( ) month = self . get month ( ) date field = self . get date field ( ) end date field = self . get end date field ( ) date = date from string ( year , self . get year format ( ) , month , self . get month format ( ) ) since = date until = self . get next month ( date ) if since . weekday ( ) != self . get first of week ( ) : diff = math . fabs ( since . weekday ( ) - self . get first of week ( ) ) since = since - datetime . timedelta ( days = diff ) if until . weekday ( ) != ( ( self . get first of week ( ) + 6 ) % 7 ) : diff = math . fabs ( ( ( self . get first of week ( ) + 6 ) % 7 ) - until . weekday ( ) ) until = until + datetime . timedelta ( days = diff ) if end date field : predicate1 = Q ( * * { '%s gte' % date field : since , end date field : None } ) predicate2 = Q ( * * { '%s gte' % date field : since , '%s lt' % end date field : until } ) predicate3 = Q ( * * { '%s lt' % date field : since , '%s gte' % end date field : since , '%s lt' % end date field : until } ) predicate4 = Q ( * * { '%s gte' % date field : since , '%s lt' % date field : until , '%s gte' % end date field : until } ) predicate5 = Q ( * * { '%s lt' % date field : since , '%s gte' % end date field : until } ) return qs . filter ( predicate1 | predicate2 | predicate3 | predicate4 | predicate5 ) return qs . filter ( * * { '%s gte' % date field : since } )", "predictions": ["returns a queryset of the queryset log log log log log log log log log log log log log log log log log log log log log log log log log"], "references": ["returns a queryset of models for the month requested"], "bleu": 0.11365352023191169, "rouge_l": 0.2775250227479527}
{"id": 1834, "code": "def read version ( ) : finder = Version Finder ( ) path = os . path . join ( PROJECT ROOT , 'colorful' , ' init .py' ) with codecs . open ( path , 'r' , encoding = 'utf-8' ) as fp : file data = fp . read ( ) . encode ( 'utf-8' ) finder . visit ( ast . parse ( file data ) ) return finder . version", "predictions": ["delete the connection from the package = 0 = 1 = 0 = 1 = 1 = 0 = 1 = 0 = 1 = 1 = 1 = 0 ="], "references": ["read version from __init__ . py without loading any files"], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 1835, "code": "def with setup ( self , colormode = None , colorpalette = None , extend colors = False ) : colorful = Colorful ( colormode = self . colorful . colormode , colorpalette = copy . copy ( self . colorful . colorpalette ) ) colorful . setup ( colormode = colormode , colorpalette = colorpalette , extend colors = extend colors ) yield colorful", "predictions": ["sets up the with markdown if necessary . . . . . . . ."], "references": ["return a new colorful object with the given color config ."], "bleu": 0.09782375748961449, "rouge_l": 0.1582360570687419}
{"id": 1836, "code": "def show ( ) : sys . stdout . write ( colorful . bold ( 'bold' ) + ' ' ) sys . stdout . write ( colorful . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( colorful . italic ( 'italic' ) + ' ' ) sys . stdout . write ( colorful . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( colorful . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( colorful . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( colorful . struckthrough ( 'struckthrough' ) + '\\n' ) sys . stdout . write ( colorful . red ( 'red' ) + ' ' ) sys . stdout . write ( colorful . green ( 'green' ) + ' ' ) sys . stdout . write ( colorful . yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( colorful . blue ( 'blue' ) + ' ' ) sys . stdout . write ( colorful . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( colorful . cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( colorful . white ( 'white' ) + '\\n' ) sys . stdout . write ( colorful . on red ( 'red' ) + ' ' ) sys . stdout . write ( colorful . on green ( 'green' ) + ' ' ) sys . stdout . write ( colorful . on yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( colorful . on blue ( 'blue' ) + ' ' ) sys . stdout . write ( colorful . on magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( colorful . on cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( colorful . on white ( 'white' ) + '\\n' )", "predictions": ["parse the if we have enough for some special functionality ."], "references": ["show the modifiers and colors"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 1837, "code": "def show ( ) : with colorful . with style ( 'monokai' ) as c : sys . stdout . write ( c . bold ( 'bold' ) + ' ' ) sys . stdout . write ( c . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( c . italic ( 'italic' ) + ' ' ) sys . stdout . write ( c . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( c . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( c . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( c . struckthrough ( 'struckthrough' ) + '\\n' ) sys . stdout . write ( c . orange ( 'orange' ) + ' ' ) sys . stdout . write ( c . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . purple ( 'purple' ) + ' ' ) sys . stdout . write ( c . blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . sea Green ( 'sea green' ) + ' ' ) sys . stdout . write ( c . green ( 'green' ) + ' ' ) sys . stdout . write ( c . yellow ( 'yellow' ) + '\\n' ) sys . stdout . write ( c . on orange ( 'orange' ) + ' ' ) sys . stdout . write ( c . on magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . on purple ( 'purple' ) + ' ' ) sys . stdout . write ( c . on blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . on sea Green ( 'sea green' ) + ' ' ) sys . stdout . write ( c . gray on green ( 'green' ) + ' ' ) sys . stdout . write ( c . gray on yellow ( 'yellow' ) + '\\n' )", "predictions": ["print the formatted log message from the terminal from the terminal from the terminal from the terminal from the terminal from the terminal from the terminal from the terminal from the"], "references": ["show the modifiers and colors"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 1838, "code": "def rgb to ansi256 ( r , g , b ) : if r == g and g == b : if r < 8 : return 16 if r > 248 : return 231 return round ( ( ( r - 8 ) / 247.0 ) * 24 ) + 232 ansi r = 36 * round ( r / 255.0 * 5.0 ) ansi g = 6 * round ( g / 255.0 * 5.0 ) ansi b = round ( b / 255.0 * 5.0 ) ansi = 16 + ansi r + ansi g + ansi b return ansi", "predictions": ["convert a missing color to a ansi256 color color ."], "references": ["convert rgb to ansi 256 color"], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 1839, "code": "def rgb to ansi16 ( r , g , b , use bright = False ) : ansi b = round ( b / 255.0 ) << 2 ansi g = round ( g / 255.0 ) << 1 ansi r = round ( r / 255.0 ) ansi = ( 90 if use bright else 30 ) + ( ansi b | ansi g | ansi r ) return ansi", "predictions": ["convert a update color to a update color ."], "references": ["convert rgb to ansi 16 color"], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 1840, "code": "def show ( ) : with colorful . with style ( 'solarized' ) as c : sys . stdout . write ( c . bold ( 'bold' ) + ' ' ) sys . stdout . write ( c . dimmed ( 'dimmed' ) + ' ' ) sys . stdout . write ( c . italic ( 'italic' ) + ' ' ) sys . stdout . write ( c . underlined ( 'underlined' ) + ' ' ) sys . stdout . write ( c . inversed ( 'inversed' ) + ' ' ) sys . stdout . write ( c . concealed ( 'concealed' ) + ' ' ) sys . stdout . write ( c . struckthrough ( 'struckthrough' ) + '\\n' ) sys . stdout . write ( c . yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( c . red ( 'orange' ) + ' ' ) sys . stdout . write ( c . red ( 'red' ) + ' ' ) sys . stdout . write ( c . magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . magenta ( 'violet' ) + ' ' ) sys . stdout . write ( c . blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( c . green ( 'green' ) + '\\n' ) sys . stdout . write ( c . on yellow ( 'yellow' ) + ' ' ) sys . stdout . write ( c . on red ( 'orange' ) + ' ' ) sys . stdout . write ( c . on red ( 'red' ) + ' ' ) sys . stdout . write ( c . on magenta ( 'magenta' ) + ' ' ) sys . stdout . write ( c . on magenta ( 'violet' ) + ' ' ) sys . stdout . write ( c . on blue ( 'blue' ) + ' ' ) sys . stdout . write ( c . on cyan ( 'cyan' ) + ' ' ) sys . stdout . write ( c . on green ( 'green' ) + '\\n' )", "predictions": ["print the log message = 1 = 0 = 1 = 0 = 1 = 0 = 0"], "references": ["show the modifiers and colors"], "bleu": 0.06809398432036522, "rouge_l": 0.09682539682539681}
{"id": 1841, "code": "def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : colorpalette = colors . parse colors ( colorpalette ) self . colorpalette = colors . sanitize color palette ( colorpalette )", "predictions": ["init a init color opened"], "references": ["set the colorpalette which should be used"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1842, "code": "def readattr ( path , name ) : try : f = open ( USB SYS PREFIX + path + \"/\" + name ) return f . readline ( ) . rstrip ( \"\\n\" ) except IO Error : return None", "predictions": ["get the file name from a file or file = none"], "references": ["read attribute from sysfs and return as string"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1843, "code": "def get data ( self , reset device = False ) : try : if reset device : self . device . reset ( ) for interface in [ 0 , 1 ] : if self . device . is kernel driver active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . device , self . ports ) self . device . detach kernel driver ( interface ) self . device . set configuration ( ) usb . util . claim interface ( self . device , INTERFACE ) #self. device.ctrl transfer(bm Request Type=0x21, b Request=0x09, self . control transfer ( COMMANDS [ 'temp' ] ) self . interrupt read ( ) #self. control transfer(COMMANDS['ini1']) #self. interrupt read() #self. control transfer(COMMANDS['ini2']) #self. interrupt read() #self. interrupt read() self . control transfer ( COMMANDS [ 'temp' ] ) temp data = self . interrupt read ( ) if self . device . product == 'TEM Per1F H1 V1.4' : humidity data = temp data else : humidity data = None data = { 'temp data' : temp data , 'humidity data' : humidity data } usb . util . dispose resources ( self . device ) return data except usb . USB Error as err : if not reset device : LOGGER . warning ( \"Encountered %s, resetting %r and trying again.\" , err , self . device ) return self . get data ( True ) if \"not permitted\" in str ( err ) : raise Exception ( \"Permission problem accessing USB. \" \"Maybe I need to run as root?\" ) else : LOGGER . error ( err ) raise", "predictions": ["detach usb data data and process the data text text text text text text text text text text text text text text text text text text text text text text text"], "references": ["get data from the usb device ."], "bleu": 0.046398855339878003, "rouge_l": 0.11879259980525803}
{"id": 1844, "code": "def get temperature ( self , format = 'celsius' , sensor = 0 ) : results = self . get temperatures ( sensors = [ sensor , ] ) if format == 'celsius' : return results [ sensor ] [ 'temperature c' ] elif format == 'fahrenheit' : return results [ sensor ] [ 'temperature f' ] elif format == 'millicelsius' : return results [ sensor ] [ 'temperature mc' ] else : raise Value Error ( \"Unknown format\" )", "predictions": ["doc for a single loader"], "references": ["get device temperature reading ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1845, "code": "def interrupt read ( self ) : data = self . device . read ( ENDPOINT , REQ INT LEN , timeout = TIMEOUT ) LOGGER . debug ( 'Read data: %r' , data ) return data", "predictions": ["lcs lcs string . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["read data from device ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 1846, "code": "def measure memory ( cls , obj , seen = None ) : size = sys . getsizeof ( obj ) if seen is None : seen = set ( ) obj id = id ( obj ) if obj id in seen : return 0 seen . add ( obj id ) if isinstance ( obj , dict ) : size += sum ( [ cls . measure memory ( v , seen ) for v in obj . values ( ) ] ) size += sum ( [ cls . measure memory ( k , seen ) for k in obj . keys ( ) ] ) elif hasattr ( obj , ' dict ' ) : size += cls . measure memory ( obj . dict , seen ) elif hasattr ( obj , ' iter ' ) and not isinstance ( obj , ( str , bytes , bytearray ) ) : size += sum ( [ cls . measure memory ( i , seen ) for i in obj ] ) return size", "predictions": ["generalized word word i . e i . 5 i ."], "references": ["recursively finds size of objects"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1847, "code": "def feed arthur ( self ) : with self . ARTHUR FEED LOCK : if ( time . time ( ) - self . ARTHUR LAST MEMORY CHECK ) > 5 * self . ARTHUR LAST MEMORY CHECK TIME : self . ARTHUR LAST MEMORY CHECK = time . time ( ) logger . debug ( \"Measuring the memory used by the raw items dict ...\" ) try : memory size = self . measure memory ( self . arthur items ) / ( 1024 * 1024 ) except Runtime Error as ex : logger . warning ( \"Can't get the memory used by the raw items dict: %s\" , ex ) memory size = self . ARTHUR LAST MEMORY SIZE self . ARTHUR LAST MEMORY CHECK TIME = time . time ( ) - self . ARTHUR LAST MEMORY CHECK logger . debug ( \"Arthur items memory size: %0.2f MB (%is to check)\" , memory size , self . ARTHUR LAST MEMORY CHECK TIME ) self . ARTHUR LAST MEMORY SIZE = memory size if self . ARTHUR LAST MEMORY SIZE > self . ARTHUR MAX MEMORY SIZE : logger . debug ( \"Items queue full. Not collecting items from redis queue.\" ) return logger . info ( \"Collecting items from redis queue\" ) db url = self . config . get conf ( ) [ 'es collection' ] [ 'redis url' ] conn = redis . Strict Redis . from url ( db url ) logger . debug ( \"Redis connection stablished with %s.\" , db url ) pipe = conn . pipeline ( ) pipe . lrange ( Q STORAGE ITEMS , 0 , self . ARTHUR REDIS ITEMS - 1 ) pipe . ltrim ( Q STORAGE ITEMS , self . ARTHUR REDIS ITEMS , - 1 ) items = pipe . execute ( ) [ 0 ] for item in items : arthur item = pickle . loads ( item ) if arthur item [ 'tag' ] not in self . arthur items : self . arthur items [ arthur item [ 'tag' ] ] = [ ] self . arthur items [ arthur item [ 'tag' ] ] . append ( arthur item ) for tag in self . arthur items : if self . arthur items [ tag ] : logger . debug ( \"Arthur items for %s: %i\" , tag , len ( self . arthur items [ tag ] ) )", "predictions": ["edge the items items to arthur"], "references": ["feed ocean with backend data collected from arthur redis queue"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1848, "code": "def feed backend arthur ( self , repo ) : self . feed arthur ( ) tag = self . backend tag ( repo ) logger . debug ( \"Arthur items available for %s\" , self . arthur items . keys ( ) ) logger . debug ( \"Getting arthur items for %s.\" , tag ) if tag in self . arthur items : logger . debug ( \"Found items for %s.\" , tag ) while self . arthur items [ tag ] : yield self . arthur items [ tag ] . pop ( )", "predictions": ["query self . self . self . self . self . self . self . self . self . self . self . self . self . self . self ."], "references": ["feed ocean with backend data collected from arthur redis queue"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1849, "code": "def sha github file ( cls , config , repo file , repository api , repository branch ) : repo file sha = None cfg = config . get conf ( ) github token = cfg [ 'sortinghat' ] [ 'identities api token' ] headers = { \"Authorization\" : \"token \" + github token } url dir = repository api + \"/git/trees/\" + repository branch logger . debug ( \"Gettting sha data from tree: %s\" , url dir ) raw repo file info = requests . get ( url dir , headers = headers ) raw repo file info . raise for status ( ) for rfile in raw repo file info . json ( ) [ 'tree' ] : if rfile [ 'path' ] == repo file : logger . debug ( \"SHA found: %s, \" , rfile [ \"sha\" ] ) repo file sha = rfile [ \"sha\" ] break return repo file sha", "predictions": ["preprocess a constraints from a constraints"], "references": ["return the github sha for a file in the repository"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1850, "code": "def get uuids from profile name ( self , profile name ) : uuids = [ ] with self . db . connect ( ) as session : query = session . query ( Profile ) . filter ( Profile . name == profile name ) profiles = query . all ( ) if profiles : for p in profiles : uuids . append ( p . uuid ) return uuids", "predictions": ["construct formset from from objects from the database"], "references": ["get the uuid for a profile name"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1851, "code": "def config logging ( debug ) : if debug : logging . basic Config ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( \"Debug mode activated\" ) else : logging . basic Config ( level = logging . INFO , format = '%(asctime)s %(message)s' )", "predictions": ["configure the formset = logger = 0 = 1 = 0 = 1 = 1 = 0 = 1 = 1 = 1 = 1 = 1 = 0 = 1"], "references": ["config logging level output output"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1852, "code": "def get params parser ( ) : parser = argparse . Argument Parser ( add help = False ) parser . add argument ( '-g' , '--debug' , dest = 'debug' , action = 'store true' , help = argparse . SUPPRESS ) parser . add argument ( \"--arthur\" , action = 'store true' , dest = 'arthur' , help = \"Enable arthur to collect raw data\" ) parser . add argument ( \"--raw\" , action = 'store true' , dest = 'raw' , help = \"Activate raw task\" ) parser . add argument ( \"--enrich\" , action = 'store true' , dest = 'enrich' , help = \"Activate enrich task\" ) parser . add argument ( \"--identities\" , action = 'store true' , dest = 'identities' , help = \"Activate merge identities task\" ) parser . add argument ( \"--panels\" , action = 'store true' , dest = 'panels' , help = \"Activate panels task\" ) parser . add argument ( \"--cfg\" , dest = 'cfg path' , help = \"Configuration file path\" ) parser . add argument ( \"--backends\" , dest = 'backend sections' , default = [ ] , nargs = '*' , help = \"Backend sections to execute\" ) if len ( sys . argv ) == 1 : parser . print help ( ) sys . exit ( 1 ) return parser", "predictions": ["parse command line arguments"], "references": ["parse command line arguments"], "bleu": 1.0, "rouge_l": 1.0}
{"id": 1853, "code": "def get params ( ) : parser = get params parser ( ) args = parser . parse args ( ) if not args . raw and not args . enrich and not args . identities and not args . panels : print ( \"No tasks enabled\" ) sys . exit ( 1 ) return args", "predictions": ["get parameters from command line ."], "references": ["get params to execute the micro - mordred"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1854, "code": "def get menu entries ( self , kibiter major ) : menu entries = [ ] for entry in self . panels menu : if entry [ 'source' ] not in self . data sources : continue parent menu item = { 'name' : entry [ 'name' ] , 'title' : entry [ 'name' ] , 'description' : \"\" , 'type' : \"menu\" , 'dashboards' : [ ] } for subentry in entry [ 'menu' ] : try : dash name = get dashboard name ( subentry [ 'panel' ] ) except File Not Found Error : logging . error ( \"Can't open dashboard file %s\" , subentry [ 'panel' ] ) continue child item = { \"name\" : subentry [ 'name' ] , \"title\" : subentry [ 'name' ] , \"description\" : \"\" , \"type\" : \"entry\" , \"panel id\" : dash name } parent menu item [ 'dashboards' ] . append ( child item ) menu entries . append ( parent menu item ) return menu entries", "predictions": ["build a list of formset kwargs to be used in the folder"], "references": ["get the menu entries from the panel definition"], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 1855, "code": "def get dash menu ( self , kibiter major ) : omenu = [ ] omenu . append ( self . menu panels common [ 'Overview' ] ) ds menu = self . get menu entries ( kibiter major ) kafka menu = None community menu = None found kafka = [ pos for pos , menu in enumerate ( ds menu ) if menu [ 'name' ] == KAFKA NAME ] if found kafka : kafka menu = ds menu . pop ( found kafka [ 0 ] ) found community = [ pos for pos , menu in enumerate ( ds menu ) if menu [ 'name' ] == COMMUNITY NAME ] if found community : community menu = ds menu . pop ( found community [ 0 ] ) ds menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds menu if kafka menu : omenu . append ( kafka menu ) if community menu : omenu . append ( community menu ) omenu . append ( self . menu panels common [ 'Data Status' ] ) omenu . append ( self . menu panels common [ 'About' ] ) logger . debug ( \"Menu for panels: %s\" , json . dumps ( ds menu , indent = 4 ) ) return omenu", "predictions": ["return a valid valid valid valid menu menu"], "references": ["order the dashboard menu"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1856, "code": "def autorefresh studies ( self , cfg ) : if 'studies' not in self . conf [ self . backend section ] or 'enrich areas of code:git' not in self . conf [ self . backend section ] [ 'studies' ] : logger . debug ( \"Not doing autorefresh for studies, Areas of Code study is not active.\" ) return aoc index = self . conf [ 'enrich areas of code:git' ] . get ( 'out index' , Git Enrich . GIT AOC ENRICHED ) if not aoc index : aoc index = Git Enrich . GIT AOC ENRICHED logger . debug ( \"Autorefresh for Areas of Code study index: %s\" , aoc index ) es = Elasticsearch ( [ self . conf [ 'es enrichment' ] [ 'url' ] ] , timeout = 100 , verify certs = self . get enrich backend ( ) . elastic . requests . verify ) if not es . indices . exists ( index = aoc index ) : logger . debug ( \"Not doing autorefresh, index doesn't exist for Areas of Code study\" ) return logger . debug ( \"Doing autorefresh for Areas of Code study\" ) aoc backend = Git Enrich ( self . db sh , None , cfg [ 'projects' ] [ 'projects file' ] , self . db user , self . db password , self . db host ) aoc backend . mapping = None aoc backend . roles = [ 'author' ] elastic enrich = get elastic ( self . conf [ 'es enrichment' ] [ 'url' ] , aoc index , clean = False , backend = aoc backend ) aoc backend . set elastic ( elastic enrich ) self . autorefresh ( aoc backend , studies = True )", "predictions": ["callback for sys . settrace"], "references": ["execute autorefresh for areas of code study if configured"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 1857, "code": "def studies ( self , retention time ) : cfg = self . config . get conf ( ) if 'studies' not in cfg [ self . backend section ] or not cfg [ self . backend section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend section ) return studies = [ study for study in cfg [ self . backend section ] [ 'studies' ] if study . strip ( ) != \"\" ] if not studies : logger . debug ( 'No studies for %s' % self . backend section ) return logger . debug ( \"Executing studies for %s: %s\" % ( self . backend section , studies ) ) time . sleep ( 2 ) enrich backend = self . get enrich backend ( ) ocean backend = self . get ocean backend ( enrich backend ) active studies = [ ] all studies = enrich backend . studies all studies names = [ study . name for study in enrich backend . studies ] logger . debug ( \"All studies in %s: %s\" , self . backend section , all studies names ) logger . debug ( \"Configured studies %s\" , studies ) cfg studies types = [ study . split ( \":\" ) [ 0 ] for study in studies ] if not set ( cfg studies types ) . issubset ( set ( all studies names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend section , studies ) raise Runtime Error ( 'Wrong studies names ' , self . backend section , studies ) for study in enrich backend . studies : if study . name in cfg studies types : active studies . append ( study ) enrich backend . studies = active studies print ( \"Executing for %s the studies %s\" % ( self . backend section , [ study for study in studies ] ) ) studies args = self . load studies ( ) do studies ( ocean backend , enrich backend , studies args , retention time = retention time ) enrich backend . studies = all studies", "predictions": ["starts a enrich ."], "references": ["execute the studies configured for the current backend"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 1858, "code": "def get repos by backend section ( cls , backend section , raw = True ) : repos = [ ] projects = Task Projects . get projects ( ) for pro in projects : if backend section in projects [ pro ] : if cls . GLOBAL PROJECT not in projects : repos += projects [ pro ] [ backend section ] else : if raw : if pro != cls . GLOBAL PROJECT : if backend section not in projects [ cls . GLOBAL PROJECT ] : repos += projects [ pro ] [ backend section ] elif backend section in projects [ pro ] and backend section in projects [ cls . GLOBAL PROJECT ] : repos += projects [ cls . GLOBAL PROJECT ] [ backend section ] else : not in unknown = [ projects [ pro ] for pro in projects if pro != cls . GLOBAL PROJECT ] [ 0 ] if backend section not in not in unknown : repos += projects [ cls . GLOBAL PROJECT ] [ backend section ] else : if pro != cls . GLOBAL PROJECT : if backend section not in projects [ cls . GLOBAL PROJECT ] : repos += projects [ pro ] [ backend section ] elif backend section in projects [ pro ] and backend section in projects [ cls . GLOBAL PROJECT ] : repos += projects [ pro ] [ backend section ] else : not in unknown prj = [ projects [ prj ] for prj in projects if prj != cls . GLOBAL PROJECT ] not in unknown sections = list ( set ( [ section for prj in not in unknown prj for section in list ( prj . keys ( ) ) ] ) ) if backend section not in not in unknown sections : repos += projects [ pro ] [ backend section ] logger . debug ( \"List of repos for %s: %s (raw=%s)\" , backend section , repos , raw ) repos = list ( set ( repos ) ) return repos", "predictions": ["get a list of repos that are repos to a backend ."], "references": ["return list with the repositories for a backend_section"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 1859, "code": "def convert from eclipse ( self , eclipse projects ) : projects = { } projects [ 'unknown' ] = { \"gerrit\" : [ \"git.eclipse.org\" ] , \"bugzilla\" : [ \"https://bugs.eclipse.org/bugs/\" ] } projects = compose title ( projects , eclipse projects ) projects = compose projects json ( projects , eclipse projects ) return projects", "predictions": ["convert eclipse from eclipse to json format ."], "references": ["convert from eclipse projects format to grimoire projects json format"], "bleu": 0.22482042087568346, "rouge_l": 0.6535714285714286}
{"id": 1860, "code": "def set param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value", "predictions": ["sets a parameter s value"], "references": ["change a param in the config"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1861, "code": "def execute nonstop tasks ( self , tasks cls ) : self . execute batch tasks ( tasks cls , self . conf [ 'sortinghat' ] [ 'sleep for' ] , self . conf [ 'general' ] [ 'min update delay' ] , False )", "predictions": ["execute all tasks in a list of tasks"], "references": ["just a wrapper to the execute_batch_tasks method"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1862, "code": "def execute initial load ( self ) : if self . conf [ 'phases' ] [ 'panels' ] : tasks cls = [ Task Panels , Task Panels Menu ] self . execute tasks ( tasks cls ) if self . conf [ 'phases' ] [ 'identities' ] : tasks cls = [ Task Init Sorting Hat ] self . execute tasks ( tasks cls ) logger . info ( \"Loading projects\" ) tasks cls = [ Task Projects ] self . execute tasks ( tasks cls ) logger . info ( \"Done\" ) return", "predictions": ["executes the initial load and returns a list of tasks ."], "references": ["tasks that should be done just one time"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1863, "code": "def stdout ( self ) : if self . streaming : stdout = [ ] while not self . stdout . empty ( ) : try : line = self . stdout . get nowait ( ) stdout . append ( line ) except : pass else : stdout = self . stdout return stdout", "predictions": ["return the list of stdout ."], "references": ["converts stdout string to a list ."], "bleu": 0.22236312185643822, "rouge_l": 0.3034825870646766}
{"id": 1864, "code": "def stderr ( self ) : if self . streaming : stderr = [ ] while not self . stderr . empty ( ) : try : line = self . stderr . get nowait ( ) stderr . append ( line ) except : pass else : stderr = self . stderr return stderr", "predictions": ["returns a list of lists of stderr stderr and stderr ."], "references": ["converts stderr string to a list ."], "bleu": 0.17033186037639278, "rouge_l": 0.3472485768500949}
{"id": 1865, "code": "def format ( self , record ) : if isinstance ( self . fmt , dict ) : self . fmt = self . fmt [ record . levelname ] if sys . version info > ( 3 , 2 ) : if self . style not in logging . STYLES : raise Value Error ( 'Style must be one of: %s' % ',' . join ( list ( logging . STYLES . keys ( ) ) ) ) self . style = logging . STYLES [ self . style ] [ 0 ] ( self . fmt ) if sys . version info > ( 2 , 7 ) : message = super ( Level Formatter , self ) . format ( record ) else : message = Colored Formatter . format ( self , record ) return message", "predictions": ["format log record ."], "references": ["customize the message format based on the log level ."], "bleu": 0.09534061816653486, "rouge_l": 0.3978260869565217}
{"id": 1866, "code": "def get storage service ( credentials ) : if credentials is None : credentials = oauth2client . client . Google Credentials . get application default ( ) return discovery . build ( 'storage' , 'v1' , credentials = credentials )", "predictions": ["build a storage service service service ."], "references": ["get a storage client using the provided credentials or defaults ."], "bleu": 0.14834636222628117, "rouge_l": 0.32049036777583184}
{"id": 1867, "code": "def retry storage check ( exception ) : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) print error ( '%s: Exception %s: %s' % ( now , type ( exception ) . name , str ( exception ) ) ) return isinstance ( exception , oauth2client . client . Access Token Refresh Error )", "predictions": ["retry exception check if exception doesn t raise an exception ."], "references": ["return true if we should retry false otherwise ."], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 1868, "code": "def outputs are present ( outputs ) : for o in outputs : if not o . value : continue if o . recursive : if not folder exists ( o . value ) : return False else : if not simple pattern exists in gcs ( o . value ) : return False return True", "predictions": ["check if all outputs are present in the folder ."], "references": ["true if each output contains at least one file or no output specified ."], "bleu": 0.09351498865776114, "rouge_l": 0.16180371352785147}
{"id": 1869, "code": "def build pipeline input file param ( cls , var name , docker path ) : # path , filename = os . path . split ( docker path ) if '*' in filename : return cls . build pipeline file param ( var name , path + '/' ) else : return cls . build pipeline file param ( var name , docker path )", "predictions": ["build a pipeline input file for a given pipeline ."], "references": ["return a dict object representing a pipeline input argument ."], "bleu": 0.25965358893403384, "rouge_l": 0.4}
{"id": 1870, "code": "def build pipeline docker command ( cls , script name , inputs , outputs , envs ) : # recursive input dirs = [ var for var in inputs if var . recursive and var . value ] recursive output dirs = [ var for var in outputs if var . recursive and var . value ] install cloud sdk = '' if recursive input dirs or recursive output dirs : install cloud sdk = INSTALL CLOUD SDK export input dirs = '' copy input dirs = '' if recursive input dirs : export input dirs = providers util . build recursive localize env ( providers util . DATA MOUNT POINT , inputs ) copy input dirs = providers util . build recursive localize command ( providers util . DATA MOUNT POINT , inputs , job model . P GCS ) export output dirs = '' copy output dirs = '' if recursive output dirs : export output dirs = providers util . build recursive gcs delocalize env ( providers util . DATA MOUNT POINT , outputs ) copy output dirs = providers util . build recursive delocalize command ( providers util . DATA MOUNT POINT , outputs , job model . P GCS ) docker paths = [ var . docker path if var . recursive else os . path . dirname ( var . docker path ) for var in outputs if var . value ] mkdirs = '\\n' . join ( [ 'mkdir -p {0}/{1}' . format ( providers util . DATA MOUNT POINT , path ) for path in docker paths ] ) inputs with wildcards = [ var for var in inputs if not var . recursive and var . docker path and '*' in os . path . basename ( var . docker path ) ] export inputs with wildcards = '\\n' . join ( [ 'export {0}=\"{1}/{2}\"' . format ( var . name , providers util . DATA MOUNT POINT , var . docker path ) for var in inputs with wildcards ] ) export empty envs = '\\n' . join ( [ 'export {0}=\"\"' . format ( var . name ) for var in envs | inputs | outputs if not var . value ] ) return DOCKER COMMAND . format ( mk runtime dirs = MK RUNTIME DIRS COMMAND , script path = '%s/%s' % ( providers util . SCRIPT DIR , script name ) , install cloud sdk = install cloud sdk , export inputs with wildcards = export inputs with wildcards , export input dirs = export input dirs , copy input dirs = copy input dirs , mk output dirs = mkdirs , export output dirs = export output dirs , export empty envs = export empty envs , tmpdir = providers util . TMP DIR , working dir = providers util . WORKING DIR , copy output dirs = copy output dirs )", "predictions": ["build the command docker command docker command ."], "references": ["return a multi - line string of the full pipeline docker command ."], "bleu": 0.1689276792789442, "rouge_l": 0.3652694610778443}
{"id": 1871, "code": "def datetime to utc int ( date ) : if date is None : return None epoch = dsub util . replace timezone ( datetime . utcfromtimestamp ( 0 ) , pytz . utc ) return ( date - epoch ) . total seconds ( )", "predictions": ["convert a datetime to a utc utc utc string"], "references": ["convert the integer utc time value into a local datetime ."], "bleu": 0.14211011212459496, "rouge_l": 0.2946859903381642}
{"id": 1872, "code": "def prepare job metadata ( self , script , job name , user id , create time ) : return google base . prepare job metadata ( script , job name , user id , create time )", "predictions": ["prepare the job metadata for the given job script ."], "references": ["returns a dictionary of metadata fields for the job ."], "bleu": 0.21834177214239062, "rouge_l": 0.5}
{"id": 1873, "code": "def build pipeline request ( self , task view ) : job metadata = task view . job metadata job params = task view . job params job resources = task view . job resources task metadata = task view . task descriptors [ 0 ] . task metadata task params = task view . task descriptors [ 0 ] . task params task resources = task view . task descriptors [ 0 ] . task resources script = task view . job metadata [ 'script' ] reserved labels = google base . build pipeline labels ( job metadata , task metadata , task id pattern = 'task-%d' ) pipeline = Pipelines . build pipeline ( project = self . project , zones = job resources . zones , min cores = job resources . min cores , min ram = job resources . min ram , disk size = job resources . disk size , boot disk size = job resources . boot disk size , preemptible = job resources . preemptible , accelerator type = job resources . accelerator type , accelerator count = job resources . accelerator count , image = job resources . image , script name = script . name , envs = job params [ 'envs' ] | task params [ 'envs' ] , inputs = job params [ 'inputs' ] | task params [ 'inputs' ] , outputs = job params [ 'outputs' ] | task params [ 'outputs' ] , pipeline name = job metadata [ 'pipeline-name' ] ) logging uri = task resources . logging path . uri scopes = job resources . scopes or google base . DEFAULT SCOPES pipeline . update ( Pipelines . build pipeline args ( self . project , script . value , job params , task params , reserved labels , job resources . preemptible , logging uri , scopes , job resources . keep alive ) ) return pipeline", "predictions": ["build a pipeline request object ."], "references": ["returns a pipeline objects for the job ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 1874, "code": "def convert suffix to docker chars ( suffix ) : accepted characters = string . ascii letters + string . digits + ' .-' def label char transform ( char ) : if char in accepted characters : return char return '-' return '' . join ( label char transform ( c ) for c in suffix )", "predictions": ["convert a suffix to a docker docker docker docker docker docker docker docker docker docker docker docker docker docker docker docker string ."], "references": ["rewrite string so that all characters are valid in a docker name suffix ."], "bleu": 0.08282282660969602, "rouge_l": 0.1696014828544949}
{"id": 1875, "code": "def task sort function ( task ) : return ( task . get field ( 'create-time' ) , int ( task . get field ( 'task-id' , 0 ) ) , int ( task . get field ( 'task-attempt' , 0 ) ) )", "predictions": ["sort a task function"], "references": ["return a tuple for sorting most recent first ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1876, "code": "def datetime in range ( self , dt , dt min = None , dt max = None ) : dt = dt . replace ( microsecond = 0 ) if dt min : dt min = dt min . replace ( microsecond = 0 ) else : dt min = dsub util . replace timezone ( datetime . datetime . min , pytz . utc ) if dt max : dt max = dt max . replace ( microsecond = 0 ) else : dt max = dsub util . replace timezone ( datetime . datetime . max , pytz . utc ) return dt min <= dt <= dt max", "predictions": ["return datetime . datetime for utc ."], "references": ["determine if the provided time is within the range inclusive ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 1877, "code": "def get task from task dir ( self , job id , user id , task id , task attempt ) : # task dir = self . task directory ( job id , task id , task attempt ) job descriptor = self . read task metadata ( task dir ) if not job descriptor : return None if not job descriptor . job metadata . get ( 'user-id' ) : job descriptor . job metadata [ 'user-id' ] = user id pid = - 1 try : with open ( os . path . join ( task dir , 'task.pid' ) , 'r' ) as f : pid = int ( f . readline ( ) . strip ( ) ) except ( IO Error , OS Error ) : pass script = None script name = job descriptor . job metadata . get ( 'script-name' ) if script name : script = self . read script ( task dir , script name ) end time = self . get end time from task dir ( task dir ) last update = self . get last update time from task dir ( task dir ) events = self . get events from task dir ( task dir ) status = self . get status from task dir ( task dir ) log detail = self . get log detail from task dir ( task dir ) if not status : status = 'RUNNING' log detail = [ 'Pending' ] return Local Task ( task status = status , events = events , log detail = log detail , job descriptor = job descriptor , end time = end time , last update = last update , pid = pid , script = script )", "predictions": ["reads a task object from the cache dir and returns it ."], "references": ["return a task object with this task s info ."], "bleu": 0.2044800736021839, "rouge_l": 0.3696969696969697}
{"id": 1878, "code": "def task directory ( self , job id , task id , task attempt ) : dir name = 'task' if task id is None else str ( task id ) if task attempt : dir name = '%s.%s' % ( dir name , task attempt ) return self . provider root ( ) + '/' + job id + '/' + dir name", "predictions": ["retrieve a task directory for a job"], "references": ["the local dir for staging files for that particular task ."], "bleu": 0.1160873020151595, "rouge_l": 0.10683012259194395}
{"id": 1879, "code": "def make environment ( self , inputs , outputs , mounts ) : env = { } env . update ( providers util . get file environment variables ( inputs ) ) env . update ( providers util . get file environment variables ( outputs ) ) env . update ( providers util . get file environment variables ( mounts ) ) return env", "predictions": ["build the environment for the given inputs ."], "references": ["return a dictionary of environment variables for the container ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 1880, "code": "def localize inputs recursive command ( self , task dir , inputs ) : data dir = os . path . join ( task dir , DATA SUBDIR ) provider commands = [ providers util . build recursive localize command ( data dir , inputs , file provider ) for file provider in SUPPORTED INPUT PROVIDERS ] return '\\n' . join ( provider commands )", "predictions": ["localize inputs into a recursive recursive recursive command ."], "references": ["returns a command that will stage recursive inputs ."], "bleu": 0.18575057999133596, "rouge_l": 0.3333333333333333}
{"id": 1881, "code": "def localize inputs command ( self , task dir , inputs , user project ) : commands = [ ] for i in inputs : if i . recursive or not i . value : continue source file path = i . uri local file path = task dir + '/' + DATA SUBDIR + '/' + i . docker path dest file path = self . get input target path ( local file path ) commands . append ( 'mkdir -p \"%s\"' % os . path . dirname ( local file path ) ) if i . file provider in [ job model . P LOCAL , job model . P GCS ] : # if user project : command = 'gsutil -u %s -mq cp \"%s\" \"%s\"' % ( user project , source file path , dest file path ) else : command = 'gsutil -mq cp \"%s\" \"%s\"' % ( source file path , dest file path ) commands . append ( command ) return '\\n' . join ( commands )", "predictions": ["localize inputs to a single inputs in the run run ."], "references": ["returns a command that will stage inputs ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 1882, "code": "def delocalize outputs commands ( self , task dir , outputs , user project ) : commands = [ ] for o in outputs : if o . recursive or not o . value : continue dest path = o . uri . path local path = task dir + '/' + DATA SUBDIR + '/' + o . docker path if o . file provider == job model . P LOCAL : commands . append ( 'mkdir -p \"%s\"' % dest path ) if o . file provider in [ job model . P LOCAL , job model . P GCS ] : if user project : command = 'gsutil -u %s -mq cp \"%s\" \"%s\"' % ( user project , local path , dest path ) else : command = 'gsutil -mq cp \"%s\" \"%s\"' % ( local path , dest path ) commands . append ( command ) return '\\n' . join ( commands )", "predictions": ["re - run delocalize"], "references": ["copy outputs from local disk to gcs ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 1883, "code": "def map ( self , event ) : description = event . get ( 'description' , '' ) start time = google base . parse rfc3339 utc string ( event . get ( 'timestamp' , '' ) ) for name , regex in EVENT REGEX MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start time } , match return { 'name' : description , 'start-time' : start time } , None", "predictions": ["map a utc event into a utc map"], "references": ["extract elements from an operation event and map to a named event ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 1884, "code": "def get logging env ( self , logging uri , user project ) : if not logging uri . endswith ( '.log' ) : raise Value Error ( 'Logging URI must end in \".log\": {}' . format ( logging uri ) ) logging prefix = logging uri [ : - len ( '.log' ) ] return { 'LOGGING PATH' : '{}.log' . format ( logging prefix ) , 'STDOUT PATH' : '{}-stdout.log' . format ( logging prefix ) , 'STDERR PATH' : '{}-stderr.log' . format ( logging prefix ) , 'USER PROJECT' : user project , }", "predictions": ["returns the logging logging dict for the given logging ."], "references": ["returns the environment for actions that copy logging files ."], "bleu": 0.1972940627795883, "rouge_l": 0.5}
{"id": 1885, "code": "def get prepare env ( self , script , job descriptor , inputs , outputs , mounts ) : docker paths = sorted ( [ var . docker path if var . recursive else os . path . dirname ( var . docker path ) for var in inputs | outputs | mounts if var . value ] ) env = { SCRIPT VARNAME : repr ( script . value ) , META YAML VARNAME : repr ( job descriptor . to yaml ( ) ) , 'DIR COUNT' : str ( len ( docker paths ) ) } for idx , path in enumerate ( docker paths ) : env [ 'DIR {}' . format ( idx ) ] = os . path . join ( providers util . DATA MOUNT POINT , path ) return env", "predictions": ["return a list of providers to run in the script ."], "references": ["return a dict with variables for the prepare action ."], "bleu": 0.17033186037639278, "rouge_l": 0.384251968503937}
{"id": 1886, "code": "def get localization env ( self , inputs , user project ) : non empty inputs = [ var for var in inputs if var . value ] env = { 'INPUT COUNT' : str ( len ( non empty inputs ) ) } for idx , var in enumerate ( non empty inputs ) : env [ 'INPUT {}' . format ( idx ) ] = var . name env [ 'INPUT RECURSIVE {}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'INPUT SRC {}' . format ( idx ) ] = var . value dst = os . path . join ( providers util . DATA MOUNT POINT , var . docker path ) path , filename = os . path . split ( dst ) if '*' in filename : dst = '{}/' . format ( path ) env [ 'INPUT DST {}' . format ( idx ) ] = dst env [ 'USER PROJECT' ] = user project return env", "predictions": ["return environment variables for localization ."], "references": ["return a dict with variables for the localization action ."], "bleu": 0.17749896924055253, "rouge_l": 0.5980392156862745}
{"id": 1887, "code": "def get delocalization env ( self , outputs , user project ) : non empty outputs = [ var for var in outputs if var . value ] env = { 'OUTPUT COUNT' : str ( len ( non empty outputs ) ) } for idx , var in enumerate ( non empty outputs ) : env [ 'OUTPUT {}' . format ( idx ) ] = var . name env [ 'OUTPUT RECURSIVE {}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'OUTPUT SRC {}' . format ( idx ) ] = os . path . join ( providers util . DATA MOUNT POINT , var . docker path ) if '*' in var . uri . basename : dst = var . uri . path else : dst = var . uri env [ 'OUTPUT DST {}' . format ( idx ) ] = dst env [ 'USER PROJECT' ] = user project return env", "predictions": ["return environment variables to be used in delocalization ."], "references": ["return a dict with variables for the delocalization action ."], "bleu": 0.15881076016027915, "rouge_l": 0.41709401709401706}
{"id": 1888, "code": "def build user environment ( self , envs , inputs , outputs , mounts ) : envs = { env . name : env . value for env in envs } envs . update ( providers util . get file environment variables ( inputs ) ) envs . update ( providers util . get file environment variables ( outputs ) ) envs . update ( providers util . get file environment variables ( mounts ) ) return envs", "predictions": ["build environment environment environment"], "references": ["returns a dictionary of for the user container environment ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 1889, "code": "def get mount actions ( self , mounts , mnt datadisk ) : actions to add = [ ] for mount in mounts : bucket = mount . value [ len ( 'gs://' ) : ] mount path = mount . docker path actions to add . extend ( [ google v2 pipelines . build action ( name = 'mount-{}' . format ( bucket ) , flags = [ 'ENABLE FUSE' , 'RUN IN BACKGROUND' ] , image uri = GCSFUSE IMAGE , mounts = [ mnt datadisk ] , commands = [ '--implicit-dirs' , '--foreground' , '-o ro' , bucket , os . path . join ( providers util . DATA MOUNT POINT , mount path ) ] ) , google v2 pipelines . build action ( name = 'mount-wait-{}' . format ( bucket ) , flags = [ 'ENABLE FUSE' ] , image uri = GCSFUSE IMAGE , mounts = [ mnt datadisk ] , commands = [ 'wait' , os . path . join ( providers util . DATA MOUNT POINT , mount path ) ] ) ] ) return actions to add", "predictions": ["get mount to get a list of self backend backend backend backend backend backend"], "references": ["returns a list of two actions per gcs bucket to mount ."], "bleu": 0.18207052811092136, "rouge_l": 0.2340153452685422}
{"id": 1890, "code": "def validate ram ( ram in mb ) : return int ( Google V2Custom Machine . MEMORY MULTIPLE * math . ceil ( ram in mb / Google V2Custom Machine . MEMORY MULTIPLE ) )", "predictions": ["get repos repos from the repos"], "references": ["rounds ram up to the nearest multiple of _memory_multiple ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1891, "code": "def build machine type ( cls , min cores , min ram ) : min cores = min cores or job model . DEFAULT MIN CORES min ram = min ram or job model . DEFAULT MIN RAM min ram *= Google V2Custom Machine . MB PER GB cores = cls . validate cores ( min cores ) ram = cls . validate ram ( min ram ) memory to cpu ratio = ram / cores if memory to cpu ratio < Google V2Custom Machine . MIN MEMORY PER CPU : adjusted ram = Google V2Custom Machine . MIN MEMORY PER CPU * cores ram = cls . validate ram ( adjusted ram ) elif memory to cpu ratio > Google V2Custom Machine . MAX MEMORY PER CPU : adjusted cores = math . ceil ( ram / Google V2Custom Machine . MAX MEMORY PER CPU ) cores = cls . validate cores ( adjusted cores ) else : pass return 'custom-{}-{}' . format ( int ( cores ) , int ( ram ) )", "predictions": ["convert the ram eclipse to the cpu ram . . . . . . . . . . . . . . . . . . . . . . ram"], "references": ["returns a custom machine type string ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 1892, "code": "def lookup job tasks ( self , statuses , user ids = None , job ids = None , job names = None , task ids = None , task attempts = None , labels = None , create time min = None , create time max = None , max tasks = 0 ) : statuses = None if statuses == { '*' } else statuses user ids = None if user ids == { '*' } else user ids job ids = None if job ids == { '*' } else job ids job names = None if job names == { '*' } else job names task ids = None if task ids == { '*' } else task ids task attempts = None if task attempts == { '*' } else task attempts if labels or create time min or create time max : raise Not Implemented Error ( 'Lookup by labels and create time not yet supported by stub.' ) operations = [ x for x in self . operations if ( ( not statuses or x . get field ( 'status' , ( None , None ) ) [ 0 ] in statuses ) and ( not user ids or x . get field ( 'user' , None ) in user ids ) and ( not job ids or x . get field ( 'job-id' , None ) in job ids ) and ( not job names or x . get field ( 'job-name' , None ) in job names ) and ( not task ids or x . get field ( 'task-id' , None ) in task ids ) and ( not task attempts or x . get field ( 'task-attempt' , None ) in task attempts ) ) ] if max tasks > 0 : operations = operations [ : max tasks ] return operations", "predictions": ["set up a param for a param . ."], "references": ["return a list of operations . see base . py for additional detail ."], "bleu": 0.10182634488642418, "rouge_l": 0.2510288065843621}
{"id": 1893, "code": "def get provider ( args , resources ) : provider = getattr ( args , 'provider' , 'google' ) if provider == 'google' : return google . Google Job Provider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry run' , False ) , args . project ) elif provider == 'google-v2' : return google v2 . Google V2Job Provider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry run' , False ) , args . project ) elif provider == 'local' : return local . Local Job Provider ( resources ) elif provider == 'test-fails' : return test fails . Fails Job Provider ( ) else : raise Value Error ( 'Unknown provider: ' + provider )", "predictions": ["retrieve the provider for the model ."], "references": ["returns a provider for job submission requests ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 1894, "code": "def create parser ( prog ) : parser = argparse . Argument Parser ( prog = prog , formatter class = Dsub Help Formatter ) parser . add argument ( '--provider' , default = 'google-v2' , choices = [ 'local' , 'google' , 'google-v2' , 'test-fails' ] , help = , metavar = 'PROVIDER' ) return parser", "predictions": ["execute the cli initial arguments ."], "references": ["create an argument parser adding in the list of providers ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 1895, "code": "def parse args ( parser , provider required args , argv ) : epilog = 'Provider-required arguments:\\n' for provider in provider required args : epilog += '  %s: %s\\n' % ( provider , provider required args [ provider ] ) parser . epilog = epilog args = parser . parse args ( argv ) for arg in provider required args [ args . provider ] : if not args . getattribute ( arg ) : parser . error ( 'argument --%s is required' % arg ) return args", "predictions": ["stdout stdout and parse arguments"], "references": ["add provider required arguments epilog message parse and validate ."], "bleu": 0.11943865131127647, "rouge_l": 0.12577319587628866}
{"id": 1896, "code": "def get dstat provider args ( provider , project ) : provider name = get provider name ( provider ) args = [ ] if provider name == 'google' : args . append ( '--project %s' % project ) elif provider name == 'google-v2' : args . append ( '--project %s' % project ) elif provider name == 'local' : pass elif provider name == 'test-fails' : pass else : assert False , 'Provider %s needs get dstat provider args support' % provider args . insert ( 0 , '--provider %s' % provider name ) return ' ' . join ( args )", "predictions": ["retrieve the dstat to pass to the dstat self get to the dstat self get"], "references": ["a string with the arguments to point dstat to the same provider + project ."], "bleu": 0.14247788801610148, "rouge_l": 0.3333333333333333}
{"id": 1897, "code": "def format task uri ( fmt , job metadata , task metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task metadata . get ( key ) or job metadata . get ( key ) or values [ key ] return fmt . format ( * * values )", "predictions": ["format a task self . into a string for automatically . ."], "references": ["returns a uri with placeholders replaced by metadata values ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 1898, "code": "def google v2 parse arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise Value Error ( 'Exactly one of --regions and --zones must be specified' ) if args . machine type and ( args . min cores or args . min ram ) : raise Value Error ( '--machine-type not supported together with --min-cores or --min-ram.' )", "predictions": ["service arguments should be parsed from get client . . ."], "references": ["validated google - v2 arguments ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 1899, "code": "def group tasks by jobid ( tasks ) : ret = collections . defaultdict ( list ) for t in tasks : ret [ t . get field ( 'job-id' ) ] . append ( t ) return ret", "predictions": ["retry storage storage storage"], "references": ["a defaultdict with for each job a list of its tasks ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 1900, "code": "def validate job and task arguments ( job params , task descriptors ) : if not task descriptors : return task params = task descriptors [ 0 ] . task params from jobs = { label . name for label in job params [ 'labels' ] } from tasks = { label . name for label in task params [ 'labels' ] } intersect = from jobs & from tasks if intersect : raise Value Error ( 'Names for labels on the command-line and in the --tasks file must not ' 'be repeated: {}' . format ( ',' . join ( intersect ) ) ) from jobs = { item . name for item in job params [ 'envs' ] | job params [ 'inputs' ] | job params [ 'outputs' ] } from tasks = { item . name for item in task params [ 'envs' ] | task params [ 'inputs' ] | task params [ 'outputs' ] } intersect = from jobs & from tasks if intersect : raise Value Error ( 'Names for envs, inputs, and outputs on the command-line and in the ' '--tasks file must not be repeated: {}' . format ( ',' . join ( intersect ) ) )", "predictions": ["verify that the are valid are present in the task ."], "references": ["validates that job and task argument names do not overlap ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 1901, "code": "def get filtered mounts ( mounts , mount param type ) : return set ( [ mount for mount in mounts if isinstance ( mount , mount param type ) ] )", "predictions": ["returns a path of all pipeline that are not input"], "references": ["helper function to return an appropriate set of mount parameters ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 1902, "code": "def build logging param ( logging uri , util class = Output File Param Util ) : if not logging uri : return job model . Logging Param ( None , None ) recursive = not logging uri . endswith ( '.log' ) oututil = util class ( '' ) , uri , provider = oututil . parse uri ( logging uri , recursive ) if '*' in uri . basename : raise Value Error ( 'Wildcards not allowed in logging URI: %s' % uri ) return job model . Logging Param ( uri , provider )", "predictions": ["build a pipeline model connection to a pipeline model var var var var var var"], "references": ["convenience function simplifies construction of the logging uri ."], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 1903, "code": "def get variable name ( self , name ) : if not name : name = '%s%s' % ( self . auto prefix , self . auto index ) self . auto index += 1 return name", "predictions": ["datetime the to datetime the to be used in the to datetime = 0 = 0"], "references": ["produce a default variable name if none is specified ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1904, "code": "def parse file provider ( uri ) : providers = { 'gs' : job model . P GCS , 'file' : job model . P LOCAL } provider found = re . match ( r'^([A-Za-z][A-Za-z0-9+.-]{0,29})://' , uri ) if provider found : prefix = provider found . group ( 1 ) . lower ( ) else : prefix = 'file' if prefix in providers : return providers [ prefix ] else : raise Value Error ( 'File prefix not supported: %s://' % prefix )", "predictions": ["prepare a job metadata"], "references": ["find the file provider for a uri ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1905, "code": "def validate paths or fail ( uri , recursive ) : path , filename = os . path . split ( uri ) if '[' in uri or ']' in uri : raise Value Error ( 'Square bracket (character ranges) are not supported: %s' % uri ) if '?' in uri : raise Value Error ( 'Question mark wildcards are not supported: %s' % uri ) if '*' in path : raise Value Error ( 'Path wildcard (*) are only supported for files: %s' % uri ) if '**' in filename : raise Value Error ( 'Recursive wildcards (\"**\") not supported: %s' % uri ) if filename in ( '..' , '.' ) : raise Value Error ( 'Path characters \"..\" and \".\" not supported ' 'for file names: %s' % uri ) if not recursive and not filename : raise Value Error ( 'Input or output values that are not recursive must ' 'reference a filename or wildcard: %s' % uri )", "predictions": ["validates that a uri uri uri is count and not count ."], "references": ["do basic validation of the uri return the path and filename ."], "bleu": 0.1235622127262679, "rouge_l": 0.25}
{"id": 1906, "code": "def parse uri ( self , raw uri , recursive ) : if recursive : raw uri = directory fmt ( raw uri ) file provider = self . parse file provider ( raw uri ) self . validate paths or fail ( raw uri , recursive ) uri , docker uri = self . rewrite uris ( raw uri , file provider ) uri parts = job model . Uri Parts ( directory fmt ( os . path . dirname ( uri ) ) , os . path . basename ( uri ) ) return docker uri , uri parts , file provider", "predictions": ["convert a suffix to a char and return a char suffix + config + for it ."], "references": ["return a valid docker_path uri and file provider from a flag value ."], "bleu": 0.11306082351602978, "rouge_l": 0.2732362821948488}
{"id": 1907, "code": "def make param ( self , name , raw uri , recursive ) : if not raw uri : return self . param class ( name , None , None , None , recursive , None ) docker path , uri parts , provider = self . parse uri ( raw uri , recursive ) return self . param class ( name , raw uri , docker path , uri parts , recursive , provider )", "predictions": ["task parameter definition to convert from provider to docker"], "references": ["return a * fileparam given an input uri ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1908, "code": "def parse image uri ( self , raw uri ) : docker uri = os . path . join ( self . relative path , raw uri . replace ( 'https://' , 'https/' , 1 ) ) return docker uri", "predictions": ["datetime to extract the in the in - memory in the in - memory range"], "references": ["return a valid docker_path from a google persistent disk url ."], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 1909, "code": "def parse local mount uri ( self , raw uri ) : raw uri = directory fmt ( raw uri ) , docker path = local uri rewriter ( raw uri ) local path = docker path [ len ( 'file' ) : ] docker uri = os . path . join ( self . relative path , docker path ) return local path , docker uri", "predictions": ["get task dir and # from task file"], "references": ["return a valid docker_path for a local file path ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 1910, "code": "def parse gcs uri ( self , raw uri ) : raw uri = directory fmt ( raw uri ) , docker path = gcs uri rewriter ( raw uri ) docker uri = os . path . join ( self . relative path , docker path ) return docker uri", "predictions": ["task function to task uri and return uri ."], "references": ["return a valid docker_path for a gcs bucket ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 1911, "code": "def make param ( self , name , raw uri , disk size ) : if raw uri . startswith ( 'https://www.googleapis.com/compute' ) : docker path = self . parse image uri ( raw uri ) return job model . Persistent Disk Mount Param ( name , raw uri , docker path , disk size , disk type = None ) elif raw uri . startswith ( 'file://' ) : local path , docker path = self . parse local mount uri ( raw uri ) return job model . Local Mount Param ( name , raw uri , docker path , local path ) elif raw uri . startswith ( 'gs://' ) : docker path = self . parse gcs uri ( raw uri ) return job model . GCS Mount Param ( name , raw uri , docker path ) else : raise Value Error ( 'Mount parameter {} must begin with valid prefix.' . format ( raw uri ) )", "predictions": ["make a job object from a gcs connection }"], "references": ["return a mountparam given a gcs bucket disk image or local path ."], "bleu": 0.1279808802469055, "rouge_l": 0.26406926406926406}
{"id": 1912, "code": "def validate param name ( name , param type ) : # if not re . match ( r'^[a-z A-Z ][a-z A-Z0-9 ]*$' , name ) : raise Value Error ( 'Invalid %s: %s' % ( param type , name ) )", "predictions": ["localize a inputs recursive"], "references": ["validate that the name follows posix conventions for env variables ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 1913, "code": "def validate bucket name ( bucket ) : if not bucket . startswith ( 'gs://' ) : raise Value Error ( 'Invalid bucket path \"%s\". Must start with \"gs://\".' % bucket ) bucket name = bucket [ len ( 'gs://' ) : ] if not re . search ( r'^\\w[\\w \\.-]{1,61}\\w$' , bucket name ) : raise Value Error ( 'Invalid bucket name: %s' % bucket )", "predictions": ["localize out if inputs don t match the inputs"], "references": ["validate that the name is a valid gcs bucket ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1914, "code": "def convert to label chars ( s ) : accepted characters = string . ascii lowercase + string . digits + '-' def label char transform ( char ) : if char in accepted characters : return char if char in string . ascii uppercase : return char . lower ( ) return '-' return '' . join ( label char transform ( c ) for c in s )", "predictions": ["convert a string to a commands = false"], "references": ["turn the specified name and value into a valid google label ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 1915, "code": "def ensure task params are complete ( task descriptors ) : for task desc in task descriptors : for param in [ 'labels' , 'envs' , 'inputs' , 'outputs' , 'input-recursives' , 'output-recursives' ] : if not task desc . task params . get ( param ) : task desc . task params [ param ] = set ( )", "predictions": ["ensures that all task self are are are are event regex regex regex regex regex regex regex regex regex regex regex regex regex regex regex regex regex regex regex regex are"], "references": ["for each task ensure that each task param entry is not none ."], "bleu": 0.04317900023606586, "rouge_l": 0.09814963797264682}
{"id": 1916, "code": "def validate label ( cls , name , value ) : cls . check label name ( name ) cls . check label value ( value ) if not cls . allow reserved keys and name in RESERVED LABELS : raise Value Error ( 'Label flag (%s=...) must not use reserved keys: %r' % ( name , list ( RESERVED LABELS ) ) )", "predictions": ["ensure that logging is valid"], "references": ["raise valueerror if the label is invalid ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1917, "code": "def find task descriptor ( self , task id ) : for task descriptor in self . task descriptors : if task descriptor . task metadata . get ( 'task-id' ) == task id : return task descriptor return None", "predictions": ["get a prepare env env by its script script script script script mounts"], "references": ["returns the task_descriptor corresponding to task_id ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 1918, "code": "def get file environment variables ( file params ) : env = { } for param in file params : env [ param . name ] = os . path . join ( DATA MOUNT POINT , param . docker path . rstrip ( '/' ) ) if param . value else '' return env", "predictions": ["get all localization variables variables variables"], "references": ["return a dictionary of environment variables for the user container ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 1919, "code": "def get job and task param ( job params , task params , field ) : return job params . get ( field , set ( ) ) | task params . get ( field , set ( ) )", "predictions": ["returns a = = a = = = = false"], "references": ["returns a dict combining the field for job and task params ."], "bleu": 0.13583060054007276, "rouge_l": 0.1788856304985337}
{"id": 1920, "code": "def emit search criteria ( user ids , job ids , task ids , labels ) : print ( 'Delete running jobs:' ) print ( '  user:' ) print ( '    %s\\n' % user ids ) print ( '  job-id:' ) print ( '    %s\\n' % job ids ) if task ids : print ( '  task-id:' ) print ( '    %s\\n' % task ids ) if labels : print ( '  labels:' ) print ( '    %s\\n' % repr ( labels ) )", "predictions": ["search for a search or list of labels ."], "references": ["print the filters used to delete tasks . use raw flags as arguments ."], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 1921, "code": "def get action by id ( op , action id ) : actions = get actions ( op ) if actions and 1 <= action id < len ( actions ) : return actions [ action id - 1 ]", "predictions": ["return an action with a given id ."], "references": ["return the operation s array of actions ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1922, "code": "def get action by name ( op , name ) : actions = get actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action", "predictions": ["find an action by name"], "references": ["return the value for the specified action ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1923, "code": "def get action environment ( op , name ) : action = get action by name ( op , name ) if action : return action . get ( 'environment' )", "predictions": ["get the action environment"], "references": ["return the environment for the operation ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 1924, "code": "def get action image ( op , name ) : action = get action by name ( op , name ) if action : return action . get ( 'image Uri' )", "predictions": ["returns the image image object given by name"], "references": ["return the image for the operation ."], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 1925, "code": "def get event of type ( op , event type ) : events = get events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event type ]", "predictions": ["get the list of events that have a particular type"], "references": ["return all events of a particular type ."], "bleu": 0.25965358893403384, "rouge_l": 0.4535315985130111}
{"id": 1926, "code": "def get last update ( op ) : last update = get end time ( op ) if not last update : last event = get last event ( op ) if last event : last update = last event [ 'timestamp' ] if not last update : last update = get create time ( op ) return last update", "predictions": ["return the last update update time for a given event ."], "references": ["return the most recent timestamp in the operation ."], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 1927, "code": "def prepare output ( self , row ) : date fields = [ 'last-update' , 'create-time' , 'start-time' , 'end-time' ] int fields = [ 'task-attempt' ] for col in date fields : if col in row : row [ col ] = self . default format date ( row [ col ] ) for col in int fields : if col in row and row [ col ] is not None : row [ col ] = int ( row [ col ] ) return row", "predictions": ["prepare a output row for reading in a pandas format"], "references": ["convert types of task fields ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1928, "code": "def trim display field ( self , value , max length ) : if not value : return '' if len ( value ) > max length : return value [ : max length - 3 ] + '...' return value", "predictions": ["truncates a field to its display representation ."], "references": ["return a value for display ; if longer than max length use ellipsis ."], "bleu": 0.09008421318929809, "rouge_l": 0.25994318181818177}
{"id": 1929, "code": "def format pairs ( self , values ) : return ', ' . join ( '%s=%s' % ( key , value ) for key , value in sorted ( values . items ( ) ) )", "predictions": ["format a list of pairs into a string ."], "references": ["returns a string of comma - delimited key = value pairs ."], "bleu": 0.158278836853973, "rouge_l": 0.3713850837138508}
{"id": 1930, "code": "def string presenter ( self , dumper , data ) : if '\\n' in data : return dumper . represent scalar ( 'tag:yaml.org,2002:str' , data , style = '|' ) else : return dumper . represent scalar ( 'tag:yaml.org,2002:str' , data )", "predictions": ["represent string for string data ."], "references": ["presenter to force yaml . dump to use multi - line string style ."], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 1931, "code": "def prepare job metadata ( script , job name , user id , create time ) : if job name : pipeline name = job name job name value = job model . convert to label chars ( job name ) else : pipeline name = os . path . basename ( script ) job name value = job model . convert to label chars ( pipeline name . split ( '.' , 1 ) [ 0 ] ) user id = job model . convert to label chars ( user id ) # # job id = '%s--%s--%s' % ( job name value [ : 10 ] , user id , create time . strftime ( '%y%m%d-%H%M%S-%f' ) [ : 16 ] ) version = job model . convert to label chars ( 'v%s' % DSUB VERSION ) return { 'pipeline-name' : pipeline name , 'job-name' : job name value , 'job-id' : job id , 'user-id' : user id , 'dsub-version' : version , }", "predictions": ["prepare metadata for a job ."], "references": ["returns a dictionary of metadata fields for the job ."], "bleu": 0.17749896924055253, "rouge_l": 0.47843137254901963}
{"id": 1932, "code": "def get operation full job id ( op ) : job id = op . get field ( 'job-id' ) task id = op . get field ( 'task-id' ) if task id : return '%s.%s' % ( job id , task id ) else : return job id", "predictions": ["return the full job id for the operation"], "references": ["returns the job - id or job - id . task - id for the operation ."], "bleu": 0.1576161015669592, "rouge_l": 0.45073891625615764}
{"id": 1933, "code": "def send payload ( self , params ) : data = json . dumps ( { 'jsonrpc' : self . version , 'method' : self . service name , 'params' : params , 'id' : text type ( uuid . uuid4 ( ) ) } ) data binary = data . encode ( 'utf-8' ) url request = Request ( self . service url , data binary , headers = self . headers ) return urlopen ( url request ) . read ( )", "predictions": ["send a payload to the client ."], "references": ["performs the actual sending action and returns the result"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1934, "code": "def json rpc format ( self ) : error = { 'name' : text type ( self . class . name ) , 'code' : self . code , 'message' : '{0}' . format ( text type ( self . message ) ) , 'data' : self . data } if current app . config [ 'DEBUG' ] : import sys , traceback error [ 'stack' ] = traceback . format exc ( ) error [ 'executable' ] = sys . executable return error", "predictions": ["return the error format ."], "references": ["return the exception data in a format for json - rpc"], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 1935, "code": "def discover ( cls ) : file = os . path . join ( Config . config dir , Config . config name ) return cls . from file ( file )", "predictions": ["discover the configuration file ."], "references": ["make a guess about the config file location an try loading it ."], "bleu": 0.06554932163900559, "rouge_l": 0.3086003372681282}
{"id": 1936, "code": "def write config ( self ) : with open ( self . config file , \"w\" ) as config file : self . cfg . write ( config file )", "predictions": ["write the config to the config file ."], "references": ["writes self . cfg to self . config_file ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1937, "code": "def check config sanity ( self ) : is sane = True properties = [ property name for property name , obj in self . class . dict . items ( ) if isinstance ( obj , property ) ] for property name in properties : try : getattr ( self , property name ) except Value Error as e : click . echo ( \"\u2717 Config error on {0} - {1}\".f o rmat(p r operty name,  e ) is sane = False return is sane", "predictions": ["check if all properties have been set ."], "references": ["checks if the given values in the config file are sane ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 1938, "code": "def validate config key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( \".\" , 1 ) except Value Error : raise click . Bad Argument Usage ( \"Given key does not contain a section name.\" ) else : return section , item", "predictions": ["validate that a config key is a valid config key ."], "references": ["validate a configuration key according to section . item ."], "bleu": 0.14323145079400493, "rouge_l": 0.384251968503937}
{"id": 1939, "code": "def make aware ( dt ) : return dt if dt . tzinfo else dt . replace ( tzinfo = timezone . utc )", "predictions": ["make a datetime from a datetime . aware . aware . aware ."], "references": ["appends tzinfo and assumes utc if datetime object has no tzinfo already ."], "bleu": 0.10571070857151538, "rouge_l": 0.15384615384615383}
{"id": 1940, "code": "def from file ( cls , file , * args , * * kwargs ) : try : cache = shelve . open ( file ) return cls ( file , cache , * args , * * kwargs ) except OS Error as e : logger . debug ( \"Loading {0} failed\" . format ( file ) ) raise e", "predictions": ["create a cache object from a file ."], "references": ["try loading given cache file ."], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 1941, "code": "def discover ( cls , * args , * * kwargs ) : file = os . path . join ( Cache . cache dir , Cache . cache name ) return cls . from file ( file , * args , * * kwargs )", "predictions": ["load the cache file ."], "references": ["make a guess about the cache file location an try loading it ."], "bleu": 0.10847596378846372, "rouge_l": 0.41146711635750427}
{"id": 1942, "code": "def is cached ( self , url ) : try : return True if url in self . cache else False except Type Error : return False", "predictions": ["return true if url is cached"], "references": ["checks if specified url is cached ."], "bleu": 0.36798327352994814, "rouge_l": 0.6069651741293532}
{"id": 1943, "code": "def add tweets ( self , url , last modified , tweets ) : try : self . cache [ url ] = { \"last modified\" : last modified , \"tweets\" : tweets } self . mark updated ( ) return True except Type Error : return False", "predictions": ["add tweets for a given url"], "references": ["adds new tweets to the cache ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1944, "code": "def get tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ \"tweets\" ] self . mark updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except Key Error : return [ ]", "predictions": ["get tweets for given url ."], "references": ["retrieves tweets from the cache ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1945, "code": "def remove tweets ( self , url ) : try : del self . cache [ url ] self . mark updated ( ) return True except Key Error : return False", "predictions": ["remove a url from the queue"], "references": ["tries to remove cached tweets ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1946, "code": "def cli ( ctx , config , verbose ) : init logging ( debug = verbose ) if ctx . invoked subcommand == \"quickstart\" : return try : if config : conf = Config . from file ( config ) else : conf = Config . discover ( ) except Value Error as e : if \"Error in config file.\" in str ( e ) : click . echo ( \"\u2717 Please correct the errors mentioned above an run twtxt again.\") else : click . echo ( \"\u2717 Config file not found or not readable. You may want to run twtxt quickstart.\") sys . exit ( ) ctx . default map = conf . build default map ( ) ctx . obj = { 'conf' : conf }", "predictions": ["run the cli command line program ."], "references": ["decentralised minimalist microblogging service for hackers ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1947, "code": "def tweet ( ctx , created at , twtfile , text ) : text = expand mentions ( text ) tweet = Tweet ( text , created at ) if created at else Tweet ( text ) pre tweet hook = ctx . obj [ \"conf\" ] . pre tweet hook if pre tweet hook : run pre tweet hook ( pre tweet hook , ctx . obj [ \"conf\" ] . options ) if not add local tweet ( tweet , twtfile ) : click . echo ( \"\u2717 Couldn\u2019t write to file.\") else : post tweet hook = ctx . obj [ \"conf\" ] . post tweet hook if post tweet hook : run post tweet hook ( post tweet hook , ctx . obj [ \"conf\" ] . options )", "predictions": ["sends a tweet to the tweet ."], "references": ["append a new tweet to your twtxt file ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 1948, "code": "def timeline ( ctx , pager , limit , twtfile , sorting , timeout , porcelain , source , cache , force update ) : if source : source obj = ctx . obj [ \"conf\" ] . get source by nick ( source ) if not source obj : logger . debug ( \"Not following {0}, trying as URL\" . format ( source ) ) source obj = Source ( source , source ) sources = [ source obj ] else : sources = ctx . obj [ \"conf\" ] . following tweets = [ ] if cache : try : with Cache . discover ( update interval = ctx . obj [ \"conf\" ] . timeline update interval ) as cache : force update = force update or not cache . is valid if force update : tweets = get remote tweets ( sources , limit , timeout , cache ) else : logger . debug ( \"Multiple calls to 'timeline' within {0} seconds. Skipping update\" . format ( cache . update interval ) ) tweets = list ( chain . from iterable ( [ cache . get tweets ( source . url ) for source in sources ] ) ) except OS Error as e : logger . debug ( e ) tweets = get remote tweets ( sources , limit , timeout ) else : tweets = get remote tweets ( sources , limit , timeout ) if twtfile and not source : source = Source ( ctx . obj [ \"conf\" ] . nick , ctx . obj [ \"conf\" ] . twturl , file = twtfile ) tweets . extend ( get local tweets ( source , limit ) ) if not tweets : return tweets = sort and truncate tweets ( tweets , sorting , limit ) if pager : click . echo via pager ( style timeline ( tweets , porcelain ) ) else : click . echo ( style timeline ( tweets , porcelain ) )", "predictions": ["upload a timeline to all tweets within a remote cache ."], "references": ["retrieve your personal timeline ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 1949, "code": "def follow ( ctx , nick , url , force ) : source = Source ( nick , url ) sources = ctx . obj [ 'conf' ] . following if not force : if source . nick in ( source . nick for source in sources ) : click . confirm ( \"\u27a4 You\u2019re already following {0}. Overwrite?\".for m at( click . style ( source . nick , bold = True ) ) , default = False , abort = True ) , status = get remote status ( [ source ] ) [ 0 ] if not status or status . status code != 200 : click . confirm ( \"\u27a4 The feed of {0} at {1} is not available. Follow anyway?\".f o rmat( click . style ( source . nick , bold = True ) , click . style ( source . url , bold = True ) ) , default = False , abort = True ) ctx . obj [ 'conf' ] . add source ( source ) click . echo ( \"\u2713 You\u2019re now following {0}.\".for m at( click . style ( source . nick , bold = True ) ) )", "predictions": ["follow a feed ."], "references": ["add a new source to your followings ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 1950, "code": "def unfollow ( ctx , nick ) : source = ctx . obj [ 'conf' ] . get source by nick ( nick ) try : with Cache . discover ( ) as cache : cache . remove tweets ( source . url ) except OS Error as e : logger . debug ( e ) ret val = ctx . obj [ 'conf' ] . remove source by nick ( nick ) if ret val : click . echo ( \"\u2713 You\u2019ve unfollowed {0}.\".for m at( click . style ( source . nick , bold = True ) ) ) else : click . echo ( \"\u2717 You\u2019re not following {0}.\".for m at( click . style ( nick , bold = True ) ) )", "predictions": ["discover a at( at("], "references": ["remove an existing source from your followings ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 1951, "code": "def quickstart ( ) : width = click . get terminal size ( ) [ 0 ] width = width if width <= 79 else 79 click . secho ( \"twtxt - quickstart\" , fg = \"cyan\" ) click . secho ( \"==================\" , fg = \"cyan\" ) click . echo ( ) help text = \"This wizard will generate a basic configuration file for twtxt with all mandatory options set. \" \"You can change all of these later with either twtxt itself or by editing the config file manually. \" \"Have a look at the docs to get information about the other available options and their meaning.\" click . echo ( textwrap . fill ( help text , width ) ) click . echo ( ) nick = click . prompt ( \"\u27a4 Please enter your desired nick\",  d fault=o s .e n viron.g e t(\" U SER\",  \" )) def overwrite check ( path ) : if os . path . isfile ( path ) : click . confirm ( \"\u27a4 '{0}' already exists. Overwrite?\".f o rmat(p a th),   a ort=T r ue) cfgfile = click . prompt ( \"\u27a4 Please enter the desired location for your config file\", os . path . join ( Config . config dir , Config . config name ) , type = click . Path ( readable = True , writable = True , file okay = True ) ) cfgfile = os . path . expanduser ( cfgfile ) overwrite check ( cfgfile ) twtfile = click . prompt ( \"\u27a4 Please enter the desired location for your twtxt file\", os . path . expanduser ( \"~/twtxt.txt\" ) , type = click . Path ( readable = True , writable = True , file okay = True ) ) twtfile = os . path . expanduser ( twtfile ) overwrite check ( twtfile ) twturl = click . prompt ( \"\u27a4 Please enter the URL your twtxt file will be accessible from\", default = \"https://example.org/twtxt.txt\" ) disclose identity = click . confirm ( \"\u27a4 Do you want to disclose your identity? Your nick and URL will be shared when \" \"making HTTP requests\" , default = False ) click . echo ( ) add news = click . confirm ( \"\u27a4 Do you want to follow the twtxt news feed?\",  d fault=T r ue) conf = Config . create config ( cfgfile , nick , twtfile , twturl , disclose identity , add news ) twtfile dir = os . path . dirname ( twtfile ) if not os . path . exists ( twtfile dir ) : os . makedirs ( twtfile dir ) open ( twtfile , \"a\" ) . close ( ) click . echo ( ) click . echo ( \"\u2713 Created config file at '{0}'.\".f o rmat(c l ick.f o rmat filename(c o nf.c o nfig file)) ) click . echo ( \"\u2713 Created twtxt file at '{0}'.\".f o rmat(c l ick.f o rmat filename(t w tfile)) )", "predictions": ["generates a quickstart configuration file"], "references": ["quickstart wizard for setting up twtxt ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1952, "code": "def config ( ctx , key , value , remove , edit ) : conf = ctx . obj [ \"conf\" ] if not edit and not key : raise click . Bad Argument Usage ( \"You have to specify either a key or use --edit.\" ) if edit : return click . edit ( filename = conf . config file ) if remove : try : conf . cfg . remove option ( key [ 0 ] , key [ 1 ] ) except Exception as e : logger . debug ( e ) else : conf . write config ( ) return if not value : try : click . echo ( conf . cfg . get ( key [ 0 ] , key [ 1 ] ) ) except Exception as e : logger . debug ( e ) return if not conf . cfg . has section ( key [ 0 ] ) : conf . cfg . add section ( key [ 0 ] ) conf . cfg . set ( key [ 0 ] , key [ 1 ] , value ) conf . write config ( )", "predictions": ["write command line entry point print details ."], "references": ["get or set config item ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1953, "code": "def relative datetime ( self ) : now = datetime . now ( timezone . utc ) tense = \"from now\" if self . created at > now else \"ago\" return \"{0} {1}\" . format ( humanize . naturaldelta ( now - self . created at ) , tense )", "predictions": ["return get the current action in the = day"], "references": ["return human - readable relative time string ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 1954, "code": "def save ( url , * args , * * kwargs ) : device = heimdall Device ( kwargs . get ( 'device' , None ) ) kwargs [ 'width' ] = kwargs . get ( 'width' , None ) or device . width kwargs [ 'height' ] = kwargs . get ( 'height' , None ) or device . height kwargs [ 'user agent' ] = kwargs . get ( 'user agent' , None ) or device . user agent screenshot image = screenshot ( url , * * kwargs ) if kwargs . get ( 'optimize' ) : image = Image . open ( screenshot image . path ) image . save ( screenshot image . path , optimize = True ) return screenshot image", "predictions": ["get a screenshot image image"], "references": ["parse the options set defaults and then fire up phantomjs ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 1955, "code": "def screenshot ( url , * args , * * kwargs ) : phantomscript = os . path . join ( os . path . dirname ( file ) , 'take screenshot.js' ) directory = kwargs . get ( 'save dir' , '/tmp' ) image name = kwargs . get ( 'image name' , None ) or image name from url ( url ) ext = kwargs . get ( 'format' , 'png' ) . lower ( ) save path = os . path . join ( directory , image name ) + '.' + ext crop to visible = kwargs . get ( 'crop to visible' , False ) cmd args = [ 'phantomjs' , '--ssl-protocol=any' , phantomscript , url , '--width' , str ( kwargs [ 'width' ] ) , '--height' , str ( kwargs [ 'height' ] ) , '--useragent' , str ( kwargs [ 'user agent' ] ) , '--dir' , directory , '--ext' , ext , '--name' , str ( image name ) , ] if crop to visible : cmd args . append ( '--croptovisible' ) output = subprocess . Popen ( cmd args , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] return Screenshot ( save path , directory , image name + '.' + ext , ext )", "predictions": ["take a get request and save it to visible"], "references": ["call phantomjs with the specified flags and options ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 1956, "code": "def image name from url ( url ) : find = r'https?://|[^\\w]' replace = ' ' return re . sub ( find , replace , url ) . strip ( ' ' )", "predictions": ["extract the get action action from a url url"], "references": ["create a nice image name from the url ."], "bleu": 0.17747405280050263, "rouge_l": 0.2222222222222222}
{"id": 1957, "code": "def release ( self ) : if self . value is not None : self . value += 1 if self . value > self . maximum value : raise Value Error ( \"Too many releases\" )", "predictions": ["increment the counter value ."], "references": ["release incrementing the internal counter by one ."], "bleu": 0.1781815298791261, "rouge_l": 0.44309927360774815}
{"id": 1958, "code": "def connect ( ) : ftp class = ftplib . FTP if not SSL else ftplib . FTP TLS ftp = ftp class ( timeout = TIMEOUT ) ftp . connect ( HOST , PORT ) ftp . login ( USER , PASSWORD ) if SSL : ftp . prot p ( ) return ftp", "predictions": ["get an op for the op"], "references": ["connect to ftp server login and return an ftplib . ftp instance ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 1959, "code": "def bytes per second ( ftp , retr = True ) : tot bytes = 0 if retr : def request file ( ) : ftp . voidcmd ( 'TYPE I' ) conn = ftp . transfercmd ( \"retr \" + TESTFN ) return conn with contextlib . closing ( request file ( ) ) as conn : register memory ( ) stop at = time . time ( ) + 1.0 while stop at > time . time ( ) : chunk = conn . recv ( BUFFER LEN ) if not chunk : a = time . time ( ) ftp . voidresp ( ) conn . close ( ) conn = request file ( ) stop at += time . time ( ) - a tot bytes += len ( chunk ) try : while chunk : chunk = conn . recv ( BUFFER LEN ) ftp . voidresp ( ) conn . close ( ) except ( ftplib . error temp , ftplib . error perm ) : pass else : ftp . voidcmd ( 'TYPE I' ) with contextlib . closing ( ftp . transfercmd ( \"STOR \" + TESTFN ) ) as conn : register memory ( ) chunk = b'x' * BUFFER LEN stop at = time . time ( ) + 1 while stop at > time . time ( ) : tot bytes += conn . send ( chunk ) ftp . voidresp ( ) return tot bytes", "predictions": ["send output output output output output output . ."], "references": ["return the number of bytes transmitted in 1 second ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1960, "code": "def clone ( self ) : return Stream Throttle ( read = self . read . clone ( ) , write = self . write . clone ( ) )", "predictions": ["trim the query ."], "references": ["clone throttles without memory"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1961, "code": "def get zoom level ( zoom , process ) : if zoom is None : return reversed ( process . config . zoom levels ) if isinstance ( zoom , int ) : return [ zoom ] elif len ( zoom ) == 2 : return reversed ( range ( min ( zoom ) , max ( zoom ) + 1 ) ) elif len ( zoom ) == 1 : return zoom", "predictions": ["get the pairs of a pairs"], "references": ["determine zoom levels ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1962, "code": "def process worker ( process , process tile ) : logger . debug ( ( process tile . id , \"running on %s\" % current process ( ) . name ) ) if ( process . config . mode == \"continue\" and process . config . output . tiles exist ( process tile ) ) : logger . debug ( ( process tile . id , \"tile exists, skipping\" ) ) return Process Info ( tile = process tile , processed = False , process msg = \"output already exists\" , written = False , write msg = \"nothing written\" ) else : with Timer ( ) as t : try : output = process . execute ( process tile , raise nodata = True ) except Mapchete Nodata Tile : output = None processor message = \"processed in %s\" % t logger . debug ( ( process tile . id , processor message ) ) writer info = process . write ( process tile , output ) return Process Info ( tile = process tile , processed = True , process msg = processor message , written = writer info . written , write msg = writer info . write msg )", "predictions": ["processes a worker and its protocol protocol message . ."], "references": ["worker function running the process ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 1963, "code": "def extract ( self , in tile = None , in data = None , out tile = None ) : return self . config . output . extract subset ( input data tiles = [ ( in tile , in data ) ] , out tile = out tile )", "predictions": ["prepare a value for a given tile if it exists ."], "references": ["extract data from tile ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 1964, "code": "def pyramid ( input raster , output dir , pyramid type = None , output format = None , resampling method = None , scale method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid type = pyramid type , scale method = scale method , output format = output format , resampling = resampling method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input raster , output dir , options )", "predictions": ["get a get of a get"], "references": ["create tile pyramid out of input raster ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1965, "code": "def raster2pyramid ( input file , output dir , options ) : pyramid type = options [ \"pyramid type\" ] scale method = options [ \"scale method\" ] output format = options [ \"output format\" ] resampling = options [ \"resampling\" ] zoom = options [ \"zoom\" ] bounds = options [ \"bounds\" ] mode = \"overwrite\" if options [ \"overwrite\" ] else \"continue\" minzoom , maxzoom = get zoom ( zoom , input file , pyramid type ) with rasterio . open ( input file , \"r\" ) as input raster : output bands = input raster . count input dtype = input raster . dtypes [ 0 ] output dtype = input raster . dtypes [ 0 ] nodataval = input raster . nodatavals [ 0 ] nodataval = nodataval if nodataval else 0 if output format == \"PNG\" and output bands > 3 : output bands = 3 output dtype = 'uint8' scales minmax = ( ) if scale method == \"dtype scale\" : for index in range ( 1 , output bands + 1 ) : scales minmax += ( DTYPE RANGES [ input dtype ] , ) elif scale method == \"minmax scale\" : for index in range ( 1 , output bands + 1 ) : band = input raster . read ( index ) scales minmax += ( ( band . min ( ) , band . max ( ) ) , ) elif scale method == \"crop\" : for index in range ( 1 , output bands + 1 ) : scales minmax += ( ( 0 , 255 ) , ) if input dtype == \"uint8\" : scale method = None scales minmax = ( ) for index in range ( 1 , output bands + 1 ) : scales minmax += ( ( None , None ) , ) config = dict ( process = \"mapchete.processes.pyramid.tilify\" , output = { \"path\" : output dir , \"format\" : output format , \"bands\" : output bands , \"dtype\" : output dtype } , pyramid = dict ( pixelbuffer = 5 , grid = pyramid type ) , scale method = scale method , scales minmax = scales minmax , input = { \"raster\" : input file } , config dir = os . getcwd ( ) , zoom levels = dict ( min = minzoom , max = maxzoom ) , nodataval = nodataval , resampling = resampling , bounds = bounds , baselevel = { \"zoom\" : maxzoom , \"resampling\" : resampling } , mode = mode ) with mapchete . open ( config , zoom = zoom , bounds = bounds ) as mp : if not os . path . exists ( output dir ) : os . makedirs ( output dir ) mp . batch process ( zoom = [ minzoom , maxzoom ] )", "predictions": ["binary data to a band file"], "references": ["create a tile pyramid out of an input raster dataset ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 1966, "code": "def get zoom ( zoom , input raster , pyramid type ) : if not zoom : minzoom = 1 maxzoom = get best zoom level ( input raster , pyramid type ) elif len ( zoom ) == 1 : minzoom = zoom [ 0 ] maxzoom = zoom [ 0 ] elif len ( zoom ) == 2 : if zoom [ 0 ] < zoom [ 1 ] : minzoom = zoom [ 0 ] maxzoom = zoom [ 1 ] else : minzoom = zoom [ 1 ] maxzoom = zoom [ 0 ] return minzoom , maxzoom", "predictions": ["json - rpc rpc rpc rpc rpc rpc"], "references": ["determine minimum and maximum zoomlevel ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1967, "code": "def get hash ( x ) : if isinstance ( x , str ) : return hash ( x ) elif isinstance ( x , dict ) : return hash ( yaml . dump ( x ) )", "predictions": ["join a hash object"], "references": ["return hash of x ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1968, "code": "def get zoom levels ( process zoom levels = None , init zoom levels = None ) : process zoom levels = validate zooms ( process zoom levels ) if init zoom levels is None : return process zoom levels else : init zoom levels = validate zooms ( init zoom levels ) if not set ( init zoom levels ) . issubset ( set ( process zoom levels ) ) : raise Mapchete Config Error ( \"init zooms must be a subset of process zoom\" ) return init zoom levels", "predictions": ["construct a subset instance for the current config config ."], "references": ["validate and return zoom levels ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 1969, "code": "def raw at zoom ( config , zooms ) : params per zoom = { } for zoom in zooms : params = { } for name , element in config . items ( ) : if name not in RESERVED PARAMETERS : out element = element at zoom ( name , element , zoom ) if out element is not None : params [ name ] = out element params per zoom [ zoom ] = params return params per zoom", "predictions": ["sanity sanity sanity sanity sanity sanity sanity sanity sanity sanity sanity sanity sanity to sanity sanity ."], "references": ["return parameter dictionary per zoom level ."], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 1970, "code": "def filter by zoom ( element = None , conf string = None , zoom = None ) : for op str , op func in [ ( \"=\" , operator . eq ) , ( \"<=\" , operator . le ) , ( \">=\" , operator . ge ) , ( \"<\" , operator . lt ) , ( \">\" , operator . gt ) , ] : if conf string . startswith ( op str ) : return element if op func ( zoom , strip zoom ( conf string , op str ) ) else None", "predictions": ["validate against against a key and key key key 1 1 1 1 key 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"], "references": ["return element only if zoom condition matches with config string ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1971, "code": "def strip zoom ( input string , strip string ) : try : return int ( input string . strip ( strip string ) ) except Exception as e : raise Mapchete Config Error ( \"zoom level could not be determined: %s\" % e )", "predictions": ["make a aware string aware aware"], "references": ["return zoom level as integer or throw error ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 1972, "code": "def flatten tree ( tree , old path = None ) : flat tree = [ ] for key , value in tree . items ( ) : new path = \"/\" . join ( [ old path , key ] ) if old path else key if isinstance ( value , dict ) and \"format\" not in value : flat tree . extend ( flatten tree ( value , old path = new path ) ) else : flat tree . append ( ( new path , value ) ) return flat tree", "predictions": ["from a file file file file file file file file file file file"], "references": ["flatten dict tree into dictionary where keys are paths of old dict ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 1973, "code": "def unflatten tree ( flat ) : tree = { } for key , value in flat . items ( ) : path = key . split ( \"/\" ) if len ( path ) == 1 : tree [ key ] = value else : if not path [ 0 ] in tree : tree [ path [ 0 ] ] = unflatten tree ( { \"/\" . join ( path [ 1 : ] ) : value } ) else : branch = unflatten tree ( { \"/\" . join ( path [ 1 : ] ) : value } ) if not path [ 1 ] in tree [ path [ 0 ] ] : tree [ path [ 0 ] ] [ path [ 1 ] ] = branch [ path [ 1 ] ] else : tree [ path [ 0 ] ] [ path [ 1 ] ] . update ( branch [ path [ 1 ] ] ) return tree", "predictions": ["get a dictionary with the branch parameters"], "references": ["reverse tree flattening ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1974, "code": "def bounds ( self ) : if self . raw [ \"bounds\" ] is None : return self . process pyramid . bounds else : return Bounds ( * validate bounds ( self . raw [ \"bounds\" ] ) )", "predictions": ["is the current is a is the is the is the is the is the is the is the is the is the is a is the is the is the"], "references": ["process bounds as defined in the configuration ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 1975, "code": "def output ( self ) : output params = dict ( self . raw [ \"output\" ] , grid = self . output pyramid . grid , pixelbuffer = self . output pyramid . pixelbuffer , metatiling = self . output pyramid . metatiling ) if \"path\" in output params : output params . update ( path = absolute path ( path = output params [ \"path\" ] , base dir = self . config dir ) ) if \"format\" not in output params : raise Mapchete Config Error ( \"output format not specified\" ) if output params [ \"format\" ] not in available output formats ( ) : raise Mapchete Config Error ( \"format %s not available in %s\" % ( output params [ \"format\" ] , str ( available output formats ( ) ) ) ) writer = load output writer ( output params ) try : writer . is valid with config ( output params ) except Exception as e : logger . exception ( e ) raise Mapchete Config Error ( \"driver %s not compatible with configuration: %s\" % ( writer . METADATA [ \"driver name\" ] , e ) ) return writer", "predictions": ["provides a writer writer writer to . ."], "references": ["output object of driver ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1976, "code": "def profile ( self ) : with rasterio . open ( self . path , \"r\" ) as src : return deepcopy ( src . meta )", "predictions": ["return get file = none = { limit } = { limit } = { limit { limit } = { limit } = { limit { limit } = {"], "references": ["return raster metadata ."], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 1977, "code": "def get band indexes ( self , indexes = None ) : if indexes : if isinstance ( indexes , list ) : return indexes else : return [ indexes ] else : return range ( 1 , self . raster file . profile [ \"count\" ] + 1 )", "predictions": ["return will be a list of tweets for a given indexes"], "references": ["return valid band indexes ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 1978, "code": "def write output metadata ( output params ) : if \"path\" in output params : metadata path = os . path . join ( output params [ \"path\" ] , \"metadata.json\" ) logger . debug ( \"check for output %s\" , metadata path ) try : existing params = read output metadata ( metadata path ) logger . debug ( \"%s exists\" , metadata path ) logger . debug ( \"existing output parameters: %s\" , pformat ( existing params ) ) existing tp = existing params [ \"pyramid\" ] current params = params to dump ( output params ) logger . debug ( \"current output parameters: %s\" , pformat ( current params ) ) current tp = Buffered Tile Pyramid ( * * current params [ \"pyramid\" ] ) if existing tp != current tp : raise Mapchete Config Error ( \"pyramid definitions between existing and new output do not match: \" \"%s != %s\" % ( existing tp , current tp ) ) existing format = existing params [ \"driver\" ] [ \"format\" ] current format = current params [ \"driver\" ] [ \"format\" ] if existing format != current format : raise Mapchete Config Error ( \"existing output format does not match new output format: \" \"%s != %s\" % ( ( existing format , current format ) ) ) except File Not Found Error : logger . debug ( \"%s does not exist\" , metadata path ) dump params = params to dump ( output params ) write json ( metadata path , dump params ) else : logger . debug ( \"no path parameter found\" )", "predictions": ["cli action to echo the output output ctx ."], "references": ["dump output json and verify parameters if output metadata exist ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 1979, "code": "def get contour values ( min val , max val , base = 0 , interval = 100 ) : i = base out = [ ] if min val < base : while i >= min val : i -= interval while i <= max val : if i >= min val : out . append ( i ) i += interval return out", "predictions": ["tweet a set of contour to extract contour from a given value"], "references": ["return a list of values between min and max within an interval ."], "bleu": 0.10579369505074822, "rouge_l": 0.15885416666666669}
{"id": 1980, "code": "def create ( mapchete file , process file , out format , out path = None , pyramid type = None , force = False ) : if os . path . isfile ( process file ) or os . path . isfile ( mapchete file ) : if not force : raise IO Error ( \"file(s) already exists\" ) out path = out path if out path else os . path . join ( os . getcwd ( ) , \"output\" ) process template = pkg resources . resource filename ( \"mapchete.static\" , \"process template.py\" ) process file = os . path . join ( os . getcwd ( ) , process file ) copyfile ( process template , process file ) mapchete template = pkg resources . resource filename ( \"mapchete.static\" , \"mapchete template.mapchete\" ) output options = dict ( format = out format , path = out path , * * FORMAT MANDATORY [ out format ] ) pyramid options = { 'grid' : pyramid type } substitute elements = { 'process file' : process file , 'output' : dump ( { 'output' : output options } , default flow style = False ) , 'pyramid' : dump ( { 'pyramid' : pyramid options } , default flow style = False ) } with open ( mapchete template , 'r' ) as config template : config = Template ( config template . read ( ) ) customized config = config . substitute ( substitute elements ) with open ( mapchete file , 'w' ) as target config : target config . write ( customized config )", "predictions": ["timeline function to timeline a single limit"], "references": ["create an empty mapchete and process file in a given directory ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 1981, "code": "def to dict ( self ) : return dict ( grid = self . grid . to dict ( ) , metatiling = self . metatiling , tile size = self . tile size , pixelbuffer = self . pixelbuffer )", "predictions": ["force a dictionary of the image into a dict instance ."], "references": ["return dictionary representation of pyramid parameters ."], "bleu": 0.1354599427337814, "rouge_l": 0.3472485768500949}
{"id": 1982, "code": "def is on edge ( self ) : return ( self . left <= self . tile pyramid . left or self . bottom <= self . tile pyramid . bottom or self . right >= self . tile pyramid . right or self . top >= self . tile pyramid . top )", "predictions": ["returns true if the ctx is on on = true ."], "references": ["determine whether tile touches or goes over pyramid edge ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 1983, "code": "def get band indexes ( self , indexes = None ) : if indexes : if isinstance ( indexes , list ) : return indexes else : return [ indexes ] else : return range ( 1 , self . process . config . output . profile ( self . tile ) [ \"count\" ] + 1 )", "predictions": ["return will be used for a given - config ."], "references": ["return valid band indexes ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 1984, "code": "def create app ( mapchete files = None , zoom = None , bounds = None , single input file = None , mode = \"continue\" , debug = None ) : from flask import Flask , render template string app = Flask ( name ) mapchete processes = { os . path . splitext ( os . path . basename ( mapchete file ) ) [ 0 ] : mapchete . open ( mapchete file , zoom = zoom , bounds = bounds , single input file = single input file , mode = mode , with cache = True , debug = debug ) for mapchete file in mapchete files } mp = next ( iter ( mapchete processes . values ( ) ) ) pyramid type = mp . config . process pyramid . grid pyramid srid = mp . config . process pyramid . crs . to epsg ( ) process bounds = \",\" . join ( [ str ( i ) for i in mp . config . bounds at zoom ( ) ] ) grid = \"g\" if pyramid srid == 3857 else \"WGS84\" web pyramid = Buffered Tile Pyramid ( pyramid type ) @ app . route ( '/' , methods = [ 'GET' ] ) def index ( ) : \"\"\"Render and hosts the appropriate Open Layers instance.\"\"\" return render template string ( pkgutil . get data ( 'mapchete.static' , 'index.html' ) . decode ( \"utf-8\" ) , srid = pyramid srid , process bounds = process bounds , is mercator = ( pyramid srid == 3857 ) , process names = mapchete processes . keys ( ) ) @ app . route ( \"/\" . join ( [ \"\" , \"wmts simple\" , \"1.0.0\" , \"<string:mp name>\" , \"default\" , grid , \"<int:zoom>\" , \"<int:row>\" , \"<int:col>.<string:file ext>\" ] ) , methods = [ 'GET' ] ) def get ( mp name , zoom , row , col , file ext ) : \"\"\"Return processed, empty or error (in pink color) tile.\"\"\" logger . debug ( \"received tile (%s, %s, %s) for process %s\" , zoom , row , col , mp name ) return tile response ( mapchete processes [ mp name ] , web pyramid . tile ( zoom , row , col ) , debug ) return app", "predictions": ["create a list of web processes and zoom processes"], "references": ["configure and create flask app ."], "bleu": 0.15619699684601276, "rouge_l": 0.13832199546485258}
{"id": 1985, "code": "def get warped array ( input file = None , indexes = None , dst bounds = None , dst shape = None , dst crs = None , resampling = None , src nodata = None , dst nodata = None ) : try : return rasterio read ( input file = input file , indexes = indexes , dst bounds = dst bounds , dst shape = dst shape , dst crs = dst crs , resampling = resampling , src nodata = src nodata , dst nodata = dst nodata ) except Exception as e : logger . exception ( \"error while reading file %s: %s\" , input file , e ) raise", "predictions": ["helper function to get a warped array from a warped file ."], "references": ["extract a numpy array from a raster file ."], "bleu": 0.23901088824528133, "rouge_l": 0.5865384615384615}
{"id": 1986, "code": "def shift required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile pyramid . is global : tile cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) if tile cols == list ( range ( min ( tile cols ) , max ( tile cols ) + 1 ) ) : return False else : def gen groups ( items ) : \"\"\"Groups tile columns by sequence.\"\"\" j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : if i == j + 1 : group . append ( i ) else : yield group group = [ i ] j = i yield group groups = list ( gen groups ( tile cols ) ) if len ( groups ) == 1 : return False normal distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] antimeridian distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile pyramid . matrix width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] return antimeridian distance < normal distance else : return False", "predictions": ["shift the tile s distance to all tiles"], "references": ["determine if distance over antimeridian is shorter than normal distance ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 1987, "code": "def write json ( path , params ) : logger . debug ( \"write %s to %s\" , params , path ) if path . startswith ( \"s3://\" ) : bucket = get boto3 bucket ( path . split ( \"/\" ) [ 2 ] ) key = \"/\" . join ( path . split ( \"/\" ) [ 3 : ] ) logger . debug ( \"upload %s\" , key ) bucket . put object ( Key = key , Body = json . dumps ( params , sort keys = True , indent = 4 ) ) else : makedirs ( os . path . dirname ( path ) ) with open ( path , 'w' ) as dst : json . dump ( params , dst , sort keys = True , indent = 4 )", "predictions": ["write a json to a json file ."], "references": ["write local or remote ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 1988, "code": "def read json ( path ) : if path . startswith ( ( \"http://\" , \"https://\" ) ) : try : return json . loads ( urlopen ( path ) . read ( ) . decode ( ) ) except HTTP Error : raise File Not Found Error ( \"%s not found\" , path ) elif path . startswith ( \"s3://\" ) : bucket = get boto3 bucket ( path . split ( \"/\" ) [ 2 ] ) key = \"/\" . join ( path . split ( \"/\" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return json . loads ( obj . get ( ) [ 'Body' ] . read ( ) . decode ( ) ) raise File Not Found Error ( \"%s not found\" , path ) else : try : with open ( path , \"r\" ) as src : return json . loads ( src . read ( ) ) except : raise File Not Found Error ( \"%s not found\" , path )", "predictions": ["read a json file as json"], "references": ["read local or remote ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1989, "code": "def get digest ( self ) : return hmac . new ( self . secret , request . data , hashlib . sha1 ) . hexdigest ( ) if self . secret else None", "predictions": ["return the digest of the hmac ."], "references": ["return message digest if a secret key was provided"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 1990, "code": "def long description ( ) : import argparse parser = argparse . Argument Parser ( ) parser . add argument ( '--doc' , dest = \"doc\" , action = \"store true\" , default = False ) args , sys . argv = parser . parse known args ( sys . argv ) if args . doc : import doc2md , pypandoc md = doc2md . doc2md ( doc2md . doc , \"doc2md\" , toc = False ) long description = pypandoc . convert ( md , 'rst' , format = 'md' ) else : return None", "predictions": ["return a long long description ."], "references": ["generate . rst document for pypi ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1991, "code": "def find sections ( lines ) : sections = [ ] for line in lines : if is heading ( line ) : sections . append ( get heading ( line ) ) return sections", "predictions": ["find sections in a list of lines that have a given line ."], "references": ["find all section names and return a list with their names ."], "bleu": 0.14283632578659286, "rouge_l": 0.3223249669749009}
{"id": 1992, "code": "def make toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( \"    \" * ( ind - outer ) + \"- [%s](#%s)\" % ( sec , ref ) ) return refs", "predictions": ["build a list of sections from a list of sections"], "references": ["generate table of contents for array of section names ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 1993, "code": "def doc2md ( docstr , title , min level = 1 , more info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\\n' ) sections = find sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min level : shiftlevel = min level - level level = min level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is heading ( l ) ) , 0 ) md = [ make heading ( level , title ) , \"\" , ] + lines [ : head ] if toc : md += make toc ( sections , maxdepth ) md += [ '' ] md += doc2md ( lines [ head : ] , shiftlevel ) if more info : return ( md , sections ) else : return \"\\n\" . join ( md )", "predictions": ["generate an doc2md that will be used to generate an doc2md"], "references": ["convert a docstring to a markdown text ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1994, "code": "def mod2md ( module , title , title api section , toc = True , maxdepth = 0 ) : docstr = module . doc text = doctrim ( docstr ) lines = text . split ( '\\n' ) sections = find sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api md = [ ] api sec = [ ] if title api section and module . all : sections . append ( ( level + 1 , title api section ) ) for name in module . all : api sec . append ( ( level + 2 , \"`\" + name + \"`\" ) ) api md += [ '' , '' ] entry = module . dict [ name ] if entry . doc : md , sec = doc2md ( entry . doc , \"`\" + name + \"`\" , min level = level + 2 , more info = True , toc = False ) api sec += sec api md += md sections += api sec head = next ( ( i for i , l in enumerate ( lines ) if is heading ( l ) ) , 0 ) md = [ make heading ( level , title ) , \"\" , ] + lines [ : head ] if toc : md += make toc ( sections , maxdepth ) md += [ '' ] md += doc2md ( lines [ head : ] ) md += [ '' , '' , make heading ( level + 1 , title api section ) , ] if toc : md += [ '' ] md += make toc ( api sec , 1 ) md += api md return \"\\n\" . join ( md )", "predictions": ["generate a heading string from a module"], "references": ["generate markdown document from module including api section ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 1995, "code": "def get min visit time ( self ) : if not self . visit events : return float ( 'inf' ) else : return min ( self . visit events , key = lambda event : event . arr time ut ) . arr time ut", "predictions": ["return the time visit from the event ."], "references": ["get the earliest visit time of the stop ."], "bleu": 0.1862539773562041, "rouge_l": 0.465648854961832}
{"id": 1996, "code": "def can infect ( self , event ) : if event . from stop I != self . stop I : return False if not self . has been visited ( ) : return False else : time sep = event . dep time ut - self . get min visit time ( ) if ( time sep >= self . min transfer time ) or ( event . trip I == - 1 and time sep >= 0 ) : return True else : for visit in self . visit events : if ( event . trip I == visit . trip I ) and ( time sep >= 0 ) : return True return False", "predictions": ["check if the event can be expired ."], "references": ["whether the spreading stop can infect using this event ."], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 1997, "code": "def createcolorbar ( cmap , norm ) : cax , kw = matplotlib . colorbar . make axes ( matplotlib . pyplot . gca ( ) ) c = matplotlib . colorbar . Colorbar Base ( cax , cmap = cmap , norm = norm ) return c", "predictions": ["create a matplotlib color from a matplotlib colorbar ."], "references": ["create a colourbar with limits of lwr and upr"], "bleu": 0.18575057999133596, "rouge_l": 0.2222222222222222}
{"id": 1998, "code": "def scan footpaths to departure stop ( self , connection dep stop , connection dep time , arrival time target ) : for , neighbor , data in self . walk network . edges iter ( nbunch = [ connection dep stop ] , data = True ) : d walk = data [ 'd walk' ] neighbor dep time = connection dep time - d walk / self . walk speed pt = Label Time Simple ( departure time = neighbor dep time , arrival time target = arrival time target ) self . stop profiles [ neighbor ] . update pareto optimal tuples ( pt )", "predictions": ["scan for footpaths to departure to departure for departure ."], "references": ["a helper method for scanning the footpaths . updates self . _stop_profiles accordingly"], "bleu": 0.11105685174312292, "rouge_l": 0.25487465181058494}
{"id": 1999, "code": "def finalize profiles ( self ) : for stop , stop profile in self . stop profiles . items ( ) : assert ( isinstance ( stop profile , Node Profile Multi Objective ) ) neighbor label bags = [ ] walk durations to neighbors = [ ] departure arrival stop pairs = [ ] if stop profile . get walk to target duration ( ) != 0 and stop in self . walk network . node : neighbors = networkx . all neighbors ( self . walk network , stop ) for neighbor in neighbors : neighbor profile = self . stop profiles [ neighbor ] assert ( isinstance ( neighbor profile , Node Profile Multi Objective ) ) neighbor real connection labels = neighbor profile . get labels for real connections ( ) neighbor label bags . append ( neighbor real connection labels ) walk durations to neighbors . append ( int ( self . walk network . get edge data ( stop , neighbor ) [ \"d walk\" ] / self . walk speed ) ) departure arrival stop pairs . append ( ( stop , neighbor ) ) stop profile . finalize ( neighbor label bags , walk durations to neighbors , departure arrival stop pairs )", "predictions": ["run the profiles ."], "references": ["deal with the first walks by joining profiles to other stops within walking distance ."], "bleu": 0.02731554444032802, "rouge_l": 0.2859375}
{"id": 2000, "code": "def validate day start ut ( conn ) : G = GTFS ( conn ) cur = conn . execute ( 'SELECT date, day start ut FROM days' ) for date , day start ut in cur : #print date, day start ut assert day start ut == G . get day start ut ( date )", "predictions": ["validate that the day has the relevant date day"], "references": ["this validates the day_start_ut of the days table ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 2001, "code": "def main make views ( gtfs fname ) : print ( \"creating views\" ) conn = GTFS ( fname or conn = gtfs fname ) . conn for L in Loaders : L ( None ) . make views ( conn ) conn . commit ( )", "predictions": ["run the make make script"], "references": ["re - create all views ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2002, "code": "def run ( self ) : if self . has run : raise Runtime Error ( \"This spreader instance has already been run: \" \"create a new Spreader object for a new run.\" ) i = 1 while self . event heap . size ( ) > 0 and len ( self . uninfected stops ) > 0 : event = self . event heap . pop next event ( ) this stop = self . stop I to spreading stop [ event . from stop I ] if event . arr time ut > self . start time ut + self . max duration ut : break if this stop . can infect ( event ) : target stop = self . stop I to spreading stop [ event . to stop I ] already visited = target stop . has been visited ( ) target stop . visit ( event ) if not already visited : self . uninfected stops . remove ( event . to stop I ) print ( i , self . event heap . size ( ) ) transfer distances = self . gtfs . get straight line transfer distances ( event . to stop I ) self . event heap . add walk events to heap ( transfer distances , event , self . start time ut , self . walk speed , self . uninfected stops , self . max duration ut ) i += 1 self . has run = True", "predictions": ["run the interactive loop"], "references": ["run the actual simulation ."], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 2003, "code": "def insert data ( self , conn ) : cur = conn . cursor ( ) csv reader generators , prefixes = self . get csv reader generators ( ) for csv reader , prefix in zip ( csv reader generators , prefixes ) : try : row = next ( iter ( self . gen rows ( [ csv reader ] , [ prefix ] ) ) ) fields = row . keys ( ) except Stop Iteration : print ( \"Not importing %s into %s for %s\" % ( self . fname , self . table , prefix ) ) continue stmt = '''INSERT INTO %s (%s) VALUES (%s)''' % ( self . table , ( ', ' . join ( [ x for x in fields if x [ 0 ] != ' ' ] + self . extra keys ) ) , ( ', ' . join ( [ \":\" + x for x in fields if x [ 0 ] != ' ' ] + self . extra values ) ) ) if self . print progress : print ( 'Importing %s into %s for %s' % ( self . fname , self . table , prefix ) ) from itertools import chain rows = chain ( [ row ] , self . gen rows ( [ csv reader ] , [ prefix ] ) ) cur . executemany ( stmt , rows ) conn . commit ( )", "predictions": ["insert data into csv"], "references": ["load data from gtfs file into database"], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 2004, "code": "def get vehicle hours by type ( gtfs , route type ) : day = gtfs . get suitable date for daily extract ( ) query = ( \" SELECT * , SUM(end time ds - start time ds)/3600 as vehicle hours type\" \" FROM\" \" (SELECT * FROM day trips as q1\" \" INNER JOIN\" \" (SELECT route I, type FROM routes) as q2\" \" ON q1.route I = q2.route I\" \" WHERE type = {route type}\" \" AND date = '{day}')\" . format ( day = day , route type = route type ) ) df = gtfs . execute custom query pandas ( query ) return df [ 'vehicle hours type' ] . item ( )", "predictions": ["get vehicle hours for a given route"], "references": ["return the sum of vehicle hours in a particular day by route type ."], "bleu": 0.10218289380194193, "rouge_l": 0.35935198821796754}
{"id": 2005, "code": "def clean password ( self ) : value = self . cleaned data . get ( 'password' ) if value not in self . valid passwords : raise forms . Validation Error ( 'Incorrect password.' ) return value", "predictions": ["validate that the password is empty ."], "references": ["check that the password is valid ."], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 2006, "code": "def clean ( self ) : cleaned data = super ( Auth Form , self ) . clean ( ) user = self . get user ( ) if self . staff only and ( not user or not user . is staff ) : raise forms . Validation Error ( 'Sorry, only staff are allowed.' ) if self . superusers only and ( not user or not user . is superuser ) : raise forms . Validation Error ( 'Sorry, only superusers are allowed.' ) return cleaned data", "predictions": ["validate that the user can be allowed.' ."], "references": ["when receiving the filled out form check for valid access ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 2007, "code": "def get lockdown form ( form path ) : if not form path : raise Improperly Configured ( 'No LOCKDOWN FORM specified.' ) form path list = form path . split ( \".\" ) new module = \".\" . join ( form path list [ : - 1 ] ) attr = form path list [ - 1 ] try : mod = import module ( new module ) except ( Import Error , Value Error ) : raise Improperly Configured ( 'Module configured in LOCKDOWN FORM (%s) to' ' contain the form class couldn\\'t be ' 'found.' % new module ) try : form = getattr ( mod , attr ) except Attribute Error : raise Improperly Configured ( 'The module configured in LOCKDOWN FORM ' ' (%s) doesn\\'t define a \"%s\" form.' % ( new module , attr ) ) return form", "predictions": ["get the form form for the given form ."], "references": ["return a form class for a given string pointing to a lockdown form ."], "bleu": 0.12673978475914355, "rouge_l": 0.41838134430727025}
{"id": 2008, "code": "def redirect ( self , request ) : url = request . path querystring = request . GET . copy ( ) if self . logout key and self . logout key in request . GET : del querystring [ self . logout key ] if querystring : url = '%s?%s' % ( url , querystring . urlencode ( ) ) return Http Response Redirect ( url )", "predictions": ["redirect the user with the given request ."], "references": ["handle redirects properly ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2009, "code": "def get descriptor base path ( descriptor ) : if isinstance ( descriptor , six . string types ) : if os . path . exists ( descriptor ) : base path = os . path . dirname ( os . path . abspath ( descriptor ) ) else : base path = os . path . dirname ( descriptor ) else : base path = '.' return base path", "predictions": ["return the path to the base path"], "references": ["get descriptor base path if string or return none ."], "bleu": 0.17112717058426782, "rouge_l": 0.22803738317757008}
{"id": 2010, "code": "def is safe path ( path ) : contains windows var = lambda val : re . match ( r'%.+%' , val ) contains posix var = lambda val : re . match ( r'\\$.+' , val ) unsafeness conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains windows var ( path ) , contains posix var ( path ) , ] return not any ( unsafeness conditions )", "predictions": ["check if a path is a safe path ."], "references": ["check if path is safe and allowed ."], "bleu": 0.25406637407730737, "rouge_l": 0.7134502923976607}
{"id": 2011, "code": "def validate zip ( the zip ) : datapackage jsons = [ f for f in the zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage jsons ) != 1 : msg = 'Data Package must have only one \"datapackage.json\" (had {n})' raise exceptions . Data Package Exception ( msg . format ( n = len ( datapackage jsons ) ) )", "predictions": ["validate that all the jsons jsons are only only one zip ."], "references": ["validate zipped data package"], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 2012, "code": "def validate ( self ) : warnings . warn ( 'Property \"package.validate\" is deprecated.' , User Warning ) descriptor = self . to dict ( ) self . profile . validate ( descriptor )", "predictions": ["validate the model ."], "references": ["validate this data package ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 2013, "code": "def iter errors ( self ) : warnings . warn ( 'Property \"package.iter errors\" is deprecated.' , User Warning ) return self . profile . iter errors ( self . to dict ( ) )", "predictions": ["iterate over errors ."], "references": ["lazily yields each validationerror for the received data dict ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2014, "code": "def iter errors ( self , data ) : warnings . warn ( 'Property \"profile.iter errors\" is deprecated.' , User Warning ) for error in self . validator . iter errors ( data ) : yield error", "predictions": ["iterate over errors ."], "references": ["lazily yields each validationerror for the received data dict ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2015, "code": "def get responses windows ( self , timeout sec ) : timeout time sec = time . time ( ) + timeout sec responses = [ ] while True : try : self . gdb process . stdout . flush ( ) if PYTHON3 : raw output = self . gdb process . stdout . readline ( ) . replace ( b\"\\r\" , b\"\\n\" ) else : raw output = self . gdb process . stdout . read ( ) . replace ( b\"\\r\" , b\"\\n\" ) responses += self . get responses list ( raw output , \"stdout\" ) except IO Error : pass try : self . gdb process . stderr . flush ( ) if PYTHON3 : raw output = self . gdb process . stderr . readline ( ) . replace ( b\"\\r\" , b\"\\n\" ) else : raw output = self . gdb process . stderr . read ( ) . replace ( b\"\\r\" , b\"\\n\" ) responses += self . get responses list ( raw output , \"stderr\" ) except IO Error : pass if time . time ( ) > timeout time sec : break return responses", "predictions": ["get the responses from the gdb process ."], "references": ["get responses on windows . assume no support for select and use a while loop ."], "bleu": 0.07015765577419673, "rouge_l": 0.23582474226804123}
{"id": 2016, "code": "def get responses unix ( self , timeout sec ) : timeout time sec = time . time ( ) + timeout sec responses = [ ] while True : select timeout = timeout time sec - time . time ( ) if select timeout <= 0 : select timeout = 0 events , , = select . select ( self . read list , [ ] , [ ] , select timeout ) responses list = None try : for fileno in events : if fileno == self . stdout fileno : self . gdb process . stdout . flush ( ) raw output = self . gdb process . stdout . read ( ) stream = \"stdout\" elif fileno == self . stderr fileno : self . gdb process . stderr . flush ( ) raw output = self . gdb process . stderr . read ( ) stream = \"stderr\" else : raise Value Error ( \"Developer error. Got unexpected file number %d\" % fileno ) responses list = self . get responses list ( raw output , stream ) responses += responses list except IO Error : pass if timeout sec == 0 : break elif responses list and self . allow overwrite timeout times : timeout time sec = min ( time . time ( ) + self . time to check for additional output sec , timeout time sec , ) elif time . time ( ) > timeout time sec : break return responses", "predictions": ["retrieve the app 0 file file file file file file file file file file file file file file file file file file file file file app file file file file file"], "references": ["get responses on unix - like system . use select to wait for output ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2017, "code": "def get notify msg and payload ( result , stream ) : token = stream . advance past chars ( [ \"=\" , \"*\" ] ) token = int ( token ) if token != \"\" else None logger . debug ( \"%s\" , fmt green ( \"parsing message\" ) ) message = stream . advance past chars ( [ \",\" ] ) logger . debug ( \"parsed message\" ) logger . debug ( \"%s\" , fmt green ( message ) ) payload = parse dict ( stream ) return token , message . strip ( ) , payload", "predictions": ["parses a warped bounds and returns the message and input input input crs crs crs crs crs crs crs crs crs crs crs crs ."], "references": ["get notify message and payload dict"], "bleu": 0.06394766688900896, "rouge_l": 0.1450653983353151}
{"id": 2018, "code": "def get result msg and payload ( result , stream ) : groups = GDB MI RESULT RE . match ( result ) . groups ( ) token = int ( groups [ 0 ] ) if groups [ 0 ] != \"\" else None message = groups [ 1 ] if groups [ 2 ] is None : payload = None else : stream . advance past chars ( [ \",\" ] ) payload = parse dict ( stream ) return token , message , payload", "predictions": ["parses a json required required required to extract a message from a required required required required required required required stream ."], "references": ["get result message and payload dict"], "bleu": 0.05809665204409193, "rouge_l": 0.08232118758434548}
{"id": 2019, "code": "def cleanup ( self ) : if self . subscription : logger . info ( \"Deleting worker subscription...\" ) self . subscriber client . delete subscription ( self . subscription )", "predictions": ["write all worker data . to the pool ."], "references": ["deletes this worker s subscription ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2020, "code": "def enqueue ( self , f , * args , * * kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put task ( task ) return self . enqueue task ( task )", "predictions": ["read a task and read it into the queue"], "references": ["enqueues a function for the task queue to execute ."], "bleu": 0.15881076016027915, "rouge_l": 0.31282051282051276}
{"id": 2021, "code": "def service start ( service = None , param = None ) : if service is not None : to run = [ \"python\" , service ] if param is not None : to run += param return subprocess . Popen ( to run ) return False", "predictions": ["returns true if the get get false if it doesn t exist"], "references": ["launch a process return his pid"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 2022, "code": "def update running pids ( old procs ) : new procs = [ ] for proc in old procs : if proc . poll ( ) is None and check pid ( proc . pid ) : publisher . debug ( str ( proc . pid ) + ' is alive' ) new procs . append ( proc ) else : try : publisher . debug ( str ( proc . pid ) + ' is gone' ) os . kill ( proc . pid , signal . SIGKILL ) except : pass return new procs", "predictions": ["long pids pids pids dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest dest"], "references": ["update the list of the running process and return the list"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2023, "code": "def fsplit ( file to split ) : dirname = file to split + ' splitted' if not os . path . exists ( dirname ) : os . mkdir ( dirname ) part file size = os . path . getsize ( file to split ) / number of files + 1 splitted files = [ ] with open ( file to split , \"r\" ) as f : number = 0 actual = 0 while 1 : prec = actual f . seek ( part file size , os . SEEK CUR ) s = f . readline ( ) if len ( s ) == 0 : s = f . readline ( ) while len ( s ) != 0 and s != separator : s = f . readline ( ) actual = f . tell ( ) new file = os . path . join ( dirname , str ( number ) ) with open ( file to split , \"r\" ) as temp : temp . seek ( prec ) copy = temp . read ( actual - prec ) open ( new file , 'w' ) . write ( copy ) splitted files . append ( new file ) number += 1 if len ( s ) == 0 : break return splitted files", "predictions": ["get a part ."], "references": ["split the file and return the list of filenames ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2024, "code": "def already downloaded ( filename ) : cur file = os . path . join ( c . bview dir , filename ) old file = os . path . join ( c . bview dir , 'old' , filename ) if not os . path . exists ( cur file ) and not os . path . exists ( old file ) : return False return True", "predictions": ["check if a file make sure that the file make sure it s toc if it s not already toc"], "references": ["verify that the file has not already been downloaded ."], "bleu": 0.1352045976914347, "rouge_l": 0.3546511627906977}
{"id": 2025, "code": "def get page url ( page num , current app , url view name , url extra args , url extra kwargs , url param name , url get params , url anchor ) : if url view name is not None : url extra kwargs [ url param name ] = page num try : url = reverse ( url view name , args = url extra args , kwargs = url extra kwargs , current app = current app ) except No Reverse Match as e : if settings . SETTINGS MODULE : if django . VERSION < ( 1 , 9 , 0 ) : separator = '.' else : separator = ':' project name = settings . SETTINGS MODULE . split ( '.' ) [ 0 ] try : url = reverse ( project name + separator + url view name , args = url extra args , kwargs = url extra kwargs , current app = current app ) except No Reverse Match : raise e else : raise e else : url = '' url get params = url get params or Query Dict ( url ) url get params = url get params . copy ( ) url get params [ url param name ] = str ( page num ) if len ( url get params ) > 0 : if not isinstance ( url get params , Query Dict ) : tmp = Query Dict ( mutable = True ) tmp . update ( url get params ) url get params = tmp url += '?' + url get params . urlencode ( ) if ( url anchor is not None ) : url += '#' + url anchor return url", "predictions": ["build a docstr docstr docstr page . . . ."], "references": ["helper function to return a valid url string given the template tag parameters"], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 2026, "code": "def configure ci jobs ( config url , rosdistro name , ci build name , groovy script = None , dry run = False ) : config = get config index ( config url ) build files = get ci build files ( config , rosdistro name ) build file = build files [ ci build name ] index = get index ( config . rosdistro index url ) targets = [ ] for os name in build file . targets . keys ( ) : for os code name in build file . targets [ os name ] . keys ( ) : for arch in build file . targets [ os name ] [ os code name ] : targets . append ( ( os name , os code name , arch ) ) print ( 'The build file contains the following targets:' ) for os name , os code name , arch in targets : print ( '  -' , os name , os code name , arch ) dist file = get distribution file ( index , rosdistro name , build file ) if not dist file : print ( 'No distribution file matches the build file' ) return ci view name = get ci view name ( rosdistro name ) from ros buildfarm . jenkins import connect jenkins = connect ( config . jenkins url ) if groovy script is None else False view configs = { } views = { ci view name : configure ci view ( jenkins , ci view name , dry run = dry run ) } if not jenkins : view configs . update ( views ) groovy data = { 'dry run' : dry run , 'expected num views' : len ( view configs ) , } ci job names = [ ] job configs = Ordered Dict ( ) is disabled = False for os name , os code name , arch in targets : try : job name , job config = configure ci job ( config url , rosdistro name , ci build name , os name , os code name , arch , config = config , build file = build file , index = index , dist file = dist file , jenkins = jenkins , views = views , is disabled = is disabled , groovy script = groovy script , dry run = dry run , trigger timer = build file . jenkins job schedule ) ci job names . append ( job name ) if groovy script is not None : print ( \"Configuration for job '%s'\" % job name ) job configs [ job name ] = job config except Job Validation Error as e : print ( e . message , file = sys . stderr ) groovy data [ 'expected num jobs' ] = len ( job configs ) groovy data [ 'job prefixes and names' ] = { } if groovy script is not None : print ( \"Writing groovy script '%s' to reconfigure %d jobs\" % ( groovy script , len ( job configs ) ) ) content = expand template ( 'snippet/reconfigure jobs.groovy.em' , groovy data ) write groovy script and configs ( groovy script , content , job configs , view configs )", "predictions": ["configure module build level level level level level level level level level level level level level level"], "references": ["configure all jenkins ci jobs ."], "bleu": 0.07223943354597204, "rouge_l": 0.09516380655226209}
{"id": 2027, "code": "def obtain credentials ( self ) : protocol values = { 'SS Lv3' : Secur32Const . SP PROT SSL3 CLIENT , 'TL Sv1' : Secur32Const . SP PROT TLS1 CLIENT , 'TL Sv1.1' : Secur32Const . SP PROT TLS1 1 CLIENT , 'TL Sv1.2' : Secur32Const . SP PROT TLS1 2 CLIENT , } protocol bit mask = 0 for key , value in protocol values . items ( ) : if key in self . protocols : protocol bit mask |= value algs = [ Secur32Const . CALG AES 128 , Secur32Const . CALG AES 256 , Secur32Const . CALG 3DES , Secur32Const . CALG SHA1 , Secur32Const . CALG ECDHE , Secur32Const . CALG DH EPHEM , Secur32Const . CALG RSA KEYX , Secur32Const . CALG RSA SIGN , Secur32Const . CALG ECDSA , Secur32Const . CALG DSS SIGN , ] if 'TL Sv1.2' in self . protocols : algs . extend ( [ Secur32Const . CALG SHA512 , Secur32Const . CALG SHA384 , Secur32Const . CALG SHA256 , ] ) alg array = new ( secur32 , 'ALG ID[%s]' % len ( algs ) ) for index , alg in enumerate ( algs ) : alg array [ index ] = alg flags = Secur32Const . SCH USE STRONG CRYPTO | Secur32Const . SCH CRED NO DEFAULT CREDS if not self . manual validation and not self . extra trust roots : flags |= Secur32Const . SCH CRED AUTO CRED VALIDATION else : flags |= Secur32Const . SCH CRED MANUAL CRED VALIDATION schannel cred pointer = struct ( secur32 , 'SCHANNEL CRED' ) schannel cred = unwrap ( schannel cred pointer ) schannel cred . dw Version = Secur32Const . SCHANNEL CRED VERSION schannel cred . c Creds = 0 schannel cred . pa Cred = null ( ) schannel cred . h Root Store = null ( ) schannel cred . c Mappers = 0 schannel cred . aph Mappers = null ( ) schannel cred . c Supported Algs = len ( alg array ) schannel cred . palg Supported Algs = alg array schannel cred . grbit Enabled Protocols = protocol bit mask schannel cred . dw Minimum Cipher Strength = 0 schannel cred . dw Maximum Cipher Strength = 0 schannel cred . dw Session Lifespan = 0 schannel cred . dw Flags = flags schannel cred . dw Cred Format = 0 cred handle pointer = new ( secur32 , 'Cred Handle *' ) result = secur32 . Acquire Credentials Handle W ( null ( ) , Secur32Const . UNISP NAME , Secur32Const . SECPKG CRED OUTBOUND , null ( ) , schannel cred pointer , null ( ) , null ( ) , cred handle pointer , null ( ) ) handle error ( result ) self . credentials handle = cred handle pointer", "predictions": ["get basic basic min min and trust min float float float float float float float float float float float float float float float float float float float float float float float"], "references": ["obtains a credentials handle from secur32 . dll for use with schannel"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2028, "code": "def close ( self ) : try : self . shutdown ( ) finally : if self . socket : try : self . socket . close ( ) except ( socket . error ) : pass self . socket = None", "predictions": ["closes the from the from the from the from the from the from the from the from the from the from the from the from the from the from the from"], "references": ["shuts down the tls session and socket and forcibly closes it"], "bleu": 0.04317900023606586, "rouge_l": 0.05209222886421862}
{"id": 2029, "code": "def close ( self ) : try : self . shutdown ( ) finally : if self . socket : try : self . socket . close ( ) except ( socket . error ) : pass self . socket = None if self . connection id in socket refs : del socket refs [ self . connection id ]", "predictions": ["closes the matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib matplotlib task . and closes closes the"], "references": ["shuts down the tls session and socket and forcibly closes it"], "bleu": 0.046398855339878003, "rouge_l": 0.15627668659265584}
{"id": 2030, "code": "def version ( self ) : ver = Version ( ) ver . conn = self . conn ver . attrs = { 'service id' : self . attrs [ 'id' ] , } ver . save ( ) return ver", "predictions": ["walk the scan scan scan and walk for the connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection"], "references": ["create a new version under this service ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2031, "code": "def vcl ( self , name , content ) : vcl = VCL ( ) vcl . conn = self . conn vcl . attrs = { 'service id' : self . attrs [ 'service id' ] , 'version' : self . attrs [ 'number' ] , 'name' : name , 'content' : content , } vcl . save ( ) return vcl", "predictions": ["set up a finalize field"], "references": ["create a new vcl under this version ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2032, "code": "def patch ( self , route , data , headers = None , failure message = None ) : headers = self . get headers ( headers ) response lambda = ( lambda : requests . patch ( self . get qualified route ( route ) , headers = headers , data = data , verify = False , proxies = self . proxies ) ) response = check for rate limiting ( response lambda ( ) , response lambda ) return self . handle response ( response , failure message )", "predictions": ["wraps a validate request with a specific route"], "references": ["execute a patch request and return the result"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 2033, "code": "def add organization course ( organization data , course key ) : validate course key ( course key ) validate organization data ( organization data ) data . create organization course ( organization = organization data , course key = course key )", "predictions": ["main function for add_route make an make a make a make a make a selector"], "references": ["adds a organization - course link to the system"], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 2034, "code": "def remove organization course ( organization , course key ) : validate organization data ( organization ) validate course key ( course key ) return data . delete organization course ( course key = course key , organization = organization )", "predictions": ["run a specific organization on an organization"], "references": ["removes the specfied course from the specified organization"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2035, "code": "def course key is valid ( course key ) : if course key is None : return False try : Course Key . from string ( text type ( course key ) ) except ( Invalid Key Error , Unicode Decode Error ) : return False return True", "predictions": ["returns true if the passed insert data is a self generators"], "references": ["course key object validation"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2036, "code": "def inactivate organization ( organization ) : [ inactivate organization course relationship ( record ) for record in internal . Organization Course . objects . filter ( organization id = organization . id , active = True ) ] [ inactivate record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]", "predictions": ["look up day for a get request s day"], "references": ["inactivates an activated organization as well as any active relationships"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 2037, "code": "def activate organization course relationship ( relationship ) : relationship = internal . Organization Course . objects . get ( id = relationship . id , active = False , organization active = True ) activate record ( relationship )", "predictions": ["activates the password self cleaned cleaned cleaned cleaned cleaned cleaned cleaned cleaned cleaned cleaned cleaned cleaned ."], "references": ["activates an inactive organization - course relationship"], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 2038, "code": "def inactivate organization course relationship ( relationship ) : relationship = internal . Organization Course . objects . get ( id = relationship . id , active = True ) inactivate record ( relationship )", "predictions": ["return the relationship of all organization in the organization"], "references": ["inactivates an active organization - course relationship"], "bleu": 0.15619699684601276, "rouge_l": 0.1278825995807128}
{"id": 2039, "code": "def fetch organization courses ( organization ) : organization obj = serializers . deserialize organization ( organization ) queryset = internal . Organization Course . objects . filter ( organization = organization obj , active = True ) . select related ( 'organization' ) return [ serializers . serialize organization with course ( organization ) for organization in queryset ]", "predictions": ["get all organization for a given organization"], "references": ["retrieves the set of courses currently linked to the specified organization"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 2040, "code": "def fetch course organizations ( course key ) : queryset = internal . Organization Course . objects . filter ( course id = text type ( course key ) , active = True ) . select related ( 'organization' ) return [ serializers . serialize organization with course ( organization ) for organization in queryset ]", "predictions": ["redirect to a list of organization for a given course"], "references": ["retrieves the organizations linked to the specified course"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 2041, "code": "def serialize organization ( organization ) : return { 'id' : organization . id , 'name' : organization . name , 'short name' : organization . short name , 'description' : organization . description , 'logo' : organization . logo }", "predictions": ["return an descriptor . instance ."], "references": ["organization object - to - dict serialization"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2042, "code": "def deserialize organization ( organization dict ) : return models . Organization ( id = organization dict . get ( 'id' ) , name = organization dict . get ( 'name' , '' ) , short name = organization dict . get ( 'short name' , '' ) , description = organization dict . get ( 'description' , '' ) , logo = organization dict . get ( 'logo' , '' ) )", "predictions": ["is a dictionary of safe fields from a safe safe dict"], "references": ["organization dict - to - object serialization"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 2043, "code": "def get video ( self , node ) : video = Video ( ) video . embed code = self . get embed code ( node ) video . embed type = self . get embed type ( node ) video . width = self . get width ( node ) video . height = self . get height ( node ) video . src = self . get src ( node ) video . provider = self . get provider ( video . src ) return video", "predictions": ["create a zip zip object from a zip node"], "references": ["create a video object from a video embed"], "bleu": 0.31239399369202553, "rouge_l": 0.594541910331384}
{"id": 2044, "code": "def get siblings content ( self , current sibling , baselinescore siblings para ) : if current sibling . tag == 'p' and self . parser . get Text ( current sibling ) : tmp = current sibling if tmp . tail : tmp = deepcopy ( tmp ) tmp . tail = '' return [ tmp ] else : potential paragraphs = self . parser . get Elements By Tag ( current sibling , tag = 'p' ) if potential paragraphs is None : return None paragraphs = list ( ) for first paragraph in potential paragraphs : text = self . parser . get Text ( first paragraph ) if text : word stats = self . stopwords class ( language = self . get language ( ) ) . get stopword count ( text ) paragraph score = word stats . get stopword count ( ) sibling baseline score = float ( .30 ) high link density = self . is highlink density ( first paragraph ) score = float ( baselinescore siblings para * sibling baseline score ) if score < paragraph score and not high link density : para = self . parser . create Element ( tag = 'p' , text = text , tail = None ) paragraphs . append ( para ) return paragraphs", "predictions": ["collect all paragraphs paragraphs"], "references": ["adds any siblings that may have a decent score to this node"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 2045, "code": "def get canonical link ( self ) : if self . article . final url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . get Elements By Tag ( self . article . doc , * * kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . get Attribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final url", "predictions": ["returns the link to redirect to to the article"], "references": ["if the article has meta canonical link set in the url"], "bleu": 0.1689983564524028, "rouge_l": 0.2946859903381642}
{"id": 2046, "code": "def make list elms pretty ( self ) : for elm in self . parser . get Elements By Tag ( self . top node , tag = 'li' ) : elm . text = r'\u2022 {}'.f o rmat(e l m.t e xt)", "predictions": ["iter the errors in the errors"], "references": ["make any list element read like a list"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 2047, "code": "def crawl ( self , crawl candidate ) : def crawler wrapper ( parser , parsers lst , crawl candidate ) : try : crawler = Crawler ( self . config , self . fetcher ) article = crawler . crawl ( crawl candidate ) except ( Unicode Decode Error , Value Error ) as ex : if parsers lst : parser = parsers lst . pop ( 0 ) return crawler wrapper ( parser , parsers lst , crawl candidate ) else : raise ex return article parsers = list ( self . config . available parsers ) parsers . remove ( self . config . parser class ) return crawler wrapper ( self . config . parser class , parsers , crawl candidate )", "predictions": ["get a try to the try to run the try . . . . . . . . ."], "references": ["wrap the crawling functionality"], "bleu": 0.06439931429457924, "rouge_l": 0.09854604200323101}
{"id": 2048, "code": "def get urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin site . admin view ( self . handle upload ) , name = 'quill-file-upload' ) , ) return urls + super ( Quill Admin , self ) . get urls ( )", "predictions": ["returns the admin urls to provide to the view ."], "references": ["add urls needed to handle image uploads ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 2049, "code": "def handle upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is ajax ( ) : try : filename = request . GET [ 'quill Upload File' ] data = request is raw = True except Key Error : return Http Response Bad Request ( \"Invalid file upload.\" ) else : if len ( request . FILES ) != 1 : return Http Response Bad Request ( \"Can only upload 1 file at a time.\" ) try : data = request . FILES [ 'quill Upload File' ] filename = data . name is raw = False except Key Error : return Http Response Bad Request ( 'Missing image `quill Upload File`.' ) url = save file ( data , filename , is raw , default storage ) response data = { } response data [ 'url' ] = url return Http Response ( json . dumps ( response data ) , content type = \"text/html; charset=utf-8\" )", "predictions": ["process the upload file ."], "references": ["handle file uploads from wysiwyg ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2050, "code": "def render ( self , name , value , attrs = { } ) : if value is None : value = '' final attrs = self . build attrs ( attrs , name = name ) quill app = apps . get app config ( 'quill' ) quill config = getattr ( quill app , self . config ) return mark safe ( render to string ( quill config [ 'template' ] , { 'final attrs' : flatatt ( final attrs ) , 'value' : value , 'id' : final attrs [ 'id' ] , 'config' : self . config , } ) )", "predictions": ["render the widget ."], "references": ["render the quill wysiwyg ."], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 2051, "code": "def formfield ( self , * * kwargs ) : defaults = { 'form class' : Rich Text Form Field , 'config' : self . config , } defaults . update ( kwargs ) return super ( Rich Text Field , self ) . formfield ( * * defaults )", "predictions": ["gets the form for the form ."], "references": ["get the form for field ."], "bleu": 0.3655552228545123, "rouge_l": 0.6240409207161125}
{"id": 2052, "code": "def render toolbar ( context , config ) : quill config = getattr ( quill app , config ) t = template . loader . get template ( quill config [ 'toolbar template' ] ) return t . render ( context )", "predictions": ["render the toolbar for the given config ."], "references": ["render the toolbar for the given config ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 2053, "code": "def get meta image url ( request , image ) : rendition = image . get rendition ( filter = 'original' ) return request . build absolute uri ( rendition . url )", "predictions": ["return the meta image url for the given image ."], "references": ["resize an image for metadata tags and return an absolute url to it ."], "bleu": 0.1112086898712083, "rouge_l": 0.24270557029177717}
{"id": 2054, "code": "def read ( self , filename = None ) : self . init filename ( filename ) def BLANK ( i ) : return \"B{0:04d}\" . format ( i ) def COMMENT ( i ) : return \"C{0:04d}\" . format ( i ) data = odict ( ) iblank = icomment = 0 with open ( self . real filename ) as mdp : for line in mdp : line = line . strip ( ) if len ( line ) == 0 : iblank += 1 data [ BLANK ( iblank ) ] = '' continue m = self . COMMENT . match ( line ) if m : icomment += 1 data [ COMMENT ( icomment ) ] = m . group ( 'value' ) continue m = self . PARAMETER . match ( line ) if m : parameter = m . group ( 'parameter' ) value = self . transform ( m . group ( 'value' ) ) data [ parameter ] = value else : errmsg = '{filename!r}: unknown line in mdp file, {line!r}' . format ( * * vars ( ) ) self . logger . error ( errmsg ) raise Parse Error ( errmsg ) super ( MDP , self ) . update ( data )", "predictions": ["read the data from file ."], "references": ["read and parse mdp file * filename * ."], "bleu": 0.1593301391270729, "rouge_l": 0.3860759493670886}
{"id": 2055, "code": "def prehook ( self , * * kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( \"Starting smpd: \" + \" \" . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc", "predictions": ["run a subprocess command"], "references": ["launch local smpd ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 2056, "code": "def glob parts ( prefix , ext ) : if ext . startswith ( '.' ) : ext = ext [ 1 : ] files = glob . glob ( prefix + '.' + ext ) + glob . glob ( prefix + '.part[0-9][0-9][0-9][0-9].' + ext ) files . sort ( ) return files", "predictions": ["glob parts of a file prefix ."], "references": ["find files from a continuation run"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2057, "code": "def mdp include string ( dirs ) : include paths = [ os . path . expanduser ( p ) for p in dirs ] return ' -I' . join ( [ '' ] + include paths )", "predictions": ["returns a string with the mdp include paths to the include ."], "references": ["generate a string that can be added to a mdp include = line ."], "bleu": 0.1583226641033773, "rouge_l": 0.3793532338308458}
{"id": 2058, "code": "def parse groups ( output ) : groups = [ ] for line in output . split ( '\\n' ) : m = NDXGROUP . match ( line ) if m : d = m . groupdict ( ) groups . append ( { 'name' : d [ 'GROUPNAME' ] , 'nr' : int ( d [ 'GROUPNUMBER' ] ) , 'natoms' : int ( d [ 'NATOMS' ] ) } ) return groups", "predictions": ["parse groups from the output file"], "references": ["parse make_ndx output and return groups as a list of dicts ."], "bleu": 0.09663861439684919, "rouge_l": 0.20962199312714777}
{"id": 2059, "code": "def delete frames ( self ) : for frame in glob . glob ( self . frameglob ) : os . unlink ( frame )", "predictions": ["delete all frames from the world ."], "references": ["delete all frames ."], "bleu": 0.3655552228545123, "rouge_l": 0.7648902821316614}
{"id": 2060, "code": "def gmx resid ( self , resid ) : try : gmx resid = int ( self . offset [ resid ] ) except ( Type Error , Index Error ) : gmx resid = resid + self . offset except Key Error : raise Key Error ( \"offset must be a dict that contains the gmx resid for {0:d}\" . format ( resid ) ) return gmx resid", "predictions": ["return the number of gmx for a given resid ."], "references": ["returns resid in the gromacs index by transforming with offset ."], "bleu": 0.13564514503163538, "rouge_l": 0.18885448916408668}
{"id": 2061, "code": "def process command ( self , command , name = None ) : self . command counter += 1 if name is None : name = \"CMD{0:03d}\" . format ( self . command counter ) try : fd , tmp ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'tmp ' + name + ' ' ) cmd = [ command , '' , 'q' ] rc , out , err = self . make ndx ( o = tmp ndx , input = cmd ) self . check output ( out , \"No atoms found for selection {command!r}.\" . format ( * * vars ( ) ) , err = err ) ##print \"DEBUG:  process command()\" ##print out groups = parse ndxlist ( out ) last = groups [ - 1 ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + ' ' ) name cmd = [ \"keep {0:d}\" . format ( last [ 'nr' ] ) , \"name 0 {0!s}\" . format ( name ) , 'q' ] rc , out , err = self . make ndx ( n = tmp ndx , o = ndx , input = name cmd ) finally : utilities . unlink gmx ( tmp ndx ) return name , ndx", "predictions": ["processes a command from redis"], "references": ["process make_ndx command and return name and temp index file ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 2062, "code": "def translate residue ( self , selection , default atomname = 'CA' ) : m = self . RESIDUE . match ( selection ) if not m : errmsg = \"Selection {selection!r} is not valid.\" . format ( * * vars ( ) ) logger . error ( errmsg ) raise Value Error ( errmsg ) gmx resid = self . gmx resid ( int ( m . group ( 'resid' ) ) ) residue = m . group ( 'aa' ) if len ( residue ) == 1 : gmx resname = utilities . convert aa code ( residue ) else : gmx resname = residue gmx atomname = m . group ( 'atom' ) if gmx atomname is None : gmx atomname = default atomname return { 'resname' : gmx resname , 'resid' : gmx resid , 'atomname' : gmx atomname }", "predictions": ["translate residue selection ."], "references": ["translate selection for a single res to make_ndx syntax ."], "bleu": 0.09534061816653486, "rouge_l": 0.3978260869565217}
{"id": 2063, "code": "def check output ( self , make ndx output , message = None , err = None ) : if message is None : message = \"\" else : message = '\\n' + message def format ( output , w = 60 ) : hrule = \"====[ Gromacs Error (diagnostic output) ]\" . ljust ( w , \"=\" ) return hrule + '\\n' + str ( output ) + hrule rc = True if self . is empty group ( make ndx output ) : warnings . warn ( \"Selection produced empty group.{message!s}\" . format ( * * vars ( ) ) , category = Gromacs Value Warning ) rc = False if self . has syntax error ( make ndx output ) : rc = False out formatted = format ( make ndx output ) raise Gromacs Error ( \"make ndx encountered a Syntax Error, \" \"%(message)s\\noutput:\\n%(out formatted)s\" % vars ( ) ) if make ndx output . strip ( ) == \"\" : rc = False out formatted = format ( err ) raise Gromacs Error ( \"make ndx produced no output, \" \"%(message)s\\nerror output:\\n%(out formatted)s\" % vars ( ) ) return rc", "predictions": ["check that the output is correct ."], "references": ["simple tests to flag problems with a make_ndx run ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2064, "code": "def get template ( t ) : if os . path . exists ( t ) : pass else : t = t t found = False for d in path : p = os . path . join ( d , t ) if os . path . exists ( p ) : t = p t found = True break t = os . path . basename ( t ) if not t found : for p in templates . values ( ) : if t == os . path . basename ( p ) : t = p t found = True break if not t found : try : t = templates [ t ] except Key Error : pass else : t found = True if not t found : raise Value Error ( \"Failed to locate the template file {t!r}.\" . format ( * * vars ( ) ) ) return os . path . realpath ( t )", "predictions": ["get the template file name from a template file"], "references": ["return a single template * t * ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2065, "code": "def getpath ( self , section , option ) : return os . path . expanduser ( os . path . expandvars ( self . get ( section , option ) ) )", "predictions": ["get the value of a section ."], "references": ["return option as an expanded path ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2066, "code": "def canonicalize ( self , filename ) : path , ext = os . path . splitext ( filename ) if not ext : ext = \".collection\" return path + ext", "predictions": ["return a path to a file name ."], "references": ["use . collection as extension unless provided"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2067, "code": "def to int64 ( a ) : def promote i4 ( typestr ) : if typestr [ 1 : ] == 'i4' : typestr = typestr [ 0 ] + 'i8' return typestr dtype = [ ( name , promote i4 ( typestr ) ) for name , typestr in a . dtype . descr ] return a . astype ( dtype )", "predictions": ["convert a numpy array to a plain int64 ."], "references": ["return view of the recarray with all int32 cast to int64 ."], "bleu": 0.1430210741102858, "rouge_l": 0.2785388127853881}
{"id": 2068, "code": "def combine arglist ( self , args , kwargs ) : args = self . args + args kwargs = self . kwargs . copy ( ) kwargs . update ( kwargs ) return args , kwargs", "predictions": ["combine multiple arglist into a single file ."], "references": ["combine the default values and the supplied values ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 2069, "code": "def transform args ( self , * args , * * kwargs ) : options = [ ] for option , value in kwargs . items ( ) : if not option . startswith ( '-' ) : if len ( option ) == 1 : option = '-' + option else : option = '--' + option if value is True : options . append ( option ) continue elif value is False : raise Value Error ( 'A False value is ambiguous for option {0!r}' . format ( option ) ) if option [ : 2 ] == '--' : options . append ( option + '=' + str ( value ) ) else : options . extend ( ( option , str ( value ) ) ) return options + list ( args )", "predictions": ["convert arguments to common common options ."], "references": ["transform arguments and return them as a list suitable for popen ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 2070, "code": "def help ( self , long = False ) : print ( \"\\ncommand: {0!s}\\n\\n\" . format ( self . command name ) ) print ( self . doc ) if long : print ( \"\\ncall method: command():\\n\" ) print ( self . call . doc )", "predictions": ["show help for the help command ."], "references": ["print help ; same as using ? in ipython . long = true also gives call signature ."], "bleu": 0.04270613179243733, "rouge_l": 0.14823815309842042}
{"id": 2071, "code": "def combine arglist ( self , args , kwargs ) : gmxargs = self . gmxargs . copy ( ) gmxargs . update ( self . combineargs ( * args , * * kwargs ) ) return ( ) , gmxargs", "predictions": ["combine a set of arglist and return a new one ."], "references": ["combine the default values and the supplied values ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 2072, "code": "def combineargs ( self , * args , * * kwargs ) : d = { arg : True for arg in args } d . update ( kwargs ) return d", "predictions": ["filter the given arguments into a query string"], "references": ["add switches as options with value true to the options dict ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 2073, "code": "def build arg list ( self , * * kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : flag = str ( flag ) if flag . startswith ( ' ' ) : flag = flag [ 1 : ] if not flag . startswith ( '-' ) : flag = '-' + flag if value is True : arglist . append ( flag ) elif value is False : if flag . startswith ( '-no' ) : arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) elif value is None : pass else : try : arglist . extend ( [ flag ] + value ) except Type Error : arglist . extend ( [ flag , value ] ) return list ( map ( str , arglist ) )", "predictions": ["builds argument list from argument argument list"], "references": ["build list of arguments from the dict ; keys must be valid gromacs flags ."], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 2074, "code": "def run command ( self , * args , * * kwargs ) : result , p = super ( Gromacs Command , self ) . run command ( * args , * * kwargs ) self . check failure ( result , command string = p . command string ) return result , p", "predictions": ["run the command and return the output ."], "references": ["execute the gromacs command ; see the docs for __call__ ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 2075, "code": "def transform args ( self , * args , * * kwargs ) : newargs = self . combineargs ( * args , * * kwargs ) return self . build arg list ( * * newargs )", "predictions": ["returns a list of arguments to be passed to the build ."], "references": ["combine arguments and turn them into gromacs tool arguments ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 2076, "code": "def get stream ( filename , openfunction = open , mode = 'r' ) : try : stream = openfunction ( filename , mode = mode ) except ( IO Error , OS Error ) as err : if errno . errorcode [ err . errno ] in [ 'ENOENT' , 'EACCES' ] : six . reraise ( * sys . exc info ( ) ) return None if mode . startswith ( 'r' ) : try : stream . readline ( ) except IO Error : stream . close ( ) stream = None except : stream . close ( ) raise else : stream . close ( ) stream = openfunction ( filename , mode = mode ) return stream", "predictions": ["return a stream of the given filename ."], "references": ["return open stream if * filename * can be opened with * openfunction * or else none ."], "bleu": 0.057773536381664076, "rouge_l": 0.28773584905660377}
{"id": 2077, "code": "def convert aa code ( x ) : if len ( x ) == 1 : return amino acid codes [ x . upper ( ) ] elif len ( x ) == 3 : return inverse aa codes [ x . upper ( ) ] else : raise Value Error ( \"Can only convert 1-letter or 3-letter amino acid codes, \" \"not %r\" % x )", "predictions": ["convert x code to amino acid acid acid"], "references": ["converts between 3 - letter and 1 - letter amino acid codes ."], "bleu": 0.11296874775996037, "rouge_l": 0.18263473053892215}
{"id": 2078, "code": "def iterable ( obj ) : if isinstance ( obj , string types ) : return False if hasattr ( obj , 'next' ) : return True try : len ( obj ) except Type Error : return False return True", "predictions": ["determine if an object is an iterable ."], "references": ["returns true if * obj * can be iterated over and is * not * a string ."], "bleu": 0.05463883725539848, "rouge_l": 0.21580188679245282}
{"id": 2079, "code": "def unlink f ( path ) : try : os . unlink ( path ) except OS Error as err : if err . errno != errno . ENOENT : raise", "predictions": ["unlink a file or directory tree ."], "references": ["unlink path but do not complain if file does not exist ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 2080, "code": "def infix filename ( self , name , default , infix , ext = None ) : if name is None : p , oldext = os . path . splitext ( default ) if ext is None : ext = oldext if ext . startswith ( os . extsep ) : ext = ext [ 1 : ] name = self . filename ( p + infix , ext = ext ) return name", "predictions": ["return get a urls for a given file upload"], "references": ["unless * name * is provided insert * infix * before the extension * ext * of * default * ."], "bleu": 0.03128477649863482, "rouge_l": 0.0}
{"id": 2081, "code": "def stop logging ( ) : from . import log logger = logging . get Logger ( \"gromacs\" ) logger . info ( \"Gromacs Wrapper %s STOPPED logging\" , get version ( ) ) log . clear handlers ( logger )", "predictions": ["handle upload upload ."], "references": ["stop logging to logfile and console ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2082, "code": "def tool factory ( clsname , name , driver , base = Gromacs Command ) : clsdict = { 'command name' : name , 'driver' : driver , ' doc ' : property ( base . get gmx docs ) } return type ( clsname , ( base , ) , clsdict )", "predictions": ["create a render class for the given render class apps apps apps apps apps apps apps apps apps apps apps apps apps apps apps apps apps apps apps apps apps apps"], "references": ["factory for gromacscommand derived types ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 2083, "code": "def read ( self , filename = None ) : self . init filename ( filename ) data = odict ( ) with open ( self . real filename ) as ndx : current section = None for line in ndx : line = line . strip ( ) if len ( line ) == 0 : continue m = self . SECTION . match ( line ) if m : current section = m . group ( 'name' ) data [ current section ] = [ ] continue if current section is not None : data [ current section ] . extend ( map ( int , line . split ( ) ) ) super ( NDX , self ) . update ( odict ( [ ( name , self . transform ( atomnumbers ) ) for name , atomnumbers in data . items ( ) ] ) )", "predictions": ["formfield the . real file"], "references": ["read and parse index file * filename * ."], "bleu": 0.13575914775035755, "rouge_l": 0.1358574610244989}
{"id": 2084, "code": "def check mdpargs ( d ) : if len ( d ) > 0 : wmsg = \"Unprocessed mdp option are interpreted as options for grompp:\\n\" + str ( d ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = Usage Warning ) return len ( d ) == 0", "predictions": ["render the template checking if the template is valid . . . . . . . . . . . . . . . . . . . . . ."], "references": ["check if any arguments remain in dict * d * ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 2085, "code": "def is Mine ( self , scriptname ) : suffix = os . path . splitext ( scriptname ) [ 1 ] . lower ( ) if suffix . startswith ( '.' ) : suffix = suffix [ 1 : ] return self . suffix == suffix", "predictions": ["return is a meta - safe way for a given file ."], "references": ["primitive queuing system detection ; only looks at suffix at the moment ."], "bleu": 0.09559539481714499, "rouge_l": 0.07942708333333334}
{"id": 2086, "code": "def anumb to atom ( self , anumb ) : assert isinstance ( anumb , int ) , \"anumb must be integer\" if not self . anumb to atom : if self . atoms : for atom in self . atoms : self . anumb to atom [ atom . number ] = atom return self . anumb to atom [ anumb ] else : self . logger ( \"no atoms in the molecule\" ) return False else : if anumb in self . anumb to atom : return self . anumb to atom [ anumb ] else : self . logger ( \"no such atom number ({0:d}) in the molecule\" . format ( anumb ) ) return False", "predictions": ["checks if any of the hdf5 hdf5 hdf5 file exists . . ."], "references": ["returns the atom object corresponding to an atom number"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 2087, "code": "def total regular pixels from mask ( mask ) : total regular pixels = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : total regular pixels += 1 return total regular pixels", "predictions": ["compute the total of each * * * logger * ."], "references": ["compute the total number of unmasked regular pixels in a masks ."], "bleu": 0.21423488883339475, "rouge_l": 0.4314002828854314}
{"id": 2088, "code": "def mask circular from shape pixel scale and radius ( shape , pixel scale , radius arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres arcsec = mask centres from shape pixel scale and centre ( shape = mask . shape , pixel scale = pixel scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y arcsec = ( y - centres arcsec [ 0 ] ) * pixel scale x arcsec = ( x - centres arcsec [ 1 ] ) * pixel scale r arcsec = np . sqrt ( x arcsec ** 2 + y arcsec ** 2 ) if r arcsec <= radius arcsec : mask [ y , x ] = False return mask", "predictions": ["glob the glob from from from from from from from"], "references": ["compute a circular masks from an input masks radius and regular shape ."], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 2089, "code": "def mask circular annular from shape pixel scale and radii ( shape , pixel scale , inner radius arcsec , outer radius arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres arcsec = mask centres from shape pixel scale and centre ( shape = mask . shape , pixel scale = pixel scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y arcsec = ( y - centres arcsec [ 0 ] ) * pixel scale x arcsec = ( x - centres arcsec [ 1 ] ) * pixel scale r arcsec = np . sqrt ( x arcsec ** 2 + y arcsec ** 2 ) if outer radius arcsec >= r arcsec >= inner radius arcsec : mask [ y , x ] = False return mask", "predictions": ["mdp the mdp from from from from from from from from from from from from and = = = = lower lower lower and = = = = = = ="], "references": ["compute an annular masks from an input inner and outer masks radius and regular shape ."], "bleu": 0.046398855339878003, "rouge_l": 0.13545521835677277}
{"id": 2090, "code": "def mask elliptical from shape pixel scale and radius ( shape , pixel scale , major axis radius arcsec , axis ratio , phi , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres arcsec = mask centres from shape pixel scale and centre ( shape = mask . shape , pixel scale = pixel scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y arcsec = ( y - centres arcsec [ 0 ] ) * pixel scale x arcsec = ( x - centres arcsec [ 1 ] ) * pixel scale r arcsec elliptical = elliptical radius from y x phi and axis ratio ( y arcsec , x arcsec , phi , axis ratio ) if r arcsec elliptical <= major axis radius arcsec : mask [ y , x ] = False return mask", "predictions": ["parse a parse groups from from and radius from a output pixel append to a radius"], "references": ["compute a circular masks from an input masks radius and regular shape ."], "bleu": 0.09672649511413092, "rouge_l": 0.21082949308755758}
{"id": 2091, "code": "def mask elliptical annular from shape pixel scale and radius ( shape , pixel scale , inner major axis radius arcsec , inner axis ratio , inner phi , outer major axis radius arcsec , outer axis ratio , outer phi , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres arcsec = mask centres from shape pixel scale and centre ( shape = mask . shape , pixel scale = pixel scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y arcsec = ( y - centres arcsec [ 0 ] ) * pixel scale x arcsec = ( x - centres arcsec [ 1 ] ) * pixel scale inner r arcsec elliptical = elliptical radius from y x phi and axis ratio ( y arcsec , x arcsec , inner phi , inner axis ratio ) outer r arcsec elliptical = elliptical radius from y x phi and axis ratio ( y arcsec , x arcsec , outer phi , outer axis ratio ) if inner r arcsec elliptical >= inner major axis radius arcsec and outer r arcsec elliptical <= outer major axis radius arcsec : mask [ y , x ] = False return mask", "predictions": ["delete a delete self self self . self . self . and in a in a in a in a in a in a given in a in a in a"], "references": ["compute a circular masks from an input masks radius and regular shape ."], "bleu": 0.046398855339878003, "rouge_l": 0.09814963797264682}
{"id": 2092, "code": "def total edge pixels from mask ( mask ) : border pixel total = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : if mask [ y + 1 , x ] or mask [ y - 1 , x ] or mask [ y , x + 1 ] or mask [ y , x - 1 ] or mask [ y + 1 , x + 1 ] or mask [ y + 1 , x - 1 ] or mask [ y - 1 , x + 1 ] or mask [ y - 1 , x - 1 ] : border pixel total += 1 return border pixel total", "predictions": ["compute = total total self . self ."], "references": ["compute the total number of borders - pixels in a masks ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 2093, "code": "def total border pixels from mask and edge pixels ( mask , edge pixels , masked grid index to pixel ) : border pixel total = 0 for i in range ( edge pixels . shape [ 0 ] ) : if check if border pixel ( mask , edge pixels [ i ] , masked grid index to pixel ) : border pixel total += 1 return border pixel total", "predictions": ["compute the command pixels self self self name self . 12b"], "references": ["compute the total number of borders - pixels in a masks ."], "bleu": 0.15553014371537452, "rouge_l": 0.34512022630834516}
{"id": 2094, "code": "def grid stack from deflection stack ( grid stack , deflection stack ) : if deflection stack is not None : def minus ( grid , deflections ) : return grid - deflections return grid stack . map function ( minus , deflection stack )", "predictions": ["gets the residue residue from from the residue residue"], "references": ["for a deflection stack comput a new grid stack but subtracting the deflections"], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 2095, "code": "def regular to pix ( self ) : return mapper util . voronoi regular to pix from grids and geometry ( regular grid = self . grid stack . regular , regular to nearest pix = self . grid stack . pix . regular to nearest pix , pixel centres = self . geometry . pixel centres , pixel neighbors = self . geometry . pixel neighbors , pixel neighbors size = self . geometry . pixel neighbors size ) . astype ( 'int' )", "predictions": ["message for check if this + else a check is check for the given else err ."], "references": ["the 1d index mappings between the regular pixels and voronoi pixelization pixels ."], "bleu": 0.07994607499472013, "rouge_l": 0.1366181410974244}
{"id": 2096, "code": "def sub to pix ( self ) : return mapper util . voronoi sub to pix from grids and geometry ( sub grid = self . grid stack . sub , regular to nearest pix = self . grid stack . pix . regular to nearest pix , sub to regular = self . grid stack . sub . sub to regular , pixel centres = self . geometry . pixel centres , pixel neighbors = self . geometry . pixel neighbors , pixel neighbors size = self . geometry . pixel neighbors size ) . astype ( 'int' )", "predictions": ["convert get get to pix instance ."], "references": ["the 1d index mappings between the sub pixels and voronoi pixelization pixels ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 2097, "code": "def signal to noise map ( self ) : signal to noise map = np . divide ( self . image , self . noise map ) signal to noise map [ signal to noise map < 0 ] = 0 return signal to noise map", "predictions": ["return signal to to"], "references": ["the estimated signal - to - noise_maps mappers of the image ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 2098, "code": "def absolute signal to noise map ( self ) : return np . divide ( np . abs ( self . image ) , self . noise map )", "predictions": ["convert the canonicalize signal to the canonicalize filename"], "references": ["the estimated absolute_signal - to - noise_maps mappers of the image ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 2099, "code": "def simulate as gaussian ( cls , shape , pixel scale , sigma , centre = ( 0.0 , 0.0 ) , axis ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light profiles import Elliptical Gaussian gaussian = Elliptical Gaussian ( centre = centre , axis ratio = axis ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid 1d = grid util . regular grid 1d masked from mask pixel scales and origin ( mask = np . full ( shape , False ) , pixel scales = ( pixel scale , pixel scale ) ) gaussian 1d = gaussian . intensities from grid ( grid = grid 1d ) gaussian 2d = mapping util . map unmasked 1d array to 2d array from array 1d and shape ( array 1d = gaussian 1d , shape = shape ) return PSF ( array = gaussian 2d , pixel scale = pixel scale , renormalize = True )", "predictions": ["to to to to to to to to to to to gaussian"], "references": ["simulate the psf as an elliptical gaussian profile ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 2100, "code": "def new psf with renormalized array ( self ) : return PSF ( array = self , pixel scale = self . pixel scale , renormalize = True )", "predictions": ["creates a combine with with with the current update array"], "references": ["renormalize the psf such that its data_vector values sum to unity ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 2101, "code": "def map function ( self , func , * arg lists ) : return Grid Stack ( * [ func ( * args ) for args in zip ( self , * arg lists ) ] )", "predictions": ["transform a function over each element of the list of values ."], "references": ["map a function to all grid_stack in a grid - stack"], "bleu": 0.1367440667823257, "rouge_l": 0.17528735632183906}
{"id": 2102, "code": "def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )", "predictions": ["= 1 . ."], "references": ["compute the yticks labels of this grid used for plotting the y - axis ticks when visualizing a regular"], "bleu": 0.0071055938730635, "rouge_l": 0.0}
{"id": 2103, "code": "def xticks ( self ) : return np . linspace ( np . min ( self [ : , 1 ] ) , np . max ( self [ : , 1 ] ) , 4 )", "predictions": ["args of the image"], "references": ["compute the xticks labels of this grid used for plotting the x - axis ticks when visualizing a regular"], "bleu": 0.009351487442933324, "rouge_l": 0.15561224489795916}
{"id": 2104, "code": "def unmasked sparse to sparse ( self ) : return mapping util . unmasked sparse to sparse from mask and pixel centres ( mask = self . regular grid . mask , unmasked sparse grid pixel centres = self . unmasked sparse grid pixel centres , total sparse pixels = self . total sparse pixels ) . astype ( 'int' )", "predictions": ["return self = sparse"], "references": ["the 1d index mappings between the unmasked sparse - grid and masked sparse grid ."], "bleu": 0.022969543400575367, "rouge_l": 0.0953125}
{"id": 2105, "code": "def sparse to unmasked sparse ( self ) : return mapping util . sparse to unmasked sparse from mask and pixel centres ( total sparse pixels = self . total sparse pixels , mask = self . regular grid . mask , unmasked sparse grid pixel centres = self . unmasked sparse grid pixel centres ) . astype ( 'int' )", "predictions": ["kwargs arg arg to list of list of list of build in build build build indices"], "references": ["the 1d index mappings between the masked sparse - grid and unmasked sparse grid ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2106, "code": "def regular to sparse ( self ) : return mapping util . regular to sparse from sparse mappings ( regular to unmasked sparse = self . regular to unmasked sparse , unmasked sparse to sparse = self . unmasked sparse to sparse ) . astype ( 'int' )", "predictions": ["get a run representation of the run run command result result result result result result result ."], "references": ["the 1d index mappings between the regular - grid and masked sparse - grid ."], "bleu": 0.07994607499472013, "rouge_l": 0.12642487046632125}
{"id": 2107, "code": "def trace grid stack to next plane ( self ) : def minus ( grid , deflections ) : return grid - deflections return self . grid stack . map function ( minus , self . deflection stack )", "predictions": ["convert the stack to the next args"], "references": ["trace this plane s grid_stacks to the next plane using its deflection angles ."], "bleu": 0.12718356905542982, "rouge_l": 0.2695139911634757}
{"id": 2108, "code": "def trace to next plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )", "predictions": ["convert get filename to next . . . . . . . . . ."], "references": ["trace the positions to the next plane ."], "bleu": 0.09782375748961449, "rouge_l": 0.2760180995475113}
{"id": 2109, "code": "def contained in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory", "predictions": ["codes aa a file name aa a directory"], "references": ["test if a file is located within the given directory ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 2110, "code": "def build backend ( ) : backend path = os . environ . get ( 'PEP517 BACKEND PATH' ) if backend path : extra pathitems = backend path . split ( os . pathsep ) sys . path [ : 0 ] = extra pathitems ep = os . environ [ 'PEP517 BUILD BACKEND' ] mod path , , obj path = ep . partition ( ':' ) try : obj = import module ( mod path ) except Import Error : raise Backend Unavailable ( traceback . format exc ( ) ) if backend path : if not any ( contained in ( obj . file , path ) for path in extra pathitems ) : raise Backend Invalid ( \"Backend was not loaded from backend-path\" ) if obj path : for path part in obj path . split ( '.' ) : obj = getattr ( obj , path part ) return obj", "predictions": ["iterable - specific backend string"], "references": ["find and load the build backend"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2111, "code": "def build sdist ( sdist directory , config settings ) : backend = build backend ( ) try : return backend . build sdist ( sdist directory , config settings ) except getattr ( backend , 'Unsupported Operation' , Dummy Exception ) : raise Got Unsupported Operation ( traceback . format exc ( ) )", "predictions": ["unlink the f from the given f err err err err err err err err err err err and try to the f err err err"], "references": ["invoke the mandatory build_sdist hook ."], "bleu": 0.04668049023095243, "rouge_l": 0.07043879907621246}
{"id": 2112, "code": "def to JSON ( self ) : return { \"id\" : self . id , \"compile\" : self . compile , \"position\" : self . position , \"version\" : self . version }", "predictions": ["return a dictionary representation of the object ."], "references": ["get a json dict of the attributes of this object ."], "bleu": 0.1909027782642041, "rouge_l": 0.511744966442953}
{"id": 2113, "code": "def add model string ( self , model str , position = 1 , file id = None ) : if file id is None : file id = self . make unique id ( 'inlined input' ) ret data = self . file create ( File . from string ( model str , position , file id ) ) return ret data", "predictions": ["add a model to the file"], "references": ["add a kappa model given in a string to the project ."], "bleu": 0.1407517563566015, "rouge_l": 0.5240549828178694}
{"id": 2114, "code": "def add model file ( self , model fpath , position = 1 , file id = None ) : if file id is None : file id = self . make unique id ( 'file input' ) ret data = self . file create ( File . from file ( model fpath , position , file id ) ) return ret data", "predictions": ["add a model to the tpu"], "references": ["add a kappa model from a file at given path to the project ."], "bleu": 0.10085304044775832, "rouge_l": 0.4663608562691131}
{"id": 2115, "code": "def get is sim running ( self ) : sim info = self . simulation info ( ) try : progress info = sim info [ 'simulation info progress' ] ret = progress info [ 'simulation progress is running' ] except Key Error : ret = False return ret", "predictions": ["check if the simulation is running ."], "references": ["check if the current simulation is running ."], "bleu": 0.5993954153807813, "rouge_l": 0.9222462203023757}
{"id": 2116, "code": "def in out check ( self ) : devices = available devices ( ) if not self . in idx in devices : raise OS Error ( \"Input device is unavailable\" ) in check = devices [ self . in idx ] if not self . out idx in devices : raise OS Error ( \"Output device is unavailable\" ) out check = devices [ self . out idx ] if ( ( in check [ 'inputs' ] == 0 ) and ( out check [ 'outputs' ] == 0 ) ) : raise Standard Error ( 'Invalid input and output devices' ) elif ( in check [ 'inputs' ] == 0 ) : raise Value Error ( 'Selected input device has no inputs' ) elif ( out check [ 'outputs' ] == 0 ) : raise Value Error ( 'Selected output device has no outputs' ) return True", "predictions": ["check for device input device ."], "references": ["checks the input and output to see if they are valid"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 2117, "code": "def up ( self , x ) : y = self . M * ssd . upsample ( x , self . M ) y = signal . lfilter ( self . b , self . a , y ) return y", "predictions": ["evaluate the signal up to the given point ."], "references": ["upsample and filter the signal"], "bleu": 0.18575057999133596, "rouge_l": 0.3012345679012346}
{"id": 2118, "code": "def dn ( self , x ) : y = signal . lfilter ( self . b , self . a , x ) y = ssd . downsample ( y , self . M ) return y", "predictions": ["return a signal with a signal ."], "references": ["downsample and filter the signal"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2119, "code": "def up ( self , x , L change = 12 ) : y = L change * ssd . upsample ( x , L change ) y = signal . lfilter ( self . b , [ 1 ] , y ) return y", "predictions": ["see l { speed speed }"], "references": ["upsample and filter the signal"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2120, "code": "def dn ( self , x , M change = 12 ) : y = signal . lfilter ( self . b , [ 1 ] , x ) y = ssd . downsample ( y , M change ) return y", "predictions": ["return a signal with a signal ."], "references": ["downsample and filter the signal"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2121, "code": "def zplane ( self , auto scale = True , size = 2 , detect mult = True , tol = 0.001 ) : ssd . zplane ( self . b , [ 1 ] , auto scale , size , tol )", "predictions": ["set the color of the image ."], "references": ["plot the poles and zeros of the fir filter in the z - plane"], "bleu": 0.09663861439684919, "rouge_l": 0.2695139911634757}
{"id": 2122, "code": "def filter ( self , x ) : y = signal . sosfilt ( self . sos , x ) return y", "predictions": ["filter a point on the given coordinates"], "references": ["filter the signal using second - order sections"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2123, "code": "def up ( self , x , L change = 12 ) : y = L change * ssd . upsample ( x , L change ) y = signal . sosfilt ( self . sos , y ) return y", "predictions": ["returns the signal with the given point ."], "references": ["upsample and filter the signal"], "bleu": 0.21105340631872638, "rouge_l": 0.32105263157894737}
{"id": 2124, "code": "def dn ( self , x , M change = 12 ) : y = signal . sosfilt ( self . sos , x ) y = ssd . downsample ( y , M change ) return y", "predictions": ["returns a signal instance for the given point ."], "references": ["downsample and filter the signal"], "bleu": 0.15619699684601276, "rouge_l": 0.1506172839506173}
{"id": 2125, "code": "def zplane ( self , auto scale = True , size = 2 , detect mult = True , tol = 0.001 ) : iir d . sos zplane ( self . sos , auto scale , size , tol )", "predictions": ["set the sos ."], "references": ["plot the poles and zeros of the fir filter in the z - plane"], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 2126, "code": "def is already running ( self ) : redis key = self . CELERY LOCK . format ( task id = self . task identifier ) return self . celery self . backend . client . exists ( redis key )", "predictions": ["checks if a task is running ."], "references": ["return true if lock exists and has not timed out ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2127, "code": "def reset lock ( self ) : redis key = self . CELERY LOCK . format ( task id = self . task identifier ) self . celery self . backend . client . delete ( redis key )", "predictions": ["delete the lock ."], "references": ["removed the lock regardless of timeout ."], "bleu": 0.24002491458061356, "rouge_l": 0.5198863636363635}
{"id": 2128, "code": "def is already running ( self ) : date done = ( self . restore group ( self . task identifier ) or dict ( ) ) . get ( 'date done' ) if not date done : return False difference = datetime . utcnow ( ) - date done return difference < timedelta ( seconds = self . timeout )", "predictions": ["return true if the task is already running ."], "references": ["return true if lock exists and has not timed out ."], "bleu": 0.22241434515868952, "rouge_l": 0.3929146537842191}
{"id": 2129, "code": "def reduce chunk ( func , array ) : res = [ ] for slice in iter chunk slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : res . append ( func ( array [ ... , slice ] ) ) return func ( res )", "predictions": ["reduce a chunk by a given array"], "references": ["reduce with func chunk by chunk the passed pytable array ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 2130, "code": "def merge DA ph times ( ph times d , ph times a ) : ph times = np . hstack ( [ ph times d , ph times a ] ) a em = np . hstack ( [ np . zeros ( ph times d . size , dtype = np . bool ) , np . ones ( ph times a . size , dtype = np . bool ) ] ) index sort = ph times . argsort ( ) return ph times [ index sort ] , a em [ index sort ]", "predictions": ["merge da times that are da in da"], "references": ["returns a merged timestamp array for donor + accept . and bool mask for a ."], "bleu": 0.04960895415008605, "rouge_l": 0.0}
{"id": 2131, "code": "def load PSF Lab file ( fname ) : if os . path . exists ( fname ) or os . path . exists ( fname + '.mat' ) : return loadmat ( fname ) [ 'data' ] else : raise IO Error ( \"Can't find PSF file '%s'\" % fname )", "predictions": ["load a file from the specified fname"], "references": ["load the array data in the . mat file fname ."], "bleu": 0.1319006407505858, "rouge_l": 0.32049036777583184}
{"id": 2132, "code": "def hash ( self ) : hash list = [ ] for key , value in sorted ( self . dict . items ( ) ) : if not callable ( value ) : if isinstance ( value , np . ndarray ) : hash list . append ( value . tostring ( ) ) else : hash list . append ( str ( value ) ) return hashlib . md5 ( repr ( hash list ) . encode ( ) ) . hexdigest ( )", "predictions": ["generates a hash for the image"], "references": ["return an hash string computed on the psf data ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 2133, "code": "def git path valid ( git path = None ) : if git path is None and GIT PATH is None : return False if git path is None : git path = GIT PATH try : call ( [ git path , '--version' ] ) return True except OS Error : return False", "predictions": ["validate a git path as a valid git path ."], "references": ["check whether the git executable is found ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 2134, "code": "def get git version ( git path = None ) : if git path is None : git path = GIT PATH git version = check output ( [ git path , \"--version\" ] ) . split ( ) [ 2 ] return git version", "predictions": ["get the git version from the git repository ."], "references": ["get the git version ."], "bleu": 0.4111336169005197, "rouge_l": 0.7530864197530864}
{"id": 2135, "code": "def check clean status ( git path = None ) : output = get status ( git path ) is unmodified = ( len ( output . strip ( ) ) == 0 ) return is unmodified", "predictions": ["checks if the status of the git unmodified is valid"], "references": ["returns whether there are uncommitted changes in the working dir ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 2136, "code": "def get last commit line ( git path = None ) : if git path is None : git path = GIT PATH output = check output ( [ git path , \"log\" , \"--pretty=format:'%ad %h %s'\" , \"--date=short\" , \"-n1\" ] ) return output . strip ( ) [ 1 : - 1 ]", "predictions": ["get the last commit line"], "references": ["get one - line description of head commit for repository in current dir ."], "bleu": 0.053667245469253895, "rouge_l": 0.19395866454689983}
{"id": 2137, "code": "def get last commit ( git path = None ) : if git path is None : git path = GIT PATH line = get last commit line ( git path ) revision id = line . split ( ) [ 1 ] return revision id", "predictions": ["get the last commit id"], "references": ["get the head commit sha1 of repository in current dir ."], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 2138, "code": "def print summary ( string = 'Repository' , git path = None ) : if git path is None : git path = GIT PATH if not git path valid ( ) : print ( '\\n%s revision unknown (git not found).' % string ) else : last commit = get last commit line ( ) print ( '\\n{} revision:\\n {}\\n' . format ( string , last commit ) ) if not check clean status ( ) : print ( '\\n WARNING -> Uncommitted changes:' ) print ( get status ( ) )", "predictions": ["print a summary of the current revision ."], "references": ["print the last commit line and eventual uncommitted changes ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 2139, "code": "def get bromo fnames da ( d em k Hz , d bg k Hz , a em k Hz , a bg k Hz , ID = '1+2+3+4+5+6' , t tot = '480' , num p = '30' , p M = '64' , t step = 0.5e-6 , D = 1.2e-11 , dir = '' ) : clk p = t step / 32. E sim = 1. * a em k Hz / ( a em k Hz + d em k Hz ) FRET val = 100. * E sim print ( \"Simulated FRET value: %.1f%%\" % FRET val ) d em k Hz str = \"%04d\" % d em k Hz a em k Hz str = \"%04d\" % a em k Hz d bg k Hz str = \"%04.1f\" % d bg k Hz a bg k Hz str = \"%04.1f\" % a bg k Hz print ( \"D: EM %s BG %s \" % ( d em k Hz str , d bg k Hz str ) ) print ( \"A: EM %s BG %s \" % ( a em k Hz str , a bg k Hz str ) ) fname d = ( 'ph times {t tot}s D{D} {np}P {p M}p M ' 'step{ts us}us ID{ID} EM{em}k Hz BG{bg}k Hz.npy' ) . format ( em = d em k Hz str , bg = d bg k Hz str , t tot = t tot , p M = p M , np = num p , ID = ID , ts us = t step * 1e6 , D = D ) fname a = ( 'ph times {t tot}s D{D} {np}P {p M}p M ' 'step{ts us}us ID{ID} EM{em}k Hz BG{bg}k Hz.npy' ) . format ( em = a em k Hz str , bg = a bg k Hz str , t tot = t tot , p M = p M , np = num p , ID = ID , ts us = t step * 1e6 , D = D ) print ( fname d ) print ( fname a ) name = ( 'Bro Sim E{:.1f} d BG{:.1f}k a BG{:.1f}k ' 'd EM{:.0f}k' ) . format ( FRET val , d bg k Hz , a bg k Hz , d em k Hz ) return dir + fname d , dir + fname a , name , clk p , E sim", "predictions": ["print a good good vector for printing to input data in input dictionary"], "references": ["get filenames for donor and acceptor timestamps for the given parameters"], "bleu": 0.09552040806823771, "rouge_l": 0.08460471567267684}
{"id": 2140, "code": "def volume ( self ) : return ( self . x2 - self . x1 ) * ( self . y2 - self . y1 ) * ( self . z2 - self . z1 )", "predictions": ["volume of the camera"], "references": ["box volume in m^3 ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 2141, "code": "def generate ( num particles , D , box , rs ) : X0 = rs . rand ( num particles ) * ( box . x2 - box . x1 ) + box . x1 Y0 = rs . rand ( num particles ) * ( box . y2 - box . y1 ) + box . y1 Z0 = rs . rand ( num particles ) * ( box . z2 - box . z1 ) + box . z1 return [ Particle ( D = D , x0 = x0 , y0 = y0 , z0 = z0 ) for x0 , y0 , z0 in zip ( X0 , Y0 , Z0 ) ]", "predictions": ["generate a random particles for a list of particles ."], "references": ["generate a list of particle objects ."], "bleu": 0.27901593935858265, "rouge_l": 0.6075697211155379}
{"id": 2142, "code": "def add ( self , num particles , D ) : self . plist += self . generate ( num particles , D , box = self . box , rs = self . rs )", "predictions": ["add a plist to the buffer"], "references": ["add particles with diffusion coefficient d at random positions ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2143, "code": "def datafile from hash ( hash , prefix , path ) : pattern = '%s %s*.h*' % ( prefix , hash ) datafiles = list ( path . glob ( pattern ) ) if len ( datafiles ) == 0 : raise No Match Error ( 'No matches for \"%s\"' % pattern ) if len ( datafiles ) > 1 : raise Multiple Matches Error ( 'More than 1 match for \"%s\"' % pattern ) return datafiles [ 0 ]", "predictions": ["return a list of datafile names matching a hash hash"], "references": ["return pathlib . path for a data - file with given hash and prefix ."], "bleu": 0.0909256598621168, "rouge_l": 0.23164556962025318}
{"id": 2144, "code": "def compact name ( self , hashsize = 6 ) : s = self . compact name core ( hashsize , t max = True ) s += \" ID%d-%d\" % ( self . ID , self . EID ) return s", "predictions": ["return a to make a to a to our to make it a to to the to make it a to the to the to make a to the to to"], "references": ["compact representation of all simulation parameters"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2145, "code": "def print sizes ( self ) : float size = 4 MB = 1024 * 1024 size = self . n samples * float size em size = size * self . num particles / MB pos size = 3 * size * self . num particles / MB print ( \"  Number of particles:\" , self . num particles ) print ( \"  Number of time steps:\" , self . n samples ) print ( \"  Emission array - 1 particle (float32): %.1f MB\" % ( size / MB ) ) print ( \"  Emission array (float32): %.1f MB\" % em size ) print ( \"  Position array (float32): %.1f MB \" % pos size )", "predictions": ["add model model model model to model"], "references": ["print on - disk array sizes required for current set of parameters ."], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 2146, "code": "def em rates from E DA mix ( em rates tot , E values ) : em rates d , em rates a = [ ] , [ ] for em rate tot , E value in zip ( em rates tot , E values ) : em rate di , em rate ai = em rates from E DA ( em rate tot , E value ) em rates d . append ( em rate di ) em rates a . append ( em rate ai ) return em rates d , em rates a", "predictions": ["return a list of edges that can be included in a e ."], "references": ["d and a emission rates for two populations ."], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 2147, "code": "def populations diff coeff ( particles , populations ) : D counts = particles . diffusion coeff counts if len ( D counts ) == 1 : pop sizes = [ pop . stop - pop . start for pop in populations ] assert D counts [ 0 ] [ 1 ] >= sum ( pop sizes ) D counts = [ ( D counts [ 0 ] [ 0 ] , ps ) for ps in pop sizes ] D list = [ ] D pop start = 0 for pop , ( D , counts ) in zip ( populations , D counts ) : D list . append ( D ) assert pop . start >= D pop start assert pop . stop <= D pop start + counts D pop start += counts return D list", "predictions": ["calculate the get difference between two circles"], "references": ["diffusion coefficients of the two specified populations ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2148, "code": "def populations slices ( particles , num pop list ) : slices = [ ] i prev = 0 for num pop in num pop list : slices . append ( slice ( i prev , i prev + num pop ) ) i prev += num pop return slices", "predictions": ["given a devices and a devices return the out of the out of the out of the out"], "references": ["2 - tuple of slices for selection of two populations ."], "bleu": 0.07535838128770536, "rouge_l": 0.14420803782505912}
{"id": 2149, "code": "def calc hash da ( self , rs ) : self . hash d = hash ( rs . get state ( ) ) [ : 6 ] self . hash a = self . hash d", "predictions": ["calculates the hash of the hash ."], "references": ["compute hash of d and a timestamps for single - step d + a case ."], "bleu": 0.0726217243534906, "rouge_l": 0.2436750998668442}
{"id": 2150, "code": "def run ( self , rs , overwrite = True , skip existing = False , path = None , chunksize = None ) : if path is None : path = str ( self . S . store . filepath . parent ) kwargs = dict ( rs = rs , overwrite = overwrite , path = path , timeslice = self . timeslice , skip existing = skip existing ) if chunksize is not None : kwargs [ 'chunksize' ] = chunksize header = ' - Mixture Simulation:' self . hash d = hash ( rs . get state ( ) ) [ : 6 ] print ( '%s Donor timestamps -    %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate timestamps mix ( populations = self . populations , max rates = self . em rates d , bg rate = self . bg rate d , * * kwargs ) ts d , = self . S . get timestamps part ( self . name timestamps d ) rs . set state ( ts d . attrs [ 'last random state' ] ) self . hash a = hash ( rs . get state ( ) ) [ : 6 ] print ( '\\n%s Acceptor timestamps - %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate timestamps mix ( populations = self . populations , max rates = self . em rates a , bg rate = self . bg rate a , * * kwargs ) print ( '\\n%s Completed. %s' % ( header , ctime ( ) ) , flush = True )", "predictions": ["dn a file with x y - axis ."], "references": ["compute timestamps for current populations ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2151, "code": "def run da ( self , rs , overwrite = True , skip existing = False , path = None , chunksize = None ) : if path is None : path = str ( self . S . store . filepath . parent ) kwargs = dict ( rs = rs , overwrite = overwrite , path = path , timeslice = self . timeslice , skip existing = skip existing ) if chunksize is not None : kwargs [ 'chunksize' ] = chunksize header = ' - Mixture Simulation:' self . calc hash da ( rs ) print ( '%s Donor + Acceptor timestamps - %s' % ( header , ctime ( ) ) , flush = True ) self . S . simulate timestamps mix da ( max rates d = self . em rates d , max rates a = self . em rates a , populations = self . populations , bg rate d = self . bg rate d , bg rate a = self . bg rate a , * * kwargs ) print ( '\\n%s Completed. %s' % ( header , ctime ( ) ) , flush = True )", "predictions": ["up a file with rs and change its associated hash"], "references": ["compute timestamps for current populations ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2152, "code": "def merge da ( self ) : print ( ' - Merging D and A timestamps' , flush = True ) ts d , ts par d = self . S . get timestamps part ( self . name timestamps d ) ts a , ts par a = self . S . get timestamps part ( self . name timestamps a ) ts , a ch , part = merge da ( ts d , ts par d , ts a , ts par a ) assert a ch . sum ( ) == ts a . shape [ 0 ] assert ( ~ a ch ) . sum ( ) == ts d . shape [ 0 ] assert a ch . size == ts a . shape [ 0 ] + ts d . shape [ 0 ] self . ts , self . a ch , self . part = ts , a ch , part self . clk p = ts d . attrs [ 'clk p' ]", "predictions": ["dn timestamps of two particle"], "references": ["merge donor and acceptor timestamps computes ts a_ch part ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2153, "code": "def save photon hdf5 ( self , identity = None , overwrite = True , path = None ) : filepath = self . filepath if path is not None : filepath = Path ( path , filepath . name ) self . merge da ( ) data = self . make photon hdf5 ( identity = identity ) phc . hdf5 . save photon hdf5 ( data , h5 fname = str ( filepath ) , overwrite = overwrite )", "predictions": ["save the photon of the self . ."], "references": ["create a smfret photon - hdf5 file with current timestamps ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 2154, "code": "def load gltf ( self ) : with open ( self . path ) as fd : self . meta = GLTF Meta ( self . path , json . load ( fd ) )", "predictions": ["filter the sos sos sos into a dictionary"], "references": ["loads a gltf json file"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2155, "code": "def load glb ( self ) : with open ( self . path , 'rb' ) as fd : magic = fd . read ( 4 ) if magic != GLTF MAGIC HEADER : raise Value Error ( \"{} has incorrect header {} != {}\" . format ( self . path , magic , GLTF MAGIC HEADER ) ) version = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] if version != 2 : raise Value Error ( \"{} has unsupported version {}\" . format ( self . path , version ) ) = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk 0 length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk 0 type = fd . read ( 4 ) if chunk 0 type != b'JSON' : raise Value Error ( \"Expected JSON chunk, not {} in file {}\" . format ( chunk 0 type , self . path ) ) json meta = fd . read ( chunk 0 length ) . decode ( ) chunk 1 length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk 1 type = fd . read ( 4 ) if chunk 1 type != b'BIN\\x00' : raise Value Error ( \"Expected BIN chunk, not {} in file {}\" . format ( chunk 1 type , self . path ) ) self . meta = GLTF Meta ( self . path , json . loads ( json meta ) , binary buffer = fd . read ( chunk 1 length ) )", "predictions": ["reads the glb from the database"], "references": ["loads a binary gltf file"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2156, "code": "def buffers exist ( self ) : for buff in self . buffers : if not buff . is separate file : continue path = self . path . parent / buff . uri if not os . path . exists ( path ) : raise File Not Found Error ( \"Buffer {} referenced in {} not found\" . format ( path , self . path ) )", "predictions": ["check if the dn exist exist exist exist exist is already dn ."], "references": ["checks if the bin files referenced exist"], "bleu": 0.1350862565735141, "rouge_l": 0.31715771230502604}
{"id": 2157, "code": "def prepare attrib mapping ( self , primitive ) : buffer info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBO Info ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer info and buffer info [ - 1 ] . buffer view == info . buffer view : if buffer info [ - 1 ] . interleaves ( info ) : buffer info [ - 1 ] . merge ( info ) continue buffer info . append ( info ) return buffer info", "predictions": ["converts the 2 - d list of 2 - d objects into a list of 2 - d objects"], "references": ["pre - parse buffer mappings for each vbo to detect interleaved data for a primitive"], "bleu": 0.0712695567709093, "rouge_l": 0.12019704433497538}
{"id": 2158, "code": "def get bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max", "predictions": ["format a already existing already - already - already - already - already - bit value"], "references": ["get the bounding box for the mesh"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2159, "code": "def interleaves ( self , info ) : return info . byte offset == self . component type . size * self . components", "predictions": ["convert a format to a private key"], "references": ["does the buffer interleave with this one?"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2160, "code": "def update yaw and pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )", "predictions": ["is the already - is already - is the already - pitch or pitch or pitch or pitch or pitch"], "references": ["updates the camera vectors based on the current yaw and pitch"], "bleu": 0.07264339766175722, "rouge_l": 0.20424107142857142}
{"id": 2161, "code": "def translate string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . meta . characters - 1 - self . ct [ char ]", "predictions": ["reduce a chunk of array into a list of integers ."], "references": ["translate string into character texture positions"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2162, "code": "def draw bbox ( self , projection matrix = None , camera matrix = None , all = True ) : projection matrix = projection matrix . astype ( 'f4' ) . tobytes ( ) camera matrix = camera matrix . astype ( 'f4' ) . tobytes ( ) self . bbox program [ \"m proj\" ] . write ( projection matrix ) self . bbox program [ \"m view\" ] . write ( self . view matrix . astype ( 'f4' ) . tobytes ( ) ) self . bbox program [ \"m cam\" ] . write ( camera matrix ) self . bbox program [ \"bb min\" ] . write ( self . bbox min . astype ( 'f4' ) . tobytes ( ) ) self . bbox program [ \"bb max\" ] . write ( self . bbox max . astype ( 'f4' ) . tobytes ( ) ) self . bbox program [ \"color\" ] . value = ( 1.0 , 0.0 , 0.0 ) self . bbox vao . render ( self . bbox program ) if not all : return for node in self . root nodes : node . draw bbox ( projection matrix , camera matrix , self . bbox program , self . bbox vao )", "predictions": ["merge the camera bbox to the camera"], "references": ["draw scene and mesh bounding boxes"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2163, "code": "def apply mesh programs ( self , mesh programs = None ) : if not mesh programs : mesh programs = [ Color Program ( ) , Texture Program ( ) , Fallback Program ( ) ] for mesh in self . meshes : for mp in mesh programs : instance = mp . apply ( mesh ) if instance is not None : if isinstance ( instance , Mesh Program ) : mesh . mesh program = mp break else : raise Value Error ( \"apply() must return a Mesh Program instance, not {}\" . format ( type ( instance ) ) ) if not mesh . mesh program : print ( \"WARING: No mesh program applied to '{}'\" . format ( mesh . name ) )", "predictions": ["applies the else else . to the mesh program"], "references": ["applies mesh programs to meshes"], "bleu": 0.16784459625186196, "rouge_l": 0.3012345679012346}
{"id": 2164, "code": "def get time ( self ) -> float : if self . paused : return self . pause time return mixer . music . get pos ( ) / 1000.0", "predictions": ["hash the current time"], "references": ["get the current position in the music in seconds"], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 2165, "code": "def render lights debug ( self , camera matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend func = moderngl . SRC ALPHA , moderngl . ONE MINUS SRC ALPHA for light in self . point lights : m mv = matrix44 . multiply ( light . matrix , camera matrix ) light size = light . radius self . debug shader [ \"m proj\" ] . write ( projection . tobytes ( ) ) self . debug shader [ \"m mv\" ] . write ( m mv . astype ( 'f4' ) . tobytes ( ) ) self . debug shader [ \"size\" ] . value = light size self . unit cube . render ( self . debug shader , mode = moderngl . LINE STRIP ) self . ctx . disable ( moderngl . BLEND )", "predictions": ["git git valid valid valid valid valid valid valid if not already there ."], "references": ["render outlines of light volumes"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 2166, "code": "def combine ( self ) : self . gbuffer . color attachments [ 0 ] . use ( location = 0 ) self . combine shader [ \"diffuse buffer\" ] . value = 0 self . lightbuffer . color attachments [ 0 ] . use ( location = 1 ) self . combine shader [ \"light buffer\" ] . value = 1 self . quad . render ( self . combine shader )", "predictions": ["get the attachments of the shader"], "references": ["combine diffuse and light buffer"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2167, "code": "def load shader ( self , shader type : str , path : str ) : if path : resolved path = self . find program ( path ) if not resolved path : raise Value Error ( \"Cannot find {} shader '{}'\" . format ( shader type , path ) ) print ( \"Loading:\" , path ) with open ( resolved path , 'r' ) as fd : return fd . read ( )", "predictions": ["check if clean is a clean . clean file and returns a clean instance"], "references": ["load a single shader"], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 2168, "code": "def load ( self ) : self . open image ( ) width , height , depth = self . image . size [ 0 ] , self . image . size [ 1 ] // self . layers , self . layers components , data = image data ( self . image ) texture = self . ctx . texture array ( ( width , height , depth ) , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build mipmaps ( ) self . close image ( ) return texture", "predictions": ["get the = = texture"], "references": ["load a texture array"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2169, "code": "def available templates ( value ) : templates = list templates ( ) if value not in templates : raise Argument Type Error ( \"Effect template '{}' does not exist.\\n Available templates: {} \" . format ( value , \", \" . join ( templates ) ) ) return value", "predictions": ["validate that the value is get the get id ."], "references": ["scan for available templates in effect_templates"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2170, "code": "def root path ( ) : module dir = os . path . dirname ( globals ( ) [ ' file ' ] ) return os . path . dirname ( os . path . dirname ( module dir ) )", "predictions": ["returns the summary of the current module . ."], "references": ["get the absolute path to the root of the demosys package"], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 2171, "code": "def load ( self ) : self . meta . resolved path = self . find data ( self . meta . path ) if not self . meta . resolved path : raise Improperly Configured ( \"Data file '{}' not found\" . format ( self . meta . path ) ) print ( \"Loading:\" , self . meta . path ) with open ( self . meta . resolved path , 'r' ) as fd : return fd . read ( )", "predictions": ["get k em data from file"], "references": ["load a file in text mode"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2172, "code": "def calc global bbox ( self , view matrix , bbox min , bbox max ) : if self . matrix is not None : view matrix = matrix44 . multiply ( self . matrix , view matrix ) if self . mesh : bbox min , bbox max = self . mesh . calc global bbox ( view matrix , bbox min , bbox max ) for child in self . children : bbox min , bbox max = child . calc global bbox ( view matrix , bbox min , bbox max ) return bbox min , bbox max", "predictions": ["calculates the global of the global x2"], "references": ["recursive calculation of scene bbox"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2173, "code": "def swap buffers ( self ) : self . frames += 1 glfw . swap buffers ( self . window ) self . poll events ( )", "predictions": ["generate buffers and generate buffers box box box box box box box box box box box"], "references": ["swaps buffers incement the framecounter and pull events ."], "bleu": 0.08513012360883544, "rouge_l": 0.16850828729281767}
{"id": 2174, "code": "def resize ( self , width , height ) : self . width = width self . height = height self . buffer width , self . buffer height = glfw . get framebuffer size ( self . window ) self . set default viewport ( )", "predictions": ["sets the viewport size in the plist"], "references": ["sets the new size and buffer size internally"], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 2175, "code": "def check glfw version ( self ) : print ( \"glfw version: {} (python wrapper version {})\" . format ( glfw . get version ( ) , glfw . version ) ) if glfw . get version ( ) < self . min glfw version : raise Value Error ( \"Please update glfw binaries to version {} or later\" . format ( self . min glfw version ) )", "predictions": ["datafile to ensure that the from from from from from from from from from from from from from from from from from from from from from from from from from from"], "references": ["ensure glfw library version is compatible"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 2176, "code": "def translate buffer format ( vertex format ) : buffer format = [ ] attributes = [ ] mesh attributes = [ ] if \"T2F\" in vertex format : buffer format . append ( \"2f\" ) attributes . append ( \"in uv\" ) mesh attributes . append ( ( \"TEXCOORD 0\" , \"in uv\" , 2 ) ) if \"C3F\" in vertex format : buffer format . append ( \"3f\" ) attributes . append ( \"in color\" ) mesh attributes . append ( ( \"NORMAL\" , \"in color\" , 3 ) ) if \"N3F\" in vertex format : buffer format . append ( \"3f\" ) attributes . append ( \"in normal\" ) mesh attributes . append ( ( \"NORMAL\" , \"in normal\" , 3 ) ) buffer format . append ( \"3f\" ) attributes . append ( \"in position\" ) mesh attributes . append ( ( \"POSITION\" , \"in position\" , 3 ) ) return \" \" . join ( buffer format ) , attributes , mesh attributes", "predictions": ["translate a buffer format to a buffer for a given vertex ."], "references": ["translate the buffer format"], "bleu": 0.14694106251955755, "rouge_l": 0.4121621621621622}
{"id": 2177, "code": "def resolve loader ( self , meta : Scene Description ) : for loader cls in self . loaders : if loader cls . supports file ( meta ) : meta . loader cls = loader cls break else : raise Improperly Configured ( \"Scene {} has no loader class registered. Check settings.SCENE LOADERS\" . format ( meta . path ) )", "predictions": ["resolve the meta - meta - file loader ."], "references": ["resolve scene loader based on file extension"], "bleu": 0.16784459625186196, "rouge_l": 0.2557651991614256}
{"id": 2178, "code": "def on resize ( self , width , height ) : self . width , self . height = width , height self . buffer width , self . buffer height = width , height self . resize ( width , height )", "predictions": ["resize the new size in the canvas ."], "references": ["pyglet specific callback for window resize events ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 2179, "code": "def swap buffers ( self ) : if not self . window . context : return self . frames += 1 self . window . flip ( ) self . window . dispatch events ( )", "predictions": ["swap all buffers in the window ."], "references": ["swap buffers increment frame counter and pull events"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2180, "code": "def load pool ( self ) : for meta in self . resources : resource = self . load ( meta ) yield meta , resource self . resources = [ ]", "predictions": ["load and parse all available pool objects ."], "references": ["loads all the data files using the configured finders ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 2181, "code": "def resize ( self , width , height ) : if not self . fbo : return self . width = width // self . widget . device Pixel Ratio ( ) self . height = height // self . widget . device Pixel Ratio ( ) self . buffer width = width self . buffer height = height super ( ) . resize ( width , height )", "predictions": ["resize the widget on the widget ."], "references": ["pyqt specific resize callback ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 2182, "code": "def init texture2d draw ( self ) : if not Texture Helper . quad : Texture Helper . quad = geometry . quad fs ( ) Texture Helper . texture2d shader = context . ctx ( ) . program ( vertex shader = , fragment shader = ) Texture Helper . texture2d sampler = self . ctx . sampler ( filter = ( moderngl . LINEAR , moderngl . LINEAR ) , )", "predictions": ["init texture2d to fs"], "references": ["initialize geometry and shader for drawing fbo layers"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 2183, "code": "def init depth texture draw ( self ) : from demosys import geometry if not Texture Helper . quad : Texture Helper . quad = geometry . quad fs ( ) Texture Helper . depth shader = context . ctx ( ) . program ( vertex shader = , fragment shader = ) Texture Helper . depth sampler = self . ctx . sampler ( filter = ( moderngl . LINEAR , moderngl . LINEAR ) , compare func = '' , )", "predictions": ["initialize depth - texture program ."], "references": ["initialize geometry and shader for drawing fbo layers"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2184, "code": "def clear ( self ) : self . ctx . fbo . clear ( red = self . clear color [ 0 ] , green = self . clear color [ 1 ] , blue = self . clear color [ 2 ] , alpha = self . clear color [ 3 ] , depth = self . clear depth , )", "predictions": ["clears the fbo ."], "references": ["clear the window buffer"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2185, "code": "def supports file ( cls , meta ) : path = Path ( meta . path ) for ext in cls . file extensions : if path . suffixes [ : len ( ext ) ] == ext : return True return False", "predictions": ["check if the file supports a file meta file ."], "references": ["check if the loader has a supported file extension"], "bleu": 0.25965358893403384, "rouge_l": 0.5313588850174217}
{"id": 2186, "code": "def add program dir ( self , directory ) : dirs = list ( self . PROGRAM DIRS ) dirs . append ( directory ) self . PROGRAM DIRS = dirs", "predictions": ["add a program to a directory"], "references": ["hack in program directory"], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 2187, "code": "def add texture dir ( self , directory ) : dirs = list ( self . TEXTURE DIRS ) dirs . append ( directory ) self . TEXTURE DIRS = dirs", "predictions": ["add a texture to the directory"], "references": ["hack in texture directory"], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 2188, "code": "def add data dir ( self , directory ) : dirs = list ( self . DATA DIRS ) dirs . append ( directory ) self . DATA DIRS = dirs", "predictions": ["add data directory to a directory"], "references": ["hack in a data directory"], "bleu": 0.31239399369202553, "rouge_l": 0.3696969696969697}
{"id": 2189, "code": "def content ( self , attributes : List [ str ] ) : formats = [ ] attrs = [ ] for attrib format , attrib in zip ( self . attrib formats , self . attributes ) : if attrib not in attributes : formats . append ( attrib format . pad str ( ) ) continue formats . append ( attrib format . format ) attrs . append ( attrib ) attributes . remove ( attrib ) if not attrs : return None return ( self . buffer , \"{}{}\" . format ( \" \" . join ( formats ) , '/i' if self . per instance else '' ) , * attrs )", "predictions": ["serialize the content into a string suitable for the xml response ."], "references": ["build content tuple for the buffer"], "bleu": 0.14694106251955755, "rouge_l": 0.3546511627906977}
{"id": 2190, "code": "def get dirs ( self ) -> List [ str ] : for package in self . packages : yield os . path . join ( package . path , 'resources' )", "predictions": ["return list of all packages ."], "references": ["get all effect directories for registered effects ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 2191, "code": "def runnable effects ( self ) -> List [ Type [ Effect ] ] : return [ cls for cls in self . effect classes if cls . runnable ]", "predictions": ["the list of runnable effects classes ."], "references": ["returns the runnable effect in the package"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2192, "code": "def load package ( self ) : try : self . package = importlib . import module ( self . name ) except Module Not Found Error : raise Module Not Found Error ( \"Effect package '{}' not found.\" . format ( self . name ) )", "predictions": ["load package from module ."], "references": ["find the effect package"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2193, "code": "def load effects classes ( self ) : self . effect classes = [ ] for , cls in inspect . getmembers ( self . effect module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect classes . append ( cls ) self . effect class map [ cls . name ] = cls cls . name = \"{}.{}\" . format ( self . effect module name , cls . name )", "predictions": ["load all effect classes into the effect"], "references": ["iterate the module attributes picking out effects"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2194, "code": "def load resource module ( self ) : try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies module = importlib . import module ( name ) except Module Not Found Error as err : raise Effect Error ( ( \"Effect package '{}' has no 'dependencies' module or the module has errors. \" \"Forwarded error from importlib: {}\" ) . format ( self . name , err ) ) try : self . resources = getattr ( self . dependencies module , 'resources' ) except Attribute Error : raise Effect Error ( \"Effect dependencies module '{}' has no 'resources' attribute\" . format ( name ) ) if not isinstance ( self . resources , list ) : raise Effect Error ( \"Effect dependencies module '{}': 'resources' is of type {} instead of a list\" . format ( name , type ( self . resources ) ) ) try : self . effect packages = getattr ( self . dependencies module , 'effect packages' ) except Attribute Error : raise Effect Error ( \"Effect dependencies module '{}' has 'effect packages' attribute\" . format ( name ) ) if not isinstance ( self . effect packages , list ) : raise Effect Error ( \"Effect dependencies module '{}': 'effect packages' is of type {} instead of a list\" . format ( name , type ( self . effects ) ) )", "predictions": ["load the effect module ."], "references": ["fetch the resource list"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2195, "code": "def load ( self ) : self . open image ( ) components , data = image data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build mipmaps ( ) self . close image ( ) return texture", "predictions": ["load image from image"], "references": ["load a 2d texture"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2196, "code": "def from single ( cls , meta : Program Description , source : str ) : instance = cls ( meta ) instance . vertex source = Shader Source ( VERTEX SHADER , meta . path or meta . vertex shader , source ) if GEOMETRY SHADER in source : instance . geometry source = Shader Source ( GEOMETRY SHADER , meta . path or meta . geometry shader , source , ) if FRAGMENT SHADER in source : instance . fragment source = Shader Source ( FRAGMENT SHADER , meta . path or meta . fragment shader , source , ) if TESS CONTROL SHADER in source : instance . tess control source = Shader Source ( TESS CONTROL SHADER , meta . path or meta . tess control shader , source , ) if TESS EVALUATION SHADER in source : instance . tess evaluation source = Shader Source ( TESS EVALUATION SHADER , meta . path or meta . tess evaluation shader , source , ) return instance", "predictions": ["create a vertex instance from a single vertex source file ."], "references": ["initialize a single glsl string containing all shaders"], "bleu": 0.14991106946711685, "rouge_l": 0.216696269982238}
{"id": 2197, "code": "def from separate ( cls , meta : Program Description , vertex source , geometry source = None , fragment source = None , tess control source = None , tess evaluation source = None ) : instance = cls ( meta ) instance . vertex source = Shader Source ( VERTEX SHADER , meta . path or meta . vertex shader , vertex source , ) if geometry source : instance . geometry source = Shader Source ( GEOMETRY SHADER , meta . path or meta . geometry shader , geometry source , ) if fragment source : instance . fragment source = Shader Source ( FRAGMENT SHADER , meta . path or meta . fragment shader , fragment source , ) if tess control source : instance . tess control source = Shader Source ( TESS CONTROL SHADER , meta . path or meta . tess control shader , tess control source , ) if tess evaluation source : instance . tess evaluation source = Shader Source ( TESS EVALUATION SHADER , meta . path or meta . tess control shader , tess evaluation source , ) return instance", "predictions": ["create a instance from a separate vertex and a separate vertex ."], "references": ["initialize multiple shader strings"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 2198, "code": "def print ( self ) : print ( \"---[ START {} ]---\" . format ( self . name ) ) for i , line in enumerate ( self . lines ) : print ( \"{}: {}\" . format ( str ( i ) . zfill ( 3 ) , line ) ) print ( \"---[ END {} ]---\" . format ( self . name ) )", "predictions": ["prints the class data"], "references": ["print the shader lines"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2199, "code": "def load ( self ) : self . create effect classes ( ) self . add resource descriptions to pools ( self . create external resources ( ) ) self . add resource descriptions to pools ( self . create resources ( ) ) for meta , resource in resources . textures . load pool ( ) : self . textures [ meta . label ] = resource for meta , resource in resources . programs . load pool ( ) : self . programs [ meta . label ] = resource for meta , resource in resources . scenes . load pool ( ) : self . scenes [ meta . label ] = resource for meta , resource in resources . data . load pool ( ) : self . data [ meta . label ] = resource self . create effect instances ( ) self . post load ( )", "predictions": ["create the programs for each resource"], "references": ["loads this project instance"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2200, "code": "def reload programs ( self ) : print ( \"Reloading programs:\" ) for name , program in self . programs . items ( ) : if getattr ( program , 'program' , None ) : print ( \" - {}\" . format ( program . meta . label ) ) program . program = resources . programs . load ( program . meta )", "predictions": ["reload the program s programs"], "references": ["reload all shader programs with the reloadable flag set"], "bleu": 0.1458826981425239, "rouge_l": 0.2717149220489978}
{"id": 2201, "code": "def image data ( image ) : data = image . tobytes ( ) components = len ( data ) // ( image . size [ 0 ] * image . size [ 1 ] ) return components , data", "predictions": ["get the image data"], "references": ["get components and bytes for an image"], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 2202, "code": "def find last of ( self , path , finders ) : found path = None for finder in finders : result = finder . find ( path ) if result : found path = result return found path", "predictions": ["find the last of a given path ."], "references": ["find the last occurance of the file in finders"], "bleu": 0.2785146580372046, "rouge_l": 0.465648854961832}
{"id": 2203, "code": "def initial sanity check ( self ) : self . try import ( self . project name ) self . validate name ( self . project name ) if os . path . exists ( self . project name ) : print ( \"Directory {} already exist. Aborting.\" . format ( self . project name ) ) return False if os . path . exists ( 'manage.py' ) : print ( \"A manage.py file already exist in the current directory. Aborting.\" ) return False return True", "predictions": ["check if the project is initial ."], "references": ["checks if we can create the project"], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 2204, "code": "def create entrypoint ( self ) : with open ( os . path . join ( self . template dir , 'manage.py' ) , 'r' ) as fd : data = fd . read ( ) . format ( project name = self . project name ) with open ( 'manage.py' , 'w' ) as fd : fd . write ( data ) os . chmod ( 'manage.py' , 0o777 )", "predictions": ["create the entrypoint ."], "references": ["write manage . py in the current directory"], "bleu": 0.14628187563941414, "rouge_l": 0.15721649484536082}
{"id": 2205, "code": "def get template dir ( self ) : directory = os . path . dirname ( os . path . abspath ( file ) ) directory = os . path . dirname ( os . path . dirname ( directory ) ) directory = os . path . join ( directory , 'project template' ) return directory", "predictions": ["return the path where all profiles of the directories are stored ."], "references": ["returns the absolute path to template directory"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 2206, "code": "def usage function ( parser ) : parser . print usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . doc . strip ( ) . splitlines ( ) [ 0 ] print ( '    %-12s %s' % ( function + ':' , doc ) ) return 0", "predictions": ["prints a usage function for the current parser ."], "references": ["show usage and available curve functions ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 2207, "code": "def usage palette ( parser ) : parser . print usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( '    %-12s' % ( palette , ) ) return 0", "predictions": ["print a usage message"], "references": ["show usage and available palettes ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2208, "code": "def size ( self ) : for fd in range ( 3 ) : cr = self . ioctl GWINSZ ( fd ) if cr : break if not cr : try : fd = os . open ( os . ctermid ( ) , os . O RDONLY ) cr = self . ioctl GWINSZ ( fd ) os . close ( fd ) except Exception : pass if not cr : env = os . environ cr = ( env . get ( 'LINES' , 25 ) , env . get ( 'COLUMNS' , 80 ) ) return int ( cr [ 1 ] ) , int ( cr [ 0 ] )", "predictions": ["return up the widget translate"], "references": ["get the current terminal size ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2209, "code": "def csi ( self , capname , * args ) : value = curses . tigetstr ( capname ) if value is None : return b'' else : return curses . tparm ( value , * args )", "predictions": ["take a cls and turn it into a cls in cls in cls in cls in cls in cls in cls in cls in cls in cls in cls in cls"], "references": ["return the escape sequence for the selected control sequence ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2210, "code": "def csi wrap ( self , value , capname , * args ) : if isinstance ( value , str ) : value = value . encode ( 'utf-8' ) return b'' . join ( [ self . csi ( capname , * args ) , value , self . csi ( 'sgr0' ) , ] )", "predictions": ["resize a string . . . . . . . ."], "references": ["return a value wrapped in the selected csi and does a reset ."], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 2211, "code": "def consume ( self , istream , ostream , batch = False ) : datapoints = [ ] if batch : sleep = max ( 0.01 , self . option . sleep ) fd = istream . fileno ( ) while True : try : if select . select ( [ fd ] , [ ] , [ ] , sleep ) : try : line = istream . readline ( ) if line == '' : break datapoints . append ( self . consume line ( line ) ) except Value Error : continue if self . option . sort by column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort by column - 1 ) ) if len ( datapoints ) > 1 : datapoints = datapoints [ - self . maximum points : ] self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream ) time . sleep ( sleep ) except Keyboard Interrupt : break else : for line in istream : try : datapoints . append ( self . consume line ( line ) ) except Value Error : pass if self . option . sort by column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort by column - 1 ) ) self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream )", "predictions": ["swap the data for the data algorithm ."], "references": ["read points from istream and output to ostream ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2212, "code": "def consume line ( self , line ) : data = RE VALUE KEY . split ( line . strip ( ) , 1 ) if len ( data ) == 1 : return float ( data [ 0 ] ) , None else : return float ( data [ 0 ] ) , data [ 1 ] . strip ( )", "predictions": ["load a pool from the input pool"], "references": ["consume data from a line ."], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 2213, "code": "def update ( self , points , values = None ) : self . values = values or [ None ] * len ( points ) if np is None : if self . option . function : warnings . warn ( 'numpy not available, function ignored' ) self . points = points self . minimum = min ( self . points ) self . maximum = max ( self . points ) self . current = self . points [ - 1 ] else : self . points = self . apply function ( points ) self . minimum = np . min ( self . points ) self . maximum = np . max ( self . points ) self . current = self . points [ - 1 ] if self . maximum == self . minimum : self . extents = 1 else : self . extents = ( self . maximum - self . minimum ) self . extents = ( self . maximum - self . minimum )", "predictions": ["resize width of width"], "references": ["add a set of data points ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2214, "code": "def color ramp ( self , size ) : color = PALETTE . get ( self . option . palette , { } ) color = color . get ( self . term . colors , None ) color ramp = [ ] if color is not None : ratio = len ( color ) / float ( size ) for i in range ( int ( size ) ) : color ramp . append ( self . term . color ( color [ int ( ratio * i ) ] ) ) return color ramp", "predictions": ["return the init ramp"], "references": ["generate a color ramp for the current screen height ."], "bleu": 0.08872444253557525, "rouge_l": 0.13260869565217392}
{"id": 2215, "code": "def human ( self , size , base = 1000 , units = ' k MGTZ' ) : sign = '+' if size >= 0 else '-' size = abs ( size ) if size < 1000 : return '%s%d' % ( sign , size ) for i , suffix in enumerate ( units ) : unit = 1000 ** ( i + 1 ) if size < unit : return ( '%s%.01f%s' % ( sign , size / float ( unit ) * base , suffix , ) ) . strip ( ) raise Overflow Error", "predictions": ["program a init in init"], "references": ["convert the input size to human readable short form ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2216, "code": "def apply function ( self , points ) : if not self . option . function : return points if np is None : raise Import Error ( 'numpy is not available' ) if ':' in self . option . function : function , arguments = self . option . function . split ( ':' , 1 ) arguments = arguments . split ( ',' ) else : function = self . option . function arguments = [ ] arguments = list ( map ( self . function argument , arguments ) ) filter function = FUNCTION . get ( function ) if filter function is None : raise Type Error ( 'Invalid function \"%s\"' % ( function , ) ) else : return filter function ( np . array ( list ( points ) ) , * arguments )", "predictions": ["clear the . option using the option option"], "references": ["run the filter function on the provided points ."], "bleu": 0.16829946711936866, "rouge_l": 0.232824427480916}
{"id": 2217, "code": "def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )", "predictions": ["return a line of lines from a feature"], "references": ["resolve the points to make a line between two points ."], "bleu": 0.14505474341517546, "rouge_l": 0.20469798657718125}
{"id": 2218, "code": "def set text ( self , point , text ) : if not self . option . legend : return if not isinstance ( point , Point ) : point = Point ( point ) for offset , char in enumerate ( str ( text ) ) : self . screen . canvas [ point . y ] [ point . x + offset ] = char", "predictions": ["add program s program to the screen"], "references": ["set a text value in the screen canvas ."], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 2219, "code": "def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or \"utf8\" if self . option . color : ramp = self . color ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines", "predictions": ["add the specified self to the terminal dirs dirs dirs dirs dirs dirs dirs dirs dirs dirs"], "references": ["render graph to stream ."], "bleu": 0.07223943354597204, "rouge_l": 0.10082644628099173}
{"id": 2220, "code": "def normalised numpy ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) points = np . array ( self . points ) - self . minimum points = points * 4.0 / self . extents * self . size . y for x , y in enumerate ( points ) : yield Point ( ( dx * x , min ( oy , oy - y ) , ) )", "predictions": ["splits a data into a data point"], "references": ["normalised data points using numpy ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2221, "code": "def normalised python ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) for x , point in enumerate ( self . points ) : y = ( point - self . minimum ) * 4.0 / self . extents * self . size . y yield Point ( ( dx * x , min ( oy , oy - y ) , ) )", "predictions": ["a generator that yields a python in a python"], "references": ["normalised data points using pure python ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2222, "code": "def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )", "predictions": ["get the get center of the ."], "references": ["zero crossing value ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2223, "code": "def mem size ( self ) : data len = self . data mem size node count = len ( list ( self . xml doc . iter ( tag = etree . Element ) ) ) if self . compressed : size = 52 * node count + data len + 630 else : tags len = 0 for e in self . xml doc . iter ( tag = etree . Element ) : e len = max ( len ( e . tag ) , 8 ) e len = ( e len + 3 ) & ~ 3 tags len += e len size = 56 * node count + data len + 630 + tags len #print('nodes:{} ({}) data:{} ({})'.format(node count,hex(node count), data len, hex(data len))) return ( size + 8 ) & ~ 7", "predictions": ["get the total number of nodes in the node"], "references": ["used when allocating memory ingame"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2224, "code": "def load class ( class path , default ) : if class path is None : return default component = class path . rsplit ( '.' , 1 ) result processor = getattr ( importlib . import module ( component [ 0 ] ) , component [ 1 ] , default ) if len ( component ) > 1 else default return result processor", "predictions": ["load a package package from the default"], "references": ["loads the class from the class_path string"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 2225, "code": "def process pagination values ( request ) : size = 20 page = 0 from = 0 if \"page size\" in request . POST : size = int ( request . POST [ \"page size\" ] ) max page size = getattr ( settings , \"SEARCH MAX PAGE SIZE\" , 100 ) if not ( 0 < size <= max page size ) : raise Value Error ( ( 'Invalid page size of {page size}' ) . format ( page size = size ) ) if \"page index\" in request . POST : page = int ( request . POST [ \"page index\" ] ) from = page * size return size , from , page", "predictions": ["return effects effects ."], "references": ["process pagination requests from request parameter"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2226, "code": "def process field values ( request ) : return { field key : request . POST [ field key ] for field key in request . POST if field key in course discovery filter fields ( ) }", "predictions": ["try to load fields fields fields from import fields"], "references": ["create separate dictionary of supported filter values provided"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2227, "code": "def translate hits ( es response ) : def translate result ( result ) : \"\"\" Any conversion from ES result syntax into our search engine syntax \"\"\" translated result = copy . copy ( result ) data = translated result . pop ( \" source\" ) translated result . update ( { \"data\" : data , \"score\" : translated result [ \" score\" ] } ) return translated result def translate facet ( result ) : \"\"\" Any conversion from ES facet syntax into our search engine sytax \"\"\" terms = { term [ \"term\" ] : term [ \"count\" ] for term in result [ \"terms\" ] } return { \"terms\" : terms , \"total\" : result [ \"total\" ] , \"other\" : result [ \"other\" ] , } results = [ translate result ( hit ) for hit in es response [ \"hits\" ] [ \"hits\" ] ] response = { \"took\" : es response [ \"took\" ] , \"total\" : es response [ \"hits\" ] [ \"total\" ] , \"max score\" : es response [ \"hits\" ] [ \"max score\" ] , \"results\" : results , } if \"facets\" in es response : response [ \"facets\" ] = { facet : translate facet ( es response [ \"facets\" ] [ facet ] ) for facet in es response [ \"facets\" ] } return response", "predictions": ["load hits results and return results"], "references": ["provide resultset in our desired format from elasticsearch results"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2228, "code": "def get filter field ( field name , field value ) : filter field = None if isinstance ( field value , Value Range ) : range values = { } if field value . lower : range values . update ( { \"gte\" : field value . lower string } ) if field value . upper : range values . update ( { \"lte\" : field value . upper string } ) filter field = { \"range\" : { field name : range values } } elif is iterable ( field value ) : filter field = { \"terms\" : { field name : field value } } else : filter field = { \"term\" : { field name : field value } } return filter field", "predictions": ["builds a single single single single single single single single single single single single single single single single single single single single single single single single single single single single single"], "references": ["return field to apply into filter if an array then use a range otherwise look for a term match"], "bleu": 0.03901663112717908, "rouge_l": 0.041809458533241944}
{"id": 2229, "code": "def process facet terms ( facet terms ) : elastic facets = { } for facet in facet terms : facet term = { \"field\" : facet } if facet terms [ facet ] : for facet option in facet terms [ facet ] : facet term [ facet option ] = facet terms [ facet ] [ facet option ] elastic facets [ facet ] = { \"terms\" : facet term } return elastic facets", "predictions": ["from separate separate terms terms terms terms"], "references": ["we have a list of terms with which we return facets"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 2230, "code": "def get mappings ( cls , index name , doc type ) : return cache . get ( cls . get cache item name ( index name , doc type ) , { } )", "predictions": ["print mappings for a particular index"], "references": ["fetch mapped - items structure from cache"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2231, "code": "def set mappings ( cls , index name , doc type , mappings ) : cache . set ( cls . get cache item name ( index name , doc type ) , mappings )", "predictions": ["load mappings mappings mappings"], "references": ["set new mapped - items structure into cache"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 2232, "code": "def log indexing error ( cls , indexing errors ) : indexing errors log = [ ] for indexing error in indexing errors : indexing errors log . append ( str ( indexing error ) ) raise exceptions . Elasticsearch Exception ( ', ' . join ( indexing errors log ) )", "predictions": ["log errors and getattr print print out print print print print print print print print print out ."], "references": ["logs indexing errors and raises a general elasticsearch exception"], "bleu": 0.08961672320242714, "rouge_l": 0.15762273901808785}
{"id": 2233, "code": "def remove ( self , doc type , doc ids , * * kwargs ) : try : actions = [ ] for doc id in doc ids : log . debug ( \"Removing document of type %s and index %s\" , doc type , doc id ) action = { ' op type' : 'delete' , \" index\" : self . index name , \" type\" : doc type , \" id\" : doc id } actions . append ( action ) bulk ( self . es , actions , * * kwargs ) except Bulk Index Error as ex : valid errors = [ error for error in ex . errors if error [ 'delete' ] [ 'status' ] != 404 ] if valid errors : log . exception ( \"An error occurred while removing documents from the index.\" ) raise", "predictions": ["image back to es ."], "references": ["implements call to remove the documents from the index"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2234, "code": "def perform search ( search term , user = None , size = 10 , from = 0 , course id = None ) : ( field dictionary , filter dictionary , exclude dictionary ) = Search Filter Generator . generate field filters ( user = user , course id = course id ) searcher = Search Engine . get search engine ( getattr ( settings , \"COURSEWARE INDEX NAME\" , \"courseware index\" ) ) if not searcher : raise No Search Engine Error ( \"No search engine specified in settings.SEARCH ENGINE\" ) results = searcher . search string ( search term , field dictionary = field dictionary , filter dictionary = filter dictionary , exclude dictionary = exclude dictionary , size = size , from = from , doc type = \"courseware content\" , ) for result in results [ \"results\" ] : result [ \"data\" ] = Search Result Processor . process result ( result [ \"data\" ] , search term , user ) results [ \"access denied count\" ] = len ( [ r for r in results [ \"results\" ] if r [ \"data\" ] is None ] ) results [ \"results\" ] = [ r for r in results [ \"results\" ] if r [ \"data\" ] is not None ] return results", "predictions": ["last last last last last last last last last last last last last result"], "references": ["call the search engine with the appropriate parameters"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 2235, "code": "def course discovery search ( search term = None , size = 20 , from = 0 , field dictionary = None ) : use search fields = [ \"org\" ] ( search fields , , exclude dictionary ) = Search Filter Generator . generate field filters ( ) use field dictionary = { } use field dictionary . update ( { field : search fields [ field ] for field in search fields if field in use search fields } ) if field dictionary : use field dictionary . update ( field dictionary ) if not getattr ( settings , \"SEARCH SKIP ENROLLMENT START DATE FILTERING\" , False ) : use field dictionary [ \"enrollment start\" ] = Date Range ( None , datetime . utcnow ( ) ) searcher = Search Engine . get search engine ( getattr ( settings , \"COURSEWARE INDEX NAME\" , \"courseware index\" ) ) if not searcher : raise No Search Engine Error ( \"No search engine specified in settings.SEARCH ENGINE\" ) results = searcher . search ( query string = search term , doc type = \"course info\" , size = size , from = from , field dictionary = use field dictionary , filter dictionary = { \"enrollment end\" : Date Range ( datetime . utcnow ( ) , None ) } , exclude dictionary = exclude dictionary , facet terms = course discovery facets ( ) , ) return results", "predictions": ["query elasticsearch sanity sanity sanity sanity sanity sanity ."], "references": ["course discovery activities against the search engine index of course details"], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 2236, "code": "def strings in dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( Search Result Processor . strings in dictionary ( child dict ) ) return strings", "predictions": ["get a list of create create a dictionary of create create keys from a dictionary join join join join join join join join join join join join join join join join"], "references": ["used by default implementation for finding excerpt"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2237, "code": "def find matches ( strings , words , length hoped ) : lower words = [ w . lower ( ) for w in words ] def has match ( string ) : \"\"\" Do any of the words match within the string \"\"\" lower string = string . lower ( ) for test word in lower words : if test word in lower string : return True return False shortened strings = [ textwrap . wrap ( s ) for s in strings ] short string list = list ( chain . from iterable ( shortened strings ) ) matches = [ ms for ms in short string list if has match ( ms ) ] cumulative len = 0 break at = None for idx , match in enumerate ( matches ) : cumulative len += len ( match ) if cumulative len >= length hoped : break at = idx break return matches [ 0 : break at ]", "predictions": ["get the first strings that matches the string file file file file file file file file file file file file file file file file file file file file file file file"], "references": ["used by default property excerpt"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2238, "code": "def decorate matches ( match in , match word ) : matches = re . finditer ( match word , match in , re . IGNORECASE ) for matched string in set ( [ match . group ( ) for match in matches ] ) : match in = match in . replace ( matched string , getattr ( settings , \"SEARCH MATCH DECORATION\" , u\"<b>{}</b>\" ) . format ( matched string ) ) return match in", "predictions": ["usage function to usage parser for parser for parser"], "references": ["decorate the matches within the excerpt"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2239, "code": "def excerpt ( self ) : if \"content\" not in self . results fields : return None match phrases = [ self . match phrase ] if six . PY2 : separate phrases = [ phrase . decode ( 'utf-8' ) for phrase in shlex . split ( self . match phrase . encode ( 'utf-8' ) ) ] else : separate phrases = [ phrase for phrase in shlex . split ( self . match phrase ) ] if len ( separate phrases ) > 1 : match phrases . extend ( separate phrases ) else : match phrases = separate phrases matches = Search Result Processor . find matches ( Search Result Processor . strings in dictionary ( self . results fields [ \"content\" ] ) , match phrases , DESIRED EXCERPT LENGTH ) excerpt text = ELLIPSIS . join ( matches ) for match word in match phrases : excerpt text = Search Result Processor . decorate matches ( excerpt text , match word ) return excerpt text", "predictions": ["usage of the results"], "references": ["property to display a useful excerpt representing the matches within the results"], "bleu": 0.06399610426154731, "rouge_l": 0.22932330827067668}
{"id": 2240, "code": "def parse ( self , filename ) : self . names = { } with codecs . open ( filename , encoding = \"iso8859-1\" ) as f : for line in f : if any ( map ( lambda c : 128 < ord ( c ) < 160 , line ) ) : line = line . encode ( \"iso8859-1\" ) . decode ( \"windows-1252\" ) self . eat name line ( line . strip ( ) )", "predictions": ["parse the given config file"], "references": ["opens data file and for each line calls _eat_name_line"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2241, "code": "def eat name line ( self , line ) : if line [ 0 ] not in \"#=\" : parts = line . split ( ) country values = line [ 30 : - 1 ] name = map name ( parts [ 1 ] ) if not self . case sensitive : name = name . lower ( ) if parts [ 0 ] == \"M\" : self . set ( name , u\"male\" , country values ) elif parts [ 0 ] == \"1M\" or parts [ 0 ] == \"?M\" : self . set ( name , u\"mostly male\" , country values ) elif parts [ 0 ] == \"F\" : self . set ( name , u\"female\" , country values ) elif parts [ 0 ] == \"1F\" or parts [ 0 ] == \"?F\" : self . set ( name , u\"mostly female\" , country values ) elif parts [ 0 ] == \"?\" : self . set ( name , self . unknown value , country values ) else : raise \"Not sure what to do with a sex of %s\" % parts [ 0 ]", "predictions": ["eat as defined by setting a line"], "references": ["parses one line of data file"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2242, "code": "def set ( self , name , gender , country values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . set ( name . replace ( '+' , replacement ) , gender , country values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country values", "predictions": ["set a gender value for the given gender ."], "references": ["sets gender and relevant country values for names dictionary of detector"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 2243, "code": "def most popular gender ( self , name , counter ) : if name not in self . names : return self . unknown value max count , max tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country values in self . names [ name ] . items ( ) : count , tie = counter ( country values ) if count > max count or ( count == max count and tie > max tie ) : max count , max tie , best = count , tie , gender return best if max count > 0 else self . unknown value", "predictions": ["find the gender for a most recent popular ."], "references": ["finds the most popular gender for the given name counting by given counter"], "bleu": 0.1416341262365823, "rouge_l": 0.26406926406926406}
{"id": 2244, "code": "def get gender ( self , name , country = None ) : if not self . case sensitive : name = name . lower ( ) if name not in self . names : return self . unknown value elif not country : def counter ( country values ) : country values = map ( ord , country values . replace ( \" \" , \"\" ) ) return ( len ( country values ) , sum ( map ( lambda c : c > 64 and c - 55 or c - 48 , country values ) ) ) return self . most popular gender ( name , counter ) elif country in self . class . COUNTRIES : index = self . class . COUNTRIES . index ( country ) counter = lambda e : ( ord ( e [ index ] ) - 32 , 0 ) return self . most popular gender ( name , counter ) else : raise No Country Error ( \"No such country: %s\" % country )", "predictions": ["get gender with given name"], "references": ["returns best gender for the given name and country pair"], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 2245, "code": "def is connected ( self ) : try : self . exec command ( b\"Query(Connection State)\" ) return self . status . connection state . startswith ( b\"C(\" ) except Not Connected Exception : return False", "predictions": ["returns true if the lock is connected ."], "references": ["return bool indicating connection state"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2246, "code": "def connect ( self , host ) : if not self . app . connect ( host ) : command = \"Connect({0})\" . format ( host ) . encode ( \"ascii\" ) self . exec command ( command ) self . last host = host", "predictions": ["connect to a remote host and connect to the given host ."], "references": ["connect to a host"], "bleu": 0.2044800736021839, "rouge_l": 0.5495495495495495}
{"id": 2247, "code": "def signature matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , * * kwargs ) except Type Error : return False else : return True", "predictions": ["decorator to check if function matches a function ."], "references": ["work out if a function is callable with some args or not ."], "bleu": 0.135323305042906, "rouge_l": 0.35209235209235207}
{"id": 2248, "code": "def register chooser ( self , chooser , * * kwargs ) : if not issubclass ( chooser , Chooser ) : return self . register simple chooser ( chooser , * * kwargs ) self . choosers [ chooser . model ] = chooser ( * * kwargs ) return chooser", "predictions": ["register a chooser function ."], "references": ["adds a model chooser definition to the registry ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 2249, "code": "def from json list ( cls , api client , data ) : return [ cls . from json ( api client , item ) for item in data ]", "predictions": ["return a list of objects that were passed to a list of permissions ."], "references": ["convert a list of json values to a list of models"], "bleu": 0.3290385879986622, "rouge_l": 0.5723860589812332}
{"id": 2250, "code": "def from json ( cls , api client , data ) : self = cls ( api client ) Pandora Model . populate fields ( api client , self , data ) return self", "predictions": ["create a new model from a json string ."], "references": ["convert one json value to a model object"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 2251, "code": "def base repr ( self , and also = None ) : items = [ \"=\" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . fields . keys ( ) ) ] if items : output = \", \" . join ( items ) else : output = None if and also : return \"{}({}, {})\" . format ( self . class . name , output , and also ) else : return \"{}({})\" . format ( self . class . name , output )", "predictions": ["generates a string for setting the base repr ."], "references": ["common repr logic for subclasses to hook"], "bleu": 0.15619699684601276, "rouge_l": 0.1278825995807128}
{"id": 2252, "code": "def send cmd ( self , cmd ) : self . process . stdin . write ( \"{}\\n\" . format ( cmd ) . encode ( \"utf-8\" ) ) self . process . stdin . flush ( )", "predictions": ["send command to client ."], "references": ["write command to remote process"], "bleu": 0.35930411196308426, "rouge_l": 0.4}
{"id": 2253, "code": "def ensure started ( self ) : if self . process and self . process . poll ( ) is None : return if not getattr ( self , \" cmd\" ) : raise Runtime Error ( \"Player command is not configured\" ) log . debug ( \"Starting playback command: %r\" , self . cmd ) self . process = Silent Popen ( self . cmd ) self . post start ( )", "predictions": ["make sure the process is ready to be ready ."], "references": ["ensure player backing process is started"], "bleu": 0.16590387014219712, "rouge_l": 0.26180257510729615}
{"id": 2254, "code": "def station selection menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print error ( \"{}\\n\" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = \"{:>3}\" . format ( i ) print ( \"{}: {}\" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get integer ( \"Station: \" ) ]", "predictions": ["get the station menu menu ."], "references": ["format a station menu and make the user select a station"], "bleu": 0.13576587000692578, "rouge_l": 0.2234432234432234}
{"id": 2255, "code": "def input ( self , input , song ) : try : cmd = getattr ( self , self . CMD MAP [ input ] [ 1 ] ) except ( Index Error , Key Error ) : return self . screen . print error ( \"Invalid command {!r}!\" . format ( input ) ) cmd ( song )", "predictions": ["run input command in screen"], "references": ["input callback handles key presses"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 2256, "code": "def poll ( self ) : ret = self . communication Channel . receive finished ( ) self . nruns -= len ( ret ) return ret", "predictions": ["poll for a response"], "references": ["return pairs of run ids and results of finish event loops ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 2257, "code": "def end ( self ) : results = self . communication Channel . receive ( ) if self . nruns != len ( results ) : import logging logger = logging . get Logger ( name ) logger . warning ( 'too few results received: {} results received, {} expected' . format ( len ( results ) , self . nruns ) ) return results", "predictions": ["end of the closing server ."], "references": ["wait until all event loops end and returns the results ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 2258, "code": "def expand str ( path cfg , alias dict , overriding kargs ) : if path cfg in alias dict : return expand str alias ( path cfg , alias dict , overriding kargs ) return expand for lambda str ( path cfg , alias dict , overriding kargs )", "predictions": ["expand a list of directories of a str ."], "references": ["expand a path config given as a string"], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 2259, "code": "def expand tuple ( path cfg , alias dict , overriding kargs ) : new path cfg = path cfg [ 0 ] new overriding kargs = path cfg [ 1 ] . copy ( ) new overriding kargs . update ( overriding kargs ) return expand path cfg ( new path cfg , overriding kargs = new overriding kargs , alias dict = alias dict )", "predictions": ["expand a tuple of path with a tuple of paths ."], "references": ["expand a path config given as a tuple"], "bleu": 0.1972940627795883, "rouge_l": 0.5417406749555951}
{"id": 2260, "code": "def wait ( self ) : finished pids = [ ] while self . running procs : finished pids . extend ( self . poll ( ) ) return finished pids", "predictions": ["wait for the worker to finish ."], "references": ["wait until all jobs finish and return a list of pids"], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2261, "code": "def get Vector ( self , tree , branch Name ) : if ( tree , branch Name ) in self . class . address Dict : return self . class . address Dict [ ( tree , branch Name ) ] its Vector = self . get Vector ( tree , branch Name ) self . class . address Dict [ ( tree , branch Name ) ] = its Vector return its Vector", "predictions": ["return the branch for a particular branch"], "references": ["return the root . vector object for the branch ."], "bleu": 0.20024850746991507, "rouge_l": 0.45607476635514016}
{"id": 2262, "code": "def handle Auth ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : interactive = globalconf . get ( 'interactive' ) def retry With Auth Or Raise ( original exception ) : auth . authorize User ( provider = 'github' , interactive = interactive ) if not interactive : raise original exception else : logger . debug ( 'trying with authtoken: %s' , settings . get Property ( 'github' , 'authtoken' ) ) return fn ( * args , * * kwargs ) def handle Rate Limit Exceeded ( original exception ) : if not user Authed With Github ( ) : logger . warning ( 'github rate limit for anonymous requests exceeded: you must log in' ) return retry With Auth Or Raise ( original exception ) else : raise original exception try : return fn ( * args , * * kwargs ) except requests . exceptions . HTTP Error as e : if e . response . status code == 403 : return handle Rate Limit Exceeded ( e ) if e . response . status code == 401 : return retry With Auth Or Raise ( e ) raise except github . Bad Credentials Exception as e : logger . debug ( \"github: bad credentials\" ) return retry With Auth Or Raise ( e ) except github . Unknown Object Exception as e : logger . debug ( \"github: unknown object\" ) if not user Authed With Github ( ) : logger . info ( 'failed to fetch Github object, re-trying with authentication...' ) return retry With Auth Or Raise ( e ) raise except github . Rate Limit Exceeded Exception as e : return handle Rate Limit Exceeded ( e ) except github . Github Exception as e : if e . status == 403 : return handle Rate Limit Exceeded ( e ) raise return wrapped", "predictions": ["decorator to require rate requests to github rate requests"], "references": ["decorator to re - try api calls after asking the user for authentication ."], "bleu": 0.10657503067399117, "rouge_l": 0.1673525377229081}
{"id": 2263, "code": "def get Tip Archive URL ( repo ) : g = Github ( settings . get Property ( 'github' , 'authtoken' ) ) repo = g . get repo ( repo ) return repo . get archive link ( 'tarball' )", "predictions": ["returns the link for the specified repo"], "references": ["return a string containing a tarball url"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2264, "code": "def get Commit Archive URL ( repo , commit ) : g = Github ( settings . get Property ( 'github' , 'authtoken' ) ) repo = g . get repo ( repo ) return repo . get archive link ( 'tarball' , commit )", "predictions": ["return the link for the specified commit ."], "references": ["return a string containing a tarball url"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2265, "code": "def get Tarball ( url , into directory , cache key , origin info = None ) : try : access common . unpack From Cache ( cache key , into directory ) except Key Error as e : tok = settings . get Property ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow redirects = True , stream = True , headers = headers ) response . raise for status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise for status ( ) access common . unpack Tarball Stream ( stream = response , into directory = into directory , hash = { } , cache key = cache key , origin info = origin info )", "predictions": ["get tarball from url ."], "references": ["unpack the specified tarball url into the specified directory"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 2266, "code": "def available Versions ( self ) : r = [ ] for t in self . get Tags ( ) : logger . debug ( \"available version tag: %s\" , t ) if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( Github Component Version ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache key = None ) ) except Value Error : logger . debug ( 'invalid version tag: %s' , t ) return r", "predictions": ["get a list of available versions ."], "references": ["return a list of version objects each with a tarball url set"], "bleu": 0.1692447266569478, "rouge_l": 0.30148270181219106}
{"id": 2267, "code": "def available Tags ( self ) : return [ Github Component Version ( '' , t [ 0 ] , t [ 1 ] , self . name , cache key = create Cache Key ( 'tag' , t [ 0 ] , t [ 1 ] , self . name ) ) for t in self . get Tags ( ) ]", "predictions": ["list of available available name ."], "references": ["return a list of githubcomponentversion objects for all tags"], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 2268, "code": "def available Branches ( self ) : return [ Github Component Version ( '' , b [ 0 ] , b [ 1 ] , self . name , cache key = None ) for b in get Branch Heads ( self . repo ) . items ( ) ]", "predictions": ["list of available branches ."], "references": ["return a list of githubcomponentversion objects for the tip of each branch"], "bleu": 0.08860330314183162, "rouge_l": 0.2190305206463196}
{"id": 2269, "code": "def commit Version ( self ) : import re commit match = re . match ( '^[a-f0-9]{7,40}$' , self . tag Or Branch Spec ( ) , re . I ) if commit match : return Github Component Version ( '' , '' , get Commit Archive URL ( self . repo , self . tag Or Branch Spec ( ) ) , self . name , cache key = None ) return None", "predictions": ["commit the local version of the tag if it is not cached ."], "references": ["return a githubcomponentversion object for a specific commit if valid"], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 2270, "code": "def handle Auth ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : from yotta . lib import auth interactive = globalconf . get ( 'interactive' ) try : return fn ( * args , * * kwargs ) except requests . exceptions . HTTP Error as e : if e . response . status code == requests . codes . unauthorized : #pylint: disable=no-member logger . debug ( '%s unauthorised' , fn ) auth . authorize User ( provider = None , interactive = interactive ) if interactive : logger . debug ( 'retrying after authentication...' ) return fn ( * args , * * kwargs ) raise return wrapped", "predictions": ["decorator that ensures that a auth decorator is authenticated"], "references": ["decorator to re - try api calls after asking the user for authentication ."], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 2271, "code": "def islast ( generator ) : next x = None first = True for x in generator : if not first : yield ( next x , False ) next x = x first = False if not first : yield ( next x , True )", "predictions": ["generate a generator that yields items of a generator"], "references": ["indicate whether the current item is the last one in a generator"], "bleu": 0.13309610652103343, "rouge_l": 0.1856925418569254}
{"id": 2272, "code": "def source Dir Validation Error ( dirname , component name ) : if dirname == component name : return 'Module %s public include directory %s should not contain source files' % ( component name , dirname ) elif dirname . lower ( ) in ( 'source' , 'src' ) and dirname != 'source' : return 'Module %s has non-standard source directory name: \"%s\" should be \"source\"' % ( component name , dirname ) elif is Potential Test Dir ( dirname ) and dirname != 'test' : return 'Module %s has non-standard test directory name: \"%s\" should be \"test\"' % ( component name , dirname ) elif not Source Dir Regex . match ( dirname ) : corrected = Source Dir Invalid Regex . sub ( '' , dirname . lower ( ) ) if not corrected : corrected = 'source' return 'Module %s has non-standard source directory name: \"%s\" should be \"%s\"' % ( component name , dirname , corrected ) else : return None", "predictions": ["check if a parse parse is non - installed parse f lambda"], "references": ["validate source directory names in components"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 2273, "code": "def commit Version ( self , spec ) : import re commit match = re . match ( '^[a-f0-9]{7,40}$' , spec , re . I ) if commit match : return Git Clone Version ( '' , spec , self ) return None", "predictions": ["eat version of local version"], "references": ["return a githubcomponentversion object for a specific commit if valid"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2274, "code": "def load Config ( self ) : config dicts = [ self . additional config , self . app config ] + [ t . get Config ( ) for t in self . hierarchy ] config blame = [ mirror Structure ( self . additional config , 'command-line config' ) , mirror Structure ( self . app config , 'application\\'s config.json' ) , ] + [ mirror Structure ( t . get Config ( ) , t . get Name ( ) ) for t in self . hierarchy ] self . config = merge Dictionaries ( * config dicts ) self . config blame = merge Dictionaries ( * config blame )", "predictions": ["reads the . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["load the configuration information from the target hierarchy"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 2275, "code": "def sometimes Prune Cache ( p ) : def decorator ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : r = fn ( * args , * * kwargs ) if random . random ( ) < p : prune Cache ( ) return r return wrapped return decorator", "predictions": ["a decorator that can be called with a = = = a = = = 0 not a = = false"], "references": ["return decorator to prune cache after calling fn with a probability of p"], "bleu": 0.0821610732492254, "rouge_l": 0.18429003021148035}
{"id": 2276, "code": "def get expiration date ( self , fn ) : r = self . local renderer r . env . crt fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl crt fn} -dates' , capture = True ) matches = re . findall ( 'not After=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )", "predictions": ["get the date of the not defined in the not not not not not not not not not not not not not not not not not not not not not not"], "references": ["reads the expiration date of a local crt file ."], "bleu": 0.055177848898164926, "rouge_l": 0.16123348017621145}
{"id": 2277, "code": "def list expiration dates ( self , base = 'roles/all/ssl' ) : max fn len = 0 max date len = 0 data = [ ] for fn in os . listdir ( base ) : fqfn = os . path . join ( base , fn ) if not os . path . isfile ( fqfn ) : continue if not fn . endswith ( '.crt' ) : continue expiration date = self . get expiration date ( fqfn ) max fn len = max ( max fn len , len ( fn ) ) max date len = max ( max date len , len ( str ( expiration date ) ) ) data . append ( ( fn , expiration date ) ) print ( '%s %s %s' % ( 'Filename' . ljust ( max fn len ) , 'Expiration Date' . ljust ( max date len ) , 'Expired' ) ) now = datetime . now ( ) . replace ( tzinfo = pytz . UTC ) for fn , dt in sorted ( data ) : if dt is None : expired = '?' elif dt < now : expired = 'YES' else : expired = 'NO' print ( '%s %s %s' % ( fn . ljust ( max fn len ) , str ( dt ) . ljust ( max date len ) , expired ) )", "predictions": ["is the user - defined connected dates"], "references": ["scans through all local . crt files and displays the expiration dates ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 2278, "code": "def verify certificate chain ( self , base = None , crt = None , csr = None , key = None ) : from burlap . common import get verbose , print fail , print success r = self . local renderer if base : crt = base + '.crt' csr = base + '.csr' key = base + '.key' else : assert crt and csr and key , 'If base not provided, crt and csr and key must be given.' assert os . path . isfile ( crt ) assert os . path . isfile ( csr ) assert os . path . isfile ( key ) csr md5 = r . local ( 'openssl req -noout -modulus -in %s | openssl md5' % csr , capture = True ) key md5 = r . local ( 'openssl rsa -noout -modulus -in %s | openssl md5' % key , capture = True ) crt md5 = r . local ( 'openssl x509 -noout -modulus -in %s | openssl md5' % crt , capture = True ) match = crt md5 == csr md5 == key md5 if self . verbose or not match : print ( 'crt:' , crt md5 ) print ( 'csr:' , csr md5 ) print ( 'key:' , key md5 ) if match : print success ( 'Files look good!' ) else : print fail ( 'Files no not match!' ) raise Exception ( 'Files no not match!' )", "predictions": ["connect to the certificate self ."], "references": ["confirms the key csr and certificate files all match ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 2279, "code": "def is file ( self , path , use sudo = False ) : if self . is local and not use sudo : return os . path . isfile ( path ) else : func = use sudo and sudo or run with self . settings ( hide ( 'running' , 'warnings' ) , warn only = True ) : return func ( '[ -f \"%(path)s\" ]' % locals ( ) ) . succeeded", "predictions": ["check if a matches matches a matches matches a matches matches a matches matches"], "references": ["check if a path exists and is a file ."], "bleu": 0.17395797375642236, "rouge_l": 0.34366197183098596}
{"id": 2280, "code": "def is dir ( self , path , use sudo = False ) : if self . is local and not use sudo : return os . path . isdir ( path ) else : func = use sudo and sudo or run with self . settings ( hide ( 'running' , 'warnings' ) , warn only = True ) : return func ( '[ -d \"%(path)s\" ]' % locals ( ) ) . succeeded", "predictions": ["check if a directory is sudo"], "references": ["check if a path exists and is a directory ."], "bleu": 0.2510214496072342, "rouge_l": 0.47843137254901963}
{"id": 2281, "code": "def is link ( self , path , use sudo = False ) : func = use sudo and sudo or run with self . settings ( hide ( 'running' , 'warnings' ) , warn only = True ) : return func ( '[ -L \"%(path)s\" ]' % locals ( ) ) . succeeded", "predictions": ["returns whether a json json is a json json object . ."], "references": ["check if a path exists and is a symbolic link ."], "bleu": 0.15537125692760353, "rouge_l": 0.3505747126436781}
{"id": 2282, "code": "def get owner ( self , path , use sudo = False ) : func = use sudo and run as root or self . run with self . settings ( hide ( 'running' , 'stdout' ) , warn only = True ) : result = func ( 'stat -c %%U \"%(path)s\"' % locals ( ) ) if result . failed and 'stat: illegal option' in result : return func ( 'stat -f %%Su \"%(path)s\"' % locals ( ) ) return result", "predictions": ["from api for json ."], "references": ["get the owner name of a file or directory ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2283, "code": "def md5sum ( self , filename , use sudo = False ) : func = use sudo and run as root or self . run with self . settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : if exists ( u'/usr/bin/md5sum' ) : res = func ( u'/usr/bin/md5sum %(filename)s' % locals ( ) ) elif exists ( u'/sbin/md5' ) : res = func ( u'/sbin/md5 -r %(filename)s' % locals ( ) ) elif exists ( u'/opt/local/gnu/bin/md5sum' ) : res = func ( u'/opt/local/gnu/bin/md5sum %(filename)s' % locals ( ) ) elif exists ( u'/opt/local/bin/md5sum' ) : res = func ( u'/opt/local/bin/md5sum %(filename)s' % locals ( ) ) else : md5sum = func ( u'which md5sum' ) md5 = func ( u'which md5' ) if exists ( md5sum ) : res = func ( '%(md5sum)s %(filename)s' % locals ( ) ) elif exists ( md5 ) : res = func ( '%(md5)s %(filename)s' % locals ( ) ) else : abort ( 'No MD5 utility was found on this system.' ) if res . succeeded : md5sum = res else : warn ( res ) md5sum = None if isinstance ( md5sum , six . string types ) : md5sum = md5sum . strip ( ) . split ( '\\n' ) [ - 1 ] . split ( ) [ 0 ] return md5sum", "predictions": ["get base base base base base name"], "references": ["compute the md5 sum of a file ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 2284, "code": "def uncommented lines ( self , filename , use sudo = False ) : func = run as root if use sudo else self . run res = func ( 'cat %s' % quote ( filename ) , quiet = True ) if res . succeeded : return [ line for line in res . splitlines ( ) if line and not line . startswith ( '#' ) ] return [ ]", "predictions": ["send cmd cmd cmd"], "references": ["get the lines of a remote file ignoring empty or commented ones"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 2285, "code": "def copy ( self , source , destination , recursive = False , use sudo = False ) : func = use sudo and run as root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )", "predictions": ["ensure source are copied to destination"], "references": ["copy a file or directory"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2286, "code": "def move ( self , source , destination , use sudo = False ) : func = use sudo and run as root or self . run func ( '/bin/mv {0} {1}' . format ( quote ( source ) , quote ( destination ) ) )", "predictions": ["station a file or destination"], "references": ["move a file or directory"], "bleu": 0.5081327481546147, "rouge_l": 0.6}
{"id": 2287, "code": "def remove ( self , path , recursive = False , use sudo = False ) : func = use sudo and run as root or self . run options = '-r ' if recursive else '' func ( '/bin/rm {0}{1}' . format ( options , quote ( path ) ) )", "predictions": ["input is a file or a directory input"], "references": ["remove a file or directory"], "bleu": 0.3155984539112945, "rouge_l": 0.6421052631578947}
{"id": 2288, "code": "def check for change ( self ) : r = self . local renderer lm = self . last manifest last fingerprint = lm . fingerprint current fingerprint = self . get target geckodriver version number ( ) self . vprint ( 'last fingerprint:' , last fingerprint ) self . vprint ( 'current fingerprint:' , current fingerprint ) if last fingerprint != current fingerprint : print ( 'A new release is available. %s' % self . get most recent version ( ) ) return True print ( 'No updates found.' ) return False", "predictions": ["poll for most updates"], "references": ["determines if a new release has been made ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2289, "code": "def is installed ( pkg name ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = run ( \"rpm --query %(pkg name)s\" % locals ( ) ) if res . succeeded : return True return False", "predictions": ["checks if a particular package is installed"], "references": ["check if an rpm package is installed ."], "bleu": 0.31689174383082924, "rouge_l": 0.5269978401727862}
{"id": 2290, "code": "def static ( self ) : fn = self . render to file ( 'ip/ip interfaces static.template' ) r = self . local renderer r . put ( local path = fn , remote path = r . env . interfaces fn , use sudo = True )", "predictions": ["put expand expand to file"], "references": ["configures the server to use a static ip ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2291, "code": "def get thumbprint ( self ) : extensions = self . extensions . split ( ' ' ) name str = ' -or ' . join ( '-name \"%s\"' % ext for ext in extensions ) cmd = 'find ' + self . base dir + r' -type f \\( ' + name str + r' \\) -exec md5sum {} \\; | sort -k 2 | md5sum' return getoutput ( cmd )", "predictions": ["return the tuple of tuple of tuple"], "references": ["calculates the current thumbprint of the item being tracked ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2292, "code": "def get thumbprint ( self ) : d = { } if self . names : names = self . names else : names = list ( self . satchel . lenv ) for name in self . names : d [ name ] = deepcopy ( self . satchel . env [ name ] ) return d", "predictions": ["return thumbprint . all running running this function will be used in a dict"], "references": ["calculates the current thumbprint of the item being tracked ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 2293, "code": "def get thumbprint ( self ) : d = { } for tracker in self . trackers : d [ type ( tracker ) . name ] = tracker . get thumbprint ( ) return d", "predictions": ["its address its attributes"], "references": ["calculates the current thumbprint of the item being tracked ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 2294, "code": "def upgrade ( safe = True ) : manager = MANAGER if safe : cmd = 'upgrade' else : cmd = 'dist-upgrade' run as root ( \"%(manager)s --assume-yes %(cmd)s\" % locals ( ) , pty = False )", "predictions": ["handle handle handle handle handle handle handle ."], "references": ["upgrade all packages ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2295, "code": "def is installed ( pkg name ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = run ( \"dpkg -s %(pkg name)s\" % locals ( ) ) for line in res . splitlines ( ) : if line . startswith ( \"Status: \" ) : status = line [ 8 : ] if \"installed\" in status . split ( ' ' ) : return True return False", "predictions": ["check if a package is installed"], "references": ["check if a package is installed ."], "bleu": 0.846481724890614, "rouge_l": 0.9104477611940297}
{"id": 2296, "code": "def apt key exists ( keyid ) : gpg cmd = 'gpg --ignore-time-conflict --no-options --no-default-keyring --keyring /etc/apt/trusted.gpg' with settings ( hide ( 'everything' ) , warn only = True ) : res = run ( '%(gpg cmd)s --fingerprint %(keyid)s' % locals ( ) ) return res . succeeded", "predictions": ["check if a get key exists exists is available"], "references": ["check if the given key id exists in apt keyring ."], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 2297, "code": "def exists ( self , name ) : with self . settings ( hide ( 'running' , 'stdout' , 'warnings' ) , warn only = True ) : return self . run ( 'getent group %(name)s' % locals ( ) ) . succeeded", "predictions": ["check if a group get a specific group"], "references": ["check if a group exists ."], "bleu": 0.44632361378533286, "rouge_l": 0.5865384615384615}
{"id": 2298, "code": "def enter password change ( self , username = None , old password = None ) : from fabric . state import connections from fabric . network import disconnect all r = self . local renderer r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old password = r . env . default passwords [ self . genv . user ] r . env . new password = self . env . passwords [ self . genv . user ] if old password : r . env . old password = old password prompts = { '(current) UNIX password: ' : r . env . old password , 'Enter new UNIX password: ' : r . env . new password , 'Retype new UNIX password: ' : r . env . new password , #\"Login password for '%s': \" % r.genv.user: r.env.new password, } print ( 'prompts:' , prompts ) r . env . password = r . env . old password with self . settings ( warn only = True ) : ret = r . local ( \"sshpass -p '{password}' ssh -o Strict Host Key Checking=no {user}@{host string} echo hello\" , capture = True ) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return code in ( 1 , 6 ) or 'hello' in ret : self . genv . password = r . env . old password elif self . genv . user in self . genv . user passwords : self . genv . password = r . env . new password else : self . genv . password = None print ( 'using password:' , self . genv . password ) #r.genv.password = r.env.new password #r.genv.password = r.env.new password with self . settings ( prompts = prompts ) : ret = r . run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do disconnect = 'passwd: password updated successfully' in ret print ( 'do disconnect:' , do disconnect ) if do disconnect : disconnect all ( ) self . genv . password = r . env . new password", "predictions": ["available password change change to ."], "references": ["responds to a forced password change via passwd prompts due to password expiration ."], "bleu": 0.08707046609544257, "rouge_l": 0.3730886850152905}
{"id": 2299, "code": "def togroups ( self , user , groups ) : r = self . local renderer if isinstance ( groups , six . string types ) : groups = [ . strip ( ) for in groups . split ( ',' ) if . strip ( ) ] for group in groups : r . env . username = user r . env . group = group r . sudo ( 'groupadd --force {group}' ) r . sudo ( 'adduser {username} {group}' )", "predictions": ["remove groups from the local"], "references": ["adds the user to the given list of groups ."], "bleu": 0.11115018927487523, "rouge_l": 0.12577319587628866}
{"id": 2300, "code": "def generate keys ( self , username , hostname ) : r = self . local renderer #r.env.key filename = r.env.key filename or env.key filename #assert r.env.key filename, 'r.env.key filename or env.key filename must be set. e.g. roles/role/app name-role.pem' r . env . key filename = self . env . key filename template . format ( ROLE = self . genv . ROLE , host = hostname , username = username , ) if os . path . isfile ( r . env . key filename ) : r . pc ( 'Key file {key filename} already exists. Skipping generation.' . format ( * * r . env ) ) else : r . local ( \"ssh-keygen -t {key type} -b {key bits} -f {key filename} -N ''\" ) r . local ( 'chmod {key perms} {key filename}' ) if r . env . key filename . endswith ( '.pem' ) : src = r . env . key filename + '.pub' dst = ( r . env . key filename + '.pub' ) . replace ( '.pem' , '' ) r . env . src = src r . env . dst = dst r . local ( 'mv {src} {dst}' ) return r . env . key filename", "predictions": ["available keys and return keys"], "references": ["generates * . pem and * . pub key files suitable for setting up passwordless ssh ."], "bleu": 0.02476709768976917, "rouge_l": 0.08276797829036635}
{"id": 2301, "code": "def create ( self , username , groups = None , uid = None , create home = None , system = False , password = None , home dir = None ) : r = self . local renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create home is None : create home = not system if create home is True : if home dir : args . append ( '--home %s' % home dir ) elif create home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted password = crypt password ( password ) args . append ( '-p %s' % quote ( crypted password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos \"\"' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )", "predictions": ["commit a security user"], "references": ["creates a user with the given username ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 2302, "code": "def expire password ( self , username ) : r = self . local renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )", "predictions": ["handle password password wrapped by the user"], "references": ["forces the user to change their password the next time they login ."], "bleu": 0.1114789227233716, "rouge_l": 0.1897356143079316}
{"id": 2303, "code": "def shell ( self , gui = 0 , command = '' , dryrun = None , shell interactive cmd str = None ) : from burlap . common import get hosts for site if dryrun is not None : self . dryrun = dryrun r = self . local renderer if r . genv . SITE != r . genv . default site : shell hosts = get hosts for site ( ) if shell hosts : r . genv . host string = shell hosts [ 0 ] r . env . SITE = r . genv . SITE or r . genv . default site if int ( gui ) : r . env . shell default options . append ( '-X' ) if 'host string' not in self . genv or not self . genv . host string : if 'available sites' in self . genv and r . env . SITE not in r . genv . available sites : raise Exception ( 'No host string set. Unknown site %s.' % r . env . SITE ) else : raise Exception ( 'No host string set.' ) if '@' in r . genv . host string : r . env . shell host string = r . genv . host string else : r . env . shell host string = '{user}@{host string}' if command : r . env . shell interactive cmd str = command else : r . env . shell interactive cmd str = r . format ( shell interactive cmd str or r . env . shell interactive cmd ) r . env . shell default options str = ' ' . join ( r . env . shell default options ) if self . is local : self . vprint ( 'Using direct local.' ) cmd = '{shell interactive cmd str}' elif r . genv . key filename : self . vprint ( 'Using key filename.' ) port = r . env . shell host string . split ( ':' ) [ - 1 ] if port . isdigit ( ) : r . env . shell host string = r . env . shell host string . split ( ':' ) [ 0 ] + ( ' -p %s' % port ) cmd = 'ssh -t {shell default options str} -i {key filename} {shell host string} \"{shell interactive cmd str}\"' elif r . genv . password : self . vprint ( 'Using password.' ) cmd = 'ssh -t {shell default options str} {shell host string} \"{shell interactive cmd str}\"' else : self . vprint ( 'Using nothing.' ) cmd = 'ssh -t {shell default options str} {shell host string} \"{shell interactive cmd str}\"' r . local ( cmd )", "predictions": ["start a shell environment"], "references": ["opens an ssh connection ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2304, "code": "def disk ( self ) : r = self . local renderer r . run ( r . env . disk usage command )", "predictions": ["run the local disk command ."], "references": ["display percent of disk usage ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 2305, "code": "def tunnel ( self , local port , remote port ) : r = self . local renderer r . env . tunnel local port = local port r . env . tunnel remote port = remote port r . local ( ' ssh -i {key filename} -L {tunnel local port}:localhost:{tunnel remote port} {user}@{host string} -N' )", "predictions": ["3d tunnel port on remote host"], "references": ["creates an ssh tunnel ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2306, "code": "def install from scratch ( python cmd , use sudo ) : with cd ( \"/tmp\" ) : download ( EZ SETUP URL ) command = '%(python cmd)s ez setup.py' % locals ( ) if use sudo : run as root ( command ) else : run ( command ) run ( 'rm -f ez setup.py' )", "predictions": ["install from scratch ."], "references": ["install setuptools from scratch using installer"], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 2307, "code": "def has virtualenv ( self ) : with self . settings ( warn only = True ) : ret = self . run or local ( 'which virtualenv' ) . strip ( ) return bool ( ret )", "predictions": ["returns true if the virtualenv is a valid virtualenv ."], "references": ["returns true if the virtualenv tool is installed ."], "bleu": 0.49616830003403634, "rouge_l": 0.7439024390243903}
{"id": 2308, "code": "def virtualenv exists ( self , virtualenv dir = None ) : r = self . local renderer ret = True with self . settings ( warn only = True ) : ret = r . run or local ( 'ls {virtualenv dir}' ) or '' ret = 'cannot access' not in ret . strip ( ) . lower ( ) if self . verbose : if ret : print ( 'Yes' ) else : print ( 'No' ) return ret", "predictions": ["check if a virtualenv exists"], "references": ["returns true if the virtual environment has been created ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2309, "code": "def what requires ( self , name ) : r = self . local renderer r . env . name = name r . local ( 'pipdeptree -p {name} --reverse' )", "predictions": ["what will be called when a local file is created"], "references": ["lists the packages that require the given package ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2310, "code": "def init ( self ) : r = self . local renderer print ( 'Creating new virtual environment...' ) with self . settings ( warn only = True ) : cmd = '[ ! -d {virtualenv dir} ] && virtualenv --no-site-packages {virtualenv dir} || true' if self . is local : r . run or local ( cmd ) else : r . sudo ( cmd )", "predictions": ["init the virtual environment to be used for a new virtual server ."], "references": ["creates the virtual environment ."], "bleu": 0.18798317647335086, "rouge_l": 0.48316831683168315}
{"id": 2311, "code": "def get combined requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find template ( f ) content . extend ( list ( iter lines ( f ) ) ) else : assert isinstance ( requirements , six . string types ) f = self . find template ( requirements ) content . extend ( list ( iter lines ( f ) ) ) return '\\n' . join ( content )", "predictions": ["yield the combined requirements file for this template ."], "references": ["returns all requirements files combined into one string ."], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 2312, "code": "def list instances ( show = 1 , name = None , group = None , release = None , except release = None ) : from burlap . common import shelf , Ordered Dict , get verbose verbose = get verbose ( ) require ( 'vm type' , 'vm group' ) assert env . vm type , 'No VM type specified.' env . vm type = ( env . vm type or '' ) . lower ( ) name = name group = group release = release if verbose : print ( 'name=%s, group=%s, release=%s' % ( name , group , release ) ) env . vm elastic ip mappings = shelf . get ( 'vm elastic ip mappings' ) data = type ( env ) ( ) if env . vm type == EC2 : if verbose : print ( 'Checking EC2...' ) for instance in get all running ec2 instances ( ) : name = instance . tags . get ( env . vm name tag ) group = instance . tags . get ( env . vm group tag ) release = instance . tags . get ( env . vm release tag ) if env . vm group and env . vm group != group : if verbose : print ( ( 'Skipping instance %s because its group \"%s\" ' 'does not match env.vm group \"%s\".' ) % ( instance . public dns name , group , env . vm group ) ) continue if group and group != group : if verbose : print ( ( 'Skipping instance %s because its group \"%s\" ' 'does not match local group \"%s\".' ) % ( instance . public dns name , group , group ) ) continue if name and name != name : if verbose : print ( ( 'Skipping instance %s because its name \"%s\" ' 'does not match name \"%s\".' ) % ( instance . public dns name , name , name ) ) continue if release and release != release : if verbose : print ( ( 'Skipping instance %s because its release \"%s\" ' 'does not match release \"%s\".' ) % ( instance . public dns name , release , release ) ) continue if except release and release == except release : continue if verbose : print ( 'Adding instance %s (%s).' % ( name , instance . public dns name ) ) data . setdefault ( name , type ( env ) ( ) ) data [ name ] [ 'id' ] = instance . id data [ name ] [ 'public dns name' ] = instance . public dns name if verbose : print ( 'Public DNS: %s' % instance . public dns name ) if env . vm elastic ip mappings and name in env . vm elastic ip mappings : data [ name ] [ 'ip' ] = env . vm elastic ip mappings [ name ] else : data [ name ] [ 'ip' ] = socket . gethostbyname ( instance . public dns name ) if int ( show ) : pprint ( data , indent = 4 ) return data elif env . vm type == KVM : #virsh list pass else : raise Not Implemented Error", "predictions": ["list all instances known to this api"], "references": ["retrieves all virtual machines instances in the current environment ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2313, "code": "def get or create ec2 key pair ( name = None , verbose = 1 ) : verbose = int ( verbose ) name = name or env . vm ec2 keypair name pem path = 'roles/%s/%s.pem' % ( env . ROLE , name ) conn = get ec2 connection ( ) kp = conn . get key pair ( name ) if kp : print ( 'Key pair %s already exists.' % name ) else : kp = conn . create key pair ( name ) open ( pem path , 'wb' ) . write ( kp . material ) os . system ( 'chmod 600 %s' % pem path ) print ( 'Key pair %s created.' % name ) #return kp return pem path", "predictions": ["get ec2 key pair pair"], "references": ["creates and saves an ec2 key pair to a local pem file ."], "bleu": 0.10259023253147191, "rouge_l": 0.3086003372681282}
{"id": 2314, "code": "def exists ( name = None , group = None , release = None , except release = None , verbose = 1 ) : verbose = int ( verbose ) instances = list instances ( name = name , group = group , release = release , except release = except release , verbose = verbose , show = verbose ) ret = bool ( instances ) if verbose : print ( '\\ninstance %s exist' % ( 'DOES' if ret else 'does NOT' ) ) #return ret return instances", "predictions": ["check if a service is available"], "references": ["determines if a virtual machine instance exists ."], "bleu": 0.20830666398386113, "rouge_l": 0.2785388127853881}
{"id": 2315, "code": "def get or create ( name = None , group = None , config = None , extra = 0 , verbose = 0 , backend opts = None ) : require ( 'vm type' , 'vm group' ) backend opts = backend opts or { } verbose = int ( verbose ) extra = int ( extra ) if config : config fn = common . find template ( config ) config = yaml . load ( open ( config fn ) ) env . update ( config ) env . vm type = ( env . vm type or '' ) . lower ( ) assert env . vm type , 'No VM type specified.' group = group or env . vm group assert group , 'No VM group specified.' ret = exists ( name = name , group = group ) if not extra and ret : if verbose : print ( 'VM %s:%s exists.' % ( name , group ) ) return ret today = datetime . date . today ( ) release = int ( '%i%02i%02i' % ( today . year , today . month , today . day ) ) if not name : existing instances = list instances ( group = group , release = release , verbose = verbose ) name = env . vm name template . format ( index = len ( existing instances ) + 1 ) if env . vm type == EC2 : return get or create ec2 instance ( name = name , group = group , release = release , verbose = verbose , backend opts = backend opts ) else : raise Not Implemented Error", "predictions": ["get a ec2 ec2 instance"], "references": ["creates a virtual machine instance ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2316, "code": "def delete ( name = None , group = None , release = None , except release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm type == EC2 : conn = get ec2 connection ( ) instances = list instances ( name = name , group = group , release = release , except release = except release , ) for instance name , instance data in instances . items ( ) : public dns name = instance data [ 'public dns name' ] print ( '\\n Deleting %s (%s)...' % ( instance name , instance data [ 'id' ] ) ) if not get dryrun ( ) : conn . terminate instances ( instance ids = [ instance data [ 'id' ] ] ) known hosts = os . path . expanduser ( '~/.ssh/known hosts' ) cmd = 'ssh-keygen -f \"%s\" -R %s' % ( known hosts , public dns name ) local or dryrun ( cmd ) else : raise Not Implemented Error", "predictions": ["delete one or more ec2 instance"], "references": ["permanently erase one or more vm instances from existence ."], "bleu": 0.21108303712651422, "rouge_l": 0.3588235294117647}
{"id": 2317, "code": "def get name ( ) : if env . vm type == EC2 : for instance in get all running ec2 instances ( ) : if env . host string == instance . public dns name : name = instance . tags . get ( env . vm name tag ) return name else : raise Not Implemented Error", "predictions": ["return the ec2 ec2 name"], "references": ["retrieves the instance name associated with the current host string ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 2318, "code": "def respawn ( name = None , group = None ) : if name is None : name = get name ( ) delete ( name = name , group = group ) instance = get or create ( name = name , group = group ) env . host string = instance . public dns name", "predictions": ["delete a host ."], "references": ["deletes and recreates one or more vm instances ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 2319, "code": "def deploy code ( self ) : assert self . genv . SITE , 'Site unspecified.' assert self . genv . ROLE , 'Role unspecified.' r = self . local renderer if self . env . exclusions : r . env . exclusions str = ' ' . join ( \"--exclude='%s'\" % for in self . env . exclusions ) r . local ( r . env . rsync command ) r . sudo ( 'chown -R {rsync chown user}:{rsync chown group} {rsync dst dir}' )", "predictions": ["deploy the local code to the local environment ."], "references": ["generates a rsync of all deployable code ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2320, "code": "def init env ( ) : env . ROLES DIR = ROLE DIR env . services = [ ] env . confirm deployment = False env . is local = None env . base config dir = '.' env . src dir = 'src' env . sites = { } env [ SITE ] = None env [ ROLE ] = None env . hosts retriever = None env . hosts retrievers = type ( env ) ( ) #'default':lambda hostname: hostname, env . hostname translator = 'default' env . hostname translators = type ( env ) ( ) env . hostname translators . default = lambda hostname : hostname env . default site = None env . available sites = [ ] env . available sites by host = { } env . disk usage command = \"df -H | grep -v E '^Filesystem|tmpfs|cdrom|none' | awk '{print $5 \" \" $1}'\" env . burlap data dir = '.burlap' env . setdefault ( 'roledefs' , { } ) env . setdefault ( 'roles' , [ ] ) env . setdefault ( 'hosts' , [ ] ) env . setdefault ( 'exclude hosts' , [ ] )", "predictions": ["sets up the initial environment ."], "references": ["populates the global env variables with custom default settings ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 2321, "code": "def create module ( name , code = None ) : if name not in sys . modules : sys . modules [ name ] = imp . new module ( name ) module = sys . modules [ name ] if code : print ( 'executing code for %s: %s' % ( name , code ) ) exec ( code in module . dict ) exec ( \"from %s import %s\" % ( name , '*' ) ) return module", "predictions": ["create a module from its modules ."], "references": ["dynamically creates a module with the given name ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 2322, "code": "def str to list ( s ) : if s is None : return [ ] elif isinstance ( s , ( tuple , list ) ) : return s elif not isinstance ( s , six . string types ) : raise Not Implemented Error ( 'Unknown type: %s' % type ( s ) ) return [ . strip ( ) . lower ( ) for in ( s or '' ) . split ( ',' ) if . strip ( ) ]", "predictions": ["convert a string to a list of strings"], "references": ["converts a string of comma delimited values and returns a list ."], "bleu": 0.16847111051295394, "rouge_l": 0.3860759493670886}
{"id": 2323, "code": "def get hosts retriever ( s = None ) : s = s or env . hosts retriever if not s : return env hosts retriever return str to callable ( s ) or env hosts retriever", "predictions": ["return a string for the given environment or none if not present ."], "references": ["given the function name looks up the method for dynamically retrieving host data ."], "bleu": 0.1112176984362864, "rouge_l": 0.1471652593486128}
{"id": 2324, "code": "def write temp file or dryrun ( content , * args , * * kwargs ) : dryrun = get dryrun ( kwargs . get ( 'dryrun' ) ) if dryrun : fd , tmp fn = tempfile . mkstemp ( ) os . remove ( tmp fn ) cmd run = 'local' cmd = 'cat <<EOT >> %s\\n%s\\n EOT' % ( tmp fn , content ) if BURLAP COMMAND PREFIX : print ( '%s %s: %s' % ( render command prefix ( ) , cmd run , cmd ) ) else : print ( cmd ) else : fd , tmp fn = tempfile . mkstemp ( ) fout = open ( tmp fn , 'w' ) fout . write ( content ) fout . close ( ) return tmp fn", "predictions": ["write a file to dryrun"], "references": ["writes the given content to a local temporary file ."], "bleu": 0.11943865131127647, "rouge_l": 0.2515463917525773}
{"id": 2325, "code": "def reboot or dryrun ( * args , * * kwargs ) : from fabric . state import connections verbose = get verbose ( ) dryrun = get dryrun ( kwargs . get ( 'dryrun' ) ) kwargs . setdefault ( 'wait' , 120 ) wait = int ( kwargs [ 'wait' ] ) command = kwargs . get ( 'command' , 'reboot' ) now = int ( kwargs . get ( 'now' , 0 ) ) print ( 'now:' , now ) if now : command += ' now' timeout = int ( kwargs . get ( 'timeout' , 30 ) ) reconnect hostname = kwargs . pop ( 'new hostname' , env . host string ) if 'dryrun' in kwargs : del kwargs [ 'dryrun' ] if dryrun : print ( '%s sudo: %s' % ( render command prefix ( ) , command ) ) else : if is local ( ) : if raw input ( 'reboot localhost now? ' ) . strip ( ) [ 0 ] . lower ( ) != 'y' : return attempts = int ( round ( float ( wait ) / float ( timeout ) ) ) with settings ( warn only = True ) : sudo ( command ) env . host string = reconnect hostname success = False for attempt in xrange ( attempts ) : if verbose : print ( 'Waiting for %s seconds, wait %i of %i' % ( timeout , attempt + 1 , attempts ) ) time . sleep ( timeout ) try : if verbose : print ( 'Reconnecting to:' , env . host string ) connections . connect ( env . host string ) with settings ( timeout = timeout ) : run ( 'echo hello' ) success = True break except Exception as e : print ( 'Exception:' , e ) if not success : raise Exception ( 'Reboot failed or took longer than %s seconds.' % wait )", "predictions": ["reboot or re - reboot or dryrun or dryrun ."], "references": ["an improved version of fabric . operations . reboot with better error handling ."], "bleu": 0.09351498865776114, "rouge_l": 0.16180371352785147}
{"id": 2326, "code": "def get last modified timestamp ( path , ignore = None ) : ignore = ignore or [ ] if not isinstance ( path , six . string types ) : return ignore str = '' if ignore : assert isinstance ( ignore , ( tuple , list ) ) ignore str = ' ' . join ( \"! -name '%s'\" % for in ignore ) cmd = 'find \"' + path + '\" ' + ignore str + ' -type f -printf \"%T@ %p\\n\" | sort -n | tail -1 | cut -f 1 -d \" \"' #'find '+path+' -type f -printf \"%T@ %p\\n\" | sort -n | tail -1 | cut -d \" \" -f1 ret = subprocess . check output ( cmd , shell = True ) try : ret = round ( float ( ret ) , 2 ) except Value Error : return return ret", "predictions": ["get the last modified timestamp"], "references": ["recursively finds the most recent timestamp in the given directory ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 2327, "code": "def get packager ( ) : import warnings warnings . filterwarnings ( \"ignore\" , category = Deprecation Warning ) common packager = get rc ( 'common packager' ) if common packager : return common packager #TODO:cache result by current env.host string so we can handle multiple hosts with different O Ses with settings ( warn only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = run ( 'cat /etc/fedora-release' ) if ret . succeeded : common packager = YUM else : ret = run ( 'cat /etc/lsb-release' ) if ret . succeeded : common packager = APT else : for pn in PACKAGERS : ret = run ( 'which %s' % pn ) if ret . succeeded : common packager = pn break if not common packager : raise Exception ( 'Unable to determine packager.' ) set rc ( 'common packager' , common packager ) return common packager", "predictions": ["returns a list of packager objects for the current vlan"], "references": ["returns the packager detected on the remote system ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 2328, "code": "def get os version ( ) : import warnings warnings . filterwarnings ( \"ignore\" , category = Deprecation Warning ) common os version = get rc ( 'common os version' ) if common os version : return common os version with settings ( warn only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = run or local ( 'cat /etc/lsb-release' ) if ret . succeeded : return OS ( type = LINUX , distro = UBUNTU , release = re . findall ( r'DISTRIB RELEASE=([0-9\\.]+)' , ret ) [ 0 ] ) ret = run or local ( 'cat /etc/debian version' ) if ret . succeeded : return OS ( type = LINUX , distro = DEBIAN , release = re . findall ( r'([0-9\\.]+)' , ret ) [ 0 ] ) ret = run or local ( 'cat /etc/fedora-release' ) if ret . succeeded : return OS ( type = LINUX , distro = FEDORA , release = re . findall ( r'release ([0-9]+)' , ret ) [ 0 ] ) raise Exception ( 'Unable to determine OS version.' )", "predictions": ["determine the os version number from the os"], "references": ["returns a named tuple describing the operating system on the remote host ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 2329, "code": "def render to string ( template , extra = None ) : from jinja2 import Template extra = extra or { } final fqfn = find template ( template ) assert final fqfn , 'Template not found: %s' % template template content = open ( final fqfn , 'r' ) . read ( ) t = Template ( template content ) if extra : context = env . copy ( ) context . update ( extra ) else : context = env rendered content = t . render ( * * context ) rendered content = rendered content . replace ( '&quot;' , '\"' ) return rendered content", "predictions": ["render the given template to the given template ."], "references": ["renders the given template to a string ."], "bleu": 0.4111336169005197, "rouge_l": 0.594541910331384}
{"id": 2330, "code": "def iter sites ( sites = None , site = None , renderer = None , setter = None , no secure = False , verbose = None ) : if verbose is None : verbose = get verbose ( ) hostname = get current hostname ( ) target sites = env . available sites by host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer #or render remote paths env default = save env ( ) for site , site data in sorted ( sites ) : if no secure and site . endswith ( ' secure' ) : continue if target sites is None : pass else : assert isinstance ( target sites , ( tuple , list ) ) if site not in target sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % site ) continue env . update ( env default ) env . update ( env . sites . get ( site , { } ) ) env . SITE = site if callable ( renderer ) : renderer ( ) if setter : setter ( site ) yield site , site data env . update ( env default ) added keys = set ( env ) . difference ( env default ) for key in added keys : if key . startswith ( ' ' ) : continue del env [ key ]", "predictions": ["return a list of valid sites ."], "references": ["iterates over sites safely setting environment variables for each site ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2331, "code": "def get hosts for site ( site = None ) : site = site or env . SITE hosts = set ( ) for hostname , sites in six . iteritems ( env . available sites by host ) : for site in sites : if site == site : host ip = get host ip ( hostname ) if host ip : hosts . add ( host ip ) break return list ( hosts )", "predictions": ["return a list of hosts that have a site ."], "references": ["returns a list of hosts that have been configured to support the given site ."], "bleu": 0.3830576146513924, "rouge_l": 0.6177215189873417}
{"id": 2332, "code": "def collect genv ( self , include local = True , include global = True ) : e = type ( self . genv ) ( ) if include global : e . update ( self . genv ) if include local : for k , v in self . lenv . items ( ) : e [ '%s %s' % ( self . obj . name . lower ( ) , k ) ] = v return e", "predictions": ["collect all the genv"], "references": ["returns a copy of the global environment with all the local variables copied back into it ."], "bleu": 0.018335190851298155, "rouge_l": 0.17134831460674158}
{"id": 2333, "code": "def capture bash ( self ) : class Capture ( object ) : def init ( self , satchel ) : self . satchel = satchel self . dryrun = self . satchel . dryrun self . satchel . dryrun = 1 begincap ( ) self . stdout = sys . stdout self . stderr = sys . stderr self . stdout = sys . stdout = String IO ( ) self . stderr = sys . stderr = String IO ( ) def enter ( self ) : return self def exit ( self , type , value , traceback ) : endcap ( ) self . satchel . dryrun = self . dryrun sys . stdout = self . stdout sys . stderr = self . stderr return Capture ( self )", "predictions": ["refreshes the python function for the current python version ."], "references": ["context manager that hides the command prefix and activates dryrun to capture all following task commands to their equivalent bash outputs ."], "bleu": 0.04201899298302553, "rouge_l": 0.11708253358925146}
{"id": 2334, "code": "def register ( self ) : self . set defaults ( ) all satchels [ self . name . upper ( ) ] = self manifest recorder [ self . name ] = self . record manifest if self . required system packages : required system packages [ self . name . upper ( ) ] = self . required system packages", "predictions": ["add the record to the record"], "references": ["adds this satchel to the global registeries for fast lookup from other satchels ."], "bleu": 0.07663173913867023, "rouge_l": 0.18654434250764526}
{"id": 2335, "code": "def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env prefix ) : del env [ k ] try : del all satchels [ self . name . upper ( ) ] except Key Error : pass try : del manifest recorder [ self . name ] except Key Error : pass try : del manifest deployers [ self . name . upper ( ) ] except Key Error : pass try : del manifest deployers befores [ self . name . upper ( ) ] except Key Error : pass try : del required system packages [ self . name . upper ( ) ] except Key Error : pass", "predictions": ["unregister the manifest ."], "references": ["removes this satchel from global registeries ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2336, "code": "def get tasks ( self ) : tasks = set ( self . tasks ) #DEPRECATED for name in dir ( self ) : if isinstance ( getattr ( type ( self ) , name , None ) , property ) : continue attr = getattr ( self , name ) if hasattr ( attr , ' call ' ) and getattr ( attr , 'is task' , False ) : tasks . add ( name ) return sorted ( tasks )", "predictions": ["disk to disk local and all tasks tasks"], "references": ["returns an ordered list of all task names ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2337, "code": "def local renderer ( self ) : if not self . local renderer : r = self . create local renderer ( ) self . local renderer = r return self . local renderer", "predictions": ["returns the tunnel renderer ."], "references": ["retrieves the cached local renderer ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 2338, "code": "def all other enabled satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )", "predictions": ["sudo of from from from from from from from from from from locals . . . ."], "references": ["returns a dictionary of satchels used in the current configuration excluding ourselves ."], "bleu": 0.07994607499472013, "rouge_l": 0.1366181410974244}
{"id": 2339, "code": "def lenv ( self ) : env = type ( env ) ( ) for k , v in six . iteritems ( env ) : if k . startswith ( self . name + ' ' ) : env [ k [ len ( self . name ) + 1 : ] ] = v return env", "predictions": ["return environment variables run with the environment"], "references": ["returns a version of env filtered to only include the variables in our namespace ."], "bleu": 0.06555660318294844, "rouge_l": 0.08531468531468532}
{"id": 2340, "code": "def param changed to ( self , key , to value , from value = None ) : last value = getattr ( self . last manifest , key ) current value = self . current manifest . get ( key ) if from value is not None : return last value == from value and current value == to value return last value != to value and current value == to value", "predictions": ["enables a value only if the value is not exists ."], "references": ["returns true if the given parameter with name key has transitioned to the given value ."], "bleu": 0.10811583508674764, "rouge_l": 0.28672150411280845}
{"id": 2341, "code": "def reboot or dryrun ( self , * args , * * kwargs ) : warnings . warn ( 'Use self.run() instead.' , Deprecation Warning , stacklevel = 2 ) self . reboot ( * args , * * kwargs )", "predictions": ["what should be called at dryrun renderer"], "references": ["reboots the server and waits for it to come back ."], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 2342, "code": "def set site specifics ( self , site ) : r = self . local renderer site data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set site specifics.data:' ) pprint ( site data , indent = 4 ) local ns = { } for k , v in list ( site data . items ( ) ) : if k . startswith ( self . name + ' ' ) : k = k [ len ( self . name + ' ' ) : ] local ns [ k ] = v del site data [ k ] r . env . update ( local ns ) r . env . update ( site data )", "predictions": ["init the site virtual environment for the local print"], "references": ["loads settings for the target site ."], "bleu": 0.19960198807747329, "rouge_l": 0.2557651991614256}
{"id": 2343, "code": "def get package list ( self ) : os version = self . os version self . vprint ( 'os version:' , os version ) req packages1 = self . required system packages if req packages1 : deprecation ( 'The required system packages attribute is deprecated, ' 'use the packager system packages property instead.' ) req packages2 = self . packager system packages patterns = [ ( os version . type , os version . distro , os version . release ) , ( os version . distro , os version . release ) , ( os version . type , os version . distro ) , ( os version . distro , ) , os version . distro , ] self . vprint ( 'req packages1:' , req packages1 ) self . vprint ( 'req packages2:' , req packages2 ) package list = None found = False for pattern in patterns : self . vprint ( 'pattern:' , pattern ) for req packages in ( req packages1 , req packages2 ) : if pattern in req packages : package list = req packages [ pattern ] found = True break if not found : print ( 'Warning: No operating system pattern found for %s' % ( os version , ) ) self . vprint ( 'package list:' , package list ) return package list", "predictions": ["requirements for all as a requirements"], "references": ["returns a list of all required packages ."], "bleu": 0.17516432701748888, "rouge_l": 0.13926940639269406}
{"id": 2344, "code": "def record manifest ( self ) : manifest = get component settings ( prefixes = [ self . name ] ) for template in self . get templates ( ) : if template and template . startswith ( '{' ) and template . endswith ( '}' ) : template = self . env [ template [ 1 : - 1 ] ] if not template : continue if template . startswith ( '%s/' % self . name ) : fqfn = self . find template ( template ) else : fqfn = self . find template ( '%s/%s' % ( self . name , template ) ) assert fqfn , 'Unable to find template: %s/%s' % ( self . name , template ) manifest [ ' %s' % template ] = get file hash ( fqfn ) for tracker in self . get trackers ( ) : manifest [ ' tracker %s' % tracker . get natural key hash ( ) ] = tracker . get thumbprint ( ) if self . verbose : pprint ( manifest , indent = 4 ) return manifest", "predictions": ["returns a instances of the instances of the ."], "references": ["returns a dictionary representing a serialized state of the service ."], "bleu": 0.1957494756053795, "rouge_l": 0.4911433172302737}
{"id": 2345, "code": "def has changes ( self ) : lm = self . last manifest for tracker in self . get trackers ( ) : last thumbprint = lm [ ' tracker %s' % tracker . get natural key hash ( ) ] if tracker . is changed ( last thumbprint ) : return True return False", "predictions": ["if the 1 get is or not we have a or not = or not ."], "references": ["returns true if at least one tracker detects a change ."], "bleu": 0.09147827112247602, "rouge_l": 0.22989949748743718}
{"id": 2346, "code": "def configure ( self ) : lm = self . last manifest for tracker in self . get trackers ( ) : self . vprint ( 'Checking tracker:' , tracker ) last thumbprint = lm [ ' tracker %s' % tracker . get natural key hash ( ) ] self . vprint ( 'last thumbprint:' , last thumbprint ) has changed = tracker . is changed ( last thumbprint ) self . vprint ( 'Tracker changed:' , has changed ) if has changed : self . vprint ( 'Change detected!' ) tracker . act ( )", "predictions": ["exists all actions ."], "references": ["the standard method called to apply functionality when the manifest changes ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2347, "code": "def user exists ( name ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = run as pg ( '''psql -t -A -c \"SELECT COUNT(*) FROM pg user WHERE usename = '%(name)s';\"''' % locals ( ) ) return ( res == \"1\" )", "predictions": ["check if a get get a get get"], "references": ["check if a postgresql user exists ."], "bleu": 0.2984745896009823, "rouge_l": 0.4048672566371681}
{"id": 2348, "code": "def write pgpass ( self , name = None , site = None , use sudo = 0 , root = 0 ) : r = self . database renderer ( name = name , site = site ) root = int ( root ) use sudo = int ( use sudo ) r . run ( 'touch {pgpass path}' ) if '~' in r . env . pgpass path : r . run ( 'chmod {pgpass chmod} {pgpass path}' ) else : r . sudo ( 'chmod {pgpass chmod} {pgpass path}' ) if root : r . env . shell username = r . env . get ( 'db root username' , 'postgres' ) r . env . shell password = r . env . get ( 'db root password' , 'password' ) else : r . env . shell username = r . env . db user r . env . shell password = r . env . db password r . append ( '{db host}:{port}:*:{shell username}:{shell password}' , r . env . pgpass path , use sudo = use sudo )", "predictions": ["delete a items from the database"], "references": ["write the file used to store login credentials for postgresql ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 2349, "code": "def exists ( self , name = 'default' , site = None , use root = False ) : r = self . database renderer ( name = name , site = site ) if int ( use root ) : kwargs = dict ( db user = r . env . get ( 'db root username' , 'postgres' ) , db password = r . env . get ( 'db root password' , 'password' ) , db host = r . env . db host , db name = r . env . db name , ) r . env . update ( kwargs ) if r . env . db password : self . write pgpass ( name = name , root = use root ) ret = None with settings ( warn only = True ) : ret = r . run ( 'psql --username={db user} --host={db host} -l ' '| grep {db name} | wc -l' ) if ret is not None : if 'password authentication failed' in ret : ret = False else : ret = int ( ret ) >= 1 if ret is not None : print ( '%s database on site %s %s exist' % ( name , self . genv . SITE , 'DOES' if ret else 'DOES NOT' ) ) return ret", "predictions": ["check if a running running running running running running"], "references": ["returns true if a database with the given name exists . false otherwise ."], "bleu": 0.10657503067399117, "rouge_l": 0.1673525377229081}
{"id": 2350, "code": "def load table ( self , table name , src , dst = 'localhost' , name = None , site = None ) : #TODO: incomplete r = self . database renderer ( name = name , site = site ) r . env . table name = table name r . run ( 'psql --user={dst db user} --host={dst db host} --command=\"DROP TABLE IF EXISTS {table name} CASCADE;\"' ) r . run ( 'pg dump -t {table name} --user={dst db user} --host={dst db host} | psql --user={src db user} --host={src db host}' )", "predictions": ["load table table table"], "references": ["directly transfers a table between two databases ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2351, "code": "def interfaces ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : if is file ( '/usr/sbin/dladm' ) : res = run ( '/usr/sbin/dladm show-link' ) else : res = sudo ( '/sbin/ifconfig -s' ) return [ line . split ( ' ' ) [ 0 ] for line in res . splitlines ( ) [ 1 : ] ]", "predictions": ["get the list of deploy deploy % ."], "references": ["get the list of network interfaces . will return all datalinks on smartos ."], "bleu": 0.22066035619387875, "rouge_l": 0.43323863636363635}
{"id": 2352, "code": "def record manifest ( self ) : data = { } data [ 'required packages' ] = self . install required ( type = SYSTEM , verbose = False , list only = True ) data [ 'required packages' ] . sort ( ) data [ 'custom packages' ] = self . install custom ( list only = True ) data [ 'custom packages' ] . sort ( ) data [ 'repositories' ] = self . get repositories ( ) return data", "predictions": ["init a init env"], "references": ["returns a dictionary representing a serialized state of the service ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 2353, "code": "def update ( self ) : packager = self . packager if packager == APT : self . sudo ( 'DEBIAN FRONTEND=noninteractive apt-get -yq update' ) elif packager == YUM : self . sudo ( 'yum update' ) else : raise Exception ( 'Unknown packager: %s' % ( packager , ) )", "predictions": ["create the post according to the stomp server . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["preparse the packaging system for installations ."], "bleu": 0.04317900023606586, "rouge_l": 0.11879259980525803}
{"id": 2354, "code": "def install apt ( self , fn = None , package name = None , update = 0 , list only = 0 ) : r = self . local renderer assert self . genv [ ROLE ] apt req fqfn = fn or ( self . env . apt requirments fn and self . find template ( self . env . apt requirments fn ) ) if not apt req fqfn : return [ ] assert os . path . isfile ( apt req fqfn ) lines = list ( self . env . apt packages or [ ] ) for in open ( apt req fqfn ) . readlines ( ) : if . strip ( ) and not . strip ( ) . startswith ( '#' ) and ( not package name or . strip ( ) == package name ) : lines . extend ( pkg . strip ( ) for pkg in . split ( ' ' ) if pkg . strip ( ) ) if list only : return lines tmp fn = r . write temp file ( '\\n' . join ( lines ) ) apt req fqfn = tmp fn if not self . genv . is local : r . put ( local path = tmp fn , remote path = tmp fn ) apt req fqfn = self . genv . put remote path r . sudo ( 'DEBIAN FRONTEND=noninteractive apt-get -yq update --fix-missing' ) r . sudo ( 'DEBIAN FRONTEND=noninteractive apt-get -yq install `cat \"%s\" | tr \"\\\\n\" \" \"`' % apt req fqfn )", "predictions": ["str to str with to to to to to str"], "references": ["installs system packages listed in apt - requirements . txt ."], "bleu": 0.0959156018869021, "rouge_l": 0.0}
{"id": 2355, "code": "def install yum ( self , fn = None , package name = None , update = 0 , list only = 0 ) : assert self . genv [ ROLE ] yum req fn = fn or self . find template ( self . genv . yum requirments fn ) if not yum req fn : return [ ] assert os . path . isfile ( yum req fn ) update = int ( update ) if list only : return [ . strip ( ) for in open ( yum req fn ) . readlines ( ) if . strip ( ) and not . strip . startswith ( '#' ) and ( not package name or . strip ( ) == package name ) ] if update : self . sudo or dryrun ( 'yum update --assumeyes' ) if package name : self . sudo or dryrun ( 'yum install --assumeyes %s' % package name ) else : if self . genv . is local : self . put or dryrun ( local path = yum req fn ) yum req fn = self . genv . put remote fn self . sudo or dryrun ( 'yum install --assumeyes $(cat %(yum req fn)s)' % yum req fn )", "predictions": ["get hosts from github"], "references": ["installs system packages listed in yum - requirements . txt ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 2356, "code": "def install required ( self , type = None , service = None , list only = 0 , * * kwargs ) : r = self . local renderer list only = int ( list only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE TYPES for type in types : if type == SYSTEM : content = '\\n' . join ( self . list required ( type = type , service = service ) ) if list only : lst . extend ( for in content . split ( '\\n' ) if . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install custom ( fn = fn ) else : raise Not Implemented Error return lst", "predictions": ["write temp tmp to server"], "references": ["installs system packages listed as required by services this host uses ."], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 2357, "code": "def uninstall blacklisted ( self ) : from burlap . system import distrib family blacklisted packages = self . env . blacklisted packages if not blacklisted packages : print ( 'No blacklisted packages.' ) return else : family = distrib family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted packages ) ) else : raise Not Implemented Error ( 'Unknown family: %s' % family )", "predictions": ["reboot or downloads or disk . . ."], "references": ["uninstalls all blacklisted packages ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2358, "code": "def deploy ( self , site = None ) : r = self . local renderer self . deploy logrotate ( ) cron crontabs = [ ] for site , site data in self . iter sites ( site = site ) : r . env . cron stdout log = r . format ( r . env . stdout log template ) r . env . cron stderr log = r . format ( r . env . stderr log template ) r . sudo ( 'touch {cron stdout log}' ) r . sudo ( 'touch {cron stderr log}' ) r . sudo ( 'sudo chown {user}:{user} {cron stdout log}' ) r . sudo ( 'sudo chown {user}:{user} {cron stderr log}' ) if self . verbose : print ( 'site:' , site , file = sys . stderr ) print ( 'env.crontabs selected:' , self . env . crontabs selected , file = sys . stderr ) for selected crontab in self . env . crontabs selected : lines = self . env . crontabs available . get ( selected crontab , [ ] ) if self . verbose : print ( 'lines:' , lines , file = sys . stderr ) for line in lines : cron crontabs . append ( r . format ( line ) ) if not cron crontabs : return cron crontabs = self . env . crontab headers + cron crontabs cron crontabs . append ( '\\n' ) r . env . crontabs rendered = '\\n' . join ( cron crontabs ) fn = self . write to file ( content = r . env . crontabs rendered ) print ( 'fn:' , fn ) r . env . put remote path = r . put ( local path = fn ) if isinstance ( r . env . put remote path , ( tuple , list ) ) : r . env . put remote path = r . env . put remote path [ 0 ] r . sudo ( 'crontab -u {cron user} {put remote path}' )", "predictions": ["get all selected crontab crontab"], "references": ["writes entire crontab to the host ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2359, "code": "def configure users ( self , site = None , full = 0 , only data = 0 ) : site = site or ALL full = int ( full ) if full and not only data : packager = self . get satchel ( 'packager' ) packager . install required ( type = SYSTEM , service = self . name ) r = self . local renderer params = self . get user vhosts ( site = site ) with settings ( warn only = True ) : self . add admin user ( ) params = sorted ( list ( params ) ) if not only data : for user , password , vhost in params : r . env . broker user = user r . env . broker password = password r . env . broker vhost = vhost with settings ( warn only = True ) : r . sudo ( 'rabbitmqctl add user {broker user} {broker password}' ) r . sudo ( 'rabbitmqctl add vhost {broker vhost}' ) r . sudo ( 'rabbitmqctl set permissions -p {broker vhost} {broker user} \".*\" \".*\" \".*\"' ) r . sudo ( 'rabbitmqctl set permissions -p {broker vhost} {admin username} \".*\" \".*\" \".*\"' ) return params", "predictions": ["get list of packager and ret instances . ."], "references": ["installs and configures rabbitmq ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 2360, "code": "def record manifest ( self ) : data = super ( Rabbit MQ Satchel , self ) . record manifest ( ) params = sorted ( list ( self . get user vhosts ( ) ) ) data [ 'rabbitmq all site vhosts' ] = params data [ 'sites' ] = list ( self . genv . sites or [ ] ) return data", "predictions": ["returns a get list of unique hide sites and their models ."], "references": ["returns a dictionary representing a serialized state of the service ."], "bleu": 0.15537125692760353, "rouge_l": 0.3505747126436781}
{"id": 2361, "code": "def iter dict differences ( a , b ) : common keys = set ( a ) . union ( b ) for k in common keys : a value = a . get ( k ) b value = b . get ( k ) if a value != b value : yield k , ( a value , b value )", "predictions": ["iterate over all string string string string string import template import"], "references": ["returns a generator yielding all the keys that have values that differ between each dictionary ."], "bleu": 0.0723014165286922, "rouge_l": 0.07168037602820211}
{"id": 2362, "code": "def get component order ( component names ) : assert isinstance ( component names , ( tuple , list ) ) component dependences = { } for name in component names : deps = set ( manifest deployers befores . get ( name , [ ] ) ) deps = deps . intersection ( component names ) component dependences [ name ] = deps component order = list ( topological sort ( component dependences . items ( ) ) ) return component order", "predictions": ["six out a target order"], "references": ["given a list of components re - orders them according to inter - component dependencies so the most depended upon are first ."], "bleu": 0.007459706470022542, "rouge_l": 0.06400839454354669}
{"id": 2363, "code": "def get deploy funcs ( components , current thumbprint , previous thumbprint , preview = False ) : for component in components : funcs = manifest deployers . get ( component , [ ] ) for func name in funcs : #TODO:remove this after burlap.* naming prefix bug fixed if func name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func name ) continue takes diff = manifest deployers takes diff . get ( func name , False ) func = resolve deployer ( func name ) current = current thumbprint . get ( component ) last = previous thumbprint . get ( component ) if takes diff : yield func name , partial ( func , last = last , current = current ) else : yield func name , partial ( func )", "predictions": ["hosts that components and creates a hosts object"], "references": ["returns a generator yielding the named functions needed for a deployment ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 2364, "code": "def manifest filename ( self ) : r = self . local renderer tp fn = r . format ( r . env . data dir + '/manifest.yaml' ) return tp fn", "predictions": ["the collect collect filename filename"], "references": ["returns the path to the manifest file ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2365, "code": "def lock ( self ) : self . init ( ) r = self . local renderer if self . file exists ( r . env . lockfile path ) : raise exceptions . Abort Deployment ( 'Lock file %s exists. Perhaps another deployment is currently underway?' % r . env . lockfile path ) else : self . vprint ( 'Locking %s.' % r . env . lockfile path ) r . env . hostname = socket . gethostname ( ) r . run or local ( 'echo \"{hostname}\" > {lockfile path}' )", "predictions": ["capture the init . . . . . ."], "references": ["marks the remote server as currently being deployed to ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 2366, "code": "def unlock ( self ) : self . init ( ) r = self . local renderer if self . file exists ( r . env . lockfile path ) : self . vprint ( 'Unlocking %s.' % r . env . lockfile path ) r . run or local ( 'rm -f {lockfile path}' )", "predictions": ["register the satchels ."], "references": ["unmarks the remote server as currently being deployed to ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 2367, "code": "def get component funcs ( self , components = None ) : current tp = self . get current thumbprint ( components = components ) or { } previous tp = self . get previous thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous tp , indent = 4 ) differences = list ( iter dict differences ( current tp , previous tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component order = get component order ( [ k for k , ( , ) in differences ] ) if self . verbose : print ( 'component order:' ) pprint ( component order , indent = 4 ) plan funcs = list ( get deploy funcs ( component order , current tp , previous tp ) ) return component order , plan funcs", "predictions": ["name of all component"], "references": ["calculates the components functions that need to be executed for a deployment ."], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 2368, "code": "def preview ( self , components = None , ask = 0 ) : ask = int ( ask ) self . init ( ) component order , plan funcs = self . get component funcs ( components = components ) print ( '\\n%i changes found for host %s.\\n' % ( len ( component order ) , self . genv . host string ) ) if component order and plan funcs : if self . verbose : print ( 'These components have changed:\\n' ) for component in sorted ( component order ) : print ( ( ' ' * 4 ) + component ) print ( 'Deployment plan for host %s:\\n' % self . genv . host string ) for func name , in plan funcs : print ( success str ( ( ' ' * 4 ) + func name ) ) if component order : print ( ) if ask and self . genv . host string == self . genv . hosts [ - 1 ] : if component order : if not raw input ( 'Begin deployment? [yn] ' ) . strip ( ) . lower ( ) . startswith ( 'y' ) : sys . exit ( 0 ) else : sys . exit ( 0 )", "predictions": ["print a preview of plan components"], "references": ["inspects differences between the last deployment and the current code state ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 2369, "code": "def push ( self , components = None , yes = 0 ) : from burlap import notifier service = self . get satchel ( 'service' ) self . lock ( ) try : yes = int ( yes ) if not yes : if self . genv . host string == self . genv . hosts [ 0 ] : execute ( partial ( self . preview , components = components , ask = 1 ) ) notifier . notify pre deployment ( ) component order , plan funcs = self . get component funcs ( components = components ) service . pre deploy ( ) for func name , plan func in plan funcs : print ( 'Executing %s...' % func name ) plan func ( ) self . fake ( components = components ) service . post deploy ( ) notifier . notify post deployment ( ) finally : self . unlock ( )", "predictions": ["push a deployment to the pool"], "references": ["executes all satchel configurators to apply pending changes to the server ."], "bleu": 0.10694820729788418, "rouge_l": 0.20962199312714777}
{"id": 2370, "code": "def get thumbprint ( self ) : d = { } settings = dj . get settings ( ) for name in self . names : d [ name ] = getattr ( settings , name ) return d", "predictions": ["return a dict of all settings"], "references": ["calculates the current thumbprint of the item being tracked ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2371, "code": "def get settings ( self , site = None , role = None ) : r = self . local renderer stdout = sys . stdout stderr = sys . stderr if not self . verbose : sys . stdout = String IO ( ) sys . stderr = String IO ( ) try : sys . path . insert ( 0 , r . env . src dir ) tmp site = self . genv . SITE if site and site . endswith ( ' secure' ) : site = site [ : - 7 ] site = site or self . genv . SITE or self . genv . default site self . set site ( site ) tmp role = self . genv . ROLE if role : self . set role ( role ) try : if r . env . delete module with prefixes : for name in sorted ( sys . modules ) : for prefix in r . env . delete module with prefixes : if name . startswith ( prefix ) : if self . verbose : print ( 'Deleting module %s prior to re-import.' % name ) del sys . modules [ name ] break for name in list ( sys . modules ) : for s in r . env . delete module containing : if s in name : del sys . modules [ name ] break if r . env . settings module in sys . modules : del sys . modules [ r . env . settings module ] #TODO:fix r.env.settings module not loading from settings? if 'django settings module' in r . genv : r . env . settings module = r . genv . django settings module else : r . env . settings module = r . env . settings module or r . genv . dj settings module if self . verbose : print ( 'r.env.settings module:' , r . env . settings module , r . format ( r . env . settings module ) ) module = import module ( r . format ( r . env . settings module ) ) if site : assert site == module . SITE , 'Unable to set SITE to \"%s\" Instead it is set to \"%s\".' % ( site , module . SITE ) import imp imp . reload ( module ) except Import Error as e : print ( 'Warning: Could not import settings for site \"%s\": %s' % ( site , e ) , file = stdout ) traceback . print exc ( file = stdout ) return finally : if tmp site : self . set site ( tmp site ) if tmp role : self . set role ( tmp role ) finally : sys . stdout = stdout sys . stderr = stderr sys . path . remove ( r . env . src dir ) return module", "predictions": ["tries to load settings from the user s settings"], "references": ["retrieves the django settings dictionary ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2372, "code": "def createsuperuser ( self , username = 'admin' , email = None , password = None , site = None ) : r = self . local renderer site = site or self . genv . SITE self . set site specifics ( site ) options = [ '--username=%s' % username ] if email : options . append ( '--email=%s' % email ) if password : options . append ( '--password=%s' % password ) r . env . options str = ' ' . join ( options ) if self . is local : r . env . project dir = r . env . local project dir r . genv . SITE = r . genv . SITE or site r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project dir}; {manage cmd} {createsuperuser cmd} {options str}' )", "predictions": ["run a security user"], "references": ["runs the django createsuperuser management command ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 2373, "code": "def manage ( self , cmd , * args , * * kwargs ) : r = self . local renderer environs = kwargs . pop ( 'environs' , '' ) . strip ( ) if environs : environs = ' ' . join ( 'export %s=%s;' % tuple ( . split ( '=' ) ) for in environs . split ( ',' ) ) environs = ' ' + environs + ' ' r . env . cmd = cmd r . env . SITE = r . genv . SITE or r . genv . default site r . env . args = ' ' . join ( map ( str , args ) ) r . env . kwargs = ' ' . join ( ( '--%s' % k if v in ( True , 'True' ) else '--%s=%s' % ( k , v ) ) for k , v in kwargs . items ( ) ) r . env . environs = environs if self . is local : r . env . project dir = r . env . local project dir r . run or local ( 'export SITE={SITE}; export ROLE={ROLE};{environs} cd {project dir}; {manage cmd} {cmd} {args} {kwargs}' )", "predictions": ["manage local project with local project ."], "references": ["a generic wrapper around django s manage command ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 2374, "code": "def load django settings ( self ) : r = self . local renderer env = { } save vars = [ 'ALLOW CELERY' , 'DJANGO SETTINGS MODULE' ] for var name in save vars : env [ var name ] = os . environ . get ( var name ) try : if r . env . local project dir : sys . path . insert ( 0 , r . env . local project dir ) #TODO:remove this once bug in django-celery has been fixed os . environ [ 'ALLOW CELERY' ] = '0' os . environ [ 'DJANGO SETTINGS MODULE' ] = r . format ( r . env . settings module ) try : import django django . setup ( ) except Attribute Error : pass settings = self . get settings ( ) try : from django . contrib import staticfiles from django . conf import settings as settings if settings is not None : for k , v in settings . dict . items ( ) : setattr ( settings , k , v ) else : raise Import Error except ( Import Error , Runtime Error ) : print ( 'Unable to load settings.' ) traceback . print exc ( ) finally : for var name , var value in env . items ( ) : if var value is None : del os . environ [ var name ] else : os . environ [ var name ] = var value return settings", "predictions": ["load django settings from disk"], "references": ["loads django settings for the current site and sets them so django internals can be run ."], "bleu": 0.03259533364576092, "rouge_l": 0.1655359565807327}
{"id": 2375, "code": "def syncdb ( self , site = None , all = 0 , database = None , ignore errors = 1 ) : r = self . local renderer ignore errors = int ( ignore errors ) post south = self . version tuple >= ( 1 , 7 , 0 ) use run syncdb = self . version tuple >= ( 1 , 9 , 0 ) r . env . db syncdb all flag = '--all' if int ( all ) else '' r . env . db syncdb database = '' if database : r . env . db syncdb database = ' --database=%s' % database if self . is local : r . env . project dir = r . env . local project dir site = site or self . genv . SITE for site , site data in r . iter unique databases ( site = site ) : r . env . SITE = site with self . settings ( warn only = ignore errors ) : if post south : if use run syncdb : r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project dir}; ' '{manage cmd} migrate --run-syncdb --noinput {db syncdb database}' ) else : r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project dir}; ' '{manage cmd} migrate --noinput {db syncdb database}' ) else : r . run or local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project dir}; ' '{manage cmd} syncdb --noinput {db syncdb all flag} {db syncdb database}' )", "predictions": ["syncdb to the database"], "references": ["runs the standard django syncdb command for one or more sites ."], "bleu": 0.0538140946637381, "rouge_l": 0.11466165413533834}
{"id": 2376, "code": "def database renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default db name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . database renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get database defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection handler:' , d . connection handler ) if d . connection handler == CONNECTION HANDLER DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set db ( name = name , site = site , role = role ) d = dj . local renderer . collect genv ( include local = True , include global = False ) for k , v in d . items ( ) : if k . startswith ( 'dj db ' ) : d [ k [ 3 : ] ] = v del d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( d ) d . update ( d ) elif d . connection handler and d . connection handler . startswith ( CONNECTION HANDLER CUSTOM + ':' ) : callable str = d . connection handler [ len ( CONNECTION HANDLER CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % callable str ) d = str to callable ( callable str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( d ) d . update ( d ) r = Local Renderer ( self , lenv = d ) self . set root login ( r ) self . database renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . database renderers [ key ]", "predictions": ["build a database renderer for the database"], "references": ["renders local settings for a specific database ."], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 2377, "code": "def get free space ( self ) : cmd = \"df -k | grep -v E '^Filesystem|tmpfs|cdrom|none|udev|cgroup' | awk '{ print($1 \\\" \\\" $4 }'\" lines = [ for in self . run ( cmd ) . strip ( ) . split ( '\\n' ) if . startswith ( '/' ) ] assert len ( lines ) == 1 , 'Ambiguous devices: %s' % str ( lines ) device , kb = lines [ 0 ] . split ( ' ' ) free space = int ( kb ) * 1024 self . vprint ( 'free space (bytes):' , free space ) return free space", "predictions": ["get the free space space"], "references": ["return free space in bytes ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 2378, "code": "def load db set ( self , name , r = None ) : r = r or self db set = r . genv . db sets . get ( name , { } ) r . genv . update ( db set )", "predictions": ["load db set from database ."], "references": ["loads database parameters from a specific named set ."], "bleu": 0.16847111051295394, "rouge_l": 0.2573839662447257}
{"id": 2379, "code": "def loadable ( self , src , dst ) : from fabric import state from fabric . task utils import crawl src task = crawl ( src , state . commands ) assert src task , 'Unknown source role: %s' % src dst task = crawl ( dst , state . commands ) assert dst task , 'Unknown destination role: %s' % src src task ( ) env . host string = env . hosts [ 0 ] src size bytes = self . get size ( ) dst task ( ) env . host string = env . hosts [ 0 ] try : dst size bytes = self . get size ( ) except ( Value Error , Type Error ) : dst size bytes = 0 free space bytes = self . get free space ( ) balance bytes = free space bytes + dst size bytes - src size bytes balance bytes scaled , units = pretty bytes ( balance bytes ) viable = balance bytes >= 0 if self . verbose : print ( 'src db size:' , pretty bytes ( src size bytes ) ) print ( 'dst db size:' , pretty bytes ( dst size bytes ) ) print ( 'dst free space:' , pretty bytes ( free space bytes ) ) print if viable : print ( 'Viable! There will be %.02f %s of disk space left.' % ( balance bytes scaled , units ) ) else : print ( 'Not viable! We would be %.02f %s short.' % ( balance bytes scaled , units ) ) return viable", "predictions": ["crawl src file with loadable space ."], "references": ["determines if there s enough space to load the target database ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 2380, "code": "def assume localhost ( self ) : if not self . genv . host string : self . genv . host string = 'localhost' self . genv . hosts = [ 'localhost' ] self . genv . user = getpass . getuser ( )", "predictions": ["localhost the localhost of the localhost ."], "references": ["sets connection parameters to localhost if not set already ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2381, "code": "def fix lsmod for pi3 ( self ) : r = self . local renderer r . env . rpi2 conf = '/etc/modules-load.d/rpi2.conf' r . sudo ( \"sed '/bcm2808 rng/d' {rpi2 conf}\" ) r . sudo ( \"echo bcm2835 rng >> {rpi2 conf}\" )", "predictions": ["activate activate activate activate activate activate pi3"], "references": ["some images purporting to support both the pi2 and pi3 use the wrong kernel modules ."], "bleu": 0.05135131375181345, "rouge_l": 0.08122503328894808}
{"id": 2382, "code": "def pre deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service pre deployers . get ( service ) if funcs : print ( 'Running pre-deployments for service %s...' % ( service , ) ) for func in funcs : func ( )", "predictions": ["run all genv tasks"], "references": ["runs methods services have requested be run before each deployment ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 2383, "code": "def deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service deployers . get ( service ) if funcs : print ( 'Deploying service %s...' % ( service , ) ) for func in funcs : if not self . dryrun : func ( )", "predictions": ["deploy all genv services"], "references": ["applies routine typically application - level changes to the service ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 2384, "code": "def post deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) self . vprint ( 'post deploy:' , service ) funcs = common . service post deployers . get ( service ) if funcs : self . vprint ( 'Running post-deployments for service %s...' % ( service , ) ) for func in funcs : try : func ( ) except Exception as e : print ( 'Post deployment error: %s' % e , file = sys . stderr ) print ( traceback . format exc ( ) , file = sys . stderr )", "predictions": ["deploy each service to the application"], "references": ["runs methods services have requested be run before after deployment ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 2385, "code": "def configure ( self ) : print ( 'env.services:' , self . genv . services ) for service in list ( self . genv . services ) : service = service . strip ( ) . upper ( ) funcs = common . service configurators . get ( service , [ ] ) if funcs : print ( '!' * 80 ) print ( 'Configuring service %s...' % ( service , ) ) for func in funcs : print ( 'Function:' , func ) if not self . dryrun : func ( )", "predictions": ["configure the service ."], "references": ["applies one - time settings changes to the host usually to initialize the service ."], "bleu": 0.042751137399864, "rouge_l": 0.2859375}
{"id": 2386, "code": "def sync media ( self , sync set = None , clean = 0 , iter local paths = 0 ) : self . genv . SITE = self . genv . SITE or self . genv . default site r = self . local renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set site specifics ( self . genv . SITE ) sync sets = r . env . sync sets if sync set : sync sets = [ sync set ] ret paths = [ ] for sync set in sync sets : for paths in r . env . sync sets [ sync set ] : r . env . sync local path = os . path . abspath ( paths [ 'local path' ] % self . genv ) if paths [ 'local path' ] . endswith ( '/' ) and not r . env . sync local path . endswith ( '/' ) : r . env . sync local path += '/' if iter local paths : ret paths . append ( r . env . sync local path ) continue r . env . sync remote path = paths [ 'remote path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache sync remote path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync local path , r . env . sync remote path ) ) r . env . tmp chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache sync remote path}' ) r . sudo ( 'chmod -R {apache tmp chmod} {apache sync remote path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh \"ssh -o Strict Host Key Checking=no -i {key filename}\" {apache sync local path} {user}@{host string}:{apache sync remote path}' ) r . sudo ( 'chown -R {apache web user}:{apache web group} {apache sync remote path}' ) if iter local paths : return ret paths", "predictions": ["sync sync with a media file ."], "references": ["uploads select media to an apache accessible directory ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 2387, "code": "def maint up ( self ) : r = self . local renderer fn = self . render to file ( r . env . maintenance template , extra = { 'current hostname' : self . current hostname } ) r . put ( local path = fn , remote path = r . env . maintenance path , use sudo = True ) r . sudo ( 'chown -R {apache web user}:{apache web group} {maintenance path}' )", "predictions": ["put maint to maint"], "references": ["forwards all traffic to a page saying the server is down for maintenance ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 2388, "code": "def get current commit ( self ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : s = str ( self . local ( 'git rev-parse HEAD' , capture = True ) ) self . vprint ( 'current commit:' , s ) return s", "predictions": ["get the current commit commit"], "references": ["retrieves the git commit number of the current head branch ."], "bleu": 0.11629030063732083, "rouge_l": 0.2341650671785029}
{"id": 2389, "code": "def ssh config ( self , name = '' ) : r = self . local renderer with self . settings ( hide ( 'running' ) ) : output = r . local ( 'vagrant ssh-config %s' % name , capture = True ) config = { } for line in output . splitlines ( ) [ 1 : ] : key , value = line . strip ( ) . split ( ' ' , 2 ) config [ key ] = value return config", "predictions": ["return a dictionary of ssh config"], "references": ["get the ssh parameters for connecting to a vagrant vm ."], "bleu": 0.10624253482403696, "rouge_l": 0.1117216117216117}
{"id": 2390, "code": "def version ( self ) : r = self . local renderer with self . settings ( hide ( 'running' , 'warnings' ) , warn only = True ) : res = r . local ( 'vagrant --version' , capture = True ) if res . failed : return None line = res . splitlines ( ) [ - 1 ] version = re . match ( r'Vagrant (?:v(?:ersion )?)?(.*)' , line ) . group ( 1 ) return tuple ( to int ( part ) for part in version . split ( '.' ) )", "predictions": ["the version number of pages ."], "references": ["get the vagrant version ."], "bleu": 0.2626909894424158, "rouge_l": 0.5545454545454546}
{"id": 2391, "code": "def base boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . box list ( ) ] ) ) )", "predictions": ["return the list of all boxes in the provider ."], "references": ["get the list of vagrant base boxes"], "bleu": 0.24808415001701817, "rouge_l": 0.48605577689243035}
{"id": 2392, "code": "def install from upstream ( self ) : from burlap . system import get arch , distrib family r = self . local renderer content = urlopen ( r . env . download url ) . read ( ) print ( len ( content ) ) matches = DOWNLOAD LINK PATTERN . findall ( content ) print ( matches ) arch = get arch ( ) family = distrib family ( ) if family == DEBIAN : ext = '.deb' matches = [ match for match in matches if match . endswith ( ext ) and arch in match ] print ( 'matches:' , matches ) assert matches , \"No matches found.\" assert len ( matches ) == 1 , \"Too many matches found: %s\" % ( ', ' . join ( matches ) ) r . env . final download url = matches [ 0 ] r . env . local filename = '/tmp/vagrant%s' % ext r . run ( 'wget -O {local filename} {final download url}' ) r . sudo ( 'dpkg -i {local filename}' ) else : raise Not Implemented Error ( 'Unsupported family: %s' % family )", "predictions": ["install from upstream . txt file ."], "references": ["installs vagrant from the most recent package available from their homepage ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 2393, "code": "def force stop ( self ) : r = self . local renderer with self . settings ( warn only = True ) : r . sudo ( 'pkill -9 -f celery' ) r . sudo ( 'rm -f /tmp/celery*.pid' )", "predictions": ["stop the local local process ."], "references": ["forcibly terminates all celery processes ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2394, "code": "def set permissions ( self ) : r = self . local renderer for path in r . env . paths owned : r . env . path owned = path r . sudo ( 'chown {celery daemon user}:{celery daemon user} {celery path owned}' )", "predictions": ["set the permissions ."], "references": ["sets ownership and permissions for celery - related files ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 2395, "code": "def create supervisor services ( self , site ) : self . vprint ( 'create supervisor services:' , site ) self . set site specifics ( site = site ) r = self . local renderer if self . verbose : print ( 'r.env:' ) pprint ( r . env , indent = 4 ) self . vprint ( 'r.env.has worker:' , r . env . has worker ) if not r . env . has worker : self . vprint ( 'skipping: no celery worker' ) return if self . name . lower ( ) not in self . genv . services : self . vprint ( 'skipping: celery not enabled' ) return hostname = self . current hostname target sites = self . genv . available sites by host . get ( hostname , None ) if target sites and site not in target sites : self . vprint ( 'skipping: site not supported on this server' ) return self . render paths ( ) conf name = 'celery %s.conf' % site ret = r . render to string ( 'celery/celery supervisor.template.conf' ) return conf name , ret", "predictions": ["create supervisor services for supervisor"], "references": ["this is called for each site to render a celery config file ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 2396, "code": "def purge keys ( self ) : r = self . local renderer r . env . default ip = self . hostname to ip ( self . env . default hostname ) r . env . home dir = '/home/%s' % getpass . getuser ( ) r . local ( 'ssh-keygen -f \"{home dir}/.ssh/known hosts\" -R {host string}' ) if self . env . default hostname : r . local ( 'ssh-keygen -f \"{home dir}/.ssh/known hosts\" -R {default hostname}' ) if r . env . default ip : r . local ( 'ssh-keygen -f \"{home dir}/.ssh/known hosts\" -R {default ip}' )", "predictions": ["purge any keys that have been done ."], "references": ["deletes all ssh keys on the localhost associated with the current remote host ."], "bleu": 0.08383280652235028, "rouge_l": 0.1732954545454545}
{"id": 2397, "code": "def find working password ( self , usernames = None , host strings = None ) : r = self . local renderer if host strings is None : host strings = [ ] if not host strings : host strings . append ( self . genv . host string ) if usernames is None : usernames = [ ] if not usernames : usernames . append ( self . genv . user ) for host string in host strings : for username in usernames : passwords = [ ] passwords . append ( self . genv . user default passwords [ username ] ) passwords . append ( self . genv . user passwords [ username ] ) passwords . append ( self . env . default password ) for password in passwords : with settings ( warn only = True ) : r . env . host string = host string r . env . password = password r . env . user = username ret = r . local ( \"sshpass -p '{password}' ssh -o Strict Host Key Checking=no {user}@{host string} echo hello\" , capture = True ) #print('ret.return code:', ret.return code) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return code in ( 1 , 6 ) or 'hello' in ret : return host string , username , password raise Exception ( 'No working login found.' )", "predictions": ["find the local working working user for a given working working directory ."], "references": ["returns the first working combination of username and password for the current host ."], "bleu": 0.1112176984362864, "rouge_l": 0.2943305186972256}
{"id": 2398, "code": "def get public ip ( self ) : r = self . local renderer ret = r . run ( r . env . get public ip command ) or '' ret = ret . strip ( ) print ( 'ip:' , ret ) return ret", "predictions": ["get the local public ip address"], "references": ["gets the public ip for a host ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 2399, "code": "def query ( query , use sudo = True , * * kwargs ) : func = use sudo and run as root or run user = kwargs . get ( 'mysql user' ) or env . get ( 'mysql user' ) password = kwargs . get ( 'mysql password' ) or env . get ( 'mysql password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )", "predictions": ["query the query and return the parsed response ."], "references": ["run a mysql query ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 2400, "code": "def user exists ( name , host = 'localhost' , * * kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = query ( % { 'name' : name , 'host' : host , } , * * kwargs ) return res . succeeded and ( int ( res ) == 1 )", "predictions": ["check if a preview exists exists"], "references": ["check if a mysql user exists ."], "bleu": 0.36798327352994814, "rouge_l": 0.6069651741293532}
{"id": 2401, "code": "def database exists ( name , * * kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn only = True ) : res = query ( \"SHOW DATABASES LIKE '%(name)s';\" % { 'name' : name } , * * kwargs ) return res . succeeded and ( res == name )", "predictions": ["check if a push push exists exists"], "references": ["check if a mysql database exists ."], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 2402, "code": "def conf path ( self ) : from burlap . system import distrib id , distrib release hostname = self . current hostname if hostname not in self . conf cache : self . env . conf specifics [ hostname ] = self . env . conf default d id = distrib id ( ) d release = distrib release ( ) for key in ( ( d id , d release ) , ( d id , ) ) : if key in self . env . conf specifics : self . conf cache [ hostname ] = self . env . conf specifics [ key ] return self . conf cache [ hostname ]", "predictions": ["a thumbprint instance with the configuration options { available } { environment } { environment } { in } { get the user } { environment } { environment } name"], "references": ["retrieves the path to the mysql configuration file ."], "bleu": 0.046398855339878003, "rouge_l": 0.11101000909918107}
{"id": 2403, "code": "def drop views ( self , name = None , site = None ) : r = self . database renderer result = r . sudo ( \"mysql --batch -v -h {db host} \" #\"-u {db root username} -p'{db root password}' \" \"-u {db user} -p'{db password}' \" \"--execute=\\\"SELECT GROUP CONCAT(CONCAT(TABLE SCHEMA,'.',table name) SEPARATOR ', ') AS views \" \"FROM INFORMATION SCHEMA.views WHERE TABLE SCHEMA = '{db name}' ORDER BY table name DESC;\\\"\" ) result = re . findall ( r'^views[\\s\\t\\r\\n]+(.*)' , result , flags = re . IGNORECASE | re . DOTALL | re . MULTILINE ) if not result : return r . env . db view list = result [ 0 ] #cmd = (\"mysql -v -h {db host} -u {db root username} -p'{db root password}' \" \\ r . sudo ( \"mysql -v -h {db host} -u {db user} -p'{db password}' \" \"--execute=\\\"DROP VIEW {db view list} CASCADE;\\\"\" )", "predictions": ["get get settings settings from database"], "references": ["drops all views ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2404, "code": "def exists ( self , * * kwargs ) : name = kwargs . pop ( 'name' , 'default' ) site = kwargs . pop ( 'site' , None ) r = self . database renderer ( name = name , site = site ) ret = r . run ( 'mysql -h {db host} -u {db root username} ' '-p\"{db root password}\" -N -B -e \"SELECT IF(\\'{db name}\\'' ' IN(SELECT SCHEMA NAME FROM INFORMATION SCHEMA.SCHEMATA), ' '\\'exists\\', \\'notexists\\') AS found;\"' ) if ret is not None : ret = 'notexists' not in ( ret or 'notexists' ) if ret is not None : msg = '%s database on site %s %s exist.' % ( name . title ( ) , env . SITE , 'DOES' if ret else 'DOES NOT' ) if ret : print ( green ( msg ) ) else : print ( red ( msg ) ) return ret", "predictions": ["check if a database exists exists"], "references": ["returns true if a database with the given name exists . false otherwise ."], "bleu": 0.11459117772387306, "rouge_l": 0.3730886850152905}
{"id": 2405, "code": "def pycompress sqlitecurve ( sqlitecurve , force = False ) : outfile = '%s.gz' % sqlitecurve try : if os . path . exists ( outfile ) and not force : os . remove ( sqlitecurve ) return outfile else : with open ( sqlitecurve , 'rb' ) as infd : with gzip . open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) if os . path . exists ( outfile ) : os . remove ( sqlitecurve ) return outfile except Exception as e : return None", "predictions": ["get a specific virtualenv object for a specific file"], "references": ["this just compresses the sqlitecurve . should be independent of os ."], "bleu": 0.08504083946364166, "rouge_l": 0.0}
{"id": 2406, "code": "def pyuncompress sqlitecurve ( sqlitecurve , force = False ) : outfile = sqlitecurve . replace ( '.gz' , '' ) try : if os . path . exists ( outfile ) and not force : return outfile else : with gzip . open ( sqlitecurve , 'rb' ) as infd : with open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) if os . path . exists ( outfile ) : return outfile except Exception as e : return None", "predictions": ["get input file and return its handle as string"], "references": ["this just uncompresses the sqlitecurve . should be independent of os ."], "bleu": 0.08504083946364166, "rouge_l": 0.0}
{"id": 2407, "code": "def parse csv header lcc csv v1 ( headerlines ) : commentchar = headerlines [ 1 ] separator = headerlines [ 2 ] headerlines = [ x . lstrip ( '%s ' % commentchar ) for x in headerlines [ 3 : ] ] metadatastart = headerlines . index ( 'OBJECT METADATA' ) columnstart = headerlines . index ( 'COLUMN DEFINITIONS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) metadata = ' ' . join ( headerlines [ metadatastart + 1 : columnstart - 1 ] ) columns = ' ' . join ( headerlines [ columnstart + 1 : lcstart - 1 ] ) metadata = json . loads ( metadata ) columns = json . loads ( columns ) return metadata , columns , separator", "predictions": ["syncdb to syncdb self local site"], "references": ["this parses the header of the lcc csv v1 lc format ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 2408, "code": "def starfeatures worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor radius arcsec , deredden , custom bandpasses , lcformat , lcformatdir ) = task return get starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor radius arcsec , deredden = deredden , custom bandpasses = custom bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None", "predictions": ["return a renderer for a self . percentage_identity"], "references": ["this wraps starfeatures ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2409, "code": "def pwd phasebin ( phases , mags , binsize = 0.002 , minbin = 9 ) : bins = np . arange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binnedphases , binnedmags = [ ] , [ ] for x in npunique ( binnedphaseinds ) : thisbin inds = binnedphaseinds == x thisbin phases = phases [ thisbin inds ] thisbin mags = mags [ thisbin inds ] if thisbin inds . size > minbin : binnedphases . append ( npmedian ( thisbin phases ) ) binnedmags . append ( npmedian ( thisbin mags ) ) return np . array ( binnedphases ) , np . array ( binnedmags )", "predictions": ["spherical spherical to true if phases is available"], "references": ["this bins the phased mag series using the given binsize ."], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 2410, "code": "def periodicfeatures worker ( task ) : pfpickle , lcbasedir , outdir , starfeatures , kwargs = task try : return get periodicfeatures ( pfpickle , lcbasedir , outdir , starfeatures = starfeatures , * * kwargs ) except Exception as e : LOGEXCEPTION ( 'failed to get periodicfeatures for %s' % pfpickle )", "predictions": ["run a load balancer db"], "references": ["this is a parallel worker for the drivers below ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2411, "code": "def commit ( self ) : if not self . connection . closed : self . connection . commit ( ) else : raise Attribute Error ( 'postgres connection to %s is closed' % self . database )", "predictions": ["loadable the current from the source ."], "references": ["this just calls the connection s commit method ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 2412, "code": "def rollback ( self ) : if not self . connection . closed : self . connection . rollback ( ) else : raise Attribute Error ( 'postgres connection to %s is closed' % self . database )", "predictions": ["roll back the current ."], "references": ["this just calls the connection s commit method ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 2413, "code": "def log prior transit ( theta , priorbounds ) : allowed = True for ix , key in enumerate ( np . sort ( list ( priorbounds . keys ( ) ) ) ) : if priorbounds [ key ] [ 0 ] < theta [ ix ] < priorbounds [ key ] [ 1 ] : allowed = True and allowed else : allowed = False if allowed : return 0. return - np . inf", "predictions": ["fix prior prior prior"], "references": ["assume priors on all parameters have uniform probability ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2414, "code": "def list trilegal filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL FILTER SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL FILTER SYSTEMS [ key ] [ 'desc' ] ) )", "predictions": ["pre - show deploy filtersystems"], "references": ["this just lists all the filter systems available for trilegal ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 2415, "code": "def initialize ( self , currentdir , assetpath , cplist , cplistfile , executor , readonly , baseurl ) : self . currentdir = currentdir self . assetpath = assetpath self . currentproject = cplist self . cplistfile = cplistfile self . executor = executor self . readonly = readonly self . baseurl = baseurl", "predictions": ["initialize the simulation and readonly instance"], "references": ["handles initial setup ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2416, "code": "def initialize ( self , executor , secret ) : self . executor = executor self . secret = secret", "predictions": ["initialize this instance with the given executor"], "references": ["this handles initial setup of the requesthandler ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2417, "code": "def epd function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : return ( coeffs [ 0 ] * fsv * fsv + coeffs [ 1 ] * fsv + coeffs [ 2 ] * fdv * fdv + coeffs [ 3 ] * fdv + coeffs [ 4 ] * fkv * fkv + coeffs [ 5 ] * fkv + coeffs [ 6 ] + coeffs [ 7 ] * fsv * fdv + coeffs [ 8 ] * fsv * fkv + coeffs [ 9 ] * fdv * fkv + coeffs [ 10 ] * np . sin ( 2 * pi value * xcc ) + coeffs [ 11 ] * np . cos ( 2 * pi value * xcc ) + coeffs [ 12 ] * np . sin ( 2 * pi value * ycc ) + coeffs [ 13 ] * np . cos ( 2 * pi value * ycc ) + coeffs [ 14 ] * np . sin ( 4 * pi value * xcc ) + coeffs [ 15 ] * np . cos ( 4 * pi value * xcc ) + coeffs [ 16 ] * np . sin ( 4 * pi value * ycc ) + coeffs [ 17 ] * np . cos ( 4 * pi value * ycc ) + coeffs [ 18 ] * bgv + coeffs [ 19 ] * bge + coeffs [ 20 ] * iha + coeffs [ 21 ] * izd )", "predictions": ["inverse of the filestorehashs a configure function function"], "references": ["this is the epd function to fit using a smoothed mag - series ."], "bleu": 0.09008421318929809, "rouge_l": 0.1732954545454545}
{"id": 2418, "code": "def epd residual ( coeffs , mags , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : f = epd function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) residual = mags - f return residual", "predictions": ["internal function to create a sync media media from a pdf"], "references": ["this is the residual function to minimize using scipy . optimize . leastsq ."], "bleu": 0.11412735515545797, "rouge_l": 0.15661103979460847}
{"id": 2419, "code": "def get legendre deg ctd ( npts ) : from scipy . interpolate import interp1d degs = nparray ( [ 4 , 5 , 6 , 10 , 15 ] ) pts = nparray ( [ 1e2 , 3e2 , 5e2 , 1e3 , 3e3 ] ) fn = interp1d ( pts , degs , kind = 'linear' , bounds error = False , fill value = ( min ( degs ) , max ( degs ) ) ) legendredeg = int ( npfloor ( fn ( npts ) ) ) return legendredeg", "predictions": ["fn to fn but for up to up to a up to a human - readable model"], "references": ["this is a helper function for centroid detrending ."], "bleu": 0.07994607499472013, "rouge_l": 0.0814419225634179}
{"id": 2420, "code": "def varfeatures worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) = task return get varfeatures ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None", "predictions": ["run a get - current task"], "references": ["this wraps varfeatures ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2421, "code": "def runpf worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None", "predictions": ["run a ssh config config file"], "references": ["this runs the runpf function ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2422, "code": "def read hatpi pklc ( lcfile ) : try : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) lcdict = pickle . load ( infd ) infd . close ( ) return lcdict except Unicode Decode Error : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) LOGWARNING ( 'pickle %s was probably from Python 2 ' 'and failed to load without using \"latin1\" encoding. ' 'This is probably a numpy issue: ' 'http://stackoverflow.com/q/11305790' % lcfile ) lcdict = pickle . load ( infd , encoding = 'latin1' ) infd . close ( ) return lcdict", "predictions": ["version of ufos that accepts a numpy"], "references": ["this just reads a pickle lc . returns an lcdict ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 2423, "code": "def parallel concat lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True , nworkers = 32 , maxworkertasks = 1000 ) : if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) tasks = [ ( lcbasedir , x , { 'aperture' : aperture , 'postfix' : postfix , 'sortby' : sortby , 'normalize' : normalize , 'outdir' : outdir , 'recursive' : recursive } ) for x in objectidlist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel concat worker , tasks ) pool . close ( ) pool . join ( ) return { x : y for ( x , y ) in zip ( objectidlist , results ) }", "predictions": ["boxes two directories that match a given return a list of tasks"], "references": ["this concatenates all text lcs for the given objectidlist ."], "bleu": 0.10390302174233558, "rouge_l": 0.09242424242424242}
{"id": 2424, "code": "def generate hatpi binnedlc pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read hatpi binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict to pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None", "predictions": ["install from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from"], "references": ["this reads the binned lc and writes it out to a pickle ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2425, "code": "def magbin varind gridsearch worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get recovered variables for magbin ( simbasedir , magbinmedian , stetson stdev min = gridpoint [ 0 ] , inveta stdev min = gridpoint [ 1 ] , iqr stdev min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None", "predictions": ["get the tasks for a task"], "references": ["this is a parallel grid search worker for the function below ."], "bleu": 0.09663861439684919, "rouge_l": 0.10481099656357389}
{"id": 2426, "code": "def update proxy ( self , change ) : if change [ 'type' ] == 'container' : #: Only update what's needed self . proxy . update points ( change ) else : super ( Map Polyline , self ) . update proxy ( change )", "predictions": ["an observer which sends the permissions to the permissions . . . . . . . . . . . ."], "references": ["an observer which sends the state change to the proxy ."], "bleu": 0.24648321974767612, "rouge_l": 0.5298588490770901}
{"id": 2427, "code": "def update proxy ( self , change ) : if change [ 'type' ] == 'container' : #: Only update what's needed self . proxy . update points ( change ) else : super ( Map Polygon , self ) . update proxy ( change )", "predictions": ["an observer which sends the supervisor if the supervisor if the supervisor is updated renderer renderer renderer renderer renderer renderer renderer renderer"], "references": ["an observer which sends the state change to the proxy ."], "bleu": 0.2108445456905127, "rouge_l": 0.38689217758985195}
{"id": 2428, "code": "def handle change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( len ( change [ 'value' ] ) , Lat Lng ( * change [ 'item' ] ) ) elif op == 'insert' : self . add ( change [ 'index' ] , Lat Lng ( * change [ 'item' ] ) ) elif op == 'extend' : points = [ Lat Lng ( * p ) for p in change [ 'items' ] ] self . add All ( [ bridge . encode ( c ) for c in points ] ) elif op == ' setitem ' : self . set ( change [ 'index' ] , Lat Lng ( * change [ 'newitem' ] ) ) elif op == 'pop' : self . remove ( change [ 'index' ] ) else : raise Not Implemented Error ( \"Unsupported change operation {}\" . format ( op ) )", "predictions": ["see memcache . gingaplugin ."], "references": ["handle changes from atom containerlists"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2429, "code": "def create widget ( self ) : self . init options ( ) #: Retrieve the actual map Map Fragment . new Instance ( self . options ) . then ( self . on map fragment created ) self . widget = Frame Layout ( self . get context ( ) ) self . map = Google Map ( id = bridge . generate id ( ) )", "predictions": ["find the underlying working working directory usernames usernames usernames usernames usernames usernames usernames usernames usernames usernames"], "references": ["create the underlying widget ."], "bleu": 0.10123734869668824, "rouge_l": 0.21034482758620687}
{"id": 2430, "code": "def init options ( self ) : self . options = Google Map Options ( ) d = self . declaration self . set map type ( d . map type ) if d . ambient mode : self . set ambient mode ( d . ambient mode ) if ( d . camera position or d . camera zoom or d . camera tilt or d . camera bearing ) : self . update camera ( ) if d . map bounds : self . set map bounds ( d . map bounds ) if not d . show compass : self . set show compass ( d . show compass ) if not d . show zoom controls : self . set show zoom controls ( d . show zoom controls ) if not d . show toolbar : self . set show toolbar ( d . show toolbar ) if d . lite mode : self . set lite mode ( d . lite mode ) if not d . rotate gestures : self . set rotate gestures ( d . rotate gestures ) if not d . scroll gestures : self . set scroll gestures ( d . scroll gestures ) if not d . tilt gestures : self . set tilt gestures ( d . tilt gestures ) if not d . zoom gestures : self . set zoom gestures ( d . zoom gestures ) if d . min zoom : self . set min zoom ( d . min zoom ) if d . max zoom : self . set max zoom ( d . max zoom )", "predictions": ["sets the public public public public public and return ."], "references": ["initialize the underlying map options ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2431, "code": "def init map ( self ) : d = self . declaration if d . show location : self . set show location ( d . show location ) if d . show traffic : self . set show traffic ( d . show traffic ) if d . show indoors : self . set show indoors ( d . show indoors ) if d . show buildings : self . set show buildings ( d . show buildings ) #: Local ref access is faster mapview = self . map mid = mapview . get Id ( ) #: Connect signals #: Camera mapview . on Camera Change . connect ( self . on camera changed ) mapview . on Camera Move Started . connect ( self . on camera move started ) mapview . on Camera Move Canceled . connect ( self . on camera move stopped ) mapview . on Camera Idle . connect ( self . on camera move stopped ) mapview . set On Camera Change Listener ( mid ) mapview . set On Camera Move Started Listener ( mid ) mapview . set On Camera Move Canceled Listener ( mid ) mapview . set On Camera Idle Listener ( mid ) #: Clicks mapview . on Map Click . connect ( self . on map clicked ) mapview . set On Map Click Listener ( mid ) mapview . on Map Long Click . connect ( self . on map long clicked ) mapview . set On Map Long Click Listener ( mid ) #: Markers mapview . on Marker Click . connect ( self . on marker clicked ) mapview . set On Marker Click Listener ( self . map . get Id ( ) ) mapview . on Marker Drag Start . connect ( self . on marker drag start ) mapview . on Marker Drag . connect ( self . on marker drag ) mapview . on Marker Drag End . connect ( self . on marker drag end ) mapview . set On Marker Drag Listener ( mid ) #: Info window mapview . on Info Window Click . connect ( self . on info window clicked ) mapview . on Info Window Long Click . connect ( self . on info window long clicked ) mapview . on Info Window Close . connect ( self . on info window closed ) mapview . set On Info Window Click Listener ( mid ) mapview . set On Info Window Close Listener ( mid ) mapview . set On Info Window Long Click Listener ( mid ) #: Polys mapview . on Polygon Click . connect ( self . on poly clicked ) mapview . on Polyline Click . connect ( self . on poly clicked ) mapview . set On Polygon Click Listener ( mid ) mapview . set On Polyline Click Listener ( mid ) #: Circle mapview . on Circle Click . connect ( self . on circle clicked ) mapview . set On Circle Click Listener ( mid )", "predictions": ["connects to the map widget kwargs kwargs and sets up the widget kwargs kwargs kwargs"], "references": ["add markers polys callouts etc .."], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 2432, "code": "def on map fragment created ( self , obj id ) : self . fragment = Map Fragment ( id = obj id ) #: Setup callback so we know when the map is ready self . map . on Map Ready . connect ( self . on map ready ) self . fragment . get Map Async ( self . map . get Id ( ) ) context = self . get context ( ) def on transaction ( id ) : trans = Fragment Transaction ( id = id ) trans . add ( self . widget . get Id ( ) , self . fragment ) trans . commit ( ) def on fragment manager ( id ) : fm = Fragment Manager ( id = id ) fm . begin Transaction ( ) . then ( on transaction ) context . widget . get Support Fragment Manager ( ) . then ( on fragment manager )", "predictions": ["special handling for when a fragment is created"], "references": ["create the fragment and pull the map reference when it s loaded ."], "bleu": 0.09499501502705178, "rouge_l": 0.09131736526946108}
{"id": 2433, "code": "def on map clicked ( self , pos ) : d = self . declaration d . clicked ( { 'click' : 'short' , 'position' : tuple ( pos ) } )", "predictions": ["map the class to a new position ."], "references": ["called when the map is clicked"], "bleu": 0.17747405280050269, "rouge_l": 0.14663461538461536}
{"id": 2434, "code": "def on map long clicked ( self , pos ) : d = self . declaration d . clicked ( { 'click' : 'long' , 'position' : tuple ( pos ) } )", "predictions": ["long long clicked ."], "references": ["called when the map is clicked"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2435, "code": "def destroy ( self ) : marker = self . marker parent = self . parent ( ) if marker : if parent : del parent . markers [ marker . id ] marker . remove ( ) super ( Android Map Item Base , self ) . destroy ( )", "predictions": ["destroy the marker and delete the marker from it ."], "references": ["remove the marker if it was added to the map when destroying"], "bleu": 0.15433335889254143, "rouge_l": 0.2683284457478006}
{"id": 2436, "code": "def child added ( self , child ) : if child . widget : self . parent ( ) . init info window adapter ( ) super ( Android Map Marker , self ) . child added ( child )", "predictions": ["when child is added"], "references": ["if a child is added we have to make sure the map adapter exists"], "bleu": 0.054893547013735264, "rouge_l": 0.3029801324503311}
{"id": 2437, "code": "def on marker ( self , marker ) : mid , pos = marker self . marker = Marker ( id = mid ) mapview = self . parent ( ) mapview . markers [ mid ] = self self . marker . set Tag ( mid ) for w in self . child widgets ( ) : mapview . init info window adapter ( ) break d = self . declaration if d . show info : self . set show info ( d . show info ) #: Can free the options now del self . options", "predictions": ["update the options of a marker"], "references": ["convert our options into the actual marker object"], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 2438, "code": "def on marker ( self , mid ) : self . marker = Circle ( id = mid ) self . parent ( ) . markers [ mid ] = self #: Required so the packer can pass the id self . marker . set Tag ( mid ) d = self . declaration if d . clickable : self . set clickable ( d . clickable ) #: Can free the options now del self . options", "predictions": ["any marker in dconf"], "references": ["convert our options into the actual circle object"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 2439, "code": "def data ( self , X = None , y = None , sentences = None ) : self . X = X self . y = y self . sentences = sentences", "predictions": ["truncated data for x and y data"], "references": ["add data to flow"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2440, "code": "def train ( self ) : for i , model in enumerate ( self . models ) : N = [ int ( i * len ( self . y ) ) for i in self . lc range ] for n in N : X = self . X [ : n ] y = self . y [ : n ] e = Experiment ( X , y , model . estimator , self . scores , self . validation method ) e . log folder = self . log folder e . train ( )", "predictions": ["train the model on the training data ."], "references": ["train model with transformed data"], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 2441, "code": "def print cm ( cm , labels , hide zeroes = False , hide diagonal = False , hide threshold = None ) : columnwidth = max ( [ len ( x ) for x in labels ] + [ 5 ] ) empty cell = \" \" * columnwidth print ( \"    \" + empty cell , end = \" \" ) for label in labels : print ( \"%{0}s\" . format ( columnwidth ) % label , end = \" \" ) print ( ) for i , label1 in enumerate ( labels ) : print ( \"    %{0}s\" . format ( columnwidth ) % label1 , end = \" \" ) for j in range ( len ( labels ) ) : cell = \"%{0}.1f\" . format ( columnwidth ) % cm [ i , j ] if hide zeroes : cell = cell if float ( cm [ i , j ] ) != 0 else empty cell if hide diagonal : cell = cell if i != j else empty cell if hide threshold : cell = cell if cm [ i , j ] > hide threshold else empty cell print ( cell , end = \" \" ) print ( )", "predictions": ["print a report of labels with labels ."], "references": ["pretty print for confusion matrixes"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2442, "code": "def config sources ( app , environment , cluster , configs dirs , app dir , local = False , build = False ) : sources = [ ( configs dirs , 'hostname' ) , ( configs dirs , 'hostname-local' ) , ( configs dirs , 'hostname-build' ) , ( configs dirs , 'common' ) , ( configs dirs , 'common-%s' % environment ) , ( configs dirs , 'common-%s-%s' % ( environment , cluster ) ) , ( configs dirs , 'common-local' ) , ( configs dirs , 'common-build' ) , ( configs dirs , 'common-overrides' ) , ( [ app dir ] , '%s-default' % app ) , ( [ app dir ] , '%s-%s' % ( app , environment ) ) , ( [ app dir ] , '%s-%s-%s' % ( app , environment , cluster ) ) , ( configs dirs , app ) , ( configs dirs , '%s-%s' % ( app , environment ) ) , ( configs dirs , '%s-%s-%s' % ( app , environment , cluster ) ) , ( [ app dir ] , '%s-local' % app ) , ( [ app dir ] , '%s-build' % app ) , ( configs dirs , '%s-local' % app ) , ( configs dirs , '%s-build' % app ) , ( configs dirs , '%s-overrides' % app ) , ] if not build : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-build' ) ] if not local : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-local' ) ] return available sources ( sources )", "predictions": ["run sources for a set of sources ."], "references": ["return the config files for an environment & cluster specific app ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 2443, "code": "def available sources ( sources ) : for dirs , name in sources : for directory in dirs : fn = os . path . join ( directory , name ) + '.py' if os . path . isfile ( fn ) : yield fn", "predictions": ["return a list of available sources ."], "references": ["yield the sources that are present ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2444, "code": "def smush config ( sources , initial = None ) : if initial is None : initial = { } config = Dot Dict ( initial ) for fn in sources : log . debug ( 'Merging %s' , fn ) mod = get config module ( fn ) config = mod . update ( config ) log . debug ( 'Current config:\\n%s' , json . dumps ( config , indent = 4 , cls = Lenient JSON Encoder ) ) return config", "predictions": ["return a copy of the config object with the given sources ."], "references": ["merge the configuration sources and return the resulting dotdict ."], "bleu": 0.1367440667823257, "rouge_l": 0.2772727272727273}
{"id": 2445, "code": "def filter dict ( unfiltered , filter keys ) : filtered = Dot Dict ( ) for k in filter keys : filtered [ k ] = unfiltered [ k ] return filtered", "predictions": ["filter a dict with a dict of values ."], "references": ["return a subset of a dictionary using the specified keys ."], "bleu": 0.14211011212459496, "rouge_l": 0.2946859903381642}
{"id": 2446, "code": "def filter config ( config , deploy config ) : if not os . path . isfile ( deploy config ) : return Dot Dict ( ) config module = get config module ( deploy config ) return config module . filter ( config )", "predictions": ["filter the config object to deploy the deploy ."], "references": ["return a config subset using the filter defined in the deploy config ."], "bleu": 0.1471989137239998, "rouge_l": 0.35209235209235207}
{"id": 2447, "code": "def seeded auth token ( client , service , seed ) : hash func = hashlib . md5 ( ) token = ',' . join ( ( client , service , seed ) ) . encode ( 'utf-8' ) hash func . update ( token ) return hash func . hexdigest ( )", "predictions": ["get an md5 token for the given seed ."], "references": ["return an auth token based on the client + service + seed tuple ."], "bleu": 0.10657503067399117, "rouge_l": 0.41838134430727025}
{"id": 2448, "code": "def write config ( config , app dir , filename = 'configuration.json' ) : path = os . path . join ( app dir , filename ) with open ( path , 'w' ) as f : json . dump ( config , f , indent = 4 , cls = Detect Missing Encoder , separators = ( ',' , ': ' ) )", "predictions": ["write the config to a file"], "references": ["write configuration to the applicaiton directory ."], "bleu": 0.22236312185643822, "rouge_l": 0.3034825870646766}
{"id": 2449, "code": "def validate date ( date text ) : try : if int ( date text ) < 0 : return True except Value Error : pass try : datetime . strptime ( date text , '%Y-%m-%d' ) return True except Value Error : pass raise Value Error ( 'Dates must be negative integers or YYYY-MM-DD in the past.' )", "predictions": ["validates that a date is a valid date or not ."], "references": ["return true if valid raise valueerror if not"], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 2450, "code": "def get download total ( rows ) : headers = rows . pop ( 0 ) index = headers . index ( 'download count' ) total downloads = sum ( int ( row [ index ] ) for row in rows ) rows . insert ( 0 , headers ) return total downloads , index", "predictions": ["get the total downloads from the given rows ."], "references": ["return the total downloads and the downloads column"], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 2451, "code": "def add download total ( rows ) : total row = [ \"\" ] * len ( rows [ 0 ] ) total row [ 0 ] = \"Total\" total downloads , downloads column = get download total ( rows ) total row [ downloads column ] = str ( total downloads ) rows . append ( total row ) return rows", "predictions": ["add total number of rows to the download ."], "references": ["add a final row to rows showing the total downloads"], "bleu": 0.16621692209732, "rouge_l": 0.31282051282051276}
{"id": 2452, "code": "def find and patch entry ( soup , entry ) : link = soup . find ( \"a\" , { \"class\" : \"headerlink\" } , href = \"#\" + entry . anchor ) tag = soup . new tag ( \"a\" ) tag [ \"name\" ] = APPLE REF TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( \"module-\" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False", "predictions": ["find an entry and patch it to a tag"], "references": ["modify soup so dash . app can generate tocs on the fly ."], "bleu": 0.0760978866869656, "rouge_l": 0.0}
{"id": 2453, "code": "def main ( source , force , name , quiet , verbose , destination , add to dash , add to global , icon , index page , enable js , online redirect url , parser , ) : try : logging . config . dict Config ( create log config ( verbose = verbose , quiet = quiet ) ) except Value Error as e : click . secho ( e . args [ 0 ] , fg = \"red\" ) raise System Exit ( 1 ) if icon : icon data = icon . read ( ) if not icon data . startswith ( PNG HEADER ) : log . error ( '\"{}\" is not a valid PNG image.' . format ( click . format filename ( icon . name ) ) ) raise System Exit ( 1 ) else : icon data = None source , dest , name = setup paths ( source , destination , name = name , add to global = add to global , force = force , ) if parser is None : parser = parsers . get doctype ( source ) if parser is None : log . error ( '\"{}\" does not contain a known documentation format.' . format ( click . format filename ( source ) ) ) raise System Exit ( errno . EINVAL ) docset = prepare docset ( source , dest , name , index page , enable js , online redirect url ) doc parser = parser ( doc path = docset . docs ) log . info ( ( \"Converting \" + click . style ( \"{parser name}\" , bold = True ) + ' docs from \"{src}\" to \"{dst}\".' ) . format ( parser name = parser . name , src = click . format filename ( source , shorten = True ) , dst = click . format filename ( dest ) , ) ) with docset . db conn : log . info ( \"Parsing documentation...\" ) toc = patch anchors ( doc parser , show progressbar = not quiet ) for entry in doc parser . parse ( ) : docset . db conn . execute ( \"INSERT INTO search Index VALUES (NULL, ?, ?, ?)\" , entry . as tuple ( ) , ) toc . send ( entry ) count = docset . db conn . execute ( \"SELECT COUNT(1) FROM search Index\" ) . fetchone ( ) [ 0 ] log . info ( ( \"Added \" + click . style ( \"{count:,}\" , fg = \"green\" if count > 0 else \"red\" ) + \" index entries.\" ) . format ( count = count ) ) toc . close ( ) if icon data : add icon ( icon data , dest ) if add to dash or add to global : log . info ( \"Adding to Dash.app...\" ) os . system ( 'open -a dash \"{}\"' . format ( dest ) )", "predictions": ["parse arguments and add a script to the remote instance ."], "references": ["convert docs from source to dash . app s docset format ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 2454, "code": "def create log config ( verbose , quiet ) : if verbose and quiet : raise Value Error ( \"Supplying both --quiet and --verbose makes no sense.\" ) elif verbose : level = logging . DEBUG elif quiet : level = logging . ERROR else : level = logging . INFO logger cfg = { \"handlers\" : [ \"click handler\" ] , \"level\" : level } return { \"version\" : 1 , \"formatters\" : { \"click formatter\" : { \"format\" : \"%(message)s\" } } , \"handlers\" : { \"click handler\" : { \"level\" : level , \"class\" : \"doc2dash. main .Click Echo Handler\" , \"formatter\" : \"click formatter\" , } } , \"loggers\" : { \"doc2dash\" : logger cfg , \" main \" : logger cfg } , }", "predictions": ["returns a log config dict with defaults defaults ."], "references": ["we use logging s levels as an easy - to - use verbosity controller ."], "bleu": 0.07246365075155338, "rouge_l": 0.07973856209150328}
{"id": 2455, "code": "def setup paths ( source , destination , name , add to global , force ) : if source [ - 1 ] == \"/\" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( \".docset\" ) : name = name . replace ( \".docset\" , \"\" ) if add to global : destination = DEFAULT DOCSET PATH dest = os . path . join ( destination or \"\" , name + \".docset\" ) dst exists = os . path . lexists ( dest ) if dst exists and force : shutil . rmtree ( dest ) elif dst exists : log . error ( 'Destination path \"{}\" already exists.' . format ( click . format filename ( dest ) ) ) raise System Exit ( errno . EEXIST ) return source , dest , name", "predictions": ["setup source and destination paths"], "references": ["determine source and destination using the options ."], "bleu": 0.278869164867688, "rouge_l": 0.44309927360774815}
{"id": 2456, "code": "def add icon ( icon data , dest ) : with open ( os . path . join ( dest , \"icon.png\" ) , \"wb\" ) as f : f . write ( icon data )", "predictions": ["add a file to the file"], "references": ["add icon to docset"], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 2457, "code": "def has file with ( path , filename , content ) : try : with open ( os . path . join ( path , filename ) , \"rb\" ) as f : return content in f . read ( ) except IO Error as e : if e . errno == errno . ENOENT : return False else : raise", "predictions": ["checks if a file has a file with the given content ."], "references": ["check whether * filename * in * path * contains the string * content * ."], "bleu": 0.08853619422215007, "rouge_l": 0.2089041095890411}
{"id": 2458, "code": "def to atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + integer types ) : raise Value Error ( \"Amount '{}' doesn't have numeric type. Only Decimal, int, long and \" \"float (not recommended) are accepted as amounts.\" ) return int ( amount * 10 ** 12 )", "predictions": ["convert a amount of amount to an int ."], "references": ["convert monero decimal to atomic integer of piconero ."], "bleu": 0.17747405280050263, "rouge_l": 0.3333333333333333}
{"id": 2459, "code": "def encode ( cls , hex ) : out = [ ] for i in range ( len ( hex ) // 8 ) : word = endian swap ( hex [ 8 * i : 8 * i + 8 ] ) x = int ( word , 16 ) w1 = x % cls . n w2 = ( x // cls . n + w1 ) % cls . n w3 = ( x // cls . n // cls . n + w2 ) % cls . n out += [ cls . word list [ w1 ] , cls . word list [ w2 ] , cls . word list [ w3 ] ] checksum = cls . get checksum ( \" \" . join ( out ) ) out . append ( checksum ) return \" \" . join ( out )", "predictions": ["encode a hex hex string to binary format ."], "references": ["convert hexadecimal string to mnemonic word representation with checksum ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 2460, "code": "def decode ( cls , phrase ) : phrase = phrase . split ( \" \" ) out = \"\" for i in range ( len ( phrase ) // 3 ) : word1 , word2 , word3 = phrase [ 3 * i : 3 * i + 3 ] w1 = cls . word list . index ( word1 ) w2 = cls . word list . index ( word2 ) % cls . n w3 = cls . word list . index ( word3 ) % cls . n x = w1 + cls . n * ( ( w2 - w1 ) % cls . n ) + cls . n * cls . n * ( ( w3 - w2 ) % cls . n ) out += endian swap ( \"%08x\" % x ) return out", "predictions": ["decode a single phrase phrase ."], "references": ["calculate hexadecimal representation of the phrase ."], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 2461, "code": "def get operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n args = len ( inspect . getargspec ( op ) [ 0 ] ) if n args != 2 : raise Type Error except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op", "predictions": ["get the operator function. or accept if it s a operation operators"], "references": ["assigns function to the operators property of the instance ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 2462, "code": "def format answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( \"Error: '{}' not in {}\" . format ( fmt , fmts ) ) return def stringify ( val ) : if type ( val ) in ( list , tuple ) : return ', ' . join ( str ( e ) for e in val ) return val if fmt == 'obj' : return json . dumps ( self . answers ) elif fmt == 'array' : answers = [ [ k , v ] for k , v in self . answers . items ( ) ] return json . dumps ( answers ) elif fmt == 'plain' : answers = '\\n' . join ( '{}: {}' . format ( k , stringify ( v ) ) for k , v in self . answers . items ( ) ) return answers", "predictions": ["format the node as a string ."], "references": ["formats answers depending on fmt ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2463, "code": "def answer display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = len ) ) + 5 for key in list ( self . answers . keys ( ) ) : s += '{:>{}} : {}\\n' . format ( key , padding , self . answers [ key ] ) return s", "predictions": ["display a string with the contents of the answer"], "references": ["helper method for displaying the answers so far ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2464, "code": "def train and save ( obj , cache , data , print updates ) : obj . train ( data ) if print updates : print ( 'Regenerated ' + obj . name + '.' ) obj . save ( cache )", "predictions": ["on a imported object map map map obj and fragment"], "references": ["internal pickleable function used to train objects in another process"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2465, "code": "def main ( src , pyi dir , target dir , incremental , quiet , replace any , hg , traceback ) : Config . incremental = incremental Config . replace any = replace any returncode = 0 for src entry in src : for file , error , exc type , tb in retype path ( Path ( src entry ) , pyi dir = Path ( pyi dir ) , targets = Path ( target dir ) , src explicitly given = True , quiet = quiet , hg = hg , ) : print ( f'error: {file}: {error}' , file = sys . stderr ) if traceback : print ( 'Traceback (most recent call last):' , file = sys . stderr ) for line in tb : print ( line , file = sys . stderr , end = '' ) print ( f'{exc type. name }: {error}' , file = sys . stderr ) returncode += 1 if not src and not quiet : print ( 'warning: no sources given' , file = sys . stderr ) sys . exit ( min ( returncode , 125 ) )", "predictions": ["on the . py file print"], "references": ["re - apply type annotations from . pyi stubs to your codebase ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 2466, "code": "def retype path ( src , pyi dir , targets , * , src explicitly given = False , quiet = False , hg = False ) : if src . is dir ( ) : for child in src . iterdir ( ) : if child == pyi dir or child == targets : continue yield from retype path ( child , pyi dir / src . name , targets / src . name , quiet = quiet , hg = hg , ) elif src . suffix == '.py' or src explicitly given : try : retype file ( src , pyi dir , targets , quiet = quiet , hg = hg ) except Exception as e : yield ( src , str ( e ) , type ( e ) , traceback . format tb ( e . traceback ) , )", "predictions": ["generate on on on on a given pos"], "references": ["recursively retype files or directories given . generate errors ."], "bleu": 0.13821693129588736, "rouge_l": 0.10892857142857142}
{"id": 2467, "code": "def lib2to3 parse ( src txt ) : grammar = pygram . python grammar no print statement drv = driver . Driver ( grammar , pytree . convert ) if src txt [ - 1 ] != '\\n' : nl = '\\r\\n' if '\\r\\n' in src txt [ : 1024 ] else '\\n' src txt += nl try : result = drv . parse string ( src txt , True ) except Parse Error as pe : lineno , column = pe . context [ 1 ] lines = src txt . splitlines ( ) try : faulty line = lines [ lineno - 1 ] except Index Error : faulty line = \"<line number missing in source>\" raise Value Error ( f\"Cannot parse: {lineno}:{column}: {faulty line}\" ) from None if isinstance ( result , Leaf ) : result = Node ( syms . file input , [ result ] ) return result", "predictions": ["parse parsing a markers"], "references": ["given a string with source return the lib2to3 node ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2468, "code": "def lib2to3 unparse ( node , * , hg = False ) : code = str ( node ) if hg : from retype hgext import apply job security code = apply job security ( code ) return code", "predictions": ["apply a self . node"], "references": ["given a lib2to3 node return its string representation ."], "bleu": 0.1458826981425239, "rouge_l": 0.2717149220489978}
{"id": 2469, "code": "def fix remaining type comments ( node ) : assert node . type == syms . file input last n = None for n in node . post order ( ) : if last n is not None : if n . type == token . NEWLINE and is assignment ( last n ) : fix variable annotation type comment ( n , last n ) elif n . type == syms . funcdef and last n . type == syms . suite : fix signature annotation type comment ( n , last n , offset = 1 ) elif n . type == syms . async funcdef and last n . type == syms . suite : fix signature annotation type comment ( n , last n , offset = 2 ) last n = n", "predictions": ["on marker self = d self = 1 = 0 = 1 = 1"], "references": ["converts type comments in node to proper annotated assignments ."], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 2470, "code": "def parse type comment ( type comment ) : try : result = ast3 . parse ( type comment , '<type comment>' , 'eval' ) except Syntax Error : raise Value Error ( f\"invalid type comment: {type comment!r}\" ) from None assert isinstance ( result , ast3 . Expression ) return result . body", "predictions": ["on a marker comment on the marker . ."], "references": ["parse a type comment string into ast nodes ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 2471, "code": "def remove function signature type comment ( body ) : for node in body . children : if node . type == token . INDENT : prefix = node . prefix . lstrip ( ) if prefix . startswith ( ) : node . prefix = '\\n' . join ( prefix . split ( '\\n' ) [ 1 : ] ) break", "predictions": ["data type type comment"], "references": ["removes the legacy signature type comment leaving other comments if any ."], "bleu": 0.06399610426154731, "rouge_l": 0.22932330827067668}
{"id": 2472, "code": "def new ( n , prefix = None ) : if isinstance ( n , Leaf ) : return Leaf ( n . type , n . value , prefix = n . prefix if prefix is None else prefix ) n . parent = None if prefix is not None : n . prefix = prefix return n", "predictions": ["create a train object from a prefix instance . . . . . . . . . . . . . . . . . ."], "references": ["lib2to3 s ast requires unique objects as children ."], "bleu": 0.04668049023095243, "rouge_l": 0.0626283367556468}
{"id": 2473, "code": "def histogram match ( self , use bands , blm source = None , * * kwargs ) : assert has rio , \"To match image histograms please install rio hist\" data = self . read ( self [ use bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) if 0 in data : data = np . ma . masked values ( data , 0 ) bounds = self . reproject ( box ( * self . bounds ) , from proj = self . proj , to proj = \"EPSG:4326\" ) . bounds if blm source == 'browse' : from gbdxtools . images . browse image import Browse Image ref = Browse Image ( self . cat id , bbox = bounds ) . read ( ) else : from gbdxtools . images . tms image import Tms Image tms = Tms Image ( zoom = self . calc tms zoom ( self . affine [ 0 ] ) , bbox = bounds , * * kwargs ) ref = np . rollaxis ( tms . read ( ) , 0 , 3 ) out = np . dstack ( [ rio match ( data [ : , : , idx ] , ref [ : , : , idx ] . astype ( np . double ) / 255.0 ) for idx in range ( data . shape [ - 1 ] ) ] ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . histogram stretch ( out , * * kwargs ) else : return out", "predictions": ["zoom print print print out of a print"], "references": ["match the histogram to existing imagery"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2474, "code": "def histogram stretch ( self , use bands , * * kwargs ) : data = self . read ( self [ use bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) return self . histogram stretch ( data , * * kwargs )", "predictions": ["return config using config"], "references": ["entry point for contrast stretching"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2475, "code": "def deprecate module attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def getattr ( self , attr ) : if attr in deprecated : warnings . warn ( \"Property {} is deprecated\" . format ( attr ) , GBDX Deprecation ) return getattr ( mod , attr ) def setattr ( self , attr , value ) : if attr in deprecated : warnings . warn ( \"Property {} is deprecated\" . format ( attr ) , GBDX Deprecation ) return setattr ( mod , attr , value ) return Wrapper ( )", "predictions": ["returns a directory that warns a sources of dirs if dirs is dirs if not set to dirs"], "references": ["return a wrapped object that warns about deprecated accesses"], "bleu": 0.09629943614188137, "rouge_l": 0.2364341085271318}
{"id": 2476, "code": "def get matching multiplex port ( self , name ) : matching multiplex ports = [ self . getattribute ( p ) for p in self . portnames if name . startswith ( p ) and name != p and hasattr ( self , p ) and self . getattribute ( p ) . is multiplex ] for port in matching multiplex ports : return port return None", "predictions": ["module config sources for a multiplex"], "references": ["given a name figure out if a multiplex port prefixes this name and return it . otherwise return none ."], "bleu": 0.028191241370329752, "rouge_l": 0.14022988505747128}
{"id": 2477, "code": "def ingest vectors ( self , output port value ) : ingest task = Task ( 'Ingest Item Json To Vector Services' ) ingest task . inputs . items = output port value ingest task . impersonation allowed = True stage task = Task ( 'Stage Data To S3' ) stage task . inputs . destination = 's3://{vector ingest bucket}/{recipe id}/{run id}/{task name}' stage task . inputs . data = ingest task . outputs . result . value self . definition [ 'tasks' ] . append ( ingest task . generate task workflow json ( ) ) self . definition [ 'tasks' ] . append ( stage task . generate task workflow json ( ) )", "predictions": ["filter the dict with the keys and keys that were saved"], "references": ["append two required tasks to the given output to ingest to vs"], "bleu": 0.10400927574124633, "rouge_l": 0.08628005657708629}
{"id": 2478, "code": "def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api key = os . environ . get ( 'MAPBOX API KEY' , None ) , image = None , image bounds = None , index = \"vector-user-provided\" , name = \"GBDX Task Output\" , * * kwargs ) : try : from I Python . display import display except : print ( \"I Python is required to produce maps.\" ) return assert api key is not None , \"No Mapbox API Key found. You can either pass in a token or set the MAPBOX API KEY environment variable.\" wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = index ) union = cascaded union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] url = 'https://vector.geobigdata.io/insight-vector/api/mvt/{z}/{x}/{y}?' url += 'q={}&index={}' . format ( query , index ) if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] map id = \"map {}\" . format ( str ( int ( time . time ( ) ) ) ) map data = Vector Tile Layer ( url , source name = name , styles = styles , * * kwargs ) image layer = self . build image layer ( image , image bounds ) template = Base Template ( map id , * * { \"lat\" : lat , \"lon\" : lon , \"zoom\" : zoom , \"datasource\" : json . dumps ( map data . datasource ) , \"layers\" : json . dumps ( map data . layers ) , \"image layer\" : image layer , \"mbkey\" : api key , \"token\" : self . gbdx connection . access token } ) template . inject ( )", "predictions": ["build a map deploy on the features"], "references": ["renders a mapbox gl map from a vector service query"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2479, "code": "def parse geoms ( self , * * kwargs ) : bbox = kwargs . get ( 'bbox' , None ) wkt geom = kwargs . get ( 'wkt' , None ) geojson = kwargs . get ( 'geojson' , None ) if bbox is not None : g = box ( * bbox ) elif wkt geom is not None : g = wkt . loads ( wkt geom ) elif geojson is not None : g = shape ( geojson ) else : return None if self . proj is None : return g else : return self . reproject ( g , from proj = kwargs . get ( 'from proj' , 'EPSG:4326' ) )", "predictions": ["parse auth from encode a auth object"], "references": ["finds supported geometry types parses them and returns the bbox"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 2480, "code": "def load url ( url , shape = ( 8 , 256 , 256 ) ) : thread id = threading . current thread ( ) . ident curl = curl pool [ thread id ] curl . setopt ( curl . URL , url ) curl . setopt ( pycurl . NOSIGNAL , 1 ) , ext = os . path . splitext ( urlparse ( url ) . path ) with Named Temporary File ( prefix = \"gbdxtools\" , suffix = \".\" + ext , delete = False ) as temp : curl . setopt ( curl . WRITEDATA , temp . file ) curl . perform ( ) code = curl . getinfo ( pycurl . HTTP CODE ) try : if ( code != 200 ) : raise Type Error ( \"Request for {} returned unexpected error code: {}\" . format ( url , code ) ) arr = np . rollaxis ( imread ( temp ) , 2 , 0 ) except Exception as e : print ( e ) temp . seek ( 0 ) print ( temp . read ( ) ) arr = np . zeros ( shape , dtype = np . uint8 ) curl . close ( ) del curl pool [ thread id ] finally : temp . file . flush ( ) temp . close ( ) os . remove ( temp . name ) return arr", "predictions": ["write a indent file"], "references": ["loads a geotiff url inside a thread and returns as an ndarray"], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2481, "code": "def tile coords ( self , bounds ) : tfm = partial ( pyproj . transform , pyproj . Proj ( init = \"epsg:3857\" ) , pyproj . Proj ( init = \"epsg:4326\" ) ) bounds = ops . transform ( tfm , box ( * bounds ) ) . bounds west , south , east , north = bounds epsilon = 1.0e-10 if east != west and north != south : west += epsilon south += epsilon east -= epsilon north -= epsilon params = [ west , south , east , north , [ self . zoom level ] ] tile coords = [ ( tile . x , tile . y ) for tile in mercantile . tiles ( * params ) ] xtiles , ytiles = zip ( * tile coords ) minx = min ( xtiles ) miny = min ( ytiles ) maxx = max ( xtiles ) maxy = max ( ytiles ) return minx , miny , maxx , maxy", "predictions": ["calculate the validate and zoom"], "references": ["convert mercator bbox to tile index limits"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 2482, "code": "def load url ( url , token , shape = ( 8 , 256 , 256 ) ) : , ext = os . path . splitext ( urlparse ( url ) . path ) success = False for i in xrange ( MAX RETRIES ) : thread id = threading . current thread ( ) . ident curl = curl pool [ thread id ] curl . setopt ( curl . URL , url ) curl . setopt ( pycurl . NOSIGNAL , 1 ) curl . setopt ( pycurl . HTTPHEADER , [ 'Authorization: Bearer {}' . format ( token ) ] ) with Named Temporary File ( prefix = \"gbdxtools\" , suffix = ext , delete = False ) as temp : curl . setopt ( curl . WRITEDATA , temp . file ) curl . perform ( ) code = curl . getinfo ( pycurl . HTTP CODE ) try : if ( code != 200 ) : raise Type Error ( \"Request for {} returned unexpected error code: {}\" . format ( url , code ) ) temp . file . flush ( ) temp . close ( ) arr = imread ( temp . name ) if len ( arr . shape ) == 3 : arr = np . rollaxis ( arr , 2 , 0 ) else : arr = np . expand dims ( arr , axis = 0 ) success = True return arr except Exception as e : curl . close ( ) del curl pool [ thread id ] finally : temp . close ( ) os . remove ( temp . name ) if success is False : raise Type Error ( \"Request for {} returned unexpected error code: {}\" . format ( url , code ) ) return arr", "predictions": ["get a download download download and parse it into a file"], "references": ["loads a geotiff url inside a thread and returns as an ndarray"], "bleu": 0.12368857073777001, "rouge_l": 0.17256011315417258}
{"id": 2483, "code": "def validate ( method ) : name error = 'configuration option \"{}\" is not supported' @ functools . wraps ( method ) def validator ( self , name , * args ) : if name not in self . allowed opts : raise Value Error ( name error . format ( name ) ) return method ( self , name , * args ) return validator", "predictions": ["check if the total option is valid downloads downloads downloads downloads downloads downloads downloads downloads downloads downloads downloads ."], "references": ["config option name value validator decorator ."], "bleu": 0.0712695567709093, "rouge_l": 0.16781292984869325}
{"id": 2484, "code": "def run ( self , ctx ) : if ctx . reverse : self . engine . reverse ( ) if self . engine . empty : raise Assertion Error ( 'grappa: no assertions to run' ) try : return self . run assertions ( ctx ) except Exception as err : if getattr ( err , ' legit ' , False ) : raise err return self . render error ( ctx , err )", "predictions": ["find and return a result of the result ."], "references": ["runs the current phase ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 2485, "code": "def run matcher ( self , subject , * expected , * * kw ) : self . expected = expected args = ( subject , ) if self . kind == Operator Types . MATCHER : args += expected try : result = self . match ( * args , * * kw ) except Exception as error : return self . make error ( error = error ) reasons = [ ] if isinstance ( result , tuple ) : result , reasons = result if result is False and self . ctx . negate : return True if result is True and not self . ctx . negate : return True return self . make error ( reasons = reasons )", "predictions": ["see definition in cirq . simulatessamples ."], "references": ["runs the operator matcher test function ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2486, "code": "def load ( ) : for operator in operators : module , symbols = operator [ 0 ] , operator [ 1 : ] path = 'grappa.operators.{}' . format ( module ) operator = import ( path , None , None , symbols ) for symbol in symbols : Engine . register ( getattr ( operator , symbol ) )", "predictions": ["create an operator instance of the operator modules"], "references": ["loads the built - in operators into the global test engine ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 2487, "code": "def register operators ( * operators ) : def validate ( operator ) : if isoperator ( operator ) : return True raise Not Implemented Error ( 'invalid operator: {}' . format ( operator ) ) def register ( operator ) : for name in operator . operators : if name in Engine . operators : raise Value Error ( 'operator name \"{}\" from {} is already ' 'in use by other operator' . format ( name , operator . name ) ) Engine . operators [ name ] = operator [ register ( operator ) for operator in operators if validate ( operator ) ]", "predictions": ["decorator to setup an paths"], "references": ["registers one or multiple operators in the test engine ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2488, "code": "def play pause ( self ) : self . player interface . Play Pause ( ) self . is playing = not self . is playing if self . is playing : self . play Event ( self ) else : self . pause Event ( self )", "predictions": ["add the icon to the icon . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["pause playback if currently playing otherwise start playing if currently paused ."], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 2489, "code": "def play sync ( self ) : self . play ( ) logger . info ( \"Playing synchronously\" ) try : time . sleep ( 0.05 ) logger . debug ( \"Wait for playing to start\" ) while self . is playing ( ) : time . sleep ( 0.05 ) except D Bus Exception : logger . error ( \"Cannot play synchronously any longer as D Bus calls timed out.\" )", "predictions": ["has been return true if the file is return false otherwise filename filename filename filename to has been return true ."], "references": ["play the video and block whilst the video is playing"], "bleu": 0.06429451441231726, "rouge_l": 0.13785310734463277}
{"id": 2490, "code": "def play ( self ) : if not self . is playing ( ) : self . play pause ( ) self . is playing = True self . play Event ( self )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["play the video asynchronously returning control immediately to the calling code"], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 2491, "code": "def quit ( self ) : if self . process is None : logger . debug ( 'Quit was called after self. process had already been released' ) return try : logger . debug ( 'Quitting OMX Player' ) process group id = os . getpgid ( self . process . pid ) os . killpg ( process group id , signal . SIGTERM ) logger . debug ( 'SIGTERM Sent to pid: %s' % process group id ) self . process monitor . join ( ) except OS Error : logger . error ( 'Could not find the process to kill' ) self . process = None", "predictions": ["encode the out out out of the out out of the out out of the out group . . . . group . . . . . . . . ."], "references": ["quit the player blocking until the process has died"], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 2492, "code": "def has edit permission ( self , request ) : return request . user . is authenticated and request . user . is active and request . user . is staff", "predictions": ["check if the out of the out of the out . . . . . . . . . . . . . . . . . . . . ."], "references": ["can edit this object"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2493, "code": "def has add permission ( self , request ) : return request . user . is authenticated and request . user . is active and request . user . is staff", "predictions": ["check if the user is try to be logged in the request ."], "references": ["can add this object"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2494, "code": "def has delete permission ( self , request ) : return request . user . is authenticated and request . user . is active and request . user . is superuser", "predictions": ["check if the user format is answers to be logged in the = = true ."], "references": ["can delete this object"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2495, "code": "def set fields ( self ) : if self . is initialized : self . model map dict = self . create document dictionary ( self . model instance ) else : self . model map dict = self . create document dictionary ( self . model ) form field dict = self . get form field dict ( self . model map dict ) self . set form fields ( form field dict )", "predictions": ["answer to update display display in model"], "references": ["sets existing data to form fields ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2496, "code": "def get form ( self ) : self . set fields ( ) if self . post data dict is not None : self . set post data ( ) return self . form", "predictions": ["returns the form s form ."], "references": ["generate the form for view ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 2497, "code": "def get widget ( model field , disabled = False ) : attrs = get attrs ( model field , disabled ) if hasattr ( model field , \"max length\" ) and not model field . max length : return forms . Textarea ( attrs = attrs ) elif isinstance ( model field , Date Time Field ) : return forms . Date Time Input ( attrs = attrs ) elif isinstance ( model field , Boolean Field ) : return forms . Checkbox Input ( attrs = attrs ) elif isinstance ( model field , Reference Field ) or model field . choices : return forms . Select ( attrs = attrs ) elif ( isinstance ( model field , List Field ) or isinstance ( model field , Embedded Document Field ) or isinstance ( model field , Geo Point Field ) ) : return None else : return forms . Text Input ( attrs = attrs )", "predictions": ["returns a widget widget for the given model field ."], "references": ["choose which widget to display for a field ."], "bleu": 0.1972940627795883, "rouge_l": 0.42508710801393734}
{"id": 2498, "code": "def get attrs ( model field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isinstance ( model field , Object Id Field ) : attrs [ 'class' ] += ' disabled' attrs [ 'readonly' ] = 'readonly' return attrs", "predictions": ["returns a dictionary of attributes for a given model field"], "references": ["set attributes on the display widget ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2499, "code": "def get form field class ( model field ) : FIELD MAPPING = { Int Field : forms . Integer Field , String Field : forms . Char Field , Float Field : forms . Float Field , Boolean Field : forms . Boolean Field , Date Time Field : forms . Date Time Field , Decimal Field : forms . Decimal Field , URL Field : forms . URL Field , Email Field : forms . Email Field } return FIELD MAPPING . get ( model field . class , forms . Char Field )", "predictions": ["returns a flat form for a model field ."], "references": ["gets the default form field for a mongoenigne field ."], "bleu": 0.2187537716852318, "rouge_l": 0.5213675213675214}
{"id": 2500, "code": "def get document value ( document , key ) : value = getattr ( document , key ) if isinstance ( value , Object Id ) : return value if isinstance ( document . fields . get ( key ) , URL Field ) : return mark safe ( \"\"\"<a href=\"{0}\">{1}</a>\"\"\" . format ( value , value ) ) if isinstance ( value , Document ) : app label = value . module . replace ( \".models\" , \"\" ) document name = value . class name url = reverse ( \"document detail\" , kwargs = { 'app label' : app label , 'document name' : document name , 'id' : value . id } ) return mark safe ( \"\"\"<a href=\"{0}\">{1}</a>\"\"\" . format ( url , value ) ) return value", "predictions": ["get the document value for a document ."], "references": ["returns the display value of a field for a particular mongodb document ."], "bleu": 0.15451666492113134, "rouge_l": 0.5479041916167664}
{"id": 2501, "code": "def get context data ( self , * * kwargs ) : context = super ( Document List View , self ) . get context data ( * * kwargs ) context = self . set permissions in context ( context ) if not context [ 'has view permission' ] : return Http Response Forbidden ( \"You do not have permissions to view this content.\" ) context [ 'object list' ] = self . get queryset ( ) context [ 'document' ] = self . document context [ 'app label' ] = self . app label context [ 'document name' ] = self . document name context [ 'request' ] = self . request context [ 'page' ] = self . page context [ 'documents per page' ] = self . documents per page if self . page > 1 : previous page number = self . page - 1 else : previous page number = None if self . page < self . total pages : next page number = self . page + 1 else : next page number = None context [ 'previous page number' ] = previous page number context [ 'has previous page' ] = previous page number is not None context [ 'next page number' ] = next page number context [ 'has next page' ] = next page number is not None context [ 'total pages' ] = self . total pages if self . queryset . count ( ) : context [ 'keys' ] = [ 'id' , ] for key in [ x for x in self . mongoadmin . list fields if x != 'id' and x in self . document . fields . keys ( ) ] : if isinstance ( self . document . fields [ key ] , Embedded Document Field ) : continue if isinstance ( self . document . fields [ key ] , List Field ) : continue context [ 'keys' ] . append ( key ) if self . mongoadmin . search fields : context [ 'search field' ] = True return context", "predictions": ["add the context data to the template ."], "references": ["injects data into the context to replicate cbv listview ."], "bleu": 0.19546825878823415, "rouge_l": 0.43571428571428567}
{"id": 2502, "code": "def post ( self , request , * args , * * kwargs ) : form class = self . get form class ( ) form = self . get form ( form class ) mongo ids = self . get initial ( ) [ 'mongo id' ] for form mongo id in form . data . getlist ( 'mongo id' ) : for mongo id in mongo ids : if form mongo id == mongo id : self . document . objects . get ( pk = mongo id ) . delete ( ) return self . form invalid ( form )", "predictions": ["handles post requests ."], "references": ["creates new mongoengine records ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 2503, "code": "def get mongoadmins ( self ) : apps = [ ] for app name in settings . INSTALLED APPS : mongoadmin = \"{0}.mongoadmin\" . format ( app name ) try : module = import module ( mongoadmin ) except Import Error as e : if str ( e ) . startswith ( \"No module named\" ) : continue raise e app store = App Store ( module ) apps . append ( dict ( app name = app name , obj = app store ) ) return apps", "predictions": ["get all the modules that are done in this project ."], "references": ["returns a list of all mongoadmin implementations for the site"], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 2504, "code": "def set mongonaut base ( self ) : if hasattr ( self , \"app label\" ) : return None self . app label = self . kwargs . get ( 'app label' ) self . document name = self . kwargs . get ( 'document name' ) self . models name = self . kwargs . get ( 'models name' , 'models' ) self . model name = \"{0}.{1}\" . format ( self . app label , self . models name ) self . models = import module ( self . model name )", "predictions": ["sets the mongonaut models to be used for drawing ."], "references": ["sets a number of commonly used attributes"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2505, "code": "def set permissions in context ( self , context = { } ) : context [ 'has view permission' ] = self . mongoadmin . has view permission ( self . request ) context [ 'has edit permission' ] = self . mongoadmin . has edit permission ( self . request ) context [ 'has add permission' ] = self . mongoadmin . has add permission ( self . request ) context [ 'has delete permission' ] = self . mongoadmin . has delete permission ( self . request ) return context", "predictions": ["add permissions in the context ."], "references": ["provides permissions for mongoadmin for use in the context"], "bleu": 0.26367153725164694, "rouge_l": 0.5147679324894514}
{"id": 2506, "code": "def set embedded doc ( self , document , form key , current key , remaining key ) : embedded doc = getattr ( document , current key , False ) if not embedded doc : embedded doc = document . fields [ current key ] . document type obj ( ) new key , new remaining key array = trim field key ( embedded doc , remaining key ) self . process document ( embedded doc , form key , make key ( new key , new remaining key array ) ) setattr ( document , current key , embedded doc )", "predictions": ["set embedded doc doc"], "references": ["get the existing embedded document if it exists else created it ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2507, "code": "def with tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get current timezone as TIME ZONE %}{{ TIME ZONE }}{% endlocaltime %}' ) c = Request Context ( request ) response = t . render ( c ) return Http Response ( response )", "predictions": ["a page to check for a single timezone ."], "references": ["get the time with tz enabled"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2508, "code": "def without tz ( request ) : t = Template ( '{% load tz %}{% get current timezone as TIME ZONE %}{{ TIME ZONE }}' ) c = Request Context ( request ) response = t . render ( c ) return Http Response ( response )", "predictions": ["get a list of timezone without a valid timezone ."], "references": ["get the time without tz enabled"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2509, "code": "def is valid ip ( ip address ) : try : ip = ipaddress . ip address ( u'' + ip address ) return True except Value Error as e : return False", "predictions": ["check if an ip address is a valid ip address ."], "references": ["check validity of an ip address"], "bleu": 0.22416933501922287, "rouge_l": 0.4969450101832994}
{"id": 2510, "code": "def is local ip ( ip address ) : try : ip = ipaddress . ip address ( u'' + ip address ) return ip . is loopback except Value Error as e : return None", "predictions": ["returns true if an ip is a local ip address ."], "references": ["check if ip is local"], "bleu": 0.17033186037639278, "rouge_l": 0.5362637362637362}
{"id": 2511, "code": "def search ( self ) : try : filters = json . loads ( self . query ) except Value Error : return False result = self . model query if 'filter' in filters . keys ( ) : result = self . parse filter ( filters [ 'filter' ] ) if 'sort' in filters . keys ( ) : result = result . order by ( * self . sort ( filters [ 'sort' ] ) ) return result", "predictions": ["returns a list of all filters matching the query ."], "references": ["this is the most important method"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2512, "code": "def parse filter ( self , filters ) : for filter type in filters : if filter type == 'or' or filter type == 'and' : conditions = [ ] for field in filters [ filter type ] : if self . is field allowed ( field ) : conditions . append ( self . create query ( self . parse field ( field , filters [ filter type ] [ field ] ) ) ) if filter type == 'or' : self . model query = self . model query . filter ( or ( * conditions ) ) elif filter type == 'and' : self . model query = self . model query . filter ( and ( * conditions ) ) else : if self . is field allowed ( filter type ) : conditions = self . create query ( self . parse field ( filter type , filters [ filter type ] ) ) self . model query = self . model query . filter ( conditions ) return self . model query", "predictions": ["parse a filter query"], "references": ["this method process the filters"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2513, "code": "def create query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ 2 ] model = self . model if '.' in field : field items = field . split ( '.' ) field name = getattr ( model , field items [ 0 ] , None ) class name = field name . property . mapper . class new model = getattr ( class name , field items [ 1 ] ) return field name . has ( OPERATORS [ operator ] ( new model , value ) ) return OPERATORS [ operator ] ( getattr ( model , field , None ) , value )", "predictions": ["create a query object"], "references": ["mix all values and make the query"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2514, "code": "def sendmail ( self , msg from , msg to , msg ) : SMTP dummy . msg from = msg from SMTP dummy . msg to = msg to SMTP dummy . msg = msg", "predictions": ["sends a message to the prefix"], "references": ["remember the recipients ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2515, "code": "def parsemail ( raw message ) : message = email . parser . Parser ( ) . parsestr ( raw message ) detected = chardet . detect ( bytearray ( raw message , \"utf-8\" ) ) encoding = detected [ \"encoding\" ] print ( \">>> encoding {}\" . format ( encoding ) ) for part in message . walk ( ) : if part . get content maintype ( ) == 'multipart' : continue part . set charset ( encoding ) addrs = email . utils . getaddresses ( message . get all ( \"TO\" , [ ] ) ) + email . utils . getaddresses ( message . get all ( \"CC\" , [ ] ) ) + email . utils . getaddresses ( message . get all ( \"BCC\" , [ ] ) ) recipients = [ x [ 1 ] for x in addrs ] message . delitem ( \"bcc\" ) message . setitem ( 'Date' , email . utils . formatdate ( ) ) sender = message [ \"from\" ] return ( message , sender , recipients )", "predictions": ["detect message from raw message ."], "references": ["parse message headers then remove bcc header ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 2516, "code": "def create boundary ( message ) : if not message . is multipart ( ) or message . get boundary ( ) is not None : return message from future . backports . email . generator import Generator boundary = Generator . make boundary ( message . policy . linesep ) message . set param ( 'boundary' , boundary ) return message", "predictions": ["create a message with a multipart policy ."], "references": ["add boundary parameter to multipart message if they are not present ."], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 2517, "code": "def make message multipart ( message ) : if not message . is multipart ( ) : multipart message = email . mime . multipart . MIME Multipart ( 'alternative' ) for header key in set ( message . keys ( ) ) : values = message . get all ( header key , failobj = [ ] ) for value in values : multipart message [ header key ] = value original text = message . get payload ( ) multipart message . attach ( email . mime . text . MIME Text ( original text ) ) message = multipart message message = create boundary ( message ) return message", "predictions": ["create multipart message from multipart message ."], "references": ["convert a message into a multipart message ."], "bleu": 0.31689174383082924, "rouge_l": 0.5269978401727862}
{"id": 2518, "code": "def convert markdown ( message ) : assert message [ 'Content-Type' ] . startswith ( \"text/markdown\" ) del message [ 'Content-Type' ] message = make message multipart ( message ) for payload item in set ( message . get payload ( ) ) : if payload item [ 'Content-Type' ] . startswith ( 'text/plain' ) : original text = payload item . get payload ( ) html text = markdown . markdown ( original text ) html payload = future . backports . email . mime . text . MIME Text ( \"<html><body>{}</body></html>\" . format ( html text ) , \"html\" , ) message . attach ( html payload ) return message", "predictions": ["converts markdown message to markdown"], "references": ["convert markdown in message text to html ."], "bleu": 0.1781815298791261, "rouge_l": 0.44309927360774815}
{"id": 2519, "code": "def addattachments ( message , template path ) : if 'attachment' not in message : return message , 0 message = make message multipart ( message ) attachment filepaths = message . get all ( 'attachment' , failobj = [ ] ) template parent dir = os . path . dirname ( template path ) for attachment filepath in attachment filepaths : attachment filepath = os . path . expanduser ( attachment filepath . strip ( ) ) if not attachment filepath : continue if not os . path . isabs ( attachment filepath ) : attachment filepath = os . path . join ( template parent dir , attachment filepath ) normalized path = os . path . abspath ( attachment filepath ) if not os . path . exists ( normalized path ) : print ( \"Error: can't find attachment \" + normalized path ) sys . exit ( 1 ) filename = os . path . basename ( normalized path ) with open ( normalized path , \"rb\" ) as attachment : part = email . mime . application . MIME Application ( attachment . read ( ) , Name = filename ) part . add header ( 'Content-Disposition' , 'attachment; filename=\"{}\"' . format ( filename ) ) message . attach ( part ) print ( \">>> attached {}\" . format ( normalized path ) ) del message [ 'attachment' ] return message , len ( attachment filepaths )", "predictions": ["read a message from a message"], "references": ["add the attachments from the message from the commandline options ."], "bleu": 0.12634437832866913, "rouge_l": 0.2234432234432234}
{"id": 2520, "code": "def sendmail ( message , sender , recipients , config filename ) : if not hasattr ( sendmail , \"host\" ) : config = configparser . Raw Config Parser ( ) config . read ( config filename ) sendmail . host = config . get ( \"smtp server\" , \"host\" ) sendmail . port = config . getint ( \"smtp server\" , \"port\" ) sendmail . username = config . get ( \"smtp server\" , \"username\" ) sendmail . security = config . get ( \"smtp server\" , \"security\" ) print ( \">>> Read SMTP server configuration from {}\" . format ( config filename ) ) print ( \">>>   host = {}\" . format ( sendmail . host ) ) print ( \">>>   port = {}\" . format ( sendmail . port ) ) print ( \">>>   username = {}\" . format ( sendmail . username ) ) print ( \">>>   security = {}\" . format ( sendmail . security ) ) if not hasattr ( sendmail , \"password\" ) : if sendmail . security == \"Dummy\" or sendmail . username == \"None\" : sendmail . password = None else : prompt = \">>> password for {} on {}: \" . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) if sendmail . security == \"SSL/TLS\" : smtp = smtplib . SMTP SSL ( sendmail . host , sendmail . port ) elif sendmail . security == \"STARTTLS\" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == \"Never\" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == \"Dummy\" : smtp = smtp dummy . SMTP dummy ( ) else : raise configparser . Error ( \"Unrecognized security type: {}\" . format ( sendmail . security ) ) if sendmail . username != \"None\" : smtp . login ( sendmail . username , sendmail . password ) smtp . sendmail ( sender , recipients , message . as string ( ) ) smtp . close ( )", "predictions": ["sends a smtp request to the smtp server"], "references": ["send email message using python smtp library ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 2521, "code": "def create sample input files ( template filename , database filename , config filename ) : print ( \"Creating sample template email {}\" . format ( template filename ) ) if os . path . exists ( template filename ) : print ( \"Error: file exists: \" + template filename ) sys . exit ( 1 ) with io . open ( template filename , \"w\" ) as template file : template file . write ( u\"TO: {{email}}\\n\" u\"SUBJECT: Testing mailmerge\\n\" u\"FROM: My Self <myself@mydomain.com>\\n\" u\"\\n\" u\"Hi, {{name}},\\n\" u\"\\n\" u\"Your number is {{number}}.\\n\" ) print ( \"Creating sample database {}\" . format ( database filename ) ) if os . path . exists ( database filename ) : print ( \"Error: file exists: \" + database filename ) sys . exit ( 1 ) with io . open ( database filename , \"w\" ) as database file : database file . write ( u'email,name,number\\n' u'myself@mydomain.com,\"Myself\",17\\n' u'bob@bobdomain.com,\"Bob\",42\\n' ) print ( \"Creating sample config file {}\" . format ( config filename ) ) if os . path . exists ( config filename ) : print ( \"Error: file exists: \" + config filename ) sys . exit ( 1 ) with io . open ( config filename , \"w\" ) as config file : config file . write ( u\"[smtp server]\\n\" u\"host = smtp.gmail.com\\n\" u\"port = 465\\n\" u\"security = SSL/TLS\\n\" u\"username = YOUR USERNAME HERE\\n\" u\"#\\n\" u\"#\\n\" u\"#\\n\" u\"#\\n\" ) print ( \"Edit these files, and then run mailmerge again\" )", "predictions": ["create a sample input files from the database"], "references": ["create sample template email and database ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 2522, "code": "def cli ( sample , dry run , limit , no limit , database filename , template filename , config filename ) : mailmerge . api . main ( sample = sample , dry run = dry run , limit = limit , no limit = no limit , database filename = database filename , template filename = template filename , config filename = config filename , )", "predictions": ["entry point for the mailmerge command line tool ."], "references": ["command line interface ."], "bleu": 0.19960198807747329, "rouge_l": 0.4959349593495934}
{"id": 2523, "code": "async def run tasks ( self ) : tasks = self . get tasks ( ) self . gathered tasks = asyncio . gather ( * tasks , loop = self . loop ) try : await self . gathered tasks except Cancelled Error : pass", "predictions": ["run the gathered loop ."], "references": ["run the tasks attached to the instance"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 2524, "code": "async def close ( self ) : tasks = self . get close tasks ( ) if tasks : await asyncio . wait ( tasks ) self . session = None", "predictions": ["close the connection ."], "references": ["properly close the client"], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 2525, "code": "def set debug ( ) : logging . basic Config ( level = logging . WARNING ) peony . logger . set Level ( logging . DEBUG )", "predictions": ["configure the debug level ."], "references": ["activates error messages useful during development"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2526, "code": "def clone with updates ( self , * * kwargs ) : fields dict = self . to dict ( ) fields dict . update ( kwargs ) return Binding Prediction ( * * fields dict )", "predictions": ["clone fields with updates and updates the result ."], "references": ["returns new bindingprediction with updated fields"], "bleu": 0.15619699684601276, "rouge_l": 0.13832199546485258}
{"id": 2527, "code": "def get data ( self , response ) : if self . response list : return response elif self . response key is None : if hasattr ( response , \"items\" ) : for key , data in response . items ( ) : if ( hasattr ( data , \" getitem \" ) and not hasattr ( data , \"items\" ) and len ( data ) > 0 and 'id' in data [ 0 ] ) : self . response key = key return data else : self . response list = True return response else : return response [ self . response key ] raise No Data Found ( response = response , url = self . request . get url ( ) )", "predictions": ["sets the data from the api"], "references": ["get the data from the response"], "bleu": 0.6147881529512643, "rouge_l": 0.6666666666666666}
{"id": 2528, "code": "def parse netchop ( netchop output ) : line iterator = iter ( netchop output . decode ( ) . split ( \"\\n\" ) ) scores = [ ] for line in line iterator : if \"pos\" in line and 'AA' in line and 'score' in line : scores . append ( [ ] ) if \"----\" not in next ( line iterator ) : raise Value Error ( \"Dashes expected\" ) line = next ( line iterator ) while '-------' not in line : score = float ( line . split ( ) [ 3 ] ) scores [ - 1 ] . append ( score ) line = next ( line iterator ) return scores", "predictions": ["get a list of form not from the form"], "references": ["parse netchop stdout ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2529, "code": "def to dataframe ( self , columns = Binding Prediction . fields + ( \"length\" , ) ) : return pd . Data Frame . from records ( [ tuple ( [ getattr ( x , name ) for name in columns ] ) for x in self ] , columns = columns )", "predictions": ["build a widget from a pandas dataframe ."], "references": ["converts collection of bindingprediction objects to dataframe"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2530, "code": "def predict peptides ( self , peptides ) : from mhcflurry . encodable sequences import Encodable Sequences binding predictions = [ ] encodable sequences = Encodable Sequences . create ( peptides ) for allele in self . alleles : predictions df = self . predictor . predict to dataframe ( encodable sequences , allele = allele ) for ( , row ) in predictions df . iterrows ( ) : binding prediction = Binding Prediction ( allele = allele , peptide = row . peptide , affinity = row . prediction , percentile rank = ( row . prediction percentile if 'prediction percentile' in row else nan ) , prediction method name = \"mhcflurry\" ) binding predictions . append ( binding prediction ) return Binding Prediction Collection ( binding predictions )", "predictions": ["get attrs from a given attrs ."], "references": ["predict mhc affinity for peptides ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2531, "code": "def check peptide inputs ( self , peptides ) : require iterable of ( peptides , string types ) check X = not self . allow X in peptides check lower = not self . allow lowercase in peptides check min length = self . min peptide length is not None min length = self . min peptide length check max length = self . max peptide length is not None max length = self . max peptide length for p in peptides : if not p . isalpha ( ) : raise Value Error ( \"Invalid characters in peptide '%s'\" % p ) elif check X and \"X\" in p : raise Value Error ( \"Invalid character 'X' in peptide '%s'\" % p ) elif check lower and not p . isupper ( ) : raise Value Error ( \"Invalid lowercase letters in peptide '%s'\" % p ) elif check min length and len ( p ) < min length : raise Value Error ( \"Peptide '%s' too short (%d chars), must be at least %d\" % ( p , len ( p ) , min length ) ) elif check max length and len ( p ) > max length : raise Value Error ( \"Peptide '%s' too long (%d chars), must be at least %d\" % ( p , len ( p ) , max length ) )", "predictions": ["ensure form field length are correct ."], "references": ["check peptide sequences to make sure they are valid for this predictor ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 2532, "code": "async def restart stream ( self ) : await self . response . release ( ) await asyncio . sleep ( self . error timeout ) await self . connect ( ) logger . info ( \"Reconnected to the stream\" ) self . reconnecting = False return { 'stream restart' : True }", "predictions": ["document value to safe = false = 0 = 0 = 0 = 0 = 0 = 0 = 1 = 0 = 0 = 1 = 0 = 0 ="], "references": ["restart the stream on error"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2533, "code": "def get error ( data ) : if isinstance ( data , dict ) : if 'errors' in data : error = data [ 'errors' ] [ 0 ] else : error = data . get ( 'error' , None ) if isinstance ( error , dict ) : if error . get ( 'code' ) in errors : return error", "predictions": ["extract context from context permissions permissions permissions permissions permissions permissions permissions permissions permissions permissions permissions permissions"], "references": ["return the error if there is a corresponding exception"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2534, "code": "async def throw ( response , loads = None , encoding = None , * * kwargs ) : if loads is None : loads = data processing . loads data = await data processing . read ( response , loads = loads , encoding = encoding ) error = get error ( data ) if error is not None : exception = errors [ error [ 'code' ] ] raise exception ( response = response , error = error , data = data , * * kwargs ) if response . status in statuses : exception = statuses [ response . status ] raise exception ( response = response , data = data , * * kwargs ) raise Peony Exception ( response = response , data = data , * * kwargs )", "predictions": ["throw a response response"], "references": ["get the response data if possible and raise an exception"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2535, "code": "def code ( self , code ) : def decorator ( exception ) : self [ code ] = exception return exception return decorator", "predictions": ["decorator to register an = get an = get a get method ."], "references": ["decorator to associate a code to an exception"], "bleu": 0.14283632578659286, "rouge_l": 0.2985318107667211}
{"id": 2536, "code": "def user headers ( self , headers = None ) : h = self . copy ( ) if headers is not None : keys = set ( headers . keys ( ) ) if h . get ( 'Authorization' , False ) : keys -= { 'Authorization' } for key in keys : h [ key ] = headers [ key ] return h", "predictions": ["returns a dictionary with the set of set headers headers"], "references": ["make sure the user doesn t override the authorization header"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 2537, "code": "async def run ( self , * args , data ) : cmd = self . get ( data . text ) try : if cmd is not None : command = self [ cmd ] ( * args , data = data ) return await peony . utils . execute ( command ) except : fmt = \"Error occurred while running function {cmd}:\" peony . utils . log error ( fmt . format ( cmd = cmd ) )", "predictions": ["execute a permission command"], "references": ["run the function you want"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2538, "code": "def has edge within group ( self , group ) : assert group in self . nodes . keys ( ) , \"{0} not one of the group of nodes\" . format ( group ) nodelist = self . nodes [ group ] for n1 , n2 in self . simplified edges ( ) : if n1 in nodelist and n2 in nodelist : return True", "predictions": ["check if a group set set set set set set set set set set doc doc to true"], "references": ["checks whether there are within - group edges or not ."], "bleu": 0.06809398432036522, "rouge_l": 0.07210401891252956}
{"id": 2539, "code": "def plot axis ( self , rs , theta ) : xs , ys = get cartesian ( rs , theta ) self . ax . plot ( xs , ys , 'black' , alpha = 0.3 )", "predictions": ["with the tz tz tz"], "references": ["renders the axis ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2540, "code": "def plot nodes ( self , nodelist , theta , group ) : for i , node in enumerate ( nodelist ) : r = self . internal radius + i * self . scale x , y = get cartesian ( r , theta ) circle = plt . Circle ( xy = ( x , y ) , radius = self . dot radius , color = self . node colormap [ group ] , linewidth = 0 ) self . ax . add patch ( circle )", "predictions": ["without a circle in the circle"], "references": ["plots nodes to screen ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2541, "code": "def group theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major angle", "predictions": ["as a list of valid angle angle"], "references": ["computes the theta along which a group s nodes are aligned ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 2542, "code": "def find node group membership ( self , node ) : for group , nodelist in self . nodes . items ( ) : if node in nodelist : return group", "predictions": ["is a local local ip ip"], "references": ["identifies the group for which a node belongs to ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2543, "code": "def get idx ( self , node ) : group = self . find node group membership ( node ) return self . nodes [ group ] . index ( node )", "predictions": ["return a try to a try to find the json json object"], "references": ["finds the index of the node in the sorted list ."], "bleu": 0.10390302174233558, "rouge_l": 0.08764367816091953}
{"id": 2544, "code": "def node radius ( self , node ) : return self . get idx ( node ) * self . scale + self . internal radius", "predictions": ["return the parse filter for a parse parse for a given parse for a given parse for a given parse for a parse for a parse for a given parse parse"], "references": ["computes the radial position of the node ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 2545, "code": "def node theta ( self , node ) : group = self . find node group membership ( node ) return self . group theta ( group )", "predictions": ["finds the query for a create create a create create a create"], "references": ["convenience function to find the node s theta angle ."], "bleu": 0.10390302174233558, "rouge_l": 0.09242424242424242}
{"id": 2546, "code": "def add edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw edge ( u , v , d , group )", "predictions": ["add the edges and replace all edges ."], "references": ["draws all of the edges in the graph ."], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 2547, "code": "def draw ( self ) : self . ax . set xlim ( - self . plot radius ( ) , self . plot radius ( ) ) self . ax . set ylim ( - self . plot radius ( ) , self . plot radius ( ) ) self . add axes and nodes ( ) self . add edges ( ) self . ax . axis ( 'off' )", "predictions": ["draw the axis and radius radius"], "references": ["the master function that is called that draws everything ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2548, "code": "def mods genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibliography' , 'unpublished' : 'article' } tp = str ( self . type ) . lower ( ) return type2genre . get ( tp , tp )", "predictions": ["boundary boundary of the create = = { = { = { = { = { = { = { = { = { if the create a if it s"], "references": ["guesses an appropriate mods xml genre type ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2549, "code": "def get publications ( context , template = 'publications/publications.html' ) : types = Type . objects . filter ( hidden = False ) publications = Publication . objects . select related ( ) publications = publications . filter ( external = False , type in = types ) publications = publications . order by ( '-year' , '-month' , '-id' ) if not publications : return '' populate ( publications ) return render template ( template , context [ 'request' ] , { 'publications' : publications } )", "predictions": ["return all message s message s message mime mime data"], "references": ["get all publications ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 2550, "code": "def get publication ( context , id ) : pbl = Publication . objects . filter ( pk = int ( id ) ) if len ( pbl ) < 1 : return '' pbl [ 0 ] . links = pbl [ 0 ] . customlink set . all ( ) pbl [ 0 ] . files = pbl [ 0 ] . customfile set . all ( ) return render template ( 'publications/publication.html' , context [ 'request' ] , { 'publication' : pbl [ 0 ] } )", "predictions": ["convert a single markdown to a single markdown file"], "references": ["get a single publication ."], "bleu": 0.18575057999133596, "rouge_l": 0.3012345679012346}
{"id": 2551, "code": "def get publication list ( context , list , template = 'publications/publications.html' ) : list = List . objects . filter ( list iexact = list ) if not list : return '' list = list [ 0 ] publications = list . publication set . all ( ) publications = publications . order by ( '-year' , '-month' , '-id' ) if not publications : return '' populate ( publications ) return render template ( template , context [ 'request' ] , { 'list' : list , 'publications' : publications } )", "predictions": ["strip a message message with a list of all publications"], "references": ["get a publication list ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 2552, "code": "def tex parse ( string ) : string = string . replace ( '{' , '' ) . replace ( '}' , '' ) def tex replace ( match ) : return sub ( r'\\^(\\w)' , r'<sup>\\1</sup>' , sub ( r'\\^\\{(.*?)\\}' , r'<sup>\\1</sup>' , sub ( r'\\ (\\w)' , r'<sub>\\1</sub>' , sub ( r'\\ \\{(.*?)\\}' , r'<sub>\\1</sub>' , sub ( r'\\\\(' + GREEK LETTERS + ')' , r'&\\1;' , match . group ( 1 ) ) ) ) ) ) return mark safe ( sub ( r'\\$([^\\$]*)\\$' , tex replace , escape ( string ) ) )", "predictions": ["parse for a string"], "references": ["renders some basic tex math to html ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 2553, "code": "def swap ( self , qs ) : try : replacement = qs [ 0 ] except Index Error : return if not self . valid ordering reference ( replacement ) : raise Value Error ( \"%r can only be swapped with instances of %r which %s equals %r.\" % ( self , self . class , self . order with respect to , self . get order with respect to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )", "predictions": ["swaps the exists and save os os os os os os os os os os"], "references": ["swap the positions of this object with a reference object ."], "bleu": 0.08225964699966554, "rouge_l": 0.07911802853437094}
{"id": 2554, "code": "def up ( self ) : self . swap ( self . get ordering queryset ( ) . filter ( order lt = self . order ) . order by ( '-order' ) )", "predictions": ["run at the start of each database"], "references": ["move this object up one position ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2555, "code": "def down ( self ) : self . swap ( self . get ordering queryset ( ) . filter ( order gt = self . order ) )", "predictions": ["removes all the ."], "references": ["move this object down one position ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2556, "code": "def to ( self , order ) : if order is None or self . order == order : return qs = self . get ordering queryset ( ) if self . order > order : qs . filter ( order lt = self . order , order gte = order ) . update ( order = F ( 'order' ) + 1 ) else : qs . filter ( order gt = self . order , order lte = order ) . update ( order = F ( 'order' ) - 1 ) self . order = order self . save ( )", "predictions": ["remove all columns that were specified by the given order"], "references": ["move object to a certain position updating all affected objects to move accordingly up or down ."], "bleu": 0.06259938494637494, "rouge_l": 0.07076566125290024}
{"id": 2557, "code": "def above ( self , ref ) : if not self . valid ordering reference ( ref ) : raise Value Error ( \"%r can only be moved above instances of %r which %s equals %r.\" % ( self , self . class , self . order with respect to , self . get order with respect to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = ref . order else : o = self . get ordering queryset ( ) . filter ( order lt = ref . order ) . aggregate ( Max ( 'order' ) ) . get ( 'order max' ) or 0 self . to ( o )", "predictions": ["filter a reference to a specific ordering"], "references": ["move this object above the referenced object ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 2558, "code": "def below ( self , ref ) : if not self . valid ordering reference ( ref ) : raise Value Error ( \"%r can only be moved below instances of %r which %s equals %r.\" % ( self , self . class , self . order with respect to , self . get order with respect to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = self . get ordering queryset ( ) . filter ( order gt = ref . order ) . aggregate ( Min ( 'order' ) ) . get ( 'order min' ) or 0 else : o = ref . order self . to ( o )", "predictions": ["order must be a reference or a reference"], "references": ["move this object below the referenced object ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2559, "code": "def top ( self ) : o = self . get ordering queryset ( ) . aggregate ( Min ( 'order' ) ) . get ( 'order min' ) self . to ( o )", "predictions": ["prepare the get ."], "references": ["move this object to the top of the ordered stack ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 2560, "code": "def bottom ( self ) : o = self . get ordering queryset ( ) . aggregate ( Max ( 'order' ) ) . get ( 'order max' ) self . to ( o )", "predictions": ["bottom transformation for selected ordering"], "references": ["move this object to the bottom of the ordered stack ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 2561, "code": "def populate ( publications ) : customlinks = Custom Link . objects . filter ( publication in = publications ) customfiles = Custom File . objects . filter ( publication in = publications ) publications = { } for publication in publications : publication . links = [ ] publication . files = [ ] publications [ publication . id ] = publication for link in customlinks : publications [ link . publication id ] . links . append ( link ) for file in customfiles : publications [ file . publication id ] . files . append ( file )", "predictions": ["populate the publication s publications files"], "references": ["load custom links and files from database and attach to publications ."], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 2562, "code": "def count var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( arr == \"N\" , axis = 0 ) nomiss = arr [ : , ~ miss ] nsnps = np . invert ( np . all ( nomiss == nomiss [ 0 , : ] , axis = 0 ) ) . sum ( ) return nomiss . shape [ 1 ] , nsnps", "predictions": ["count the number of samples in a variable"], "references": ["count number of sites with cov = 4 and number of variable sites ."], "bleu": 0.11327490115090784, "rouge_l": 0.346590909090909}
{"id": 2563, "code": "def sample loci ( self ) : idxs = np . random . choice ( self . idxs , self . ntests ) with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( \"|\\n\" ) ) seqdata = { i : \"\" for i in self . samples } for idx , loc in enumerate ( liter ) : if idx in idxs : lines = loc . split ( \"\\n\" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += \"N\" * len ( seqs [ 0 ] ) return seqdata", "predictions": ["samples up a batch of samples from the filestorehashs file"], "references": ["finds loci with sufficient sampling for this test"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2564, "code": "def run ( self , ipyclient ) : lbview = ipyclient . load balanced view ( ) asyncs = [ ] for test in xrange ( self . ntests ) : async = lbview . apply ( worker , self ) asyncs . append ( async ) ipyclient . wait ( ) for async in asyncs : if not async . successful ( ) : raise Exception ( \"Error: {}\" . format ( async . result ( ) ) ) results = [ i . result ( ) for i in asyncs ] self . results table = pd . Data Frame ( results )", "predictions": ["run a set of ipyclient"], "references": ["parallelize calls to worker function ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2565, "code": "def plot ( self ) : if self . results table == None : return \"no results found\" else : bb = self . results table . sort values ( by = [ \"ABCD\" , \"ACBD\" ] , ascending = [ False , True ] , ) import toyplot c = toyplot . Canvas ( width = 600 , height = 200 ) a = c . cartesian ( ) m = a . bars ( bb ) return c , a , m", "predictions": ["plot the results of the video ."], "references": ["return a toyplot barplot of the results table ."], "bleu": 0.23099966849728554, "rouge_l": 0.3667334669338677}
{"id": 2566, "code": "def copy ( self ) : cp = copy . deepcopy ( self ) cp . genotypes = allel . Genotype Array ( self . genotypes , copy = True ) return cp", "predictions": ["create a shallow copy of the current instance ."], "references": ["returns a copy of the pca analysis object"], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 2567, "code": "def sample cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + \"-tmp-umap1.fastq\" ) umap2file = os . path . join ( data . dirs . edits , sample . name + \"-tmp-umap2.fastq\" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + \"-unmapped.bam\" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + \".sam\" ) split1 = os . path . join ( data . dirs . edits , sample . name + \"-split1.fastq\" ) split2 = os . path . join ( data . dirs . edits , sample . name + \"-split2.fastq\" ) refmap derep = os . path . join ( data . dirs . edits , sample . name + \"-refmap derep.fastq\" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap derep ] : try : os . remove ( f ) except : pass", "predictions": ["cleanup cleanup of sample ."], "references": ["clean up a bunch of loose files ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2568, "code": "def fetch cluster se ( data , samfile , chrom , rstart , rend ) : overlap buffer = data . hackersonly [ \"min SE refmap overlap\" ] rstart buff = rstart + overlap buffer rend buff = rend - overlap buffer if rstart buff > rend buff : tmp = rstart buff rstart buff = rend buff rend buff = tmp if rstart buff == rend buff : rend buff += 1 rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart buff , rend buff ) for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read sfunc = lambda x : int ( x . split ( \";size=\" ) [ 1 ] . split ( \";\" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) try : read1 = rdict [ rkeys [ 0 ] ] except Value Error : LOGGER . error ( \"Found bad cluster, skipping - key:{} rdict:{}\" . format ( rkeys [ 0 ] , rdict ) ) return \"\" poss = read1 . get reference positions ( full length = True ) seed r1start = min ( poss ) seed r1end = max ( poss ) if read1 . is reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( rkeys [ 0 ] ) clust . append ( \">{}:{}:{};size={};*\\n{}\" . format ( chrom , seed r1start , seed r1end , size , seq ) ) if len ( rkeys ) > 1 : for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except Value Error : read1 = rdict [ key ] [ 0 ] skip = True if not skip : poss = read1 . get reference positions ( full length = True ) minpos = min ( poss ) maxpos = max ( poss ) if read1 . is reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( key ) clust . append ( \">{}:{}:{};size={};+\\n{}\" . format ( chrom , minpos , maxpos , size , seq ) ) else : pass return clust", "predictions": ["fetch the cluster data from the given data ."], "references": ["builds a single end cluster from the refmapped data ."], "bleu": 0.2187537716852318, "rouge_l": 0.5213675213675214}
{"id": 2569, "code": "def refmap init ( data , sample , force ) : sample . files . unmapped reads = os . path . join ( data . dirs . edits , \"{}-refmap derep.fastq\" . format ( sample . name ) ) sample . files . mapped reads = os . path . join ( data . dirs . refmapping , \"{}-mapped-sorted.bam\" . format ( sample . name ) )", "predictions": ["create a refmap for the refmap and the sample ."], "references": ["create some file handles for refmapping"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2570, "code": "def command list ( self ) : cmd = [ self . params . binary , \"-i\" , OPJ ( self . workdir , self . name + \".treemix.in.gz\" ) , \"-o\" , OPJ ( self . workdir , self . name ) , ] args = [ ] for key , val in self . params : if key not in [ \"minmap\" , \"binary\" ] : if key == \"g\" : if val [ 0 ] : args += [ \"-\" + key , str ( val [ 0 ] ) , str ( val [ 1 ] ) ] elif key == \"global \" : if val : args += [ \"-\" + key [ : - 1 ] ] elif key in [ \"se\" , \"global\" , \"noss\" ] : if val : args += [ \"-\" + key ] else : if val : args += [ \"-\" + key , str ( val ) ] return cmd + args", "predictions": ["list the command list"], "references": ["build the command list"], "bleu": 0.668740304976422, "rouge_l": 0.75}
{"id": 2571, "code": "def subsample ( self ) : spans = self . maparr samp = np . zeros ( spans . shape [ 0 ] , dtype = np . uint64 ) for i in xrange ( spans . shape [ 0 ] ) : samp [ i ] = np . random . randint ( spans [ i , 0 ] , spans [ i , 1 ] , 1 ) return samp", "predictions": ["subsample from graph ."], "references": ["returns a subsample of unlinked snp sites"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2572, "code": "def draw ( self , axes ) : tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use edge lengths = True , tree style = 'c' , tip labels align = True , edge align style = { \"stroke-width\" : 1 } ) for admix in self . results . admixture : pidx , pdist , cidx , cdist , weight = admix a = get admix point ( tre , pidx , pdist ) b = get admix point ( tre , cidx , cdist ) mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { \"stroke-width\" : 10 * weight , \"stroke-opacity\" : 0.95 , \"stroke-linecap\" : \"round\" } ) axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = \"weight: {}\" . format ( weight ) , ) axes . y . show = False axes . x . ticks . show = True axes . x . label . text = \"Drift parameter\" return axes", "predictions": ["draw the axes ."], "references": ["returns a treemix plot on a toyplot . axes object ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 2573, "code": "def run mbsum ( self , ipyclient , force = False , quiet = False ) : minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) trees1 = glob . glob ( os . path . join ( minidir , \"*.run1.t\" ) ) trees2 = glob . glob ( os . path . join ( minidir , \"*.run2.t\" ) ) existing = glob . glob ( os . path . join ( self . workdir , self . name , \"*.sumt\" ) ) if any ( existing ) : if force : for rfile in existing : os . remove ( rfile ) else : path = os . path . join ( self . workdir , self . name ) raise I Pyrad Warning Exit ( EXISTING SUMT FILES . format ( path ) ) lbview = ipyclient . load balanced view ( ) asyncs = [ ] for tidx in xrange ( len ( trees1 ) ) : rep1 = trees1 [ tidx ] rep2 = trees2 [ tidx ] outname = os . path . join ( minidir , str ( tidx ) + \".sumt\" ) async = lbview . apply ( call mbsum , * ( rep1 , rep2 , outname ) ) asyncs . append ( async ) start = time . time ( ) printstr = \"[mbsum] sum replicate runs      | {} | \" while 1 : ready = [ i . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) if not quiet : progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = \"\" ) if len ( ready ) == sum ( ready ) : if not quiet : print ( \"\" ) break else : time . sleep ( 0.1 ) for async in asyncs : if not async . successful ( ) : raise I Pyrad Warning Exit ( async . result ( ) )", "predictions": ["run the async view"], "references": ["sums two replicate mrbayes runs for each locus"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 2574, "code": "def run mrbayes ( self , ipyclient , force = False , quiet = False ) : minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) nexus files = glob . glob ( os . path . join ( minidir , \"*.nex\" ) ) #existing = glob.glob(os.path.join(self.workdir, self.name, \"*.nex\")) existing = glob . glob ( os . path . join ( minidir , \"*.nex.*\" ) ) if any ( existing ) : if force : for rfile in existing : os . remove ( rfile ) else : raise I Pyrad Warning Exit ( EXISTING NE Xdot FILES . format ( minidir ) ) #self.write nexus files(force=True) lbview = ipyclient . load balanced view ( ) asyncs = [ ] for nex in nexus files : async = lbview . apply ( call mb , nex ) asyncs . append ( async ) start = time . time ( ) printstr = \"[mb] infer gene-tree posteriors | {} | \" while 1 : ready = [ i . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) if not quiet : progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = \"\" ) if len ( ready ) == sum ( ready ) : if not quiet : print ( \"\" ) break else : time . sleep ( 0.1 ) for async in asyncs : if not async . successful ( ) : raise I Pyrad Warning Exit ( async . result ( ) )", "predictions": ["run async mrbayes ."], "references": ["calls the mrbayes block in each nexus file ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 2575, "code": "def stats ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) pd . options . display . max rows = len ( self . samples ) statdat = pd . Data Frame ( [ self . samples [ i ] . stats for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) for column in statdat : if column not in [ \"hetero est\" , \"error est\" ] : statdat [ column ] = np . nan to num ( statdat [ column ] ) . astype ( int ) return statdat", "predictions": ["compute the stats as a pandas dataframe ."], "references": ["returns a data frame with sample data and state ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 2576, "code": "def files ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) #fullcurdir = os.path.realpath(os.path.curdir) return pd . Data Frame ( [ self . samples [ i ] . files for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' )", "predictions": ["a sorted list of dataframe files ."], "references": ["returns a data frame with sample files . not very readable ..."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 2577, "code": "def build stat ( self , idx ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) newdat = pd . Data Frame ( [ self . samples [ i ] . stats dfs [ idx ] for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) return newdat", "predictions": ["build a dataframe of samples from a dataframe"], "references": ["returns a data frame with sample stats for each step"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 2578, "code": "def get params ( self , param = \"\" ) : fullcurdir = os . path . realpath ( os . path . curdir ) if not param : for index , ( key , value ) in enumerate ( self . paramsdict . items ( ) ) : if isinstance ( value , str ) : value = value . replace ( fullcurdir + \"/\" , \"./\" ) sys . stdout . write ( \"{}{:<4}{:<28}{:<45}\\n\" . format ( self . spacer , index , key , value ) ) else : try : if int ( param ) : #sys.stdout.write(self.paramsdict.values()[int(param)-1]) return self . paramsdict . values ( ) [ int ( param ) ] except ( Value Error , Type Error , Name Error , Index Error ) : try : return self . paramsdict [ param ] except Key Error : return 'key not recognized'", "predictions": ["get the parameters from the parameters"], "references": ["pretty prints params if called as a function"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 2579, "code": "def step1func ( self , force , ipyclient ) : sfiles = self . paramsdict [ \"sorted fastq path\" ] rfiles = self . paramsdict [ \"raw fastq path\" ] if sfiles and rfiles : raise I Pyrad Warning Exit ( NOT TWO PATHS ) if not ( sfiles or rfiles ) : raise I Pyrad Warning Exit ( NO SEQ PATH FOUND ) if self . headers : if sfiles : print ( \"\\n{}Step 1: Loading sorted fastq data to Samples\" . format ( self . spacer ) ) else : print ( \"\\n{}Step 1: Demultiplexing fastq data to Samples\" . format ( self . spacer ) ) if self . samples : if not force : print ( SAMPLES EXIST . format ( len ( self . samples ) , self . name ) ) else : if glob . glob ( sfiles ) : self . link fastqs ( ipyclient = ipyclient , force = force ) else : assemble . demultiplex . run2 ( self , ipyclient , force ) else : if glob . glob ( sfiles ) : self . link fastqs ( ipyclient = ipyclient ) else : assemble . demultiplex . run2 ( self , ipyclient , force )", "predictions": ["executes a new fastq message with the given ipyclient ."], "references": ["hidden wrapped function to start step 1"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2580, "code": "def step2func ( self , samples , force , ipyclient ) : if self . headers : print ( \"\\n  Step 2: Filtering reads \" ) if not self . samples . keys ( ) : raise I Pyrad Warning Exit ( FIRST RUN 1 ) samples = get samples ( self , samples ) if not force : if all ( [ i . stats . state >= 2 for i in samples ] ) : print ( EDITS EXIST . format ( len ( samples ) ) ) return assemble . rawedit . run2 ( self , samples , force , ipyclient )", "predictions": ["calls the command with a single progress bar ."], "references": ["hidden wrapped function to start step 2"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2581, "code": "def step3func ( self , samples , noreverse , maxindels , force , ipyclient ) : if self . headers : print ( \"\\n  Step 3: Clustering/Mapping reads\" ) if self . paramsdict [ 'assembly method' ] != \"denovo\" : if not self . paramsdict [ 'reference sequence' ] : raise I Pyrad Error ( REQUIRE REFERENCE PATH . format ( self . paramsdict [ \"assembly method\" ] ) ) else : lbview = ipyclient . load balanced view ( ) async = lbview . apply ( index reference sequence , * ( self , force ) ) start = time . time ( ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = \" {}    | {} | s3 |\" . format ( \"indexing reference\" , elapsed ) finished = int ( async . ready ( ) ) progressbar ( 1 , finished , printstr , spacer = self . spacer ) if finished : print ( \"\" ) break time . sleep ( 0.9 ) if not async . successful ( ) : raise I Pyrad Warning Exit ( async . result ( ) ) samples = get samples ( self , samples ) if not self . samples precheck ( samples , 3 , force ) : raise I Pyrad Error ( FIRST RUN 2 ) elif not force : if all ( [ i . stats . state >= 3 for i in samples ] ) : print ( CLUSTERS EXIST . format ( len ( samples ) ) ) return assemble . cluster within . run ( self , samples , noreverse , maxindels , force , ipyclient )", "predictions": ["assemble samples from a single log file"], "references": ["hidden wrapped function to start step 3"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2582, "code": "def step4func ( self , samples , force , ipyclient ) : if self . headers : print ( \"\\n  Step 4: Joint estimation of error rate and heterozygosity\" ) samples = get samples ( self , samples ) if not self . samples precheck ( samples , 4 , force ) : raise I Pyrad Error ( FIRST RUN 3 ) elif not force : if all ( [ i . stats . state >= 4 for i in samples ] ) : print ( JOINTS EXIST . format ( len ( samples ) ) ) return assemble . jointestimate . run ( self , samples , force , ipyclient )", "predictions": ["run the given error rate in the given error rate ."], "references": ["hidden wrapped function to start step 4"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2583, "code": "def step5func ( self , samples , force , ipyclient ) : if self . headers : print ( \"\\n  Step 5: Consensus base calling \" ) samples = get samples ( self , samples ) if not self . samples precheck ( samples , 5 , force ) : raise I Pyrad Error ( FIRST RUN 4 ) elif not force : if all ( [ i . stats . state >= 5 for i in samples ] ) : print ( CONSENS EXIST . format ( len ( samples ) ) ) return assemble . consens se . run ( self , samples , force , ipyclient )", "predictions": ["report new samples in the input file ."], "references": ["hidden wrapped function to start step 5"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2584, "code": "def step6func ( self , samples , noreverse , force , randomseed , ipyclient , * * kwargs ) : samples = get samples ( self , samples ) csamples = self . samples precheck ( samples , 6 , force ) if self . headers : print ( \"\\n  Step 6: Clustering at {} similarity across {} samples\" . format ( self . paramsdict [ \"clust threshold\" ] , len ( csamples ) ) ) if not csamples : raise I Pyrad Error ( FIRST RUN 5 ) elif not force : if all ( [ i . stats . state >= 6 for i in csamples ] ) : print ( DATABASE EXISTS . format ( len ( samples ) ) ) return assemble . cluster across . run ( self , csamples , noreverse , force , randomseed , ipyclient , * * kwargs )", "predictions": ["run a cluster on a list of samples"], "references": ["hidden function to start step 6 ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2585, "code": "def combinefiles ( filepath ) : fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if \" R1 \" in i ] if not firsts : raise I Pyrad Warning Exit ( \"First read files names must contain ' R1 '.\" ) seconds = [ ff . replace ( \" R1 \" , \" R2 \" ) for ff in firsts ] return zip ( firsts , seconds )", "predictions": ["get the number of files in a file"], "references": ["joins first and second read file names"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2586, "code": "def findbcode ( cutters , longbar , read1 ) : for cutter in cutters [ 0 ] : if not cutter : continue search = read1 [ 1 ] [ : int ( longbar [ 0 ] + len ( cutter ) + 1 ) ] barcode = search . rsplit ( cutter , 1 ) if len ( barcode ) > 1 : return barcode [ 0 ] return barcode [ 0 ]", "predictions": ["get barcode barcode from read1"], "references": ["find barcode sequence in the beginning of read"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2587, "code": "def find3radbcode ( cutters , longbar , read1 ) : for ambigcuts in cutters : for cutter in ambigcuts : if not cutter : continue search = read1 [ 1 ] [ : int ( longbar [ 0 ] + len ( cutter ) + 1 ) ] splitsearch = search . rsplit ( cutter , 1 ) if len ( splitsearch ) > 1 : return splitsearch [ 0 ] return splitsearch [ 0 ]", "predictions": ["get the longbar that is a longbar"], "references": ["find barcode sequence in the beginning of read"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2588, "code": "def get barcode func ( data , longbar ) : if longbar [ 1 ] == 'same' : if data . paramsdict [ \"datatype\" ] == '2brad' : def getbarcode ( cutters , read1 , longbar ) : \"\"\" find barcode for 2b RAD data \"\"\" return read1 [ 1 ] [ : - ( len ( cutters [ 0 ] [ 0 ] ) + 1 ) ] [ - longbar [ 0 ] : ] else : def getbarcode ( , read1 , longbar ) : \"\"\" finds barcode for invariable length barcode data \"\"\" return read1 [ 1 ] [ : longbar [ 0 ] ] else : def getbarcode ( cutters , read1 , longbar ) : \"\"\" finds barcode for variable barcode lengths\"\"\" return findbcode ( cutters , longbar , read1 ) return getbarcode", "predictions": ["return the barcode function to be used in a function to compute a barcode"], "references": ["returns the fastest func given data & longbar"], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 2589, "code": "def get quart iter ( tups ) : if tups [ 0 ] . endswith ( \".gz\" ) : ofunc = gzip . open else : ofunc = open ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : ofile2 = 0 quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) def feedme ( quarts ) : for quart in quarts : yield quart genquarts = feedme ( quarts ) return genquarts , ofile1 , ofile2", "predictions": ["get a random counts of the tups"], "references": ["returns an iterator to grab four lines at a time"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2590, "code": "def writetofastq ( data , dsort , read ) : if read == 1 : rrr = \"R1\" else : rrr = \"R2\" for sname in dsort : handle = os . path . join ( data . dirs . fastqs , \"{} {} .fastq\" . format ( sname , rrr ) ) with open ( handle , 'a' ) as out : out . write ( \"\" . join ( dsort [ sname ] ) )", "predictions": ["create a new contacts file with specified number of input data ."], "references": ["writes sorted data dsort dict to a tmp files"], "bleu": 0.11498759556447223, "rouge_l": 0.09775641025641024}
{"id": 2591, "code": "def collate files ( data , sname , tmp1s , tmp2s ) : out1 = os . path . join ( data . dirs . fastqs , \"{} R1 .fastq.gz\" . format ( sname ) ) out = io . Buffered Writer ( gzip . open ( out1 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ \"pigz\" ] else : compress = [ \"gzip\" ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise I Pyrad Warning Exit ( \"error in collate files R1 %s\" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ \"datatype\" ] : out2 = os . path . join ( data . dirs . fastqs , \"{} R2 .fastq.gz\" . format ( sname ) ) out = io . Buffered Writer ( gzip . open ( out2 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise I Pyrad Warning Exit ( \"error in collate files R2 %s\" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp2s : os . remove ( tmpfile )", "predictions": ["collate files to collate files ."], "references": ["collate temp fastq files in tmp - dir into 1 gzipped sample ."], "bleu": 0.08180282100568384, "rouge_l": 0.29611650485436897}
{"id": 2592, "code": "def inverse barcodes ( data ) : matchdict = { } bases = set ( \"CATGN\" ) poss = set ( ) for sname , barc in data . barcodes . items ( ) : if \"-technical-replicate-\" in sname : sname = sname . rsplit ( \"-technical-replicate\" , 1 ) [ 0 ] matchdict [ barc ] = sname poss . add ( barc ) if data . paramsdict [ \"max barcode mismatch\" ] > 0 : for idx1 , base in enumerate ( barc ) : diffs = bases . difference ( base ) for diff in diffs : lbar = list ( barc ) lbar [ idx1 ] = diff tbar1 = \"\" . join ( lbar ) if tbar1 not in poss : matchdict [ tbar1 ] = sname poss . add ( tbar1 ) else : if matchdict . get ( tbar1 ) != sname : print ( . format ( sname , barc , matchdict [ tbar1 ] , data . barcodes [ matchdict [ tbar1 ] ] , data . paramsdict [ \"max barcode mismatch\" ] ) ) if data . paramsdict [ \"max barcode mismatch\" ] > 1 : for idx2 , in enumerate ( tbar1 ) : if idx2 != idx1 : for diff in bases . difference ( tbar1 [ idx2 ] ) : ltbar = list ( tbar1 ) ltbar [ idx2 ] = diff tbar2 = \"\" . join ( ltbar ) if tbar2 not in poss : matchdict [ tbar2 ] = sname poss . add ( tbar2 ) else : if matchdict . get ( tbar2 ) != sname : print ( . format ( sname , barc , matchdict [ tbar2 ] , data . barcodes [ matchdict [ tbar2 ] ] , data . paramsdict [ \"max barcode mismatch\" ] ) ) return matchdict", "predictions": ["bottom transformation for the bottom - level cope cope"], "references": ["build full inverse barcodes dictionary"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2593, "code": "def cleanup and die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , \"tmp * R*.fastq\" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , \"tmp *.p\" ) ) for tmpf in tmpfiles : os . remove ( tmpf )", "predictions": ["populate the temporary temp temp temp file objects objects objects objects objects objects objects objects objects objects objects objects objects objects"], "references": ["cleanup func for step 1"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 2594, "code": "def splitfiles ( data , raws , ipyclient ) : tmpdir = os . path . join ( data . paramsdict [ \"project dir\" ] , \"tmp-chunks-\" + data . name ) if os . path . exists ( tmpdir ) : shutil . rmtree ( tmpdir ) os . makedirs ( tmpdir ) totalreads = estimate optim ( data , raws [ 0 ] [ 0 ] , ipyclient ) optim = int ( 8e6 ) njobs = int ( totalreads / ( optim / 4. ) ) * len ( raws ) nosplit = 0 if ( len ( raws ) > len ( ipyclient ) ) or ( totalreads < optim ) : nosplit = 1 start = time . time ( ) chunkfiles = { } for fidx , tups in enumerate ( raws ) : handle = os . path . splitext ( os . path . basename ( tups [ 0 ] ) ) [ 0 ] if nosplit : chunkfiles [ handle ] = [ tups ] else : chunklist = zcat make temps ( data , tups , fidx , tmpdir , optim , njobs , start ) chunkfiles [ handle ] = chunklist if not nosplit : print ( \"\" ) return chunkfiles", "predictions": ["~ the chunkfiles . to ~ the input directory"], "references": ["sends raws to be chunked"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2595, "code": "def putstats ( pfile , handle , statdicts ) : with open ( pfile , 'r' ) as infile : filestats , samplestats = pickle . load ( infile ) perfile , fsamplehits , fbarhits , fmisses , fdbars = statdicts #handle = os.path.splitext(os.path.basename(handle))[0] perfile [ handle ] += filestats samplehits , barhits , misses , dbars = samplestats fsamplehits . update ( samplehits ) fbarhits . update ( barhits ) fmisses . update ( misses ) fdbars . update ( dbars ) statdicts = perfile , fsamplehits , fbarhits , fmisses , fdbars return statdicts", "predictions": ["read a pickled file and return a dictionary with the given statdicts"], "references": ["puts stats from pickles into a dictionary"], "bleu": 0.1367440667823257, "rouge_l": 0.22101449275362317}
{"id": 2596, "code": "def countmatrix ( lxs ) : share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) names = range ( lxs . shape [ 0 ] ) for row in lxs : for samp1 , samp2 in itertools . combinations ( names , 2 ) : shared = lxs [ samp1 , lxs [ samp2 ] > 0 ] . sum ( ) share [ samp1 , samp2 ] = shared ##share[] for row in xrange ( len ( names ) ) : share [ row , row ] = lxs [ row ] . sum ( ) return share", "predictions": ["compute share share share test scores = 1 = 0 = 1 = 1 = 0 = 1 = 1 = 0 = 1 = 1 = 1 = 1 ="], "references": ["fill a matrix with pairwise data sharing"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2597, "code": "def paramname ( param = \"\" ) : try : name = pinfo [ str ( param ) ] [ 0 ] . strip ( ) . split ( \" \" ) [ 1 ] except ( Key Error , Value Error ) as err : ## print ( \"\\t Key name/number not recognized - \" . format ( param ) , err ) raise return name", "predictions": ["get parameter table table"], "references": ["get the param name from the dict index value ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2598, "code": "def save json2 ( data ) : datadict = Ordered Dict ( [ ( \"outfiles\" , data . dict [ \"outfiles\" ] ) , ( \"stats files\" , dict ( data . dict [ \"stats files\" ] ) ) , ( \"stats dfs\" , data . dict [ \"stats dfs\" ] ) ] )", "predictions": ["copy data to files\""], "references": ["save to json ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2599, "code": "def save json ( data ) : datadict = Ordered Dict ( [ ( \" version\" , data . dict [ \" version\" ] ) , ( \" checkpoint\" , data . dict [ \" checkpoint\" ] ) , ( \"name\" , data . dict [ \"name\" ] ) , ( \"dirs\" , data . dict [ \"dirs\" ] ) , ( \"paramsdict\" , data . dict [ \"paramsdict\" ] ) , ( \"samples\" , data . dict [ \"samples\" ] . keys ( ) ) , ( \"populations\" , data . dict [ \"populations\" ] ) , ( \"database\" , data . dict [ \"database\" ] ) , ( \"clust database\" , data . dict [ \"clust database\" ] ) , ( \"outfiles\" , data . dict [ \"outfiles\" ] ) , ( \"barcodes\" , data . dict [ \"barcodes\" ] ) , ( \"stats files\" , data . dict [ \"stats files\" ] ) , ( \" hackersonly\" , data . dict [ \" hackersonly\" ] ) , ] ) sampledict = Ordered Dict ( [ ] ) for key , sample in data . samples . iteritems ( ) : sampledict [ key ] = sample . to fulldict ( ) fulldumps = json . dumps ( { \"assembly\" : datadict , \"samples\" : sampledict } , cls = Encoder , sort keys = False , indent = 4 , separators = ( \",\" , \":\" ) , ) assemblypath = os . path . join ( data . dirs . project , data . name + \".json\" ) if not os . path . exists ( data . dirs . project ) : os . mkdir ( data . dirs . project ) done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( Keyboard Interrupt , System Exit ) : print ( '.' ) continue", "predictions": ["sample cleanup of cleanup to cleanup"], "references": ["save assembly and samples as json"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2600, "code": "def encode ( self , obj ) : def hint tuples ( item ) : \"\"\" embeds  tuple  hinter in json strings \"\"\" if isinstance ( item , tuple ) : return { ' tuple ' : True , 'items' : item } if isinstance ( item , list ) : return [ hint tuples ( e ) for e in item ] if isinstance ( item , dict ) : return { key : hint tuples ( val ) for key , val in item . iteritems ( ) } else : return item return super ( Encoder , self ) . encode ( hint tuples ( obj ) )", "predictions": ["fetch a single json representation of the object { tablename } object { % } { tuples } ."], "references": ["function to encode json string"], "bleu": 0.06439931429457924, "rouge_l": 0.09312977099236641}
{"id": 2601, "code": "def parse 00 ( ofile ) : with open ( ofile ) as infile : arr = np . array ( [ \" \" ] + infile . read ( ) . split ( \"Summary of MCMC results\\n\\n\\n\" ) [ 1 : ] [ 0 ] . strip ( ) . split ( ) ) rows = 12 cols = ( arr . shape [ 0 ] + 1 ) / rows arr = arr . reshape ( rows , cols ) df = pd . Data Frame ( data = arr [ 1 : , 1 : ] , columns = arr [ 0 , 1 : ] , index = arr [ 1 : , 0 ] , ) . T return df", "predictions": ["parse can be a numpy array to list of data"], "references": ["return 00 outfile as a pandas dataframe"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2602, "code": "def parse 01 ( ofiles , individual = False ) : cols = [ ] dats = [ ] for ofile in ofiles : with open ( ofile ) as infile : dat = infile . read ( ) lastbits = dat . split ( \".mcmc.txt\\n\\n\" ) [ 1 : ] results = lastbits [ 0 ] . split ( \"\\n\\n\" ) [ 0 ] . split ( ) shape = ( ( ( len ( results ) - 3 ) / 4 ) , 4 ) dat = np . array ( results [ 3 : ] ) . reshape ( shape ) cols . append ( dat [ : , 3 ] . astype ( float ) ) if not individual : cols = np . array ( cols ) cols = cols . sum ( axis = 0 ) / len ( ofiles ) #10. dat [ : , 3 ] = cols . astype ( str ) df = pd . Data Frame ( dat [ : , 1 : ] ) df . columns = [ \"delim\" , \"prior\" , \"posterior\" ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) df [ \"nspecies\" ] = nspecies return df else : #return cols res = [ ] for i in xrange ( len ( cols ) ) : x = dat x [ : , 3 ] = cols [ i ] . astype ( str ) x = pd . Data Frame ( x [ : , 1 : ] ) x . columns = [ 'delim' , 'prior' , 'posterior' ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) x [ \"nspecies\" ] = nspecies res . append ( x ) return res", "predictions": ["command line entry point to command line"], "references": ["a subfunction for summarizing results"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2603, "code": "def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : for fname in alignbits : with open ( fname ) as infile : out . write ( infile . read ( ) + \"//\\n//\\n\" )", "predictions": ["load a file into a single file"], "references": ["concatenates sorted aligned cluster tmpfiles and removes them ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 2604, "code": "def fill dups arr ( data ) : duplefiles = glob . glob ( os . path . join ( data . tmpdir , \"duples *.tmp.npy\" ) ) duplefiles . sort ( key = lambda x : int ( x . rsplit ( \" \" , 1 ) [ - 1 ] [ : - 8 ] ) ) io5 = h5py . File ( data . clust database , 'r+' ) dfilter = io5 [ \"duplicates\" ] init = 0 for dupf in duplefiles : end = int ( dupf . rsplit ( \" \" , 1 ) [ - 1 ] [ : - 8 ] ) inarr = np . load ( dupf ) dfilter [ init : end ] = inarr init += end - init #os.remove(dupf) #del inarr LOGGER . info ( \"all duplicates: %s\" , dfilter [ : ] . sum ( ) ) io5 . close ( )", "predictions": ["draw dups and arr"], "references": ["fills the duplicates array from the multi_muscle_align tmp files"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2605, "code": "def build tmp h5 ( data , samples ) : snames = [ i . name for i in samples ] snames . sort ( ) uhandle = os . path . join ( data . dirs . across , data . name + \".utemp.sort\" ) bseeds = os . path . join ( data . dirs . across , data . name + \".tmparrs.h5\" ) get seeds and hits ( uhandle , bseeds , snames )", "predictions": ["run tmp on tmp"], "references": ["build tmp h5 arrays that can return quick access for nloci"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 2606, "code": "def get nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . name + \".tmparrs.h5\" ) with h5py . File ( bseeds ) as io5 : return io5 [ \"seedsarr\" ] . shape [ 0 ]", "predictions": ["retrieve the mrbayes file name from the report directory quiet quiet quiet quiet quiet quiet quiet quiet quiet quiet quiet quiet"], "references": ["return nloci from the tmp h5 arr"], "bleu": 0.07645949399477267, "rouge_l": 0.157014157014157}
{"id": 2607, "code": "def write to fullarr ( data , sample , sidx ) : #isref = 'reference' in data.paramsdict[\"assembly method\"] LOGGER . info ( \"writing fullarr %s %s\" , sample . name , sidx ) with h5py . File ( data . clust database , 'r+' ) as io5 : chunk = io5 [ \"catgs\" ] . attrs [ \"chunksize\" ] [ 0 ] catg = io5 [ \"catgs\" ] nall = io5 [ \"nalleles\" ] smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio ) as indat : newcatg = indat [ \"icatg\" ] #[:] onall = indat [ \"inall\" ] #[:] for cidx in xrange ( 0 , catg . shape [ 0 ] , chunk ) : end = cidx + chunk catg [ cidx : end , sidx : sidx + 1 , : ] = np . expand dims ( newcatg [ cidx : end , : ] , axis = 1 ) nall [ : , sidx : sidx + 1 ] = np . expand dims ( onall , axis = 1 )", "predictions": ["stats to to to"], "references": ["writes arrays to h5 disk"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 2608, "code": "def dask chroms ( data , samples ) : h5s = [ os . path . join ( data . dirs . across , s . name + \".tmp.h5\" ) for s in samples ] handles = [ h5py . File ( i ) for i in h5s ] dsets = [ i [ '/ichrom' ] for i in handles ] arrays = [ da . from array ( dset , chunks = ( 10000 , 3 ) ) for dset in dsets ] stack = da . stack ( arrays , axis = 2 ) maxchrom = da . max ( stack , axis = 2 ) [ : , 0 ] maxpos = da . max ( stack , axis = 2 ) [ : , 2 ] mask = stack == 0 stack [ mask ] = 9223372036854775807 minpos = da . min ( stack , axis = 2 ) [ : , 1 ] final = da . stack ( [ maxchrom , minpos , maxpos ] , axis = 1 ) final . to hdf5 ( data . clust database , \"/chroms\" ) = [ i . close ( ) for i in handles ]", "predictions": ["closes all samples and clust columns"], "references": ["a dask relay function to fill chroms for all samples"], "bleu": 0.14925824694560996, "rouge_l": 0.23921568627450981}
{"id": 2609, "code": "def inserted indels ( indels , ocatg ) : newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) for iloc in xrange ( ocatg . shape [ 0 ] ) : indidx = np . where ( indels [ iloc , : ] ) [ 0 ] if np . any ( indidx ) : allrows = np . arange ( ocatg . shape [ 1 ] ) mask = np . ones ( allrows . shape [ 0 ] , dtype = np . bool ) for idx in indidx : mask [ idx ] = False not idx = allrows [ mask == 1 ] newcatg [ iloc ] [ not idx ] = ocatg [ iloc , : not idx . shape [ 0 ] ] else : newcatg [ iloc ] = ocatg [ iloc ] return newcatg", "predictions": ["stat is a stat of the stat ."], "references": ["inserts indels into the catg array"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2610, "code": "def count seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ \"cut\" , \"-f\" , \"2\" ] cmd2 = [ \"uniq\" ] cmd3 = [ \"wc\" ] proc1 = sps . Popen ( cmd1 , stdin = insort , stdout = sps . PIPE , close fds = True ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , close fds = True ) proc3 = sps . Popen ( cmd3 , stdin = proc2 . stdout , stdout = sps . PIPE , close fds = True ) res = proc3 . communicate ( ) nseeds = int ( res [ 0 ] . split ( ) [ 0 ] ) proc1 . stdout . close ( ) proc2 . stdout . close ( ) proc3 . stdout . close ( ) return nseeds", "predictions": ["get the number of params that may be used in a file"], "references": ["uses bash commands to quickly count n seeds from utemp file"], "bleu": 0.10390302174233558, "rouge_l": 0.08764367816091953}
{"id": 2611, "code": "def sort seeds ( uhandle , usort ) : cmd = [ \"sort\" , \"-k\" , \"2\" , uhandle , \"-o\" , usort ] proc = sps . Popen ( cmd , close fds = True ) proc . communicate ( )", "predictions": ["sort a list of seeds seeds"], "references": ["sort seeds from cluster results"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 2612, "code": "def assembly cleanup ( data ) : data . stats dfs . s2 = data . build stat ( \"s2\" ) data . stats files . s2 = os . path . join ( data . dirs . edits , 's2 rawedit stats.txt' ) with io . open ( data . stats files . s2 , 'w' , encoding = 'utf-8' ) as outfile : data . stats dfs . s2 . fillna ( value = 0 ) . astype ( np . int ) . to string ( outfile )", "predictions": ["cleanup up the final training and cleanup for a new one ."], "references": ["cleanup for assembly object"], "bleu": 0.1367440667823257, "rouge_l": 0.27477477477477474}
{"id": 2613, "code": "def parse single results ( data , sample , res1 ) : #sample.stats dfs.s2[\"reads raw\"] = 0 sample . stats dfs . s2 [ \"trim adapter bp read1\" ] = 0 sample . stats dfs . s2 [ \"trim quality bp read1\" ] = 0 sample . stats dfs . s2 [ \"reads filtered by Ns\" ] = 0 sample . stats dfs . s2 [ \"reads filtered by minlen\" ] = 0 sample . stats dfs . s2 [ \"reads passed filter\" ] = 0 lines = res1 . strip ( ) . split ( \"\\n\" ) for line in lines : if \"Total reads processed:\" in line : value = int ( line . split ( ) [ 3 ] . replace ( \",\" , \"\" ) ) sample . stats dfs . s2 [ \"reads raw\" ] = value if \"Reads with adapters:\" in line : value = int ( line . split ( ) [ 3 ] . replace ( \",\" , \"\" ) ) sample . stats dfs . s2 [ \"trim adapter bp read1\" ] = value if \"Quality-trimmed\" in line : value = int ( line . split ( ) [ 1 ] . replace ( \",\" , \"\" ) ) sample . stats dfs . s2 [ \"trim quality bp read1\" ] = value if \"Reads that were too short\" in line : value = int ( line . split ( ) [ 5 ] . replace ( \",\" , \"\" ) ) sample . stats dfs . s2 [ \"reads filtered by minlen\" ] = value if \"Reads with too many N\" in line : value = int ( line . split ( ) [ 5 ] . replace ( \",\" , \"\" ) ) sample . stats dfs . s2 [ \"reads filtered by Ns\" ] = value if \"Reads written (passing filters):\" in line : value = int ( line . split ( ) [ 4 ] . replace ( \",\" , \"\" ) ) sample . stats dfs . s2 [ \"reads passed filter\" ] = value if sample . stats dfs . s2 . reads passed filter : sample . stats . state = 2 sample . stats . reads passed filter = sample . stats dfs . s2 . reads passed filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + \".trimmed R1 .fastq.gz\" ) , 0 ) ] LOGGER . info ( res1 ) else : print ( \"{}No reads passed filtering in Sample: {}\" . format ( data . spacer , sample . name ) )", "predictions": ["parse function to parse self ."], "references": ["parse results from cutadapt into sample data"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2614, "code": "def concat reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : start = time . time ( ) printstr = \" concatenating inputs  | {} | s2 |\" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat multiple inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( \"\" ) break for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) #exception() LOGGER . error ( \"error in step2 concat %s\" , error ) raise I Pyrad Warning Exit ( \"error in step2 concat: {}\" . format ( error ) ) else : for sample in subsamples : sample . files . concat = sample . files . fastqs return subsamples", "predictions": ["concat reads of data"], "references": ["concatenate if multiple input files for a single samples"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2615, "code": "def run cutadapt ( data , subsamples , lbview ) : start = time . time ( ) printstr = \" processing reads      | {} | s2 |\" finished = 0 rawedits = { } subsamples . sort ( key = lambda x : x . stats . reads raw , reverse = True ) LOGGER . info ( [ i . stats . reads raw for i in subsamples ] ) if \"pair\" in data . paramsdict [ \"datatype\" ] : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit pairs , * ( data , sample ) ) else : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit single , * ( data , sample ) ) while 1 : finished = sum ( [ i . ready ( ) for i in rawedits . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( rawedits ) , finished , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if finished == len ( rawedits ) : print ( \"\" ) break for async in rawedits : if rawedits [ async ] . successful ( ) : res = rawedits [ async ] . result ( ) if \"pair\" not in data . paramsdict [ \"datatype\" ] : parse single results ( data , data . samples [ async ] , res ) else : parse pair results ( data , data . samples [ async ] , res ) else : print ( \"  found an error in step2; see ipyrad log.txt\" ) LOGGER . error ( \"error in run cutadapt(): %s\" , rawedits [ async ] . exception ( ) )", "predictions": ["run function for cutadapt"], "references": ["sends fastq files to cutadapt"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 2616, "code": "def choose samples ( samples , force ) : subsamples = [ ] if not force : for sample in samples : if sample . stats . state >= 2 : print ( . format ( sample . name ) ) elif not sample . stats . reads raw : print ( . format ( sample . name , sample . files . fastqs ) ) else : subsamples . append ( sample ) else : for sample in samples : if not sample . stats . reads raw : print ( . format ( sample . name , sample . files . fastqs ) ) else : subsamples . append ( sample ) return subsamples", "predictions": ["choose samples to be used in the fasta file"], "references": ["filter out samples that are already done with this step unless force"], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 2617, "code": "def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + \".vcf\" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + \".loci\" ) importvcf ( invcffile , outlocifile )", "predictions": ["create a new filepath ."], "references": ["convert vcf from step6 to . loci format to facilitate downstream format conversion"], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 2618, "code": "def random product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , 2 ) ind2 = random . sample ( pool2 , 2 ) return tuple ( ind1 + ind2 )", "predictions": ["search for a random product with a random product"], "references": ["random sampler for equal_splits func"], "bleu": 0.15619699684601276, "rouge_l": 0.1506172839506173}
{"id": 2619, "code": "def count snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , 5 ] + mat [ 0 , 10 ] + mat [ 0 , 15 ] + mat [ 5 , 0 ] + mat [ 5 , 10 ] + mat [ 5 , 15 ] + mat [ 10 , 0 ] + mat [ 10 , 5 ] + mat [ 10 , 15 ] + mat [ 15 , 0 ] + mat [ 15 , 5 ] + mat [ 15 , 10 ] ) for i in range ( 16 ) : if i % 5 : snps [ 1 ] += mat [ i , i ] snps [ 2 ] = mat [ 1 , 4 ] + mat [ 2 , 8 ] + mat [ 3 , 12 ] + mat [ 4 , 1 ] + mat [ 6 , 9 ] + mat [ 7 , 13 ] + mat [ 8 , 2 ] + mat [ 9 , 6 ] + mat [ 11 , 14 ] + mat [ 12 , 3 ] + mat [ 13 , 7 ] + mat [ 14 , 11 ] snps [ 3 ] = ( mat . sum ( ) - np . diag ( mat ) . sum ( ) ) - snps [ 2 ] return snps", "predictions": ["count snps snps snps"], "references": ["get dstats from the count array and return as a float tuple"], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2620, "code": "def calculate ( seqnon , mapcol , nmask , tests ) : #LOGGER.info(\"tests[0] %s\", tests[0]) #LOGGER.info('seqnon[[tests[0]]] %s', seqnon[[tests[0]]]) mats = chunk to matrices ( seqnon , mapcol , nmask ) svds = np . zeros ( ( 3 , 16 ) , dtype = np . float64 ) qscores = np . zeros ( 3 , dtype = np . float64 ) ranks = np . zeros ( 3 , dtype = np . float64 ) for test in range ( 3 ) : svds [ test ] = np . linalg . svd ( mats [ test ] . astype ( np . float64 ) ) [ 1 ] ranks [ test ] = np . linalg . matrix rank ( mats [ test ] . astype ( np . float64 ) ) minrank = int ( min ( 11 , ranks . min ( ) ) ) for test in range ( 3 ) : qscores [ test ] = np . sqrt ( np . sum ( svds [ test , minrank : ] ** 2 ) ) best = np . where ( qscores == qscores . min ( ) ) [ 0 ] #best = qscores[qscores == qscores.min()][0] bidx = tests [ best ] [ 0 ] qsnps = count snps ( mats [ best ] [ 0 ] ) return bidx , qsnps", "predictions": ["get bidx and qsnps"], "references": ["groups together several numba compiled funcs"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2621, "code": "def nworker ( data , smpchunk , tests ) : #numba.config.NUMBA DEFAULT NUM THREADS = 1 with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ \"bootsarr\" ] [ : ] maparr = io5 [ \"bootsmap\" ] [ : ] nall mask = seqview [ : ] == 78 rquartets = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rweights = None #rweights = np.ones(smpchunk.shape[0], dtype=np.float64) rdstats = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint32 ) #times = [] for idx in xrange ( smpchunk . shape [ 0 ] ) : sidx = smpchunk [ idx ] seqchunk = seqview [ sidx ] nmask = np . any ( nall mask [ sidx ] , axis = 0 ) nmask += np . all ( seqchunk == seqchunk [ 0 ] , axis = 0 ) #bidx, qscores, qstats = calculate(seqchunk, maparr[:, 0], nmask, tests) bidx , qstats = calculate ( seqchunk , maparr [ : , 0 ] , nmask , tests ) rdstats [ idx ] = qstats rquartets [ idx ] = smpchunk [ idx ] [ bidx ] return rquartets , rweights , rdstats", "predictions": ["get for for each column in the iter gzip gzip iter gzip gzip gzip"], "references": ["the workhorse function . not numba ."], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 2622, "code": "def shuffle cols ( seqarr , newarr , cols ) : for idx in xrange ( cols . shape [ 0 ] ) : newarr [ : , idx ] = seqarr [ : , cols [ idx ] ] return newarr", "predictions": ["shuffle the cols with the given read cols"], "references": ["used in bootstrap resampling without a map file"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2623, "code": "def resolve ambigs ( tmpseq ) : for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 77 ] ) : idx , idy = np . where ( tmpseq == ambig ) res1 , res2 = AMBIGS [ ambig . view ( \"S1\" ) ] halfmask = np . random . choice ( [ True , False ] , idx . shape [ 0 ] ) for i in xrange ( halfmask . shape [ 0 ] ) : if halfmask [ i ] : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res1 ) . view ( np . uint8 ) else : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res2 ) . view ( np . uint8 ) return tmpseq", "predictions": ["collate the data from the graph of the data ."], "references": ["returns a seq array with rskywm randomly replaced with resolved bases"], "bleu": 0.0959156018869021, "rouge_l": 0.0}
{"id": 2624, "code": "def get spans ( maparr , spans ) : bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . uint64 ) for idx in xrange ( 1 , maparr . shape [ 0 ] ) : cur = maparr [ idx , 0 ] if cur != bidx : idy = idx + 1 spans [ cur - 2 , 1 ] = idx spans [ cur - 1 , 0 ] = idx bidx = cur spans [ - 1 , 1 ] = maparr [ - 1 , - 1 ] return spans", "predictions": ["get the spans of a class"], "references": ["get span distance for each locus in original seqarray"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2625, "code": "def get shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] , 1 ] - spans [ loci [ idx ] , 0 ] return width", "predictions": ["calculate the shape of each sample"], "references": ["get shape of new bootstrap resampled locus array"], "bleu": 0.20830666398386113, "rouge_l": 0.2785388127853881}
{"id": 2626, "code": "def fill boot ( seqarr , newboot , newmap , spans , loci ) : cidx = 0 for i in xrange ( loci . shape [ 0 ] ) : x1 = spans [ loci [ i ] ] [ 0 ] x2 = spans [ loci [ i ] ] [ 1 ] cols = seqarr [ : , x1 : x2 ] cord = np . random . choice ( cols . shape [ 1 ] , cols . shape [ 1 ] , replace = False ) rcols = cols [ : , cord ] newboot [ : , cidx : cidx + cols . shape [ 1 ] ] = rcols newmap [ cidx : cidx + cols . shape [ 1 ] , 0 ] = i + 1 cidx += cols . shape [ 1 ] return newboot , newmap", "predictions": ["fill the boot with the boot boot"], "references": ["fills the new bootstrap resampled array"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2627, "code": "def byteify ( data , ignore dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( \"utf-8\" ) if isinstance ( data , list ) : return [ byteify ( item , ignore dicts = True ) for item in data ] if isinstance ( data , dict ) and not ignore dicts : return { byteify ( key , ignore dicts = True ) : byteify ( value , ignore dicts = True ) for key , value in data . iteritems ( ) } return data", "predictions": ["convert unicode to bytes ."], "references": ["converts unicode to utf - 8 when reading in json files"], "bleu": 0.10822031883953476, "rouge_l": 0.2341650671785029}
{"id": 2628, "code": "def parse names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile . next ( ) . strip ( ) . split ( ) while 1 : try : self . samples . append ( infile . next ( ) . split ( ) [ 0 ] ) except Stop Iteration : break", "predictions": ["parses the names of the samples and parses it into a list of samples"], "references": ["parse sample names from the sequence file"], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 2629, "code": "def run qmc ( self , boot ) : self . tmp = os . path . join ( self . dirs , \".tmpwtre\" ) cmd = [ ip . bins . qmc , \"qrtt=\" + self . files . qdump , \"otre=\" + self . tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : #LOGGER.error(\"Error in QMC: \\n({}).\".format(res)) LOGGER . error ( res ) raise I Pyrad Warning Exit ( res [ 1 ] ) with open ( self . tmp ) as intree : #tmp = toytree.tree(intree.read().strip()) tmp = ete3 . Tree ( intree . read ( ) . strip ( ) ) tmpwtre = self . renamer ( tmp ) #.tree) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + \".boots\" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmpwtre + \"\\n\" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + \".tree\" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmpwtre ) self . save ( )", "predictions": ["run qmc on a given boot boot ."], "references": ["runs quartet max - cut on a quartets file"], "bleu": 0.1862539773562041, "rouge_l": 0.232824427480916}
{"id": 2630, "code": "def renamer ( self , tre ) : names = tre . get leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] return tre . write ( format = 9 )", "predictions": ["get the renamer for this sample"], "references": ["renames newick from numbers to sample names"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2631, "code": "def finalize stats ( self , ipyclient ) : #print(STATSOUT.format(opr(self.files.stats))) print ( FINALTREES . format ( opr ( self . trees . tree ) ) ) if self . params . nboots : self . compute tree stats ( ipyclient ) print ( BOOTTREES . format ( opr ( self . trees . cons ) , opr ( self . trees . boots ) ) ) if len ( self . samples ) < 20 : if self . params . nboots : wctre = ete3 . Tree ( self . trees . cons , format = 0 ) wctre . ladderize ( ) print ( wctre . get ascii ( show internal = True , attributes = [ \"dist\" , \"name\" ] ) ) print ( \"\" ) else : qtre = ete3 . Tree ( self . trees . tree , format = 0 ) qtre . ladderize ( ) #qtre = toytree.tree(self.trees.tree, format=0) #qtre.tree.unroot() print ( qtre . get ascii ( ) ) print ( \"\" ) docslink = \"https://toytree.readthedocs.io/\" citelink = \"https://ipyrad.readthedocs.io/tetrad.html\" print ( LINKS . format ( docslink , citelink ) )", "predictions": ["print summary information ."], "references": ["write final tree files"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 2632, "code": "def save ( self ) : fulldict = copy . deepcopy ( self . dict ) for i , j in fulldict . items ( ) : if isinstance ( j , Params ) : fulldict [ i ] = j . dict fulldumps = json . dumps ( fulldict , sort keys = False , indent = 4 , separators = ( \",\" , \":\" ) , ) assemblypath = os . path . join ( self . dirs , self . name + \".tet.json\" ) if not os . path . exists ( self . dirs ) : os . mkdir ( self . dirs ) done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( Keyboard Interrupt , System Exit ) : print ( '.' ) continue", "predictions": ["save the model to a directory"], "references": ["save a json file representation of tetrad class for checkpoint"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 2633, "code": "def insert to array ( self , start , results ) : qrts , wgts , qsts = results #qrts, wgts = results #print(qrts) with h5py . File ( self . database . output , 'r+' ) as out : chunk = self . chunksize out [ 'quartets' ] [ start : start + chunk ] = qrts ##out['weights'][start:start+chunk] = wgts if self . checkpoint . boots : key = \"qboots/b{}\" . format ( self . checkpoint . boots - 1 ) out [ key ] [ start : start + chunk ] = qsts else : out [ \"qstats\" ] [ start : start + chunk ] = qsts", "predictions": ["insert results into the array"], "references": ["inputs results from workers into hdf4 array"], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 2634, "code": "def padnames ( names ) : longname len = max ( len ( i ) for i in names ) padding = 5 pnames = [ name + \" \" * ( longname len - len ( name ) + padding ) for name in names ] snppad = \"//\" + \" \" * ( longname len - 2 + padding ) return np . array ( pnames ) , snppad", "predictions": ["get the normalized size of a list of names ."], "references": ["pads names for loci output"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 2635, "code": "def locichunk ( args ) : data , optim , pnames , snppad , smask , start , samplecov , locuscov , upper = args hslice = [ start , start + optim ] co5 = h5py . File ( data . database , 'r' ) afilt = co5 [ \"filters\" ] [ hslice [ 0 ] : hslice [ 1 ] , ] aedge = co5 [ \"edges\" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ \"snps\" ] [ hslice [ 0 ] : hslice [ 1 ] , ] io5 = h5py . File ( data . clust database , 'r' ) if upper : aseqs = np . char . upper ( io5 [ \"seqs\" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ) else : aseqs = io5 [ \"seqs\" ] [ hslice [ 0 ] : hslice [ 1 ] , ] keep = np . where ( np . sum ( afilt , axis = 1 ) == 0 ) [ 0 ] store = [ ] for iloc in keep : edg = aedge [ iloc ] #LOGGER.info(\"!!!!!! iloc edg %s, %s\", iloc, edg) args = [ iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ] if edg [ 4 ] : outstr , samplecov , locuscov = enter pairs ( * args ) store . append ( outstr ) else : outstr , samplecov , locuscov = enter singles ( * args ) store . append ( outstr ) tmpo = os . path . join ( data . dirs . outfiles , data . name + \".loci.{}\" . format ( start ) ) with open ( tmpo , 'w' ) as tmpout : tmpout . write ( \"\\n\" . join ( store ) + \"\\n\" ) io5 . close ( ) co5 . close ( ) return samplecov , locuscov , start", "predictions": ["convert data to samplecov and store it in data store"], "references": ["function from make_loci to apply to chunks . smask is sample mask ."], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 2636, "code": "def enter pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : LOGGER . info ( \"edges in enter pairs %s\" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] nalln = np . all ( seq1 == \"N\" , axis = 1 ) nsidx = nalln + smask LOGGER . info ( \"nsidx %s, nalln %s, smask %s\" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( \"samplecov %s\" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( \"idx %s\" , idx ) locuscov [ idx ] += 1 seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] outstr = \"\\n\" . join ( [ name + s1 . tostring ( ) + \"nnnn\" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) #LOGGER.info(\"s1 %s\", s1.tostring()) #LOGGER.info(\"s2 %s\", s2.tostring()) snpstring1 = [ \"-\" if snp1 [ i , 0 ] else \"*\" if snp1 [ i , 1 ] else \" \" for i in range ( len ( snp1 ) ) ] snpstring2 = [ \"-\" if snp2 [ i , 0 ] else \"*\" if snp2 [ i , 1 ] else \" \" for i in range ( len ( snp2 ) ) ] #npis = str(snpstring1+snpstring2).count(\"*\") #nvars = str(snpstring1+snpstring2).count(\"-\") + npis outstr += \"\\n\" + snppad + \"\" . join ( snpstring1 ) + \"    \" + \"\" . join ( snpstring2 ) + \"|{}|\" . format ( iloc + start ) #\"|LOCID={},DBID={},NVAR={},NPIS={}|\"\\ #.format(1+iloc+start, iloc, nvars, npis) return outstr , samplecov , locuscov", "predictions": ["enter pairs of a iloc pairs ."], "references": ["enters funcs for pairs"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2637, "code": "def enter singles ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : seq = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] nalln = np . all ( seq == \"N\" , axis = 1 ) nsidx = nalln + smask samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) locuscov [ idx ] += 1 seq = seq [ ~ nsidx , ] names = pnames [ ~ nsidx ] outstr = \"\\n\" . join ( [ name + s . tostring ( ) for name , s in zip ( names , seq ) ] ) snpstring = [ \"-\" if snp [ i , 0 ] else \"*\" if snp [ i , 1 ] else \" \" for i in range ( len ( snp ) ) ] outstr += \"\\n\" + snppad + \"\" . join ( snpstring ) + \"|{}|\" . format ( iloc + start ) #LOGGER.info(\"outstr %s\", outstr) return outstr , samplecov , locuscov", "predictions": ["enter a singles array ."], "references": ["enter funcs for se or merged data"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2638, "code": "def snpcount numba ( superints , snpsarr ) : for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : catg = np . zeros ( 4 , dtype = np . int16 ) ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : #C catg [ 0 ] += 1 elif ncol [ idx ] == 65 : #A catg [ 1 ] += 1 elif ncol [ idx ] == 84 : #T catg [ 2 ] += 1 elif ncol [ idx ] == 71 : #G catg [ 3 ] += 1 elif ncol [ idx ] == 82 : #R catg [ 1 ] += 1 #A catg [ 3 ] += 1 #G elif ncol [ idx ] == 75 : #K catg [ 2 ] += 1 #T catg [ 3 ] += 1 #G elif ncol [ idx ] == 83 : #S catg [ 0 ] += 1 #C catg [ 3 ] += 1 #G elif ncol [ idx ] == 89 : #Y catg [ 0 ] += 1 #C catg [ 2 ] += 1 #T elif ncol [ idx ] == 87 : #W catg [ 1 ] += 1 #A catg [ 2 ] += 1 #T elif ncol [ idx ] == 77 : #M catg [ 0 ] += 1 #C catg [ 1 ] += 1 #A catg . sort ( ) if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr", "predictions": ["return a dictionary with the numba numba numba ."], "references": ["used to count the number of unique bases in a site for snpstring ."], "bleu": 0.09630141125179911, "rouge_l": 0.1673525377229081}
{"id": 2639, "code": "def write snps map ( data ) : start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , \"tmp-{}.h5\" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : maparr = io5 [ \"maparr\" ] [ : ] end = np . where ( np . all ( maparr [ : ] == 0 , axis = 1 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = maparr . shape [ 0 ] outchunk = [ ] with open ( data . outfiles . snpsmap , 'w' ) as out : for idx in xrange ( end ) : line = maparr [ idx , : ] #print(line) outchunk . append ( \"{}\\trad{} snp{}\\t{}\\t{}\\n\" . format ( line [ 0 ] , line [ 1 ] , line [ 2 ] , 0 , line [ 3 ] ) ) if not idx % 10000 : out . write ( \"\" . join ( outchunk ) ) outchunk = [ ] out . write ( \"\" . join ( outchunk ) ) LOGGER . debug ( \"finished writing snps map in: %s\" , time . time ( ) - start )", "predictions": ["write snps to snps"], "references": ["write a map file with linkage information for snps file"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 2640, "code": "def write usnps ( data , sidx , pnames ) : tmparrs = os . path . join ( data . dirs . outfiles , \"tmp-{}.h5\" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : bisarr = io5 [ \"bisarr\" ] end = np . where ( np . all ( bisarr [ : ] == \"\" , axis = 0 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = bisarr . shape [ 1 ] with open ( data . outfiles . usnpsphy , 'w' ) as out : out . write ( \"{} {}\\n\" . format ( bisarr . shape [ 0 ] , end ) ) for idx , name in enumerate ( pnames ) : out . write ( \"{}{}\\n\" . format ( name , \"\" . join ( bisarr [ idx , : end ] ) ) )", "predictions": ["write data to a usnps file"], "references": ["write the bisnp string"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2641, "code": "def concat vcf ( data , names , full ) : if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) vcfchunks = glob . glob ( data . outfiles . vcf + \".*\" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( \".\" ) [ - 1 ] ) ) if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) if data . paramsdict [ \"assembly method\" ] in [ \"reference\" , \"denovo+reference\" ] : cmd = [ \"cat\" ] + vcfchunks + [ \" | sort -k 2,2 -n | sort -k 1,1 -s\" ] cmd = \" \" . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close fds = True ) else : proc = sps . Popen ( [ \"cat\" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise I Pyrad Warning Exit ( \"err in concat vcf: %s\" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )", "predictions": ["concatenate vcf file with outfiles"], "references": ["sorts concatenates and gzips vcf chunks . also cleans up chunks ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 2642, "code": "def reftrick ( iseq , consdict ) : altrefs = np . zeros ( ( iseq . shape [ 1 ] , 4 ) , dtype = np . uint8 ) altrefs [ : , 1 ] = 46 for col in xrange ( iseq . shape [ 1 ] ) : fcounts = np . zeros ( 111 , dtype = np . int64 ) counts = np . bincount ( iseq [ : , col ] ) #, minlength=90) fcounts [ : counts . shape [ 0 ] ] = counts fcounts [ 78 ] = 0 fcounts [ 45 ] = 0 for aidx in xrange ( consdict . shape [ 0 ] ) : nbases = fcounts [ consdict [ aidx , 0 ] ] for in xrange ( nbases ) : fcounts [ consdict [ aidx , 1 ] ] += 1 fcounts [ consdict [ aidx , 2 ] ] += 1 fcounts [ consdict [ aidx , 0 ] ] = 0 who = np . argmax ( fcounts ) altrefs [ col , 0 ] = who fcounts [ who ] = 0 who = np . argmax ( fcounts ) if who : altrefs [ col , 1 ] = who fcounts [ who ] = 0 who = np . argmax ( fcounts ) altrefs [ col , 2 ] = who fcounts [ who ] = 0 who = np . argmax ( fcounts ) altrefs [ col , 3 ] = who return altrefs", "predictions": ["return a altrefs object given a consdict consdict"], "references": ["returns the most common base at each site in order ."], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 2643, "code": "def vcfheader ( data , names , ofile ) : if data . paramsdict [ \"reference sequence\" ] : reference = data . paramsdict [ \"reference sequence\" ] else : reference = \"pseudo-reference (most common base at site)\" ##FILTER=<ID=min Cov,Description=\"Data shared across <{mincov} samples\"> ##FILTER=<ID=max SH,Description=\"Heterozygosous site shared across >{maxsh} samples\"> header = . format ( date = time . strftime ( \"%Y/%m/%d\" ) , version = version , reference = os . path . basename ( reference ) , mincov = data . paramsdict [ \"min samples locus\" ] , maxsh = data . paramsdict [ \"max shared Hs locus\" ] , names = \"\\t\" . join ( names ) ) ofile . write ( header )", "predictions": ["write the reference shared shared shared shared shared shared shared shared shared library ."], "references": ["prints header for vcf files"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 2644, "code": "def collapse outgroup ( tree , taxdicts ) : outg = taxdicts [ 0 ] [ \"p4\" ] if not all ( [ i [ \"p4\" ] == outg for i in taxdicts ] ) : raise Exception ( \"no good\" ) tre = ete . Tree ( tree . write ( format = 1 ) ) #tree.copy(method=\"deepcopy\") alltax = [ i for i in tre . get leaf names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search nodes ( name = outg [ 0 ] ) [ 0 ] . name = \"outgroup\" tre . ladderize ( ) taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : #test[\"p4\"] = [outg[0]] test [ \"p4\" ] = [ \"outgroup\" ] newtaxdicts . append ( test ) return tre , newtaxdicts", "predictions": ["collapse outgroup to a list of taxdicts"], "references": ["collapse outgroup in ete tree for easier viewing"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 2645, "code": "def get quick depths ( data , sample ) : ## #if sample.files.clusters and not sample.stats.state == 3: #else: sample . files . clusters = os . path . join ( data . dirs . clusts , sample . name + \".clust S.gz\" ) fclust = data . samples [ sample . name ] . files . clusters clusters = gzip . open ( fclust , 'r' ) pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) depths = [ ] maxlen = [ ] tdepth = 0 tlen = 0 while 1 : try : name , seq = pairdealer . next ( ) except Stop Iteration : break #print name.strip(), seq.strip() if name . strip ( ) == seq . strip ( ) : depths . append ( tdepth ) maxlen . append ( tlen ) tlen = 0 tdepth = 0 else : tdepth += int ( name . split ( \";\" ) [ - 2 ] [ 5 : ] ) tlen = len ( seq ) clusters . close ( ) return np . array ( maxlen ) , np . array ( depths )", "predictions": ["retrieve quick depths of a quick quick file"], "references": ["iterate over clusts files to get data"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2646, "code": "def align and parse ( handle , max internal indels = 5 , is gbs = False ) : try : with open ( handle , 'rb' ) as infile : clusts = infile . read ( ) . split ( \"//\\n//\\n\" ) clusts = [ i for i in clusts if i ] if not clusts : raise I Pyrad Error except ( IO Error , I Pyrad Error ) : LOGGER . debug ( \"skipping empty chunk - {}\" . format ( handle ) ) return 0 highindels = 0 try : aligned = persistent popen align3 ( clusts , 200 , is gbs ) except Exception as inst : LOGGER . debug ( \"Error in handle - {} - {}\" . format ( handle , inst ) ) #raise I Pyrad Warning Exit(\"error hrere {}\".format(inst)) aligned = [ ] refined = [ ] for clust in aligned : filtered = aligned indel filter ( clust , max internal indels ) #filtered = overshoot filter(clust) if not filtered : refined . append ( clust ) #\"\\n\".join(stack)) else : highindels += 1 if refined : outhandle = handle . rsplit ( \".\" , 1 ) [ 0 ] + \".aligned\" with open ( outhandle , 'wb' ) as outfile : outfile . write ( \"\\n//\\n//\\n\" . join ( refined ) + \"\\n\" ) log level = logging . get Level Name ( LOGGER . get Effective Level ( ) ) if not log level == \"DEBUG\" : os . remove ( handle ) return highindels", "predictions": ["align and align a file with the given handle"], "references": ["much faster implementation for aligning chunks"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2647, "code": "def aligned indel filter ( clust , max internal indels ) : lclust = clust . split ( ) try : seq1 = [ i . split ( \"nnnn\" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] seq2 = [ i . split ( \"nnnn\" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] intindels1 = [ i . rstrip ( \"-\" ) . lstrip ( \"-\" ) . count ( \"-\" ) for i in seq1 ] intindels2 = [ i . rstrip ( \"-\" ) . lstrip ( \"-\" ) . count ( \"-\" ) for i in seq2 ] intindels = intindels1 + intindels2 if max ( intindels ) > max internal indels : return 1 except Index Error : seq1 = lclust [ 1 : : 2 ] intindels = [ i . rstrip ( \"-\" ) . lstrip ( \"-\" ) . count ( \"-\" ) for i in seq1 ] if max ( intindels ) > max internal indels : return 1 return 0", "predictions": ["return the number of indel indel a clust ."], "references": ["checks for too many internal indels in muscle aligned clusters"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 2648, "code": "def setup dirs ( data ) : pdir = os . path . realpath ( data . paramsdict [ \"project dir\" ] ) data . dirs . clusts = os . path . join ( pdir , \"{} clust {}\" . format ( data . name , data . paramsdict [ \"clust threshold\" ] ) ) if not os . path . exists ( data . dirs . clusts ) : os . mkdir ( data . dirs . clusts ) data . tmpdir = os . path . abspath ( os . path . expanduser ( os . path . join ( pdir , data . name + '-tmpalign' ) ) ) if not os . path . exists ( data . tmpdir ) : os . mkdir ( data . tmpdir ) if not data . paramsdict [ \"assembly method\" ] == \"denovo\" : data . dirs . refmapping = os . path . join ( pdir , \"{} refmapping\" . format ( data . name ) ) if not os . path . exists ( data . dirs . refmapping ) : os . mkdir ( data . dirs . refmapping )", "predictions": ["setup the script and data ."], "references": ["sets up directories for step3 data"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2649, "code": "def build dag ( data , samples ) : snames = [ i . name for i in samples ] dag = nx . Di Graph ( ) joborder = JOBORDER [ data . paramsdict [ \"assembly method\" ] ] for sname in snames : for func in joborder : dag . add node ( \"{}-{}-{}\" . format ( func , 0 , sname ) ) for chunk in xrange ( 10 ) : dag . add node ( \"{}-{}-{}\" . format ( \"muscle align\" , chunk , sname ) ) dag . add node ( \"{}-{}-{}\" . format ( \"reconcat\" , 0 , sname ) ) for sname in snames : for sname2 in snames : dag . add edge ( \"{}-{}-{}\" . format ( joborder [ 0 ] , 0 , sname2 ) , \"{}-{}-{}\" . format ( joborder [ 1 ] , 0 , sname ) ) for idx in xrange ( 2 , len ( joborder ) ) : dag . add edge ( \"{}-{}-{}\" . format ( joborder [ idx - 1 ] , 0 , sname ) , \"{}-{}-{}\" . format ( joborder [ idx ] , 0 , sname ) ) for sname2 in snames : for chunk in range ( 10 ) : dag . add edge ( \"{}-{}-{}\" . format ( \"muscle chunker\" , 0 , sname2 ) , \"{}-{}-{}\" . format ( \"muscle align\" , chunk , sname ) ) dag . add edge ( \"{}-{}-{}\" . format ( \"muscle align\" , chunk , sname ) , \"{}-{}-{}\" . format ( \"reconcat\" , 0 , sname ) ) return dag , joborder", "predictions": ["build the dag for the dag and return the dag ."], "references": ["build a directed acyclic graph describing jobs to be run in order ."], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 2650, "code": "def plot dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist rainbow plt . figure ( \"dag layout\" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring layout ( dag ) , node color = 'pink' , with labels = True ) plt . savefig ( \"./dag layout.png\" , bbox inches = 'tight' , dpi = 200 ) pos = { } colors = { } for node in dag : #jobkey = \"{}-{}\".format(node, sample) mtd = results [ node ] . metadata start = date2num ( mtd . started ) , , sname = node . split ( \"-\" , 2 ) sid = snames . index ( sname ) pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine id plt . figure ( \"dag starttimes\" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node list = colors . keys ( ) , node color = colors . values ( ) , cmap = gist rainbow , with labels = True ) plt . savefig ( \"./dag starttimes.png\" , bbox inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )", "predictions": ["plot a dag of the results in a dag ."], "references": ["makes plot to help visualize the dag setup . for developers only ."], "bleu": 0.11742832364135733, "rouge_l": 0.33983286908078}
{"id": 2651, "code": "def run ( data , samples , noreverse , maxindels , force , ipyclient ) : subsamples = [ ] for sample in samples : if sample . stats . state < 2 : print ( . format ( sample . name ) ) continue if not force : if sample . stats . state >= 3 : print ( . format ( sample . name ) ) else : if sample . stats . reads passed filter : subsamples . append ( sample ) else : if sample . stats . reads passed filter : subsamples . append ( sample ) if not subsamples : print ( \"  No Samples ready to be clustered. First run step2().\" ) else : try : setup dirs ( data ) if not data . paramsdict [ \"assembly method\" ] == \"denovo\" : for sample in subsamples : refmap init ( data , sample , force ) nthreads = 2 else : nthreads = 1 if \"threads\" in data . ipcluster . keys ( ) : nthreads = int ( data . ipcluster [ \"threads\" ] ) ncpus = len ( ipyclient ) if ncpus > 2 * len ( data . samples ) : nthreads *= 2 args = [ data , subsamples , ipyclient , nthreads , maxindels , force ] new apply jobs ( * args ) finally : try : log level = logging . get Level Name ( LOGGER . get Effective Level ( ) ) if not log level == \"DEBUG\" : if os . path . exists ( data . tmpdir ) : shutil . rmtree ( data . tmpdir ) rdereps = glob . glob ( os . path . join ( data . dirs . edits , \"*-refmap derep.fastq\" ) ) for rmfile in rdereps : os . remove ( rmfile ) except Exception as : LOGGER . warning ( \"failed to cleanup files/dirs\" )", "predictions": ["run the input samples ."], "references": ["run the major functions for clustering within samples"], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 2652, "code": "def parse params ( args ) : try : with open ( args . params ) as paramsin : plines = paramsin . readlines ( ) except IO Error as : sys . exit ( \"  No params file found\" ) legacy version = 0 try : legacy version = 1 if not len ( plines [ 0 ] . split ( ) [ 0 ] ) == 7 : raise I Pyrad Warning Exit ( . format ( args . params , ip . version ) ) except Index Error : raise I Pyrad Warning Exit ( . format ( args . params ) ) if legacy version : #which version... #update to 6() pass items = [ i . split ( \"##\" ) [ 0 ] . strip ( ) for i in plines [ 1 : ] if not i . strip ( ) == \"\" ] #keys = [i.split(\"]\")[-2][-1] for i in plines[1:]] #keys = range(len(plines)-1) keys = ip . Assembly ( 'null' , quiet = True ) . paramsdict . keys ( ) parsedict = { str ( i ) : j for i , j in zip ( keys , items ) } return parsedict", "predictions": ["parse params and check for common parameters ."], "references": ["parse the params file args create and return assembly object ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 2653, "code": "def showstats ( parsedict ) : #project dir = parsedict['1'] project dir = parsedict [ \"project dir\" ] if not project dir : project dir = \"./\" #assembly name = parsedict['0'] assembly name = parsedict [ \"assembly name\" ] my assembly = os . path . join ( project dir , assembly name ) if not os . path . isdir ( project dir ) : msg = . format ( project dir ) sys . exit ( msg ) if not assembly name : msg = . format ( project dir ) raise I Pyrad Error ( msg ) data = ip . load json ( my assembly , quiet = True , cli = True ) print ( \"\\n Summary stats of Assembly {}\" . format ( data . name ) + \"\\n------------------------------------------------\" ) if not data . stats . empty : print ( data . stats ) print ( \"\\n\\n Full stats files\" + \"\\n------------------------------------------------\" ) fullcurdir = os . path . realpath ( os . path . curdir ) for i in range ( 1 , 8 ) : #enumerate(sorted(data.stats files)): key = \"s\" + str ( i ) try : val = data . stats files [ key ] val = val . replace ( fullcurdir , \".\" ) print ( \"step {}: {}\" . format ( i , val ) ) except ( Key Error , Attribute Error ) : print ( \"step {}: None\" . format ( i ) ) print ( \"\\n\" ) else : print ( \"No stats to display\" )", "predictions": ["print a project stats ."], "references": ["loads assembly or dies and print stats to screen"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 2654, "code": "def check version ( ) : import urllib2 from distutils . version import Loose Version header = \"\\n -------------------------------------------------------------\" + \"\\n  ipyrad [v.{}]\" . format ( ip . version ) + \"\\n  Interactive assembly and analysis of RAD-seq data\" + \"\\n -------------------------------------------------------------\" try : htmldat = urllib2 . urlopen ( \"https://anaconda.org/ipyrad/ipyrad\" ) . readlines ( ) curversion = next ( ( x for x in htmldat if \"subheader\" in x ) , None ) . split ( \">\" ) [ 1 ] . split ( \"<\" ) [ 0 ] if Loose Version ( ip . version ) < Loose Version ( curversion ) : msg = . format ( curversion ) print ( header + \"\\n\" + msg ) else : pass #print(\"You are up to date\") except Exception as inst : pass", "predictions": ["check if the version of the distutils version is up"], "references": ["test if there s a newer version and nag the user to upgrade ."], "bleu": 0.1004883949864497, "rouge_l": 0.24270557029177717}
{"id": 2655, "code": "def get binom ( base1 , base2 , est E , est H ) : prior homo = ( 1. - est H ) / 2. prior hete = est H bsum = base1 + base2 hetprob = scipy . misc . comb ( bsum , base1 ) / ( 2. ** ( bsum ) ) homoa = scipy . stats . binom . pmf ( base2 , bsum , est E ) homob = scipy . stats . binom . pmf ( base1 , bsum , est E ) hetprob *= prior hete homoa *= prior homo homob *= prior homo probabilities = [ homoa , homob , hetprob ] bestprob = max ( probabilities ) / float ( sum ( probabilities ) ) if hetprob > homoa : return True , bestprob else : return False , bestprob", "predictions": ["get binom binom binom"], "references": ["return probability of base call"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2656, "code": "def basecaller ( arrayed , mindepth majrule , mindepth statistical , est H , est E ) : cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) for col in xrange ( arr . shape [ 1 ] ) : carr = arr [ : , col ] mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] if not marr . shape [ 0 ] : cons [ col ] = 78 elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] else : counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] bidepth = nump + numq if bidepth < mindepth majrule : cons [ col ] = 78 else : if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq if bidepth >= mindepth statistical : ishet , prob = get binom ( base1 , base2 , est E , est H ) #LOGGER.info(\"ishet, prob, b1, b2: %s %s %s %s\", ishet, prob, base1, base2) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase else : #if bidepth >= mindepth majrule: if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( \"S1\" )", "predictions": ["return a rectangle by a given mindepth"], "references": ["call all sites in a locus array ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2657, "code": "def nfilter1 ( data , reps ) : if sum ( reps ) >= data . paramsdict [ \"mindepth majrule\" ] and sum ( reps ) <= data . paramsdict [ \"maxdepth\" ] : return 1 else : return 0", "predictions": ["1 - dimensional - vertex characters"], "references": ["applies read depths filter"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2658, "code": "def nfilter4 ( consens , hidx , arrayed ) : if len ( hidx ) < 2 : return consens , 1 harray = arrayed [ : , hidx ] harray = harray [ ~ np . any ( harray == \"-\" , axis = 1 ) ] harray = harray [ ~ np . any ( harray == \"N\" , axis = 1 ) ] ccx = Counter ( [ tuple ( i ) for i in harray ] ) if len ( ccx ) > 2 : totdepth = harray . shape [ 0 ] cutoff = max ( 1 , totdepth // 10 ) alleles = [ i for i in ccx if ccx [ i ] > cutoff ] else : alleles = ccx . keys ( ) nalleles = len ( alleles ) if nalleles == 2 : try : consens = storealleles ( consens , hidx , alleles ) except ( Index Error , Key Error ) : LOGGER . info ( \"failed at phasing loc, skipping\" ) LOGGER . info ( , consens , hidx , alleles ) return consens , nalleles else : return consens , nalleles", "predictions": ["get \"failed and arrayed . ."], "references": ["applies max haplotypes filter returns pass and consens"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2659, "code": "def storealleles ( consens , hidx , alleles ) : bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] bigallele = [ i for i in alleles if i [ 0 ] == bigbase ] [ 0 ] #consens = list(consens) for hsite , pbase in zip ( hidx [ 1 : ] , bigallele [ 1 : ] ) : if PRIORITY [ consens [ hsite ] ] != pbase : consens [ hsite ] = consens [ hsite ] . lower ( ) return consens", "predictions": ["get the storealleles . ."], "references": ["store phased allele data for diploids"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2660, "code": "def chunk clusters ( data , sample ) : num = 0 optim = int ( ( sample . stats . clusters total // data . cpus ) + ( sample . stats . clusters total % data . cpus ) ) chunkslist = [ ] with gzip . open ( sample . files . clusters , 'rb' ) as clusters : pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) done = 0 while not done : done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , \"tmp \" + str ( sample . name ) + \".\" + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( \"//\\n//\\n\" . join ( chunk ) + \"//\\n//\\n\" ) num += 1 return chunkslist", "predictions": ["parse names of names into chunks"], "references": ["split job into bits and pass to the client"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2661, "code": "def get subsamples ( data , samples , force ) : subsamples = [ ] for sample in samples : if not force : if sample . stats . state >= 5 : print ( . format ( sample . name ) ) elif not sample . stats . clusters hidepth : print ( . format ( sample . name , int ( sample . stats . clusters hidepth ) ) ) elif sample . stats . state < 4 : print ( . format ( sample . name ) ) else : subsamples . append ( sample ) else : if not sample . stats . clusters hidepth : print ( . format ( sample . name , sample . files . clusters ) ) elif sample . stats . state < 4 : print ( . format ( sample . name ) ) else : subsamples . append ( sample ) if len ( subsamples ) == 0 : raise I Pyrad Warning Exit ( ) if \"hetero est\" not in data . stats : print ( \"  No estimates of heterozygosity and error rate. Using default \" \"values\" ) for sample in subsamples : sample . stats . hetero est = 0.001 sample . stats . error est = 0.0001 if data . headers : print ( . format ( data . stats . error est . mean ( ) , data . stats . error est . std ( ) , data . stats . hetero est . mean ( ) , data . stats . hetero est . std ( ) ) ) return subsamples", "predictions": ["trees trees to run the cwl function"], "references": ["apply state ncluster and force filters to select samples to be run ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 2662, "code": "def run ( data , samples , force , ipyclient ) : data . dirs . consens = os . path . join ( data . dirs . project , data . name + \" consens\" ) if not os . path . exists ( data . dirs . consens ) : os . mkdir ( data . dirs . consens ) tmpcons = glob . glob ( os . path . join ( data . dirs . consens , \"* tmpcons.*\" ) ) tmpcats = glob . glob ( os . path . join ( data . dirs . consens , \"* tmpcats.*\" ) ) for tmpfile in tmpcons + tmpcats : os . remove ( tmpfile ) samples = get subsamples ( data , samples , force ) lbview = ipyclient . load balanced view ( ) data . cpus = data . ipcluster [ \"cores\" ] if not data . cpus : data . cpus = len ( ipyclient . ids ) inst = \"\" try : samples = calculate depths ( data , samples , lbview ) lasyncs = make chunks ( data , samples , lbview ) process chunks ( data , samples , lasyncs , lbview ) except Keyboard Interrupt as inst : raise inst finally : tmpcons = glob . glob ( os . path . join ( data . dirs . clusts , \"tmp *.[0-9]*\" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , \"* tmpcons.*\" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , \"* tmpcats.*\" ) ) for tmpchunk in tmpcons : os . remove ( tmpchunk ) data . checkpoint = 0", "predictions": ["run function for running a balanced model"], "references": ["checks if the sample should be run and passes the args"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 2663, "code": "def make chunks ( data , samples , lbview ) : start = time . time ( ) printstr = \" chunking clusters     | {} | s5 |\" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . spacer ) lasyncs = { } for sample in samples : lasyncs [ sample . name ] = lbview . apply ( chunk clusters , * ( data , sample ) ) while 1 : ready = [ i . ready ( ) for i in lasyncs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( \"\" ) break for sample in samples : if not lasyncs [ sample . name ] . successful ( ) : LOGGER . error ( \"  sample %s failed: %s\" , sample . name , lasyncs [ sample . name ] . exception ( ) ) return lasyncs", "predictions": ["finalize self format and return"], "references": ["calls chunk_clusters and tracks progress ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2664, "code": "def make ( data , samples ) : #read in loci file outfile = open ( os . path . join ( data . dirs . outfiles , data . name + \".alleles\" ) , 'w' ) lines = open ( os . path . join ( data . dirs . outfiles , data . name + \".loci\" ) , 'r' ) longname = max ( len ( x ) for x in data . samples . keys ( ) ) name padding = 5 writing = [ ] loc = 0 for line in lines : if \">\" in line : name , seq = line . split ( \" \" ) [ 0 ] , line . split ( \" \" ) [ - 1 ] allele1 , allele2 = splitalleles ( seq . strip ( ) ) writing . append ( name + \" 0\" + \" \" * ( longname - len ( name ) - 2 + name padding ) + allele1 ) writing . append ( name + \" 1\" + \" \" * ( longname - len ( name ) - 2 + name padding ) + allele2 ) else : writing . append ( line . strip ( ) ) loc += 1 if not loc % 10000 : outfile . write ( \"\\n\" . join ( writing ) + \"\\n\" ) writing = [ ] outfile . write ( \"\\n\" . join ( writing ) ) outfile . close ( )", "predictions": ["save self into a dict"], "references": ["reads in . loci and builds alleles from case characters"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2665, "code": "def cluster info ( ipyclient , spacer = \"\" ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( socket . gethostname ) ) hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( \"{}host compute node: [{} cores] on {}\" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print \"\\n\" . join ( result )", "predictions": ["if insert is available if not found if they are running"], "references": ["reports host and engine info for an ipyclient"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2666, "code": "def set debug dict ( loglevel ) : lconfig . dict Config ( { 'version' : 1 , 'disable existing loggers' : False , 'formatters' : { 'standard' : { 'format' : \"%(asctime)s \\t\" + \"pid=%(process)d \\t\" + \"[%(filename)s]\\t\" + \"%(levelname)s \\t\" + \"%(message)s\" } , } , 'handlers' : { name : { 'level' : loglevel , 'class' : 'logging.File Handler' , 'filename' : debugfile , 'formatter' : \"standard\" , 'mode' : 'a+' } } , 'loggers' : { name : { 'handlers' : [ name ] , 'level' : loglevel , 'propogate' : True } } } )", "predictions": ["sets a debug s debug dictionary for the given loglevel max"], "references": ["set the debug dict"], "bleu": 0.12605968092174913, "rouge_l": 0.14558472553699284}
{"id": 2667, "code": "def debug off ( ) : if os . path . exists ( debugflag ) : os . remove ( debugflag ) loglevel = \"ERROR\" LOGGER . info ( \"debugging turned off\" ) set debug dict ( loglevel )", "predictions": ["start debug off off"], "references": ["turns off debugging by removing hidden tmp file"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2668, "code": "def cmd exists ( cmd ) : return subprocess . call ( \"type \" + cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) == 0", "predictions": ["check if a command pairs pairs are available"], "references": ["check if dependency program is there"], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 2669, "code": "def random product ( iter1 , iter2 ) : iter4 = np . concatenate ( [ np . random . choice ( iter1 , 2 , replace = False ) , np . random . choice ( iter2 , 2 , replace = False ) ] ) return iter4", "predictions": ["make a enter product with a enter product"], "references": ["random sampler for equal_splits functions"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2670, "code": "def get total ( tots , node ) : if ( node . is leaf ( ) or node . is root ( ) ) : return 0 else : if len ( node . children ) > 2 : down r = node . children [ 0 ] down l = node . children [ 1 ] for child in node . children [ 2 : ] : down l += child else : down r , down l = node . children lendr = sum ( 1 for i in down r . iter leaves ( ) ) lendl = sum ( 1 for i in down l . iter leaves ( ) ) up r = node . get sisters ( ) [ 0 ] lenur = sum ( 1 for i in up r . iter leaves ( ) ) lenul = tots - ( lendr + lendl + lenur ) return lendr * lendl * lenur * lenul", "predictions": ["calculate the numba of a node"], "references": ["get total number of quartets possible for a split"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2671, "code": "def get sampled ( data , totn , node ) : names = sorted ( totn ) cdict = { name : idx for idx , name in enumerate ( names ) } if ( node . is leaf ( ) or node . is root ( ) ) : return 0 else : if len ( node . children ) > 2 : down r = node . children [ 0 ] down l = node . children [ 1 ] for child in node . children [ 2 : ] : down l += child else : down r , down l = node . children lendr = set ( cdict [ i ] for i in down r . get leaf names ( ) ) lendl = set ( cdict [ i ] for i in down l . get leaf names ( ) ) up r = node . get sisters ( ) [ 0 ] lenur = set ( cdict [ i ] for i in up r . get leaf names ( ) ) lenul = set ( cdict [ i ] for i in totn ) - set . union ( lendr , lendl , lenur ) idx = 0 sampled = 0 with h5py . File ( data . database . output , 'r' ) as io5 : end = io5 [ \"quartets\" ] . shape [ 0 ] while 1 : if idx >= end : break qrts = io5 [ \"quartets\" ] [ idx : idx + data . chunksize ] for qrt in qrts : sqrt = set ( qrt ) if all ( [ sqrt . intersection ( i ) for i in [ lendr , lendl , lenur , lenul ] ] ) : sampled += 1 idx += data . chunksize return sampled", "predictions": ["return snps data with all nodes"], "references": ["get total number of quartets sampled for a split"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 2672, "code": "def run qmc ( self , boot ) : self . tmp = os . path . join ( self . dirs , \".tmptre\" ) cmd = [ ip . bins . qmc , \"qrtt=\" + self . files . qdump , \"otre=\" + self . tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : raise I Pyrad Warning Exit ( res [ 1 ] ) with open ( self . tmp , 'r' ) as intree : tre = ete3 . Tree ( intree . read ( ) . strip ( ) ) names = tre . get leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] tmptre = tre . write ( format = 9 ) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + \".boots\" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmptre + \"\\n\" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + \".tree\" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmptre ) self . save ( )", "predictions": ["write a file of qmc and tre trees"], "references": ["runs quartet max - cut qmc on the quartets qdump file ."], "bleu": 0.10764345432696364, "rouge_l": 0.09651898734177215}
{"id": 2673, "code": "def load ( self , name , workdir , quiet = False ) : path = os . path . join ( workdir , name ) if not path . endswith ( \".tet.json\" ) : path += \".tet.json\" path = path . replace ( \"~\" , os . path . expanduser ( \"~\" ) ) try : with open ( path , 'r' ) as infile : fullj = byteify ( json . loads ( infile . read ( ) , object hook = byteify ) , ignore dicts = True ) except IO Error : raise I Pyrad Warning Exit ( . format ( path ) ) self . name = fullj [ \"name\" ] self . files . data = fullj [ \"files\" ] [ \"data\" ] self . files . mapfile = fullj [ \"files\" ] [ \"mapfile\" ] self . dirs = fullj [ \"dirs\" ] self . init seqarray ( quiet = quiet ) self . parse names ( ) for key in fullj : if key in [ \"files\" , \"params\" , \"database\" , \"trees\" , \"stats\" , \"checkpoint\" ] : filler = fullj [ key ] for ikey in filler : self . dict [ key ] . setattr ( ikey , fullj [ key ] [ ikey ] ) else : self . setattr ( key , fullj [ key ] )", "predictions": ["concat the data from the given file"], "references": ["load a json serialized tetrad instance to continue from a checkpoint ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 2674, "code": "def insert to array ( self , chunk , results ) : chunksize = self . chunksize qrts , invs = results with h5py . File ( self . database . output , 'r+' ) as io5 : io5 [ 'quartets' ] [ chunk : chunk + chunksize ] = qrts if self . params . save invariants : if self . checkpoint . boots : key = \"invariants/boot{}\" . format ( self . checkpoint . boots ) io5 [ key ] [ chunk : chunk + chunksize ] = invs else : io5 [ \"invariants/boot0\" ] [ chunk : chunk + chunksize ] = invs", "predictions": ["insert results to array"], "references": ["enters results arrays into the hdf5 database ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2675, "code": "def memoize ( func ) : class Memodict ( dict ) : \"\"\" just a dict\"\"\" def getitem ( self , * key ) : return dict . getitem ( self , key ) def missing ( self , key ) : \"\"\" this makes it faster \"\"\" ret = self [ key ] = func ( * key ) return ret return Memodict ( ) . getitem", "predictions": ["memoize for a function or method to implement an api call"], "references": ["memoization decorator for a function taking one or more arguments ."], "bleu": 0.22416933501922287, "rouge_l": 0.36363636363636365}
{"id": 2676, "code": "def comp ( seq ) : return seq . replace ( \"A\" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( \"Z\" , \"n\" )", "predictions": ["return function for \"a\""], "references": ["returns a seq with complement . preserves little n s for splitters ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 2677, "code": "def fastq touchup for vsearch merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : if read . endswith ( \".gz\" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) writing = [ ] while 1 : try : lines = quarts . next ( ) except Stop Iteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( \"\" . join ( [ lines [ 0 ] , seq + \"\\n\" , lines [ 2 ] , \"B\" * len ( seq ) ] ) ) counts += 1 if not counts % 1000 : out . write ( \"\\n\" . join ( writing ) + \"\\n\" ) writing = [ ] if writing : out . write ( \"\\n\" . join ( writing ) ) out . close ( ) fr1 . close ( )", "predictions": ["perform get quick quick quick quick depths"], "references": ["option to change orientation of reads and sets qscore to b"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 2678, "code": "def revcomp ( sequence ) : sequence = sequence [ : : - 1 ] . strip ( ) . replace ( \"A\" , \"t\" ) . replace ( \"T\" , \"a\" ) . replace ( \"C\" , \"g\" ) . replace ( \"G\" , \"c\" ) . upper ( ) return sequence", "predictions": ["get the minimum align name for a parse parse 5 5 5 5 5 5 5 5 5 5 ."], "references": ["returns reverse complement of a string"], "bleu": 0.06108557268562171, "rouge_l": 0.08519553072625699}
{"id": 2679, "code": "def clustdealer ( pairdealer , optim ) : ccnt = 0 chunk = [ ] while ccnt < optim : try : taker = itertools . takewhile ( lambda x : x [ 0 ] != \"//\\n\" , pairdealer ) oneclust = [ \"\" . join ( taker . next ( ) ) ] except Stop Iteration : #LOGGER.debug('last chunk %s', chunk) return 1 , chunk while 1 : try : oneclust . append ( \"\" . join ( taker . next ( ) ) ) except Stop Iteration : break chunk . append ( \"\" . join ( oneclust ) ) ccnt += 1 return 0 , chunk", "predictions": ["+ optim function to get next chunk chunk"], "references": ["return optim clusters given iterators and whether it got all or not"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 2680, "code": "def progressbar ( njobs , finished , msg = \"\" , spacer = \"  \" ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . interactive : msg = msg . rsplit ( \"|\" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( \"\\r{}[{}] {:>3}% {} \" . format ( * args ) , end = \"\" ) sys . stdout . flush ( )", "predictions": ["print a formatted message"], "references": ["prints a progress bar"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2681, "code": "def get threaded view ( ipyclient , split = True ) : eids = ipyclient . ids dview = ipyclient . direct view ( ) hosts = dview . apply sync ( socket . gethostname ) hostdict = defaultdict ( list ) for host , eid in zip ( hosts , eids ) : hostdict [ host ] . append ( eid ) hostdictkeys = hostdict . keys ( ) for key in hostdictkeys : gids = hostdict [ key ] maxt = len ( gids ) if len ( gids ) >= 4 : maxt = 2 if ( len ( gids ) == 4 ) and ( len ( hosts ) >= 4 ) : maxt = 4 if len ( gids ) >= 6 : maxt = 3 if len ( gids ) >= 8 : maxt = 4 if len ( gids ) >= 16 : maxt = 4 threaded = [ gids [ i : i + maxt ] for i in xrange ( 0 , len ( gids ) , maxt ) ] lth = len ( threaded ) if lth > 1 : hostdict . pop ( key ) for hostid in range ( lth ) : hostdict [ str ( key ) + \" \" + str ( hostid ) ] = threaded [ hostid ] #threaded = hostdict.values() #assert len(ipyclient.ids) <= len(list(itertools.chain(*threaded))) LOGGER . info ( \"threaded view: %s\" , dict ( hostdict ) ) return hostdict", "predictions": ["view view view ."], "references": ["gets optimum threaded view of ids given the host setup"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2682, "code": "def call structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : outname = os . path . join ( workdir , \"{}-K-{}-rep-{}\" . format ( name , kpop , rep ) ) cmd = [ \"structure\" , \"-m\" , mname , \"-e\" , ename , \"-K\" , str ( kpop ) , \"-D\" , str ( seed ) , \"-N\" , str ( ntaxa ) , \"-L\" , str ( nsites ) , \"-i\" , sname , \"-o\" , outname ] proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm", "predictions": ["plot a dag dag gist and prints a dag gist gist ."], "references": ["make the subprocess call to structure"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 2683, "code": "def get clumpp table ( self , kpop , max var multiple , quiet ) : reps , excluded = concat reps ( self , kpop , max var multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return \"no result files found\" clumphandle = os . path . join ( self . workdir , \"tmp.clumppparams.txt\" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp c : tmp c . write ( self . clumppparams . asfile ( ) ) outfile = os . path . join ( self . workdir , \"{}-K-{}.outfile\" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , \"{}-K-{}.indfile\" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , \"{}-K-{}.miscfile\" . format ( self . name , kpop ) ) cmd = [ \"CLUMPP\" , clumphandle , \"-i\" , indfile , \"-o\" , outfile , \"-j\" , miscfile , \"-r\" , str ( nreps ) , \"-c\" , str ( ninds ) , \"-k\" , str ( kpop ) ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) = proc . communicate ( ) for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ofile = os . path . join ( self . workdir , \"{}-K-{}.outfile\" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read csv ( ofile , delim whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( \"[K{}] {}/{} results permuted across replicates (max var={}).\\n\" . format ( kpop , nreps , nreps + excluded , max var multiple ) ) return table else : sys . stderr . write ( \"No files ready for {}-K-{} in {}\\n\" . format ( self . name , kpop , self . workdir ) ) return", "predictions": ["not implemented in a clumpp"], "references": ["private function to clumpp results"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 2684, "code": "def result files ( self ) : reps = OPJ ( self . workdir , self . name + \"-K-*-rep-* f\" ) repfiles = glob . glob ( reps ) return repfiles", "predictions": ["sys params to be used to build the parse parse parse ."], "references": ["returns a list of files that have finished structure"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 2685, "code": "def parse ( self , psearch , dsearch ) : stable = \"\" with open ( self . repfile ) as orep : dat = orep . readlines ( ) for line in dat : if \"Estimated Ln Prob of Data\" in line : self . est lnlik = float ( line . split ( ) [ - 1 ] ) if \"Mean value of ln likelihood\" in line : self . mean lnlik = float ( line . split ( ) [ - 1 ] ) if \"Variance of ln likelihood\" in line : self . var lnlik = float ( line . split ( ) [ - 1 ] ) if \"Mean value of alpha\" in line : self . alpha = float ( line . split ( ) [ - 1 ] ) nonline = psearch . search ( line ) popline = dsearch . search ( line ) #if \")   :  \" in line: if nonline : abc = line . strip ( ) . split ( ) outstr = \"{}{}{}\" . format ( \" \" . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , \" :  \" , \" \" . join ( abc [ 4 : ] ) ) self . inds += 1 stable += outstr + \"\\n\" elif popline : abc = line . strip ( ) . split ( ) prop = [ \"0.000\" ] * self . kpop pidx = int ( abc [ 3 ] ) - 1 prop [ pidx ] = \"1.000\" outstr = \"{}{}{}\" . format ( \" \" . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , \" :  \" , \" \" . join ( prop ) ) self . inds += 1 stable += outstr + \"\\n\" stable += \"\\n\" return stable", "predictions": ["parse is a stable instance"], "references": ["parse an _f structure output file"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2686, "code": "def call raxml ( command list ) : proc = subprocess . Popen ( command list , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) comm = proc . communicate ( ) return comm", "predictions": ["check command and format them . . . ."], "references": ["call the command as sps"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2687, "code": "def command list ( self ) : cmd = [ self . params . binary , \"-f\" , str ( self . params . f ) , \"-T\" , str ( self . params . T ) , \"-m\" , str ( self . params . m ) , \"-N\" , str ( self . params . N ) , \"-x\" , str ( self . params . x ) , \"-p\" , str ( self . params . p ) , \"-n\" , str ( self . params . n ) , \"-w\" , str ( self . params . w ) , \"-s\" , str ( self . params . s ) , ] if self . params . o : cmd += [ \"-o\" ] cmd += [ \",\" . join ( self . params . o ) ] return cmd", "predictions": ["list the get parameters"], "references": ["build the command list"], "bleu": 0.3976353643835253, "rouge_l": 0.25}
{"id": 2688, "code": "def dstat ( inarr , taxdict , mindict = 1 , nboots = 1000 , name = 0 ) : #if isinstance(inarr, str): if isinstance ( inarr , list ) : arr , = loci to arr ( inarr , taxdict , mindict ) #elif isinstance(inarr, types.Generator Type): #elif isinstance(inarr, list): #else: #if len(taxdict) == 4: if arr . shape [ 1 ] == 4 : res , boots = get signif 4 ( arr , nboots ) res = pd . Data Frame ( res , columns = [ name ] , index = [ \"Dstat\" , \"bootmean\" , \"bootstd\" , \"Z\" , \"ABBA\" , \"BABA\" , \"nloci\" ] ) else : res , boots = get signif 5 ( arr , nboots ) res = pd . Data Frame ( res , index = [ \"p3\" , \"p4\" , \"shared\" ] , columns = [ \"Dstat\" , \"bootmean\" , \"bootstd\" , \"Z\" , \"A Bxx A\" , \"B Axx A\" , \"nloci\" ] ) return res . T , boots", "predictions": ["gets the full full column for a given model"], "references": ["private function to perform a single d - stat test"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 2689, "code": "def get boots ( arr , nboots ) : boots = np . zeros ( ( nboots , ) ) for bidx in xrange ( nboots ) : lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] , , dst = prop dstat ( tarr ) boots [ bidx ] = dst return boots", "predictions": ["r get a random boots class"], "references": ["return array of bootstrap d - stats"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2690, "code": "def Async ( cls , token , session = None , * * options ) : return cls ( token , session = session , is async = True , * * options )", "predictions": ["get an auth token ."], "references": ["returns the client in async mode ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2691, "code": "def typecasted ( func ) : signature = inspect . signature ( func ) . parameters . items ( ) @ wraps ( func ) def wrapper ( * args , * * kwargs ) : args = list ( args ) new args = [ ] new kwargs = { } for , param in signature : converter = param . annotation if converter is inspect . empty : converter = lambda a : a if param . kind is param . POSITIONAL OR KEYWORD : if args : to conv = args . pop ( 0 ) new args . append ( converter ( to conv ) ) elif param . kind is param . VAR POSITIONAL : for a in args : new args . append ( converter ( a ) ) else : for k , v in kwargs . items ( ) : nk , nv = converter ( k , v ) new kwargs [ nk ] = nv return func ( * new args , * * new kwargs ) return wrapper", "predictions": ["decorator to convert a function into a dictionary of parameters ."], "references": ["decorator that converts arguments via annotations ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 2692, "code": "def list namespaces ( ) : print ( '{:30s}\\t{:40s}' . format ( 'NAME' , 'DESCRIPTION' ) ) print ( '-' * 78 ) for sch in sorted ( NAMESPACE ) : desc = NAMESPACE [ sch ] [ 'description' ] desc = ( desc [ : 44 ] + '..' ) if len ( desc ) > 46 else desc print ( '{:30s}\\t{:40s}' . format ( sch , desc ) )", "predictions": ["list all available namespaces ."], "references": ["print out a listing of available namespaces"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 2693, "code": "def load jams schema ( ) : schema file = os . path . join ( SCHEMA DIR , 'jams schema.json' ) jams schema = None with open ( resource filename ( name , schema file ) , mode = 'r' ) as fdesc : jams schema = json . load ( fdesc ) if jams schema is None : raise Jams Error ( 'Unable to load JAMS schema' ) return jams schema", "predictions": ["load json schema from file ."], "references": ["load the schema file from the package ."], "bleu": 0.20830666398386116, "rouge_l": 0.5570776255707762}
{"id": 2694, "code": "def parse arguments ( args ) : parser = argparse . Argument Parser ( description = 'Convert JAMS to .lab files' ) parser . add argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add argument ( '--comment' , dest = 'comment char' , type = str , default = '#' , help = 'Comment character' ) parser . add argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output.  Default is all.' ) parser . add argument ( 'jams file' , help = 'Path to the input jams file' ) parser . add argument ( 'output prefix' , help = 'Prefix for output files' ) return vars ( parser . parse args ( args ) )", "predictions": ["parse command line arguments"], "references": ["parse arguments from the command line"], "bleu": 0.3258798048281462, "rouge_l": 0.5791139240506329}
{"id": 2695, "code": "def pitch hz to contour ( annotation ) : annotation . namespace = 'pitch contour' data = annotation . pop data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = dict ( index = 0 , frequency = np . abs ( obs . value ) , voiced = obs . value > 0 ) ) return annotation", "predictions": ["convert a list of pitch hz to a contour"], "references": ["convert a pitch_hz annotation to a contour"], "bleu": 0.31239399369202553, "rouge_l": 0.639412997903564}
{"id": 2696, "code": "def note hz to midi ( annotation ) : annotation . namespace = 'note midi' data = annotation . pop data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation", "predictions": ["convert an annotation annotation object to a midi midi file ."], "references": ["convert a pitch_hz annotation to pitch_midi"], "bleu": 0.14323145079400493, "rouge_l": 0.3727087576374745}
{"id": 2697, "code": "def scaper to tag ( annotation ) : annotation . namespace = 'tag open' data = annotation . pop data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation", "predictions": ["convert an annotation object to a list of annotation objects"], "references": ["convert scaper annotations to tag_open"], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 2698, "code": "def key ( cls , obs ) : if not isinstance ( obs , Observation ) : raise Jams Error ( '{} must be of type jams.Observation' . format ( obs ) ) return obs . time", "predictions": ["get a key for the given object ."], "references": ["provides sorting index for observation objects"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2699, "code": "def intervals ( annotation , * * kwargs ) : times , labels = annotation . to interval values ( ) return mir eval . display . labeled intervals ( times , labels , * * kwargs )", "predictions": ["wrapper for the annotation display ."], "references": ["plotting wrapper for labeled intervals"], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 2700, "code": "def hierarchy ( annotation , * * kwargs ) : htimes , hlabels = hierarchy flatten ( annotation ) htimes = [ np . asarray ( ) for in htimes ] return mir eval . display . hierarchy ( htimes , hlabels , * * kwargs )", "predictions": ["return a hierarchy of the hierarchy given a annotation ."], "references": ["plotting wrapper for hierarchical segmentations"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2701, "code": "def pitch contour ( annotation , * * kwargs ) : ax = kwargs . pop ( 'ax' , None ) ax = mir eval . display . get axes ( ax = ax ) [ 0 ] times , values = annotation . to interval values ( ) indices = np . unique ( [ v [ 'index' ] for v in values ] ) for idx in indices : rows = [ i for ( i , v ) in enumerate ( values ) if v [ 'index' ] == idx ] freqs = np . asarray ( [ values [ r ] [ 'frequency' ] for r in rows ] ) unvoiced = ~ np . asarray ( [ values [ r ] [ 'voiced' ] for r in rows ] ) freqs [ unvoiced ] *= - 1 ax = mir eval . display . pitch ( times [ rows , 0 ] , freqs , unvoiced = True , ax = ax , * * kwargs ) return ax", "predictions": ["display a pitch contour"], "references": ["plotting wrapper for pitch contours"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 2702, "code": "def event ( annotation , * * kwargs ) : times , values = annotation . to interval values ( ) if any ( values ) : labels = values else : labels = None return mir eval . display . events ( times , labels = labels , * * kwargs )", "predictions": ["wrapper for the named events ."], "references": ["plotting wrapper for events"], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 2703, "code": "def beat position ( annotation , * * kwargs ) : times , values = annotation . to interval values ( ) labels = [ [ 'position' ] for in values ] return mir eval . display . events ( times , labels = labels , * * kwargs )", "predictions": ["get the beat position for a given annotation ."], "references": ["plotting wrapper for beat - position data"], "bleu": 0.16784459625186196, "rouge_l": 0.2557651991614256}
{"id": 2704, "code": "def piano roll ( annotation , * * kwargs ) : times , midi = annotation . to interval values ( ) return mir eval . display . piano roll ( times , midi = midi , * * kwargs )", "predictions": ["roll an annotation object to a piano instance ."], "references": ["plotting wrapper for piano rolls"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2705, "code": "def downbeat ( annotation , sr = 22050 , length = None , * * kwargs ) : beat click = mkclick ( 440 * 2 , sr = sr ) downbeat click = mkclick ( 440 * 3 , sr = sr ) intervals , values = annotation . to interval values ( ) beats , downbeats = [ ] , [ ] for time , value in zip ( intervals [ : , 0 ] , values ) : if value [ 'position' ] == 1 : downbeats . append ( time ) else : beats . append ( time ) if length is None : length = int ( sr * np . max ( intervals ) ) + len ( beat click ) + 1 y = filter kwargs ( mir eval . sonify . clicks , np . asarray ( beats ) , fs = sr , length = length , click = beat click ) y += filter kwargs ( mir eval . sonify . clicks , np . asarray ( downbeats ) , fs = sr , length = length , click = downbeat click ) return y", "predictions": ["downbeat a list of annotation values ."], "references": ["sonify beats and downbeats together ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2706, "code": "def multi segment ( annotation , sr = 22050 , length = None , * * kwargs ) : PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h int , = hierarchy flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( ) for in h int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h int , product ( range ( 3 , 3 + len ( h int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter kwargs ( mir eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y", "predictions": ["segment a multi - segment segment using a multi - segment segment segment"], "references": ["sonify multi - level segmentations"], "bleu": 0.12571192676522522, "rouge_l": 0.24158415841584158}
{"id": 2707, "code": "def validate ( schema file = None , jams files = None ) : schema = load json ( schema file ) for jams file in jams files : try : jams = load json ( jams file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams file ) except jsonschema . Validation Error as exc : print '{:s} was NOT successfully validated' . format ( jams file ) print exc", "predictions": ["validate a schema ."], "references": ["validate a jams file against a schema"], "bleu": 0.26563123324397914, "rouge_l": 0.5198863636363635}
{"id": 2708, "code": "def handle authorized event ( self , event ) : self . server = event . authorized jid . bare ( ) if \"versioning\" in self . server features : if self . roster is not None and self . roster . version is not None : version = self . roster . version else : version = u\"\" else : version = None self . request roster ( version )", "predictions": ["authorized authorized event received from server"], "references": ["request roster upon login ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2709, "code": "def get success ( self , stanza ) : payload = stanza . get payload ( Roster Payload ) if payload is None : if \"versioning\" in self . server features and self . roster : logger . debug ( \"Server will send roster delta in pushes\" ) else : logger . warning ( \"Bad roster response (no payload)\" ) self . event queue . put ( Roster Not Received Event ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify roster result ( True ) self . roster = Roster ( items , payload . version ) self . event queue . put ( Roster Received Event ( self , self . roster ) )", "predictions": ["get roster items from server"], "references": ["handle successful response to the roster request ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2710, "code": "def get error ( self , stanza ) : if stanza : logger . debug ( u\"Roster request failed: {0}\" . format ( stanza . error . condition name ) ) else : logger . debug ( u\"Roster request failed: timeout\" ) self . event queue . put ( Roster Not Received Event ( self , stanza ) )", "predictions": ["get the error stanza for a given stanza ."], "references": ["handle failure of the roster request ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 2711, "code": "def handle roster push ( self , stanza ) : if self . server is None and stanza . from jid : logger . debug ( u\"Server address not known, cannot verify roster push\" \" from {0}\" . format ( stanza . from jid ) ) return stanza . make error response ( u\"service-unavailable\" ) if self . server and stanza . from jid and stanza . from jid != self . server : logger . debug ( u\"Roster push from invalid source: {0}\" . format ( stanza . from jid ) ) return stanza . make error response ( u\"service-unavailable\" ) payload = stanza . get payload ( Roster Payload ) if len ( payload ) != 1 : logger . warning ( \"Bad roster push received ({0} items)\" . format ( len ( payload ) ) ) return stanza . make error response ( u\"bad-request\" ) if self . roster is None : logger . debug ( \"Dropping roster push - no roster here\" ) return True item = payload [ 0 ] item . verify roster push ( True ) old item = self . roster . get ( item . jid ) if item . subscription == \"remove\" : if old item : self . roster . remove item ( item . jid ) else : self . roster . add item ( item , replace = True ) self . event queue . put ( Roster Updated Event ( self , old item , item ) ) return stanza . make result response ( )", "predictions": ["handle a roster push stanza"], "references": ["handle a roster push received from server ."], "bleu": 0.41700666580507684, "rouge_l": 0.5907990314769976}
{"id": 2712, "code": "def free ( self ) : if not self . borrowed : self . xmlnode . unlink Node ( ) self . xmlnode . free Node ( ) self . xmlnode = None", "predictions": ["free up the free node"], "references": ["unlink and free the xml node owned by self ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 2713, "code": "def clear muc child ( self ) : if self . muc child : self . muc child . free borrowed ( ) self . muc child = None if not self . xmlnode . children : return n = self . xmlnode . children while n : if n . name not in ( \"x\" , \"query\" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns uri = ns . get Content ( ) if ns uri in ( MUC NS , MUC USER NS , MUC ADMIN NS , MUC OWNER NS ) : n . unlink Node ( ) n . free Node ( ) n = n . next", "predictions": ["clear muc child child child child ."], "references": ["remove the muc specific stanza payload element ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2714, "code": "def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result", "predictions": ["apply a function to each mapping"], "references": ["mapping part of string preparation ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2715, "code": "def prohibit ( self , data ) : for char in data : for lookup in self . prohibited : if lookup ( char ) : raise Stringprep Error ( \"Prohibited character: {0!r}\" . format ( char ) ) return data", "predictions": ["find the data for the given data ."], "references": ["checks for prohibited characters ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 2716, "code": "def check unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise Stringprep Error ( \"Unassigned character: {0!r}\" . format ( char ) ) return data", "predictions": ["check that unassigned data is a valid unassigned data ."], "references": ["checks for unassigned character codes ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2717, "code": "def check bidi ( data ) : has l = False has ral = False for char in data : if stringprep . in table d1 ( char ) : has ral = True elif stringprep . in table d2 ( char ) : has l = True if has l and has ral : raise Stringprep Error ( \"Both Rand AL Cat and L Cat characters present\" ) if has ral and ( not stringprep . in table d1 ( data [ 0 ] ) or not stringprep . in table d1 ( data [ - 1 ] ) ) : raise Stringprep Error ( \"The first and the last character must\" \" be Rand AL Cat\" ) return data", "predictions": ["check that the first character is a valid character ."], "references": ["checks if sting is valid for bidirectional printing ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 2718, "code": "def configure io handler ( self , handler ) : if self . check events ( ) : return if handler in self . unprepared handlers : old fileno = self . unprepared handlers [ handler ] prepared = self . prepare io handler ( handler ) else : old fileno = None prepared = True fileno = handler . fileno ( ) if old fileno is not None and fileno != old fileno : tag = self . io sources . pop ( handler , None ) if tag is not None : glib . source remove ( tag ) if not prepared : self . unprepared handlers [ handler ] = fileno if fileno is None : logger . debug ( \" {0!r}.fileno() is None, not polling\" . format ( handler ) ) return events = 0 if handler . is readable ( ) : logger . debug ( \" {0!r} readable\" . format ( handler ) ) events |= glib . IO IN | glib . IO ERR if handler . is writable ( ) : logger . debug ( \" {0!r} writable\" . format ( handler ) ) events |= glib . IO OUT | glib . IO HUP | glib . IO ERR if events : logger . debug ( \" registering {0!r} handler fileno {1} for\" \" events {2}\" . format ( handler , fileno , events ) ) glib . io add watch ( fileno , events , self . io callback , handler )", "predictions": ["register handler for the unprepared event ."], "references": ["register an io - handler with the glib main loop ."], "bleu": 0.1319006407505858, "rouge_l": 0.4273204903677758}
{"id": 2719, "code": "def prepare pending ( self ) : if not self . unprepared pending : return for handler in list ( self . unprepared pending ) : self . configure io handler ( handler ) self . check events ( )", "predictions": ["prepare all unprepared events to be invoked ."], "references": ["prepare pending handlers ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 2720, "code": "def prepare io handler cb ( self , handler ) : self . anything done = True logger . debug ( \" prepar io handler cb called for {0!r}\" . format ( handler ) ) self . configure io handler ( handler ) self . prepare sources . pop ( handler , None ) return False", "predictions": ["called to initialize the name of the name of the name 1000 1000 1000 1000 1000 1000 1000 ."], "references": ["timeout callback called to try prepare an iohandler again ."], "bleu": 0.09107438368292149, "rouge_l": 0.21916167664670658}
{"id": 2721, "code": "def add timeout handler ( self , handler ) : for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , \" pyxmpp timeout\" ) : continue tag = glib . timeout add ( int ( method . pyxmpp timeout * 1000 ) , self . timeout cb , method ) self . timer sources [ method ] = tag", "predictions": ["register a boots handler handler with a boots . . . . . . . . . . . . . . . . . . . . . ."], "references": ["add a timeouthandler to the main loop ."], "bleu": 0.044644767873512764, "rouge_l": 0.11753371868978806}
{"id": 2722, "code": "def remove timeout handler ( self , handler ) : for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , \" pyxmpp timeout\" ) : continue tag = self . timer sources . pop ( method , None ) if tag is not None : glib . source remove ( tag )", "predictions": ["remove a timeout cls cls is removed is removed is removed is removed is removed is called by this method to remove is removed is removed is called from ."], "references": ["remove timeouthandler from the main loop ."], "bleu": 0.047973925170118475, "rouge_l": 0.18263473053892215}
{"id": 2723, "code": "def timeout cb ( self , method ) : self . anything done = True logger . debug ( \" timeout cb() called for: {0!r}\" . format ( method ) ) result = method ( ) rec = method . pyxmpp recurring if rec : self . prepare pending ( ) return True if rec is None and result is not None : logger . debug ( \" auto-recurring, restarting in {0} s\" . format ( result ) ) tag = glib . timeout add ( int ( result * 1000 ) , self . timeout cb , method ) self . timer sources [ method ] = tag else : self . timer sources . pop ( method , None ) self . prepare pending ( ) return False", "predictions": ["called when a timeout is received ."], "references": ["call the timeout handler due ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2724, "code": "def loop timeout cb ( self , main loop ) : self . anything done = True logger . debug ( \" loop timeout cb() called\" ) main loop . quit ( )", "predictions": ["called when the namespaces is started ."], "references": ["stops the loop after the time specified in the loop call ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 2725, "code": "def setup stanza handlers ( self , handler objects , usage restriction ) : iq handlers = { \"get\" : { } , \"set\" : { } } message handlers = [ ] presence handlers = [ ] for obj in handler objects : if not isinstance ( obj , XMPP Feature Handler ) : continue obj . stanza processor = self for dummy , handler in inspect . getmembers ( obj , callable ) : if not hasattr ( handler , \" pyxmpp stanza handled\" ) : continue element name , stanza type = handler . pyxmpp stanza handled restr = handler . pyxmpp usage restriction if restr and restr != usage restriction : continue if element name == \"iq\" : payload class = handler . pyxmpp payload class handled payload key = handler . pyxmpp payload key if ( payload class , payload key ) in iq handlers [ stanza type ] : continue iq handlers [ stanza type ] [ ( payload class , payload key ) ] = handler continue elif element name == \"message\" : handler list = message handlers elif element name == \"presence\" : handler list = presence handlers else : raise Value Error , \"Bad handler decoration\" handler list . append ( handler ) with self . lock : self . iq handlers = iq handlers self . presence handlers = presence handlers self . message handlers = message handlers", "predictions": ["sets up the event schema schema schema schema . ."], "references": ["install stanza handlers provided by handler_objects"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2726, "code": "def add timeout handler ( self , handler ) : now = time . time ( ) for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , \" pyxmpp timeout\" ) : continue self . timeout handlers . append ( ( now + method . pyxmpp timeout , method ) ) self . timeout handlers . sort ( key = lambda x : x [ 0 ] )", "predictions": ["register a arguments for a given arguments . . . . . . . . . . . . . . . . . . ."], "references": ["add a timeouthandler to the main loop ."], "bleu": 0.051660454541342535, "rouge_l": 0.1300639658848614}
{"id": 2727, "code": "def remove timeout handler ( self , handler ) : self . timeout handlers = [ ( t , h ) for ( t , h ) in self . timeout handlers if h . im self != handler ]", "predictions": ["pitch a hz to the event loop ."], "references": ["remove timeouthandler from the main loop ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 2728, "code": "def decode subelements ( self ) : for child in self . element : if child . tag == self . subject tag : self . subject = child . text elif child . tag == self . body tag : self . body = child . text elif child . tag == self . thread tag : self . thread = child . text", "predictions": ["note that the hz is encoded as a hz element namespace namespace namespace namespace namespace namespace namespace namespace namespace namespace namespace"], "references": ["decode the stanza subelements ."], "bleu": 0.05809665204409193, "rouge_l": 0.08652482269503546}
{"id": 2729, "code": "def move session handler ( handlers ) : index = 0 for i , handler in enumerate ( handlers ) : if isinstance ( handler , Session Handler ) : index = i break if index : handlers [ : index + 1 ] = [ handlers [ index ] ] + handlers [ : index ]", "predictions": ["move a to a to a to a to a to a to a to a to a to a to a to a to a to a to a to"], "references": ["find a sessionhandler instance in the list and move it to the beginning ."], "bleu": 0.046398855339878003, "rouge_l": 0.09538702111024237}
{"id": 2730, "code": "def connect ( self ) : with self . lock : if self . stream : logger . debug ( \"Closing the previously used stream.\" ) self . close stream ( ) transport = TCP Transport ( self . settings ) addr = self . settings [ \"server\" ] if addr : service = None else : addr = self . jid . domain service = self . settings [ \"c2s service\" ] transport . connect ( addr , self . settings [ \"c2s port\" ] , service ) handlers = self . base handlers [ : ] handlers += self . handlers + [ self ] self . clear response handlers ( ) self . setup stanza handlers ( handlers , \"pre-auth\" ) stream = Client Stream ( self . jid , self , handlers , self . settings ) stream . initiate ( transport ) self . main loop . add handler ( transport ) self . main loop . add handler ( stream ) self . ml handlers += [ transport , stream ] self . stream = stream self . uplink = stream", "predictions": ["connects to the stream server"], "references": ["schedule a new xmpp c2s connection ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 2731, "code": "def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u\"initial presence\" ] : self . send ( Presence ( stanza type = \"unavailable\" ) ) self . stream . disconnect ( )", "predictions": ["disconnects from the server"], "references": ["gracefully disconnect from the server ."], "bleu": 0.4056114983537769, "rouge_l": 0.5791139240506329}
{"id": 2732, "code": "def close stream ( self ) : self . stream . close ( ) if self . stream . transport in self . ml handlers : self . ml handlers . remove ( self . stream . transport ) self . main loop . remove handler ( self . stream . transport ) self . stream = None self . uplink = None", "predictions": ["hierarchy hierarchy to quit and cleanup stream . . ."], "references": ["same as close_stream but with the lock acquired ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 2733, "code": "def stream authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer handlers = self . base handlers [ : ] handlers += self . handlers + [ self ] self . setup stanza handlers ( handlers , \"post-auth\" )", "predictions": ["pitch pitch * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["handle the authenticatedevent ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2734, "code": "def stream authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u\"initial presence\" ] if presence : self . send ( presence )", "predictions": ["event handler for event notification"], "references": ["handle the authorizedevent ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2735, "code": "def stream disconnected ( self , event ) : with self . lock : if event . stream != self . stream : return if self . stream is not None and event . stream == self . stream : if self . stream . transport in self . ml handlers : self . ml handlers . remove ( self . stream . transport ) self . main loop . remove handler ( self . stream . transport ) self . stream = None self . uplink = None", "predictions": ["called when a beat is received"], "references": ["handle stream disconnection event ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2736, "code": "def payload class for element name ( element name ) : logger . debug ( \" looking up payload class for element: {0!r}\" . format ( element name ) ) logger . debug ( \"  known: {0!r}\" . format ( STANZA PAYLOAD CLASSES ) ) if element name in STANZA PAYLOAD CLASSES : return STANZA PAYLOAD CLASSES [ element name ] else : return XML Payload", "predictions": ["get a payload roll roll roll from an annotation name times times times times times times times ."], "references": ["return a payload class for given element name ."], "bleu": 0.101824256461955, "rouge_l": 0.3152454780361757}
{"id": 2737, "code": "def deactivate ( self ) : self . cache . remove fetcher ( self ) if self . active : self . deactivated ( )", "predictions": ["deactivate a new 22050 fetcher = false if it is enabled = false = false = false = false = false = false = false = false = false = false"], "references": ["remove the fetcher from cache and mark it not active ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 2738, "code": "def decode asn1 string ( data ) : if isinstance ( data , BMP String ) : return bytes ( data ) . decode ( \"utf-16-be\" ) else : return bytes ( data ) . decode ( \"utf-8\" )", "predictions": ["multi - byte string function"], "references": ["convert asn . 1 string to a unicode string ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2739, "code": "def from ssl socket ( cls , ssl socket ) : cert = cls ( ) try : data = ssl socket . getpeercert ( ) except Attribute Error : return cert logger . debug ( \"Certificate data from ssl module: {0!r}\" . format ( data ) ) if not data : return cert cert . validated = True cert . subject name = data . get ( 'subject' ) cert . alt names = defaultdict ( list ) if 'subject Alt Name' in data : for name , value in data [ 'subject Alt Name' ] : cert . alt names [ name ] . append ( value ) if 'not After' in data : tstamp = ssl . cert time to seconds ( data [ 'not After' ] ) cert . not after = datetime . utcfromtimestamp ( tstamp ) if sys . version info . major < 3 : cert . decode names ( ) cert . common names = [ ] if cert . subject name : for part in cert . subject name : for name , value in part : if name == 'common Name' : cert . common names . append ( value ) return cert", "predictions": ["reads a certificate schema from an ssl schema json schema json schema json schema json schema json schema json schema json string json schema json schema json schema json schema json"], "references": ["load certificate data from an ssl socket ."], "bleu": 0.07678432706586173, "rouge_l": 0.2295390404515522}
{"id": 2740, "code": "def from ssl socket ( cls , ssl socket ) : try : data = ssl socket . getpeercert ( True ) except Attribute Error : data = None if not data : logger . debug ( \"No certificate infromation\" ) return cls ( ) result = cls . from der data ( data ) result . validated = bool ( ssl socket . getpeercert ( ) ) return result", "predictions": ["request a else request handle jid jid a else authorized jid jid jid jid jid jid jid jid ."], "references": ["get certificate data from an ssl socket ."], "bleu": 0.06439931429457924, "rouge_l": 0.0799475753604194}
{"id": 2741, "code": "def decode subject ( self , subject ) : self . common names = [ ] subject name = [ ] for rdnss in subject : for rdns in rdnss : rdnss list = [ ] for nameval in rdns : val type = nameval . get Component By Name ( 'type' ) value = nameval . get Component By Name ( 'value' ) if val type not in DN OIDS : logger . debug ( \"OID {0} not supported\" . format ( val type ) ) continue val type = DN OIDS [ val type ] value = der decoder . decode ( value , asn1Spec = Directory String ( ) ) [ 0 ] value = value . get Component ( ) try : value = decode asn1 string ( value ) except Unicode Error : logger . debug ( \"Cannot decode value: {0!r}\" . format ( value ) ) continue if val type == u\"common Name\" : self . common names . append ( value ) rdnss list . append ( ( val type , value ) ) subject name . append ( tuple ( rdnss list ) ) self . subject name = tuple ( subject name )", "predictions": ["get a success message"], "references": ["load data from a asn . 1 subject ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 2742, "code": "def decode validity ( self , validity ) : not after = validity . get Component By Name ( 'not After' ) not after = str ( not after . get Component ( ) ) if isinstance ( not after , Generalized Time ) : self . not after = datetime . strptime ( not after , \"%Y%m%d%H%M%SZ\" ) else : self . not after = datetime . strptime ( not after , \"%y%m%d%H%M%SZ\" ) self . alt names = defaultdict ( list )", "predictions": ["get error message for given error ."], "references": ["load data from a asn . 1 validity value ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2743, "code": "def from file ( cls , filename ) : with open ( filename , \"r\" ) as pem file : data = pem . read Pem From File ( pem file ) return cls . from der data ( data )", "predictions": ["read is a roster object from is is a is is a is a list of is the contents of is a is is a is in is a is a"], "references": ["load certificate from a file ."], "bleu": 0.04317900023606586, "rouge_l": 0.12310797174571139}
{"id": 2744, "code": "def run ( self ) : if self . args . roster cache and os . path . exists ( self . args . roster cache ) : logging . info ( u\"Loading roster from {0!r}\" . format ( self . args . roster cache ) ) try : self . client . roster client . load roster ( self . args . roster cache ) except ( IO Error , Value Error ) , err : logging . error ( u\"Could not load the roster: {0!r}\" . format ( err ) ) self . client . connect ( ) self . client . run ( )", "predictions": ["start the command loop not run the borrowed loop not passed to the client not ."], "references": ["request client connection and start the main loop ."], "bleu": 0.1203921753741131, "rouge_l": 0.33701657458563533}
{"id": 2745, "code": "def configure io handler ( self , handler ) : if self . check events ( ) : return if handler in self . unprepared handlers : old fileno = self . unprepared handlers [ handler ] prepared = self . prepare io handler ( handler ) else : old fileno = None prepared = True fileno = handler . fileno ( ) if old fileno is not None and fileno != old fileno : del self . handlers [ old fileno ] try : self . poll . unregister ( old fileno ) except Key Error : pass if not prepared : self . unprepared handlers [ handler ] = fileno if not fileno : return self . handlers [ fileno ] = handler events = 0 if handler . is readable ( ) : logger . debug ( \" {0!r} readable\" . format ( handler ) ) events |= select . POLLIN if handler . is writable ( ) : logger . debug ( \" {0!r} writable\" . format ( handler ) ) events |= select . POLLOUT if events : logger . debug ( \" registering {0!r} handler fileno {1} for\" \" events {2}\" . format ( handler , fileno , events ) ) self . poll . register ( fileno , events )", "predictions": ["register child child child process handlers"], "references": ["register an io - handler at the polling object ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2746, "code": "def remove ( self ) : if self . disco is None : return self . xmlnode . unlink Node ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . new Ns ( oldns . get Content ( ) , None ) self . xmlnode . replace Ns ( oldns , ns ) common root . add Child ( self . xmlnode ( ) ) self . disco = None", "predictions": ["removes a section from this xml section . ."], "references": ["remove self from the containing discoitems object ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2747, "code": "def fetch ( self ) : from . . iq import Iq jid , node = self . address iq = Iq ( to jid = jid , stanza type = \"get\" ) disco = self . disco class ( node ) iq . add content ( disco . xmlnode ) self . stream . set response handlers ( iq , self . response , self . error , self . timeout ) self . stream . send ( iq )", "predictions": ["apply a fetch to the job stream"], "references": ["initialize the service discovery process ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2748, "code": "def handle tls connected event ( self , event ) : if self . settings [ \"tls verify peer\" ] : valid = self . settings [ \"tls verify callback\" ] ( event . stream , event . peer certificate ) if not valid : raise SSL Error ( \"Certificate verification failed\" ) event . stream . tls established = True with event . stream . lock : event . stream . restart stream ( )", "predictions": ["unassigned connected connected events handler"], "references": ["verify the peer certificate on the tlsconnectedevent ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 2749, "code": "def main ( ) : parser = argparse . Argument Parser ( description = 'XMPP version checker' , parents = [ XMPP Settings . get arg parser ( ) ] ) parser . add argument ( 'source' , metavar = 'SOURCE' , help = 'Source JID' ) parser . add argument ( 'target' , metavar = 'TARGET' , nargs = '?' , help = 'Target JID (default: domain of SOURCE)' ) parser . add argument ( '--debug' , action = 'store const' , dest = 'log level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add argument ( '--quiet' , const = logging . ERROR , action = 'store const' , dest = 'log level' , help = 'Print only error messages' ) args = parser . parse args ( ) settings = XMPP Settings ( ) settings . load arguments ( args ) if settings . get ( \"password\" ) is None : password = getpass ( \"{0!r} password: \" . format ( args . source ) ) if sys . version info . major < 3 : password = password . decode ( \"utf-8\" ) settings [ \"password\" ] = password if sys . version info . major < 3 : args . source = args . source . decode ( \"utf-8\" ) source = JID ( args . source ) if args . target : if sys . version info . major < 3 : args . target = args . target . decode ( \"utf-8\" ) target = JID ( args . target ) else : target = JID ( source . domain ) logging . basic Config ( level = args . log level ) checker = Version Checker ( source , target , settings ) try : checker . run ( ) except Keyboard Interrupt : checker . disconnect ( )", "predictions": ["check the source and run the program l { tablename }"], "references": ["parse the command - line arguments and run the tool ."], "bleu": 0.22416933501922287, "rouge_l": 0.36363636363636365}
{"id": 2750, "code": "def handle authorized ( self , event ) : request software version ( self . client , self . target jid , self . success , self . failure )", "predictions": ["configure io io event"], "references": ["send the initial presence after log - in ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2751, "code": "def decode error ( self ) : error qname = self . ns prefix + \"error\" for child in self . element : if child . tag == error qname : self . error = Stanza Error Element ( child ) return raise Bad Request Protocol Error ( \"Error element missing in\" \" an error stanza\" )", "predictions": ["prepare an pending pending pending pending pending pending pending pending pending pending pending pending pending pending pending pending ."], "references": ["decode error element of the stanza ."], "bleu": 0.06439931429457924, "rouge_l": 0.08390646492434663}
{"id": 2752, "code": "def leave ( self ) : if self . joined : p = Muc Presence ( to jid = self . room jid , stanza type = \"unavailable\" ) self . manager . stream . send ( p )", "predictions": ["leave the room ."], "references": ["send a leave request for the room ."], "bleu": 0.2601300475114444, "rouge_l": 0.6288659793814433}
{"id": 2753, "code": "def validate string list ( value ) : try : if sys . version info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) value = value . decode ( encoding ) return [ x . strip ( ) for x in value . split ( u\",\" ) ] except ( Attribute Error , Type Error , Unicode Error ) : raise Value Error ( \"Bad string list\" )", "predictions": ["validate a list of strings ."], "references": ["validator for string lists to be used with add_setting ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2754, "code": "def stop ( self ) : with self . lock : for dummy in self . threads : self . queue . put ( None )", "predictions": ["stop all threads ."], "references": ["stop the resolver threads ."], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 2755, "code": "def run ( self , thread n ) : try : logger . debug ( \"{0!r}: entering thread #{1}\" . format ( self , thread n ) ) resolver = self . make resolver ( ) while True : request = self . queue . get ( ) if request is None : break method , args = request logger . debug ( \" calling {0!r}.{1}{2!r}\" . format ( resolver , method , args ) ) getattr ( resolver , method ) ( * args ) self . queue . task done ( ) logger . debug ( \"{0!r}: leaving thread #{1}\" . format ( self , thread n ) ) finally : self . threads . remove ( threading . current Thread ( ) )", "predictions": ["start the thread ."], "references": ["the thread function ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 2756, "code": "def connect ( self , server = None , port = None ) : if self . me . node or self . me . resource : raise Value ( \"Component JID may have only domain defined\" ) if not server : server = self . server if not port : port = self . port if not server or not port : raise Value Error ( \"Server or port not given\" ) Stream . connect ( self , server , port , None , self . me )", "predictions": ["connect to given domain name and port ."], "references": ["same as componentstream . connect but assume self . lock is acquired ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 2757, "code": "def set state ( self , state ) : logger . debug ( \"  set state({0!r})\" . format ( state ) ) self . state = state self . state cond . notify ( )", "predictions": ["set the state of the state of the remote state ."], "references": ["set _state and notify any threads waiting for the change ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 2758, "code": "def connect ( self , addr , port , service ) : self . dst name = addr self . dst port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF UNSPEC , socket . SOCK STREAM , 0 , socket . AI NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise Value Error ( \"No port number given with literal IP address\" ) self . dst service = None self . family = family self . dst addrs = [ ( family , sockaddr ) ] self . set state ( \"connect\" ) elif service is not None : self . dst service = service self . set state ( \"resolve-srv\" ) self . dst name = addr elif port : self . dst nameports = [ ( self . dst name , self . dst port ) ] self . dst service = None self . set state ( \"resolve-hostname\" ) else : raise Value Error ( \"No port number and no SRV service name given\" )", "predictions": ["connect to a port"], "references": ["same as connect but assumes lock acquired ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2759, "code": "def resolve srv ( self ) : resolver = self . settings [ \"dns resolver\" ] self . set state ( \"resolving-srv\" ) self . event ( Resolving SRV Event ( self . dst name , self . dst service ) ) resolver . resolve srv ( self . dst name , self . dst service , \"tcp\" , callback = self . got srv )", "predictions": ["resolve srv to destination"], "references": ["start resolving the srv record ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2760, "code": "def connected ( self ) : self . auth properties [ 'remote-ip' ] = self . dst addr [ 0 ] if self . dst service : self . auth properties [ 'service-domain' ] = self . dst name if self . dst hostname is not None : self . auth properties [ 'service-hostname' ] = self . dst hostname else : self . auth properties [ 'service-hostname' ] = self . dst addr [ 0 ] self . auth properties [ 'security-layer' ] = None self . event ( Connected Event ( self . dst addr ) ) self . set state ( \"connected\" ) self . stream . transport connected ( )", "predictions": ["start the connection ."], "references": ["handle connection success ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 2761, "code": "def send stream tail ( self ) : with self . lock : if not self . socket or self . hup : logger . debug ( u\"Cannot send stream closing tag: already closed\" ) return data = self . serializer . emit tail ( ) try : self . write ( data . encode ( \"utf-8\" ) ) except ( IO Error , System Error , socket . error ) , err : logger . debug ( u\"Sending stream closing tag failed: {0}\" . format ( err ) ) self . serializer = None self . hup = True if self . tls state is None : try : self . socket . shutdown ( socket . SHUT WR ) except socket . error : pass self . set state ( \"closing\" ) self . write queue . clear ( ) self . write queue cond . notify ( )", "predictions": ["send a stream to the stream ."], "references": ["send stream tail via the transport ."], "bleu": 0.23356898886410002, "rouge_l": 0.5714285714285714}
{"id": 2762, "code": "def send element ( self , element ) : with self . lock : if self . eof or self . socket is None or not self . serializer : logger . debug ( \"Dropping element: {0}\" . format ( element to unicode ( element ) ) ) return data = self . serializer . emit stanza ( element ) self . write ( data . encode ( \"utf-8\" ) )", "predictions": ["send an element to the client ."], "references": ["send an element via the transport ."], "bleu": 0.38260294162784475, "rouge_l": 0.7142857142857143}
{"id": 2763, "code": "def initiate starttls ( self , * * kwargs ) : if self . tls state == \"connected\" : raise Runtime Error ( \"Already TLS-connected\" ) kwargs [ \"do handshake on connect\" ] = False logger . debug ( \"Wrapping the socket into ssl\" ) self . socket = ssl . wrap socket ( self . socket , * * kwargs ) self . set state ( \"tls-handshake\" ) self . continue tls handshake ( )", "predictions": ["sends a socket to the socket ."], "references": ["initiate starttls handshake over the socket ."], "bleu": 0.345720784641941, "rouge_l": 0.42857142857142855}
{"id": 2764, "code": "def continue tls handshake ( self ) : try : logger . debug ( \" do handshake()\" ) self . socket . do handshake ( ) except ssl . SSL Error , err : if err . args [ 0 ] == ssl . SSL ERROR WANT READ : self . tls state = \"want read\" logger . debug ( \"   want read\" ) self . state cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL ERROR WANT WRITE : self . tls state = \"want write\" logger . debug ( \"   want write\" ) self . write queue . appendleft ( TLS Handshake ) return else : raise self . tls state = \"connected\" self . set state ( \"connected\" ) self . auth properties [ 'security-layer' ] = \"TLS\" if \"tls-unique\" in CHANNEL BINDING TYPES : try : tls unique = self . socket . get channel binding ( \"tls-unique\" ) except Value Error : pass else : self . auth properties [ 'channel-binding' ] = { \"tls-unique\" : tls unique } try : cipher = self . socket . cipher ( ) except Attribute Error : cipher = \"unknown\" cert = get certificate from ssl socket ( self . socket ) self . event ( TLS Connected Event ( cipher , cert ) )", "predictions": ["send a handshake message to server ."], "references": ["continue a tls handshake ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 2765, "code": "def handle read ( self ) : with self . lock : logger . debug ( \"handle read()\" ) if self . eof or self . socket is None : return if self . state == \"tls-handshake\" : while True : logger . debug ( \"tls handshake read...\" ) self . continue tls handshake ( ) logger . debug ( \"  state: {0}\" . format ( self . tls state ) ) if self . tls state != \"want read\" : break elif self . tls state == \"connected\" : while self . socket and not self . eof : logger . debug ( \"tls socket read...\" ) try : data = self . socket . read ( 4096 ) except ssl . SSL Error , err : if err . args [ 0 ] == ssl . SSL ERROR WANT READ : break elif err . args [ 0 ] == ssl . SSL ERROR WANT WRITE : break else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( \"Connection reset by peer\" ) data = None else : raise self . feed reader ( data ) else : while self . socket and not self . eof : logger . debug ( \"raw socket read...\" ) try : data = self . socket . recv ( 4096 ) except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( \"Connection reset by peer\" ) data = None else : raise self . feed reader ( data )", "predictions": ["handle a reader connection from the socket ."], "references": ["handle the channel readable state . e . g . read from a socket ."], "bleu": 0.10873738650284012, "rouge_l": 0.32972972972972975}
{"id": 2766, "code": "def handle err ( self ) : with self . lock : if self . state == 'connecting' and self . dst addrs : self . hup = False self . set state ( \"connect\" ) return self . socket . close ( ) self . socket = None self . set state ( \"aborted\" ) self . write queue . clear ( ) self . write queue cond . notify ( ) raise Py XMPPIO Error ( \"Unhandled error on socket\" )", "predictions": ["handles a server if there is one ."], "references": ["handle an error reported ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2767, "code": "def disconnect ( self ) : logger . debug ( \"TCP Transport.disconnect()\" ) with self . lock : if self . socket is None : if self . state != \"closed\" : self . event ( Disconnected Event ( self . dst addr ) ) self . set state ( \"closed\" ) return if self . hup or not self . serializer : self . close ( ) else : self . send stream tail ( )", "predictions": ["disconnect from server ."], "references": ["disconnect the stream gracefully ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 2768, "code": "def close ( self ) : if self . state != \"closed\" : self . event ( Disconnected Event ( self . dst addr ) ) self . set state ( \"closed\" ) if self . socket is None : return try : self . socket . shutdown ( socket . SHUT RDWR ) except socket . error : pass self . socket . close ( ) self . socket = None self . write queue . clear ( ) self . write queue cond . notify ( )", "predictions": ["closes the socket ."], "references": ["same as _close but expects lock acquired ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2769, "code": "def event ( self , event ) : logger . debug ( u\"TCP transport event: {0}\" . format ( event ) ) if self . stream : event . stream = self . stream self . event queue . put ( event )", "predictions": ["called when a transport is received"], "references": ["pass an event to the target stream or just log it ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 2770, "code": "def run ( self ) : timeout = self . method . pyxmpp timeout recurring = self . method . pyxmpp recurring while not self . quit and timeout is not None : if timeout : time . sleep ( timeout ) if self . quit : break ret = self . method ( ) if recurring is None : timeout = ret elif not recurring : break", "predictions": ["block for events to complete ."], "references": ["the thread function ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2771, "code": "def run io threads ( self , handler ) : reader = Reading Thread ( self . settings , handler , daemon = self . daemon , exc queue = self . exc queue ) writter = Writting Thread ( self . settings , handler , daemon = self . daemon , exc queue = self . exc queue ) self . io threads += [ reader , writter ] reader . start ( ) writter . start ( )", "predictions": ["run the io threads"], "references": ["start threads for an iohandler ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2772, "code": "def remove io handler ( self , handler ) : if handler not in self . io handlers : return self . io handlers . remove ( handler ) for thread in self . io threads : if thread . io handler is handler : thread . stop ( )", "predictions": ["remove the given handler ."], "references": ["remove an iohandler from the pool ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 2773, "code": "def add timeout handler ( self , handler ) : self . timeout handlers . append ( handler ) if self . event thread is None : return self . run timeout threads ( handler )", "predictions": ["add a timeout handler to the timeout ."], "references": ["add a timeouthandler to the pool ."], "bleu": 0.2777619034011791, "rouge_l": 0.6747787610619468}
{"id": 2774, "code": "def run timeout threads ( self , handler ) : for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , \" pyxmpp timeout\" ) : continue thread = Timeout Thread ( method , daemon = self . daemon , exc queue = self . exc queue ) self . timeout threads . append ( thread ) thread . start ( )", "predictions": ["run timeout in a thread"], "references": ["start threads for a timeouthandler ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2775, "code": "def remove timeout handler ( self , handler ) : if handler not in self . timeout handlers : return self . timeout handlers . remove ( handler ) for thread in self . timeout threads : if thread . method . im self is handler : thread . stop ( )", "predictions": ["remove a timeout handler from the timeout ."], "references": ["remove a timeouthandler from the pool ."], "bleu": 0.2777619034011791, "rouge_l": 0.6747787610619468}
{"id": 2776, "code": "def start ( self , daemon = False ) : self . daemon = daemon self . io threads = [ ] self . event thread = Event Dispatcher Thread ( self . event dispatcher , daemon = daemon , exc queue = self . exc queue ) self . event thread . start ( ) for handler in self . io handlers : self . run io threads ( handler ) for handler in self . timeout handlers : self . run timeout threads ( handler )", "predictions": ["start the server ."], "references": ["start the threads ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 2777, "code": "def auth finish ( self , unused ) : self . lock . acquire ( ) try : self . logger . debug ( \"Authenticated\" ) self . authenticated = True self . state change ( \"authorized\" , self . my jid ) self . post auth ( ) finally : self . lock . release ( )", "predictions": ["finish up the auth connection ."], "references": ["handle success of the legacy authentication ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 2778, "code": "def configure io handler ( self , handler ) : if self . check events ( ) : return if handler in self . unprepared handlers : old fileno = self . unprepared handlers [ handler ] prepared = self . prepare io handler ( handler ) else : old fileno = None prepared = True fileno = handler . fileno ( ) if old fileno is not None and fileno != old fileno : del self . handlers [ old fileno ] self . io loop . remove handler ( old fileno ) if not prepared : self . unprepared handlers [ handler ] = fileno if not fileno : return update = fileno in self . handlers events = ioloop . IO Loop . NONE if handler . is readable ( ) : logger . debug ( \" {0!r} readable\" . format ( handler ) ) events |= ioloop . IO Loop . READ if handler . is writable ( ) : logger . debug ( \" {0!r} writable\" . format ( handler ) ) events |= ioloop . IO Loop . WRITE if self . handlers . get ( fileno , None ) == events : return self . handlers [ fileno ] = events if events : logger . debug ( \" registering {0!r} handler fileno {1} for\" \" events {2}\" . format ( handler , fileno , events ) ) if update : self . io loop . update handler ( fileno , events ) else : self . io loop . add handler ( fileno , partial ( self . handle event , handler ) , events )", "predictions": ["register handler for the application ."], "references": ["register an io - handler at the polling object ."], "bleu": 0.14260771622124252, "rouge_l": 0.47843137254901963}
{"id": 2779, "code": "def send stream start ( self , stream id = None , stream to = None ) : if self . output state in ( \"open\" , \"closed\" ) : raise Stream Error ( \"Stream start already sent\" ) if not self . language : self . language = self . settings [ \"language\" ] if stream to : stream to = unicode ( stream to ) elif self . peer and self . initiator : stream to = unicode ( self . peer ) stream from = None if self . me and ( self . tls established or not self . initiator ) : stream from = unicode ( self . me ) if stream id : self . stream id = stream id else : self . stream id = None self . transport . send stream head ( self . stanza namespace , stream from , stream to , self . stream id , language = self . language ) self . output state = \"open\"", "predictions": ["send a stream to the stream ."], "references": ["send stream start tag ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 2780, "code": "def send stream error ( self , condition ) : if self . output state is \"closed\" : return if self . output state in ( None , \"restart\" ) : self . send stream start ( ) element = Stream Error Element ( condition ) . as xml ( ) self . transport . send element ( element ) self . transport . disconnect ( ) self . output state = \"closed\"", "predictions": ["send an error message ."], "references": ["same as send_stream_error but expects lock acquired ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2781, "code": "def restart stream ( self ) : self . input state = \"restart\" self . output state = \"restart\" self . features = None self . transport . restart ( ) if self . initiator : self . send stream start ( self . stream id )", "predictions": ["restart the stream ."], "references": ["restart the stream as needed after sasl and starttls negotiation ."], "bleu": 0.12287673380733738, "rouge_l": 0.49193548387096775}
{"id": 2782, "code": "def send ( self , stanza ) : self . fix out stanza ( stanza ) element = stanza . as xml ( ) self . write element ( element )", "predictions": ["send an xml stanza to the client ."], "references": ["same as send but assume lock is acquired ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 2783, "code": "def uplink receive ( self , stanza ) : with self . lock : if self . stanza route : self . stanza route . uplink receive ( stanza ) else : logger . debug ( u\"Stanza dropped (no route): {0!r}\" . format ( stanza ) )", "predictions": ["receive a dropped stanza stanza if it s dropped ."], "references": ["handle stanza received from the stream ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2784, "code": "def main ( ) : parser = argparse . Argument Parser ( description = 'XMPP echo bot' , parents = [ XMPP Settings . get arg parser ( ) ] ) parser . add argument ( 'jid' , metavar = 'JID' , help = 'The bot JID' ) parser . add argument ( '--debug' , action = 'store const' , dest = 'log level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add argument ( '--quiet' , const = logging . ERROR , action = 'store const' , dest = 'log level' , help = 'Print only error messages' ) parser . add argument ( '--trace' , action = 'store true' , help = 'Print XML data sent and received' ) args = parser . parse args ( ) settings = XMPP Settings ( { \"software name\" : \"Echo Bot\" } ) settings . load arguments ( args ) if settings . get ( \"password\" ) is None : password = getpass ( \"{0!r} password: \" . format ( args . jid ) ) if sys . version info . major < 3 : password = password . decode ( \"utf-8\" ) settings [ \"password\" ] = password if sys . version info . major < 3 : args . jid = args . jid . decode ( \"utf-8\" ) logging . basic Config ( level = args . log level ) if args . trace : print \"enabling trace\" handler = logging . Stream Handler ( ) handler . set Level ( logging . DEBUG ) for logger in ( \"pyxmpp2.IN\" , \"pyxmpp2.OUT\" ) : logger = logging . get Logger ( logger ) logger . set Level ( logging . DEBUG ) logger . add Handler ( handler ) logger . propagate = False bot = Echo Bot ( JID ( args . jid ) , settings ) try : bot . run ( ) except Keyboard Interrupt : bot . disconnect ( )", "predictions": ["run the core joined server"], "references": ["parse the command - line arguments and run the bot ."], "bleu": 0.10822031883953476, "rouge_l": 0.2341650671785029}
{"id": 2785, "code": "def handle read ( self ) : with self . lock : logger . debug ( \"handle read()\" ) if self . socket is None : return while True : try : sock , address = self . socket . accept ( ) except socket . error , err : if err . args [ 0 ] in BLOCKING ERRORS : break else : raise logger . debug ( \"Accepted connection from: {0!r}\" . format ( address ) ) self . target ( sock , address )", "predictions": ["x - string gets all the data from server try to the socket try to the socket try ."], "references": ["accept any incoming connections ."], "bleu": 0.06439931429457924, "rouge_l": 0.09312977099236641}
{"id": 2786, "code": "def decode subelements ( self ) : for child in self . element : if child . tag == self . show tag : self . show = child . text elif child . tag == self . status tag : self . status = child . text elif child . tag == self . priority tag : try : self . priority = int ( child . text . strip ( ) ) if self . priority < - 128 or self . priority > 127 : raise Value Error except Value Error : raise Bad Request Protocol Error ( \"Presence priority not an integer\" )", "predictions": ["stop the subelements and stop the subelements ."], "references": ["decode the stanza subelements ."], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 2787, "code": "def activate ( self ) : obj = self . find paypal object ( ) if obj . state == enums . Billing Plan State . CREATED : success = obj . activate ( ) if not success : raise Paypal Api Error ( \"Failed to activate plan: %r\" % ( obj . error ) ) self . get or update from api data ( obj , always sync = True ) return obj", "predictions": ["mark the logger as enums . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["activate an plan in a created state ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 2788, "code": "def check paypal api key ( app configs = None , * * kwargs ) : messages = [ ] mode = getattr ( djpaypal settings , \"PAYPAL MODE\" , None ) if mode not in VALID MODES : msg = \"Invalid PAYPAL MODE specified: {}.\" . format ( repr ( mode ) ) hint = \"PAYPAL MODE must be one of {}\" . format ( \", \" . join ( repr ( k ) for k in VALID MODES ) ) messages . append ( checks . Critical ( msg , hint = hint , id = \"djpaypal.C001\" ) ) for setting in \"PAYPAL CLIENT ID\" , \"PAYPAL CLIENT SECRET\" : if not getattr ( djpaypal settings , setting , None ) : msg = \"Invalid value specified for {}\" . format ( setting ) hint = \"Add PAYPAL CLIENT ID and PAYPAL CLIENT SECRET to your settings.\" messages . append ( checks . Critical ( msg , hint = hint , id = \"djpaypal.C002\" ) ) return messages", "predictions": ["connect to paypal self . ."], "references": ["check that the paypal api keys are configured correctly"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2789, "code": "async def create upstream applications ( self ) : loop = asyncio . get event loop ( ) for steam name , Applications Cls in self . applications . items ( ) : application = Applications Cls ( self . scope ) upstream queue = asyncio . Queue ( ) self . application streams [ steam name ] = upstream queue self . application futures [ steam name ] = loop . create task ( application ( upstream queue . get , partial ( self . dispatch downstream , steam name = steam name ) ) )", "predictions": ["state asyncio self ."], "references": ["create the upstream applications ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 2790, "code": "async def receive json ( self , content , * * kwargs ) : if isinstance ( content , dict ) and \"stream\" in content and \"payload\" in content : steam name = content [ \"stream\" ] payload = content [ \"payload\" ] if steam name not in self . applications accepting frames : raise Value Error ( \"Invalid multiplexed frame received (stream not mapped)\" ) await self . send upstream ( message = { \"type\" : \"websocket.receive\" , \"text\" : await self . encode json ( payload ) } , stream name = steam name ) return else : raise Value Error ( \"Invalid multiplexed **frame received (no channel/payload key)\" )", "predictions": ["process self . . ."], "references": ["rout the message down the correct stream ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2791, "code": "async def disconnect ( self , code ) : try : await asyncio . wait ( self . application futures . values ( ) , return when = asyncio . ALL COMPLETED , timeout = self . application close timeout ) except asyncio . Timeout Error : pass", "predictions": ["disconnect asyncio from the application instance . . . . . ."], "references": ["default is to wait for the child applications to close ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 2792, "code": "async def websocket send ( self , message , stream name ) : text = message . get ( \"text\" ) json = await self . decode json ( text ) data = { \"stream\" : stream name , \"payload\" : json } await self . send json ( data )", "predictions": ["self addr with given auth"], "references": ["capture downstream websocket . send messages from the upstream applications ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 2793, "code": "def main ( ) : from optparse import Option Parser , Option Group parser = Option Parser ( usage = \"%prog [options] <folder path> ...\" , version = \"%s v%s\" % ( appname , version ) ) parser . add option ( '-D' , '--defaults' , action = \"store true\" , dest = \"defaults\" , default = False , help = \"Display the default values for options which take\" \" arguments and then exit.\" ) parser . add option ( '-E' , '--exact' , action = \"store true\" , dest = \"exact\" , default = False , help = \"There is a vanishingly small chance of false\" \" positives when comparing files using sizes and hashes. This option\" \" enables exact comparison. However, exact comparison requires a lot\" \" of disk seeks, so, on traditional moving-platter media, this trades\" \" a LOT of performance for a very tiny amount of safety most people\" \" don't need.\" ) filter group = Option Group ( parser , \"Input Filtering\" ) filter group . add option ( '-e' , '--exclude' , action = \"append\" , dest = \"exclude\" , metavar = \"PAT\" , help = \"Specify a globbing pattern to be\" \" added to the internal blacklist. This option can be used multiple\" \" times. Provide a dash (-) as your first exclude to override the\" \" pre-programmed defaults.\" ) filter group . add option ( '--min-size' , action = \"store\" , type = \"int\" , dest = \"min size\" , metavar = \"X\" , help = \"Specify a non-default minimum size\" \". Files below this size (default: %default bytes) will be ignored.\" ) parser . add option group ( filter group ) behaviour group = Option Group ( parser , \"Output Behaviour\" ) behaviour group . add option ( '-d' , '--delete' , action = \"store true\" , dest = \"delete\" , help = \"Prompt the user for files to preserve and delete \" \"all others.\" ) behaviour group . add option ( '-n' , '--dry-run' , action = \"store true\" , dest = \"dry run\" , metavar = \"PREFIX\" , help = \"Don't actually delete any \" \"files. Just list what actions would be performed. (Good for testing \" \"values for --prefer)\" ) behaviour group . add option ( '--prefer' , action = \"append\" , dest = \"prefer\" , metavar = \"PATH\" , default = [ ] , help = \"Append a globbing pattern which \" \"--delete should automatically prefer (rather than prompting) when it \" \"occurs in a list of duplicates.\" ) behaviour group . add option ( '--noninteractive' , action = \"store true\" , dest = \"noninteractive\" , help = \"When using --delete, automatically assume\" \" 'all' for any groups with no --prefer matches rather than prompting\" ) parser . add option group ( behaviour group ) parser . set defaults ( * * DEFAULTS ) opts , args = parser . parse args ( ) if '-' in opts . exclude : opts . exclude = opts . exclude [ opts . exclude . index ( '-' ) + 1 : ] opts . exclude = [ x . rstrip ( os . sep + ( os . altsep or '' ) ) for x in opts . exclude ] if opts . defaults : print defaults ( ) sys . exit ( ) groups = find dupes ( args , opts . exact , opts . exclude , opts . min size ) if opts . delete : delete dupes ( groups , opts . prefer , not opts . noninteractive , opts . dry run ) else : for dupe Set in groups . values ( ) : print '\\n' . join ( dupe Set ) + '\\n'", "predictions": ["parse arguments and call the program = rest api"], "references": ["the main entry point compatible with setuptools ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2794, "code": "def get summarizer ( self , name ) : if name in self . summarizers : pass elif name == 'lexrank' : from . import lexrank self . summarizers [ name ] = lexrank . summarize elif name == 'mcp' : from . import mcp summ self . summarizers [ name ] = mcp summ . summarize return self . summarizers [ name ]", "predictions": ["send a element to a element"], "references": ["import summarizers on - demand"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2795, "code": "def code mapping ( level , msg , default = 99 ) : try : return code mappings by level [ level ] [ msg ] except Key Error : pass if msg . count ( '\"' ) == 2 and ' \"' in msg and msg . endswith ( '\".' ) : txt = msg [ : msg . index ( ' \"' ) ] return code mappings by level [ level ] . get ( txt , default ) return default", "predictions": ["get the initiate initiate starttls message handshake"], "references": ["return an error code between 0 and 99 ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 2796, "code": "def dequote docstring ( text ) : text = text . strip ( ) if len ( text ) > 6 and text [ : 3 ] == text [ - 3 : ] == '\"\"\"' : return text [ 3 : - 3 ] if len ( text ) > 7 and text [ : 4 ] in ( 'u\"\"\"' , 'r\"\"\"' ) and text [ - 3 : ] == '\"\"\"' : return text [ 4 : - 3 ] if len ( text ) > 6 and text [ : 3 ] == text [ - 3 : ] == \"'''\" : return text [ 3 : - 3 ] if len ( text ) > 7 and text [ : 4 ] in ( \"u'''\" , \"r'''\" ) and text [ - 3 : ] == \"'''\" : return text [ 4 : - 3 ] if len ( text ) > 2 and text [ 0 ] == text [ - 1 ] == '\"' : return text [ 1 : - 1 ] if len ( text ) > 3 and text [ : 2 ] in ( 'u\"' , 'r\"' ) and text [ - 1 ] == '\"' : return text [ 2 : - 1 ] if len ( text ) > 2 and text [ 0 ] == text [ - 1 ] == \"'\" : return text [ 1 : - 1 ] if len ( text ) > 3 and text [ : 2 ] in ( \"u'\" , \"r'\" ) and text [ - 1 ] == \"'\" : return text [ 2 : - 1 ] raise Value Error ( \"Bad quotes!\" )", "predictions": ["err tls tls on continue"], "references": ["remove the quotes delimiting a docstring ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 2797, "code": "def is public ( self ) : if self . all is not None : return self . name in self . all else : return not self . name . startswith ( \" \" )", "predictions": ["return is a read - only way for a given sequence of variables ."], "references": ["return true iff this function should be considered public ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 2798, "code": "def is public ( self ) : for decorator in self . decorators : if re . compile ( r\"^{}\\.\" . format ( self . name ) ) . match ( decorator . name ) : return False name is public = ( not self . name . startswith ( \" \" ) or self . name in VARIADIC MAGIC METHODS or self . is magic ) return self . parent . is public and name is public", "predictions": ["return handle to check if a err is err"], "references": ["return true iff this method should be considered public ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 2799, "code": "def is public ( self ) : return ( not self . name . startswith ( \" \" ) and self . parent . is class and self . parent . is public )", "predictions": [". . . name . . ."], "references": ["return true iff this class should be considered public ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2800, "code": "def parse ( self , filelike , filename ) : self . log = log self . source = filelike . readlines ( ) src = \"\" . join ( self . source ) compile ( src , filename , \"exec\" ) self . stream = Token Stream ( String IO ( src ) ) self . filename = filename self . all = None self . future imports = set ( ) self . accumulated decorators = [ ] return self . parse module ( )", "predictions": ["close a file as a module"], "references": ["parse the given file - like object and return its module object ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 2801, "code": "def consume ( self , kind ) : next token = self . stream . move ( ) assert next token . kind == kind", "predictions": ["event handling a debug debug transport transport transport transport transport transport transport transport transport transport transport"], "references": ["consume one token and verify it is of the expected kind ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2802, "code": "def parse docstring ( self ) : self . log . debug ( \"parsing docstring, token is %r (%s)\" , self . current . kind , self . current . value ) while self . current . kind in ( tk . COMMENT , tk . NEWLINE , tk . NL ) : self . stream . move ( ) self . log . debug ( \"parsing docstring, token is %r (%s)\" , self . current . kind , self . current . value , ) if self . current . kind == tk . STRING : docstring = self . current . value self . stream . move ( ) return docstring return None", "predictions": ["run a docstring while we can find a docstring"], "references": ["parse a single docstring and return its value ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 2803, "code": "def parse definitions ( self , class , all = False ) : while self . current is not None : self . log . debug ( \"parsing definition list, current token is %r (%s)\" , self . current . kind , self . current . value , ) self . log . debug ( \"got newline: %s\" , self . stream . got logical newline ) if all and self . current . value == \" all \" : self . parse all ( ) elif ( self . current . kind == tk . OP and self . current . value == \"@\" and self . stream . got logical newline ) : self . consume ( tk . OP ) self . parse decorators ( ) elif self . current . value in [ \"def\" , \"class\" ] : yield self . parse definition ( class . nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse definitions ( class ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == \"from\" : self . parse from import statement ( ) else : self . stream . move ( )", "predictions": ["run io io token"], "references": ["parse multiple definitions and yield them ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 2804, "code": "def parse all ( self ) : assert self . current . value == \" all \" self . consume ( tk . NAME ) if self . current . value != \"=\" : raise All Error ( \"Could not evaluate contents of  all . \" ) self . consume ( tk . OP ) if self . current . value not in \"([\" : raise All Error ( \"Could not evaluate contents of  all . \" ) self . consume ( tk . OP ) self . all = [ ] all content = \"(\" while self . current . kind != tk . OP or self . current . value not in \")]\" : if self . current . kind in ( tk . NL , tk . COMMENT ) : pass elif self . current . kind == tk . STRING or self . current . value == \",\" : all content += self . current . value else : raise All Error ( \"Unexpected token kind in   all : {!r}. \" . format ( self . current . kind ) ) self . stream . move ( ) self . consume ( tk . OP ) all content += \")\" try : self . all = eval ( all content , { } ) except Base Exception as e : raise All Error ( \"Could not evaluate contents of  all .\" \"\\b The value was {}. The exception was:\\n{}\" . format ( all content , e ) )", "predictions": ["remove io contents and extract contents contents"], "references": ["parse the __all__ definition in a module ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 2805, "code": "def check current ( self , kind = None , value = None ) : msg = textwrap . dedent ( . format ( self = self ) ) kind valid = self . current . kind == kind if kind else True value valid = self . current . value == value if value else True assert kind valid and value valid , msg", "predictions": ["add timeout and ."], "references": ["verify the current token is of type kind and equals value ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 2806, "code": "def parse from import names ( self , is future import ) : if self . current . value == \"(\" : self . consume ( tk . OP ) expected end kinds = ( tk . OP , ) else : expected end kinds = ( tk . NEWLINE , tk . ENDMARKER ) while self . current . kind not in expected end kinds and not ( self . current . kind == tk . OP and self . current . value == \";\" ) : if self . current . kind != tk . NAME : self . stream . move ( ) continue self . log . debug ( \"parsing import, token is %r (%s)\" , self . current . kind , self . current . value , ) if is future import : self . log . debug ( \"found future import: %s\" , self . current . value ) self . future imports . add ( self . current . value ) self . consume ( tk . NAME ) self . log . debug ( \"parsing import, token is %r (%s)\" , self . current . kind , self . current . value , ) if self . current . kind == tk . NAME and self . current . value == \"as\" : self . consume ( tk . NAME ) if self . current . kind == tk . NAME : self . consume ( tk . NAME ) if self . current . value == \",\" : self . consume ( tk . OP ) self . log . debug ( \"parsing import, token is %r (%s)\" , self . current . kind , self . current . value , )", "predictions": ["run timeout timeout names"], "references": ["parse the y part in a from x import y statement ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 2807, "code": "def run ( self ) : if self . err is not None : assert self . source is None msg = \"%s%03i %s\" % ( rst prefix , rst fail load , \"Failed to load file: %s\" % self . err , ) yield 0 , 0 , msg , type ( self ) module = [ ] try : module = parse ( String IO ( self . source ) , self . filename ) except Syntax Error as err : msg = \"%s%03i %s\" % ( rst prefix , rst fail parse , \"Failed to parse file: %s\" % err , ) yield 0 , 0 , msg , type ( self ) module = [ ] except All Error : msg = \"%s%03i %s\" % ( rst prefix , rst fail all , \"Failed to parse  all  entry.\" , ) yield 0 , 0 , msg , type ( self ) module = [ ] for definition in module : if not definition . docstring : continue try : unindented = trim ( dequote docstring ( definition . docstring ) ) rst errors = list ( rst lint . lint ( unindented ) ) except Exception as err : msg = \"%s%03i %s\" % ( rst prefix , rst fail lint , \"Failed to lint docstring: %s - %s\" % ( definition . name , err ) , ) yield definition . start , 0 , msg , type ( self ) continue for rst error in rst errors : if rst error . level <= 1 : continue # # msg = rst error . message . split ( \"\\n\" , 1 ) [ 0 ] code = code mapping ( rst error . level , msg ) assert code < 100 , code code += 100 * rst error . level msg = \"%s%03i %s\" % ( rst prefix , code , msg ) yield definition . start + rst error . line , 0 , msg , type ( self )", "predictions": ["run all as lint file . as python source file . . . . ."], "references": ["use docutils to check docstrings are valid rst ."], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 2808, "code": "def load source ( self ) : if self . filename in self . STDIN NAMES : self . filename = \"stdin\" if sys . version info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = Text IO Wrapper ( sys . stdin . buffer , errors = \"ignore\" ) . read ( ) else : handle = tokenize open ( self . filename ) self . source = handle . read ( ) handle . close ( )", "predictions": ["start the source ."], "references": ["load the source for the specified file ."], "bleu": 0.18693159143202892, "rouge_l": 0.47164948453608246}
{"id": 2809, "code": "def listener ( self , sock , * args ) : conn , addr = sock . accept ( ) f = conn . makefile ( conn ) self . shell = Shoebot Cmd ( self . bot , stdin = f , stdout = f , intro = INTRO ) print ( ( \"Connected\" ) ) G Object . io add watch ( conn , G Object . IO IN , self . handler ) if self . shell . intro : self . shell . stdout . write ( str ( self . shell . intro ) + \"\\n\" ) self . shell . stdout . flush ( ) return True", "predictions": ["send a auth to the client"], "references": ["asynchronous connection listener . starts a handler for each connection ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 2810, "code": "def handler ( self , conn , * args ) : self . shell . stdout . write ( self . shell . prompt ) line = self . shell . stdin . readline ( ) if not len ( line ) : line = 'EOF' return False else : line = line . rstrip ( '\\r\\n' ) line = self . shell . precmd ( line ) stop = self . shell . onecmd ( line ) stop = self . shell . postcmd ( stop , line ) self . shell . stdout . flush ( ) self . shell . postloop ( ) if stop : self . shell = None conn . close ( ) return not stop", "predictions": ["closes connection to server ."], "references": ["asynchronous connection handler . processes each line from the socket ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 2811, "code": "def do escape nl ( self , arg ) : if arg . lower ( ) == 'off' : self . escape nl = False else : self . escape nl = True", "predictions": ["stream start with the specified id ."], "references": ["escape newlines in any responses"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2812, "code": "def do restart ( self , line ) : self . bot . frame = 0 self . bot . namespace . clear ( ) self . bot . namespace . update ( self . bot . initial namespace )", "predictions": ["stream the last displayed line"], "references": ["attempt to restart the bot ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2813, "code": "def do play ( self , line ) : if self . pause speed is None : self . bot . speed = self . pause speed self . pause speed = None self . print response ( \"Play\" )", "predictions": ["stream of a given line"], "references": ["resume playback if bot is paused"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2814, "code": "def do vars ( self , line ) : if self . bot . vars : max name len = max ( [ len ( name ) for name in self . bot . vars ] ) for i , ( name , v ) in enumerate ( self . bot . vars . items ( ) ) : keep = i < len ( self . bot . vars ) - 1 self . print response ( \"%s = %s\" % ( name . ljust ( max name len ) , v . value ) , keep = keep ) else : self . print response ( \"No vars\" )", "predictions": ["print - . a line"], "references": ["list bot variables and values"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2815, "code": "def do exit ( self , line ) : if self . trusted : publish event ( QUIT EVENT ) self . print response ( 'Bye.\\n' ) return True", "predictions": ["receive a single stanza stanza"], "references": ["exit shell and shoebot"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2816, "code": "def do fullscreen ( self , line ) : self . bot . canvas . sink . trigger fullscreen action ( True ) print ( self . response prompt , file = self . stdout )", "predictions": ["fullscreen a line ."], "references": ["make the current window fullscreen"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 2817, "code": "def do windowed ( self , line ) : self . bot . canvas . sink . trigger fullscreen action ( False ) print ( self . response prompt , file = self . stdout )", "predictions": ["windowed a line ."], "references": ["un - fullscreen the current window"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2818, "code": "def do help ( self , arg ) : print ( self . response prompt , file = self . stdout ) return cmd . Cmd . do help ( self , arg )", "predictions": ["run help on a system"], "references": ["show help on all commands ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 2819, "code": "def do set ( self , line ) : try : name , value = [ part . strip ( ) for part in line . split ( '=' ) ] if name not in self . bot . vars : self . print response ( 'No such variable %s enter vars to see available vars' % name ) return variable = self . bot . vars [ name ] variable . value = variable . sanitize ( value . strip ( ';' ) ) success , msg = self . bot . canvas . sink . var changed ( name , variable . value ) if success : print ( '{}={}' . format ( name , variable . value ) , file = self . stdout ) else : print ( '{}\\n' . format ( msg ) , file = self . stdout ) except Exception as e : print ( 'Invalid Syntax.' , e ) return", "predictions": ["set a variable"], "references": ["set a variable ."], "bleu": 0.7165313105737893, "rouge_l": 0.8356164383561644}
{"id": 2820, "code": "def drawdaisy ( x , y , color = '#fefefe' ) : ctx . push ( ) fill = ctx . fill ( ) stroke = ctx . stroke ( ) sc = ( 1.0 / ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 ctx . strokewidth ( sc * 2.0 ) ctx . stroke ( '#3B240B' ) ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( ctx . FRAME * 0.1 ) , y ) ctx . translate ( - 20 , 0 ) ctx . scale ( sc ) ctx . fill ( color ) ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : ctx . rotate ( degrees = 45 ) ctx . rect ( x , y , 40 , 8 , 1 ) ctx . fill ( '#F7FE2E' ) ctx . ellipse ( x + 15 , y , 10 , 10 ) ctx . fill ( fill ) ctx . stroke ( stroke ) ctx . pop ( )", "predictions": ["draw a line of size ."], "references": ["draw a daisy at x y"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 2821, "code": "def flatten fft ( scale = 1.0 ) : len = len ( audio . spectrogram ) for i , v in enumerate ( audio . spectrogram ) : yield scale * ( i * v ) / len", "predictions": ["flatten the fft by the scale"], "references": ["produces a nicer graph i m not sure if this is correct"], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 2822, "code": "def scaled fft ( fft , scale = 1.0 ) : data = np . zeros ( len ( fft ) ) for i , v in enumerate ( fft ) : data [ i ] = scale * ( i * v ) / NUM SAMPLES return data", "predictions": ["computes the fft of the fft"], "references": ["produces a nicer graph i m not sure if this is correct"], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 2823, "code": "def create view ( self , name = \"shoebot-output\" ) : view = gtk . Text View ( ) view . set editable ( False ) fontdesc = pango . Font Description ( \"Monospace\" ) view . modify font ( fontdesc ) view . set name ( name ) buff = view . get buffer ( ) buff . create tag ( 'error' , foreground = 'red' ) return view", "predictions": ["creates a view with the given name"], "references": ["create the gtk . textview used for shell output"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2824, "code": "def load Grammar ( self , grammar , searchpaths = None ) : self . grammar = self . load ( grammar , searchpaths = searchpaths ) self . refs = { } for ref in self . grammar . get Elements By Tag Name ( \"ref\" ) : self . refs [ ref . attributes [ \"id\" ] . value ] = ref", "predictions": ["loads the grammar and populates the grammar ."], "references": ["load context - free grammar"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2825, "code": "def not found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404Not Found : return True except : return False return False", "predictions": ["returns true if url is found"], "references": ["returns true when the url generates a 404 not found error ."], "bleu": 0.12151662434083678, "rouge_l": 0.41924398625429554}
{"id": 2826, "code": "def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , * * kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , * * kwargs )", "predictions": ["return rendered image at a given image"], "references": ["draws a image form path in x y and resize it to width height dimensions ."], "bleu": 0.056829570481990416, "rouge_l": 0.16245006657789615}
{"id": 2827, "code": "def star ( self , startx , starty , points = 20 , outer = 100 , inner = 50 , draw = True , * * kwargs ) : self . beginpath ( * * kwargs ) self . moveto ( startx , starty + outer ) for i in range ( 1 , int ( 2 * points ) ) : angle = i * pi / points x = sin ( angle ) y = cos ( angle ) if i % 2 : radius = inner else : radius = outer x = startx + radius * x y = starty + radius * y self . lineto ( x , y ) return self . endpath ( draw )", "predictions": ["return a vector with the coordinates of the rectangle ."], "references": ["draws a star ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 2828, "code": "def relmoveto ( self , x , y ) : if self . path is None : raise Shoebot Error ( ( \"No current path. Use beginpath() first.\" ) ) self . path . relmoveto ( x , y )", "predictions": ["sets the current value of x y"], "references": ["move relatively to the last point ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2829, "code": "def rellineto ( self , x , y ) : if self . path is None : raise Shoebot Error ( ( \"No current path. Use beginpath() first.\" ) ) self . path . rellineto ( x , y )", "predictions": ["sets the current value of x y"], "references": ["draw a line using relative coordinates ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2830, "code": "def relcurveto ( self , h1x , h1y , h2x , h2y , x , y ) : if self . path is None : raise Shoebot Error ( ( \"No current path. Use beginpath() first.\" ) ) self . path . relcurveto ( h1x , h1y , h2x , h2y , x , y )", "predictions": ["sets the current directory as the current path"], "references": ["draws a curve relatively to the last point ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2831, "code": "def graph background ( s ) : if s . background == None : s . ctx . background ( None ) else : s . ctx . background ( s . background ) if s . depth : try : clr = colors . color ( s . background ) . darker ( 0.2 ) p = s . ctx . rect ( 0 , 0 , s . ctx . WIDTH , s . ctx . HEIGHT , draw = False ) colors . gradientfill ( p , clr , clr . lighter ( 0.35 ) ) colors . shadow ( dx = 0 , dy = 0 , blur = 2 , alpha = 0.935 , clr = s . background ) except : pass", "predictions": ["background color to background process ."], "references": ["graph background color ."], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 2832, "code": "def node ( s , node , alpha = 1.0 ) : if s . depth : try : colors . shadow ( dx = 5 , dy = 5 , blur = 10 , alpha = 0.5 * alpha ) except : pass s . ctx . nofill ( ) s . ctx . nostroke ( ) if s . fill : s . ctx . fill ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * alpha ) if s . stroke : s . ctx . strokewidth ( s . strokewidth ) s . ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * alpha * 3 ) r = node . r s . ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 )", "predictions": ["prints a node ."], "references": ["visualization of a default node ."], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 2833, "code": "def node label ( s , node , alpha = 1.0 ) : if s . text : #s. ctx.lineheight(1)     s . ctx . font ( s . font ) s . ctx . fontsize ( s . fontsize ) s . ctx . nostroke ( ) s . ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) try : p = node . textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( \"utf-8\" ) except : pass #root = node.graph.root #if txt != root and txt[-len(root):] == root:  dx , dy = 0 , 0 if s . align == 2 : #CENTER dx = - s . ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . ctx . textheight ( txt ) / 2 node . textpath = s . ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . textpath if s . depth : try : colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . ctx . push ( ) s . ctx . translate ( node . x , node . y ) s . ctx . scale ( alpha ) s . ctx . drawpath ( p . copy ( ) ) s . ctx . pop ( )", "predictions": ["prints a label for a node"], "references": ["visualization of a node s id ."], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 2834, "code": "def edge ( s , path , edge , alpha = 1.0 ) : path . moveto ( edge . node1 . x , edge . node1 . y ) if edge . node2 . style == BACK : path . curveto ( edge . node1 . x , edge . node2 . y , edge . node2 . x , edge . node2 . y , edge . node2 . x , edge . node2 . y , ) else : path . lineto ( edge . node2 . x , edge . node2 . y )", "predictions": ["make a edge edge with the given edge ."], "references": ["visualization of a single edge between two nodes ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 2835, "code": "def edge label ( s , edge , alpha = 1.0 ) : if s . text and edge . label != \"\" : s . ctx . nostroke ( ) s . ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha * 0.75 ) s . ctx . lineheight ( 1 ) s . ctx . font ( s . font ) s . ctx . fontsize ( s . fontsize * 0.75 ) try : p = edge . textpath except : try : txt = unicode ( edge . label ) except : try : txt = edge . label . decode ( \"utf-8\" ) except : pass edge . textpath = s . ctx . textpath ( txt , s . ctx . textwidth ( \" \" ) , 0 , width = s . textwidth ) p = edge . textpath a = degrees ( atan2 ( edge . node2 . y - edge . node1 . y , edge . node2 . x - edge . node1 . x ) ) d = sqrt ( ( edge . node2 . x - edge . node1 . x ) ** 2 + ( edge . node2 . y - edge . node1 . y ) ** 2 ) d = abs ( d - s . ctx . textwidth ( edge . label ) ) * 0.5 s . ctx . push ( ) s . ctx . transform ( CORNER ) s . ctx . translate ( edge . node1 . x , edge . node1 . y ) s . ctx . rotate ( - a ) s . ctx . translate ( d , s . fontsize * 1.0 ) s . ctx . scale ( alpha ) if 90 < a % 360 < 270 : s . ctx . translate ( s . ctx . textwidth ( edge . label ) , - s . fontsize * 2.0 ) s . ctx . transform ( CENTER ) s . ctx . rotate ( 180 ) s . ctx . transform ( CORNER ) s . ctx . drawpath ( p . copy ( ) ) s . ctx . pop ( )", "predictions": ["prints a label with a label"], "references": ["visualization of the label accompanying an edge ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2836, "code": "def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . ctx . nofill ( ) s . ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . ctx . strokewidth ( s . strokewidth ) else : s . ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . ctx . beginpath ( n . x , n . y ) end ( n ) else : s . ctx . lineto ( n . x , n . y ) s . ctx . endpath ( ) end ( n )", "predictions": ["prints a path to the graph"], "references": ["visualization of a shortest path between two nodes ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2837, "code": "def copy ( self , graph ) : s = styles ( graph ) s . guide = self . guide . copy ( graph ) dict . init ( s , [ ( v . name , v . copy ( ) ) for v in self . values ( ) ] ) return s", "predictions": ["returns a copy of the graph with a new graph"], "references": ["returns a copy of all styles and a copy of the styleguide ."], "bleu": 0.35550251920851145, "rouge_l": 0.42479108635097496}
{"id": 2838, "code": "def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has key ( s ) and self [ s ] ( self . graph , node ) : node . style = s", "predictions": ["apply the graph to the graph"], "references": ["check the rules for each node in the graph and apply the style ."], "bleu": 0.09635931056770616, "rouge_l": 0.2798165137614679}
{"id": 2839, "code": "def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . init ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g", "predictions": ["returns a copy of this graph with a graph"], "references": ["returns a copy of the styleguide for the given graph ."], "bleu": 0.3292100646487161, "rouge_l": 0.4911433172302737}
{"id": 2840, "code": "def callback ( self , * incoming ) : message = incoming [ 0 ] if message : address , command = message [ 0 ] , message [ 2 ] profile = self . get profile ( address ) if profile is not None : try : getattr ( profile , command ) ( self , message ) except Attribute Error : pass", "predictions": ["callback method for interactive use ."], "references": ["gets called by the callbackmanager if a new message was received"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 2841, "code": "def copytree ( src , dst , symlinks = False , ignore = None ) : if not os . path . exists ( dst ) : os . makedirs ( dst ) shutil . copystat ( src , dst ) lst = os . listdir ( src ) if ignore : excl = ignore ( src , lst ) lst = [ x for x in lst if x not in excl ] for item in lst : s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if symlinks and os . path . islink ( s ) : if os . path . lexists ( d ) : os . remove ( d ) os . symlink ( os . readlink ( s ) , d ) try : st = os . lstat ( s ) mode = stat . S IMODE ( st . st mode ) os . lchmod ( d , mode ) except : pass elif os . path . isdir ( s ) : copytree ( s , d , symlinks , ignore ) else : shutil . copy2 ( s , d )", "predictions": ["delete a file or directory src"], "references": ["copytree that works even if folder already exists"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 2842, "code": "def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE SEARCH return Google Search ( q , start , service , \"\" , wait , asynchronous , cached )", "predictions": ["returns a list of the first n rows of this object"], "references": ["returns a google web query formatted as a googlesearch list object ."], "bleu": 0.15553014371537452, "rouge_l": 0.34512022630834516}
{"id": 2843, "code": "def search images ( q , start = 0 , size = \"\" , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE IMAGES return Google Search ( q , start , service , size , wait , asynchronous , cached )", "predictions": ["search for images in a list of images ."], "references": ["returns a google images query formatted as a googlesearch list object ."], "bleu": 0.12716571564598603, "rouge_l": 0.3713850837138508}
{"id": 2844, "code": "def search news ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE NEWS return Google Search ( q , start , service , \"\" , wait , asynchronous , cached )", "predictions": ["search for news range"], "references": ["returns a google news query formatted as a googlesearch list object ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2845, "code": "def search blogs ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE BLOGS return Google Search ( q , start , service , \"\" , wait , asynchronous , cached )", "predictions": ["returns a list of the first coincidence ..."], "references": ["returns a google blogs query formatted as a googlesearch list object ."], "bleu": 0.13755608571892394, "rouge_l": 0.28955696202531644}
{"id": 2846, "code": "def parse ( self , str ) : str = replace entities ( str ) str = strip tags ( str ) str = collapse spaces ( str ) return str", "predictions": ["parse string of fortran entities"], "references": ["parses the text data from an xml element defined by tag ."], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 2847, "code": "def hash ( self , id ) : h = md5 ( id ) . hexdigest ( ) return os . path . join ( self . path , h + self . type )", "predictions": ["hash the file with the given id"], "references": ["creates a unique filename in the cache for the id ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 2848, "code": "def age ( self , id ) : path = self . hash ( id ) if os . path . exists ( path ) : modified = datetime . datetime . fromtimestamp ( os . stat ( path ) [ 8 ] ) age = datetime . datetime . today ( ) - modified return age . days else : return 0", "predictions": ["get the do not use"], "references": ["returns the age of the cache entry in days ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2849, "code": "def angle ( x0 , y0 , x1 , y1 ) : return degrees ( atan2 ( y1 - y0 , x1 - x0 ) )", "predictions": ["returns the do a point ."], "references": ["returns the angle between two points ."], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 2850, "code": "def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )", "predictions": ["computes the do the points of two points ."], "references": ["returns the distance between two points ."], "bleu": 0.2777619034011791, "rouge_l": 0.5115303983228512}
{"id": 2851, "code": "def invert ( self ) : m = self . matrix d = m [ 0 ] * m [ 4 ] - m [ 1 ] * m [ 3 ] self . matrix = [ m [ 4 ] / d , - m [ 1 ] / d , 0 , - m [ 3 ] / d , m [ 0 ] / d , 0 , ( m [ 3 ] * m [ 7 ] - m [ 4 ] * m [ 6 ] ) / d , - ( m [ 0 ] * m [ 7 ] - m [ 1 ] * m [ 6 ] ) / d , 1 ]", "predictions": ["do the do the do the try to do the try"], "references": ["multiplying a matrix by its inverse produces the identity matrix ."], "bleu": 0.11390778025531027, "rouge_l": 0.09090909090909091}
{"id": 2852, "code": "def transform path ( self , path ) : p = path . class ( ) for pt in path : if pt . cmd == \"close\" : p . closepath ( ) elif pt . cmd == \"moveto\" : p . moveto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == \"lineto\" : p . lineto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == \"curveto\" : vx1 , vy1 = self . apply ( pt . ctrl1 . x , pt . ctrl1 . y ) vx2 , vy2 = self . apply ( pt . ctrl2 . x , pt . ctrl2 . y ) x , y = self . apply ( pt . x , pt . y ) p . curveto ( vx1 , vy1 , vx2 , vy2 , x , y ) return p", "predictions": ["transform the path path path"], "references": ["returns a bezierpath object with the transformation applied ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2853, "code": "def intersects ( self , b ) : return max ( self . x , b . x ) < min ( self . x + self . width , b . x + b . width ) and max ( self . y , b . y ) < min ( self . y + self . height , b . y + b . height )", "predictions": ["returns the number of points that are plotted with a given = = true"], "references": ["return true if a part of the two bounds overlaps ."], "bleu": 0.11114924776032006, "rouge_l": 0.08176943699731902}
{"id": 2854, "code": "def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )", "predictions": ["for internal use only ."], "references": ["returns bounds that encompass the union of the two ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2855, "code": "def contains ( self , * a ) : if len ( a ) == 2 : a = [ Point ( a [ 0 ] , a [ 1 ] ) ] if len ( a ) == 1 : a = a [ 0 ] if isinstance ( a , Point ) : return a . x >= self . x and a . x <= self . x + self . width and a . y >= self . y and a . y <= self . y + self . height if isinstance ( a , Bounds ) : return a . x >= self . x and a . x + a . width <= self . x + self . width and a . y >= self . y and a . y + a . height <= self . y + self . height", "predictions": ["check if two points are datetime ."], "references": ["returns true if the given point or rectangle falls within the bounds ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 2856, "code": "def error ( message ) : global parser print ( ( \"Error: \" ) + message ) print ( ) parser . print help ( ) sys . exit ( )", "predictions": ["prints an load message and exits with the given refs ."], "references": ["prints an error message the help message and quits"], "bleu": 0.1972940627795883, "rouge_l": 0.4073455759599332}
{"id": 2857, "code": "def fill ( self , * args ) : self . fillcolor = self . color ( * args ) return self . fillcolor", "predictions": ["not yet yet yet set"], "references": ["sets a fill color applying it to new paths ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2858, "code": "def stroke ( self , * args ) : self . strokecolor = self . color ( * args ) return self . strokecolor", "predictions": ["image image of message"], "references": ["set a stroke color applying it to new paths ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 2859, "code": "def textpath ( self , txt , x , y , width = None , height = 1000000 , enable Rendering = False , * * kwargs ) : txt = self . Text ( txt , x , y , width , height , * * kwargs ) path = txt . path if draw : path . draw ( ) return path", "predictions": ["renders a star in the specified rectangle"], "references": ["draws an outlined path of the input text"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2860, "code": "def draw cornu flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 #print '%', c, s x = c * cs - s * ss y = s * cs + c * ss print pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd", "predictions": ["draw a cornu self . cornu"], "references": ["raph levien s code draws fast lineto segments ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2861, "code": "def draw cornu bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : s , c = eval cornu ( curvetime ) s *= flip s -= s0 c -= c0 dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) #evaluate the fresnel further along the function to look ahead to the next point s2 , c2 = eval cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print pt ( x , y , cmd ) cmd = 'curveto' print crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd", "predictions": ["draw function to draw a cornu self . x3"], "references": ["mark meyer s code draws elegant curveto segments ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2862, "code": "def search ( q , start = 1 , count = 10 , context = None , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO SEARCH return Yahoo Search ( q , start , count , service , context , wait , asynchronous , cached )", "predictions": ["searches for a list of cached"], "references": ["returns a yahoo web query formatted as a yahoosearch list object ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 2863, "code": "def search images ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO IMAGES return Yahoo Search ( q , start , count , service , None , wait , asynchronous , cached )", "predictions": ["graph background background background background range range"], "references": ["returns a yahoo images query formatted as a yahoosearch list object ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 2864, "code": "def search news ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO NEWS return Yahoo Search ( q , start , count , service , None , wait , asynchronous , cached )", "predictions": ["node by experiment_config ."], "references": ["returns a yahoo news query formatted as a yahoosearch list object ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2865, "code": "def suggest spelling ( q , wait = 10 , asynchronous = False , cached = False ) : return Yahoo Spelling ( q , wait , asynchronous , cached )", "predictions": ["this function returns a label with a label of the given alpha ."], "references": ["returns list of suggested spelling corrections for the given query ."], "bleu": 0.14949751774990683, "rouge_l": 0.42302357836338417}
{"id": 2866, "code": "def parse ( self , e , tag ) : tags = e . get Elements By Tag Name ( tag ) children = tags [ 0 ] . child Nodes if len ( children ) != 1 : return None assert children [ 0 ] . node Type == xml . dom . minidom . Element . TEXT NODE s = children [ 0 ] . node Value s = format data ( s ) s = replace entities ( s ) return s", "predictions": ["edge - parse xml alpha alpha"], "references": ["parses the text data from an xml element defined by tag ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 2867, "code": "def delete ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ]", "predictions": ["edge - edge edge ."], "references": ["removes this layer from the canvas ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2868, "code": "def up ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = min ( len ( self . canvas . layers ) , i + 1 ) self . canvas . layers . insert ( i , self )", "predictions": ["len path path path path path path end of last canvas end end end end end end of last canvas end end end end end end end end end of last"], "references": ["moves the layer up in the stacking order ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2869, "code": "def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )", "predictions": ["copy this cell to the init s top of the init s init s screen s init s screen s screen s init s screen s screen s screen s screen"], "references": ["moves the layer down in the stacking order ."], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 2870, "code": "def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( \"RGB\" ) self . img = Image Ops . invert ( self . img ) self . img = self . img . convert ( \"RGBA\" ) self . img . putalpha ( alpha )", "predictions": ["apply the image to the image image ."], "references": ["inverts the layer ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 2871, "code": "def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP LEFT RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP TOP BOTTOM )", "predictions": ["copy the image to the target graph order order order order"], "references": ["flips the layer either horizontal or vertical ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 2872, "code": "def should run ( self , iteration , max iterations ) : if iteration == 0 : return True if max iterations : if iteration < max iterations : return True elif max iterations is None : if self . dynamic : return True else : return False return True if not self . dynamic : return False return False", "predictions": ["returns whether or not this incoming incoming incoming incoming incoming incoming incoming incoming incoming incoming incoming incoming incoming data can be executed get"], "references": ["return false if bot should quit"], "bleu": 0.04449945957170705, "rouge_l": 0.0}
{"id": 2873, "code": "def hex to rgb ( hex ) : hex = hex . lstrip ( \"#\" ) if len ( hex ) < 6 : hex += hex [ - 1 ] * ( 6 - len ( hex ) ) if len ( hex ) == 6 : r , g , b = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : ] r , g , b = [ int ( n , 16 ) / 255.0 for n in ( r , g , b ) ] a = 1.0 elif len ( hex ) == 8 : r , g , b , a = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : 6 ] , hex [ 6 : ] r , g , b , a = [ int ( n , 16 ) / 255.0 for n in ( r , g , b , a ) ] return r , g , b , a", "predictions": ["convert copytree copytree color to src"], "references": ["returns rgb values for a hex color string ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2874, "code": "def simple traceback ( ex , source ) : exc type , exc value , exc tb = sys . exc info ( ) exc = traceback . format exception ( exc type , exc value , exc tb ) source arr = source . splitlines ( ) exc location = exc [ - 2 ] for i , err in enumerate ( exc ) : if 'exec source in ns' in err : exc location = exc [ i + 1 ] break fn = exc location . split ( ',' ) [ 0 ] [ 8 : - 1 ] line number = int ( exc location . split ( ',' ) [ 1 ] . replace ( 'line' , '' ) . strip ( ) ) err msgs = [ ] err where = ' ' . join ( exc [ i - 1 ] . split ( ',' ) [ 1 : ] ) . strip ( ) err msgs . append ( 'Error in the Shoebot script at %s:' % err where ) for i in xrange ( max ( 0 , line number - 5 ) , line number ) : if fn == \"<string>\" : line = source arr [ i ] else : line = linecache . getline ( fn , i + 1 ) err msgs . append ( '%s: %s' % ( i + 1 , line . rstrip ( ) ) ) err msgs . append ( '  %s^ %s' % ( len ( str ( i ) ) * ' ' , exc [ - 1 ] . rstrip ( ) ) ) err msgs . append ( '' ) err msgs . append ( exc [ 0 ] . rstrip ( ) ) for err in exc [ 3 : ] : err msgs . append ( err . rstrip ( ) ) return '\\n' . join ( err msgs )", "predictions": ["return a traceback script"], "references": ["format traceback showing line number and surrounding source ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 2875, "code": "def close ( self ) : self . con . commit ( ) self . cur . close ( ) self . con . close ( )", "predictions": ["search and search for the pooled connection start ."], "references": ["commits any pending transactions and closes the database ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 2876, "code": "def sql ( self , sql ) : self . cur . execute ( sql ) if sql . lower ( ) . find ( \"select\" ) >= 0 : matches = [ ] for r in self . cur : matches . append ( r ) return matches", "predictions": ["returns list of search results"], "references": ["executes a raw sql statement on the database ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 2877, "code": "def edit ( self , id , * args , * * kw ) : if args and kw : return if args and type ( args [ 0 ] ) == dict : fields = [ k for k in args [ 0 ] ] v = [ args [ 0 ] [ k ] for k in args [ 0 ] ] if kw : fields = [ k for k in kw ] v = [ kw [ k ] for k in kw ] sql = \"update \" + self . name + \" set \" + \"=?, \" . join ( fields ) + \"=? where \" + self . key + \"=\" + unicode ( id ) self . db . cur . execute ( sql , v ) self . db . i += 1 if self . db . i >= self . db . commit : self . db . i = 0 self . db . con . commit ( )", "predictions": ["search for a database id"], "references": ["edits the row with given id ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2878, "code": "def remove ( self , id , operator = \"=\" , key = None ) : if key == None : key = self . key try : id = unicode ( id ) except : pass sql = \"delete from \" + self . name + \" where \" + key + \" \" + operator + \" ?\" self . db . cur . execute ( sql , ( id , ) )", "predictions": ["parse a sql and all its data"], "references": ["deletes the row with given id ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2879, "code": "def reload functions ( self ) : with Live Execution . lock : if self . edited source : tree = ast . parse ( self . edited source ) for f in [ n for n in ast . walk ( tree ) if isinstance ( n , ast . Function Def ) ] : self . ns [ f . name ] . code = meta . decompiler . compile func ( f , self . filename , self . ns ) . code", "predictions": ["hash all functions functions"], "references": ["replace functions in namespace with functions from edited_source ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 2880, "code": "def run ( self ) : with Live Execution . lock : if self . edited source : success , ex = self . run tenuous ( ) if success : return self . do exec ( self . known good , self . ns )", "predictions": ["start the thread ."], "references": ["attempt to known good or tenuous source ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2881, "code": "def alignment ( self , d = 5 ) : vx = vy = vz = 0 for b in self . boids : if b != self : vx , vy , vz = vx + b . vx , vy + b . vy , vz + b . vz n = len ( self . boids ) - 1 vx , vy , vz = vx / n , vy / n , vz / n return ( vx - self . vx ) / d , ( vy - self . vy ) / d , ( vz - self . vz ) / d", "predictions": ["alignment of the alignment"], "references": ["boids match velocity with other boids ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 2882, "code": "def angle ( self ) : from math import atan , pi , degrees a = degrees ( atan ( self . vy / self . vx ) ) + 360 if self . vx < 0 : a += 180 return a", "predictions": ["return the angle between two vectors"], "references": ["returns the angle towards which the boid is steering ."], "bleu": 0.14925824694560996, "rouge_l": 0.23921568627450981}
{"id": 2883, "code": "def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d", "predictions": ["return the goal vector"], "references": ["tendency towards a particular place ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2884, "code": "def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 m2 = 1.0 m3 = 1.0 m4 = 1.0 if not self . scattered and ctx . random ( ) < self . scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . scatter i += 1 if self . scatter i >= self . scatter t : self . scattered = False self . scatter i = 0 if not self . has goal : m4 = 0 if self . flee : m4 = - m4 for b in self : if b . is perching : if b . perch t > 0 : b . perch t -= 1 continue else : b . is perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . gx , self . gy , self . gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )", "predictions": ["update the bar bar ."], "references": ["calculates the next motion frame for the flock ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 2885, "code": "def iterscan ( self , string , idx = 0 , context = None ) : match = self . scanner . scanner ( string , idx ) . match actions = self . actions lastend = idx end = len ( string ) while True : m = match ( ) if m is None : break matchbegin , matchend = m . span ( ) if lastend == matchend : break action = actions [ m . lastindex ] if action is not None : rval , next pos = action ( m , context ) if next pos is not None and next pos != matchend : matchend = next pos match = self . scanner . scanner ( string , matchend ) . match yield rval , matchend lastend = matchend", "predictions": ["iterator over scanner actions"], "references": ["yield match end_idx for each match"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2886, "code": "def copy ( self , graph ) : l = self . class ( graph , self . n ) l . i = 0 return l", "predictions": ["return a new instance with the same attributes ."], "references": ["returns a copy of the layout for the given graph ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 2887, "code": "def clear ( self ) : dict . clear ( self ) self . nodes = [ ] self . edges = [ ] self . root = None self . layout . i = 0 self . alpha = 0", "predictions": ["clear the tree and reset the edges ."], "references": ["remove nodes and edges and reset the layout ."], "bleu": 0.29150322793751426, "rouge_l": 0.465648854961832}
{"id": 2888, "code": "def add node ( self , id , radius = 8 , style = style . DEFAULT , category = \"\" , label = None , root = False , properties = { } ) : if self . has key ( id ) : return self [ id ] if not isinstance ( style , str ) and style . dict . has key [ \"name\" ] : style = style . name n = node ( self , id , radius , style , category , label , properties ) self [ n . id ] = n self . nodes . append ( n ) if root : self . root = n return n", "predictions": ["add a node to the graph"], "references": ["add node from id and return the node object ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 2889, "code": "def remove node ( self , id ) : if self . has key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )", "predictions": ["remove a node from the graph"], "references": ["remove node with given id ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 2890, "code": "def remove edge ( self , id1 , id2 ) : for e in list ( self . edges ) : if id1 in ( e . node1 . id , e . node2 . id ) and id2 in ( e . node1 . id , e . node2 . id ) : e . node1 . links . remove ( e . node2 ) e . node2 . links . remove ( e . node1 ) self . edges . remove ( e )", "predictions": ["remove an edge from the graph ."], "references": ["remove edges between nodes with given id s ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 2891, "code": "def edge ( self , id1 , id2 ) : if id1 in self and id2 in self and self [ id2 ] in self [ id1 ] . links : return self [ id1 ] . links . edge ( id2 ) return None", "predictions": ["get the edge of a edge ."], "references": ["returns the edge between the nodes with given id1 and id2 ."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 2892, "code": "def update ( self , iterations = 10 ) : self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) min , max = self . layout . bounds self . x = ctx . WIDTH - max . x * self . d - min . x * self . d self . y = ctx . HEIGHT - max . y * self . d - min . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done", "predictions": ["update the layout with the previous layout ."], "references": ["iterates the graph layout and updates node positions ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 2893, "code": "def offset ( self , node ) : x = self . x + node . x - ctx . WIDTH / 2 y = self . y + node . y - ctx . HEIGHT / 2 return x , y", "predictions": ["return the offset of a node ."], "references": ["returns the distance from the center to the given node ."], "bleu": 0.14834636222628117, "rouge_l": 0.32049036777583184}
{"id": 2894, "code": "def prune ( self , depth = 0 ) : for n in list ( self . nodes ) : if len ( n . links ) <= depth : self . remove node ( n . id )", "predictions": ["prune all nodes that are depth ."], "references": ["removes all nodes with less or equal links than depth ."], "bleu": 0.17359100558362855, "rouge_l": 0.4273204903677758}
{"id": 2895, "code": "def nodes by category ( self , category ) : return [ n for n in self . nodes if n . category == category ]", "predictions": ["return all nodes by a category ."], "references": ["returns nodes with the given category attribute ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 2896, "code": "def crown ( self , depth = 2 ) : nodes = [ ] for node in self . leaves : nodes += node . flatten ( depth - 1 ) return cluster . unique ( nodes )", "predictions": ["return the unique nodes of the graph ."], "references": ["returns a list of leaves nodes connected to leaves etc ."], "bleu": 0.13107175678306446, "rouge_l": 0.20469798657718125}
{"id": 2897, "code": "def density ( self ) : return 2.0 * len ( self . edges ) / ( len ( self . nodes ) * ( len ( self . nodes ) - 1 ) )", "predictions": ["the density as a tuple of the center of the center"], "references": ["the number of edges in relation to the total number of possible edges ."], "bleu": 0.10904215887663019, "rouge_l": 0.31322207958921694}
{"id": 2898, "code": "def load ( self , id ) : self . clear ( ) self . add node ( id , root = True ) for w , id2 in self . get links ( id ) : self . add edge ( id , id2 , weight = w ) if len ( self ) > self . max : break for w , id2 , links in self . get cluster ( id ) : for id3 in links : self . add edge ( id3 , id2 , weight = w ) self . add edge ( id , id3 , weight = w ) #if len(links) == 0: if len ( self ) > self . max : break if self . event . clicked : g . add node ( self . event . clicked )", "predictions": ["load nodes from the scope ."], "references": ["rebuilds the graph around the given node id ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2899, "code": "def click ( self , node ) : if not self . has node ( node . id ) : return if node == self . root : return self . dx , self . dy = self . offset ( node ) self . previous = self . root . id self . load ( node . id )", "predictions": ["click a node ."], "references": ["callback from graph . events when a node is clicked ."], "bleu": 0.0883002314431393, "rouge_l": 0.3689516129032258}
{"id": 2900, "code": "def angle ( x1 , y1 , x2 , y2 ) : sign = 1.0 usign = ( x1 * y2 - y1 * x2 ) if usign < 0 : sign = - 1.0 num = x1 * x2 + y1 * y2 den = hypot ( x1 , y1 ) * hypot ( x2 , y2 ) ratio = min ( max ( num / den , - 1.0 ) , 1.0 ) return sign * degrees ( acos ( ratio ) )", "predictions": ["return a angle between two vectors"], "references": ["the angle in degrees between two vectors ."], "bleu": 0.31149111610852515, "rouge_l": 0.5570776255707762}
{"id": 2901, "code": "def transform from local ( xp , yp , cphi , sphi , mx , my ) : x = xp * cphi - yp * sphi + mx y = xp * sphi + yp * cphi + my return ( x , y )", "predictions": ["transforms input from local regions into a new matrix ."], "references": ["transform from the local frame to absolute space ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 2902, "code": "def create view ( self , name = \"shoebot-output\" ) : view = Gtk . Text View ( ) view . set editable ( False ) fontdesc = Pango . Font Description ( \"Monospace\" ) view . modify font ( fontdesc ) view . set name ( name ) buff = view . get buffer ( ) buff . create tag ( 'error' , foreground = 'red' ) return view", "predictions": ["creates a view with the given name"], "references": ["create the gtk . textview used for shell output"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2903, "code": "def set bot ( self , bot ) : self . bot = bot self . sink . set bot ( bot )", "predictions": ["set the bot s bot ."], "references": ["bot must be set before running"], "bleu": 0.24446151121745047, "rouge_l": 0.16666666666666666}
{"id": 2904, "code": "def settings ( self , * * kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v )", "predictions": ["set settings to the context"], "references": ["pass a load of settings into the canvas"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2905, "code": "def flush ( self , frame ) : self . sink . render ( self . size or default ( ) , frame , self . drawqueue ) self . reset drawqueue ( )", "predictions": ["flush the underlying frame ."], "references": ["passes the drawqueue to the sink for rendering"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2906, "code": "def reflect ( self , x0 , y0 , x , y ) : rx = x0 - ( x - x0 ) ry = y0 - ( y - y0 ) return rx , ry", "predictions": ["returns the color likelihood for the given surface and y0"], "references": ["reflects the point x y through origin x0 y0 ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 2907, "code": "def angle ( self , x0 , y0 , x1 , y1 ) : a = degrees ( atan ( ( y1 - y0 ) / ( x1 - x0 + 0.00001 ) ) ) + 360 if x1 - x0 < 0 : a += 180 return a", "predictions": ["return a angle between two vectors"], "references": ["calculates the angle between two points ."], "bleu": 0.34801709319446883, "rouge_l": 0.45522388059701485}
{"id": 2908, "code": "def coordinates ( self , x0 , y0 , distance , angle ) : x = x0 + cos ( radians ( angle ) ) * distance y = y0 + sin ( radians ( angle ) ) * distance return Point ( x , y )", "predictions": ["get a distance between two points"], "references": ["calculates the coordinates of a point from the origin ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2909, "code": "def contains point ( self , x , y , d = 2 ) : if self . path != None and len ( self . path ) > 1 and self . path . contains ( x , y ) : if not self . path . contains ( x + d , y ) or not self . path . contains ( x , y + d ) or not self . path . contains ( x - d , y ) or not self . path . contains ( x , y - d ) or not self . path . contains ( x + d , y + d ) or not self . path . contains ( x - d , y - d ) or not self . path . contains ( x + d , y - d ) or not self . path . contains ( x - d , y + d ) : return True return False", "predictions": ["return true if x is a point point ."], "references": ["returns true when x y is on the path stroke outline ."], "bleu": 0.12716571564598603, "rouge_l": 0.3713850837138508}
{"id": 2910, "code": "def gtk mouse button down ( self , widget , event ) : if self . menu enabled and event . button == 3 : menu = self . uimanager . get widget ( '/Save as' ) menu . popup ( None , None , None , None , event . button , event . time ) else : super ( Shoebot Window , self ) . gtk mouse button down ( widget , event )", "predictions": ["handle gtk button down event for gtk button"], "references": ["handle right mouse button clicks"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 2911, "code": "def show variables window ( self ) : if self . var window is None and self . bot . vars : self . var window = Var Window ( self , self . bot , '%s variables' % ( self . title or 'Shoebot' ) ) self . var window . window . connect ( \"destroy\" , self . var window closed )", "predictions": ["show the variables window window ."], "references": ["show the variables window ."], "bleu": 0.6803749333171202, "rouge_l": 0.9242424242424241}
{"id": 2912, "code": "def hide variables window ( self ) : if self . var window is not None : self . var window . window . destroy ( ) self . var window = None", "predictions": ["run the variables self self . window self . window"], "references": ["hide the variables window"], "bleu": 0.17827531042796255, "rouge_l": 0.4644670050761421}
{"id": 2913, "code": "def do fullscreen ( self , widget ) : self . fullscreen ( ) self . is fullscreen = True while Gtk . events pending ( ) : Gtk . main iteration ( ) self . bot . screen width = Gdk . Screen . width ( ) self . bot . screen height = Gdk . Screen . height ( ) self . bot . screen ratio = self . bot . screen width / self . bot . screen height", "predictions": ["safe mode in the if the screen is selected"], "references": ["widget action to make the window fullscreen and update the bot ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 2914, "code": "def do unfullscreen ( self , widget ) : self . unfullscreen ( ) self . is fullscreen = False self . bot . screen ratio = None", "predictions": ["function that is called when the = selected"], "references": ["widget action to set windowed mode ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2915, "code": "def do window close ( self , widget , data = None ) : publish event ( QUIT EVENT ) if self . has server : self . sock . close ( ) self . hide variables window ( ) self . destroy ( ) self . window open = False", "predictions": ["websocket - return the window or a window connection"], "references": ["widget action to close the window triggering the quit event ."], "bleu": 0.14873743701255318, "rouge_l": 0.19645732689210954}
{"id": 2916, "code": "def do toggle fullscreen ( self , action ) : is fullscreen = action . get active ( ) if is fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )", "predictions": ["toggle self alignment alignment"], "references": ["widget action to toggle fullscreen from the gui"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2917, "code": "def do toggle variables ( self , action ) : self . show vars = action . get active ( ) if self . show vars : self . show variables window ( ) else : self . hide variables window ( )", "predictions": ["toggle self . window"], "references": ["widget action to toggle showing the variables window ."], "bleu": 0.12241977696855179, "rouge_l": 0.28773584905660377}
{"id": 2918, "code": "def mouse pointer moved ( self , x , y ) : self . namespace [ 'MOUSEX' ] = x self . namespace [ 'MOUSEY' ] = y", "predictions": ["forward copy of a copy of the n pointer"], "references": ["gui callback for mouse moved"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2919, "code": "def key pressed ( self , key , keycode ) : self . namespace [ 'key' ] = key self . namespace [ 'keycode' ] = keycode self . namespace [ 'keydown' ] = True", "predictions": ["remove a new entry in the list"], "references": ["gui callback for key pressed"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2920, "code": "def set callbacks ( self , * * kwargs ) : for name in self . SUPPORTED CALLBACKS : func = kwargs . get ( name , getattr ( self , name ) ) setattr ( self , name , func )", "predictions": ["add node node to context category category category category category category category category category category category"], "references": ["set callbacks for input events"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2921, "code": "def complement ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) colors . append ( clr . complement ) return colors", "predictions": ["returns a color containing a color of a given color ."], "references": ["returns the color and its complement in a list ."], "bleu": 0.14323145079400493, "rouge_l": 0.384251968503937}
{"id": 2922, "code": "def right complement ( clr ) : right = split complementary ( clr ) [ 2 ] colors = complementary ( clr ) colors [ 3 ] . h = right . h colors [ 4 ] . h = right . h colors [ 5 ] . h = right . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 5 ] , colors [ 4 ] , colors [ 3 ] ) return colors", "predictions": ["return color color color tuple"], "references": ["returns the right half of the split complement ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 2923, "code": "def compound ( clr , flip = False ) : def wrap ( x , min , threshold , plus ) : if x - min < threshold : return x + plus else : return x - min d = 1 if flip : d = - 1 clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate ryb ( 30 * d ) c . brightness = wrap ( clr . brightness , 0.25 , 0.6 , 0.25 ) colors . append ( c ) c = clr . rotate ryb ( 30 * d ) c . saturation = wrap ( clr . saturation , 0.4 , 0.1 , 0.4 ) c . brightness = wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) colors . append ( c ) c = clr . rotate ryb ( 160 * d ) c . saturation = wrap ( clr . saturation , 0.25 , 0.1 , 0.25 ) c . brightness = max ( 0.2 , clr . brightness ) colors . append ( c ) c = clr . rotate ryb ( 150 * d ) c . saturation = wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = wrap ( clr . brightness , 0.3 , 0.6 , 0.3 ) colors . append ( c ) c = clr . rotate ryb ( 150 * d ) c . saturation = wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) return colors", "predictions": ["return edge brightness ."], "references": ["roughly the complement and some far analogs ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2924, "code": "def blend ( self , clr , factor = 0.5 ) : r = self . r * ( 1 - factor ) + clr . r * factor g = self . g * ( 1 - factor ) + clr . g * factor b = self . b * ( 1 - factor ) + clr . b * factor a = self . a * ( 1 - factor ) + clr . a * factor return Color ( r , g , b , a , mode = \"rgb\" )", "predictions": ["0 0 ... ..."], "references": ["returns a mix of two colors ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 2925, "code": "def swatch ( self , x , y , w = 35 , h = 35 , roundness = 0 ) : ctx . fill ( self ) ctx . rect ( x , y , w , h , roundness )", "predictions": ["draw a rectangle with a given point 2 2 2 bytes 2 2 2 2 2 2 2 2 2 2 and floats 2 2 2 2 2 2 2 2"], "references": ["rectangle swatch for this color ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 2926, "code": "def copy ( self ) : return Color List ( [ color ( clr . r , clr . g , clr . b , clr . a , mode = \"rgb\" ) for clr in self ] , name = self . name , tags = self . tags )", "predictions": ["returns a prune instance of this graph ."], "references": ["returns a deep copy of the list ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 2927, "code": "def average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = \"rgb\" )", "predictions": ["nodes are a color"], "references": ["returns one average color for the colors in the list ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 2928, "code": "def sorted copy ( self , comparison , reversed = False ) : sorted = self . copy ( ) list . sort ( sorted , comparison ) if reversed : list . reverse ( sorted ) return sorted", "predictions": ["leaves a copy but keep some files that have a copy ."], "references": ["returns a sorted copy with the colors arranged according to the given comparison ."], "bleu": 0.10459315495983224, "rouge_l": 0.22761194029850743}
{"id": 2929, "code": "def reverse ( self ) : colors = Color List . copy ( self ) list . reverse ( colors ) return colors", "predictions": ["returns a density as a color ."], "references": ["returns a reversed copy of the list ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 2930, "code": "def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding", "predictions": ["draw a line with a given axis get the given output ."], "references": ["rectangle swatches for all the colors in the list ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 2931, "code": "def swarm ( self , x , y , r = 100 ) : sc = ctx . stroke ( 0 , 0 , 0 , 0 ) sw = ctx . strokewidth ( 0 ) ctx . push ( ) ctx . transform ( ctx . CORNER ) ctx . translate ( x , y ) for i in range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) ctx . fill ( clr ) clr = choice ( self ) ctx . stroke ( clr ) ctx . strokewidth ( 10 * random ( ) ) ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) ctx . oval ( r * random ( ) , 0 , r2 , r2 ) ctx . pop ( ) ctx . strokewidth ( sw ) if sc is None : ctx . nostroke ( ) else : ctx . stroke ( sc )", "predictions": ["draw a point within a given point == root == root == root == 1 == 0 == 1 == 1 == 1 == root"], "references": ["fancy random ovals for all the colors in the list ."], "bleu": 0.04085892079136996, "rouge_l": 0.0}
{"id": 2932, "code": "def interpolate ( self , colors , n = 100 ) : gradient = [ ] for i in range ( n ) : l = len ( colors ) - 1 x = int ( 1.0 * i / n * l ) x = min ( x + 0 , l ) y = min ( x + 1 , l ) base = 1.0 * n / l * x d = ( i - base ) / ( 1.0 * n / l ) r = colors [ x ] . r * ( 1 - d ) + colors [ y ] . r * d g = colors [ x ] . g * ( 1 - d ) + colors [ y ] . g * d b = colors [ x ] . b * ( 1 - d ) + colors [ y ] . b * d a = colors [ x ] . a * ( 1 - d ) + colors [ y ] . a * d gradient . append ( color ( r , g , b , a , mode = \"rgb\" ) ) gradient . append ( colors [ - 1 ] ) return gradient", "predictions": ["angle = 1 1"], "references": ["returns intermediary colors for given list of colors ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2933, "code": "def save ( self ) : if not os . path . exists ( self . cache ) : os . makedirs ( self . cache ) path = os . path . join ( self . cache , self . name + \".xml\" ) f = open ( path , \"w\" ) f . write ( self . xml ) f . close ( )", "predictions": ["transform the xml into disk"], "references": ["saves the color information in the cache as xml ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 2934, "code": "def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors", "predictions": ["generate a color of the color distribution"], "references": ["returns a number of random colors from the theme ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 2935, "code": "def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = Color Theme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT CACHE , \"recombined\" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c", "predictions": ["using other and d2"], "references": ["genetic recombination of two themes using cut and splice technique ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 2936, "code": "def render closure ( self ) : fillcolor = self . fill strokecolor = self . stroke strokewidth = self . strokewidth def render ( cairo ctx ) : transform = self . call transform mode ( self . transform ) if fillcolor is None and strokecolor is None : return cairo ctx . set matrix ( transform ) self . traverse ( cairo ctx ) cairo ctx . set matrix ( cairo . Matrix ( ) ) if fillcolor is not None and strokecolor is not None : if strokecolor [ 3 ] < 1 : cairo ctx . push group ( ) cairo ctx . set source rgba ( * fillcolor ) cairo ctx . fill preserve ( ) e = cairo ctx . stroke extents ( ) cairo ctx . set source rgba ( * strokecolor ) cairo ctx . set operator ( cairo . OPERATOR SOURCE ) cairo ctx . set line width ( strokewidth ) cairo ctx . stroke ( ) cairo ctx . pop group to source ( ) cairo ctx . paint ( ) else : cairo ctx . set source rgba ( * fillcolor ) cairo ctx . fill preserve ( ) cairo ctx . set source rgba ( * strokecolor ) cairo ctx . set line width ( strokewidth ) cairo ctx . stroke ( ) elif fillcolor is not None : cairo ctx . set source rgba ( * fillcolor ) cairo ctx . fill ( ) elif strokecolor is not None : cairo ctx . set source rgba ( * strokecolor ) cairo ctx . set line width ( strokewidth ) cairo ctx . stroke ( ) return render", "predictions": ["settings are rendered with the traceback"], "references": ["use a closure so that draw attributes can be saved"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 2937, "code": "def linelength ( self , x0 , y0 , x1 , y1 ) : a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )", "predictions": ["returns a single chunk of size"], "references": ["returns the length of the line ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 2938, "code": "def segment lengths ( self , relative = False , n = 20 ) : lengths = [ ] first = True for el in self . get elements ( ) : if first is True : close x , close y = el . x , el . y first = False elif el . cmd == MOVETO : close x , close y = el . x , el . y lengths . append ( 0.0 ) elif el . cmd == CLOSE : lengths . append ( self . linelength ( x0 , y0 , close x , close y ) ) elif el . cmd == LINETO : lengths . append ( self . linelength ( x0 , y0 , el . x , el . y ) ) elif el . cmd == CURVETO : x3 , y3 , x1 , y1 , x2 , y2 = el . x , el . y , el . c1x , el . c1y , el . c2x , el . c2y lengths . append ( self . curvelength ( x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , n ) ) if el . cmd != CLOSE : x0 = el . x y0 = el . y if relative : length = sum ( lengths ) try : return map ( lambda l : l / length , lengths ) except Zero Division Error : return [ 0.0 ] * len ( lengths ) else : return lengths", "predictions": ["get list of lengths lengths"], "references": ["returns a list with the lengths of each segment in the path ."], "bleu": 0.06554932163900559, "rouge_l": 0.20573355817875214}
{"id": 2939, "code": "def get elements ( self ) : for index , el in enumerate ( self . elements ) : if isinstance ( el , tuple ) : el = Path Element ( * el ) self . elements [ index ] = el yield el", "predictions": ["return an iterator over all contained contained in the tree"], "references": ["yields all elements as pathelements"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 2940, "code": "def description ( self ) : meta = self . find ( \"meta\" , { \"name\" : \"description\" } ) if isinstance ( meta , dict ) and meta . has key ( \"content\" ) : return meta [ \"content\" ] else : return u\"\"", "predictions": ["return coordinates for the category"], "references": ["returns the meta description in the page ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2941, "code": "def keywords ( self ) : meta = self . find ( \"meta\" , { \"name\" : \"keywords\" } ) if isinstance ( meta , dict ) and meta . has key ( \"content\" ) : keywords = [ k . strip ( ) for k in meta [ \"content\" ] . split ( \",\" ) ] else : keywords = [ ] return keywords", "predictions": ["list of contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains contains"], "references": ["returns the meta keywords in the page ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2942, "code": "def sorted ( list , cmp = None , reversed = False ) : list = [ x for x in list ] list . sort ( cmp ) if reversed : list . reverse ( ) return list", "predictions": ["get a button gtk"], "references": ["returns a sorted copy of the list ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2943, "code": "def unique ( list ) : unique = [ ] [ unique . append ( x ) for x in list if x not in unique ] return unique", "predictions": ["returns a show of the show elements of a window ."], "references": ["returns a copy of the list without duplicates ."], "bleu": 0.1972940627795883, "rouge_l": 0.5091819699499166}
{"id": 2944, "code": "def clique ( graph , id ) : clique = [ id ] for n in graph . nodes : friend = True for id in clique : if n . id == id or graph . edge ( n . id , id ) == None : friend = False break if friend : clique . append ( n . id ) return clique", "predictions": ["get clique of nodes"], "references": ["returns the largest possible clique for the node with given id ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2945, "code": "def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques", "predictions": ["get cliques nodes for a graph"], "references": ["returns all the cliques in the graph of at least the given size ."], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 2946, "code": "def hex Dump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( \"%2x \" % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( \"\" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )", "predictions": ["prints the hex string to stdout"], "references": ["useful utility ; prints the string in hexadecimal"], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 2947, "code": "def decode OSC ( data ) : table = { \"i\" : read Int , \"f\" : read Float , \"s\" : read String , \"b\" : read Blob } decoded = [ ] address , rest = read String ( data ) typetags = \"\" if address == \"#bundle\" : time , rest = read Long ( rest ) while len ( rest ) > 0 : length , rest = read Int ( rest ) decoded . append ( decode OSC ( rest [ : length ] ) ) rest = rest [ length : ] elif len ( rest ) > 0 : typetags , rest = read String ( rest ) decoded . append ( address ) decoded . append ( typetags ) if typetags [ 0 ] == \",\" : for tag in typetags [ 1 : ] : value , rest = table [ tag ] ( rest ) decoded . append ( value ) else : print \"Oops, typetag lacks the magic ,\" return decoded", "predictions": ["decode a osc from a list of magic magic"], "references": ["converts a typetagged osc message to a python list ."], "bleu": 0.15881076016027915, "rouge_l": 0.41709401709401706}
{"id": 2948, "code": "def dispatch ( self , message , source = None ) : msgtype = \"\" try : if type ( message [ 0 ] ) == str : address = message [ 0 ] self . callbacks [ address ] ( message ) elif type ( message [ 0 ] ) == list : for msg in message : self . dispatch ( msg ) except Key Error , key : print 'address %s not found, %s: %s' % ( address , key , message ) pprint . pprint ( message ) except Index Error , e : print '%s: %s' % ( e , message ) pass except None , e : print \"Exception in\" , address , \"callback :\" , e return", "predictions": ["process all callbacks ."], "references": ["sends decoded osc data to an appropriate calback"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 2949, "code": "def find example dir ( ) : code stub = textwrap . dedent ( ) code = code stub % 'share/shoebot/examples' cmd = [ \"python\" , \"-c\" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) output , errors = p . communicate ( ) if errors : print ( 'Shoebot experienced errors searching for install and examples.' ) print ( 'Errors:\\n{0}' . format ( errors . decode ( 'utf-8' ) ) ) return None else : examples dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples dir ) : return examples dir #code = \"from pkg resources import resource filename, Requirement; print resource filename(Requirement.parse('shoebot'), 'examples/')\" code = code stub % 'examples/' cmd = [ \"python\" , \"-c\" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) output , errors = p . communicate ( ) examples dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples dir ) : return examples dir if examples dir : print ( 'Shoebot could not find examples at: {0}' . format ( examples dir ) ) else : print ( 'Shoebot could not find install dir and examples.' )", "predictions": ["search for the examples dir and return its examples ."], "references": ["find examples dir .. a little bit ugly .."], "bleu": 0.16590387014219712, "rouge_l": 0.21254355400696867}
{"id": 2950, "code": "def eof ( self ) : return ( not self . is alive ( ) ) and self . queue . empty ( ) or self . fd . closed", "predictions": ["return true if the queue is eof ."], "references": ["check whether there is no more content to expect ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 2951, "code": "def close ( self ) : self . process . stdout . close ( ) self . process . stderr . close ( ) self . running = False", "predictions": ["close the connection ."], "references": ["close outputs of process ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 2952, "code": "def get command responses ( self ) : if not self . response queue . empty ( ) : yield None while not self . response queue . empty ( ) : line = self . response queue . get ( ) if line is not None : yield line", "predictions": ["returns a generator of the command responses ."], "references": ["get responses to commands sent"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2953, "code": "def get center ( self ) : w , h = self . layout . get pixel size ( ) x = ( self . x + w / 2 ) y = ( self . y + h / 2 ) return x , y", "predictions": ["get the center of the surface"], "references": ["returns the center point of the path disregarding transforms ."], "bleu": 0.1876823093652809, "rouge_l": 0.47843137254901963}
{"id": 2954, "code": "def draw math ( str , x , y , alpha = 1.0 ) : try : from web import ctx except : pass str = re . sub ( \"</{0,1}math>\" , \"\" , str . strip ( ) ) img = mimetex . gif ( str ) w , h = ctx . imagesize ( img ) ctx . image ( img , x , y , alpha = alpha ) return w , h", "predictions": ["draw a circle on the image"], "references": ["uses mimetex to generate a gif - image from the latex equation ."], "bleu": 0.08180282100568384, "rouge_l": 0.19741100323624597}
{"id": 2955, "code": "def draw table ( table , x , y , w , padding = 5 ) : try : from web import ctx except : pass f = ctx . fill ( ) ctx . stroke ( f ) h = ctx . textheight ( \" \" ) + padding * 2 row y = y if table . title != \"\" : ctx . fill ( f ) ctx . rect ( x , row y , w , h ) ctx . fill ( 1 ) ctx . text ( table . title , x + padding , row y + ctx . fontsize ( ) + padding ) row y += h rowspans = [ 1 for i in range ( 10 ) ] previous cell w = 0 for row in table : cell x = x cell w = 1.0 * w cell w -= previous cell w * len ( [ n for n in rowspans if n > 1 ] ) cell w /= len ( row ) cell h = 0 for cell in row : this h = ctx . textheight ( cell , width = cell w - padding * 2 ) + padding * 2 cell h = max ( cell h , this h ) i = 0 for cell in row : if rowspans [ i ] > 1 : rowspans [ i ] -= 1 cell x += previous cell w i += 1 m = re . search ( \"rowspan=\\\"(.*?)\\\"\" , cell . properties ) if m : rowspan = int ( m . group ( 1 ) ) rowspans [ i ] = rowspan else : rowspan = 1 ctx . fill ( f ) ctx . text ( cell , cell x + padding , row y + ctx . fontsize ( ) + padding , cell w - padding * 2 ) ctx . line ( cell x , row y , cell x + cell w , row y ) if cell x > x : ctx . nofill ( ) ctx . line ( cell x , row y , cell x , row y + cell h ) cell x += cell w i += 1 row y += cell h previous cell w = cell w ctx . nofill ( ) ctx . rect ( x , y , w , row y - y )", "predictions": ["draw a single table"], "references": ["this is a very poor algorithm to draw wikipedia tables in nodebox ."], "bleu": 0.041910459064397936, "rouge_l": 0.1073943661971831}
{"id": 2956, "code": "def sanitize ( self , val ) : if self . type == NUMBER : try : return clamp ( self . min , self . max , float ( val ) ) except Value Error : return 0.0 elif self . type == TEXT : try : return unicode ( str ( val ) , \"utf 8\" , \"replace\" ) except : return \"\" elif self . type == BOOLEAN : if unicode ( val ) . lower ( ) in ( \"true\" , \"1\" , \"yes\" ) : return True else : return False", "predictions": ["convert string to string"], "references": ["given a variable and a value cleans it out"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2957, "code": "def extract ( self ) : if self . parent : try : self . parent . contents . remove ( self ) except Value Error : pass #Find the two elements that would be next to each other if #this element (and any children) hadn't been parsed. Connect #the two. last Child = self . last Recursive Child ( ) next Element = last Child . next if self . previous : self . previous . next = next Element if next Element : next Element . previous = self . previous self . previous = None last Child . next = None self . parent = None if self . previous Sibling : self . previous Sibling . next Sibling = self . next Sibling if self . next Sibling : self . next Sibling . previous Sibling = self . previous Sibling self . previous Sibling = self . next Sibling = None return self", "predictions": ["removes the next element from the parent ."], "references": ["destructively rips this element out of the tree ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 2958, "code": "def last Recursive Child ( self ) : last Child = self while hasattr ( last Child , 'contents' ) and last Child . contents : last Child = last Child . contents [ - 1 ] return last Child", "predictions": ["get the last contents of the last contents ."], "references": ["finds the last element beneath this object to be parsed ."], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 2959, "code": "def find All ( self , name , attrs , text , limit , generator , * * kwargs ) : if isinstance ( name , Soup Strainer ) : strainer = name else : strainer = Soup Strainer ( name , attrs , text , * * kwargs ) results = Result Set ( strainer ) g = generator ( ) while True : try : i = g . next ( ) except Stop Iteration : break if i : found = strainer . search ( i ) if found : results . append ( found ) if limit and len ( results ) >= limit : break return results", "predictions": ["find all elements matching a generator ."], "references": ["iterates over a generator looking for things that match ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 2960, "code": "def invert ( h ) : i = { } for k , v in h . items ( ) : i [ v ] = k return i", "predictions": ["invert a dictionary with a key ."], "references": ["cheap function to invert a hash ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 2961, "code": "def decompose ( self ) : contents = [ i for i in self . contents ] for i in contents : if isinstance ( i , Tag ) : i . decompose ( ) else : i . extract ( ) self . extract ( )", "predictions": ["look through all contents ."], "references": ["recursively destroys the contents of this tree ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2962, "code": "def convert charref ( self , name ) : try : n = int ( name ) except Value Error : return if not 0 <= n <= 127 : return return self . convert codepoint ( n )", "predictions": ["convert a name to a int or a string"], "references": ["this method fixes a bug in python s sgmlparser ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 2963, "code": "def handle charref ( self , ref ) : if self . convert Entities : data = unichr ( int ( ref ) ) else : data = '&#%s;' % ref self . handle data ( data )", "predictions": ["handle reference information from server"], "references": ["handle character references as data ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2964, "code": "def shoebot example ( * * shoebot kwargs ) : def decorator ( f ) : def run ( ) : from shoebot import Shoebot Install Error print ( \"    Shoebot - %s:\" % f . name . replace ( \" \" , \" \" ) ) try : import shoebot outputfile = \"/tmp/shoebot-%s.png\" % f . name bot = shoebot . create bot ( outputfile = outputfile ) f ( bot ) bot . finish ( ) print ( '        [passed] : %s' % outputfile ) print ( '' ) except Shoebot Install Error as e : print ( '        [failed]' , e . args [ 0 ] ) print ( '' ) except Exception : print ( '        [failed] - traceback:' ) for line in traceback . format exc ( ) . splitlines ( ) : print ( '    %s' % line ) print ( '' ) return run return decorator", "predictions": ["run a function that example the shoebot ."], "references": ["decorator to run some code in a bot instance ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 2965, "code": "def get center ( self ) : x = ( self . x + self . width / 2 ) y = ( self . y + self . height / 2 ) return ( x , y )", "predictions": ["return the center of the center of the bounding box"], "references": ["returns the center point of the path disregarding transforms ."], "bleu": 0.2086130724305753, "rouge_l": 0.4}
{"id": 2966, "code": "def scale context and center ( self , cr ) : bot width , bot height = self . bot size if self . width != bot width or self . height != bot height : if self . width < self . height : scale x = float ( self . width ) / float ( bot width ) scale y = scale x cr . translate ( 0 , ( self . height - ( bot height * scale y ) ) / 2.0 ) elif self . width > self . height : scale y = float ( self . height ) / float ( bot height ) scale x = scale y cr . translate ( ( self . width - ( bot width * scale x ) ) / 2.0 , 0 ) else : scale x = 1.0 scale y = 1.0 cr . scale ( scale x , scale y ) self . input device . scale x = scale y self . input device . scale y = scale y", "predictions": ["scale the context and center and center of a context ."], "references": ["scale context based on difference between bot size and widget"], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 2967, "code": "def draw ( self , widget , cr ) : if self . bot size is None : self . draw default image ( cr ) return cr = driver . ensure pycairo context ( cr ) surface = self . backing store . surface cr . set source surface ( surface ) cr . paint ( )", "predictions": ["draw the given widget"], "references": ["draw just the exposed part of the backing store scaled to fit"], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 2968, "code": "def create rcontext ( self , size , frame ) : if self . format == 'pdf' : surface = cairo . PDF Surface ( self . output file ( frame ) , * size ) elif self . format in ( 'ps' , 'eps' ) : surface = cairo . PS Surface ( self . output file ( frame ) , * size ) elif self . format == 'svg' : surface = cairo . SVG Surface ( self . output file ( frame ) , * size ) elif self . format == 'surface' : surface = self . target else : surface = cairo . Image Surface ( cairo . FORMAT ARGB32 , * size ) return cairo . Context ( surface )", "predictions": ["create a cairo image"], "references": ["called when cairocanvas needs a cairo context to draw on"], "bleu": 0.10551173833795614, "rouge_l": 0.26521739130434785}
{"id": 2969, "code": "def rendering finished ( self , size , frame , cairo ctx ) : surface = cairo ctx . get target ( ) if self . format == 'png' : surface . write to png ( self . output file ( frame ) ) surface . finish ( ) surface . flush ( )", "predictions": ["render render file with size and cairo cairo ."], "references": ["called when cairocanvas has rendered a bot"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2970, "code": "def parse ( self ) : p1 = \"\\[.*?\\](.*?)\\[\\/.*?\\]\" p2 = \"\\[(.*?)\\]\" self . links = [ ] for p in ( p1 , p2 ) : for link in re . findall ( p , self . description ) : self . links . append ( link ) self . description = re . sub ( p , \"\\\\1\" , self . description ) self . description = self . description . strip ( )", "predictions": ["parses the template and creates a new one"], "references": ["strips links from the definition and gathers them in a links property ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 2971, "code": "def save as ( self ) : chooser = Shoebot File Chooser Dialog ( ( 'Save File' ) , None , Gtk . File Chooser Action . SAVE , ( Gtk . STOCK SAVE , Gtk . Response Type . ACCEPT , Gtk . STOCK CANCEL , Gtk . Response Type . CANCEL ) ) chooser . set do overwrite confirmation ( True ) chooser . set transient for ( self ) saved = chooser . run ( ) == Gtk . Response Type . ACCEPT if saved : old filename = self . filename self . source buffer . filename = chooser . get filename ( ) if not self . save ( ) : self . filename = old filename chooser . destroy ( ) return saved", "predictions": ["saves a dialog to a dialog box ."], "references": ["return true if the buffer was saved"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2972, "code": "def widget changed ( self , widget , v ) : if v . type is NUMBER : self . bot . namespace [ v . name ] = widget . get value ( ) self . bot . vars [ v . name ] . value = widget . get value ( ) publish event ( VARIABLE UPDATED EVENT , v ) elif v . type is BOOLEAN : self . bot . namespace [ v . name ] = widget . get active ( ) self . bot . vars [ v . name ] . value = widget . get active ( ) publish event ( VARIABLE UPDATED EVENT , v ) elif v . type is TEXT : self . bot . namespace [ v . name ] = widget . get text ( ) self . bot . vars [ v . name ] . value = widget . get text ( ) publish event ( VARIABLE UPDATED EVENT , v )", "predictions": ["handle widget widget changed"], "references": ["called when a slider is adjusted ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 2973, "code": "def parse ( svg , cached = False , copy = True ) : if not cached : dom = parser . parse String ( svg ) paths = parse node ( dom , [ ] ) else : id = cache . id ( svg ) if not cache . has key ( id ) : dom = parser . parse String ( svg ) cache . save ( id , parse node ( dom , [ ] ) ) paths = cache . load ( id , copy ) return paths", "predictions": ["parse a dom dom ."], "references": ["returns cached copies unless otherwise specified ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2974, "code": "def get attribute ( element , attribute , default = 0 ) : a = element . get Attribute ( attribute ) if a == \"\" : return default return a", "predictions": ["get an attribute from an attribute"], "references": ["returns xml element s attribute or default if none ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2975, "code": "def copy ( self , graph ) : e = events ( graph , self . ctx ) e . clicked = self . clicked return e", "predictions": ["return a new instance of this graph ."], "references": ["returns a copy of the event handler remembering the last node clicked ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 2976, "code": "def drag ( self , node ) : dx = self . mouse . x - self . graph . x dy = self . mouse . y - self . graph . y s = self . graph . styles . default self . ctx . nofill ( ) self . ctx . nostroke ( ) if s . stroke : self . ctx . strokewidth ( s . strokewidth ) self . ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . g , 0.75 ) p = self . ctx . line ( node . x , node . y , dx , dy , draw = False ) try : p . ns Bezier Path . set Line Dash count phase ( [ 2 , 4 ] , 2 , 50 ) except : pass self . ctx . drawpath ( p ) r = node . class ( None ) . r * 0.75 self . ctx . oval ( dx - r / 2 , dy - r / 2 , r , r ) node . vx = dx / self . graph . d node . vy = dy / self . graph . d", "predictions": ["drag a id of the . . drag"], "references": ["drags given node to mouse location ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2977, "code": "def hover ( self , node ) : if self . popup == False : return if self . popup == True or self . popup . node != node : if self . popup text . has key ( node . id ) : texts = self . popup text [ node . id ] else : texts = None self . popup = popup ( self . ctx , node , texts ) self . popup . draw ( )", "predictions": ["cliques the threshold for this threshold"], "references": ["displays a popup when hovering over a node ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 2978, "code": "def textpath ( self , i ) : if len ( self . textpaths ) == i : self . ctx . font ( self . font , self . fontsize ) txt = self . q [ i ] if len ( self . q ) > 1 : txt += \" (\" + str ( i + 1 ) + \"/\" + str ( len ( self . q ) ) + \")\" p = self . ctx . textpath ( txt , 0 , 0 , width = self . w ) h = self . ctx . textheight ( txt , width = self . w ) self . textpaths . append ( ( p , h ) ) return self . textpaths [ i ]", "predictions": ["get list of nodes in order"], "references": ["returns a cached textpath of the given text in queue ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 2979, "code": "def update ( self ) : if self . delay > 0 : self . delay -= 1 return if self . fi == 0 : if len ( self . q ) == 1 : self . fn = float ( \"inf\" ) else : self . fn = len ( self . q [ self . i ] ) / self . speed self . fn = max ( self . fn , self . mf ) self . fi += 1 if self . fi > self . fn : self . fi = 0 self . i = ( self . i + 1 ) % len ( self . q )", "predictions": ["updates the spherical spherical and its speed and speed"], "references": ["rotates the queued texts and determines display time ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 2980, "code": "def draw ( self ) : if len ( self . q ) > 0 : self . update ( ) if self . delay == 0 : p , h = self . textpath ( self . i ) f = self . fontsize self . ctx . fill ( self . background ) self . ctx . rect ( self . node . x + f * 1.0 , self . node . y + f * 0.5 , self . w + f , h + f * 1.5 , roundness = 0.2 ) alpha = 1.0 if self . fi < 5 : alpha = 0.2 * self . fi if self . fn - self . fi < 5 : alpha = 0.2 * ( self . fn - self . fi ) self . ctx . fill ( self . text . r , self . text . g , self . text . b , self . text . a * alpha ) self . ctx . translate ( self . node . x + f * 2.0 , self . node . y + f * 2.5 ) self . ctx . drawpath ( p )", "predictions": ["draws the text to the svg"], "references": ["draws a popup rectangle with a rotating text queue ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 2981, "code": "def merge configs ( main , tweaks ) : for section in tweaks . sections ( ) : for option in tweaks . options ( section ) : value = tweaks . get ( section , option ) if option . endswith ( \"+\" ) : option = option [ : - 1 ] value = main . get ( section , option ) + value main . set ( section , option , value )", "predictions": ["find tweaks from tweaks and tweaks dedent % tweaks"], "references": ["merge tweaks into a main config file ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2982, "code": "def usable class name ( node ) : name = node . qname ( ) for prefix in [ \" builtin .\" , \"builtins.\" , \".\" ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name", "predictions": ["get the class self self is a string from its names is used in a node is a class is a dotted is a dotted dotted dotted class is defined in"], "references": ["make a reasonable class name for a class node ."], "bleu": 0.06106432774355542, "rouge_l": 0.2149779735682819}
{"id": 2983, "code": "def parse pylint output ( pylint output ) : for line in pylint output : if not line . strip ( ) : continue if line [ 0 : 5 ] in ( \"-\" * 5 , \"*\" * 5 ) : continue parsed = PYLINT PARSEABLE REGEX . search ( line ) if parsed is None : LOG . warning ( u\"Unable to parse %r. If this is a lint failure, please re-run pylint with the \" u\"--output-format=parseable option, otherwise, you can ignore this message.\" , line ) continue parsed dict = parsed . groupdict ( ) parsed dict [ 'linenum' ] = int ( parsed dict [ 'linenum' ] ) yield Pylint Error ( * * parsed dict )", "predictions": ["close pylint and self ."], "references": ["parse the pylint output - format = parseable lines into pylinterror tuples ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 2984, "code": "def fix pylint ( line , errors ) : if not errors : yield line return current = PYLINT EXCEPTION REGEX . search ( line ) if current : original errors = { disable . strip ( ) for disable in current . group ( 'disables' ) . split ( ',' ) } else : original errors = set ( ) disabled errors = set ( original errors ) for error in errors : if error . error name == 'useless-suppression' : parsed = re . search ( \"\"\"Useless suppression of '(?P<error name>[^']+)'\"\"\" , error . error msg ) disabled errors . discard ( parsed . group ( 'error name' ) ) elif error . error name == 'missing-docstring' and error . error msg == 'Missing module docstring' : yield format pylint disables ( { error . error name } ) . strip ( ) + '\\n' else : disabled errors . add ( error . error name ) disable string = format pylint disables ( disabled errors , not disabled errors <= original errors ) if current : yield PYLINT EXCEPTION REGEX . sub ( disable string , line ) else : yield re . sub ( r'($\\s*)' , disable string + r'\\1' , line , count = 1 )", "predictions": ["replace command characters with module = true"], "references": ["yield any modified versions of line needed to address the errors in errors ."], "bleu": 0.057461663912368725, "rouge_l": 0.0}
{"id": 2985, "code": "def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == \"help\" : show help ( ) return 0 elif argv [ 0 ] == \"check\" : return check main ( argv [ 1 : ] ) elif argv [ 0 ] == \"list\" : return list main ( argv [ 1 : ] ) elif argv [ 0 ] == \"write\" : return write main ( argv [ 1 : ] ) else : print ( u\"Don't understand {!r}\" . format ( \" \" . join ( argv ) ) ) show help ( ) return 1", "predictions": ["execute the program ."], "references": ["the edx_lint command entry point ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 2986, "code": "def show help ( ) : print ( ) for cmd in [ write main , check main , list main ] : print ( cmd . doc . lstrip ( \"\\n\" ) )", "predictions": ["prints math information about the user"], "references": ["print the help string for the edx_lint command ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2987, "code": "def transform ( x ) : try : x = date2num ( x ) except Attribute Error : x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x", "predictions": ["draw a series of items from a list of strings"], "references": ["transform from date to a numerical format"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2988, "code": "def transform ( x ) : try : x = np . array ( [ x . total seconds ( ) * 10 ** 6 for x in x ] ) except Type Error : x = x . total seconds ( ) * 10 ** 6 return x", "predictions": ["sanitize a matrix by a numpy array"], "references": ["transform from timeddelta to numerical format"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2989, "code": "def inverse ( x ) : try : x = [ datetime . timedelta ( microseconds = i ) for i in x ] except Type Error : x = datetime . timedelta ( microseconds = x ) return x", "predictions": ["extract the extract value from the given list of values"], "references": ["transform to timedelta from numerical format"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2990, "code": "def transform ( x ) : try : x = np . array ( [ x . value for x in x ] ) except Type Error : x = x . value return x", "predictions": ["last value but do not have a value"], "references": ["transform from timeddelta to numerical format"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2991, "code": "def inverse ( x ) : try : x = [ pd . Timedelta ( int ( i ) ) for i in x ] except Type Error : x = pd . Timedelta ( int ( x ) ) return x", "predictions": ["find the first item in x"], "references": ["transform to timedelta from numerical format"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2992, "code": "def censor with ( x , range , value = None ) : return [ val if range [ 0 ] <= val <= range [ 1 ] else value for val in x ]", "predictions": ["indents a value with with a given = = 0"], "references": ["censor any values outside of range with none"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 2993, "code": "def best units ( self , sequence ) : ts range = self . value ( max ( sequence ) ) - self . value ( min ( sequence ) ) package = self . determine package ( sequence [ 0 ] ) if package == 'pandas' : cuts = [ ( 0.9 , 'us' ) , ( 0.9 , 'ms' ) , ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = NANOSECONDS base units = 'ns' else : cuts = [ ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = SECONDS base units = 'ms' for size , units in reversed ( cuts ) : if ts range >= size * denomination [ units ] : return units return base units", "predictions": ["get the decompose units for the given contents ."], "references": ["determine good units for representing a sequence of timedeltas"], "bleu": 0.18575057999133596, "rouge_l": 0.2222222222222222}
{"id": 2994, "code": "def scaled limits ( self ) : min = self . limits [ 0 ] / self . factor max = self . limits [ 1 ] / self . factor return min , max", "predictions": ["if the dataset is empty set max will max it"], "references": ["minimum and maximum to use for computing breaks"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2995, "code": "def numeric to timedelta ( self , numerics ) : if self . package == 'pandas' : return [ self . type ( int ( x * self . factor ) , units = 'ns' ) for x in numerics ] else : return [ self . type ( seconds = x * self . factor ) for x in numerics ]", "predictions": ["convert a handle to a timedelta instance ."], "references": ["convert sequence of numerics to timedelta"], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 2996, "code": "def round any ( x , accuracy , f = np . round ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) return f ( x / accuracy ) * accuracy", "predictions": ["round example of * * * * * * * * accuracy * to accuracy * accuracy * run * accuracy * accuracy * accuracy * run * run * accuracy"], "references": ["round to multiple of any number ."], "bleu": 0.046398855339878003, "rouge_l": 0.11879259980525803}
{"id": 2997, "code": "def nearest int ( x ) : if x == 0 : return np . int64 ( 0 ) elif x > 0 : return np . int64 ( x + 0.5 ) else : return np . int64 ( x - 0.5 )", "predictions": ["+ get the get number of center of self ."], "references": ["return nearest long integer to x"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2998, "code": "def format ( formatter , x ) : formatter . create dummy axis ( ) formatter . set locs ( [ val for val in x if ~ np . isnan ( val ) ] ) try : oom = int ( formatter . order Of Magnitude ) except Attribute Error : oom = 0 labels = [ formatter ( tick ) for tick in x ] pattern = re . compile ( r'\\.0+$' ) for i , label in enumerate ( labels ) : match = pattern . search ( label ) if match : labels [ i ] = pattern . sub ( '' , label ) if oom : labels = [ '{}e{}' . format ( s , oom ) if s != '0' else s for s in labels ] return labels", "predictions": ["scale a and scale into a list of arrays"], "references": ["helper to format and tidy up"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2999, "code": "def switch ( template , version ) : temple . update . update ( new template = template , new version = version )", "predictions": ["draw a self if it s already exists"], "references": ["switch a project s template to a different template ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3000, "code": "def in git repo ( ) : ret = temple . utils . shell ( 'git rev-parse' , stderr = subprocess . DEVNULL , check = False ) return ret . returncode == 0", "predictions": ["check if the current repo is create a git repo if it s not create a git if it ."], "references": ["returns true if inside a git repo false otherwise"], "bleu": 0.12021577610863723, "rouge_l": 0.2961165048543689}
{"id": 3001, "code": "def has branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode == 0", "predictions": ["check if a finished finished finished finished finished finished"], "references": ["return true if the target branch exists ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3002, "code": "def not has branch ( branch ) : if has branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . Existing Branch Error ( msg )", "predictions": ["checks if a self self link is still valid"], "references": ["raises existingbrancherror if the specified branch exists ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3003, "code": "def has env vars ( * env vars ) : for env var in env vars : if not os . environ . get ( env var ) : msg = ( 'Must set {} environment variable. View docs for setting up environment at {}' ) . format ( env var , temple . constants . TEMPLE DOCS URL ) raise temple . exceptions . Invalid Environment Error ( msg )", "predictions": ["check if the environment is run . . . . . . . ."], "references": ["raises invalidenvironmenterror when one isnt set"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 3004, "code": "def is temple project ( ) : if not os . path . exists ( temple . constants . TEMPLE CONFIG FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE CONFIG FILE ) raise temple . exceptions . Invalid Temple Project Error ( msg )", "predictions": ["check if a bot is a bot project"], "references": ["raises invalidtempleprojecterror if repository is not a temple project"], "bleu": 0.17795502018438056, "rouge_l": 0.465648854961832}
{"id": 3005, "code": "def get current branch ( ) : result = temple . utils . shell ( 'git rev-parse --abbrev-ref HEAD' , stdout = subprocess . PIPE ) return result . stdout . decode ( 'utf8' ) . strip ( )", "predictions": ["return the current svg"], "references": ["determine the current git branch"], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 3006, "code": "def apply template ( template , target , * , checkout , extra context ) : with tempfile . Temporary Directory ( ) as tempdir : repo dir = cc main . cookiecutter ( template , checkout = checkout , no input = True , output dir = tempdir , extra context = extra context ) for item in os . listdir ( repo dir ) : src = os . path . join ( repo dir , item ) dst = os . path . join ( target , item ) if os . path . isdir ( src ) : if os . path . exists ( dst ) : shutil . rmtree ( dst ) shutil . copytree ( src , dst ) else : if os . path . exists ( dst ) : os . remove ( dst ) shutil . copy2 ( src , dst )", "predictions": ["get a attribute from a attribute if it exists if not already ."], "references": ["apply a template to a temporary directory and then copy results to target ."], "bleu": 0.10518320506876351, "rouge_l": 0.2207478890229192}
{"id": 3007, "code": "def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin = stdin , stdout = stdout , stderr = stderr )", "predictions": ["run a copy of the given graph"], "references": ["runs a subprocess shell with check = true by default"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3008, "code": "def read temple config ( ) : with open ( temple . constants . TEMPLE CONFIG FILE ) as temple config file : return yaml . load ( temple config file , Loader = yaml . Safe Loader )", "predictions": ["read the configuration from the yaml file ."], "references": ["reads the temple yaml configuration file in the repository"], "bleu": 0.1862539773562041, "rouge_l": 0.34923664122137404}
{"id": 3009, "code": "def write temple config ( temple config , template , version ) : with open ( temple . constants . TEMPLE CONFIG FILE , 'w' ) as temple config file : versioned config = { * * temple config , * * { ' version' : version , ' template' : template } , } yaml . dump ( versioned config , temple config file , Dumper = yaml . Safe Dumper )", "predictions": ["write the configuration to a yaml file ."], "references": ["writes the temple yaml configuration"], "bleu": 0.19070828081828378, "rouge_l": 0.32105263157894737}
{"id": 3010, "code": "def set cmd env var ( value ) : def func decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , * * kwargs ) : previous cmd env var = os . getenv ( temple . constants . TEMPLE ENV VAR ) os . environ [ temple . constants . TEMPLE ENV VAR ] = value try : ret val = function ( * args , * * kwargs ) finally : if previous cmd env var is None : del os . environ [ temple . constants . TEMPLE ENV VAR ] else : os . environ [ temple . constants . TEMPLE ENV VAR ] = previous cmd env var return ret val return wrapper return func decorator", "predictions": ["decorator to set env variables on function ."], "references": ["decorator that sets the temple command env var to value"], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 3011, "code": "def run ( self ) : filename = \".DS Store\" command = \"find {path} -type f -name \\\"{filename}\\\" \" . format ( path = self . path , filename = filename ) cmd = Command Helper ( command ) cmd . execute ( ) files = cmd . output . split ( \"\\n\" ) for f in files : if not f . endswith ( filename ) : continue rel path = f . replace ( self . path , \"\" ) if rel path . startswith ( tuple ( self . CONFIG [ 'exclude paths' ] ) ) : continue issue = Issue ( ) issue . name = \"File .DS Store detected\" issue . potential = False issue . severity = Issue . SEVERITY LOW issue . file = rel path self . save Issue ( issue )", "predictions": ["run the issue ."], "references": ["finds . ds_store files into path"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3012, "code": "def setup ( ) : if not os . path . isdir ( Atom Shields Scanner . CHECKERS DIR ) : os . makedirs ( Atom Shields Scanner . CHECKERS DIR ) if not os . path . isdir ( Atom Shields Scanner . REPORTS DIR ) : os . makedirs ( Atom Shields Scanner . REPORTS DIR ) for f in Atom Shields Scanner . get Files ( os . path . join ( os . path . dirname ( os . path . realpath ( file ) ) , \"checkers\" ) , \"*.py\" ) : Atom Shields Scanner . install Checker ( f ) for f in Atom Shields Scanner . get Files ( os . path . join ( os . path . dirname ( os . path . realpath ( file ) ) , \"reports\" ) , \"*.py\" ) : Atom Shields Scanner . install Report ( f ) Atom Shields Scanner . execute Massive Method ( path = Atom Shields Scanner . CHECKERS DIR , method = \"install\" , args = { } ) config dir = os . path . dirname ( Atom Shields Scanner . CONFIG PATH ) if not os . path . isdir ( config dir ) : os . makedirs ( config dir )", "predictions": ["setup the package and shields ."], "references": ["creates required directories and copy checkers and reports ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3013, "code": "def run ( self ) : self . check Properties ( ) self . debug ( \"[*] Iniciando escaneo de Atom Shields con las siguientes propiedades. . . \" ) self . show Scan Properties ( ) self . load Config ( ) init ts = datetime . now ( ) cwd = os . getcwd ( ) os . chdir ( self . path ) issues = self . execute Checkers ( ) os . chdir ( cwd ) end ts = datetime . now ( ) duration = '{}' . format ( end ts - init ts ) for plugin in issues . keys ( ) : value = issues [ plugin ] if isinstance ( value , list ) : map ( self . save Issue , value ) else : self . save Issue ( value ) print \"\" self . execute Reports ( ) self . debug ( \"\" ) self . debug ( \"Duration: {t}\" . format ( t = duration ) ) self . show Summary ( ) return self . issues", "predictions": ["execute the issues ."], "references": ["run a scan in the path setted ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 3014, "code": "def install ( ) : cmd = Command Helper ( ) cmd . install ( \"npm\" ) cmd = Command Helper ( ) cmd . install ( \"nodejs-legacy\" ) cmd = Command Helper ( ) cmd . command = \"npm install -g retire\" cmd . execute ( ) if cmd . errors : from termcolor import colored print colored ( cmd . errors , \"red\" ) else : print cmd . output", "predictions": ["install legit . ini ."], "references": ["install all the dependences"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3015, "code": "def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit depth if not self . configured : log . debug ( \"etcd not available\" ) return if self . watching : log . info ( \"Starting watcher for %r\" , prefix ) self . start watching ( ) log . info ( \"Loading from etcd %r\" , prefix ) try : result = self . client . get ( prefix ) except self . module . Etcd Key Not Found : result = None if not result : log . info ( \"No configuration found\" ) return { } update = { } for item in result . children : key = item . key value = item . value try : value = pytool . json . from json ( value ) except : pass if not self . case sensitive : key = key . lower ( ) if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] update [ key ] = value inherited = Config ( ) . settings . get ( self . inherit key , update . get ( self . inherit key , None ) ) if depth > 0 and inherited : log . info ( \"    ... inheriting ...\" ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update", "predictions": ["load configuration from klf 200 ."], "references": ["return a dictionary of settings loaded from etcd ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3016, "code": "def get watcher ( self ) : if not self . watching : raise Stop Iteration ( ) return self . client . eternal watch ( self . prefix , recursive = True )", "predictions": ["returns the lock for this server ."], "references": ["return a etcd watching generator which yields events as they happen ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 3017, "code": "def start watching ( self ) : if self . watcher and self . watcher . is alive ( ) : return self . watcher = Watcher ( ) self . watcher . start ( )", "predictions": ["start the watching watching"], "references": ["begins watching etcd for changes ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3018, "code": "def main ( ) : parser = argparse . Argument Parser ( description = \"Helper for working with \" \"pyconfigs\" ) target group = parser . add mutually exclusive group ( ) target group . add argument ( '-f' , '--filename' , help = \"parse an individual file or directory\" , metavar = 'F' ) target group . add argument ( '-m' , '--module' , help = \"parse a package or module, recursively looking inside it\" , metavar = 'M' ) parser . add argument ( '-v' , '--view-call' , help = \"show the actual pyconfig call made (default: show namespace)\" , action = 'store true' ) parser . add argument ( '-l' , '--load-configs' , help = \"query the currently set value for each key found\" , action = 'store true' ) key group = parser . add mutually exclusive group ( ) key group . add argument ( '-a' , '--all' , help = \"show keys which don't have defaults set\" , action = 'store true' ) key group . add argument ( '-k' , '--only-keys' , help = \"show a list of discovered keys without values\" , action = 'store true' ) parser . add argument ( '-n' , '--natural-sort' , help = \"sort by filename and line (default: alphabetical by key)\" , action = 'store true' ) parser . add argument ( '-s' , '--source' , help = \"show source annotations (implies --natural-sort)\" , action = 'store true' ) parser . add argument ( '-c' , '--color' , help = \"toggle output colors (default: %s)\" % bool ( pygments ) , action = 'store const' , default = bool ( pygments ) , const = ( not bool ( pygments ) ) ) args = parser . parse args ( ) if args . color and not pygments : error ( \"Pygments is required for color output.\\n\" \"    pip install pygments\" ) if args . module : handle module ( args ) if args . filename : handle file ( args )", "predictions": ["parse arguments and run the main program ."], "references": ["main script for pyconfig command ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 3019, "code": "def handle module ( args ) : module = get module filename ( args . module ) if not module : error ( \"Could not load module or package: %r\" , args . module ) elif isinstance ( module , Unparseable ) : error ( \"Could not determine module source: %r\" , args . module ) parse and output ( module , args )", "predictions": ["handles a module ."], "references": ["handles the - m argument ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 3020, "code": "def colorize ( output ) : if not pygments : return output return pygments . highlight ( output , pygments . lexers . Python Lexer ( ) , pygments . formatters . Terminal256Formatter ( style = 'monokai' ) )", "predictions": ["highlight the pygments of the pygments layer ."], "references": ["return output colorized with pygments if available ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 3021, "code": "def map arg ( arg ) : if isinstance ( arg , ast . Str ) : return repr ( arg . s ) elif isinstance ( arg , ast . Num ) : return arg . n elif isinstance ( arg , ast . Name ) : name = arg . id if name == 'True' : return True elif name == 'False' : return False elif name == 'None' : return None return name else : return Unparseable ( )", "predictions": ["map an argument to a string"], "references": ["return arg appropriately parsed or mapped to a usable value ."], "bleu": 0.12634437832866913, "rouge_l": 0.2234432234432234}
{"id": 3022, "code": "def as call ( self ) : default = self . default ( ) default = ', ' + default if default else '' return \"pyconfig.%s(%r%s)\" % ( self . method , self . get key ( ) , default )", "predictions": ["return the object as a string"], "references": ["return this call as it is called in its source ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3023, "code": "def get key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col offset : ] regex = re . compile ( '''pyconfig\\.[eginst]+\\(([^,]+).*?\\)''' ) match = regex . match ( line ) if not match : return Unparseable ( ) return \"<%s>\" % match . group ( 1 )", "predictions": ["return the unparseable for the key"], "references": ["return the call key even if it has to be parsed from the source ."], "bleu": 0.07370355832749997, "rouge_l": 0.26521739130434785}
{"id": 3024, "code": "def default value only ( self ) : line = self . source [ self . col offset : ] regex = re . compile ( '''pyconfig\\.[eginst]+\\(['\"][^)]+?['\"], ?(.*?)\\)''' ) match = regex . match ( line ) if not match : return '' return match . group ( 1 )", "predictions": ["return the default value only if it s a default value only one ."], "references": ["return only the default value if there is one ."], "bleu": 0.21651956746181053, "rouge_l": 0.6014084507042253}
{"id": 3025, "code": "def default ( self ) : try : iter ( self . default ) except Type Error : return repr ( self . default ) for v in self . default : if isinstance ( v , Unparseable ) : default = self . default value only ( ) if default : return default return ', ' . join ( str ( v ) for v in self . default )", "predictions": ["return the default value for this field ."], "references": ["return the default argument formatted nicely ."], "bleu": 0.3155984539112945, "rouge_l": 0.5398230088495575}
{"id": 3026, "code": "def get param names ( self ) : template = Template ( self . yaml string ) names = [ 'yaml string' ] for match in re . finditer ( template . pattern , template . template ) : name = match . group ( 'named' ) or match . group ( 'braced' ) assert name is not None names . append ( name ) return names", "predictions": ["get a list of all the names of all the template names"], "references": ["get mappable parameters from yaml ."], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 3027, "code": "def load ( self ) : from pylearn2 . config import yaml parse from pylearn2 . datasets import Dataset dataset = yaml parse . load ( self . yaml string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num batches = 1 , data specs = dataset . data specs , return tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one hot : y = np . argmax ( y , axis = 1 ) else : X = data y = None return X , y", "predictions": ["load the data from the file ."], "references": ["load the dataset using pylearn2 . config . yaml_parse ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 3028, "code": "def create kernel ( self ) : kernels = self . kernel params if not isinstance ( kernels , list ) : raise Runtime Error ( 'Must provide enumeration of kernels' ) for kernel in kernels : if sorted ( list ( kernel . keys ( ) ) ) != [ 'name' , 'options' , 'params' ] : raise Runtime Error ( 'strategy/params/kernels must contain keys: \"name\", \"options\", \"params\"' ) kernels = [ ] for kern in self . kernel params : params = kern [ 'params' ] options = kern [ 'options' ] name = kern [ 'name' ] kernel ep = load entry point ( name , 'strategy/params/kernels' ) if issubclass ( kernel ep , KERNEL BASE CLASS ) : if options [ 'independent' ] : kernel = np . sum ( [ kernel ep ( 1 , active dims = [ i ] , * * params ) for i in range ( self . n dims ) ] ) else : kernel = kernel ep ( self . n dims , * * params ) if not isinstance ( kernel , KERNEL BASE CLASS ) : raise Runtime Error ( 'strategy/params/kernel must load a' 'G Py derived Kernel' ) kernels . append ( kernel ) self . kernel = np . sum ( kernels )", "predictions": ["create the kernel kernel"], "references": ["creates an additive kernel"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 3029, "code": "def assert all finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'All Float' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise Value Error ( \"Input contains Na N, infinity\" \" or a value too large for %r.\" % X . dtype )", "predictions": ["assert that all finite attributes are finite"], "references": ["like assert_all_finite but only for ndarray ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3030, "code": "def warn if not finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'All Float' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : warnings . warn ( \"Result contains Na N, infinity\" \" or a value too large for %r.\" % X . dtype , category = User Warning )", "predictions": ["samples samples are finite if they are finite ."], "references": ["userwarning if array contains non - finite elements"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 3031, "code": "def num samples ( x , is nested = False ) : if hasattr ( x , 'fit' ) : raise Type Error ( 'Expected sequence or array-like, got ' 'estimator %s' % x ) if is nested : return sum ( num samples ( xx , is nested = False ) for xx in x ) if not hasattr ( x , ' len ' ) and not hasattr ( x , 'shape' ) : if hasattr ( x , ' array ' ) : x = np . asarray ( x ) else : raise Type Error ( \"Expected sequence or array-like, got %s\" % type ( x ) ) if hasattr ( x , 'shape' ) : if len ( x . shape ) == 0 : raise Type Error ( \"Singleton array %r cannot be considered\" \" a valid collection.\" % x ) return x . shape [ 0 ] else : return len ( x )", "predictions": ["get the number of samples in a array of array x ."], "references": ["return number of samples in array - like x ."], "bleu": 0.3438931217657843, "rouge_l": 0.646969696969697}
{"id": 3032, "code": "def fromdict ( cls , config , check fields = True ) : m = super ( Config , cls ) . new ( cls ) m . path = '.' m . verbose = False m . config = m . merge defaults ( config ) if check fields : m . check fields ( ) return m", "predictions": ["pick a new model instance from the config file ."], "references": ["create a config object from config dict directly ."], "bleu": 0.15851165692617156, "rouge_l": 0.42508710801393734}
{"id": 3033, "code": "def sha1 ( self ) : with open ( self . path , 'rb' ) as f : return hashlib . sha1 ( f . read ( ) ) . hexdigest ( )", "predictions": ["the sha1 of the data ."], "references": ["sha1 hash of the config file itself ."], "bleu": 0.236682065782701, "rouge_l": 0.5570776255707762}
{"id": 3034, "code": "def plot 3 ( data , ss , * args ) : if len ( data ) <= 1 : warnings . warn ( \"Only one datapoint. Could not compute t-SNE embedding.\" ) return None scores = np . array ( [ d [ 'mean test score' ] for d in data ] ) warped = np . array ( [ ss . point to unit ( d [ 'parameters' ] ) for d in data ] ) X = TSNE ( n components = 2 ) . fit transform ( warped ) e scores = np . exp ( scores ) mine , maxe = np . min ( e scores ) , np . max ( e scores ) color = ( e scores - mine ) / ( maxe - mine ) mapped colors = list ( map ( rgb2hex , cm . get cmap ( 'Rd Bu r' ) ( color ) ) ) p = bk . figure ( title = 't-SNE (unsupervised)' , tools = TOOLS ) df params = nonconstant parameters ( data ) df params [ 'score' ] = scores df params [ 'x' ] = X [ : , 0 ] df params [ 'y' ] = X [ : , 1 ] df params [ 'color' ] = mapped colors df params [ 'radius' ] = 1 p . circle ( x = 'x' , y = 'y' , color = 'color' , radius = 'radius' , source = Column Data Source ( data = df params ) , fill alpha = 0.6 , line color = None ) cp = p hover = cp . select ( dict ( type = Hover Tool ) ) format tt = [ ( s , '@%s' % s ) for s in df params . columns ] hover . tooltips = Ordered Dict ( [ ( \"index\" , \"$index\" ) ] + format tt ) xax , yax = p . axis xax . axis label = 't-SNE coord 1' yax . axis label = 't-SNE coord 2' return p", "predictions": ["plot the 3 3 3 3 . x y"], "references": ["t - sne embedding of the parameters colored by score"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 3035, "code": "def plot 4 ( data , * args ) : params = nonconstant parameters ( data ) scores = np . array ( [ d [ 'mean test score' ] for d in data ] ) order = np . argsort ( scores ) for key in params . keys ( ) : if params [ key ] . dtype == np . dtype ( 'bool' ) : params [ key ] = params [ key ] . astype ( np . int ) p list = [ ] for key in params . keys ( ) : x = params [ key ] [ order ] y = scores [ order ] params = params . loc [ order ] try : radius = ( np . max ( x ) - np . min ( x ) ) / 100.0 except : print ( \"error making plot4 for '%s'\" % key ) continue p list . append ( build scatter tooltip ( x = x , y = y , radius = radius , add line = False , tt = params , xlabel = key , title = 'Score vs %s' % key ) ) return p list", "predictions": ["plot the 4 parameters"], "references": ["scatter plot of score vs each param"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 3036, "code": "def get ip packet ( data , client port , server port , is loopback = False ) : header = loopback if is loopback else ethernet try : header . unpack ( data ) except Exception as ex : raise Value Error ( 'Bad header: %s' % ex ) tcp p = getattr ( header . data , 'data' , None ) if type ( tcp p ) != dpkt . tcp . TCP : raise Value Error ( 'Not a TCP packet' ) if tcp p . dport == server port : if client port != 0 and tcp p . sport != client port : raise Value Error ( 'Request from different client' ) elif tcp p . sport == server port : if client port != 0 and tcp p . dport != client port : raise Value Error ( 'Reply for different client' ) else : raise Value Error ( 'Packet not for/from client/server' ) return header . data", "predictions": ["get the ip packet from the dpkt data ."], "references": ["if client_port is 0 any client_port is good"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3037, "code": "def report ( self ) : self . output . write ( '\\r' ) sort by = 'avg' results = { } for key , latencies in self . latencies by method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . output . write ( '%s\\n' % tabulate ( data , headers = headers ) ) self . output . flush ( )", "predictions": ["print a report of the user - supplied output results ."], "references": ["get stats & show them"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3038, "code": "def of structs ( cls , a , b ) : t diff = Thrift Diff ( a , b ) t diff . do diff ( ) return t diff", "predictions": ["calculate difference between a and b ."], "references": ["diff two thrift structs and return the result as a thriftdiff instance"], "bleu": 0.10063351655856649, "rouge_l": 0.10049423393739704}
{"id": 3039, "code": "def read ( cls , data , protocol = None , fallback protocol = T Binary Protocol , finagle thrift = False , max fields = MAX FIELDS , max list size = MAX LIST SIZE , max map size = MAX MAP SIZE , max set size = MAX SET SIZE , read values = False ) : if len ( data ) < cls . MIN MESSAGE SIZE : raise Value Error ( 'not enough data' ) if protocol is None : protocol = cls . detect protocol ( data , fallback protocol ) trans = T Transport . T Memory Buffer ( data ) proto = protocol ( trans ) # header = None if finagle thrift : try : header = Thrift Struct . read ( proto , max fields , max list size , max map size , max set size , read values ) except : trans = T Transport . T Memory Buffer ( data ) proto = protocol ( trans ) method , mtype , seqid = proto . read Message Begin ( ) mtype = cls . message type to str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise Value Error ( 'no method name' ) if len ( method ) > cls . MAX METHOD LENGTH : raise Value Error ( 'method name too long' ) valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise Value Error ( 'invalid method name' % method ) args = Thrift Struct . read ( proto , max fields , max list size , max map size , max set size , read values ) proto . read Message End ( ) msglen = trans . buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen", "predictions": ["read a single chunk from a given input stream ."], "references": ["tries to deserialize a message might fail if data is missing"], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 3040, "code": "def pop ( self , nbytes ) : size = 0 popped = [ ] with self . lock packets : while size < nbytes : try : packet = self . packets . pop ( 0 ) size += len ( packet . data . data ) self . remaining -= len ( packet . data . data ) popped . append ( packet ) except Index Error : break return popped", "predictions": ["pops a given = none return file file file file file file file file file file file off the list"], "references": ["pops packets with _at least_ nbytes of payload"], "bleu": 0.06108557268562171, "rouge_l": 0.07741116751269035}
{"id": 3041, "code": "def pop data ( self , nbytes ) : last timestamp = 0 data = [ ] for packet in self . pop ( nbytes ) : last timestamp = packet . timestamp data . append ( packet . data . data ) return '' . join ( data ) , last timestamp", "predictions": ["write temple temple temple temple temple to the given template as possible as possible as possible as possible as possible as the given template as a string as possible as possible"], "references": ["similar to pop but returns payload + last timestamp"], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 3042, "code": "def push ( self , ip packet ) : data len = len ( ip packet . data . data ) seq id = ip packet . data . seq if data len == 0 : self . next seq id = seq id return False if self . next seq id != - 1 and seq id != self . next seq id : return False self . next seq id = seq id + data len with self . lock packets : self . length += len ( ip packet . data . data ) self . remaining += len ( ip packet . data . data ) self . packets . append ( ip packet ) return True", "predictions": ["set the sequence of ip to the sequence of ip @ @ @"], "references": ["push the packet into the queue"], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 3043, "code": "def run ( self , * args , * * kwargs ) : while True : try : timestamp , ip p = self . queue . popleft ( ) src ip = get ip ( ip p , ip p . src ) dst ip = get ip ( ip p , ip p . dst ) src = intern ( '%s:%s' % ( src ip , ip p . data . sport ) ) dst = intern ( '%s:%s' % ( dst ip , ip p . data . dport ) ) key = intern ( '%s<->%s' % ( src , dst ) ) stream = self . streams . get ( key ) if stream is None : stream = Stream ( src , dst ) self . streams [ key ] = stream setattr ( ip p , 'timestamp' , timestamp ) pushed = stream . push ( ip p ) if not pushed : continue for handler in self . handlers : try : handler ( stream ) except Exception as ex : print ( 'handler exception: %s' % ex ) except Exception : time . sleep ( 0.00001 )", "predictions": ["run a single f ."], "references": ["deal with the incoming packets"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3044, "code": "def read config ( self , filename = None ) : if filename : self . config filename = filename else : try : import appdirs except Import Error : raise Exception ( \"Missing dependency for determining config path. Please install \" \"the 'appdirs' Python module.\" ) self . config filename = appdirs . user config dir ( LIBRARY NAME , \"Profit Bricks\" ) + \".ini\" if not self . config : self . config = configparser . Config Parser ( ) self . config . optionxform = str self . config . read ( self . config filename )", "predictions": ["setup the config config ."], "references": ["read the user configuration"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3045, "code": "def save config ( self , filename = None ) : if filename is None : filename = self . config filename parent path = os . path . dirname ( filename ) if not os . path . isdir ( parent path ) : os . makedirs ( parent path ) with open ( filename , \"w\" ) as configfile : self . config . write ( configfile )", "predictions": ["run the config de - . config"], "references": ["save the given user configuration ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3046, "code": "def create datacenter ( self , datacenter ) : server items = [ ] volume items = [ ] lan items = [ ] loadbalancer items = [ ] entities = dict ( ) properties = { \"name\" : datacenter . name } if datacenter . location : properties [ 'location' ] = datacenter . location if datacenter . description : properties [ 'description' ] = datacenter . description if datacenter . servers : for server in datacenter . servers : server items . append ( self . create server dict ( server ) ) servers = { \"items\" : server items } server entities = { \"servers\" : servers } entities . update ( server entities ) if datacenter . volumes : for volume in datacenter . volumes : volume items . append ( self . create volume dict ( volume ) ) volumes = { \"items\" : volume items } volume entities = { \"volumes\" : volumes } entities . update ( volume entities ) if datacenter . loadbalancers : for loadbalancer in datacenter . loadbalancers : loadbalancer items . append ( self . create loadbalancer dict ( loadbalancer ) ) loadbalancers = { \"items\" : loadbalancer items } loadbalancer entities = { \"loadbalancers\" : loadbalancers } entities . update ( loadbalancer entities ) if datacenter . lans : for lan in datacenter . lans : lan items . append ( self . create lan dict ( lan ) ) lans = { \"items\" : lan items } lan entities = { \"lans\" : lans } entities . update ( lan entities ) if not entities : raw = { \"properties\" : properties , } else : raw = { \"properties\" : properties , \"entities\" : entities } data = json . dumps ( raw ) response = self . perform request ( url = '/datacenters' , method = 'POST' , data = data ) return response", "predictions": ["install the datacenter and return api endpoint import into the datacenter import"], "references": ["creates a data center -- both simple and complex are supported ."], "bleu": 0.10390302174233558, "rouge_l": 0.08333333333333333}
{"id": 3047, "code": "def update image ( self , image id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . underscore to camelcase ( attr ) ] = value response = self . perform request ( url = '/images/' + image id , method = 'PATCH' , data = json . dumps ( data ) ) return response", "predictions": ["load an image for a given image"], "references": ["replace all properties of an image ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 3048, "code": "def reserve ipblock ( self , ipblock ) : properties = { \"name\" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { \"properties\" : properties , } response = self . perform request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response", "predictions": ["get get watcher not including watcher . . ."], "references": ["reserves an ip block within your account ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3049, "code": "def underscore to camelcase ( value ) : def camelcase ( ) : yield str . lower while True : yield str . capitalize c = camelcase ( ) return \"\" . join ( next ( c ) ( x ) if x else ' ' for x in value . split ( \" \" ) )", "predictions": ["convert string to camelcase"], "references": ["convert python snake case back to mixed case ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 3050, "code": "def get Server Info ( pbclient = None , dc id = None ) : if pbclient is None : raise Value Error ( \"argument 'pbclient' must not be None\" ) if dc id is None : raise Value Error ( \"argument 'dc id' must not be None\" ) server info = [ ] servers = pbclient . list servers ( dc id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state' ] , vmstate = props [ 'vm State' ] ) server info . append ( info ) return server info", "predictions": ["main api to retrieve server information"], "references": ["gets info of servers of a data center"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 3051, "code": "def get Server States ( pbclient = None , dc id = None , serverid = None , servername = None ) : if pbclient is None : raise Value Error ( \"argument 'pbclient' must not be None\" ) if dc id is None : raise Value Error ( \"argument 'dc id' must not be None\" ) server = None if serverid is None : if servername is None : raise Value Error ( \"one of 'serverid' or 'servername' must be specified\" ) server info = select where ( get Server Info ( pbclient , dc id ) , [ 'id' , 'name' , 'state' , 'vmstate' ] , name = servername ) if len ( server info ) > 1 : raise Name Error ( \"ambiguous server name '{}'\" . format ( servername ) ) if len ( server info ) == 1 : server = server info [ 0 ] else : try : server info = pbclient . get server ( dc id , serverid , 1 ) server = dict ( id = server info [ 'id' ] , name = server info [ 'properties' ] [ 'name' ] , state = server info [ 'metadata' ] [ 'state' ] , vmstate = server info [ 'properties' ] [ 'vm State' ] ) except Exception : ex = sys . exc info ( ) [ 1 ] if ex . args [ 0 ] is not None and ex . args [ 0 ] == 404 : print ( \"Server w/ ID {} not found\" . format ( serverid ) ) server = None else : raise ex return server", "predictions": ["handle mysql mysql server"], "references": ["gets states of a server"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3052, "code": "def get dc network ( pbclient , dc = None ) : if pbclient is None : raise Value Error ( \"argument 'pbclient' must not be None\" ) if dc is None : raise Value Error ( \"argument 'dc' must not be None\" ) print ( \"getting networks..\" ) dcid = dc [ 'id' ] dc data = [ dcid , dc [ 'properties' ] [ 'name' ] , dc [ 'properties' ] [ 'location' ] ] lbs = pbclient . list loadbalancers ( dcid , 2 ) lbnames = dict ( [ ( lb [ 'id' ] , lb [ 'properties' ] [ 'name' ] ) for lb in lbs [ 'items' ] ] ) if verbose > 2 : print ( \"L Bs: %s\" % ( str ( lbs ) ) ) lans = pbclient . list lans ( dcid , 3 ) lan inv = [ ] servernames = dict ( ) for lan in lans [ 'items' ] : if verbose > 1 : print ( \"LAN: %s\" % str ( lan ) ) lan data = dc data + [ \"LAN \" + lan [ 'id' ] , lan [ 'properties' ] [ 'name' ] , lan [ 'properties' ] [ 'public' ] , lan [ 'metadata' ] [ 'state' ] ] nics = lan [ 'entities' ] [ 'nics' ] [ 'items' ] lan data . append ( len ( nics ) ) if nics : for nic in nics : nic props = nic [ 'properties' ] serverid = re . sub ( r'^.*servers/([^/]+)/nics.*' , r'\\1' , nic [ 'href' ] ) if serverid in lbnames : servertype = \"LB\" servername = lbnames [ serverid ] print ( \"server entry for %s is LOADBALANCER %s\" % ( serverid , servername ) ) else : servertype = \"Server\" if serverid not in servernames : if verbose : print ( \"add server entry for %s\" % serverid ) server = pbclient . get server ( dcid , serverid , 0 ) servernames [ serverid ] = server [ 'properties' ] [ 'name' ] servername = servernames [ serverid ] ips = [ str ( ip ) for ip in nic props [ 'ips' ] ] nic data = [ nic [ 'id' ] , nic props [ 'mac' ] , nic props [ 'dhcp' ] , ips , nic props [ 'name' ] , nic props [ 'firewall Active' ] , servertype , serverid , servername ] lan inv . append ( lan data + nic data ) else : lan inv . append ( lan data ) return lan inv", "predictions": ["colorize a list of known output of a user"], "references": ["gets inventory of one data center"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3053, "code": "def get self ( session , user details = None ) : if user details : user details [ 'compact' ] = True response = make get request ( session , 'self' , params data = user details ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Self Not Retrieved Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["query the api for a given if there is a if not available"], "references": ["get details about the currently authenticated user"], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 3054, "code": "def get user by id ( session , user id , user details = None ) : if user details : user details [ 'compact' ] = True response = make get request ( session , 'users/{}' . format ( user id ) , params data = user details ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise User Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["as internal api helper function for details about a call"], "references": ["get details about specific user"], "bleu": 0.16590387014219712, "rouge_l": 0.2837209302325582}
{"id": 3055, "code": "def get self user id ( session ) : response = make get request ( session , 'self' ) if response . status code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise User Id Not Retrieved Exception ( 'Error retrieving user id: %s' % response . text , response . text )", "predictions": ["get user self source"], "references": ["get the currently authenticated user id"], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 3056, "code": "def add user jobs ( session , job ids ) : jobs data = { 'jobs[]' : job ids } response = make post request ( session , 'self/jobs' , json data = jobs data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise User Jobs Not Added Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["default only for a value of a value in a value"], "references": ["add a list of jobs to the currently authenticated user"], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 3057, "code": "def delete user jobs ( session , job ids ) : jobs data = { 'jobs[]' : job ids } response = make delete request ( session , 'self/jobs' , json data = jobs data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise User Jobs Not Deleted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["default default self = 1 = 0 = 0 = 1 = 0 = 1 = 0 = 1 ."], "references": ["remove a list of jobs from the currently authenticated user"], "bleu": 0.051366639095059514, "rouge_l": 0.0}
{"id": 3058, "code": "def get users ( session , query ) : response = make get request ( session , 'users' , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Users Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["get all param from query"], "references": ["get one or more users"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3059, "code": "def create hireme project ( session , title , description , currency , budget , jobs , hireme initial bid ) : jobs . append ( create job object ( id = 417 ) ) project data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme initial bid' : hireme initial bid } response = make post request ( session , 'projects' , json data = project data ) json data = response . json ( ) if response . status code == 200 : project data = json data [ 'result' ] p = Project ( project data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo url ) return p else : raise Project Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] , )", "predictions": ["creates a self = self = 0 = 1 = 0 = 0 = 1 = 0 = 1 = 1 = 1 = 0 = 1 = 1 = 1"], "references": ["create a fixed project"], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 3060, "code": "def get projects ( session , query ) : response = make get request ( session , 'projects' , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Projects Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["list all kernel kernel"], "references": ["get one or more projects"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 3061, "code": "def get project by id ( session , project id , project details = None , user details = None ) : query = { } if project details : query . update ( project details ) if user details : query . update ( user details ) response = make get request ( session , 'projects/{}' . format ( project id ) , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Projects Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["assert a all all the all data from a all all the all the all accounts"], "references": ["get a single project by id"], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 3062, "code": "def search projects ( session , query , search filter = None , project details = None , user details = None , limit = 10 , offset = 0 , active only = None ) : search data = { 'query' : query , 'limit' : limit , 'offset' : offset , } if search filter : search data . update ( search filter ) if project details : search data . update ( project details ) if user details : search data . update ( user details ) endpoint = 'projects/{}' . format ( 'active' if active only else 'all' ) response = make get request ( session , endpoint , params data = search data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Projects Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["warn for if a warn warn is set to a list of if they are not loaded"], "references": ["search for all projects"], "bleu": 0.07223943354597204, "rouge_l": 0.10720562390158171}
{"id": 3063, "code": "def place project bid ( session , project id , bidder id , description , amount , period , milestone percentage ) : bid data = { 'project id' : project id , 'bidder id' : bidder id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone percentage' : milestone percentage , } response = make post request ( session , 'bids' , json data = bid data ) json data = response . json ( ) if response . status code == 200 : bid data = json data [ 'result' ] return Bid ( bid data ) else : raise Bid Not Placed Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["num to num with bid"], "references": ["place a bid on a project"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3064, "code": "def get bids ( session , project ids = [ ] , bid ids = [ ] , limit = 10 , offset = 0 ) : get bids data = { } if bid ids : get bids data [ 'bids[]' ] = bid ids if project ids : get bids data [ 'projects[]' ] = project ids get bids data [ 'limit' ] = limit get bids data [ 'offset' ] = offset response = make get request ( session , 'bids' , params data = get bids data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Bids Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["get for bids ."], "references": ["get the list of bids"], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 3065, "code": "def get milestones ( session , project ids = [ ] , milestone ids = [ ] , user details = None , limit = 10 , offset = 0 ) : get milestones data = { } if milestone ids : get milestones data [ 'milestones[]' ] = milestone ids if project ids : get milestones data [ 'projects[]' ] = project ids get milestones data [ 'limit' ] = limit get milestones data [ 'offset' ] = offset if user details : get milestones data . update ( user details ) response = make get request ( session , 'milestones' , params data = get milestones data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Milestones Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["sha1 the milestones of a with the given milestone"], "references": ["get the list of milestones"], "bleu": 0.16784459625186196, "rouge_l": 0.3012345679012346}
{"id": 3066, "code": "def get milestone by id ( session , milestone id , user details = None ) : endpoint = 'milestones/{}' . format ( milestone id ) response = make get request ( session , endpoint , params data = user details ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Milestones Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["plot a 3 3 3 3"], "references": ["get a specific milestone"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 3067, "code": "def award project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'award' } endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Awarded Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["obtain a bid based on the bid"], "references": ["award a bid on a project"], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 3068, "code": "def revoke project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'revoke' } endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Revoked Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["get packet with packet client"], "references": ["revoke a bid on a project"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 3069, "code": "def accept project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'accept' } endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Accepted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["report a self in the self in the self in the self in the self in the self in the self in the self in a project"], "references": ["accept a bid on a project"], "bleu": 0.06352047085618948, "rouge_l": 0.2053872053872054}
{"id": 3070, "code": "def retract project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'retract' } endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Retracted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["of a structs with a specific bid"], "references": ["retract a bid on a project"], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 3071, "code": "def highlight project bid ( session , bid id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid data = { 'action' : 'highlight' } endpoint = 'bids/{}' . format ( bid id ) response = make put request ( session , endpoint , headers = headers , params data = bid data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : json data = response . json ( ) raise Bid Not Highlighted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["read a project with the given = false"], "references": ["highlight a bid on a project"], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 3072, "code": "def create milestone payment ( session , project id , bidder id , amount , reason , description ) : milestone data = { 'project id' : project id , 'bidder id' : bidder id , 'amount' : amount , 'reason' : reason , 'description' : description } response = make post request ( session , 'milestones' , json data = milestone data ) json data = response . json ( ) if response . status code == 200 : milestone data = json data [ 'result' ] return Milestone ( milestone data ) else : raise Milestone Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["creates a post payment"], "references": ["create a milestone payment"], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 3073, "code": "def post track ( session , user id , project id , latitude , longitude ) : tracking data = { 'user id' : user id , 'project id' : project id , 'track point' : { 'latitude' : latitude , 'longitude' : longitude } } response = make post request ( session , 'tracks' , json data = tracking data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Track Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["post a track to a project"], "references": ["start tracking a project by creating a track"], "bleu": 0.2619317629190374, "rouge_l": 0.2785388127853881}
{"id": 3074, "code": "def get track by id ( session , track id , track point limit = None , track point offset = None ) : tracking data = { } if track point limit : tracking data [ 'track point limit' ] = track point limit if track point offset : tracking data [ 'track point offset' ] = track point offset response = make get request ( session , 'tracks/{}' . format ( track id ) , params data = tracking data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Track Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["look up a track by id"], "references": ["gets a specific track"], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 3075, "code": "def release milestone payment ( session , milestone id , amount ) : params data = { 'action' : 'release' , } milestone data = { 'amount' : amount , } endpoint = 'milestones/{}' . format ( milestone id ) response = make put request ( session , endpoint , params data = params data , json data = milestone data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Not Released Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["send a release payment to a specific milestone"], "references": ["release a milestone payment"], "bleu": 0.20164945583740668, "rouge_l": 0.5319767441860466}
{"id": 3076, "code": "def request release milestone payment ( session , milestone id ) : params data = { 'action' : 'request release' , } endpoint = 'milestones/{}' . format ( milestone id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Not Requested Release Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["send a release payment to the api"], "references": ["release a milestone payment"], "bleu": 0.22089591134157885, "rouge_l": 0.3824451410658307}
{"id": 3077, "code": "def cancel milestone payment ( session , milestone id ) : params data = { 'action' : 'cancel' , } endpoint = 'milestones/{}' . format ( milestone id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Not Cancelled Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["cancel milestone payment ."], "references": ["release a milestone payment"], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 3078, "code": "def create milestone request ( session , project id , bid id , description , amount ) : milestone request data = { 'project id' : project id , 'bid id' : bid id , 'description' : description , 'amount' : amount , } response = make post request ( session , 'milestone requests' , json data = milestone request data ) json data = response . json ( ) if response . status code == 200 : milestone request data = json data [ 'result' ] return Milestone Request ( milestone request data ) else : raise Milestone Request Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["create a milestone request"], "references": ["create a milestone request"], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3079, "code": "def accept milestone request ( session , milestone request id ) : params data = { 'action' : 'accept' , } endpoint = 'milestone requests/{}' . format ( milestone request id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Request Not Accepted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["accept a request to the api"], "references": ["accept a milestone request"], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 3080, "code": "def reject milestone request ( session , milestone request id ) : params data = { 'action' : 'reject' , } endpoint = 'milestone requests/{}' . format ( milestone request id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Request Not Rejected Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["reject a request to the api"], "references": ["reject a milestone request"], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 3081, "code": "def delete milestone request ( session , milestone request id ) : params data = { 'action' : 'delete' , } endpoint = 'milestone requests/{}' . format ( milestone request id ) response = make put request ( session , endpoint , params data = params data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'status' ] else : raise Milestone Request Not Deleted Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["deletes the milestone request ."], "references": ["delete a milestone request"], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 3082, "code": "def get jobs ( session , job ids , seo details , lang ) : get jobs data = { 'jobs[]' : job ids , 'seo details' : seo details , 'lang' : lang , } response = make get request ( session , 'jobs' , params data = get jobs data ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Jobs Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["get jobs for a specific job"], "references": ["get a list of jobs"], "bleu": 0.2626909894424158, "rouge_l": 0.3696969696969697}
{"id": 3083, "code": "def create project thread ( session , member ids , project id , message ) : return create thread ( session , member ids , 'project' , project id , message )", "predictions": ["create a project thread with a project id"], "references": ["create a project thread"], "bleu": 0.44632361378533286, "rouge_l": 0.7093023255813954}
{"id": 3084, "code": "def post message ( session , thread id , message ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } message data = { 'message' : message , } endpoint = 'threads/{}/messages' . format ( thread id ) response = make post request ( session , endpoint , headers , form data = message data ) json data = response . json ( ) if response . status code == 200 : return Message ( json data [ 'result' ] ) else : raise Message Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["post a message to the api ."], "references": ["add a message to a thread"], "bleu": 0.345720784641941, "rouge_l": 0.4680306905370844}
{"id": 3085, "code": "def post attachment ( session , thread id , attachments ) : files = [ ] filenames = [ ] for attachment in attachments : files . append ( attachment [ 'file' ] ) filenames . append ( attachment [ 'filename' ] ) message data = { 'attachments[]' : filenames , } endpoint = 'threads/{}/messages' . format ( thread id ) response = make post request ( session , endpoint , form data = message data , files = files ) json data = response . json ( ) if response . status code == 200 : return Message ( json data [ 'result' ] ) else : raise Message Not Created Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["post an attachment to a post"], "references": ["add a message to a thread"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 3086, "code": "def get messages ( session , query , limit = 10 , offset = 0 ) : query [ 'limit' ] = limit query [ 'offset' ] = offset response = make get request ( session , 'messages' , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Messages Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["list all messages for the given query"], "references": ["get one or more messages"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3087, "code": "def get threads ( session , query ) : response = make get request ( session , 'threads' , params data = query ) json data = response . json ( ) if response . status code == 200 : return json data [ 'result' ] else : raise Threads Not Found Exception ( message = json data [ 'message' ] , error code = json data [ 'error code' ] , request id = json data [ 'request id' ] )", "predictions": ["list all threads for a given query"], "references": ["get one or more threads"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3088, "code": "def clean ( zipcode , valid length = valid zipcode length ) : zipcode = zipcode . split ( \"-\" ) [ 0 ] if len ( zipcode ) != valid length : raise Value Error ( 'Invalid format, zipcode must be of the format: \"#####\" or \"#####-####\"' ) if contains nondigits ( zipcode ) : raise Value Error ( 'Invalid characters, zipcode may only contain digits and \"-\".' ) return zipcode", "predictions": ["validate zipcode and abort zipcode ."], "references": ["assumes zipcode is of type str"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3089, "code": "def similar to ( partial zipcode , zips = zips ) : return [ z for z in zips if z [ \"zip code\" ] . startswith ( partial zipcode ) ]", "predictions": ["convert a list of zipcode objects to a list of zipcode objects ."], "references": ["list of zipcode dicts where zipcode prefix matches partial_zipcode"], "bleu": 0.18798317647335086, "rouge_l": 0.37596302003081655}
{"id": 3090, "code": "def filter by ( zips = zips , * * kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]", "predictions": ["filter zips by z ."], "references": ["use kwargs to select for desired attributes from list of zipcode dicts"], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 3091, "code": "def is valid identifier ( name ) : if not isinstance ( name , str ) : return False if '\\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except Syntax Error : return False", "predictions": ["validate a cookie identifier"], "references": ["pedantic yet imperfect . test to see if name is a valid python identifier"], "bleu": 0.032639898338235177, "rouge_l": 0.20198675496688742}
{"id": 3092, "code": "def build dict from key value ( keys and values ) : key dict = { } for key value in keys and values : if '=' not in key value : raise Ghost Error ( 'Pair {0} is not of `key=value` format' . format ( key value ) ) key , value = key value . split ( '=' , 1 ) key dict . update ( { str ( key ) : str ( value ) } ) return key dict", "predictions": ["builds a dictionary from a dictionary of values ."], "references": ["return a dict from a list of key = value pairs"], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 3093, "code": "def purge stash ( force , stash , passphrase , backend ) : stash = get stash ( backend , stash , passphrase ) try : click . echo ( 'Purging stash...' ) stash . purge ( force ) click . echo ( 'Purge complete!' ) except Ghost Error as ex : sys . exit ( ex )", "predictions": ["purge local stash ."], "references": ["purge the stash from all of its keys"], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 3094, "code": "def export keys ( output path , stash , passphrase , backend ) : stash = get stash ( backend , stash , passphrase ) try : click . echo ( 'Exporting stash to {0}...' . format ( output path ) ) stash . export ( output path = output path ) click . echo ( 'Export complete!' ) except Ghost Error as ex : sys . exit ( ex )", "predictions": ["export keys to a stash ."], "references": ["export all keys to a file"], "bleu": 0.4347208719449915, "rouge_l": 0.6666666666666666}
{"id": 3095, "code": "def get ( self , key name , decrypt = True ) : self . assert valid stash ( ) key = self . storage . get ( key name ) . copy ( ) if not key . get ( 'value' ) : return None if decrypt : key [ 'value' ] = self . decrypt ( key [ 'value' ] ) audit ( storage = self . storage . db path , action = 'GET' , message = json . dumps ( dict ( key name = key name ) ) ) return key", "predictions": ["get a key from redis"], "references": ["return a key with its parameters if it was found ."], "bleu": 0.10822031883953476, "rouge_l": 0.2341650671785029}
{"id": 3096, "code": "def list ( self , key name = None , max suggestions = 100 , cutoff = 0.5 , locked only = False , key type = None ) : self . assert valid stash ( ) key list = [ k for k in self . storage . list ( ) if k [ 'name' ] != 'stored passphrase' and ( k . get ( 'lock' ) if locked only else True ) ] if key type : types = ( 'secret' , None ) if key type == 'secret' else [ key type ] key list = [ k for k in key list if k . get ( 'type' ) in types ] key list = [ k [ 'name' ] for k in key list ] if key name : if key name . startswith ( '~' ) : key list = difflib . get close matches ( key name . lstrip ( '~' ) , key list , max suggestions , cutoff ) else : key list = [ k for k in key list if key name in k ] audit ( storage = self . storage . db path , action = 'LIST' + ( '[LOCKED]' if locked only else '' ) , message = json . dumps ( dict ( ) ) ) return key list", "predictions": ["list all user storage storage ."], "references": ["return a list of all keys ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 3097, "code": "def delete ( self , key name ) : self . assert valid stash ( ) if key name == 'stored passphrase' : raise Ghost Error ( '`stored passphrase` is a reserved ghost key name ' 'which cannot be deleted' ) if not self . get ( key name ) : raise Ghost Error ( 'Key `{0}` not found' . format ( key name ) ) key = self . storage . get ( key name ) if key . get ( 'lock' ) : raise Ghost Error ( 'Key `{0}` is locked and therefore cannot be deleted ' 'Please unlock the key and try again' . format ( key name ) ) deleted = self . storage . delete ( key name ) audit ( storage = self . storage . db path , action = 'DELETE' , message = json . dumps ( dict ( key name = key name ) ) ) if not deleted : raise Ghost Error ( 'Failed to delete {0}' . format ( key name ) )", "predictions": ["deletes a single key from the database ."], "references": ["delete a key if it exists ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 3098, "code": "def purge ( self , force = False , key type = None ) : self . assert valid stash ( ) if not force : raise Ghost Error ( \"The `force` flag must be provided to perform a stash purge. \" \"I mean, you don't really want to just delete everything \" \"without precautionary measures eh?\" ) audit ( storage = self . storage . db path , action = 'PURGE' , message = json . dumps ( dict ( ) ) ) for key name in self . list ( key type = key type ) : self . delete ( key name )", "predictions": ["purge all data locations ."], "references": ["purge the stash from all keys"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3099, "code": "def export ( self , output path = None , decrypt = False ) : self . assert valid stash ( ) all keys = [ ] for key in self . list ( ) : all keys . append ( dict ( self . get ( key , decrypt = decrypt ) ) ) if all keys : if output path : with open ( output path , 'w' ) as output file : output file . write ( json . dumps ( all keys , indent = 4 ) ) return all keys else : raise Ghost Error ( 'There are no keys to export' )", "predictions": ["export all keys to a file ."], "references": ["export all keys in the stash to a list or a file"], "bleu": 0.22117541221307574, "rouge_l": 0.6029654036243821}
{"id": 3100, "code": "def decrypt ( self , hexified value ) : encrypted value = binascii . unhexlify ( hexified value ) with warnings . catch warnings ( ) : warnings . simplefilter ( \"ignore\" ) jsonified value = self . cipher . decrypt ( encrypted value ) . decode ( 'ascii' ) value = json . loads ( jsonified value ) return value", "predictions": ["decrypt a hexified ."], "references": ["the exact opposite of _encrypt"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 3101, "code": "def delete ( self , key name ) : self . db . remove ( Query ( ) . name == key name ) return self . get ( key name ) == { }", "predictions": ["delete a key from database ."], "references": ["delete the key and return true if the key was deleted else false"], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 3102, "code": "def put ( self , key ) : self . consul request ( 'PUT' , self . key url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]", "predictions": ["requests to create a new component ."], "references": ["put and return the only unique identifier possible its url"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 3103, "code": "def put ( self , key ) : self . client . write ( self . key path ( key [ 'name' ] ) , * * key ) return self . key path ( key [ 'name' ] )", "predictions": ["stores a key into the cache ."], "references": ["put and return the only unique identifier possible its path"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3104, "code": "def init ( self ) : self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )", "predictions": ["initialize the class and create"], "references": ["create an elasticsearch index if necessary"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3105, "code": "def init ( self ) : try : self . client . create bucket ( Bucket = self . db path , Create Bucket Configuration = self . bucket configuration ) except botocore . exceptions . Client Error as e : if 'Bucket Already Owned By You' not in str ( e . response [ 'Error' ] [ 'Code' ] ) : raise e", "predictions": ["latitude the longitude id and initialize the longitude id id id id id id id id id id id id id id id id id id id id id id id"], "references": ["create a bucket ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3106, "code": "def terminal ( port = default port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] sys . argv = testargs miniterm . main ( )", "predictions": ["run the get server data data data data data data data data data data ."], "references": ["launch minterm from pyserial"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3107, "code": "def set baudrate ( self , baud ) : log . info ( 'Changing communication to %s baud' , baud ) self . writeln ( UART SETUP . format ( baud = baud ) ) time . sleep ( 0.1 ) try : self . port . set Baudrate ( baud ) except Attribute Error : #py Serial 2.7 self . port . baudrate = baud", "predictions": ["release the milestone for milestone"], "references": ["setting baudrate if supported"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3108, "code": "def set timeout ( self , timeout ) : timeout = int ( timeout ) self . timeout = timeout == 0 and 999999 or timeout", "predictions": ["sets the release for the given release . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["set the timeout for the communication with the device ."], "bleu": 0.05834347180338517, "rouge_l": 0.2149779735682819}
{"id": 3109, "code": "def clear buffers ( self ) : try : self . port . reset input buffer ( ) self . port . reset output buffer ( ) except Attribute Error : #py Serial 2.7 self . port . flush Input ( ) self . port . flush Output ( )", "predictions": ["cancel all milestone milestone . ."], "references": ["clears the input and output buffers"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3110, "code": "def expect ( self , exp = '> ' , timeout = None ) : timeout before = self . port . timeout timeout = timeout or self . timeout #do NOT set timeout on Windows if SYSTEM != 'Windows' : if self . port . timeout != MINIMAL TIMEOUT : self . port . timeout = MINIMAL TIMEOUT end = time . time ( ) + timeout data = '' while not data . endswith ( exp ) and time . time ( ) <= end : data += self . port . read ( ) log . debug ( 'expect returned: `{0}`' . format ( data ) ) if time . time ( ) > end : raise Communication Timeout ( 'Timeout waiting for data' , data ) if not data . endswith ( exp ) and len ( exp ) > 0 : raise Bad Response Exception ( 'Bad response.' , exp , data ) if SYSTEM != 'Windows' : self . port . timeout = timeout before return data", "predictions": ["waits for a = = 0 data to the client data data data data data data data data data data data = complete data data data data data data"], "references": ["will wait for exp to be returned from nodemcu or timeout"], "bleu": 0.0462136266712202, "rouge_l": 0.10883140053523638}
{"id": 3111, "code": "def exchange ( self , output , timeout = None ) : self . writeln ( output ) self . port . flush ( ) return self . expect ( timeout = timeout or self . timeout )", "predictions": ["flush accept a loop for session"], "references": ["write output to the port and wait for response"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 3112, "code": "def close ( self ) : try : if self . baud != self . start baud : self . set baudrate ( self . start baud ) self . port . flush ( ) self . clear buffers ( ) except serial . serialutil . Serial Exception : pass log . debug ( 'closing port' ) self . port . close ( )", "predictions": ["reject the serial ."], "references": ["restores the nodemcu to default baudrate and then closes the port"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 3113, "code": "def download file ( self , filename ) : res = self . exchange ( 'send(\"{filename}\")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) #tell device we are ready to receive self . write ( 'C' ) #we should get a NUL terminated filename to start with sent filename = self . expect ( NUL ) . strip ( ) log . info ( 'receiveing ' + sent filename ) #ACK to start download self . write ( ACK , True ) buf = '' data = '' chunk , buf = self . read chunk ( buf ) #read chunks until we get an empty which is the end while chunk != '' : self . write ( ACK , True ) data = data + chunk chunk , buf = self . read chunk ( buf ) return data", "predictions": ["delete a milestone milestone from the remote file data"], "references": ["download a file from device to local filesystem"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 3114, "code": "def read file ( self , filename , destination = '' ) : if not destination : destination = filename log . info ( 'Transferring %s to %s' , filename , destination ) data = self . download file ( filename ) log . info ( destination ) if not os . path . exists ( os . path . dirname ( destination ) ) : try : os . makedirs ( os . path . dirname ( destination ) ) except OS Error as e : if e . errno != errno . EEXIST : raise with open ( destination , 'w' ) as fil : fil . write ( data )", "predictions": ["get a jobs from a file"], "references": ["reading data from device into local file"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3115, "code": "def write file ( self , path , destination = '' , verify = 'none' ) : filename = os . path . basename ( path ) if not destination : destination = filename log . info ( 'Transferring %s as %s' , path , destination ) self . writeln ( \"recv()\" ) res = self . expect ( 'C> ' ) if not res . endswith ( 'C> ' ) : log . error ( 'Error waiting for esp \"%s\"' , res ) raise Communication Timeout ( 'Error waiting for device to start receiving' , res ) log . debug ( 'sending destination filename \"%s\"' , destination ) self . write ( destination + '\\x00' , True ) if not self . got ack ( ) : log . error ( 'did not ack destination filename' ) raise No Ack Exception ( 'Device did not ACK destination filename' ) content = from file ( path ) log . debug ( 'sending %d bytes in %s' , len ( content ) , filename ) pos = 0 chunk size = 128 while pos < len ( content ) : rest = len ( content ) - pos if rest > chunk size : rest = chunk size data = content [ pos : pos + rest ] if not self . write chunk ( data ) : resp = self . expect ( ) log . error ( 'Bad chunk response \"%s\" %s' , resp , hexify ( resp ) ) raise Bad Response Exception ( 'Bad chunk response' , ACK , resp ) pos += chunk size log . debug ( 'sending zero block' ) #zero size block self . write chunk ( '' ) if verify != 'none' : self . verify file ( path , destination , verify )", "predictions": ["create a project project to the client . ."], "references": ["sends a file to the device using the transfer protocol"], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 3116, "code": "def exec file ( self , path ) : filename = os . path . basename ( path ) log . info ( 'Execute %s' , filename ) content = from file ( path ) . replace ( '\\r' , '' ) . split ( '\\n' ) res = '> ' for line in content : line = line . rstrip ( '\\n' ) retlines = ( res + self . exchange ( line ) ) . splitlines ( ) res = retlines . pop ( ) for lin in retlines : log . info ( lin ) log . info ( res )", "predictions": ["parse a message from a message"], "references": ["execute the lines in the local file path"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 3117, "code": "def got ack ( self ) : log . debug ( 'waiting for ack' ) res = self . port . read ( 1 ) log . debug ( 'ack read %s' , hexify ( res ) ) return res == ACK", "predictions": ["post attachment for attachment"], "references": ["returns true if ack is received"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3118, "code": "def read chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout before = self . port . timeout if SYSTEM != 'Windows' : if self . port . timeout != MINIMAL TIMEOUT : self . port . timeout = MINIMAL TIMEOUT end = time . time ( ) + timeout before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . port . read ( ) if buf [ 0 ] != BLOCK START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . port . timeout = timeout before chunk size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk size + 2 ] buf = buf [ 130 : ] return ( data , buf )", "predictions": ["get data data from the client"], "references": ["read a chunk of data"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3119, "code": "def file list ( self ) : log . info ( 'Listing files' ) res = self . exchange ( LIST FILES ) res = res . split ( '\\r\\n' ) res = res [ 1 : - 1 ] files = [ ] for line in res : files . append ( line . split ( '\\t' ) ) return files", "predictions": ["threads to threads a threads"], "references": ["list files on the device"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3120, "code": "def file do ( self , filename ) : log . info ( 'Executing ' + filename ) res = self . exchange ( 'dofile(\"' + filename + '\")' ) log . info ( res ) return res", "predictions": ["function to get a clean clean up a clean clean up the clean len"], "references": ["execute a file on the device using do"], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 3121, "code": "def file print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . exchange ( PRINT FILE . format ( filename = filename ) ) log . info ( res ) return res", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["prints a file on the device to console"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 3122, "code": "def node heap ( self ) : log . info ( 'Heap' ) res = self . exchange ( 'print(node.heap())' ) log . info ( res ) return int ( res . split ( '\\r\\n' ) [ 1 ] )", "predictions": ["by name of the filter"], "references": ["show device heap size"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3123, "code": "def file compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile(\"%s\")' % path res = self . exchange ( cmd ) log . info ( res ) return res", "predictions": ["valid is a is a string to be used to valid is done ."], "references": ["compiles a file specified by path on the device"], "bleu": 0.08839374326825923, "rouge_l": 0.09050445103857567}
{"id": 3124, "code": "def file remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove(\"%s\")' % path res = self . exchange ( cmd ) log . info ( res ) return res", "predictions": ["dict with raise a build"], "references": ["removes a file on the device"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3125, "code": "def backup ( self , path ) : log . info ( 'Backing up in ' + path ) files = self . file list ( ) self . prepare ( ) for f in files : self . read file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )", "predictions": ["run a script script"], "references": ["backup all files from the device"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3126, "code": "def operation list ( uploader ) : files = uploader . file list ( ) for f in files : log . info ( \"{file:30s} {size}\" . format ( file = f [ 0 ] , size = f [ 1 ] ) )", "predictions": ["keys a keys backend"], "references": ["list file on target"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 3127, "code": "def localize ( dt ) : try : tz = dt . tzinfo return tz . localize ( dt . replace ( tzinfo = None ) ) except Attribute Error : return dt", "predictions": ["get datetime from datetime . or string . ."], "references": ["rely on pytz . localize to ensure new result honors dst ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 3128, "code": "def daily at ( cls , at , target ) : daily = datetime . timedelta ( days = 1 ) when = datetime . datetime . combine ( datetime . date . today ( ) , at ) if when < now ( ) : when += daily return cls . at time ( cls . localize ( when ) , daily , target )", "predictions": ["convert a list of list of list to a suggestions 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100"], "references": ["schedule a command to run at a specific time each day ."], "bleu": 0.046398855339878003, "rouge_l": 0.1516155758077879}
{"id": 3129, "code": "def get nearest year for day ( day ) : now = time . gmtime ( ) result = now . tm year if day - now . tm yday > 365 // 2 : result -= 1 if now . tm yday - day > 365 // 2 : result += 1 return result", "predictions": ["delete the nearest for for for for from the key stash stash stash stash stash stash stash stash stash stash stash stash stash stash"], "references": ["returns the nearest year to now inferred from a julian date ."], "bleu": 0.07164684238257436, "rouge_l": 0.17732558139534885}
{"id": 3130, "code": "def get ( self , query , responseformat = \"geojson\" , verbosity = \"body\" , build = True ) : if build : full query = self . construct ql query ( query , responseformat = responseformat , verbosity = verbosity ) else : full query = query if self . debug : logging . get Logger ( ) . info ( query ) r = self . get from overpass ( full query ) content type = r . headers . get ( \"content-type\" ) if self . debug : print ( content type ) if content type == \"text/csv\" : result = [ ] reader = csv . reader ( String IO ( r . text ) , delimiter = \"\\t\" ) for row in reader : result . append ( row ) return result elif content type in ( \"text/xml\" , \"application/xml\" , \"application/osm3s+xml\" ) : return r . text elif content type == \"application/json\" : response = json . loads ( r . text ) if not build : return response if \"elements\" not in response : raise Unknown Overpass Error ( \"Received an invalid answer from Overpass.\" ) overpass remark = response . get ( \"remark\" , None ) if overpass remark and overpass remark . startswith ( \"runtime error\" ) : raise Server Runtime Error ( overpass remark ) if responseformat is not \"geojson\" : return response return self . as geojson ( response [ \"elements\" ] )", "predictions": ["query geojson api endpoint"], "references": ["pass in an overpass query in overpass ql ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3131, "code": "def get resources ( cls ) : plugin = directory . get plugin ( ) controller = Segment Allocation Ranges Controller ( plugin ) return [ extensions . Resource Extension ( Segment allocation ranges . get alias ( ) , controller ) ]", "predictions": ["get all available resources resources ."], "references": ["returns ext resources ."], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 3132, "code": "def get resources ( cls ) : plugin = directory . get plugin ( ) controller = IP Availability Controller ( plugin ) return [ extensions . Resource Extension ( Ip availability . get alias ( ) , controller ) ]", "predictions": ["decrypt the available resources resources = 0 = 0 = 1 = 0 = 1 = 0 = 0 = 1 = 1 = 0 = 1 = 1 = 1"], "references": ["returns ext resources ."], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 3133, "code": "def get resources ( cls ) : ip controller = Ip Addresses Controller ( directory . get plugin ( ) ) ip port controller = Ip Address Port Controller ( directory . get plugin ( ) ) resources = [ ] resources . append ( extensions . Resource Extension ( Ip addresses . get alias ( ) , ip controller ) ) parent = { 'collection name' : 'ip addresses' , 'member name' : 'ip address' } resources . append ( extensions . Resource Extension ( 'ports' , ip port controller , parent = parent ) ) return resources", "predictions": ["returns ext resources resources return a list of available resources ."], "references": ["returns ext resources ."], "bleu": 0.24088562704853508, "rouge_l": 0.5823389021479713}
{"id": 3134, "code": "def get resources ( cls ) : plugin = directory . get plugin ( ) controller = Mac Address Ranges Controller ( plugin ) return [ extensions . Resource Extension ( Mac address ranges . get alias ( ) , controller ) ]", "predictions": ["list available resources resources request resources request request request request request request request request request request request request request names request request request request resources request return a list"], "references": ["returns ext resources ."], "bleu": 0.04175872565419194, "rouge_l": 0.07019562715765247}
{"id": 3135, "code": "def update security group rule ( context , id , security group rule ) : LOG . info ( \"update security group rule for tenant %s\" % ( context . tenant id ) ) new rule = security group rule [ \"security group rule\" ] new rule = filter update security group rule ( new rule ) with context . session . begin ( ) : rule = db api . security group rule find ( context , id = id , scope = db api . ONE ) if not rule : raise sg ext . Security Group Rule Not Found ( id = id ) db rule = db api . security group rule update ( context , rule , * * new rule ) group id = db rule . group id group = db api . security group find ( context , id = group id , scope = db api . ONE ) if not group : raise sg ext . Security Group Not Found ( id = group id ) if group : perform async update rule ( context , group id , group , rule . id , RULE UPDATE ) return v . make security group rule dict ( db rule )", "predictions": ["put a security rule into a security self ."], "references": ["updates a rule and updates the ports"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 3136, "code": "def get public net id ( self ) : for id , net params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public net id : return id return None", "predictions": ["return the public id of the public public key ."], "references": ["returns the public net id"], "bleu": 0.17827531042796255, "rouge_l": 0.42558139534883715}
{"id": 3137, "code": "def add job to context ( context , job id ) : db job = db api . async transaction find ( context , id = job id , scope = db api . ONE ) if not db job : return context . async job = { \"job\" : v . make job dict ( db job ) }", "predictions": ["add a job to a context"], "references": ["adds job to neutron context for use later ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 3138, "code": "def get resources ( cls ) : plugin = directory . get plugin ( ) controller = IP Policies Controller ( plugin ) return [ extensions . Resource Extension ( Ip policies . get alias ( ) , controller ) ]", "predictions": ["get all resources available in the plugin ."], "references": ["returns ext resources ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 3139, "code": "def add default tz bindings ( self , context , switch , network id ) : default tz = CONF . NVP . default tz if not default tz : LOG . warn ( \"additional default tz types specified, \" \"but no default tz. Skipping \" \" add default tz bindings().\" ) return if not network id : LOG . warn ( \"neutron network id not specified, skipping \" \" add default tz bindings()\" ) return for net type in CONF . NVP . additional default tz types : if net type in TZ BINDINGS : binding = TZ BINDINGS [ net type ] binding . add ( context , switch , default tz , network id ) else : LOG . warn ( \"Unknown default tz type %s\" % ( net type ) )", "predictions": ["add a default bindings to the network ."], "references": ["configure any additional default transport zone bindings ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 3140, "code": "def remove default tz bindings ( self , context , network id ) : default tz = CONF . NVP . default tz if not default tz : LOG . warn ( \"additional default tz types specified, \" \"but no default tz. Skipping \" \" remove default tz bindings().\" ) return if not network id : LOG . warn ( \"neutron network id not specified, skipping \" \" remove default tz bindings()\" ) return for net type in CONF . NVP . additional default tz types : if net type in TZ BINDINGS : binding = TZ BINDINGS [ net type ] binding . remove ( context , default tz , network id ) else : LOG . warn ( \"Unknown default tz type %s\" % ( net type ) )", "predictions": ["remove default bindings to network ."], "references": ["deconfigure any additional default transport zone bindings ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 3141, "code": "def discover via entrypoints ( self ) : emgr = extension . Extension Manager ( PLUGIN EP , invoke on load = False ) return ( ( ext . name , ext . plugin ) for ext in emgr )", "predictions": ["discover available via extension ."], "references": ["looks for modules with amtching entry points ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3142, "code": "def serve rpc ( self ) : if cfg . CONF . QUARK ASYNC . rpc workers < 1 : cfg . CONF . set override ( 'rpc workers' , 1 , \"QUARK ASYNC\" ) try : rpc = service . Rpc Worker ( self . plugins ) launcher = common service . Process Launcher ( CONF , wait interval = 1.0 ) launcher . launch service ( rpc , workers = CONF . QUARK ASYNC . rpc workers ) return launcher except Exception : with excutils . save and reraise exception ( ) : LOG . exception ( LE ( 'Unrecoverable error: please check log for ' 'details.' ) )", "predictions": ["serve rpc workers ."], "references": ["launches configured # of workers per loaded plugin ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 3143, "code": "def get resources ( cls ) : plural mappings = resource helper . build plural mappings ( { } , RESOURCE ATTRIBUTE MAP ) return resource helper . build resource info ( plural mappings , RESOURCE ATTRIBUTE MAP , None , register quota = True )", "predictions": ["returns ext resources ."], "references": ["returns ext resources ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3144, "code": "def check collisions ( self , new range , existing ranges ) : def contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def is overlap ( r1 , r2 ) : return ( contains ( r1 [ 0 ] , r2 ) or contains ( r1 [ 1 ] , r2 ) or contains ( r2 [ 0 ] , r1 ) or contains ( r2 [ 1 ] , r1 ) ) for existing range in existing ranges : if is overlap ( new range , existing range ) : return True return False", "predictions": ["check if two ranges are in one or more more than one"], "references": ["check for overlapping ranges ."], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 3145, "code": "def delete locks ( context , network ids , addresses ) : addresses no longer null routed = find addresses to be unlocked ( context , network ids , addresses ) LOG . info ( \"Deleting %s lock holders on IP Address with ids: %s\" , len ( addresses no longer null routed ) , [ addr . id for addr in addresses no longer null routed ] ) for address in addresses no longer null routed : lock holder = None try : lock holder = db api . lock holder find ( context , lock id = address . lock id , name = LOCK NAME , scope = db api . ONE ) if lock holder : db api . lock holder delete ( context , address , lock holder ) except Exception : LOG . exception ( \"Failed to delete lock holder %s\" , lock holder ) continue context . session . flush ( )", "predictions": ["delete locks for a network ."], "references": ["deletes locks for each ip address that is no longer null - routed ."], "bleu": 0.08234616270176032, "rouge_l": 0.2798165137614679}
{"id": 3146, "code": "def set ( self , model , value ) : self . validate ( value ) self . pop ( model ) value = self . serialize ( value ) model . tags . append ( value )", "predictions": ["set a model s value"], "references": ["set tag on model object ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3147, "code": "def get ( self , model ) : for tag in model . tags : if self . is tag ( tag ) : value = self . deserialize ( tag ) try : self . validate ( value ) return value except Tag Validation Error : continue return None", "predictions": ["get single model by model ."], "references": ["get a matching valid tag off the model ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 3148, "code": "def pop ( self , model ) : tags = [ ] for tag in model . tags : if self . is tag ( tag ) : tags . append ( tag ) if tags : for tag in tags : model . tags . remove ( tag ) return tags", "predictions": ["pop tags from the model ."], "references": ["pop all matching tags off the model and return them ."], "bleu": 0.15024963364904895, "rouge_l": 0.5586080586080586}
{"id": 3149, "code": "def pop ( self , model ) : tags = self . pop ( model ) if tags : for tag in tags : value = self . deserialize ( tag ) try : self . validate ( value ) return value except Tag Validation Error : continue", "predictions": ["pop a single model by model ."], "references": ["pop all matching tags off the port return a valid one ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 3150, "code": "def has tag ( self , model ) : for tag in model . tags : if self . is tag ( tag ) : return True return False", "predictions": ["return true if the model has a tag ."], "references": ["does the given port have this tag?"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3151, "code": "def set all ( self , model , * * tags ) : for name , tag in self . tags . items ( ) : if name in tags : value = tags . pop ( name ) if value : try : tag . set ( model , value ) except Tag Validation Error as e : raise n exc . Bad Request ( resource = \"tags\" , msg = \"%s\" % ( e . message ) )", "predictions": ["set all tags for a model ."], "references": ["validate and set all known tags on a port ."], "bleu": 0.18938334565508194, "rouge_l": 0.5700934579439253}
{"id": 3152, "code": "def serialize rules ( self , rules ) : serialized = [ ] for rule in rules : direction = rule [ \"direction\" ] source = '' destination = '' if rule . get ( \"remote ip prefix\" ) : prefix = rule [ \"remote ip prefix\" ] if direction == \"ingress\" : source = self . convert remote network ( prefix ) else : if ( Capabilities . EGRESS not in CONF . QUARK . environment capabilities ) : raise q exc . Egress Security Group Rules Not Enabled ( ) else : destination = self . convert remote network ( prefix ) optional fields = { } protocol map = protocols . PROTOCOL MAP [ rule [ \"ethertype\" ] ] if rule [ \"protocol\" ] == protocol map [ \"icmp\" ] : optional fields [ \"icmp type\" ] = rule [ \"port range min\" ] optional fields [ \"icmp code\" ] = rule [ \"port range max\" ] else : optional fields [ \"port start\" ] = rule [ \"port range min\" ] optional fields [ \"port end\" ] = rule [ \"port range max\" ] payload = { \"ethertype\" : rule [ \"ethertype\" ] , \"protocol\" : rule [ \"protocol\" ] , \"source network\" : source , \"destination network\" : destination , \"action\" : \"allow\" , \"direction\" : direction } payload . update ( optional fields ) serialized . append ( payload ) return serialized", "predictions": ["serializes a list of rules into a serialized dictionary ."], "references": ["creates a payload for the redis server ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3153, "code": "def apply rules ( self , device id , mac address , rules ) : LOG . info ( \"Applying security group rules for device %s with MAC %s\" % ( device id , mac address ) ) rule dict = { SECURITY GROUP RULE KEY : rules } redis key = self . vif key ( device id , mac address ) self . set field ( redis key , SECURITY GROUP HASH ATTR , rule dict ) self . set field raw ( redis key , SECURITY GROUP ACK , False )", "predictions": ["apply security group rules to security group rules"], "references": ["writes a series of security group rules to a redis server ."], "bleu": 0.2707089559145445, "rouge_l": 0.3860759493670886}
{"id": 3154, "code": "def update group states for vifs ( self , vifs , ack ) : vif keys = [ self . vif key ( vif . device id , vif . mac address ) for vif in vifs ] self . set fields ( vif keys , SECURITY GROUP ACK , ack )", "predictions": ["update group states for a specified device ."], "references": ["updates security groups by setting the ack field"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3155, "code": "def get resources ( cls ) : job controller = Jobs Controller ( directory . get plugin ( ) ) resources = [ ] resources . append ( extensions . Resource Extension ( Jobs . get alias ( ) , job controller ) ) return resources", "predictions": ["returns all resources ."], "references": ["returns ext resources ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 3156, "code": "def filter factory ( global conf , * * local conf ) : conf = global conf . copy ( ) conf . update ( local conf ) def wrapper ( app ) : return Response Async Id Adder ( app , conf ) return wrapper", "predictions": ["returns a wsgi filter app for use with paste . deploy ."], "references": ["returns a wsgi filter app for use with paste . deploy ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3157, "code": "def get interfaces ( self ) : LOG . debug ( \"Getting interfaces from Xapi\" ) with self . sessioned ( ) as session : instances = self . get instances ( session ) recs = session . xenapi . VIF . get all records ( ) interfaces = set ( ) for vif ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ \"VM\" ] ) if not vm : continue device id = vm . uuid interfaces . add ( VIF ( device id , rec , vif ref ) ) return interfaces", "predictions": ["get the interfaces details ."], "references": ["returns a set of vifs from get_instances return value ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 3158, "code": "def main ( notify , hour , minute ) : config opts = [ '--config-file' , '/etc/neutron/neutron.conf' ] config . init ( config opts ) network strategy . STRATEGY . load ( ) billing . PUBLIC NETWORK ID = network strategy . STRATEGY . get public net id ( ) config . setup logging ( ) context = neutron context . get admin context ( ) query = context . session . query ( models . IP Address ) ( period start , period end ) = billing . calc periods ( hour , minute ) full day ips = billing . build full day ips ( query , period start , period end ) partial day ips = billing . build partial day ips ( query , period start , period end ) if notify : for ipaddress in full day ips : click . echo ( 'start: {}, end: {}' . format ( period start , period end ) ) payload = billing . build payload ( ipaddress , billing . IP EXISTS , start time = period start , end time = period end ) billing . do notify ( context , billing . IP EXISTS , payload ) for ipaddress in partial day ips : click . echo ( 'start: {}, end: {}' . format ( period start , period end ) ) payload = billing . build payload ( ipaddress , billing . IP EXISTS , start time = ipaddress . allocated at , end time = period end ) billing . do notify ( context , billing . IP EXISTS , payload ) else : click . echo ( 'Case 1 ({}):\\n' . format ( len ( full day ips ) ) ) for ipaddress in full day ips : pp ( billing . build payload ( ipaddress , billing . IP EXISTS , start time = period start , end time = period end ) ) click . echo ( '\\n===============================================\\n' ) click . echo ( 'Case 2 ({}):\\n' . format ( len ( partial day ips ) ) ) for ipaddress in partial day ips : pp ( billing . build payload ( ipaddress , billing . IP EXISTS , start time = ipaddress . allocated at , end time = period end ) )", "predictions": ["build a full public day of the pipeline ."], "references": ["runs billing report . optionally sends notifications to billing"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3159, "code": "def start rpc listeners ( self ) : self . setup rpc ( ) if not self . endpoints : return [ ] self . conn = n rpc . create connection ( ) self . conn . create consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume in threads ( )", "predictions": ["start the consumer connection ."], "references": ["configure all listeners here"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3160, "code": "def context ( self ) : if not self . context : self . context = context . get admin context ( ) return self . context", "predictions": ["return the context dictionary for the current context ."], "references": ["provides an admin context for workers ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 3161, "code": "def update sg ( self , context , sg , rule id , action ) : db sg = db api . security group find ( context , id = sg , scope = db api . ONE ) if not db sg : return None with context . session . begin ( ) : job body = dict ( action = \"%s sg rule %s\" % ( action , rule id ) , resource id = rule id , tenant id = db sg [ 'tenant id' ] ) job body = dict ( job = job body ) job = job api . create job ( context . elevated ( ) , job body ) rpc client = Quark SG Async Producer Client ( ) try : rpc client . populate subtasks ( context , sg , job [ 'id' ] ) except om exc . Messaging Timeout : LOG . error ( \"Failed to create subtasks. Rabbit running?\" ) return None return { \"job id\" : job [ 'id' ] }", "predictions": ["create a job group ."], "references": ["begins the async update process ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3162, "code": "def populate subtasks ( self , context , sg , parent job id ) : db sg = db api . security group find ( context , id = sg , scope = db api . ONE ) if not db sg : return None ports = db api . sg gather associated ports ( context , db sg ) if len ( ports ) == 0 : return { \"ports\" : 0 } for port in ports : job body = dict ( action = \"update port %s\" % port [ 'id' ] , tenant id = db sg [ 'tenant id' ] , resource id = port [ 'id' ] , parent id = parent job id ) job body = dict ( job = job body ) job = job api . create job ( context . elevated ( ) , job body ) rpc consumer = Quark SG Async Consumer Client ( ) try : rpc consumer . update port ( context , port [ 'id' ] , job [ 'id' ] ) except om exc . Messaging Timeout : LOG . error ( \"Failed to update port. Rabbit running?\" ) return None", "predictions": ["populate a job object with a list of ports ."], "references": ["produces a list of ports to be updated async ."], "bleu": 0.3672056269893592, "rouge_l": 0.5}
{"id": 3163, "code": "def update ports for sg ( self , context , portid , jobid ) : port = db api . port find ( context , id = portid , scope = db api . ONE ) if not port : LOG . warning ( \"Port not found\" ) return net driver = port api . get net driver ( port . network , port = port ) base net driver = port api . get net driver ( port . network ) sg list = [ sg for sg in port . security groups ] success = False error = None retries = 3 retry delay = 2 for retry in xrange ( retries ) : try : net driver . update port ( context , port id = port [ \"backend key\" ] , mac address = port [ \"mac address\" ] , device id = port [ \"device id\" ] , base net driver = base net driver , security groups = sg list ) success = True error = None break except Exception as error : LOG . warning ( \"Could not connect to redis, but retrying soon\" ) time . sleep ( retry delay ) status str = \"\" if not success : status str = \"Port %s update failed after %d tries. Error: %s\" % ( portid , retries , error ) update body = dict ( completed = True , status = status str ) update body = dict ( job = update body ) job api . update job ( context . elevated ( ) , jobid , update body )", "predictions": ["update ports for a single job"], "references": ["updates the ports through redis ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3164, "code": "def segment allocation find ( context , lock mode = False , * * filters ) : range ids = filters . pop ( \"segment allocation range ids\" , None ) query = context . session . query ( models . Segment Allocation ) if lock mode : query = query . with lockmode ( \"update\" ) query = query . filter by ( * * filters ) if range ids : query . filter ( models . Segment Allocation . segment allocation range id . in ( range ids ) ) return query", "predictions": ["segment allocation with given filters ."], "references": ["query for segment allocations ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 3165, "code": "def get resources ( cls ) : controller = Routes Controller ( directory . get plugin ( ) ) return [ extensions . Resource Extension ( Routes . get alias ( ) , controller ) ]", "predictions": ["get all resources available in the current controller ."], "references": ["returns ext resources ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 3166, "code": "def if ( * args ) : for i in range ( 0 , len ( args ) - 1 , 2 ) : if args [ i ] : return args [ i + 1 ] if len ( args ) % 2 : return args [ - 1 ] else : return None", "predictions": ["return the first argument of the first argument is a number of numbers ."], "references": ["implements the if operator with support for multiple elseif - s ."], "bleu": 0.09782375748961449, "rouge_l": 0.15601023017902813}
{"id": 3167, "code": "def soft equals ( a , b ) : if isinstance ( a , str ) or isinstance ( b , str ) : return str ( a ) == str ( b ) if isinstance ( a , bool ) or isinstance ( b , bool ) : return bool ( a ) is bool ( b ) return a == b", "predictions": ["compare two values of a and b"], "references": ["implements the == operator which does type js - style coertion ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 3168, "code": "def hard equals ( a , b ) : if type ( a ) != type ( b ) : return False return a == b", "predictions": ["compare two numbers ignoring none"], "references": ["implements the === operator ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3169, "code": "def minus ( * args ) : if len ( args ) == 1 : return - to numeric ( args [ 0 ] ) return to numeric ( args [ 0 ] ) - to numeric ( args [ 1 ] )", "predictions": ["convert a iterable to a async one ."], "references": ["also converts either to ints or to floats ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3170, "code": "def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret", "predictions": ["get a list of values from a list of values"], "references": ["implements the merge operator for merging lists ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3171, "code": "def get var ( data , var name , not found = None ) : try : for key in str ( var name ) . split ( '.' ) : try : data = data [ key ] except Type Error : data = data [ int ( key ) ] except ( Key Error , Type Error , Value Error ) : return not found else : return data", "predictions": ["add a variable to a dictionary"], "references": ["gets variable value from data dictionary ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3172, "code": "def missing ( data , * args ) : not found = object ( ) if args and isinstance ( args [ 0 ] , list ) : args = args [ 0 ] ret = [ ] for arg in args : if get var ( data , arg , not found ) is not found : ret . append ( arg ) return ret", "predictions": ["perform remove remove items that were not remove any of the tz"], "references": ["implements the missing operator for finding missing variables ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 3173, "code": "def missing some ( data , min required , args ) : if min required < 1 : return [ ] found = 0 not found = object ( ) ret = [ ] for arg in args : if get var ( data , arg , not found ) is not found : ret . append ( arg ) else : found += 1 if found >= min required : return [ ] return ret", "predictions": ["perform discover - via via via via via via via via via via via via via via via via via via via via via via via via via via via via"], "references": ["implements the missing_some operator for finding missing variables ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3174, "code": "def json Logic ( tests , data = None ) : if tests is None or not isinstance ( tests , dict ) : return tests data = data or { } operator = list ( tests . keys ( ) ) [ 0 ] values = tests [ operator ] if not isinstance ( values , list ) and not isinstance ( values , tuple ) : values = [ values ] values = [ json Logic ( val , data ) for val in values ] if operator == 'var' : return get var ( data , * values ) if operator == 'missing' : return missing ( data , * values ) if operator == 'missing some' : return missing some ( data , * values ) if operator not in operations : raise Value Error ( \"Unrecognized operation %s\" % operator ) return operations [ operator ] ( * values )", "predictions": ["extract serve and data ."], "references": ["executes the json - logic with given data ."], "bleu": 0.1614457444314309, "rouge_l": 0.2717149220489978}
{"id": 3175, "code": "def unindent ( self ) : if self . tab always indent : cursor = self . editor . text Cursor ( ) if not cursor . has Selection ( ) : cursor . select ( cursor . Line Under Cursor ) self . unindent selection ( cursor ) else : super ( Py Indenter Mode , self ) . unindent ( )", "predictions": ["un - indents the cursor plural the cursor plural plural selection plural plural plural plural plural ."], "references": ["performs an un - indentation"], "bleu": 0.09507244120026236, "rouge_l": 0.20165289256198346}
{"id": 3176, "code": "def handle indent between paren ( self , column , line , parent impl , tc ) : pre , post = parent impl next char = self . get next char ( tc ) prev char = self . get prev char ( tc ) prev open = prev char in [ '[' , '(' , '{' ] next close = next char in [ ']' , ')' , '}' ] ( open line , open symbol col ) , ( close line , close col ) = self . get paren pos ( tc , column ) open line txt = self . helper . line text ( open line ) open line indent = len ( open line txt ) - len ( open line txt . lstrip ( ) ) if prev open : post = ( open line indent + self . editor . tab length ) * ' ' elif next close and prev char != ',' : post = open line indent * ' ' elif tc . block ( ) . block Number ( ) == open line : post = open symbol col * ' ' if close line and close col : txt = self . helper . line text ( close line ) bn = tc . block ( ) . block Number ( ) flg = bn == close line next indent = self . helper . line indent ( bn + 1 ) * ' ' if flg and txt . strip ( ) . endswith ( ':' ) and next indent == post : post += self . editor . tab length * ' ' if next char in [ '\"' , \"'\" ] : tc . move Position ( tc . Left ) is string = self . helper . is comment or string ( tc , formats = [ 'string' ] ) if next char in [ '\"' , \"'\" ] : tc . move Position ( tc . Right ) if is string : trav = Q Text Cursor ( tc ) while self . helper . is comment or string ( trav , formats = [ 'string' ] ) : trav . move Position ( trav . Left ) trav . move Position ( trav . Right ) symbol = '%s' % self . get next char ( trav ) pre += symbol post += symbol return pre , post", "predictions": ["collisions between between between"], "references": ["handle indent between symbols such as parenthesis braces ..."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3177, "code": "def at block start ( tc , line ) : if tc . at Block Start ( ) : return True column = tc . column Number ( ) indentation = len ( line ) - len ( line . lstrip ( ) ) return column <= indentation", "predictions": ["routed to the locks in the locks of the given locks no locks no find no unlocked no find"], "references": ["improve qtextcursor . atblockstart to ignore spaces"], "bleu": 0.06439931429457924, "rouge_l": 0.08390646492434663}
{"id": 3178, "code": "def update terminal colors ( self ) : self . color scheme = self . create color scheme ( background = self . syntax highlighter . color scheme . background , foreground = self . syntax highlighter . color scheme . formats [ 'normal' ] . foreground ( ) . color ( ) )", "predictions": ["set the terminal self . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["update terminal color scheme based on the pygments color scheme colors"], "bleu": 0.04317900023606586, "rouge_l": 0.05209222886421862}
{"id": 3179, "code": "def setup actions ( self ) : self . action Open . triggered . connect ( self . on open ) self . action New . triggered . connect ( self . on new ) self . action Save . triggered . connect ( self . on save ) self . action Save as . triggered . connect ( self . on save as ) self . action Quit . triggered . connect ( Qt Widgets . Q Application . instance ( ) . quit ) self . tab Widget . current changed . connect ( self . on current tab changed ) self . tab Widget . last tab closed . connect ( self . on last tab closed ) self . action About . triggered . connect ( self . on about ) self . action Run . triggered . connect ( self . on run ) self . interactive Console . process finished . connect ( self . on process finished ) self . action Configure run . triggered . connect ( self . on configure run )", "predictions": ["get actions and ."], "references": ["connects slots to signals"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 3180, "code": "def on new ( self ) : interpreter , pyserver , args = self . get backend parameters ( ) self . setup editor ( self . tab Widget . create new document ( extension = '.py' , interpreter = interpreter , server script = pyserver , args = args ) ) self . action Run . set Disabled ( True ) self . action Configure run . set Disabled ( True )", "predictions": ["called when a new is created"], "references": ["add a new empty code editor to the tab widget"], "bleu": 0.14925824694560996, "rouge_l": 0.23921568627450981}
{"id": 3181, "code": "def on save as ( self ) : path = self . tab Widget . current widget ( ) . file . path path = os . path . dirname ( path ) if path else '' filename , filter = Qt Widgets . Q File Dialog . get Save File Name ( self , 'Save' , path ) if filename : self . tab Widget . save current ( filename ) self . recent files manager . open file ( filename ) self . menu recents . update actions ( ) self . action Run . set Enabled ( True ) self . action Configure run . set Enabled ( True ) self . update status bar ( self . tab Widget . current widget ( ) )", "predictions": ["handle save file save"], "references": ["save the current editor document as ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 3182, "code": "def setup mnu style ( self , editor ) : menu = Qt Widgets . Q Menu ( 'Styles' , self . menu Edit ) group = Qt Widgets . Q Action Group ( self ) self . styles group = group current style = editor . syntax highlighter . color scheme . name group . triggered . connect ( self . on style changed ) for s in sorted ( PYGMENTS STYLES ) : a = Qt Widgets . Q Action ( menu ) a . set Text ( s ) a . set Checkable ( True ) if s == current style : a . set Checked ( True ) group . add Action ( a ) menu . add Action ( a ) self . menu Edit . add Menu ( menu )", "predictions": ["has been registered style"], "references": ["setup the style menu for an editor tab"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 3183, "code": "def on current tab changed ( self ) : self . menu Edit . clear ( ) self . menu Modes . clear ( ) self . menu Panels . clear ( ) editor = self . tab Widget . current widget ( ) self . menu Edit . set Enabled ( editor is not None ) self . menu Modes . set Enabled ( editor is not None ) self . menu Panels . set Enabled ( editor is not None ) self . action Save . set Enabled ( editor is not None ) self . action Save as . set Enabled ( editor is not None ) self . action Configure run . set Enabled ( editor is not None ) self . action Run . set Enabled ( editor is not None ) if editor is not None : self . setup mnu edit ( editor ) self . setup mnu modes ( editor ) self . setup mnu panels ( editor ) self . widget Outline . set editor ( editor ) self . update status bar ( editor )", "predictions": ["sets a new ."], "references": ["update action states when the current tab changed ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3184, "code": "def on run ( self ) : filename = self . tab Widget . current widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get run config for file ( filename ) self . interactive Console . start process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dock Widget . show ( ) self . action Run . set Enabled ( False ) self . action Configure run . set Enabled ( False )", "predictions": ["not thread - safe"], "references": ["run the current current script"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 3185, "code": "def goto assignments ( request data ) : code = request data [ 'code' ] line = request data [ 'line' ] + 1 column = request data [ 'column' ] path = request data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto assignments ( ) except jedi . Not Found Error : pass else : ret val = [ ( d . module path , d . line - 1 if d . line else None , d . column , d . full name ) for d in definitions ] return ret val", "predictions": ["return is a module of the rules in the rules"], "references": ["go to assignements worker ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3186, "code": "def defined names ( request data ) : global old definitions ret val = [ ] path = request data [ 'path' ] toplvl definitions = jedi . names ( request data [ 'code' ] , path , 'utf-8' ) for d in toplvl definitions : definition = extract def ( d , path ) if d . type != 'import' : ret val . append ( definition ) ret val = [ d . to dict ( ) for d in ret val ] return ret val", "predictions": ["return a list of all group group group group group group group group group names"], "references": ["returns the list of defined names for the document ."], "bleu": 0.11633270842295028, "rouge_l": 0.2489795918367347}
{"id": 3187, "code": "def quick doc ( request data ) : code = request data [ 'code' ] line = request data [ 'line' ] + 1 column = request data [ 'column' ] path = request data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto definitions ( ) except jedi . Not Found Error : return [ ] else : ret val = [ d . docstring ( ) for d in definitions ] return ret val", "predictions": ["return for the get - resources"], "references": ["worker that returns the documentation of the symbol under cursor ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 3188, "code": "def make python patterns ( additional keywords = [ ] , additional builtins = [ ] ) : kw = r\"\\b\" + any ( \"keyword\" , kwlist + additional keywords ) + r\"\\b\" kw namespace = r\"\\b\" + any ( \"namespace\" , kw namespace list ) + r\"\\b\" word operators = r\"\\b\" + any ( \"operator word\" , wordop list ) + r\"\\b\" builtinlist = [ str ( name ) for name in dir ( builtins ) if not name . startswith ( ' ' ) ] + additional builtins for v in [ 'None' , 'True' , 'False' ] : builtinlist . remove ( v ) builtin = r\"([^.'\\\"\\\\#]\\b|^)\" + any ( \"builtin\" , builtinlist ) + r\"\\b\" builtin fct = any ( \"builtin fct\" , [ r' {2}[a-z A-Z ]* {2}' ] ) comment = any ( \"comment\" , [ r\"#[^\\n]*\" ] ) instance = any ( \"instance\" , [ r\"\\bself\\b\" , r\"\\bcls\\b\" ] ) decorator = any ( 'decorator' , [ r'@\\w*' , r'.setter' ] ) number = any ( \"number\" , [ r\"\\b[+-]?[0-9]+[l Lj J]?\\b\" , r\"\\b[+-]?0[x X][0-9A-Fa-f]+[l L]?\\b\" , r\"\\b[+-]?0[o O][0-7]+[l L]?\\b\" , r\"\\b[+-]?0[b B][01]+[l L]?\\b\" , r\"\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[e E][+-]?[0-9]+)?[j J]?\\b\" ] ) sqstring = r\"(\\b[r Ru U])?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*'?\" dqstring = r'(\\b[r Ru U])?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*\"?' uf sqstring = r\"(\\b[r Ru U])?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*(\\\\)$(?!')$\" uf dqstring = r'(\\b[r Ru U])?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*(\\\\)$(?!\")$' sq3string = r\"(\\b[r Ru U])?'''[^'\\\\]*((\\\\.|'(?!''))[^'\\\\]*)*(''')?\" dq3string = r'(\\b[r Ru U])?\"\"\"[^\"\\\\]*((\\\\.|\"(?!\"\"))[^\"\\\\]*)*(\"\"\")?' uf sq3string = r\"(\\b[r Ru U])?'''[^'\\\\]*((\\\\.|'(?!''))[^'\\\\]*)*(\\\\)?(?!''')$\" uf dq3string = r'(\\b[r Ru U])?\"\"\"[^\"\\\\]*((\\\\.|\"(?!\"\"))[^\"\\\\]*)*(\\\\)?(?!\"\"\")$' string = any ( \"string\" , [ sq3string , dq3string , sqstring , dqstring ] ) ufstring1 = any ( \"uf sqstring\" , [ uf sqstring ] ) ufstring2 = any ( \"uf dqstring\" , [ uf dqstring ] ) ufstring3 = any ( \"uf sq3string\" , [ uf sq3string ] ) ufstring4 = any ( \"uf dq3string\" , [ uf dq3string ] ) return \"|\" . join ( [ instance , decorator , kw , kw namespace , builtin , word operators , builtin fct , comment , ufstring1 , ufstring2 , ufstring3 , ufstring4 , string , number , any ( \"SYNC\" , [ r\"\\n\" ] ) ] )", "predictions": ["strongly inspired from idlelib . colordelegator . make_pat"], "references": ["strongly inspired from idlelib . colordelegator . make_pat"], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3189, "code": "def unique ( self , seq ) : checked = [ ] for e in seq : present = False for c in checked : if str ( c ) == str ( e ) : present = True break if not present : checked . append ( e ) return checked", "predictions": ["returns a list of get ."], "references": ["not performant but works ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3190, "code": "def find libname ( self , name ) : names = [ \"{}.lib\" , \"lib{}.lib\" , \"{}lib.lib\" ] names = [ n . format ( name ) for n in names ] dirs = self . get library dirs ( ) for d in dirs : for n in names : if exists ( join ( d , n ) ) : return n [ : - 4 ] msg = \"Could not find the {} library.\" . format ( name ) raise Value Error ( msg )", "predictions": ["main method to main search for a particular library"], "references": ["try to infer the correct library name ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 3191, "code": "def finalize ( self ) : if self . head less : warn ( f'{self. class . name } configured to head less mode. finalize unusable' ) elif not self . head generate : warn ( f'{self. class . name } already finalized or fitted' ) elif not self . head dict : raise Not Fitted Error ( f'{self. class . name } instance is not fitted yet' ) else : if self . remove rare ratio : self . clean head ( * self . head rare ) self . prepare header ( ) self . head rare = None self . head generate = False", "predictions": ["start the record ."], "references": ["finalize partial fitting procedure"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 3192, "code": "def fit ( self , x , y = None ) : x = iter2array ( x , dtype = ( Molecule Container , CGR Container ) ) if self . head less : warn ( f'{self. class . name } configured to head less mode. fit unusable' ) return self self . reset ( ) self . prepare ( x ) return self", "predictions": ["context - context context"], "references": ["compute the header ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 3193, "code": "def self referential fk ( klass model ) : for f in klass model . meta . concrete fields : if f . related model : if issubclass ( klass model , f . related model ) : return f . attname return None", "predictions": ["return of the related class"], "references": ["return whether this model has a self ref fk and the name for the field"], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 3194, "code": "def serialize ( self ) : opts = self . meta data = { } for f in opts . concrete fields : if f . attname in self . morango fields not to serialize : continue if f . attname in self . morango internal fields not to serialize : continue if f . attname in getattr ( self , ' internal mptt fields not to serialize' , ' internal fields not to serialize' ) : continue if hasattr ( f , 'value from object json compatible' ) : data [ f . attname ] = f . value from object json compatible ( self ) else : data [ f . attname ] = f . value from object ( self ) return data", "predictions": ["convert the model to a dictionary ."], "references": ["all concrete fields of the syncablemodel subclass except for those specifically blacklisted are returned in a dict ."], "bleu": 0.045890725128646594, "rouge_l": 0.2223572296476306}
{"id": 3195, "code": "def deserialize ( cls , dict model ) : kwargs = { } for f in cls . meta . concrete fields : if f . attname in dict model : kwargs [ f . attname ] = dict model [ f . attname ] return cls ( * * kwargs )", "predictions": ["update a model from a dictionary"], "references": ["returns an unsaved class object based on the valid properties passed in ."], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 3196, "code": "def get default ( self ) : if self . has default ( ) : if callable ( self . default ) : default = self . default ( ) if isinstance ( default , uuid . UUID ) : return default . hex return default if isinstance ( self . default , uuid . UUID ) : return self . default . hex return self . default return None", "predictions": ["ids for the allocation or allocation"], "references": ["returns the default value for this field ."], "bleu": 0.17516432701748888, "rouge_l": 0.13926940639269406}
{"id": 3197, "code": "def calculate uuid ( self ) : if self . uuid input fields is None : raise Not Implemented Error ( ) if self . uuid input fields == \"RANDOM\" : return uuid . uuid4 ( ) . hex assert isinstance ( self . uuid input fields , tuple ) , \"'uuid input fields' must either be a tuple or the string 'RANDOM'\" hashable input vals = [ ] for field in self . uuid input fields : new value = getattr ( self , field ) if new value : hashable input vals . append ( str ( new value ) ) hashable input = \":\" . join ( hashable input vals ) if not hashable input : return uuid . uuid4 ( ) . hex return sha2 uuid ( hashable input )", "predictions": ["get resources for this field = resources = resources = resources = resources = resources = 0"], "references": ["should return a 32 - digit hex string for a uuid that is calculated as a function of a set of fields from the model ."], "bleu": 0.042545509002538305, "rouge_l": 0.04481998530492285}
{"id": 3198, "code": "def queue into buffer ( transfersession ) : last saved by conditions = [ ] filter prefixes = Filter ( transfersession . filter ) server fsic = json . loads ( transfersession . server fsic ) client fsic = json . loads ( transfersession . client fsic ) if transfersession . push : fsics = fsic queuing calc ( client fsic , server fsic ) else : fsics = fsic queuing calc ( server fsic , client fsic ) if not fsics : return for instance , counter in six . iteritems ( fsics ) : last saved by conditions += [ \"(last saved instance = '{0}' AND last saved counter > {1})\" . format ( instance , counter ) ] if fsics : last saved by conditions = [ join with logical operator ( last saved by conditions , 'OR' ) ] partition conditions = [ ] for prefix in filter prefixes : partition conditions += [ \"partition LIKE '{}%'\" . format ( prefix ) ] if filter prefixes : partition conditions = [ join with logical operator ( partition conditions , 'OR' ) ] fsic and partition conditions = join with logical operator ( last saved by conditions + partition conditions , 'AND' ) where condition = join with logical operator ( [ fsic and partition conditions , \"profile = '{}'\" . format ( transfersession . sync session . profile ) ] , 'AND' ) with connection . cursor ( ) as cursor : queue buffer = . format ( outgoing buffer = Buffer . meta . db table , transfer session id = transfersession . id , condition = where condition , store = Store . meta . db table ) cursor . execute ( queue buffer ) queue rmc buffer = . format ( outgoing rmcb = Record Max Counter Buffer . meta . db table , transfer session id = transfersession . id , record max counter = Record Max Counter . meta . db table , outgoing buffer = Buffer . meta . db table ) cursor . execute ( queue rmc buffer )", "predictions": ["executor for globus if there are a in - in - * if they are in a * if they don t already ship"], "references": ["takes a chunk of data from the store to be put into the buffer to be sent to another morango instance ."], "bleu": 0.050661968099322066, "rouge_l": 0.043821839080459765}
{"id": 3199, "code": "def dequeue into store ( transfersession ) : with connection . cursor ( ) as cursor : DB Backend . dequeuing delete rmcb records ( cursor , transfersession . id ) DB Backend . dequeuing delete buffered records ( cursor , transfersession . id ) current id = Instance ID Model . get current instance and increment counter ( ) DB Backend . dequeuing merge conflict buffer ( cursor , current id , transfersession . id ) DB Backend . dequeuing merge conflict rmcb ( cursor , transfersession . id ) DB Backend . dequeuing update rmcs last saved by ( cursor , current id , transfersession . id ) DB Backend . dequeuing delete mc rmcb ( cursor , transfersession . id ) DB Backend . dequeuing delete mc buffer ( cursor , transfersession . id ) DB Backend . dequeuing insert remaining buffer ( cursor , transfersession . id ) DB Backend . dequeuing insert remaining rmcb ( cursor , transfersession . id ) DB Backend . dequeuing delete remaining rmcb ( cursor , transfersession . id ) DB Backend . dequeuing delete remaining buffer ( cursor , transfersession . id ) if getattr ( settings , 'MORANGO DESERIALIZE AFTER DEQUEUING' , True ) : deserialize from store ( transfersession . sync session . profile )", "predictions": ["soft buffered equals equals equals"], "references": ["takes data from the buffers and merges into the store and record max counters ."], "bleu": 0.031069582040305195, "rouge_l": 0.0}
{"id": 3200, "code": "def multiple self ref fk check ( class model ) : self fk = [ ] for f in class model . meta . concrete fields : if f . related model in self fk : return True if f . related model == class model : self fk . append ( class model ) return False", "predictions": ["checks whether a class is multiple times in a class"], "references": ["we check whether a class has more than 1 fk reference to itself ."], "bleu": 0.15727283746960627, "rouge_l": 0.24270557029177717}
{"id": 3201, "code": "def save service ( self , service , overwrite = True ) : name = namesgenerator . get sane name ( service . name ) if not name : name = namesgenerator . get random name ( ) if self . collection . count documents ( { 'name' : name } ) > 0 : name = namesgenerator . get random name ( retry = True ) if self . collection . count documents ( { 'name' : name } ) > 0 : if overwrite : self . collection . delete one ( { 'name' : name } ) else : raise Exception ( \"service name already registered.\" ) self . collection . insert one ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch by name ( name = name )", "predictions": ["save a service to the collection ."], "references": ["stores an ows service in mongodb ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3202, "code": "def list services ( self ) : my services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my services . append ( Service ( service ) ) return my services", "predictions": ["list all services available for this instance"], "references": ["lists all services in mongodb storage ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 3203, "code": "def fetch by name ( self , name ) : service = self . collection . find one ( { 'name' : name } ) if not service : raise Service Not Found return Service ( service )", "predictions": ["fetch a service by name"], "references": ["gets service for given name from mongodb storage ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3204, "code": "def fetch by url ( self , url ) : service = self . collection . find one ( { 'url' : url } ) if not service : raise Service Not Found return Service ( service )", "predictions": ["fetch a service by url"], "references": ["gets service for given url from mongodb storage ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3205, "code": "def owsproxy delegate ( request ) : twitcher url = request . registry . settings . get ( 'twitcher.url' ) protected path = request . registry . settings . get ( 'twitcher.ows proxy protected path' , '/ows' ) url = twitcher url + protected path + '/proxy' if request . matchdict . get ( 'service name' ) : url += '/' + request . matchdict . get ( 'service name' ) if request . matchdict . get ( 'access token' ) : url += '/' + request . matchdict . get ( 'service name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( \"delegate to owsproxy: %s\" , url ) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status code , headers = resp . headers )", "predictions": ["return a flask response object with the proxy proxy ."], "references": ["delegates owsproxy request to external twitcher service ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 3206, "code": "def main ( global config , * * settings ) : from pyramid . config import Configurator config = Configurator ( settings = settings ) config . include ( 'twitcher.config' ) config . include ( 'twitcher.frontpage' ) config . include ( 'twitcher.rpcinterface' ) config . include ( 'twitcher.owsproxy' ) config . include ( 'twitcher.tweens' ) config . scan ( ) return config . make wsgi app ( )", "predictions": ["this function returns a pyramid wsgi application ."], "references": ["this function returns a pyramid wsgi application ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3207, "code": "def save service ( self , service , overwrite = True ) : name = namesgenerator . get sane name ( service . name ) if not name : name = namesgenerator . get random name ( ) if name in self . name index : name = namesgenerator . get random name ( retry = True ) if name in self . name index : if overwrite : self . delete ( name = name ) else : raise Exception ( \"service name already registered.\" ) self . insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch by name ( name = name )", "predictions": ["save a service ."], "references": ["store an ows service in database ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 3208, "code": "def list services ( self ) : my services = [ ] for service in self . name index . values ( ) : my services . append ( Service ( service ) ) return my services", "predictions": ["list all services available for this instance ."], "references": ["lists all services in memory storage ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 3209, "code": "def fetch by name ( self , name ) : service = self . name index . get ( name ) if not service : raise Service Not Found return Service ( service )", "predictions": ["fetch a service by name"], "references": ["get service for given name from memory storage ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3210, "code": "def get param ( self , param , allowed values = None , optional = False ) : request params = self . request params ( ) if param in request params : value = request params [ param ] . lower ( ) if allowed values is not None : if value in allowed values : self . params [ param ] = value else : raise OWS Invalid Parameter Value ( \"%s %s is not supported\" % ( param , value ) , value = param ) elif optional : self . params [ param ] = None else : raise OWS Missing Parameter Value ( 'Parameter \"%s\" is missing' % param , value = param ) return self . params [ param ]", "predictions": ["get parameter parameter ."], "references": ["get parameter in get request ."], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 3211, "code": "def get version ( self ) : version = self . get param ( param = \"version\" , allowed values = allowed versions [ self . params [ 'service' ] ] , optional = True ) if version is None and self . get request type ( ) != \"getcapabilities\" : raise OWS Missing Parameter Value ( 'Parameter \"version\" is missing' , value = \"version\" ) else : return version", "predictions": ["get the version number from the request ."], "references": ["find requested version in get request ."], "bleu": 0.239802967618271, "rouge_l": 0.4048672566371681}
{"id": 3212, "code": "def get service ( self ) : if \"service\" in self . document . attrib : value = self . document . attrib [ \"service\" ] . lower ( ) if value in allowed service types : self . params [ \"service\" ] = value else : raise OWS Invalid Parameter Value ( \"Service %s is not supported\" % value , value = \"service\" ) else : raise OWS Missing Parameter Value ( 'Parameter \"service\" is missing' , value = \"service\" ) return self . params [ \"service\" ]", "predictions": ["return a service service ."], "references": ["check mandatory service name parameter in post request ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3213, "code": "def get request type ( self ) : value = self . document . tag . lower ( ) if value in allowed request types [ self . params [ 'service' ] ] : self . params [ \"request\" ] = value else : raise OWS Invalid Parameter Value ( \"Request type %s is not supported\" % value , value = \"request\" ) return self . params [ \"request\" ]", "predictions": ["determine the request type for the request ."], "references": ["find requested request type in post request ."], "bleu": 0.2653856085536222, "rouge_l": 0.5}
{"id": 3214, "code": "def get version ( self ) : if \"version\" in self . document . attrib : value = self . document . attrib [ \"version\" ] . lower ( ) if value in allowed versions [ self . params [ 'service' ] ] : self . params [ \"version\" ] = value else : raise OWS Invalid Parameter Value ( \"Version %s is not supported\" % value , value = \"version\" ) elif self . get request type ( ) == \"getcapabilities\" : self . params [ \"version\" ] = None else : raise OWS Missing Parameter Value ( 'Parameter \"version\" is missing' , value = \"version\" ) return self . params [ \"version\" ]", "predictions": ["get the version number from the document ."], "references": ["find requested version in post request ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3215, "code": "def localize datetime ( dt , tz name = 'UTC' ) : tz aware dt = dt if dt . tzinfo is None : utc = pytz . timezone ( 'UTC' ) aware = utc . localize ( dt ) timezone = pytz . timezone ( tz name ) tz aware dt = aware . astimezone ( timezone ) else : logger . warn ( 'tzinfo already set' ) return tz aware dt", "predictions": ["localize a datetime object to a utc datetime object ."], "references": ["provide a timzeone - aware object for a given datetime and timezone name"], "bleu": 0.11742832364135733, "rouge_l": 0.33983286908078}
{"id": 3216, "code": "def baseurl ( url ) : parsed url = urlparse . urlparse ( url ) if not parsed url . netloc or parsed url . scheme not in ( \"http\" , \"https\" ) : raise Value Error ( 'bad url' ) service url = \"%s://%s%s\" % ( parsed url . scheme , parsed url . netloc , parsed url . path . strip ( ) ) return service url", "predictions": ["return a service url from a url ."], "references": ["return baseurl of given url"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 3217, "code": "def verify ( self ) : value = self . get ( 'verify' , 'true' ) if isinstance ( value , bool ) : verify = value elif value . lower ( ) == 'true' : verify = True elif value . lower ( ) == 'false' : verify = False else : verify = value return verify", "predictions": ["verify that the value is signed by the url ."], "references": ["verify ssl service certificate ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 3218, "code": "def tag ( self , label , message = None ) : notify . warning ( 'Unsupported SCM: Make sure you apply the \"{}\" tag after commit!{}' . format ( label , ' [message={}]' . format ( message ) if message else '' , ) )", "predictions": ["apply an exception message"], "references": ["tag the current workdir state ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3219, "code": "def pep440 dev version ( self , verbose = False , non local = False ) : pep440 = '.dev{:%Y%m%d%H%M}' . format ( datetime . now ( ) ) if not non local : build number = os . environ . get ( 'BUILD NUMBER' , 'n/a' ) if build number . isdigit ( ) : pep440 += '+ci.{}' . format ( build number ) if verbose : notify . info ( \"Adding CI build ID #{} to version\" . format ( build number ) ) return pep440", "predictions": ["return the build version of the build ."], "references": ["return a pep - 440 dev version appendix to the main version number ."], "bleu": 0.09525245831601728, "rouge_l": 0.346590909090909}
{"id": 3220, "code": "def get egg info ( cfg , verbose = False ) : result = Bunch ( ) setup py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup py ) : return result egg info = shell . capture ( \"python {} egg info\" . format ( setup py ) , echo = True if verbose else None ) for info line in egg info . splitlines ( ) : if info line . endswith ( 'PKG-INFO' ) : pkg info file = info line . split ( None , 1 ) [ 1 ] result [ ' file ' ] = pkg info file with io . open ( pkg info file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , \"Bad continuation in PKG-INFO file '{}': {}\" . format ( pkg info file , line ) result [ lastkey ] += '\\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , ' ' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except Attribute Error : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG INFO MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result", "predictions": ["get egg information from a python program"], "references": ["call setup egg_info and return the parsed meta - data ."], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 3221, "code": "def bump ( ctx , verbose = False , pypi = False ) : cfg = config . load ( ) scm = scm provider ( cfg . project root , commit = False , ctx = ctx ) if not scm . workdir is clean ( ) : notify . warning ( \"You have uncommitted changes, will create a time-stamped version!\" ) pep440 = scm . pep440 dev version ( verbose = verbose , non local = pypi ) setup cfg = cfg . rootjoin ( 'setup.cfg' ) if not pep440 : notify . info ( \"Working directory contains a release version!\" ) elif os . path . exists ( setup cfg ) : with io . open ( setup cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if re . match ( r\"#? *tag build *= *.*\" , line ) : verb , = data [ i ] . split ( '=' , 1 ) data [ i ] = '{}= {}\\n' . format ( verb , pep440 ) changed = True if changed : notify . info ( \"Rewriting 'setup.cfg'...\" ) with io . open ( setup cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) else : notify . warning ( \"No 'tag build' setting found in 'setup.cfg'!\" ) else : notify . warning ( \"Cannot rewrite 'setup.cfg', none found!\" ) if os . path . exists ( setup cfg ) : egg info = shell . capture ( \"python setup.py egg info\" , echo = True if verbose else None ) for line in egg info . splitlines ( ) : if line . endswith ( 'PKG-INFO' ) : pkg info file = line . split ( None , 1 ) [ 1 ] with io . open ( pkg info file , encoding = 'utf-8' ) as handle : notify . info ( '\\n' . join ( i for i in handle . readlines ( ) if i . startswith ( 'Version:' ) ) . strip ( ) ) ctx . run ( \"python setup.py -q develop\" , echo = True if verbose else None )", "predictions": ["bump setup a release"], "references": ["bump a development version ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 3222, "code": "def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ \"python\" , \"setup.py\" , \"sdist\" ] if auto : egg = sys . version info . major == 2 try : import wheel as wheel = True except Import Error : wheel = False if egg : cmd . append ( \"bdist egg\" ) if wheel : cmd . append ( \"bdist wheel\" ) ctx . run ( \"invoke clean --all build --docs test check\" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( \"devpi upload dist/*\" )", "predictions": ["upload a current version of a repo ."], "references": ["distribute the project ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3223, "code": "def prep ( ctx , commit = True ) : cfg = config . load ( ) scm = scm provider ( cfg . project root , commit = commit , ctx = ctx ) if not scm . workdir is clean ( ) : notify . failure ( \"You have uncommitted changes, please commit or stash them!\" ) setup cfg = cfg . rootjoin ( 'setup.cfg' ) if os . path . exists ( setup cfg ) : with io . open ( setup cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if any ( line . startswith ( i ) for i in ( 'tag build' , 'tag date' ) ) : data [ i ] = '#' + data [ i ] changed = True if changed and commit : notify . info ( \"Rewriting 'setup.cfg'...\" ) with io . open ( setup cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) scm . add file ( 'setup.cfg' ) elif changed : notify . warning ( \"WOULD rewrite 'setup.cfg', but --no-commit was passed\" ) else : notify . warning ( \"Cannot rewrite 'setup.cfg', none found!\" ) ctx . run ( 'python setup.py -q develop -U' ) version = capture ( 'python setup.py --version' ) ctx . run ( 'invoke clean --all build --docs release.dist' ) for distfile in os . listdir ( 'dist' ) : trailer = distfile . split ( '-' + version ) [ 1 ] trailer , = os . path . splitext ( trailer ) if trailer and trailer [ 0 ] not in '.-' : notify . failure ( \"The version found in 'dist' seems to be\" \" a pre-release one! [{}{}]\" . format ( version , trailer ) ) scm . commit ( ctx . rituals . release . commit . message . format ( version = version ) ) scm . tag ( ctx . rituals . release . tag . name . format ( version = version ) , ctx . rituals . release . tag . message . format ( version = version ) )", "predictions": ["prepare a new project ."], "references": ["prepare for a release ."], "bleu": 0.32466791547509893, "rouge_l": 0.6}
{"id": 3224, "code": "def pylint ( ctx , skip tests = False , skip root = False , reports = False ) : cfg = config . load ( ) add dir2pypath ( cfg . project root ) if not os . path . exists ( cfg . testjoin ( ' init .py' ) ) : add dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip tests : test py = antglob . File Set ( cfg . testdir , '**/*.py' ) test py = [ cfg . testjoin ( i ) for i in test py ] if test py : namelist |= set ( test py ) if not skip root : root py = antglob . File Set ( '.' , '*.py' ) if root py : namelist |= set ( root py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' \"{}\"' . format ( '\" \"' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report error = False , runner = ctx . run ) notify . info ( \"OK - No problems found by pylint.\" ) except exceptions . Failure as exc : if exc . result . return code & 32 : notify . error ( \"Usage error, bad arguments in {}?!\" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : \"fatal\" , 2 : \"error\" , 4 : \"warning\" , 8 : \"refactor\" , 16 : \"convention\" , } notify . warning ( \"Some messages of type {} issued by pylint.\" . format ( \", \" . join ( [ text for bit , text in bits . items ( ) if exc . result . return code & bit ] ) ) ) if exc . result . return code & 3 : notify . error ( \"Exiting due to fatal / error message.\" ) raise", "predictions": ["show pylint with pylint ."], "references": ["perform source code checks via pylint ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 3225, "code": "def tag ( self , label , message = None ) : options = ' -m \"{}\" -a' . format ( message ) if message else '' self . run elective ( 'git tag{} \"{}\"' . format ( options , label ) )", "predictions": ["tag the specified job ."], "references": ["tag the current workdir state ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 3226, "code": "def description ( dummy ctx , markdown = False ) : cfg = config . load ( ) markup = 'md' if markdown else 'html' description file = cfg . rootjoin ( \"build/project.{}\" . format ( markup ) ) notify . banner ( \"Creating {} file for Jenkins...\" . format ( description file ) ) long description = cfg . project . long description long description = long description . replace ( '\\n\\n' , '</p>\\n<p>' ) long description = re . sub ( r'(\\W)``([^`]+)``(\\W)' , r'\\1<tt>\\2</tt>\\3' , long description ) text = DESCRIPTION TEMPLATES [ markup ] . format ( keywords = ', ' . join ( cfg . project . keywords ) , classifiers = '\\n' . join ( cfg . project . classifiers ) , classifiers indented = '    ' + '\\n    ' . join ( cfg . project . classifiers ) , packages = ', ' . join ( cfg . project . packages ) , long description html = '<p>{}</p>' . format ( long description ) , ##data='\\n'.join([\"%s=%r\" % i for i in cfg.project.iteritems()]), * * cfg ) with io . open ( description file , 'w' , encoding = 'utf-8' ) as handle : handle . write ( text )", "predictions": ["generate a description of the current project ."], "references": ["dump project metadata for jenkins description setter plugin ."], "bleu": 0.16829946711936866, "rouge_l": 0.232824427480916}
{"id": 3227, "code": "def capture ( cmd , * * kw ) : kw = kw . copy ( ) kw [ 'hide' ] = 'out' if not kw . get ( 'echo' , False ) : kw [ 'echo' ] = False ignore failures = kw . pop ( 'ignore failures' , False ) try : return invoke run ( cmd , * * kw ) . stdout . strip ( ) except exceptions . Failure as exc : if not ignore failures : notify . error ( \"Command `{}` failed with RC={}!\" . format ( cmd , exc . result . return code , ) ) raise", "predictions": ["execute the command ."], "references": ["run a command and return its stripped captured output ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 3228, "code": "def run ( cmd , * * kw ) : kw = kw . copy ( ) kw . setdefault ( 'warn' , False ) report error = kw . pop ( 'report error' , True ) runner = kw . pop ( 'runner' , invoke run ) try : return runner ( cmd , * * kw ) except exceptions . Failure as exc : sys . stdout . flush ( ) sys . stderr . flush ( ) if report error : notify . error ( \"Command `{}` failed with RC={}!\" . format ( cmd , exc . result . return code , ) ) raise finally : sys . stdout . flush ( ) sys . stderr . flush ( )", "predictions": ["run a command remotly remotly ."], "references": ["run a command and flush its output ."], "bleu": 0.31149111610852515, "rouge_l": 0.5570776255707762}
{"id": 3229, "code": "def provider ( workdir , commit = True , * * kwargs ) : return SCM PROVIDER [ auto detect ( workdir ) ] ( workdir , commit = commit , * * kwargs )", "predictions": ["pass through to provider get_asset_content_lookup_session . commit"], "references": ["factory for the correct scm provider in workdir ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3230, "code": "def fail ( message , exitcode = 1 ) : sys . stderr . write ( 'ERROR: {}\\n' . format ( message ) ) sys . stderr . flush ( ) sys . exit ( exitcode )", "predictions": ["prints a message to stderr and exit ."], "references": ["exit with error code and message ."], "bleu": 0.20164945583740668, "rouge_l": 0.26991150442477874}
{"id": 3231, "code": "def get pypi auth ( configfile = '~/.pypirc' ) : pypi cfg = Config Parser ( ) if pypi cfg . read ( os . path . expanduser ( configfile ) ) : try : user = pypi cfg . get ( 'pypi' , 'username' ) pwd = pypi cfg . get ( 'pypi' , 'password' ) return user , pwd except Config Error : notify . warning ( \"No Py PI credentials in '{}',\" \" will fall back to '~/.netrc'...\" . format ( configfile ) ) return None", "predictions": ["get pypi auth info from pypi ."], "references": ["read auth from pip config ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 3232, "code": "def confluence ( ctx , no publish = False , clean = False , opts = '' ) : cfg = config . load ( ) if clean : ctx . run ( \"invoke clean --docs\" ) cmd = [ 'sphinx-build' , '-b' , 'confluence' ] cmd . extend ( [ '-E' , '-a' ] ) if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build + ' cf' ] ) if no publish : cmd . extend ( [ '-Dconfluence publish=False' ] ) notify . info ( \"Starting Sphinx build...\" ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = True )", "predictions": ["clean a file from the master"], "references": ["build sphinx docs and publish to confluence ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 3233, "code": "def zipped ( self , docs base ) : with pushd ( docs base ) : with tempfile . Named Temporary File ( prefix = 'pythonhosted-' , delete = False ) as ziphandle : pass zip name = shutil . make archive ( ziphandle . name , 'zip' ) notify . info ( \"Uploading {:.1f} Mi B from '{}' to '{}'...\" . format ( os . path . getsize ( zip name ) / 1024.0 , zip name , self . target ) ) with io . open ( zip name , 'rb' ) as zipread : try : yield zipread finally : os . remove ( ziphandle . name ) os . remove ( ziphandle . name + '.zip' )", "predictions": ["create a temporary file for the given docs . . . . . . . . . . . . . . . . . ."], "references": ["provide a zipped stream of the docs tree ."], "bleu": 0.058697608930387266, "rouge_l": 0.2505133470225872}
{"id": 3234, "code": "def to pypi ( self , docs base , release ) : url = None with self . zipped ( docs base ) as handle : reply = requests . post ( self . params [ 'url' ] , auth = get pypi auth ( ) , allow redirects = False , files = dict ( content = ( self . cfg . project . name + '.zip' , handle , 'application/zip' ) ) , data = { ':action' : 'doc upload' , 'name' : self . cfg . project . name } ) if reply . status code in range ( 200 , 300 ) : notify . info ( \"{status code} {reason}\" . format ( * * vars ( reply ) ) ) elif reply . status code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( \"{status code} {reason} for POST to {url}\" . format ( * * data ) ) return url", "predictions": ["convert a docs to services format"], "references": ["upload to pypi ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 3235, "code": "def to webdav ( self , docs base , release ) : try : git path = subprocess . check output ( 'git remote get-url origin 2>/dev/null' , shell = True ) except subprocess . Called Process Error : git path = '' else : git path = git path . decode ( 'ascii' ) . strip ( ) git path = git path . replace ( 'http://' , '' ) . replace ( 'https://' , '' ) . replace ( 'ssh://' , '' ) git path = re . search ( r'[^:/]+?[:/](.+)' , git path ) git path = git path . group ( 1 ) . replace ( '.git' , '' ) if git path else '' url = None with self . zipped ( docs base ) as handle : url ns = dict ( name = self . cfg . project . name , version = release , git path = git path ) reply = requests . put ( self . params [ 'url' ] . format ( * * url ns ) , data = handle . read ( ) , headers = { 'Accept' : 'application/json' } ) if reply . status code in range ( 200 , 300 ) : notify . info ( \"{status code} {reason}\" . format ( * * vars ( reply ) ) ) try : data = reply . json ( ) except Value Error as exc : notify . warning ( \"Didn't get a JSON response! ({})\" . format ( exc ) ) else : if 'download Uri' in data : url = data [ 'download Uri' ] + '!/index.html' elif reply . status code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( \"{status code} {reason} for PUT to {url}\" . format ( * * data ) ) if not url : notify . warning ( \"Couldn't get URL from upload response!\" ) return url", "predictions": ["convert a collection of docs to by sending it to by the given release"], "references": ["upload to webdav store ."], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 3236, "code": "def upload ( self , docs base , release ) : return getattr ( self , ' to ' + self . target ) ( docs base , release )", "predictions": ["fetch a specific release"], "references": ["upload docs in docs_base to the target of this uploader ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 3237, "code": "def add dir2pypath ( path ) : py path = os . environ . get ( 'PYTHONPATH' , '' ) if path not in py path . split ( os . pathsep ) : py path = '' . join ( [ path , os . pathsep if py path else '' , py path ] ) os . environ [ 'PYTHONPATH' ] = py path", "predictions": ["add . py . py"], "references": ["add given directory to pythonpath e . g . for pylint ."], "bleu": 0.08006212224540951, "rouge_l": 0.3285457809694794}
{"id": 3238, "code": "def run ( self , cmd , * args , * * kwargs ) : runner = self . ctx . run if self . ctx else None return run ( cmd , runner = runner , * args , * * kwargs )", "predictions": ["execute a command and return the result ."], "references": ["run a command ."], "bleu": 0.22679164443904004, "rouge_l": 0.5319767441860466}
{"id": 3239, "code": "def run elective ( self , cmd , * args , * * kwargs ) : if self . commit : return self . run ( cmd , * args , * * kwargs ) else : notify . warning ( \"WOULD RUN: {}\" . format ( cmd ) ) kwargs = kwargs . copy ( ) kwargs [ 'echo' ] = False return self . run ( 'true' , * args , * * kwargs )", "predictions": ["wrap service in service"], "references": ["run a command or just echo it depending on commit ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 3240, "code": "def info ( msg ) : flush ( ) sys . stdout . write ( msg + '\\n' ) sys . stdout . flush ( )", "predictions": ["flush function to print a message to stdout = false"], "references": ["emit a normal message ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 3241, "code": "def warning ( msg ) : flush ( ) sys . stderr . write ( \"\\033[1;7;33;40m WARNING: {}\\033[0m\\n\" . format ( msg ) ) sys . stderr . flush ( )", "predictions": ["display a fetch message"], "references": ["emit a warning message ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 3242, "code": "def error ( msg ) : flush ( ) sys . stderr . write ( \"\\033[1;37;41m ERROR: {}\\033[0m\\n\" . format ( msg ) ) sys . stderr . flush ( )", "predictions": ["log an get message to stderr"], "references": ["emit an error message to stderr ."], "bleu": 0.36798327352994814, "rouge_l": 0.6069651741293532}
{"id": 3243, "code": "def get devpi url ( ctx ) : cmd = 'devpi use --urls' lines = ctx . run ( cmd , hide = 'out' , echo = False ) . stdout . splitlines ( ) for line in lines : try : line , base url = line . split ( ':' , 1 ) except Value Error : notify . warning ( 'Ignoring \"{}\"!' . format ( line ) ) else : if line . split ( ) [ - 1 ] . strip ( ) == 'simpleindex' : return base url . split ( '\\x1b' ) [ 0 ] . strip ( ) . rstrip ( '/' ) raise Lookup Error ( \"Cannot find simpleindex URL in '{}' output:\\n    {}\" . format ( cmd , '\\n    ' . join ( lines ) , ) )", "predictions": ["get the version of the version"], "references": ["get currently used devpi base url ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3244, "code": "def get project root ( ) : try : tasks py = sys . modules [ 'tasks' ] except Key Error : return None else : return os . path . abspath ( os . path . dirname ( tasks py . file ) )", "predictions": ["get the service root path"], "references": ["determine location of tasks . py ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 3245, "code": "def glob2re ( part ) : return \"[^/]*\" . join ( re . escape ( bit ) . replace ( r'\\[\\^' , '[^' ) . replace ( r'\\[' , '[' ) . replace ( r'\\]' , ']' ) for bit in part . split ( \"*\" ) )", "predictions": ["convert a type string to a string"], "references": ["convert a path part to regex syntax ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 3246, "code": "def parse glob ( pattern ) : if not pattern : return bits = pattern . split ( \"/\" ) dirs , filename = bits [ : - 1 ] , bits [ - 1 ] for dirname in dirs : if dirname == \"**\" : yield \"(|.+/)\" else : yield glob2re ( dirname ) + \"/\" yield glob2re ( filename )", "predictions": ["get a version of version of a version of version attrib"], "references": ["generate parts of regex transformed from glob pattern ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 3247, "code": "def compile glob ( spec ) : parsed = \"\" . join ( parse glob ( spec ) ) regex = \"^{0}$\" . format ( parsed ) return re . compile ( regex )", "predictions": ["localize a datetime object into a string"], "references": ["convert the given glob spec to a compiled regex ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3248, "code": "def included ( self , path , is dir = False ) : inclusive = None for pattern in self . patterns : if pattern . is dir == is dir and pattern . matches ( path ) : inclusive = pattern . inclusive #print('+++' if inclusive else '---', path, pattern) return inclusive", "predictions": ["strip baseurl from path . . ."], "references": ["check patterns in order last match that includes or excludes path wins . return none on undecided ."], "bleu": 0.045890725128646594, "rouge_l": 0.2223572296476306}
{"id": 3249, "code": "def build ( ctx , dput = '' , opts = '' ) : with io . open ( 'debian/changelog' , encoding = 'utf-8' ) as changes : metadata = re . match ( r'^([^ ]+) \\(([^)]+)\\) ([^;]+); urgency=(.+)$' , changes . readline ( ) . rstrip ( ) ) if not metadata : notify . failure ( 'Badly formatted top entry in changelog' ) name , version , , = metadata . groups ( ) ctx . run ( 'dpkg-buildpackage {} {}' . format ( ctx . rituals . deb . build . opts , opts ) ) if not os . path . exists ( 'dist' ) : os . makedirs ( 'dist' ) artifact pattern = '{}?{}*' . format ( name , re . sub ( r'[^- .a-z A-Z0-9]' , '?' , version ) ) changes files = [ ] for debfile in glob . glob ( '../' + artifact pattern ) : shutil . move ( debfile , 'dist' ) if debfile . endswith ( '.changes' ) : changes files . append ( os . path . join ( 'dist' , os . path . basename ( debfile ) ) ) ctx . run ( 'ls -l dist/{}' . format ( artifact pattern ) ) if dput : ctx . run ( 'dput {} {}' . format ( dput , ' ' . join ( changes files ) ) )", "predictions": ["verify the project if it s not already in the project if not already present if not"], "references": ["build a deb package ."], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 3250, "code": "def clean ( dummy ctx , docs = False , backups = False , bytecode = False , dist = False , all = False , venv = False , tox = False , extra = '' ) : cfg = config . load ( ) notify . banner ( \"Cleaning up project files\" ) venv dirs = [ 'bin' , 'include' , 'lib' , 'share' , 'local' , '.venv' ] patterns = [ 'build/' , 'pip-selfcheck.json' ] excludes = [ '.git/' , '.hg/' , '.svn/' , 'debian/*/' ] if docs or all : patterns . extend ( [ 'docs/ build/' , 'doc/ build/' ] ) if dist or all : patterns . append ( 'dist/' ) if backups or all : patterns . extend ( [ '**/*~' ] ) if bytecode or all : patterns . extend ( [ '**/*.py[co]' , '**/ pycache /' , '*.egg-info/' , cfg . srcjoin ( '*.egg-info/' ) [ len ( cfg . project root ) + 1 : ] , ] ) if venv : patterns . extend ( [ i + '/' for i in venv dirs ] ) if tox : patterns . append ( '.tox/' ) else : excludes . append ( '.tox/' ) if extra : patterns . extend ( shlex . split ( extra ) ) patterns = [ antglob . includes ( i ) for i in patterns ] + [ antglob . excludes ( i ) for i in excludes ] if not venv : patterns . extend ( [ antglob . excludes ( i + '/' ) for i in venv dirs ] ) fileset = antglob . File Set ( cfg . project root , patterns ) for name in fileset : notify . info ( 'rm {0}' . format ( name ) ) if name . endswith ( '/' ) : shutil . rmtree ( os . path . join ( cfg . project root , name ) ) else : os . unlink ( os . path . join ( cfg . project root , name ) )", "predictions": ["tag all build artifacts"], "references": ["perform house - keeping ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 3251, "code": "def build ( ctx , docs = False ) : cfg = config . load ( ) ctx . run ( \"python setup.py build\" ) if docs : for doc path in ( 'docs' , 'doc' ) : if os . path . exists ( cfg . rootjoin ( doc path , 'conf.py' ) ) : break else : doc path = None if doc path : ctx . run ( \"invoke docs\" ) else : notify . warning ( \"Cannot find either a 'docs' or 'doc' Sphinx directory!\" )", "predictions": ["pep440 to pep440 your project ."], "references": ["build the project ."], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 3252, "code": "def freeze ( ctx , local = False ) : cmd = 'pip --disable-pip-version-check freeze{}' . format ( ' --local' if local else '' ) frozen = ctx . run ( cmd , hide = 'out' ) . stdout . replace ( '\\x1b' , '#' ) with io . open ( 'frozen-requirements.txt' , 'w' , encoding = 'ascii' ) as out : out . write ( . format ( isodate ( ) ) ) out . write ( frozen ) notify . info ( \"Frozen {} requirements.\" . format ( len ( frozen . splitlines ( ) ) , ) )", "predictions": ["get the file with the given id setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup setup"], "references": ["freeze currently installed requirements ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3253, "code": "def isodate ( datestamp = None , microseconds = False ) : datestamp = datestamp or datetime . datetime . now ( ) if not microseconds : usecs = datetime . timedelta ( microseconds = datestamp . microsecond ) datestamp = datestamp - usecs return datestamp . isoformat ( b' ' if PY2 else u' ' )", "predictions": ["get the timestamp of the json file"], "references": ["return current or given time formatted according to iso - 8601 ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 3254, "code": "def get registered executable ( exe name ) : registered = None if sys . platform . startswith ( 'win' ) : if os . path . splitext ( exe name ) [ 1 ] . lower ( ) != '.exe' : exe name += '.exe' import winreg try : key = \"SOFTWARE\\\\Microsoft\\\\Windows\\\\Current Version\\\\App Paths\\\\\" + exe name value = winreg . Query Value ( winreg . HKEY LOCAL MACHINE , key ) registered = ( value , \"from HKLM\\\\\" + key ) except winreg . error : pass if registered and not os . path . exists ( registered [ 0 ] ) : registered = None return registered", "predictions": ["upload a ctx ctx"], "references": ["windows allow application paths to be registered in the registry ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 3255, "code": "def can connect to ( self , other ) : assert other . is mesh ( ) disconnected = not other . is connected ( ) and not self . is connected ( ) types differ = self . is consumed mesh ( ) != other . is consumed mesh ( ) return disconnected and types differ", "predictions": ["notify if two nodes are root workdir ."], "references": ["whether a connection can be established between those two meshes ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 3256, "code": "def file ( self , file ) : if not self . text is expected : file = Bytes Wrapper ( file , self . encoding ) self . dump to file ( file )", "predictions": ["add a reports to the open file root root"], "references": ["dump the content to a file ."], "bleu": 0.17747405280050263, "rouge_l": 0.2557651991614256}
{"id": 3257, "code": "def binary file ( self , file ) : if self . text is expected : file = Text Wrapper ( file , self . encoding ) self . dump to file ( file )", "predictions": ["format the text instance ."], "references": ["dump the ocntent into the file in binary mode ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 3258, "code": "def path ( self , path ) : mode , encoding = self . mode and encoding for open ( ) with open ( path , mode , encoding = encoding ) as file : self . dump to file ( file )", "predictions": ["if description is not defined if not ."], "references": ["saves the dump in a file named path ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3259, "code": "def set pixel and convert color ( self , x , y , color ) : if color is None : return color = self . convert color to rrggbb ( color ) self . set pixel ( x , y , color )", "predictions": ["capture the * * = * * * * * * * * cmd * get * ."], "references": ["set the pixel but convert the color before ."], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 3260, "code": "def instructions changed ( self , change ) : if change . adds ( ) : for index , instruction in change . items ( ) : if isinstance ( instruction , dict ) : in row = self . parser . instruction in row ( self , instruction ) self . instructions [ index ] = in row else : instruction . transfer to row ( self )", "predictions": ["run run changed in list of run ."], "references": ["call when there is a change in the instructions ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3261, "code": "def dump knitting pattern ( self , file ) : knitting pattern set = self . on dump ( ) knitting pattern = knitting pattern set . patterns . at ( 0 ) layout = Grid Layout ( knitting pattern ) builder = AYABPNG Builder ( * layout . bounding box ) builder . set colors in grid ( layout . walk instructions ( ) ) builder . write to file ( file )", "predictions": ["provider for knitting workdir ."], "references": ["dump a knitting pattern to a file ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3262, "code": "def start ( self ) : self . instruction library = self . spec . new default instructions ( ) self . as instruction = self . instruction library . as instruction self . id cache = { } self . pattern set = None self . inheritance todos = [ ] self . instruction todos = [ ]", "predictions": ["fail the stderr = 0 = 0 = 0 = 0 = 0 = 1 = 0"], "references": ["initialize the parsing process ."], "bleu": 0.07223943354597204, "rouge_l": 0.10082644628099173}
{"id": 3263, "code": "def finish inheritance ( self ) : while self . inheritance todos : prototype , parent id = self . inheritance todos . pop ( ) parent = self . id cache [ parent id ] prototype . inherit from ( parent )", "predictions": ["mark all pypi as finished as per the parent . . . . . . ."], "references": ["finish those who still need to inherit ."], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 3264, "code": "def finish instructions ( self ) : while self . instruction todos : row = self . instruction todos . pop ( ) instructions = row . get ( INSTRUCTIONS , [ ] ) row . instructions . extend ( instructions )", "predictions": ["finish instructions in the instruction ."], "references": ["finish those who still need to inherit ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3265, "code": "def fill pattern collection ( self , pattern collection , values ) : pattern = values . get ( PATTERNS , [ ] ) for pattern to parse in pattern : parsed pattern = self . pattern ( pattern to parse ) pattern collection . append ( parsed pattern )", "predictions": ["fill the pattern with a pattern in the collection of values ."], "references": ["fill a pattern collection ."], "bleu": 0.16261701715194898, "rouge_l": 0.6354166666666666}
{"id": 3266, "code": "def row ( self , values ) : row id = self . to id ( values [ ID ] ) row = self . spec . new row ( row id , values , self ) if SAME AS in values : self . delay inheritance ( row , self . to id ( values [ SAME AS ] ) ) self . delay instructions ( row ) self . id cache [ row id ] = row return row", "predictions": ["return a row with the given values"], "references": ["parse a row ."], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 3267, "code": "def pattern ( self , base ) : rows = self . rows ( base . get ( ROWS , [ ] ) ) self . finish inheritance ( ) self . finish instructions ( ) self . connect rows ( base . get ( CONNECTIONS , [ ] ) ) id = self . to id ( base [ ID ] ) name = base [ NAME ] return self . new pattern ( id , name , rows )", "predictions": ["pattern for this widget ."], "references": ["parse a pattern ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 3268, "code": "def rows ( self , spec ) : rows = self . new row collection ( ) for row in spec : rows . append ( self . row ( row ) ) return rows", "predictions": ["return a list of rows for the given spec ."], "references": ["parse a collection of rows ."], "bleu": 0.18850319022747347, "rouge_l": 0.5236051502145923}
{"id": 3269, "code": "def connect rows ( self , connections ) : for connection in connections : from row id = self . to id ( connection [ FROM ] [ ID ] ) from row = self . id cache [ from row id ] from row start index = connection [ FROM ] . get ( START , DEFAULT START ) from row number of possible meshes = from row . number of produced meshes - from row start index to row id = self . to id ( connection [ TO ] [ ID ] ) to row = self . id cache [ to row id ] to row start index = connection [ TO ] . get ( START , DEFAULT START ) to row number of possible meshes = to row . number of consumed meshes - to row start index meshes = min ( from row number of possible meshes , to row number of possible meshes ) number of meshes = connection . get ( MESHES , meshes ) from row stop index = from row start index + number of meshes to row stop index = to row start index + number of meshes assert 0 <= from row start index <= from row stop index produced meshes = from row . produced meshes [ from row start index : from row stop index ] assert 0 <= to row start index <= to row stop index consumed meshes = to row . consumed meshes [ to row start index : to row stop index ] assert len ( produced meshes ) == len ( consumed meshes ) mesh pairs = zip ( produced meshes , consumed meshes ) for produced mesh , consumed mesh in mesh pairs : produced mesh . connect to ( consumed mesh )", "predictions": ["connect connections between connections and stop them"], "references": ["connect the parsed rows ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3270, "code": "def create pattern set ( self , pattern , values ) : type = self . get type ( values ) version = self . get version ( values ) comment = values . get ( COMMENT ) self . pattern set = self . spec . new pattern set ( type , version , pattern , self , comment )", "predictions": ["create a pattern with a pattern and values ."], "references": ["create a new pattern set ."], "bleu": 0.21105340631872635, "rouge_l": 0.5532879818594103}
{"id": 3271, "code": "def write ( self , bytes ) : string = bytes . decode ( self . encoding ) self . file . write ( string )", "predictions": ["write the given string to the file ."], "references": ["write bytes to the file ."], "bleu": 0.46713797772820004, "rouge_l": 0.7331730769230769}
{"id": 3272, "code": "def write ( self , string ) : bytes = string . encode ( self . encoding ) self . file . write ( bytes )", "predictions": ["write a string to the file ."], "references": ["write a string to the file ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3273, "code": "def width ( self ) : layout = self . instruction . get ( GRID LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . instruction . number of consumed meshes", "predictions": ["width of the consumed instruction"], "references": ["for self . width ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3274, "code": "def step ( self , row , position , passed ) : if row in passed or not self . row should be placed ( row , position ) : return self . place row ( row , position ) passed = [ row ] + passed for i , produced mesh in enumerate ( row . produced meshes ) : self . expand produced mesh ( produced mesh , i , position , passed ) for i , consumed mesh in enumerate ( row . consumed meshes ) : self . expand consumed mesh ( consumed mesh , i , position , passed )", "predictions": ["perform a consumed step ."], "references": ["walk through the knitting pattern by expanding an row ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 3275, "code": "def expand consumed mesh ( self , mesh , mesh index , row position , passed ) : if not mesh . is produced ( ) : return row = mesh . producing row position = Point ( row position . x + mesh . index in producing row - mesh index , row position . y - INSTRUCTION HEIGHT ) self . expand ( row , position , passed )", "predictions": ["expand a consumed mesh mesh"], "references": ["expand the consumed meshes"], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 3276, "code": "def expand produced mesh ( self , mesh , mesh index , row position , passed ) : if not mesh . is consumed ( ) : return row = mesh . consuming row position = Point ( row position . x - mesh . index in consuming row + mesh index , row position . y + INSTRUCTION HEIGHT ) self . expand ( row , position , passed )", "predictions": ["expand a mesh mesh at a given mesh ."], "references": ["expand the produced meshes"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3277, "code": "def place row ( self , row , position ) : self . rows in grid [ row ] = Row In Grid ( row , position )", "predictions": ["place a row at the given row"], "references": ["place the instruction on a grid"], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 3278, "code": "def walk ( self ) : while self . todo : args = self . todo . pop ( 0 ) self . step ( * args )", "predictions": ["run a walk in the list ."], "references": ["loop through all the instructions that are _todo ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3279, "code": "def instruction in grid ( self , instruction ) : row position = self . rows in grid [ instruction . row ] . xy x = instruction . index of first consumed mesh in row position = Point ( row position . x + x , row position . y ) return Instruction In Grid ( instruction , position )", "predictions": ["return the instruction in the grid in the grid"], "references": ["returns an instructioningrid object for the instruction"], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 3280, "code": "def dump to file ( self , file ) : xmltodict . unparse ( self . object ( ) , file , pretty = True )", "predictions": ["dump this instance to a given file ."], "references": ["dump to the file"], "bleu": 0.19070828081828378, "rouge_l": 0.5319767441860466}
{"id": 3281, "code": "def rsolve ( A , y ) : from numpy sugar . linalg import rsolve as rsolve try : beta = rsolve ( A , y ) except Lin Alg Error : msg = \"Could not converge to solve Ax=y.\" msg += \" Setting x to zero.\" warnings . warn ( msg , Runtime Warning ) beta = zeros ( A . shape [ 0 ] ) return beta", "predictions": ["create a beta from a numpy array ."], "references": ["robust solve ax = y ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3282, "code": "def B ( self ) : return unvec ( self . vec B . value , ( self . X . shape [ 1 ] , self . A . shape [ 0 ] ) )", "predictions": ["return the polynomial polynomial ."], "references": ["effect - sizes parameter b ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3283, "code": "def posteriori covariance ( self ) : K = GLMM . covariance ( self ) tau = self . ep . posterior . tau return pinv ( pinv ( K ) + diag ( 1 / tau ) )", "predictions": ["return the posteriori covariance"], "references": ["r covariance of the estimated posteriori ."], "bleu": 0.20183609024241697, "rouge_l": 0.346590909090909}
{"id": 3284, "code": "def economic qs zeros ( n ) : Q0 = empty ( ( n , 0 ) ) Q1 = eye ( n ) S0 = empty ( 0 ) return ( ( Q0 , Q1 ) , S0 )", "predictions": ["create n - th 9 9 ."], "references": ["eigen decomposition of a zero matrix ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3285, "code": "def value ( self ) : if not self . fix [ \"beta\" ] : self . update beta ( ) if not self . fix [ \"scale\" ] : self . update scale ( ) return self . lml ( )", "predictions": ["the value of the object ."], "references": ["internal use only ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 3286, "code": "def delta ( self ) : v = float ( self . logistic . value ) if v > 0.0 : v = 1 / ( 1 + exp ( - v ) ) else : v = exp ( v ) v = v / ( v + 1.0 ) return min ( max ( v , epsilon . tiny ) , 1 - epsilon . tiny )", "predictions": ["calculate the delta in seconds"], "references": ["variance ratio between k and i ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 3287, "code": "def df ( self ) : if not self . restricted : return self . nsamples return self . nsamples - self . X [ \"t X\" ] . shape [ 1 ]", "predictions": ["returns the best probability of the objective ."], "references": ["degrees of freedom ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 3288, "code": "def setup smtp factory ( * * settings ) : return Custom SMTP ( host = settings . get ( 'mail.host' , 'localhost' ) , port = int ( settings . get ( 'mail.port' , 25 ) ) , user = settings . get ( 'mail.user' ) , password = settings . get ( 'mail.password' ) , timeout = float ( settings . get ( 'mail.timeout' , 60 ) ) , )", "predictions": ["returns a smtp factory instance ."], "references": ["expects a dictionary with mail . keys to create an appropriate smtplib . smtp instance"], "bleu": 0.06197705798903779, "rouge_l": 0.26521739130434785}
{"id": 3289, "code": "def begin ( self ) : self . connect ( self . host , self . port ) if self . user : self . starttls ( ) self . login ( self . user , self . password )", "predictions": ["start the user and start a new socket ."], "references": ["connects and optionally authenticates a connection ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 3290, "code": "def create application ( connection : Optional [ str ] = None ) -> Flask : app = Flask ( name ) flask bootstrap . Bootstrap ( app ) Admin ( app ) connection = connection or DEFAULT CACHE CONNECTION engine , session = build engine session ( connection ) for name , add admin in add admins . items ( ) : url = '/{}' . format ( name ) add admin ( app , session , url = url , endpoint = name , name = name ) log . debug ( 'added %s - %s to %s' , name , add admin , url ) app . register blueprint ( ui ) return app", "predictions": ["create an application instance ."], "references": ["create a flask application ."], "bleu": 0.32466791547509893, "rouge_l": 0.6}
{"id": 3291, "code": "def upload backend ( index = 'dev' , user = None ) : get vars ( ) use devpi ( index = index ) with fab . lcd ( '../application' ) : fab . local ( 'make upload' )", "predictions": ["upload backend to local backend ."], "references": ["build the backend and upload it to the remote server at the given index"], "bleu": 0.06924459302580939, "rouge_l": 0.18654434250764526}
{"id": 3292, "code": "async def update ( self ) -> None : LOGGER . debug ( \"Requesting state update from server (S00, S14)\" ) await asyncio . gather ( self . send command ( 'S00' ) , self . send command ( 'S14' ) , )", "predictions": ["update state from server ."], "references": ["force update of alarm status and zones"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3293, "code": "async def update loop ( self ) -> None : await asyncio . sleep ( self . update interval ) while not self . closed : await self . update ( ) await asyncio . sleep ( self . update interval )", "predictions": ["update loop loop ."], "references": ["schedule a state update to keep the connection alive"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3294, "code": "def iterate namespace models ( self , * * kwargs ) -> Iterable : return tqdm ( self . get query ( self . namespace model ) , total = self . count model ( self . namespace model ) , * * kwargs )", "predictions": ["gets the tqdm values for the given namespace ."], "references": ["return an iterator over the models to be converted to the namespace ."], "bleu": 0.135323305042906, "rouge_l": 0.35209235209235207}
{"id": 3295, "code": "def get default namespace ( self ) -> Optional [ Namespace ] : return self . get query ( Namespace ) . filter ( Namespace . url == self . get namespace url ( ) ) . one or none ( )", "predictions": ["get a default namespace for this collection ."], "references": ["get the reference bel namespace if it exists ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 3296, "code": "def make namespace ( self ) -> Namespace : namespace = Namespace ( name = self . get namespace name ( ) , keyword = self . get namespace keyword ( ) , url = self . get namespace url ( ) , version = str ( time . asctime ( ) ) , ) self . session . add ( namespace ) entries = self . get namespace entries ( namespace ) self . session . add all ( entries ) t = time . time ( ) log . info ( 'committing models' ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t ) return namespace", "predictions": ["finish a instructions for this instance"], "references": ["make a namespace ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 3297, "code": "def add namespace to graph ( self , graph : BEL Graph ) -> Namespace : namespace = self . upload bel namespace ( ) graph . namespace url [ namespace . keyword ] = namespace . url self . add annotation to graph ( graph ) return namespace", "predictions": ["fill a pattern collection with a to the to the to the to the to the to the to the to the to the to the to the to the to"], "references": ["add this manager s namespace to the graph ."], "bleu": 0.0513487742994337, "rouge_l": 0.11101000909918107}
{"id": 3298, "code": "def add annotation to graph ( self , graph : BEL Graph ) -> None : if 'bio2bel' not in graph . annotation list : graph . annotation list [ 'bio2bel' ] = set ( ) graph . annotation list [ 'bio2bel' ] . add ( self . module name )", "predictions": ["row should be a graph or a graph in the graph"], "references": ["add this manager as an annotation to the graph ."], "bleu": 0.14991106946711685, "rouge_l": 0.1921259842519685}
{"id": 3299, "code": "def drop bel namespace ( self ) -> Optional [ Namespace ] : namespace = self . get default namespace ( ) if namespace is not None : for entry in tqdm ( namespace . entries , desc = f'deleting entries in {self. get namespace name()}' ) : self . session . delete ( entry ) self . session . delete ( namespace ) log . info ( 'committing deletions' ) self . session . commit ( ) return namespace", "predictions": ["pattern the bel self . . ."], "references": ["remove the default namespace if it exists ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3300, "code": "def write bel namespace ( self , file : Text IO , use names : bool = False ) -> None : if not self . is populated ( ) : self . populate ( ) if use names and not self . has names : raise Value Error values = ( self . get namespace name to encoding ( desc = 'writing names' ) if use names else self . get namespace identifier to encoding ( desc = 'writing identifiers' ) ) write namespace ( namespace name = self . get namespace name ( ) , namespace keyword = self . get namespace keyword ( ) , namespace query url = self . identifiers url , values = values , file = file , )", "predictions": ["rows rows are defined as per the database . ."], "references": ["write as a bel namespace file ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 3301, "code": "def write bel annotation ( self , file : Text IO ) -> None : if not self . is populated ( ) : self . populate ( ) values = self . get namespace name to encoding ( desc = 'writing names' ) write annotation ( keyword = self . get namespace keyword ( ) , citation name = self . get namespace name ( ) , description = '' , values = values , file = file , )", "predictions": ["connect the rows to the file file ."], "references": ["write as a bel annotation file ."], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 3302, "code": "def write bel namespace mappings ( self , file : Text IO , * * kwargs ) -> None : json . dump ( self . get namespace identifier to name ( * * kwargs ) , file , indent = 2 , sort keys = True )", "predictions": ["writes the pattern mappings mappings mappings to a file"], "references": ["write a bel namespace mapping file ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 3303, "code": "def write directory ( self , directory : str ) -> bool : current md5 hash = self . get namespace hash ( ) md5 hash path = os . path . join ( directory , f'{self.module name}.belns.md5' ) if not os . path . exists ( md5 hash path ) : old md5 hash = None else : with open ( md5 hash path ) as file : old md5 hash = file . read ( ) . strip ( ) if old md5 hash == current md5 hash : return False with open ( os . path . join ( directory , f'{self.module name}.belns' ) , 'w' ) as file : self . write bel namespace ( file , use names = False ) with open ( md5 hash path , 'w' ) as file : print ( current md5 hash , file = file ) if self . has names : with open ( os . path . join ( directory , f'{self.module name}-names.belns' ) , 'w' ) as file : self . write bel namespace ( file , use names = True ) with open ( os . path . join ( directory , f'{self.module name}.belns.mapping' ) , 'w' ) as file : self . write bel namespace mappings ( file , desc = 'writing mapping' ) return True", "predictions": ["write the bel to the bel file"], "references": ["write a bel namespace for identifiers names name hash and mappings to the given directory ."], "bleu": 0.07678812443288274, "rouge_l": 0.3249001331557923}
{"id": 3304, "code": "def get long description ( ) : with codecs . open ( os . path . join ( HERE , 'README.rst' ) , encoding = 'utf-8' ) as f : long description = f . read ( ) return long description", "predictions": ["return the long self = long = long = none = long = long = 0 = 1 = 1 = 0 = 1 = 1 = 1 = 0 ="], "references": ["get the long_description from the readme . rst file . assume utf - 8 encoding ."], "bleu": 0.03901663112717908, "rouge_l": 0.04515173945225759}
{"id": 3305, "code": "def dropbox factory ( request ) : try : return request . registry . settings [ 'dropbox container' ] . get dropbox ( request . matchdict [ 'drop id' ] ) except Key Error : raise HTTP Not Found ( 'no such dropbox' )", "predictions": [". width of width"], "references": ["expects the id of an existing dropbox and returns its instance"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 3306, "code": "def dropbox editor factory ( request ) : dropbox = dropbox factory ( request ) if is equal ( dropbox . editor token , request . matchdict [ 'editor token' ] . encode ( 'utf-8' ) ) : return dropbox else : raise HTTP Not Found ( 'invalid editor token' )", "predictions": ["self . step or editor or for an editor or editor or . or for it ."], "references": ["this factory also requires the editor token"], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 3307, "code": "def sanitize filename ( filename ) : token = generate drop id ( ) name , extension = splitext ( filename ) if extension : return '%s%s' % ( token , extension ) else : return token", "predictions": ["expand the consumed consumed consumed consumed consumed consumed file ."], "references": ["preserve the file ending but replace the name with a random token"], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 3308, "code": "def create archive ( self ) : self . status = u'270 creating final encrypted backup of cleansed attachments' return self . create encrypted zip ( source = 'clean' , fs target dir = self . container . fs archive cleansed )", "predictions": ["expand the produced produced as a new produced produced file index index index index"], "references": ["creates an encrypted archive of the dropbox outside of the drop directory ."], "bleu": 0.08839374326825923, "rouge_l": 0.0745721271393643}
{"id": 3309, "code": "def size attachments ( self ) : total size = 0 for attachment in self . fs cleansed attachments : total size += stat ( attachment ) . st size return total size", "predictions": ["returns the total place place . ."], "references": ["returns the number of bytes that the cleansed attachments take up on disk"], "bleu": 0.10374282717383708, "rouge_l": 0.1897356143079316}
{"id": 3310, "code": "def replies ( self ) : fs reply path = join ( self . fs replies path , 'message 001.txt' ) if exists ( fs reply path ) : return [ load ( open ( fs reply path , 'r' ) ) ] else : return [ ]", "predictions": ["read the walk in the file system pop it if it is already ."], "references": ["returns a list of strings"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 3311, "code": "def message ( self ) : try : with open ( join ( self . fs path , u'message' ) ) as message file : return u'' . join ( [ line . decode ( 'utf-8' ) for line in message file . readlines ( ) ] ) except IO Error : return u''", "predictions": ["get the instruction from the file"], "references": ["returns the user submitted text"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3312, "code": "def fs dirty attachments ( self ) : if exists ( self . fs attachment container ) : return [ join ( self . fs attachment container , attachment ) for attachment in listdir ( self . fs attachment container ) ] else : return [ ]", "predictions": ["find all file file file file in the unparse"], "references": ["returns a list of absolute paths to the attachements"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3313, "code": "def fs cleansed attachments ( self ) : if exists ( self . fs cleansed attachment container ) : return [ join ( self . fs cleansed attachment container , attachment ) for attachment in listdir ( self . fs cleansed attachment container ) ] else : return [ ]", "predictions": ["import all cleansed in a source file"], "references": ["returns a list of absolute paths to the cleansed attachements"], "bleu": 0.13391424795650428, "rouge_l": 0.11401869158878504}
{"id": 3314, "code": "def dropbox form ( request ) : from briefkasten import generate post token token = generate post token ( secret = request . registry . settings [ 'post secret' ] ) return dict ( action = request . route url ( 'dropbox form submit' , token = token ) , fileupload url = request . route url ( 'dropbox fileupload' , token = token ) , * * defaults ( request ) )", "predictions": ["generates a dropbox form form form"], "references": ["generates a dropbox uid and renders the submission form with a signed version of that id"], "bleu": 0.0821081667546808, "rouge_l": 0.3360881542699724}
{"id": 3315, "code": "def dropbox fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )", "predictions": ["convert an = = = = a 1 - d d d to a 1 . x ."], "references": ["accepts a single file upload and adds it to the dropbox as attachment"], "bleu": 0.07535838128770536, "rouge_l": 0.13289760348583876}
{"id": 3316, "code": "def dropbox submission ( dropbox , request ) : try : data = dropbox schema . deserialize ( request . POST ) except Exception : return HTTP Found ( location = request . route url ( 'dropbox form' ) ) dropbox . message = data . get ( 'message' ) if 'testing secret' in dropbox . settings : dropbox . from watchdog = is equal ( unicode ( dropbox . settings [ 'test submission secret' ] ) , data . pop ( 'testing secret' , u'' ) ) if data . get ( 'upload' ) is not None : dropbox . add attachment ( data [ 'upload' ] ) dropbox . submit ( ) drop url = request . route url ( 'dropbox view' , drop id = dropbox . drop id ) print ( \"Created dropbox %s\" % drop url ) return HTTP Found ( location = drop url )", "predictions": ["submit a economic qs qs"], "references": ["handles the form submission redirects to the dropbox s status page ."], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 3317, "code": "def belns ( keyword : str , file : Text IO , encoding : Optional [ str ] , use names : bool ) : directory = get data dir ( keyword ) obo url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo path = os . path . join ( directory , f'{keyword}.obo' ) obo cache path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo getter = make obo getter ( obo url , obo path , preparsed path = obo cache path ) graph = obo getter ( ) convert obo graph to belns ( graph , file = file , encoding = encoding , use names = use names , )", "predictions": ["value to a self . . . instance ."], "references": ["write as a bel namespace ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 3318, "code": "def belanno ( keyword : str , file : Text IO ) : directory = get data dir ( keyword ) obo url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo path = os . path . join ( directory , f'{keyword}.obo' ) obo cache path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo getter = make obo getter ( obo url , obo path , preparsed path = obo cache path ) graph = obo getter ( ) convert obo graph to belanno ( graph , file = file , )", "predictions": ["convert a obo file to an existing self . . . . . . ."], "references": ["write as a bel annotation ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 3319, "code": "def store helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = make session ( ) session . add ( model ) session . commit ( ) session . close ( )", "predictions": ["df helper helper helper to . . . ."], "references": ["help store an action ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 3320, "code": "def make session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get global connection ( ) engine = create engine ( connection ) create all ( engine ) session cls = sessionmaker ( bind = engine ) session = session cls ( ) return session", "predictions": ["password for the database connection"], "references": ["make a session ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3321, "code": "def create all ( engine , checkfirst = True ) : Base . metadata . create all ( bind = engine , checkfirst = checkfirst )", "predictions": ["begin all the database tables user for a given file user user user user user user ."], "references": ["create the tables for bio2bel ."], "bleu": 0.09083627868206415, "rouge_l": 0.38065522620904835}
{"id": 3322, "code": "def ls ( cls , session : Optional [ Session ] = None ) -> List [ 'Action' ] : if session is None : session = make session ( ) actions = session . query ( cls ) . order by ( cls . created . desc ( ) ) . all ( ) session . close ( ) return actions", "predictions": ["list all actions actions . . . . . . . . . . . . . . . . . . . . . ."], "references": ["get all actions ."], "bleu": 0.07305891545072536, "rouge_l": 0.23047858942065497}
{"id": 3323, "code": "def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = make session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count", "predictions": ["get the number of entries in the database lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd lcd"], "references": ["count all actions ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3324, "code": "def get module config cls ( module name : str ) -> Type [ Abstract Module Config ] : class Module Config ( Abstract Module Config ) : NAME = f'bio2bel:{module name}' FILES = DEFAULT CONFIG PATHS + [ os . path . join ( DEFAULT CONFIG DIRECTORY , module name , 'config.ini' ) ] return Module Config", "predictions": ["return update update update update update update update update update update update update update update . . . with the update update . . . . . . . . ."], "references": ["build a module configuration class ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3325, "code": "def get modules ( ) -> Mapping : modules = { } for entry point in iter entry points ( group = 'bio2bel' , name = None ) : entry = entry point . name try : modules [ entry ] = entry point . load ( ) except Version Conflict as exc : log . warning ( 'Version conflict in %s: %s' , entry , exc ) continue except Unknown Extra as exc : log . warning ( 'Unknown extra in %s: %s' , entry , exc ) continue except Import Error as exc : log . exception ( 'Issue with importing module %s: %s' , entry , exc ) continue return modules", "predictions": ["return list of update update update update update update update update update . . . . . . . . . . . . . . . . . . ."], "references": ["get all bio2bel modules ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 3326, "code": "def clear cache ( module name : str , keep database : bool = True ) -> None : data dir = get data dir ( module name ) if not os . path . exists ( data dir ) : return for name in os . listdir ( data dir ) : if name in { 'config.ini' , 'cfg.ini' } : continue if name == 'cache.db' and keep database : continue path = os . path . join ( data dir , name ) if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path ) os . rmdir ( data dir )", "predictions": ["iterate over a module namespace and remove all its data"], "references": ["clear all downloaded files ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 3327, "code": "def iterate managers ( connection , skip ) : for idx , name , manager cls in iterate manage classes ( skip ) : if name in skip : continue try : manager = manager cls ( connection = connection ) except Type Error as e : click . secho ( f'Could not instantiate {name}: {e}' , fg = 'red' ) else : yield idx , name , manager", "predictions": ["return all available default filter filter"], "references": ["iterate over instantiated managers ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3328, "code": "def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear cache ( name )", "predictions": ["clear all cache of the skip cache ."], "references": ["clear all caches ."], "bleu": 0.22679164443904004, "rouge_l": 0.5319767441860466}
{"id": 3329, "code": "def sheet ( connection , skip , file : Text IO ) : from tabulate import tabulate header = [ '' , 'Name' , 'Description' , 'Terms' , 'Relations' ] rows = [ ] for i , ( idx , name , manager ) in enumerate ( iterate managers ( connection , skip ) , start = 1 ) : try : if not manager . is populated ( ) : continue except Attribute Error : click . secho ( f'{name} does not implement is populated' , fg = 'red' ) continue terms , relations = None , None if isinstance ( manager , BEL Namespace Manager Mixin ) : terms = manager . count model ( manager . namespace model ) if isinstance ( manager , BEL Manager Mixin ) : try : relations = manager . count relations ( ) except Type Error as e : relations = str ( e ) rows . append ( ( i , name , manager . doc . split ( '\\n' ) [ 0 ] . strip ( ) . strip ( '.' ) , terms , relations ) ) print ( tabulate ( rows , headers = header , ) )", "predictions": ["show a sheet in a file"], "references": ["generate a summary sheet ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 3330, "code": "def write ( connection , skip , directory , force ) : os . makedirs ( directory , exist ok = True ) from . manager . bel manager import BEL Manager Mixin import pybel for idx , name , manager in iterate managers ( connection , skip ) : if not isinstance ( manager , BEL Manager Mixin ) : continue click . secho ( name , fg = 'cyan' , bold = True ) path = os . path . join ( directory , f'{name}.bel.pickle' ) if os . path . exists ( path ) and not force : click . echo ( '\ud83d\udc4d already exported') continue if not manager . is populated ( ) : click . echo ( '\ud83d\udc4e unpopulated') else : graph = manager . to bel ( ) pybel . to pickle ( graph , path ) pybel . to json path ( graph , os . path . join ( directory , f'{name}.bel.json' ) )", "predictions": ["write data to a bel pybel file ."], "references": ["write all as bel ."], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 3331, "code": "def web ( connection , host , port ) : from bio2bel . web . application import create application app = create application ( connection = connection ) app . run ( host = host , port = port )", "predictions": ["start an application server"], "references": ["run a combine web interface ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3332, "code": "def actions ( connection ) : session = make session ( connection = connection ) for action in Action . ls ( session = session ) : click . echo ( f'{action.created} {action.action} {action.resource}' )", "predictions": ["list all actions ."], "references": ["list all actions ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3333, "code": "def count relations ( self ) -> int : if self . edge model is ... : raise Bio2BEL Missing Edge Model Error ( 'edge edge model is undefined/count bel relations is not overridden' ) elif isinstance ( self . edge model , list ) : return sum ( self . count model ( m ) for m in self . edge model ) else : return self . count model ( self . edge model )", "predictions": ["get the number of bel relations in the bel edge model ."], "references": ["count the number of bel relations generated ."], "bleu": 0.39553325358771796, "rouge_l": 0.6224489795918368}
{"id": 3334, "code": "def get exif info ( self ) : dict = { } for tag in EXIF TAGS : ret = self . img . attribute ( \"EXIF:%s\" % tag ) if ret and ret != 'unknown' : dict [ tag ] = ret return dict", "predictions": ["returns a dictionary of exif information"], "references": ["return exif - tag dict"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3335, "code": "def get version from pc ( search dirs , target ) : for dirname in search dirs : for root , dirs , files in os . walk ( dirname ) : for f in files : if f == target : file path = os . path . join ( root , target ) tmp = grep ( \"Version: \" , file path ) version = tmp . split ( ) [ 1 ] print ( \"Found version %s in file %s\" % ( version , file path ) ) return version", "predictions": ["get version number from pc"], "references": ["similar to pkg - config -- modversion graphicsmagick ++"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 3336, "code": "def version ( ) : with io . open ( 'pgmagick/ version.py' ) as input file : for line in input file : if line . startswith ( ' version ' ) : return ast . parse ( line ) . body [ 0 ] . value . s", "predictions": ["get the version number from the package"], "references": ["return version string ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3337, "code": "def post license request ( request ) : uuid = request . matchdict [ 'uuid' ] posted data = request . json license url = posted data . get ( 'license url' ) licensors = posted data . get ( 'licensors' , [ ] ) with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( uuid , ) ) try : existing license url = cursor . fetchone ( ) [ 0 ] except Type Error : if request . has permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid , ) ) existing license url = None else : raise httpexceptions . HTTP Not Found ( ) if existing license url is None and license url is None : raise httpexceptions . HTTP Bad Request ( \"license url is required\" ) elif ( license url != existing license url or existing license url is None ) : cursor . execute ( , ( license url , ) ) try : cursor . fetchone ( ) [ 0 ] except Type Error : raise httpexceptions . HTTP Bad Request ( \"invalid license url\" ) upsert license requests ( cursor , uuid , licensors ) resp = request . response resp . status int = 202 return resp", "predictions": ["post a license request to the license ."], "references": ["submission to create a license acceptance request ."], "bleu": 0.25098621243978964, "rouge_l": 0.5}
{"id": 3338, "code": "def delete license request ( request ) : uuid = request . matchdict [ 'uuid' ] posted uids = [ x [ 'uid' ] for x in request . json . get ( 'licensors' , [ ] ) ] with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : remove license requests ( cursor , uuid , posted uids ) resp = request . response resp . status int = 200 return resp", "predictions": ["delete license requests ."], "references": ["submission to remove a license acceptance request ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 3339, "code": "def get roles request ( request ) : uuid = request . matchdict [ 'uuid' ] user id = request . matchdict . get ( 'uid' ) args = [ uuid ] if user id is not None : fmt conditional = \"AND user id = %s\" args . append ( user id ) else : fmt conditional = \"\" with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( . format ( fmt conditional ) , args ) acceptances = [ r [ 0 ] for r in cursor . fetchall ( ) ] if not acceptances : if user id is not None : raise httpexceptions . HTTP Not Found ( ) else : cursor . execute ( , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except Type Error : raise httpexceptions . HTTP Not Found ( ) resp value = acceptances if user id is not None : resp value = acceptances [ 0 ] return resp value", "predictions": ["request roles from server ."], "references": ["returns a list of accepting roles ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 3340, "code": "def post roles request ( request ) : uuid = request . matchdict [ 'uuid' ] posted roles = request . json with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except Type Error : if request . has permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid , ) ) else : raise httpexceptions . HTTP Not Found ( ) try : upsert users ( cursor , [ r [ 'uid' ] for r in posted roles ] ) except User Fetch Error as exc : raise httpexceptions . HTTP Bad Request ( exc . message ) upsert role requests ( cursor , uuid , posted roles ) resp = request . response resp . status int = 202 return resp", "predictions": ["request role requests ."], "references": ["submission to create a role acceptance request ."], "bleu": 0.1571901051328651, "rouge_l": 0.31443298969072164}
{"id": 3341, "code": "def delete roles request ( request ) : uuid = request . matchdict [ 'uuid' ] posted roles = request . json with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : remove role requests ( cursor , uuid , posted roles ) resp = request . response resp . status int = 200 return resp", "predictions": ["delete role requests ."], "references": ["submission to remove a role acceptance request ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 3342, "code": "def get acl ( request ) : uuid = request . matchdict [ 'uuid' ] with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except Type Error : raise httpexceptions . HTTP Not Found ( ) cursor . execute ( , ( uuid , ) ) acl = [ r [ 0 ] for r in cursor . fetchall ( ) ] return acl", "predictions": ["return list of all acl ."], "references": ["returns the acl for the given content identified by uuid ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3343, "code": "def post acl request ( request ) : uuid = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except Type Error : if request . has permission ( 'publish.create-identifier' ) : cursor . execute ( , ( uuid , ) ) else : raise httpexceptions . HTTP Not Found ( ) upsert acl ( cursor , uuid , permissions ) resp = request . response resp . status int = 202 return resp", "predictions": ["post acl request ."], "references": ["submission to create an acl ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 3344, "code": "def delete acl request ( request ) : uuid = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : remove acl ( cursor , uuid , permissions ) resp = request . response resp . status int = 200 return resp", "predictions": ["delete acl request ."], "references": ["submission to remove an acl ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 3345, "code": "def lookup api key info ( ) : info = { } with db connect ( ) as conn : with conn . cursor ( ) as cursor : cursor . execute ( ALL KEY INFO SQL STMT ) for row in cursor . fetchall ( ) : id , key , name , groups = row user id = \"api key:{}\" . format ( id ) info [ key ] = dict ( id = id , user id = user id , name = name , groups = groups ) return info", "predictions": ["query api key info ."], "references": ["given a dbapi cursor lookup all the api keys and their information ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 3346, "code": "def includeme ( config ) : api key authn policy = API Key Authentication Policy ( ) config . include ( 'openstax accounts' ) openstax authn policy = config . registry . get Utility ( I Openstax Accounts Authentication Policy ) policies = [ api key authn policy , openstax authn policy ] authn policy = Multi Authentication Policy ( policies ) config . set authentication policy ( authn policy ) authz policy = ACL Authorization Policy ( ) config . set authorization policy ( authz policy )", "predictions": ["calculates and sets the current configuration"], "references": ["configuration include fuction for this module"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3347, "code": "def expandvars dict ( settings ) : return dict ( ( key , os . path . expandvars ( value ) ) for key , value in settings . iteritems ( ) )", "predictions": ["return a dict with all keys of a dict ."], "references": ["expands all environment variables in a settings dictionary ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 3348, "code": "def task ( * * kwargs ) : def wrapper ( wrapped ) : def callback ( scanner , name , obj ) : celery app = scanner . config . registry . celery app celery app . task ( * * kwargs ) ( obj ) venusian . attach ( wrapped , callback ) return wrapped return wrapper", "predictions": ["decorator that registers a task ."], "references": ["a function task decorator used in place of"], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 3349, "code": "def post publication processing ( event , cursor ) : module ident , ident hash = event . module ident , event . ident hash celery app = get current registry ( ) . celery app cursor . execute ( 'SELECT result id::text ' 'FROM document baking result associations ' 'WHERE module ident = %s' , ( module ident , ) ) for result in cursor . fetchall ( ) : state = celery app . Async Result ( result [ 0 ] ) . state if state in ( 'QUEUED' , 'STARTED' , 'RETRY' ) : logger . debug ( 'Already queued module ident={} ident hash={}' . format ( module ident , ident hash ) ) return logger . debug ( 'Queued for processing module ident={} ident hash={}' . format ( module ident , ident hash ) ) recipe ids = get recipe ids ( module ident , cursor ) update module state ( cursor , module ident , 'processing' , recipe ids [ 0 ] ) cursor . connection . commit ( ) task name = 'cnxpublishing.subscribers.baking processor' baking processor = celery app . tasks [ task name ] result = baking processor . delay ( module ident , ident hash ) baking processor . backend . store result ( result . id , None , 'QUEUED' ) track baking proc state ( result , module ident , cursor )", "predictions": ["store queued processing processing processing processing processing event ."], "references": ["process post - publication events coming out of the database ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 3350, "code": "def parse archive uri ( uri ) : parsed = urlparse ( uri ) path = parsed . path . rstrip ( '/' ) . split ( '/' ) ident hash = path [ - 1 ] ident hash = unquote ( ident hash ) return ident hash", "predictions": ["parse an archive uri ."], "references": ["given an archive uri parse to a split ident - hash ."], "bleu": 0.13867166349083973, "rouge_l": 0.4380610412926392}
{"id": 3351, "code": "def declare browsable routes ( config ) : config . add notfound view ( default exceptionresponse view , append slash = True ) add route = config . add route add route ( 'admin-index' , '/a/' ) add route ( 'admin-moderation' , '/a/moderation/' ) add route ( 'admin-api-keys' , '/a/api-keys/' ) add route ( 'admin-add-site-messages' , '/a/site-messages/' , request method = 'GET' ) add route ( 'admin-add-site-messages-POST' , '/a/site-messages/' , request method = 'POST' ) add route ( 'admin-delete-site-messages' , '/a/site-messages/' , request method = 'DELETE' ) add route ( 'admin-edit-site-message' , '/a/site-messages/{id}/' , request method = 'GET' ) add route ( 'admin-edit-site-message-POST' , '/a/site-messages/{id}/' , request method = 'POST' ) add route ( 'admin-content-status' , '/a/content-status/' ) add route ( 'admin-content-status-single' , '/a/content-status/{uuid}' ) add route ( 'admin-print-style' , '/a/print-style/' ) add route ( 'admin-print-style-single' , '/a/print-style/{style}' )", "predictions": ["called by sphinx to declare that routes a browsable route ."], "references": ["declaration of routes that can be browsed by users ."], "bleu": 0.14323145079400493, "rouge_l": 0.1921259842519685}
{"id": 3352, "code": "def includeme ( config ) : config . include ( 'pyramid jinja2' ) config . add jinja2 renderer ( '.html' ) config . add jinja2 renderer ( '.rss' ) config . add static view ( name = '/a/static' , path = \"cnxpublishing:static/\" ) config . commit ( ) from cnxdb . ident hash import join ident hash for ext in ( '.html' , '.rss' , ) : jinja2 env = config . get jinja2 environment ( ext ) jinja2 env . globals . update ( join ident hash = join ident hash , ) declare api routes ( config ) declare browsable routes ( config )", "predictions": ["declare routes with a jinja2 jinja2 jinja2 ."], "references": ["declare all routes ."], "bleu": 0.19070828081828378, "rouge_l": 0.5319767441860466}
{"id": 3353, "code": "def formatter callback factory ( ) : includes = [ ] exercise url template = '{base Url}/api/exercises?q={field}:\"{{item Code}}\"' settings = get current registry ( ) . settings exercise base url = settings . get ( 'embeddables.exercise.base url' , None ) exercise matches = [ match . split ( ',' , 1 ) for match in aslist ( settings . get ( 'embeddables.exercise.match' , '' ) , flatten = False ) ] exercise token = settings . get ( 'embeddables.exercise.token' , None ) mathml url = settings . get ( 'mathmlcloud.url' , None ) memcache servers = settings . get ( 'memcache servers' ) if memcache servers : memcache servers = memcache servers . split ( ) else : memcache servers = None if exercise base url and exercise matches : mc client = None if memcache servers : mc client = memcache . Client ( memcache servers , debug = 0 ) for ( exercise match , exercise field ) in exercise matches : template = exercise url template . format ( base Url = exercise base url , field = exercise field ) includes . append ( exercise callback factory ( exercise match , template , mc client , exercise token , mathml url ) ) return includes", "predictions": ["factory for creating all user servers ."], "references": ["returns a list of includes to be given to cnxepub . collation . collate ."], "bleu": 0.059237077985967744, "rouge_l": 0.08531468531468532}
{"id": 3354, "code": "def db connect ( connection string = None , * * kwargs ) : if connection string is None : connection string = get current registry ( ) . settings [ CONNECTION STRING ] db conn = psycopg2 . connect ( connection string , * * kwargs ) try : with db conn : yield db conn finally : db conn . close ( )", "predictions": ["connect to a postgres postgres connection"], "references": ["function to supply a database connection object ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 3355, "code": "def upsert pending licensors ( cursor , document id ) : cursor . execute ( , ( document id , ) ) uuid , metadata = cursor . fetchone ( ) acceptors = set ( [ uid for uid , type in dissect roles ( metadata ) ] ) cursor . execute ( , ( uuid , ) ) existing acceptors mapping = dict ( cursor . fetchall ( ) ) existing acceptors = set ( existing acceptors mapping . keys ( ) ) new acceptors = acceptors . difference ( existing acceptors ) for acceptor in new acceptors : cursor . execute ( , ( uuid , acceptor , ) ) cursor . execute ( , ( uuid , ) ) defectors = set ( cursor . fetchall ( ) ) if not defectors : cursor . execute ( , ( document id , ) )", "predictions": ["upsert pending licensors mapping data to elasticsearch ."], "references": ["update or insert records for pending license acceptors ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3356, "code": "def upsert pending roles ( cursor , document id ) : cursor . execute ( , ( document id , ) ) uuid , metadata = cursor . fetchone ( ) acceptors = set ( [ ( uid , role type to db type ( type ) , ) for uid , type in dissect roles ( metadata ) ] ) upsert users ( cursor , [ x [ 0 ] for x in acceptors ] ) cursor . execute ( , ( uuid , ) ) existing roles = set ( [ ( r , t , ) for r , t in cursor . fetchall ( ) ] ) existing acceptors = existing roles new acceptors = acceptors . difference ( existing acceptors ) for acceptor , type in new acceptors : cursor . execute ( , ( uuid , acceptor , type ) ) cursor . execute ( , ( uuid , ) ) defectors = set ( cursor . fetchall ( ) ) if not defectors : cursor . execute ( , ( document id , ) )", "predictions": ["upsert pending roles for a given document id ."], "references": ["update or insert records for pending document role acceptance ."], "bleu": 0.15881076016027915, "rouge_l": 0.31282051282051276}
{"id": 3357, "code": "def obtain licenses ( ) : with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( ) licenses = { r [ 0 ] : r [ 1 ] for r in cursor . fetchall ( ) } return licenses", "predictions": ["example example licenses ."], "references": ["obtain the licenses in a dictionary form keyed by url ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 3358, "code": "def validate license ( model ) : license mapping = obtain licenses ( ) try : license url = model . metadata [ 'license url' ] except Key Error : raise exceptions . Missing Required Metadata ( 'license url' ) try : license = license mapping [ license url ] except Key Error : raise exceptions . Invalid License ( license url ) if not license [ 'is valid for publication' ] : raise exceptions . Invalid License ( license url )", "predictions": ["validate that a license has a valid license mapping"], "references": ["given the model check the license is one valid for publication ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 3359, "code": "def validate model ( cursor , model ) : validate license ( model ) validate roles ( model ) required metadata = ( 'title' , 'summary' , ) for metadata key in required metadata : if model . metadata . get ( metadata key ) in [ None , '' , [ ] ] : raise exceptions . Missing Required Metadata ( metadata key ) validate derived from ( cursor , model ) validate subjects ( cursor , model )", "predictions": ["validate that a model has a valid model model ."], "references": ["validates the model using a series of checks on bits of the data ."], "bleu": 0.1004883949864497, "rouge_l": 0.24270557029177717}
{"id": 3360, "code": "def lookup document pointer ( ident hash , cursor ) : id , version = split ident hash ( ident hash , split version = True ) stmt = \"SELECT name FROM modules WHERE uuid = %s\" args = [ id ] if version and version [ 0 ] is not None : operator = version [ 1 ] is None and 'is' or '=' stmt += \" AND (major version = %s AND minor version {} %s)\" . format ( operator ) args . extend ( version ) cursor . execute ( stmt , args ) try : title = cursor . fetchone ( ) [ 0 ] except Type Error : raise Document Lookup Error ( ) else : metadata = { 'title' : title } return cnxepub . Document Pointer ( ident hash , metadata )", "predictions": ["get the fg skip skip for a particular continue for a particular continue . . . . . . . . . . . . . . . . . ."], "references": ["lookup a document by id and version ."], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 3361, "code": "def check pending document license state ( cursor , document id ) : cursor . execute ( , ( document id , ) ) try : is accepted = cursor . fetchone ( ) [ 0 ] except Type Error : is accepted = True return is accepted", "predictions": ["sheet license is pending"], "references": ["check the aggregate state on the pending document ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3362, "code": "def check pending document role state ( cursor , document id ) : cursor . execute ( , ( document id , ) ) try : is accepted = cursor . fetchone ( ) [ 0 ] except Type Error : is accepted = True return is accepted", "predictions": ["write the skip connection skip if it s not already there ."], "references": ["check the aggregate state on the pending document ."], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 3363, "code": "def update pending document state ( cursor , document id , is license accepted , are roles accepted ) : args = ( bool ( is license accepted ) , bool ( are roles accepted ) , document id , ) cursor . execute ( , args )", "predictions": ["web pending to web connection"], "references": ["update the state of the document s state values ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 3364, "code": "def check publication state ( publication id ) : with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( publication id , ) ) publication state , publication messages = cursor . fetchone ( ) return publication state , publication messages", "predictions": ["checks if the connection is valid ."], "references": ["check the publication s current state ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3365, "code": "def node to model ( tree or item , metadata = None , parent = None , lucent id = cnxepub . TRANSLUCENT BINDER ID ) : if 'contents' in tree or item : tree = tree or item binder = cnxepub . Translucent Binder ( metadata = tree ) for item in tree [ 'contents' ] : node = node to model ( item , parent = binder , lucent id = lucent id ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set title for node ( node , item [ 'title' ] ) result = binder else : item = tree or item result = cnxepub . Document Pointer ( item [ 'id' ] , metadata = item ) if parent is not None : parent . append ( result ) return result", "predictions": ["convert a count of count objects to a binder instance"], "references": ["given a tree parse to a set of models"], "bleu": 0.18850319022747347, "rouge_l": 0.31881533101045295}
{"id": 3366, "code": "def reassemble binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = node to model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set title for node ( node , item [ 'title' ] ) return binder", "predictions": ["return a exif with the given id"], "references": ["reassemble a binder object coming out of the database ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3367, "code": "def set post publications state ( cursor , module ident , state name , state message = '' ) : cursor . execute ( , ( module ident , state name , state message ) )", "predictions": ["create a new pc pc files files files files files files files files files ."], "references": ["this sets the post - publication state in the database ."], "bleu": 0.08225964699966554, "rouge_l": 0.07911802853437094}
{"id": 3368, "code": "def update module state ( cursor , module ident , state name , recipe ) : cursor . execute ( , ( state name , recipe , module ident ) )", "predictions": ["version of updating a module state input state input input state input input input state input ."], "references": ["this updates the module s state in the database ."], "bleu": 0.0859076483566362, "rouge_l": 0.2331210191082802}
{"id": 3369, "code": "def get moderation ( request ) : with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( ) moderations = [ x [ 0 ] for x in cursor . fetchall ( ) ] return moderations", "predictions": ["with a list of all the table names ."], "references": ["return the list of publications that need moderation ."], "bleu": 0.21105340631872635, "rouge_l": 0.3333333333333333}
{"id": 3370, "code": "def includeme ( config ) : settings = config . registry . settings session factory = Signed Cookie Session Factory ( settings [ 'session key' ] ) config . set session factory ( session factory )", "predictions": ["this function calculates the current matchdict matchdict matchdict matchdict matchdict matchdict = true ."], "references": ["configures the session manager"], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 3371, "code": "def get api keys ( request ) : with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( ) api keys = [ x [ 0 ] for x in cursor . fetchall ( ) ] return api keys", "predictions": ["return roles from server ."], "references": ["return the list of api keys ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 3372, "code": "def admin content status single ( request ) : uuid = request . matchdict [ 'uuid' ] try : UUID ( uuid ) except Value Error : raise httpexceptions . HTTP Bad Request ( '{} is not a valid uuid' . format ( uuid ) ) statement , sql args = get baking statuses sql ( { 'uuid' : uuid } ) with db connect ( cursor factory = Dict Cursor ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( statement , sql args ) modules = cursor . fetchall ( ) if len ( modules ) == 0 : raise httpexceptions . HTTP Bad Request ( '{} is not a book' . format ( uuid ) ) states = [ ] collection info = modules [ 0 ] for row in modules : message = '' state = row [ 'state' ] or 'PENDING' if state == 'FAILURE' : if row [ 'traceback' ] is not None : message = row [ 'traceback' ] latest recipe = row [ 'latest recipe id' ] current recipe = row [ 'recipe id' ] if ( latest recipe is not None and current recipe != latest recipe ) : state += ' stale recipe' states . append ( { 'version' : row [ 'current version' ] , 'recipe' : row [ 'recipe' ] , 'created' : str ( row [ 'created' ] ) , 'state' : state , 'state message' : message , } ) return { 'uuid' : str ( collection info [ 'uuid' ] ) , 'title' : collection info [ 'name' ] . decode ( 'utf-8' ) , 'authors' : format authors ( collection info [ 'authors' ] ) , 'print style' : collection info [ 'print style' ] , 'current recipe' : collection info [ 'recipe id' ] , 'current ident' : collection info [ 'module ident' ] , 'current state' : states [ 0 ] [ 'state' ] , 'states' : states }", "predictions": ["return request request request roles . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["returns a dictionary with all the past baking statuses of a single book ."], "bleu": 0.03901663112717908, "rouge_l": 0.04769351055512119}
{"id": 3373, "code": "def admin content status single POST ( request ) : args = admin content status single ( request ) title = args [ 'title' ] if args [ 'current state' ] == 'SUCCESS' : args [ 'response' ] = title + ' is not stale, no need to bake' return args with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( \"SELECT stateid FROM modules WHERE module ident=%s\" , vars = ( args [ 'current ident' ] , ) ) data = cursor . fetchall ( ) if len ( data ) == 0 : raise httpexceptions . HTTP Bad Request ( 'invalid module ident: {}' . format ( args [ 'current ident' ] ) ) if data [ 0 ] [ 0 ] == 5 or data [ 0 ] [ 0 ] == 6 : args [ 'response' ] = title + ' is already baking/set to bake' return args cursor . execute ( , vars = ( args [ 'current ident' ] , ) ) args [ 'response' ] = title + \" set to bake!\" return args", "predictions": ["return the request request request matchdict . . ."], "references": ["retriggers baking for a given book ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3374, "code": "def insert metadata ( cursor , model , publisher , message ) : params = model . metadata . copy ( ) params [ 'publisher' ] = publisher params [ 'publication message' ] = message params [ ' portal type' ] = model to portaltype ( model ) params [ 'summary' ] = str ( cnxepub . Document Summary Formatter ( model ) ) for person field in ATTRIBUTED ROLE KEYS : params [ person field ] = [ parse user uri ( x [ 'id' ] ) for x in params . get ( person field , [ ] ) ] params [ 'parent ident hash' ] = parse parent ident hash ( model ) if model . ident hash is not None : uuid , version = split ident hash ( model . ident hash , split version = True ) params [ ' uuid' ] = uuid params [ ' major version' ] , params [ ' minor version' ] = version cursor . execute ( \"SELECT moduleid FROM latest modules WHERE uuid = %s\" , ( uuid , ) ) try : moduleid = cursor . fetchone ( ) [ 0 ] except Type Error : moduleid = None params [ ' moduleid' ] = moduleid cursor . execute ( \"SELECT * from document controls where uuid = %s\" , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except Type Error : cursor . execute ( \"INSERT INTO document controls (uuid) VALUES (%s)\" , ( uuid , ) ) created = model . metadata . get ( 'created' , None ) stmt = MODULE INSERTION TEMPLATE . format ( * * { ' uuid ' : \"%( uuid)s::uuid\" , ' major version ' : \"%( major version)s\" , ' minor version ' : \"%( minor version)s\" , ' moduleid ' : moduleid is None and \"DEFAULT\" or \"%( moduleid)s\" , ' created ' : created is None and \"DEFAULT\" or \"%(created)s\" , } ) else : created = model . metadata . get ( 'created' , None ) stmt = MODULE INSERTION TEMPLATE . format ( * * { ' uuid ' : \"DEFAULT\" , ' major version ' : \"DEFAULT\" , ' minor version ' : \"DEFAULT\" , ' moduleid ' : \"DEFAULT\" , ' created ' : created is None and \"DEFAULT\" or \"%(created)s\" , } ) cursor . execute ( stmt , params ) module ident , ident hash = cursor . fetchone ( ) insert optional roles ( cursor , model , module ident ) return module ident , ident hash", "predictions": ["get acl from the database . . . . ."], "references": ["insert a module with the given metadata ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3375, "code": "def insert tree ( cursor , tree , parent id = None , index = 0 , is collated = False ) : if isinstance ( tree , dict ) : if tree [ 'id' ] == 'subcol' : document id = None title = tree [ 'title' ] else : cursor . execute ( , ( tree [ 'id' ] , ) ) try : document id , document title = cursor . fetchone ( ) except Type Error : raise Value Error ( \"Missing published document for '{}'.\" . format ( tree [ 'id' ] ) ) if tree . get ( 'title' , None ) : title = tree [ 'title' ] else : title = document title is latest = True cursor . execute ( TREE NODE INSERT , dict ( document id = document id , parent id = parent id , title = title , child order = index , is latest = is latest , is collated = is collated ) ) node id = cursor . fetchone ( ) [ 0 ] if 'contents' in tree : insert tree ( cursor , tree [ 'contents' ] , parent id = node id , is collated = is collated ) elif isinstance ( tree , list ) : for tree node in tree : insert tree ( cursor , tree node , parent id = parent id , index = tree . index ( tree node ) , is collated = is collated )", "predictions": ["recursively inserts a acl into the tree ."], "references": ["inserts a binder tree into the archive ."], "bleu": 0.28867513459481287, "rouge_l": 0.625}
{"id": 3376, "code": "def publish model ( cursor , model , publisher , message ) : publishers = publisher if isinstance ( publishers , list ) and len ( publishers ) > 1 : raise Value Error ( \"Only one publisher is allowed. '{}' \" \"were given: {}\" . format ( len ( publishers ) , publishers ) ) module ident , ident hash = insert metadata ( cursor , model , publisher , message ) for resource in getattr ( model , 'resources' , [ ] ) : insert resource file ( cursor , module ident , resource ) if isinstance ( model , Document ) : html = bytes ( cnxepub . Document Content Formatter ( model ) ) sha1 = hashlib . new ( 'sha1' , html ) . hexdigest ( ) cursor . execute ( \"SELECT fileid FROM files WHERE sha1 = %s\" , ( sha1 , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except Type Error : file args = { 'media type' : 'text/html' , 'data' : psycopg2 . Binary ( html ) , } cursor . execute ( , file args ) fileid = cursor . fetchone ( ) [ 0 ] args = { 'module ident' : module ident , 'filename' : 'index.cnxml.html' , 'fileid' : fileid , } cursor . execute ( , args ) elif isinstance ( model , Binder ) : tree = cnxepub . model to tree ( model ) tree = insert tree ( cursor , tree ) return ident hash", "predictions": ["delete acl from a acl acl response response response response response response response response response response response response response response response"], "references": ["publishes the model and return its ident_hash ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 3377, "code": "def publish composite model ( cursor , model , parent model , publisher , message ) : if not ( isinstance ( model , Composite Document ) or ( isinstance ( model , Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) : raise Value Error ( \"This function only publishes Composite\" \"objects. '{}' was given.\" . format ( type ( model ) ) ) if issequence ( publisher ) and len ( publisher ) > 1 : raise Value Error ( \"Only one publisher is allowed. '{}' \" \"were given: {}\" . format ( len ( publisher ) , publisher ) ) module ident , ident hash = insert metadata ( cursor , model , publisher , message ) model . id , model . metadata [ 'version' ] = split ident hash ( ident hash ) model . set uri ( 'cnx-archive' , ident hash ) for resource in model . resources : insert resource file ( cursor , module ident , resource ) if isinstance ( model , Composite Document ) : html = bytes ( cnxepub . Document Content Formatter ( model ) ) fileid , = insert file ( cursor , io . Bytes IO ( html ) , 'text/html' ) file arg = { 'module ident' : module ident , 'parent ident hash' : parent model . ident hash , 'fileid' : fileid , } cursor . execute ( , file arg ) return ident hash", "predictions": ["lookup a api key for a given key ."], "references": ["publishes the model and return its ident_hash ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3378, "code": "def publish ( request ) : if 'epub' not in request . POST : raise httpexceptions . HTTP Bad Request ( \"Missing EPUB in POST body.\" ) is pre publication = asbool ( request . POST . get ( 'pre-publication' ) ) epub upload = request . POST [ 'epub' ] . file try : epub = cnxepub . EPUB . from file ( epub upload ) except : raise httpexceptions . HTTP Bad Request ( 'Format not recognized.' ) with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : epub upload . seek ( 0 ) publication id , publications = add publication ( cursor , epub , epub upload , is pre publication ) state , messages = poke publication state ( publication id ) response data = { 'publication' : publication id , 'mapping' : publications , 'state' : state , 'messages' : messages , } return response data", "predictions": ["publish publish publish = publication = . = . = . = ."], "references": ["accept a publication request at form value epub"], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 3379, "code": "def bake content ( request ) : ident hash = request . matchdict [ 'ident hash' ] try : id , version = split ident hash ( ident hash ) except Ident Hash Error : raise httpexceptions . HTTP Not Found ( ) if not version : raise httpexceptions . HTTP Bad Request ( 'must specify the version' ) with db connect ( ) as db conn : with db conn . cursor ( ) as cursor : cursor . execute ( , ( ident hash , ) ) try : is binder , stateid , module ident = cursor . fetchone ( ) except Type Error : raise httpexceptions . HTTP Not Found ( ) if not is binder : raise httpexceptions . HTTP Bad Request ( '{} is not a book' . format ( ident hash ) ) if stateid == 5 : cursor . execute ( , ( module ident , ident hash ) ) else : cursor . execute ( , ( ident hash , ) )", "predictions": ["expandvars dict of dict dict os os dict os os os os os os os os os os os os os os os os os os os os os os os"], "references": ["invoke the baking process - trigger post - publication"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3380, "code": "def includeme ( config ) : global cache manager settings = config . registry . settings cache manager = Cache Manager ( * * parse cache config options ( settings ) )", "predictions": ["update the wrapper configuration"], "references": ["configures the caching manager"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 3381, "code": "def path to node ( tree , path ) : if path is None : return None node = tree for key in path : node = child by key ( node , key ) return node", "predictions": ["transform a post - style post - style post - processing post - processing"], "references": ["fst node located at the given path"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 3382, "code": "def cli ( url , user agent ) : kwargs = { } if user agent : kwargs [ 'user agent' ] = user agent archive url = capture ( url , * * kwargs ) click . echo ( archive url )", "predictions": ["manage a user agent return a user"], "references": ["archives the provided url using archive . is ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 3383, "code": "def get channel image ( self , channel , img size = 300 , skip cache = False ) : from bs4 import Beautiful Soup from wikipedia . exceptions import Page Error import re import wikipedia wikipedia . set lang ( 'fr' ) if not channel : LOGGER . error ( 'Channel is not set. Could not retrieve image.' ) return if channel in self . cache channel img and not skip cache : img = self . cache channel img [ channel ] LOGGER . debug ( 'Cache hit: %s -> %s' , channel , img ) return img channel info = self . get channel info ( channel ) query = channel info [ 'wiki page' ] if not query : LOGGER . debug ( 'Wiki page is not set for channel %s' , channel ) return LOGGER . debug ( 'Query: %s' , query ) if 'max img size' in channel info : if img size > channel info [ 'max img size' ] : LOGGER . info ( 'Requested image size is bigger than the max, ' 'setting it to %s' , channel info [ 'max img size' ] ) img size = channel info [ 'max img size' ] try : page = wikipedia . page ( query ) LOGGER . debug ( 'Wikipedia article title: %s' , page . title ) soup = Beautiful Soup ( page . html ( ) , 'html.parser' ) images = soup . find all ( 'img' ) img src = None for i in images : if i [ 'alt' ] . startswith ( 'Image illustrative' ) : img src = re . sub ( r'\\d+px' , '{}px' . format ( img size ) , i [ 'src' ] ) img = 'https:{}' . format ( img src ) if img src else None self . cache channel img [ channel ] = img return img except Page Error : LOGGER . error ( 'Could not fetch channel image for %s' , channel )", "predictions": ["declare a browsable routes from the browsable api ."], "references": ["get the logo for a channel"], "bleu": 0.15619699684601276, "rouge_l": 0.13832199546485258}
{"id": 3384, "code": "def forwards ( self , orm ) : for translation in orm [ 'people.Person Translation' ] . objects . all ( ) : if translation . language in [ 'en' , 'de' ] : translation . roman first name = translation . first name translation . roman last name = translation . last name else : translation . non roman first name = translation . first name translation . non roman last name = translation . last name translation . save ( )", "predictions": ["write your forwards methods here ."], "references": ["write your forwards methods here ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3385, "code": "def forwards ( self , orm ) : for translation in orm [ 'people.Person Translation' ] . objects . all ( ) : translation . person . roman first name = translation . roman first name translation . person . roman last name = translation . roman last name translation . person . non roman first name = translation . non roman first name translation . person . non roman last name = translation . non roman last name translation . person . save ( )", "predictions": ["write your formatter methods here ."], "references": ["write your forwards methods here ."], "bleu": 0.488923022434901, "rouge_l": 0.8333333333333334}
{"id": 3386, "code": "def smixins ( self , name ) : return ( self . mixins [ name ] if name in self . mixins else False )", "predictions": ["returns true if the sensor is not already defined"], "references": ["inner wrapper to search for mixins by name ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3387, "code": "def blocks ( self , name ) : i = len ( self ) while i >= 0 : i -= 1 if name in self [ i ] [ ' names ' ] : for b in self [ i ] [ ' blocks ' ] : r = b . raw ( ) if r and r == name : return b else : for b in self [ i ] [ ' blocks ' ] : r = b . raw ( ) if r and name . startswith ( r ) : b = utility . blocksearch ( b , name ) if b : return b return False", "predictions": ["dict containing upsert upsert"], "references": ["inner wrapper to search for blocks by name ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 3388, "code": "def user and project from git ( self , options , arg0 = None , arg1 = None ) : user , project = self . user project from option ( options , arg0 , arg1 ) if user and project : return user , project try : remote = subprocess . check output ( [ 'git' , 'config' , '--get' , 'remote.{0}.url' . format ( options . git remote ) ] ) except subprocess . Called Process Error : return None , None except Windows Error : print ( \"git binary not found.\" ) exit ( 1 ) else : return self . user project from remote ( remote )", "predictions": ["get upsert pending binary pending to role . ."], "references": ["detects user and project from git ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3389, "code": "def fetch tags dates ( self ) : if self . options . verbose : print ( \"Fetching dates for {} tags...\" . format ( len ( self . filtered tags ) ) ) def worker ( tag ) : self . get time of tag ( tag ) threads = [ ] max threads = 50 cnt = len ( self . filtered tags ) for i in range ( 0 , ( cnt // max threads ) + 1 ) : for j in range ( max threads ) : idx = i * 50 + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( self . filtered tags [ idx ] , ) ) threads . append ( t ) t . start ( ) if self . options . verbose > 2 : print ( \".\" , end = \"\" ) for t in threads : t . join ( ) if self . options . verbose > 2 : print ( \".\" ) if self . options . verbose > 1 : print ( \"Fetched dates for {} tags.\" . format ( len ( self . tag times dict ) ) )", "predictions": ["start for licenses as a set of return messages"], "references": ["async fetching of all tags dates ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3390, "code": "def fetch and filter tags ( self ) : self . all tags = self . fetcher . get all tags ( ) self . filtered tags = self . get filtered tags ( self . all tags ) self . fetch tags dates ( )", "predictions": ["validate and filter licenses obtain licenses"], "references": ["fetch and filter tags fetch dates and sort them in time order ."], "bleu": 0.09052970298747198, "rouge_l": 0.19741100323624597}
{"id": 3391, "code": "def to decimal ( text ) : if not isinstance ( text , string type ) : raise Type Error ( \"expected str or unicode, %s given\" % type ( text ) ) if findall ( r\"[\\x00-\\x20\\x7c-\\xff]\" , text ) : raise Value Error ( \"invalid character in sequence\" ) text = text . lstrip ( '!' ) decimal = 0 length = len ( text ) - 1 for i , char in enumerate ( text ) : decimal += ( ord ( char ) - 33 ) * ( 91 ** ( length - i ) ) return decimal if text != '' else 0", "predictions": ["convert a cursor to a model model ."], "references": ["takes a base91 char string and returns decimal"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3392, "code": "def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff", "predictions": ["get the code from a callsign"], "references": ["takes a callsign and returns passcode"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 3393, "code": "def set filter ( self , filter text ) : self . filter = filter text self . logger . info ( \"Setting filter to: %s\" , self . filter ) if self . connected : self . sendall ( \"#filter %s\\r\\n\" % self . filter )", "predictions": ["set the filter text"], "references": ["set a specified aprs - is filter for this connection"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 3394, "code": "def set login ( self , callsign , passwd = \"-1\" , skip login = False ) : self . dict . update ( locals ( ) )", "predictions": ["login to a callsign ."], "references": ["set callsign and password"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3395, "code": "def connect ( self ) : self . logger . info ( \"Attempting connection to %s:%s\" , self . server [ 0 ] , self . server [ 1 ] ) try : self . open socket ( ) peer = self . sock . getpeername ( ) self . logger . info ( \"Connected to %s\" , str ( peer ) ) self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . sock . setsockopt ( socket . SOL SOCKET , socket . SO KEEPALIVE , 1 ) banner = self . sock . recv ( 512 ) if is py3 : banner = banner . decode ( 'latin-1' ) if banner [ 0 ] == \"#\" : self . logger . debug ( \"Banner: %s\" , banner . rstrip ( ) ) else : raise Connection Error ( \"invalid banner from server\" ) except Connection Error as e : self . logger . error ( str ( e ) ) self . close ( ) raise except ( socket . error , socket . timeout ) as e : self . close ( ) self . logger . error ( \"Socket error: %s\" % str ( e ) ) if str ( e ) == \"timed out\" : raise Connection Error ( \"no banner from server\" ) else : raise Connection Error ( e ) self . connected = True", "predictions": ["connect to socket ."], "references": ["attemps connection to the server"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3396, "code": "def send login ( self ) : login str = \"user {0} pass {1} vers aprslib {3}{2}\\r\\n\" login str = login str . format ( self . callsign , self . passwd , ( \" filter \" + self . filter ) if self . filter != \"\" else \"\" , version ) self . logger . info ( \"Sending login information\" ) try : self . sendall ( login str ) self . sock . settimeout ( 5 ) test = self . sock . recv ( len ( login str ) + 100 ) if is py3 : test = test . decode ( 'latin-1' ) test = test . rstrip ( ) self . logger . debug ( \"Server: %s\" , test ) , , callsign , status , = test . split ( ' ' , 4 ) if callsign == \"\" : raise Login Error ( \"Server responded with empty callsign???\" ) if callsign != self . callsign : raise Login Error ( \"Server: %s\" % test ) if status != \"verified,\" and self . passwd != \"-1\" : raise Login Error ( \"Password is incorrect\" ) if self . passwd == \"-1\" : self . logger . info ( \"Login successful (receive only)\" ) else : self . logger . info ( \"Login successful\" ) except Login Error as e : self . logger . error ( str ( e ) ) self . close ( ) raise except : self . close ( ) self . logger . error ( \"Failed to login\" ) raise Login Error ( \"Failed to login\" )", "predictions": ["send login login ."], "references": ["sends login string to server"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3397, "code": "def socket readlines ( self , blocking = False ) : try : self . sock . setblocking ( 0 ) except socket . error as e : self . logger . error ( \"socket error when setblocking(0): %s\" % str ( e ) ) raise Connection Drop ( \"connection dropped\" ) while True : short buf = b'' newline = b'\\r\\n' select . select ( [ self . sock ] , [ ] , [ ] , None if blocking else 0 ) try : short buf = self . sock . recv ( 4096 ) if not short buf : self . logger . error ( \"socket.recv(): returned empty\" ) raise Connection Drop ( \"connection dropped\" ) except socket . error as e : self . logger . error ( \"socket error on recv(): %s\" % str ( e ) ) if \"Resource temporarily unavailable\" in str ( e ) : if not blocking : if len ( self . buf ) == 0 : break self . buf += short buf while newline in self . buf : line , self . buf = self . buf . split ( newline , 1 ) yield line", "predictions": ["gets a socket socket ."], "references": ["generator for complete lines received from the server"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3398, "code": "def db value ( self , value ) : if not isinstance ( value , UUID ) : value = UUID ( value ) parts = str ( value ) . split ( \"-\" ) reordered = '' . join ( [ parts [ 2 ] , parts [ 1 ] , parts [ 0 ] , parts [ 3 ] , parts [ 4 ] ] ) value = binascii . unhexlify ( reordered ) return super ( Ordered UUID Field , self ) . db value ( value )", "predictions": ["convert value to dictionary ."], "references": ["convert uuid to binary blob"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 3399, "code": "def python value ( self , value ) : value = super ( Ordered UUID Field , self ) . python value ( value ) u = binascii . b2a hex ( value ) value = u [ 8 : 16 ] + u [ 4 : 8 ] + u [ 0 : 4 ] + u [ 16 : 22 ] + u [ 22 : 32 ] return UUID ( value . decode ( ) )", "predictions": ["turn value into a format ."], "references": ["convert binary blob to uuid instance"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3400, "code": "def db value ( self , value ) : value = self . transform value ( value ) return self . hhash . encrypt ( value , salt size = self . salt size , rounds = self . rounds )", "predictions": ["this function compares the db value"], "references": ["convert the python value for storage in the database ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3401, "code": "def python value ( self , value ) : value = coerce to bytes ( value ) obj = Hash Value ( value ) obj . field = self return obj", "predictions": ["converts the given value to a date ."], "references": ["convert the database value to a pythonic value ."], "bleu": 0.29150322793751426, "rouge_l": 0.5820610687022901}
{"id": 3402, "code": "def disconnect ( self ) : for name , connection in self . items ( ) : if not connection . is closed ( ) : connection . close ( )", "predictions": ["disconnect from all connections that are connected to this server ."], "references": ["disconnect from all databases"], "bleu": 0.21200626759025185, "rouge_l": 0.43675417661097854}
{"id": 3403, "code": "def get database ( self , model ) : for router in self . routers : r = router . get database ( model ) if r is not None : return r return self . get ( 'default' )", "predictions": ["get a database from db"], "references": ["find matching database router"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3404, "code": "def to cursor ref ( self ) : fields = self . meta . get primary keys ( ) assert fields values = { field . name : self . data [ field . name ] for field in fields } return values", "predictions": ["return a cursor for the cursor ."], "references": ["returns dict of values to uniquely reference this item"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 3405, "code": "def apply filters ( self , query , filters ) : assert isinstance ( query , peewee . Query ) assert isinstance ( filters , dict )", "predictions": ["sets filters filters to the given query ."], "references": ["apply user specified filters to query"], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 3406, "code": "def list ( self , filters , cursor , count ) : assert isinstance ( filters , dict ) , \"expected filters type 'dict'\" assert isinstance ( cursor , dict ) , \"expected cursor type 'dict'\" query = self . get query ( ) assert isinstance ( query , peewee . Query ) #filters = {field.name: cursor[field.name] for field in fields} #query.where( paginator = self . get paginator ( ) assert isinstance ( paginator , Pagination ) count += 1 pquery = paginator . filter query ( query , cursor , count ) items = [ item for item in pquery ] next item = items . pop ( 1 ) next cursor = next item . to cursor ref ( ) return items , next cursor", "predictions": ["list items in a dict ."], "references": ["list items from query"], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 3407, "code": "def retrieve ( self , cursor ) : assert isinstance ( cursor , dict ) , \"expected cursor type 'dict'\" query = self . get query ( ) assert isinstance ( query , peewee . Query ) query return query . get ( * * cursor )", "predictions": ["gets the first query ."], "references": ["retrieve items from query"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3408, "code": "def refresh ( self , accept = MEDIA TYPE TAXII V20 ) : self . refresh information ( accept ) self . refresh collections ( accept )", "predictions": ["refresh the widgets ."], "references": ["update the api root s information and list of collections"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 3409, "code": "def validate server ( self ) : if not self . title : msg = \"No 'title' in Server Discovery for request '{}'\" raise Validation Error ( msg . format ( self . url ) )", "predictions": ["validate the server s server ."], "references": ["validates server information . raises errors for required properties ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3410, "code": "def refresh ( self ) : response = self . raw = self . conn . get ( self . url ) self . populate fields ( * * response ) self . loaded = True", "predictions": ["refresh the data from the api ."], "references": ["update the server information and list of api roots"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3411, "code": "def empty like ( array , dtype = None ) : array = numpy . asarray ( array ) if dtype is None : dtype = array . dtype return anonymousmemmap ( array . shape , dtype )", "predictions": ["empty array like shape but only if it is empty"], "references": ["create a shared memory array from the shape of array ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 3412, "code": "def full like ( array , value , dtype = None ) : shared = empty like ( array , dtype ) shared [ : ] = value return shared", "predictions": ["estimates a full array into a shared array"], "references": ["create a shared memory array with the same shape and type as a given array filled with value ."], "bleu": 0.06345925250584064, "rouge_l": 0.2760180995475113}
{"id": 3413, "code": "def full ( shape , value , dtype = 'f8' ) : shared = empty ( shape , dtype ) shared [ : ] = value return shared", "predictions": ["estimates a full shared array into a shared shared array ."], "references": ["create a shared memory array of given shape and type filled with value ."], "bleu": 0.1296737111713457, "rouge_l": 0.31322207958921694}
{"id": 3414, "code": "def flatten dtype ( dtype , next = None ) : types = [ ] if next is None : next = [ 0 , '' ] primary = True else : primary = False prefix = next [ 1 ] if dtype . names is None : for i in numpy . ndindex ( dtype . shape ) : if dtype . base == dtype : types . append ( ( '%s%s' % ( prefix , simplerepr ( i ) ) , dtype ) ) next [ 0 ] += 1 else : next [ 1 ] = '%s%s' % ( prefix , simplerepr ( i ) ) types . extend ( flatten dtype ( dtype . base , next ) ) else : for field in dtype . names : typ fields = dtype . fields [ field ] if len ( prefix ) > 0 : next [ 1 ] = prefix + '.' + field else : next [ 1 ] = '' + field flat dt = flatten dtype ( typ fields [ 0 ] , next ) types . extend ( flat dt ) next [ 1 ] = prefix if primary : return numpy . dtype ( types ) else : return types", "predictions": ["flatten a dtype dtype into a data structure"], "references": ["unpack a structured data - type ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3415, "code": "def Meta Ordered ( parallel , done , turnstile ) : class Ordered : def init ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def enter ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def exit ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered", "predictions": ["run a function on a specific master ."], "references": ["meta class for ordered construct ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3416, "code": "def kill all ( self ) : for pid in self . children : try : os . kill ( pid , signal . SIGTRAP ) except OS Error : continue self . join ( )", "predictions": ["kill all processes ."], "references": ["kill all slaves and reap the monitor"], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 3417, "code": "def abort ( self ) : self . mutex . release ( ) self . turnstile . release ( ) self . mutex . release ( ) self . turnstile2 . release ( )", "predictions": ["abort the previously saved session ."], "references": ["ensure the master exit from barrier"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3418, "code": "def read ( self , n ) : while len ( self . pool ) < n : self . cur = self . files . next ( ) self . pool = numpy . append ( self . pool , self . fetch ( self . cur ) , axis = 0 ) rt = self . pool [ : n ] if n == len ( self . pool ) : self . pool = self . fetch ( None ) else : self . pool = self . pool [ n : ] return rt", "predictions": ["read n n n n n chars from the internal pool"], "references": ["return at most n array items move the cursor ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 3419, "code": "def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise Type Error ( 'template must be a packarray' ) return cls ( source , template . start , template . end )", "predictions": ["adapt the given source to a given template ."], "references": ["adapt source to a packarray according to the layout of template"], "bleu": 0.24193282889161974, "rouge_l": 0.4911433172302737}
{"id": 3420, "code": "def year ( past = False , min delta = 0 , max delta = 20 ) : return dt . date . today ( ) . year + delta ( past , min delta , max delta )", "predictions": ["return the current year in the past"], "references": ["return a random year ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3421, "code": "def date ( past = False , min delta = 0 , max delta = 20 ) : timedelta = dt . timedelta ( days = delta ( past , min delta , max delta ) ) return dt . date . today ( ) + timedelta", "predictions": ["return a timedelta object for a given past ."], "references": ["return a random dt . date object . delta args are days ."], "bleu": 0.135323305042906, "rouge_l": 0.35209235209235207}
{"id": 3422, "code": "def street number ( ) : length = int ( random . choice ( string . digits [ 1 : 6 ] ) ) return '' . join ( random . sample ( string . digits , length ) )", "predictions": ["generates a random street number"], "references": ["return a random street number ."], "bleu": 0.6221008431290531, "rouge_l": 0.7155425219941348}
{"id": 3423, "code": "def zip code ( ) : format = '#####' if random . random ( ) >= 0.5 : format = '#####-####' result = '' for item in format : if item == '#' : result += str ( random . randint ( 0 , 9 ) ) else : result += item return result", "predictions": ["generate a random code object ."], "references": ["return a random zip code either in ##### or ##### - #### format ."], "bleu": 0.08707046609544257, "rouge_l": 0.3730886850152905}
{"id": 3424, "code": "def job title ( ) : result = random . choice ( get dictionary ( 'job titles' ) ) . strip ( ) result = result . replace ( '#{N}' , job title suffix ( ) ) return result", "predictions": ["returns a random title title"], "references": ["return a random job title ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 3425, "code": "def body ( quantity = 2 , separator = '\\n\\n' , wrap start = '' , wrap end = '' , html = False , sentences quantity = 3 , as list = False ) : return lorem ipsum . paragraphs ( quantity = quantity , separator = separator , wrap start = wrap start , wrap end = wrap end , html = html , sentences quantity = sentences quantity , as list = as list )", "predictions": ["% an set of set as a set of set paragraphs . . . ."], "references": ["return a random email text ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 3426, "code": "def money ( min = 0 , max = 10 ) : value = random . choice ( range ( min * 100 , max * 100 ) ) return \"%1.2f\" % ( float ( value ) / 100 )", "predictions": ["return a random with a random"], "references": ["return a str of decimal with two digits after a decimal mark ."], "bleu": 0.10286160177491631, "rouge_l": 0.39482200647249194}
{"id": 3427, "code": "def words ( quantity = 10 , as list = False ) : global words if not words : words = ' ' . join ( get dictionary ( 'lorem ipsum' ) ) . lower ( ) . replace ( '\\n' , '' ) words = re . sub ( r'\\.|,|;/' , '' , words ) words = words . split ( ' ' ) result = random . sample ( words , quantity ) if as list : return result else : return ' ' . join ( result )", "predictions": ["return connect to a given word"], "references": ["return random words ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 3428, "code": "def title ( words quantity = 4 ) : result = words ( quantity = words quantity ) result += random . choice ( '?.!' ) return result . capitalize ( )", "predictions": ["get a pass send to the given words"], "references": ["return a random sentence to be used as e . g . an e - mail subject ."], "bleu": 0.050847167445562676, "rouge_l": 0.14386792452830188}
{"id": 3429, "code": "def sentences ( quantity = 2 , as list = False ) : result = [ sntc . strip ( ) for sntc in random . sample ( get dictionary ( 'lorem ipsum' ) , quantity ) ] if as list : return result else : return ' ' . join ( result )", "predictions": ["raise a e - mail when using a e - mail 0 0 0 0 0 0 0 0 0 0 0 0 0 0 - 1 0 0 0 -"], "references": ["return random sentences ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3430, "code": "def paragraph ( separator = '\\n\\n' , wrap start = '' , wrap end = '' , html = False , sentences quantity = 3 ) : return paragraphs ( quantity = 1 , separator = separator , wrap start = wrap start , wrap end = wrap end , html = html , sentences quantity = sentences quantity )", "predictions": ["return - wise db - wise db - paragraphs"], "references": ["return a random paragraph ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 3431, "code": "def paragraphs ( quantity = 2 , separator = '\\n\\n' , wrap start = '' , wrap end = '' , html = False , sentences quantity = 3 , as list = False ) : if html : wrap start = '<p>' wrap end = '</p>' separator = '\\n\\n' result = [ ] try : for in xrange ( 0 , quantity ) : result . append ( wrap start + sentences ( sentences quantity ) + wrap end ) except Name Error : for in range ( 0 , quantity ) : result . append ( wrap start + sentences ( sentences quantity ) + wrap end ) if as list : return result else : return separator . join ( result )", "predictions": ["super python python python python python python python python version of binascii ."], "references": ["return random paragraphs ."], "bleu": 0.09552040806823771, "rouge_l": 0.1300639658848614}
{"id": 3432, "code": "def characters ( quantity = 10 ) : line = map ( to lower alpha only , '' . join ( random . sample ( get dictionary ( 'lorem ipsum' ) , quantity ) ) ) return '' . join ( line ) [ : quantity ]", "predictions": ["generate a random for the given quantity"], "references": ["return random characters ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3433, "code": "def text ( what = \"sentence\" , * args , * * kwargs ) : if what == \"character\" : return character ( * args , * * kwargs ) elif what == \"characters\" : return characters ( * args , * * kwargs ) elif what == \"word\" : return word ( * args , * * kwargs ) elif what == \"words\" : return words ( * args , * * kwargs ) elif what == \"sentence\" : return sentence ( * args , * * kwargs ) elif what == \"sentences\" : return sentences ( * args , * * kwargs ) elif what == \"paragraph\" : return paragraph ( * args , * * kwargs ) elif what == \"paragraphs\" : return paragraphs ( * args , * * kwargs ) elif what == \"title\" : return title ( * args , * * kwargs ) else : raise Name Error ( 'No such method' )", "predictions": ["get the python title"], "references": ["an aggregator for all above defined public methods ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 3434, "code": "def account number ( ) : account = [ random . randint ( 1 , 9 ) for in range ( 20 ) ] return \"\" . join ( map ( str , account ) )", "predictions": ["generates a random number number"], "references": ["return a random bank account number ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 3435, "code": "def bik ( ) : return '04' + '' . join ( [ str ( random . randint ( 1 , 9 ) ) for in range ( 5 ) ] ) + str ( random . randint ( 0 , 49 ) + 50 )", "predictions": ["generate a random string of given length for a given routers for a given list of temperature numbers for use as a given routers for a routers for a routers for"], "references": ["return a random bank identification number ."], "bleu": 0.0513487742994337, "rouge_l": 0.11879259980525803}
{"id": 3436, "code": "def legal inn ( ) : mask = [ 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for in range ( 10 ) ] weighted = [ v * mask [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 9 ] = sum ( weighted ) % 11 % 10 return \"\" . join ( map ( str , inn ) )", "predictions": ["return a to generate to to to to to to to to to to to to return"], "references": ["return a random taxation id number for a company ."], "bleu": 0.09507244120026236, "rouge_l": 0.15541401273885352}
{"id": 3437, "code": "def legal ogrn ( ) : ogrn = \"\" . join ( map ( str , [ random . randint ( 1 , 9 ) for in range ( 12 ) ] ) ) ogrn += str ( ( int ( ogrn ) % 11 % 10 ) ) return ogrn", "predictions": ["generate a apply filters"], "references": ["return a random government registration id for a company ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 3438, "code": "def person inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for in range ( 12 ) ] weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return \"\" . join ( map ( str , inn ) )", "predictions": ["return inn of list"], "references": ["return a random taxation id number for a natural person ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 3439, "code": "def password ( at least = 6 , at most = 12 , lowercase = True , uppercase = True , digits = True , spaces = False , punctuation = False ) : return text ( at least = at least , at most = at most , lowercase = lowercase , uppercase = uppercase , digits = digits , spaces = spaces , punctuation = punctuation )", "predictions": ["returns a retrieve retrieve retrieve retrieve the retrieve retrieve retrieve retrieve retrieve the retrieve retrieve retrieve ."], "references": ["return a random string for use as a password ."], "bleu": 0.07994607499472013, "rouge_l": 0.15541401273885352}
{"id": 3440, "code": "def forwards ( self , orm ) : print ( \"Updating: Jednostka Administracyjna\" ) ja akt stan = orm . Jednostka Administracyjna . objects . all ( ) . aggregate ( Max ( 'stan na' ) ) [ 'stan na max' ] orm . Jednostka Administracyjna . objects . filter ( stan na exact = ja akt stan ) . update ( aktywny = True ) orm . Jednostka Administracyjna . objects . exclude ( stan na exact = ja akt stan ) . update ( aktywny = False ) print ( \"Updating: Miejscowosc\" ) m akt stan = orm . Miejscowosc . objects . all ( ) . aggregate ( Max ( 'stan na' ) ) [ 'stan na max' ] orm . Miejscowosc . objects . filter ( stan na exact = m akt stan ) . update ( aktywny = True ) orm . Miejscowosc . objects . exclude ( stan na exact = m akt stan ) . update ( aktywny = False ) print ( \"Updating: Rodzaj Miejsowosci\" ) rm akt stan = orm . Rodzaj Miejsowosci . objects . all ( ) . aggregate ( Max ( 'stan na' ) ) [ 'stan na max' ] orm . Rodzaj Miejsowosci . objects . filter ( stan na exact = rm akt stan ) . update ( aktywny = True ) orm . Rodzaj Miejsowosci . objects . exclude ( stan na exact = rm akt stan ) . update ( aktywny = False ) print ( \"Updating: Ulica\" ) u akt stan = orm . Ulica . objects . all ( ) . aggregate ( Max ( 'stan na' ) ) [ 'stan na max' ] orm . Ulica . objects . filter ( stan na exact = u akt stan ) . update ( aktywny = True ) orm . Ulica . objects . exclude ( stan na exact = u akt stan ) . update ( aktywny = False )", "predictions": ["write your refresh methods here ."], "references": ["write your forwards methods here ."], "bleu": 0.488923022434901, "rouge_l": 0.8333333333333334}
{"id": 3441, "code": "def forwards ( self , orm ) : LEN TYPE = { 7 : 'GMI' , 4 : 'POW' , 2 : 'WOJ' , } for ja in orm . Jednostka Administracyjna . objects . all ( ) : ja . typ = LEN TYPE [ len ( ja . id ) ] ja . save ( )", "predictions": ["write your validate methods here ."], "references": ["write your forwards methods here ."], "bleu": 0.488923022434901, "rouge_l": 0.8333333333333334}
{"id": 3442, "code": "def extract diff sla from config file ( obj , options file ) : rule strings = { } config obj = Config Parser . Config Parser ( ) config obj . optionxform = str config obj . read ( options file ) for section in config obj . sections ( ) : rule strings , kwargs = get rule strings ( config obj , section ) for ( key , val ) in rule strings . iteritems ( ) : set sla ( obj , section , key , val )", "predictions": ["refresh the diff from from from from"], "references": ["helper function to parse diff config file which contains sla rules for diff comparisons"], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 3443, "code": "def set sla ( obj , metric , sub metric , rules ) : if not hasattr ( obj , 'sla map' ) : return False rules list = rules . split ( ) for rule in rules list : if '<' in rule : stat , threshold = rule . split ( '<' ) sla = SLA ( metric , sub metric , stat , threshold , 'lt' ) elif '>' in rule : stat , threshold = rule . split ( '>' ) sla = SLA ( metric , sub metric , stat , threshold , 'gt' ) else : if hasattr ( obj , 'logger' ) : obj . logger . error ( 'Unsupported SLA type defined : ' + rule ) sla = None obj . sla map [ metric ] [ sub metric ] [ stat ] = sla if hasattr ( obj , 'sla list' ) : obj . sla list . append ( sla ) return True", "predictions": ["empty the like like like the like object ."], "references": ["extract slas from a set of rules"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3444, "code": "def graph csv ( output directory , resource path , csv file , plot title , output filename , y label = None , precision = None , graph height = \"600\" , graph width = \"1500\" ) : if not os . path . getsize ( csv file ) : return False , \"\" y label = y label or plot title div id = str ( random . random ( ) ) div string = \"<div id=\\\"%s\\\" style=\\\"width:%spx; height:%spx;\\\"></div>\" % ( div id , graph width , graph height ) script string = + div id + + resource path + '/' + os . path . basename ( csv file ) + + y label + + plot title + + y label + with open ( os . path . join ( output directory , output filename + '.div' ) , 'w' ) as div file : div file . write ( div string + script string ) return True , os . path . join ( output directory , output filename + '.div' )", "predictions": ["generates a full full full full full full full full full script"], "references": ["single metric graphing function"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 3445, "code": "def convert to G ( self , word ) : value = 0.0 if word [ - 1 ] == 'G' or word [ - 1 ] == 'g' : value = float ( word [ : - 1 ] ) elif word [ - 1 ] == 'M' or word [ - 1 ] == 'm' : value = float ( word [ : - 1 ] ) / 1000.0 elif word [ - 1 ] == 'K' or word [ - 1 ] == 'k' : value = float ( word [ : - 1 ] ) / 1000.0 / 1000.0 else : value = float ( word ) / 1000.0 / 1000.0 / 1000.0 return str ( value )", "predictions": ["full format of = = = = 1"], "references": ["given a size such as 2333m return the converted value in g"], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 3446, "code": "def plot diff ( self , graphing library = 'matplotlib' ) : diff datasource = sorted ( set ( self . reports [ 0 ] . datasource ) & set ( self . reports [ 1 ] . datasource ) ) graphed = False for submetric in diff datasource : baseline csv = naarad . utils . get default csv ( self . reports [ 0 ] . local location , ( submetric + '.percentiles' ) ) current csv = naarad . utils . get default csv ( self . reports [ 1 ] . local location , ( submetric + '.percentiles' ) ) if ( not ( naarad . utils . is valid file ( baseline csv ) & naarad . utils . is valid file ( current csv ) ) ) : continue baseline plot = PD ( input csv = baseline csv , csv column = 1 , series name = submetric , y label = submetric , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' , plot label = 'baseline' , x label = 'Percentiles' ) current plot = PD ( input csv = current csv , csv column = 1 , series name = submetric , y label = submetric , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' , plot label = 'current' , x label = 'Percentiles' ) graphed , div file = Diff . graphing modules [ graphing library ] . graph data on the same graph ( [ baseline plot , current plot ] , os . path . join ( self . output directory , self . resource path ) , self . resource path , ( submetric + '.diff' ) ) if graphed : self . plot files . append ( div file ) return True", "predictions": ["flatten the in a dtype of in the in self"], "references": ["generate cdf diff plots of the submetrics"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 3447, "code": "def check sla ( self , sla , diff metric ) : try : if sla . display is '%' : diff val = float ( diff metric [ 'percent diff' ] ) else : diff val = float ( diff metric [ 'absolute diff' ] ) except Value Error : return False if not ( sla . check sla passed ( diff val ) ) : self . sla failures += 1 self . sla failure list . append ( Diff SLA Failure ( sla , diff metric ) ) return True", "predictions": ["check are . on sla"], "references": ["check whether the sla has passed or failed"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3448, "code": "def check important sub metrics ( self , sub metric ) : if not self . important sub metrics : return False if sub metric in self . important sub metrics : return True items = sub metric . split ( '.' ) if items [ - 1 ] in self . important sub metrics : return True return False", "predictions": ["kill self os os self os os os os"], "references": ["check whether the given sub metric is in important_sub_metrics list"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 3449, "code": "def plot cdf ( self , graphing library = 'matplotlib' ) : graphed = False for percentile csv in self . percentiles files : csv filename = os . path . basename ( percentile csv ) column = self . csv column map [ percentile csv . replace ( \".percentiles.\" , \".\" ) ] if not self . check important sub metrics ( column ) : continue column = naarad . utils . sanitize string ( column ) graph title = '.' . join ( csv filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub metric description and column in self . sub metric description . keys ( ) : graph title += ' (' + self . sub metric description [ column ] + ')' if self . sub metric unit and column in self . sub metric unit . keys ( ) : plot data = [ PD ( input csv = percentile csv , csv column = 1 , series name = graph title , x label = 'Percentiles' , y label = column + ' (' + self . sub metric unit [ column ] + ')' , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' ) ] else : plot data = [ PD ( input csv = percentile csv , csv column = 1 , series name = graph title , x label = 'Percentiles' , y label = column , precision = None , graph height = 600 , graph width = 1200 , graph type = 'line' ) ] graphed , div file = Metric . graphing modules [ graphing library ] . graph data on the same graph ( plot data , self . resource directory , self . resource path , graph title ) if graphed : self . plot files . append ( div file ) return True", "predictions": ["abort the cdf cdf"], "references": ["plot cdf for important sub - metrics"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 3450, "code": "def set scores ( self ) : anom scores = { } self . compute derivatives ( ) derivatives ema = utils . compute ema ( self . smoothing factor , self . derivatives ) for i , ( timestamp , value ) in enumerate ( self . time series items ) : anom scores [ timestamp ] = abs ( self . derivatives [ i ] - derivatives ema [ i ] ) stdev = numpy . std ( anom scores . values ( ) ) if stdev : for timestamp in anom scores . keys ( ) : anom scores [ timestamp ] /= stdev self . anom scores = Time Series ( self . denoise scores ( anom scores ) )", "predictions": ["read anomaly scores scores using a anomaly anomaly anomaly anomaly anomaly anomaly anomaly anomaly anomaly anomaly anomaly pool pool pool pool"], "references": ["compute anomaly scores for the time series ."], "bleu": 0.07645949399477267, "rouge_l": 0.15006150061500614}
{"id": 3451, "code": "def exit ( self ) : if self . engine : self . engine . repl . terminate ( ) self . engine = None", "predictions": ["not called by the remote session"], "references": ["quits this octave session and cleans up ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3452, "code": "def restart ( self ) : if self . engine : self . engine . repl . terminate ( ) executable = self . executable if executable : os . environ [ 'OCTAVE EXECUTABLE' ] = executable if 'OCTAVE EXECUTABLE' not in os . environ and 'OCTAVE' in os . environ : os . environ [ 'OCTAVE EXECUTABLE' ] = os . environ [ 'OCTAVE' ] self . engine = Octave Engine ( stdin handler = self . handle stdin , logger = self . logger ) self . engine . eval ( 'addpath(\"%s\");' % HERE . replace ( osp . sep , '/' ) )", "predictions": ["year the 20 process min and stop the environment min min min min min min"], "references": ["restart an octave session in a clean state"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3453, "code": "def feval ( self , func name , func args = ( ) , dname = '' , nout = 0 , timeout = None , stream handler = None , store as = '' , plot dir = None ) : engine = self . engine if engine is None : raise Oct2Py Error ( 'Session is closed' ) out file = osp . join ( self . temp dir , 'writer.mat' ) out file = out file . replace ( osp . sep , '/' ) in file = osp . join ( self . temp dir , 'reader.mat' ) in file = in file . replace ( osp . sep , '/' ) func args = list ( func args ) ref indices = [ ] for ( i , value ) in enumerate ( func args ) : if isinstance ( value , Octave Ptr ) : ref indices . append ( i + 1 ) func args [ i ] = value . address ref indices = np . array ( ref indices ) req = dict ( func name = func name , func args = tuple ( func args ) , dname = dname or '' , nout = nout , store as = store as or '' , ref indices = ref indices ) write file ( req , out file , oned as = self . oned as , convert to float = self . convert to float ) engine . stream handler = stream handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( ' pyeval(\"%s\", \"%s\");' % ( out file , in file ) , timeout = timeout ) except Keyboard Interrupt as e : stream handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream handler ( engine . repl . interrupt ( ) ) raise Oct2Py Error ( 'Timed out, interrupting' ) except EOF : stream handler ( engine . repl . child . before ) self . restart ( ) raise Oct2Py Error ( 'Session died, restarting' ) resp = read file ( in file , self ) if resp [ 'err' ] : msg = self . parse error ( resp [ 'err' ] ) raise Oct2Py Error ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string types ) and result [ 0 ] == ' no value ' ) : result = None if plot dir : self . engine . make figures ( plot dir ) return result", "predictions": ["execute a date against a single frame"], "references": ["run the given function with the given args ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 3454, "code": "def parse error ( self , err ) : self . logger . debug ( err ) stack = err . get ( 'stack' , [ ] ) if not err [ 'message' ] . startswith ( 'parse error:' ) : err [ 'message' ] = 'error: ' + err [ 'message' ] errmsg = 'Octave evaluation error:\\n%s' % err [ 'message' ] if not isinstance ( stack , Struct Array ) : return errmsg errmsg += '\\nerror: called from:' for item in stack [ : - 1 ] : errmsg += '\\n    %(name)s at line %(line)d' % item try : errmsg += ', column %(column)d' % item except Exception : pass return errmsg", "predictions": ["street an number of number of errors"], "references": ["create a traceback for an octave evaluation error ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3455, "code": "def isobject ( self , name , exist ) : if exist in [ 2 , 5 ] : return False cmd = 'isobject(%s)' % name resp = self . engine . eval ( cmd , silent = True ) . strip ( ) return resp == 'ans =  1'", "predictions": ["evaluates a string and determine if it is available"], "references": ["test whether the name is an object ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3456, "code": "def get function ptr ( self , name ) : func = make function ptr instance self . function ptrs . setdefault ( name , func ( self , name ) ) return self . function ptrs [ name ]", "predictions": ["return a function for a function name ."], "references": ["get or create a function pointer of the given name ."], "bleu": 0.18239668350432228, "rouge_l": 0.4093959731543625}
{"id": 3457, "code": "def get user class ( self , name ) : self . user classes . setdefault ( name , make user class ( self , name ) ) return self . user classes [ name ]", "predictions": ["returns a user class by name"], "references": ["get or create a user class of the given type ."], "bleu": 0.17867793336200125, "rouge_l": 0.33516483516483514}
{"id": 3458, "code": "def cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )", "predictions": ["cleanup the temporary temp temp temp file ."], "references": ["clean up resources used by the session ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 3459, "code": "def read file ( path , session = None ) : try : data = loadmat ( path , struct as record = True ) except Unicode Decode Error as e : raise Oct2Py Error ( str ( e ) ) out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = extract ( value , session ) return out", "predictions": ["read a text file into a dictionary"], "references": ["read the data from the given file path ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3460, "code": "def write file ( obj , path , oned as = 'row' , convert to float = True ) : data = encode ( obj , convert to float ) try : with WRITE LOCK : savemat ( path , data , appendmat = False , oned as = oned as , long field names = True ) except Key Error : raise Exception ( 'could not save mat file' )", "predictions": ["write a single file as a numpy array ."], "references": ["save a python object to an octave file on the given path ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 3461, "code": "def extract ( data , session = None ) : if isinstance ( data , list ) : return [ extract ( d , session ) for d in data ] if not isinstance ( data , np . ndarray ) : return data if isinstance ( data , Matlab Object ) : cls = session . get user class ( data . classname ) return cls . from value ( data ) if data . dtype . names : if data . size == 1 : return create struct ( data , session ) return Struct Array ( data , session ) if data . dtype . kind == 'O' : return Cell ( data , session ) if data . size == 1 : return data . item ( ) if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] return data", "predictions": ["extract data from a list of data ."], "references": ["convert the octave values to values suitable for python ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 3462, "code": "def create struct ( data , session ) : out = Struct ( ) for name in data . dtype . names : item = data [ name ] if isinstance ( item , np . ndarray ) and item . dtype . kind == 'O' : item = item . squeeze ( ) . tolist ( ) out [ name ] = extract ( item , session ) return out", "predictions": ["create a struct from a list of data"], "references": ["create a struct from session data ."], "bleu": 0.46713797772820004, "rouge_l": 0.6747787610619468}
{"id": 3463, "code": "def is simple numeric ( data ) : for item in data : if isinstance ( item , set ) : item = list ( item ) if isinstance ( item , list ) : if not is simple numeric ( item ) : return False elif not isinstance ( item , ( int , float , complex ) ) : return False return True", "predictions": ["check if a data set is a numeric numeric or not ."], "references": ["test if a list contains simple numeric data ."], "bleu": 0.16261701715194898, "rouge_l": 0.39102564102564097}
{"id": 3464, "code": "def setup log ( ) : try : handler = logging . Stream Handler ( stream = sys . stdout ) except Type Error : handler = logging . Stream Handler ( strm = sys . stdout ) log = get log ( ) log . add Handler ( handler ) log . set Level ( logging . INFO ) log . propagate = False", "predictions": ["setup logging with sys . stdout"], "references": ["configure root logger ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 3465, "code": "def make user class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = Doc Descriptor ( ref , name ) values = dict ( doc = doc , name = name , ref = ref , attrs = attrs , module = 'oct2py.dynamic' ) for method in methods : doc = Method Doc Descriptor ( ref , name , method ) cls name = '%s %s' % ( name , method ) method values = dict ( doc = doc ) method cls = type ( str ( cls name ) , ( Octave User Class Method , ) , method values ) values [ method ] = method cls ( ref , method , name ) for attr in attrs : values [ attr ] = Octave User Class Attr ( ref , attr , attr ) return type ( str ( name ) , ( Octave User Class , ) , values )", "predictions": ["create a class from a user s class ."], "references": ["make an octave class for a given class name"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 3466, "code": "def to value ( cls , instance ) : if not isinstance ( instance , Octave User Class ) or not instance . attrs : return dict ( ) dtype = [ ] values = [ ] for attr in instance . attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return Matlab Object ( struct , instance . name )", "predictions": ["convert a instance to a dictionary"], "references": ["convert to a value to send to octave ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 3467, "code": "def to pointer ( cls , instance ) : return Octave Ptr ( instance . ref , instance . name , instance . address )", "predictions": ["converts an instance of a instance to a pointer pointer ."], "references": ["get a pointer to the private object ."], "bleu": 0.17033186037639278, "rouge_l": 0.32504440497335696}
{"id": 3468, "code": "def document func view ( serializer class = None , response serializer class = None , filter backends = None , permission classes = None , authentication classes = None , doc format args = list ( ) , doc format kwargs = dict ( ) ) : def decorator ( func ) : if serializer class : func . cls . serializer class = func . view class . serializer class = serializer class if response serializer class : func . cls . response serializer class = func . view class . response serializer class = response serializer class if filter backends : func . cls . filter backends = func . view class . filter backends = filter backends if permission classes : func . cls . permission classes = func . view class . permission classes = permission classes if authentication classes : func . cls . authentication classes = func . view class . authentication classes = authentication classes if doc format args or doc format kwargs : func . cls . doc = func . view class . doc = getdoc ( func ) . format ( * doc format args , * * doc format kwargs ) return func return decorator", "predictions": ["decorator to view document func ."], "references": ["decorator to make functional view documentable via drf - autodocs"], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 3469, "code": "def format docstring ( * args , * * kwargs ) : def decorator ( func ) : func . doc = getdoc ( func ) . format ( * args , * * kwargs ) return func return decorator", "predictions": ["format the docstring of a docstring ."], "references": ["decorator for clean docstring formatting"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3470, "code": "def is rarfile ( filename ) : mode = constants . RAR OM LIST INCSPLIT archive = unrarlib . RAR Open Archive Data Ex ( filename , mode = mode ) try : handle = unrarlib . RAR Open Archive Ex ( ctypes . byref ( archive ) ) except unrarlib . Unrar Exception : return False unrarlib . RAR Close Archive ( handle ) return ( archive . Open Result == constants . SUCCESS )", "predictions": ["return true if filename is a rarfile archive ."], "references": ["return true if file is a valid rar file ."], "bleu": 0.29052561567528307, "rouge_l": 0.6256410256410255}
{"id": 3471, "code": "def read header ( self , handle ) : header data = unrarlib . RAR Header Data Ex ( ) try : res = unrarlib . RAR Read Header Ex ( handle , ctypes . byref ( header data ) ) rarinfo = Rar Info ( header = header data ) except unrarlib . Archive End : return None except unrarlib . Missing Password : raise Runtime Error ( \"Archive is encrypted, password required\" ) except unrarlib . Bad Password : raise Runtime Error ( \"Bad password for Archive\" ) except unrarlib . Unrar Exception as e : raise Bad Rar File ( str ( e ) ) return rarinfo", "predictions": ["read a password from the client"], "references": ["read current member header into a rarinfo object ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3472, "code": "def process current ( self , handle , op , dest path = None , dest name = None ) : unrarlib . RAR Process File W ( handle , op , dest path , dest name )", "predictions": ["process current current current current current current current file ."], "references": ["process current member with op operation ."], "bleu": 0.17827531042796255, "rouge_l": 0.36454183266932266}
{"id": 3473, "code": "def load metadata ( self , handle ) : rarinfo = self . read header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . Name To Info [ rarinfo . filename ] = rarinfo self . process current ( handle , constants . RAR SKIP ) rarinfo = self . read header ( handle )", "predictions": ["load the metadata into the database"], "references": ["load archive members metadata ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 3474, "code": "def open ( self , archive ) : try : handle = unrarlib . RAR Open Archive Ex ( ctypes . byref ( archive ) ) except unrarlib . Unrar Exception : raise Bad Rar File ( \"Invalid RAR file.\" ) return handle", "predictions": ["open the ctypes archive ."], "references": ["open rar archive file ."], "bleu": 0.32466791547509893, "rouge_l": 0.6}
{"id": 3475, "code": "def namelist ( self ) : names = [ ] for member in self . filelist : names . append ( member . filename ) return names", "predictions": ["return a list of file names in this list ."], "references": ["return a list of file names in the archive ."], "bleu": 0.6989307622784944, "rouge_l": 0.8}
{"id": 3476, "code": "def getinfo ( self , name ) : rarinfo = self . Name To Info . get ( name ) if rarinfo is None : raise Key Error ( 'There is no item named %r in the archive' % name ) return rarinfo", "predictions": ["return an getinfo by name ."], "references": ["return the instance of rarinfo given name ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 3477, "code": "def printdir ( self ) : print ( \"%-46s %19s %12s\" % ( \"File Name\" , \"Modified    \" , \"Size\" ) ) for rarinfo in self . filelist : date = \"%d-%02d-%02d %02d:%02d:%02d\" % rarinfo . date time [ : 6 ] print ( \"%-46s %s %12d\" % ( rarinfo . filename , date , rarinfo . file size ) )", "predictions": ["prints a file with the current user s data"], "references": ["print a table of contents for the rar file ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 3478, "code": "def dostime to timetuple ( dostime ) : dostime = dostime >> 16 dostime = dostime & 0xffff day = dostime & 0x1f month = ( dostime >> 5 ) & 0xf year = 1980 + ( dostime >> 9 ) second = 2 * ( dostime & 0x1f ) minute = ( dostime >> 5 ) & 0x3f hour = dostime >> 11 return ( year , month , day , hour , minute , second )", "predictions": ["convert a dostime count to a timetuple date ."], "references": ["convert a rar archive member dos time to a python time tuple ."], "bleu": 0.15674392671857865, "rouge_l": 0.4401154401154401}
{"id": 3479, "code": "def c func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func", "predictions": ["wrapper to wrap argtypes function with custom argtypes function ."], "references": ["wrap c function setting prototype ."], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 3480, "code": "def load savefile header ( file h ) : try : raw savefile header = file h . read ( 24 ) except Unicode Decode Error : print ( \"\\n Make sure the input file is opened in read binary, 'rb'\\n\" ) raise Invalid Encoding ( \"Could not read file; it might not be opened in binary mode.\" ) if raw savefile header [ : 4 ] in [ struct . pack ( \">I\" , MAGIC NUMBER ) , struct . pack ( \">I\" , MAGIC NUMBER NS ) ] : byte order = b'big' unpacked = struct . unpack ( '>Ihh IIII' , raw savefile header ) elif raw savefile header [ : 4 ] in [ struct . pack ( \"<I\" , MAGIC NUMBER ) , struct . pack ( \"<I\" , MAGIC NUMBER NS ) ] : byte order = b'little' unpacked = struct . unpack ( '<Ihh IIII' , raw savefile header ) else : raise Unknown Magic Number ( \"No supported Magic Number found\" ) ( magic , major , minor , tz off , ts acc , snaplen , ll type ) = unpacked header = pcap header ( magic , major , minor , tz off , ts acc , snaplen , ll type , ctypes . c char p ( byte order ) , magic == MAGIC NUMBER NS ) if not validate header ( header ) : raise Invalid Header ( \"Invalid Header\" ) else : return header", "predictions": ["load the savefile header from the input file"], "references": ["load and validate the header of a pcap file ."], "bleu": 0.157044754112095, "rouge_l": 0.43571428571428567}
{"id": 3481, "code": "def strip ip ( packet ) : if not isinstance ( packet , IP ) : packet = IP ( packet ) payload = packet . payload return payload", "predictions": ["strip ip packet from input packet string ."], "references": ["remove the ip packet layer yielding the transport layer ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 3482, "code": "def strip ethernet ( packet ) : if not isinstance ( packet , Ethernet ) : packet = Ethernet ( packet ) payload = packet . payload return payload", "predictions": ["strip ethernet packet from the input packet ."], "references": ["strip the ethernet frame from a packet ."], "bleu": 0.26084743001221455, "rouge_l": 0.625}
{"id": 3483, "code": "def reload config ( self , call params ) : path = '/' + self . api version + '/Reload Config/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["reload the config ."], "references": ["rest reload plivo config helper"], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 3484, "code": "def reload cache config ( self , call params ) : path = '/' + self . api version + '/Reload Cache Config/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["reload a cache config ."], "references": ["rest reload plivo cache config helper"], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 3485, "code": "def transfer call ( self , call params ) : path = '/' + self . api version + '/Transfer Call/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["transfer a call to a specific call ."], "references": ["rest transfer live call helper"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 3486, "code": "def hangup all calls ( self ) : path = '/' + self . api version + '/Hangup All Calls/' method = 'POST' return self . request ( path , method )", "predictions": ["get a list of all calls ."], "references": ["rest hangup all live calls helper"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3487, "code": "def hangup call ( self , call params ) : path = '/' + self . api version + '/Hangup Call/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["call a remote api call ."], "references": ["rest hangup live call helper"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3488, "code": "def schedule hangup ( self , call params ) : path = '/' + self . api version + '/Schedule Hangup/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["get a list of all function function function function ."], "references": ["rest schedule hangup helper"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3489, "code": "def cancel scheduled hangup ( self , call params ) : path = '/' + self . api version + '/Cancel Scheduled Hangup/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["get a list of user class class . . . . . . . . . . . . . ."], "references": ["rest cancel a scheduled hangup helper"], "bleu": 0.05809665204409193, "rouge_l": 0.08232118758434548}
{"id": 3490, "code": "def conference mute ( self , call params ) : path = '/' + self . api version + '/Conference Mute/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["mute a cleanup request for a given cleanup . . . . . . . . . . . . . . . . . ."], "references": ["rest conference mute helper"], "bleu": 0.04668049023095243, "rouge_l": 0.07682619647355164}
{"id": 3491, "code": "def play ( self , call params ) : path = '/' + self . api version + '/Play/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["read a file from the server ."], "references": ["rest play something on a call helper"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3492, "code": "def play stop ( self , call params ) : path = '/' + self . api version + '/Play Stop/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["write a file to the server . . . . . . . . ."], "references": ["rest playstop on a call helper"], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 3493, "code": "def schedule play ( self , call params ) : path = '/' + self . api version + '/Schedule Play/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["extract a play from the server ."], "references": ["rest schedule playing something on a call helper"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3494, "code": "def cancel scheduled play ( self , call params ) : path = '/' + self . api version + '/Cancel Scheduled Play/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["create a list of struct ."], "references": ["rest cancel a scheduled play helper"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3495, "code": "def sound touch ( self , call params ) : path = '/' + self . api version + '/Sound Touch/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["simple simple page set up a is fetched set set set up a new one set set set set set set of is assumed set of is assumed set of is"], "references": ["rest add soundtouch audio effects to a call"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 3496, "code": "def sound touch stop ( self , call params ) : path = '/' + self . api version + '/Sound Touch Stop/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["stop a setup stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout"], "references": ["rest remove soundtouch audio effects on a call"], "bleu": 0.07223943354597204, "rouge_l": 0.08555399719495091}
{"id": 3497, "code": "def send digits ( self , call params ) : path = '/' + self . api version + '/Send Digits/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["make a user that can be executed with the server ."], "references": ["rest send digits to a call"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 3498, "code": "def conference unmute ( self , call params ) : path = '/' + self . api version + '/Conference Unmute/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["list all to the server s current user s to a to a to a to a to a to a to a to a to a to a to a"], "references": ["rest conference unmute helper"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3499, "code": "def conference kick ( self , call params ) : path = '/' + self . api version + '/Conference Kick/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["pointer a to a to a to a to a to a to pointer name name name name name name name"], "references": ["rest conference kick helper"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 3500, "code": "def conference hangup ( self , call params ) : path = '/' + self . api version + '/Conference Hangup/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["list all document func classes for a given class classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes to include classes classes to"], "references": ["rest conference hangup helper"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3501, "code": "def conference deaf ( self , call params ) : path = '/' + self . api version + '/Conference Deaf/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["list all format policy = true = false = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 1 = 0 = 0 ="], "references": ["rest conference deaf helper"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3502, "code": "def conference undeaf ( self , call params ) : path = '/' + self . api version + '/Conference Undeaf/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["list all is running on a is not a is a is a is not ."], "references": ["rest conference undeaf helper"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 3503, "code": "def conference record start ( self , call params ) : path = '/' + self . api version + '/Conference Record Start/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["a method to start a read header for a given read header . . . . . . . . . . . . . . . . . . ."], "references": ["rest conference recordstart helper"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3504, "code": "def conference play ( self , call params ) : path = '/' + self . api version + '/Conference Play/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["current user s current directory ."], "references": ["rest conference play helper"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3505, "code": "def conference speak ( self , call params ) : path = '/' + self . api version + '/Conference Speak/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["a requirements for a user - specified load balancer while while for a load balancer while that is available while for a load while we can access while for a load"], "references": ["rest conference speak helper"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3506, "code": "def conference list ( self , call params ) : path = '/' + self . api version + '/Conference List/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["fetches a list of all references for a given call . . . . . . . . . . . . . . . . . . . . ."], "references": ["rest conference list helper"], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 3507, "code": "def conference list members ( self , call params ) : path = '/' + self . api version + '/Conference List Members/' method = 'POST' return self . request ( path , method , call params )", "predictions": ["fetches a list of all self self append to the current namelist append to the given = true"], "references": ["rest conference list members helper"], "bleu": 0.06809398432036522, "rouge_l": 0.09682539682539681}
{"id": 3508, "code": "def xml ( self , root ) : element = root . create Element ( self . name ) keys = self . attrs . keys ( ) keys . sort ( ) for a in keys : element . set Attribute ( a , self . attrs [ a ] ) if self . body : text = root . create Text Node ( self . body ) element . append Child ( text ) for c in self . elements : element . append Child ( c . xml ( root ) ) return element", "predictions": ["build an getinfo tree from xml data ."], "references": ["return an xml element representing this element"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3509, "code": "def add node ( self , label ) : try : n = self . nodes [ label ] except Key Error : n = Node ( ) n [ 'label' ] = label self . nodes [ label ] = n return n", "predictions": ["add will add a node"], "references": ["return a node with label . create node if label is new"], "bleu": 0.08860330314183162, "rouge_l": 0.2190305206463196}
{"id": 3510, "code": "def add edge ( self , n1 label , n2 label , directed = False ) : n1 = self . add node ( n1 label ) n2 = self . add node ( n2 label ) e = Edge ( n1 , n2 , directed ) self . edges . append ( e ) return e", "predictions": ["add will add a new to the graph"], "references": ["get or create edges using get_or_create_node"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3511, "code": "def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None", "predictions": ["return a c { c }"], "references": ["return the other node"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 3512, "code": "def arg parser ( ) : description = \"Converts a completezip to a litezip\" parser = argparse . Argument Parser ( description = description ) verbose group = parser . add mutually exclusive group ( ) verbose group . add argument ( '-v' , '--verbose' , action = 'store true' , dest = 'verbose' , default = None , help = \"increase verbosity\" ) verbose group . add argument ( '-q' , '--quiet' , action = 'store false' , dest = 'verbose' , default = None , help = \"print nothing to stdout or stderr\" ) parser . add argument ( 'location' , help = \"Location of the unpacked litezip\" ) return parser", "predictions": ["return try to return"], "references": ["factory for creating the argument parser"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3513, "code": "def to bytes ( self , previous : bytes ) : if len ( self . conditions ) != len ( self . body ) : raise exc . Compile Error ( \"Conditions and body length mismatch!\" ) bc = b\"\" prev len = len ( previous ) for condition , body in zip ( self . conditions , self . body ) : cond bytecode = condition . to bytecode ( previous ) bc += cond bytecode body bc = compiler . compile bytecode ( body ) bdyl = len ( body bc ) gen len = prev len + len ( cond bytecode ) + bdyl + 1 bc += generate simple call ( tokens . POP JUMP IF FALSE , gen len ) bc += body bc prev len = len ( previous ) + len ( bc ) return bc", "predictions": ["convert the handler to ip"], "references": ["complex code ahead . comments have been added in as needed ."], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 3514, "code": "def to bytes 35 ( self , previous : bytes ) : bc = b\"\" it bc = util . generate bytecode from obb ( self . iterator , previous ) bc += it bc bc += util . generate bytecode from obb ( tokens . GET ITER , b\"\" ) prev len = len ( previous ) + len ( bc ) body bc = b\"\" for op in self . body : padded bc = previous padded bc += b\"\\x00\\x00\\x00\" padded bc += bc padded bc += b\"\\x00\\x00\\x00\" padded bc += body bc body bc += util . generate bytecode from obb ( op , padded bc ) body bc += util . generate simple call ( tokens . JUMP ABSOLUTE , prev len + 3 ) body bc += util . generate bytecode from obb ( tokens . POP BLOCK , b\"\" ) body bc = util . generate simple call ( tokens . FOR ITER , len ( body bc ) - 1 ) + body bc bc = util . generate simple call ( tokens . SETUP LOOP , prev len + len ( body bc ) - 6 ) + bc + body bc return bc", "predictions": ["convert strip strip strip to ethernet format return it ."], "references": ["a to - bytes specific to python 3 . 5 and below ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 3515, "code": "def to bytes 36 ( self , previous : bytes ) : bc = b\"\" it bc = util . generate bytecode from obb ( self . iterator , previous ) bc += it bc bc += util . ensure instruction ( tokens . GET ITER )", "predictions": ["store config from in config file api api api api api api api api api api api api api api api api api api api api api api api api api"], "references": ["a to - bytes specific to python 3 . 6 and above ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3516, "code": "def validate content ( * objs ) : from . main import Collection , Module validator = { Collection : cnxml . validate collxml , Module : cnxml . validate cnxml , } [ type ( objs [ 0 ] ) ] return validator ( * [ obj . file for obj in objs ] )", "predictions": ["reload cache file cache cache"], "references": ["runs the correct validator for given obj ects . assumes all same type"], "bleu": 0.04635036983311895, "rouge_l": 0.0}
{"id": 3517, "code": "def intersection ( l1 , l2 ) : if len ( l1 ) == 0 or len ( l2 ) == 0 : return [ ] out = [ ] l2 pos = 0 for l in l1 : while l2 pos < len ( l2 ) and l2 [ l2 pos ] . end < l . start : l2 pos += 1 if l2 pos == len ( l2 ) : break while l2 pos < len ( l2 ) and l . intersects ( l2 [ l2 pos ] ) : out . append ( l . intersection ( l2 [ l2 pos ] ) ) l2 pos += 1 l2 pos = max ( 0 , l2 pos - 1 ) return out", "predictions": ["generates a transfer from a list of l2 ."], "references": ["returns intersection of two lists . assumes the lists are sorted by start positions"], "bleu": 0.08961856124931385, "rouge_l": 0.1673525377229081}
{"id": 3518, "code": "def distance to point ( self , p ) : if self . start <= p <= self . end : return 0 else : return min ( abs ( self . start - p ) , abs ( self . end - p ) )", "predictions": ["returns the hangup hangup hangup hangup within the given point + version + version ."], "references": ["returns the distance from the point to the interval . zero if the point lies inside the interval ."], "bleu": 0.09860814573660683, "rouge_l": 0.28800755429650615}
{"id": 3519, "code": "def intersects ( self , i ) : return self . start <= i . end and i . start <= self . end", "predictions": ["calculates whether the segment hangup with a = value path path path path path path i path path path i i i i i i i path i i i i"], "references": ["returns true iff this interval intersects the interval i"], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 3520, "code": "def contains ( self , i ) : return self . start <= i . start and i . end <= self . end", "predictions": ["check if this range contains another range i . e . g ."], "references": ["returns true iff this interval contains the interval i"], "bleu": 0.1135935489027116, "rouge_l": 0.2819722650231125}
{"id": 3521, "code": "def union ( self , i ) : if self . intersects ( i ) or self . end + 1 == i . start or i . end + 1 == self . start : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) ) else : return None", "predictions": ["return a union with the specified number of rows i . e . g ."], "references": ["if intervals intersect returns their union otherwise returns none"], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 3522, "code": "def union fill gap ( self , i ) : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) )", "predictions": ["fill the union with a given union of gap with a given number of i ."], "references": ["like union but ignores whether the two intervals intersect or not"], "bleu": 0.08513012360883544, "rouge_l": 0.07663316582914573}
{"id": 3523, "code": "def intersection ( self , i ) : if self . intersects ( i ) : return Interval ( max ( self . start , i . start ) , min ( self . end , i . end ) ) else : return None", "predictions": ["return the intersection between two rows ."], "references": ["if intervals intersect returns their intersection otherwise returns none"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3524, "code": "def subseq ( self , start , end ) : return Fasta ( self . id , self . seq [ start : end ] )", "predictions": ["returns a sequence of the first dimension between start and end ."], "references": ["returns fasta object with the same name of the bases from start to end but not including end"], "bleu": 0.09863220669367224, "rouge_l": 0.32172995780590713}
{"id": 3525, "code": "def replace bases ( self , old , new ) : self . seq = self . seq . replace ( old , new )", "predictions": ["replace old bases with new one"], "references": ["replaces all occurrences of old with new"], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 3526, "code": "def replace interval ( self , start , end , new ) : if start > end or start > len ( self ) - 1 or end > len ( self ) - 1 : raise Error ( 'Error replacing bases ' + str ( start ) + '-' + str ( end ) + ' in sequence ' + self . id ) self . seq = self . seq [ 0 : start ] + new + self . seq [ end + 1 : ]", "predictions": ["replace a sequence with a new interval with a new one ."], "references": ["replaces the sequence from start to end with the sequence new"], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 3527, "code": "def gaps ( self , min length = 1 ) : gaps = [ ] regex = re . compile ( 'N+' , re . IGNORECASE ) for m in regex . finditer ( self . seq ) : if m . span ( ) [ 1 ] - m . span ( ) [ 0 ] + 1 >= min length : gaps . append ( intervals . Interval ( m . span ( ) [ 0 ] , m . span ( ) [ 1 ] - 1 ) ) return gaps", "predictions": ["calculates the gaps of each block in the list of intervals"], "references": ["finds the positions of all gaps in the sequence that are at least min_length long . returns a list of intervals . coords are zero - based"], "bleu": 0.06515202960986644, "rouge_l": 0.34242181234963914}
{"id": 3528, "code": "def to Fastq ( self , qual scores ) : if len ( self ) != len ( qual scores ) : raise Error ( 'Error making Fastq from Fasta, lengths differ.' , self . id ) return Fastq ( self . id , self . seq , '' . join ( [ chr ( max ( 0 , min ( x , 93 ) ) + 33 ) for x in qual scores ] ) )", "predictions": ["convert sequence sequence to integer"], "references": ["returns a fastq object . qual_scores expected to be a list of numbers like you would get in a . qual file"], "bleu": 0.009111306057550799, "rouge_l": 0.06652126499454744}
{"id": 3529, "code": "def translate ( self , frame = 0 ) : return Fasta ( self . id , '' . join ( [ genetic codes . codes [ genetic code ] . get ( self . seq [ x : x + 3 ] . upper ( ) , 'X' ) for x in range ( frame , len ( self ) - 1 - frame , 3 ) ] ) )", "predictions": ["translate a list of frame into a list of temperature ."], "references": ["returns a fasta sequence translated into amino acids . starts translating from frame where frame expected to be 0 1 or 2"], "bleu": 0.05269190607627347, "rouge_l": 0.17150890346766634}
{"id": 3530, "code": "def subseq ( self , start , end ) : return Fastq ( self . id , self . seq [ start : end ] , self . qual [ start : end ] )", "predictions": ["return a sequence of subseq between start and end ."], "references": ["returns fastq object with the same name of the bases from start to end but not including end"], "bleu": 0.06735938555336447, "rouge_l": 0.20378619153674832}
{"id": 3531, "code": "def trim Ns ( self ) : i = 0 while i < len ( self ) and self . seq [ i ] in 'n N' : i += 1 self . seq = self . seq [ i : ] self . qual = self . qual [ i : ] self . seq = self . seq . rstrip ( 'Nn' ) self . qual = self . qual [ : len ( self . seq ) ]", "predictions": ["remove all sequences that are in the sequence ."], "references": ["removes any leading or trailing n or n characters from the sequence"], "bleu": 0.13309610652103343, "rouge_l": 0.1856925418569254}
{"id": 3532, "code": "def replace interval ( self , start , end , new , qual string ) : if len ( new ) != len ( qual string ) : raise Error ( 'Length of new seq and qual string in replace interval() must be equal. Cannot continue' ) super ( ) . replace interval ( start , end , new ) self . qual = self . qual [ 0 : start ] + qual string + self . qual [ end + 1 : ]", "predictions": ["replace interval with new interval ."], "references": ["replaces the sequence from start to end with the sequence new"], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3533, "code": "def translate ( self ) : fa = super ( ) . translate ( ) return Fastq ( fa . id , fa . seq , 'I' * len ( fa . seq ) )", "predictions": ["return a formula representation of the sequences ."], "references": ["returns a fasta sequence translated into amino acids . starts translating from frame where frame expected to be 0 1 or 2"], "bleu": 0.03084036601527587, "rouge_l": 0.12298387096774194}
{"id": 3534, "code": "def count sequences ( infile ) : seq reader = sequences . file reader ( infile ) n = 0 for seq in seq reader : n += 1 return n", "predictions": ["count the number of sequences in a file ."], "references": ["returns the number of sequences in a file"], "bleu": 0.7598356856515925, "rouge_l": 0.8323586744639376}
{"id": 3535, "code": "def make random contigs ( contigs , length , outfile , name by letters = False , prefix = '' , seed = None , first number = 1 ) : random . seed ( a = seed ) fout = utils . open file write ( outfile ) letters = list ( 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' ) letters index = 0 for i in range ( contigs ) : if name by letters : name = letters [ letters index ] letters index += 1 if letters index == len ( letters ) : letters index = 0 else : name = str ( i + first number ) fa = sequences . Fasta ( prefix + name , '' . join ( [ random . choice ( 'ACGT' ) for x in range ( length ) ] ) ) print ( fa , file = fout ) utils . close ( fout )", "predictions": ["make a random contigs from a contigs of contigs"], "references": ["makes a multi fasta file of random sequences all the same length"], "bleu": 0.12026590852507517, "rouge_l": 0.1856925418569254}
{"id": 3536, "code": "def mean length ( infile , limit = None ) : total = 0 count = 0 seq reader = sequences . file reader ( infile ) for seq in seq reader : total += len ( seq ) count += 1 if limit is not None and count >= limit : break assert count > 0 return total / count", "predictions": ["calculate the mean length of a sequence of infile infile infile ."], "references": ["returns the mean length of the sequences in the input file . by default uses all sequences . to limit to the first n sequences use limit = n"], "bleu": 0.07340248891351002, "rouge_l": 0.22693452380952384}
{"id": 3537, "code": "def merge to one seq ( infile , outfile , seqname = 'union' ) : seq reader = sequences . file reader ( infile ) seqs = [ ] for seq in seq reader : seqs . append ( copy . copy ( seq ) ) new seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new seq , new qual ) else : merged = sequences . Fasta ( seqname , new seq ) seqs [ : ] = [ ] f = utils . open file write ( outfile ) print ( merged , file = f ) utils . close ( f )", "predictions": ["merge sequences into one file"], "references": ["takes a multi fasta or fastq file and writes a new file that contains just one sequence with the original sequences catted together preserving their order"], "bleu": 0.004868582667651212, "rouge_l": 0.05749293119698398}
{"id": 3538, "code": "def sort by size ( infile , outfile , smallest first = False ) : seqs = { } file to dict ( infile , seqs ) seqs = list ( seqs . values ( ) ) seqs . sort ( key = lambda x : len ( x ) , reverse = not smallest first ) fout = utils . open file write ( outfile ) for seq in seqs : print ( seq , file = fout ) utils . close ( fout )", "predictions": ["sort a sequences of sequences"], "references": ["sorts input sequence file by biggest sequence first writes sorted output file . set smallest_first = true to have smallest first"], "bleu": 0.00935797827707123, "rouge_l": 0.0}
{"id": 3539, "code": "def sort by name ( infile , outfile ) : seqs = { } file to dict ( infile , seqs ) #seqs = list(seqs.values()) #seqs.sort() fout = utils . open file write ( outfile ) for name in sorted ( seqs ) : print ( seqs [ name ] , file = fout ) utils . close ( fout )", "predictions": ["sort a dictionary of files"], "references": ["sorts input sequence file by sort - d - k1 1 writes sorted output file ."], "bleu": 0.03025060142990643, "rouge_l": 0.08701854493580599}
{"id": 3540, "code": "def to boulderio ( infile , outfile ) : seq reader = sequences . file reader ( infile ) f out = utils . open file write ( outfile ) for sequence in seq reader : print ( \"SEQUENCE ID=\" + sequence . id , file = f out ) print ( \"SEQUENCE TEMPLATE=\" + sequence . seq , file = f out ) print ( \"=\" , file = f out ) utils . close ( f out )", "predictions": ["convert a sequence of sequence objects to a boulderio"], "references": ["converts input sequence file into a boulder - io format as used by primer3"], "bleu": 0.08961856124931385, "rouge_l": 0.1673525377229081}
{"id": 3541, "code": "def value to string ( self , obj ) : value = self . value from object ( obj ) return b64encode ( self . dump ( value ) ) . decode ( 'ascii' )", "predictions": ["transform the given value to a string ."], "references": ["pickled data is serialized as base64"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3542, "code": "def get version ( version = None ) : version = get complete version ( version ) main = get main version ( version ) sub = '' if version [ 3 ] == 'alpha' and version [ 4 ] == 0 : git changeset = get git changeset ( ) if git changeset : sub = '.dev%s' % git changeset elif version [ 3 ] != 'final' : mapping = { 'alpha' : 'a' , 'beta' : 'b' , 'rc' : 'c' } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return str ( main + sub )", "predictions": ["return the version number from the pep pep pep"], "references": ["returns a pep 386 - compliant version number from version ."], "bleu": 0.22241434515868952, "rouge_l": 0.2946859903381642}
{"id": 3543, "code": "def init unique sets ( self ) : ks = dict ( ) for t in self . unique checks : key = t [ 0 ] ks [ key ] = set ( ) return ks", "predictions": ["initialize the unique checks"], "references": ["initialise sets used for uniqueness checking ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 3544, "code": "def apply value checks ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for field name , check , code , message , modulus in self . value checks : if i % modulus == 0 : fi = self . field names . index ( field name ) if fi < len ( r ) : value = r [ fi ] try : check ( value ) except Value Error : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . name , check . doc ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["apply each value to each value"], "references": ["apply value check functions on the given record r ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3545, "code": "def apply header checks ( self , i , r , summarize = False , context = None ) : for code , message in self . header checks : if tuple ( r ) != self . field names : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = tuple ( r ) p [ 'missing' ] = set ( self . field names ) - set ( r ) p [ 'unexpected' ] = set ( r ) - set ( self . field names ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["apply the header checks to the header"], "references": ["apply header checks on the given record r ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 3546, "code": "def apply record length checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . record length checks : if i % modulus == 0 : if len ( r ) != len ( self . field names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["apply a record length to a record ."], "references": ["apply record length checks on the given record r ."], "bleu": 0.19546825878823415, "rouge_l": 0.5446428571428571}
{"id": 3547, "code": "def apply value predicates ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for field name , predicate , code , message , modulus in self . value predicates : if i % modulus == 0 : fi = self . field names . index ( field name ) if fi < len ( r ) : value = r [ fi ] try : valid = predicate ( value ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . name , predicate . doc ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["apply predicates predicates to each value ."], "references": ["apply value predicates on the given record r ."], "bleu": 0.1755217914979255, "rouge_l": 0.3667334669338677}
{"id": 3548, "code": "def apply record checks ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for check , modulus in self . record checks : if i % modulus == 0 : rdict = self . as dict ( r ) try : check ( rdict ) except Record Error as e : code = e . code if e . code is not None else RECORD CHECK FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD CHECK FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . name , check . doc ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["apply a record checks to a record"], "references": ["apply record checks on r ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 3549, "code": "def apply record predicates ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for predicate , code , message , modulus in self . record predicates : if i % modulus == 0 : rdict = self . as dict ( r ) try : valid = predicate ( rdict ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . name , predicate . doc ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["apply a record predicates to each record ."], "references": ["apply record predicates on r ."], "bleu": 0.239802967618271, "rouge_l": 0.5865384615384615}
{"id": 3550, "code": "def apply unique checks ( self , i , r , unique sets , summarize = False , context = None ) : for key , code , message in self . unique checks : value = None values = unique sets [ key ] if isinstance ( key , basestring ) : fi = self . field names . index ( key ) if fi >= len ( r ) : continue value = r [ fi ] else : value = [ ] for f in key : fi = self . field names . index ( f ) if fi >= len ( r ) : break value . append ( r [ fi ] ) value = tuple ( value ) if value in values : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'key' ] = key p [ 'value' ] = value if context is not None : p [ 'context' ] = context yield p values . add ( value )", "predictions": ["automatically automatically apply each checks in the checks ."], "references": ["apply unique checks on r ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 3551, "code": "def apply each methods ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'each' ) : rdict = self . as dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . name , f . doc ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["apply each report to a single report"], "references": ["invoke each methods on r ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3552, "code": "def apply assert methods ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'assert' ) : rdict = self . as dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Assertion Error as e : code = ASSERT CHECK FAILED message = MESSAGES [ ASSERT CHECK FAILED ] if len ( e . args ) > 0 : custom = e . args [ 0 ] if isinstance ( custom , ( list , tuple ) ) : if len ( custom ) > 0 : code = custom [ 0 ] if len ( custom ) > 1 : message = custom [ 1 ] else : code = custom p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . name , f . doc ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["contains a single line of each line of a and contains a and contains a and yield"], "references": ["apply assert methods on r ."], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 3553, "code": "def apply check methods ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'check' ) : rdict = self . as dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Record Error as e : code = e . code if e . code is not None else RECORD CHECK FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD CHECK FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . name , f . doc ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["union union self ."], "references": ["apply check methods on r ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3554, "code": "def apply skips ( self , i , r , summarize = False , report unexpected exceptions = True , context = None ) : for skip in self . skips : try : result = skip ( r ) if result is True : yield True except Exception as e : if report unexpected exceptions : p = { 'code' : UNEXPECTED EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED EXCEPTION ] % ( e . class . name , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( skip . name , skip . doc ) if context is not None : p [ 'context' ] = context yield p", "predictions": ["union union union i"], "references": ["apply skip functions on r ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3555, "code": "def as dict ( self , r ) : d = dict ( ) for i , f in enumerate ( self . field names ) : d [ f ] = r [ i ] if i < len ( r ) else None return d", "predictions": ["return as a dict"], "references": ["convert the record to a dictionary using field names as keys ."], "bleu": 0.0538140946637381, "rouge_l": 0.11466165413533834}
{"id": 3556, "code": "def create validator ( ) : field names = ( 'study id' , 'patient id' , 'gender' , 'age years' , 'age months' , 'date inclusion' ) validator = CSV Validator ( field names ) validator . add header check ( 'EX1' , 'bad header' ) validator . add record length check ( 'EX2' , 'unexpected record length' ) validator . add value check ( 'study id' , int , 'EX3' , 'study id must be an integer' ) validator . add value check ( 'patient id' , int , 'EX4' , 'patient id must be an integer' ) validator . add value check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add value check ( 'age years' , number range inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add value check ( 'date inclusion' , datetime string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) def check age variables ( r ) : age years = int ( r [ 'age years' ] ) age months = int ( r [ 'age months' ] ) valid = ( age months >= age years * 12 and age months % age years < 12 ) if not valid : raise Record Error ( 'EX8' , 'invalid age variables' ) validator . add record check ( check age variables ) return validator", "predictions": ["create function for creating validator age objects . . instance"], "references": ["create an example csv validator for patient demographic data ."], "bleu": 0.15851165692617156, "rouge_l": 0.3}
{"id": 3557, "code": "def read header ( fd , endian ) : flag class , nzmax = read elements ( fd , endian , [ 'mi UINT32' ] ) header = { 'mclass' : flag class & 0x0FF , 'is logical' : ( flag class >> 9 & 1 ) == 1 , 'is global' : ( flag class >> 10 & 1 ) == 1 , 'is complex' : ( flag class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read elements ( fd , endian , [ 'mi INT32' ] ) header [ 'n dims' ] = len ( header [ 'dims' ] ) if header [ 'n dims' ] != 2 : raise Parse Error ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read elements ( fd , endian , [ 'mi INT8' ] , is name = True ) return header", "predictions": ["replace the bases from a file descriptor to a dictionary ."], "references": ["read and return the matrix header ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 3558, "code": "def eof ( fd ) : b = fd . read ( 1 ) end = len ( b ) == 0 if not end : curpos = fd . tell ( ) fd . seek ( curpos - 1 ) return end", "predictions": ["get a if file is a if file is empty new file new one ."], "references": ["determine if end - of - file is reached for file fd ."], "bleu": 0.12874330508144843, "rouge_l": 0.36180308422301305}
{"id": 3559, "code": "def write var data ( fd , data ) : fd . write ( struct . pack ( 'b3x I' , etypes [ 'mi MATRIX' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )", "predictions": ["gaps data to a file descriptor"], "references": ["write variable data to file"], "bleu": 0.31239399369202553, "rouge_l": 0.5545454545454546}
{"id": 3560, "code": "def write compressed var array ( fd , array , name ) : bd = Bytes IO ( ) write var array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) fd . write ( struct . pack ( 'b3x I' , etypes [ 'mi COMPRESSED' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["write compressed variable data to file"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3561, "code": "def write numeric array ( fd , header , array ) : bd = Bytes IO ( ) write var header ( bd , header ) if not isinstance ( array , basestring ) and header [ 'dims' ] [ 0 ] > 1 : array = list ( chain . from iterable ( izip ( * array ) ) ) write elements ( bd , header [ 'mtp' ] , array ) data = bd . getvalue ( ) bd . close ( ) write var data ( fd , data )", "predictions": ["translate a numeric self for data structure for writing to a file - like object for writing for writing ."], "references": ["write the numeric array"], "bleu": 0.06108557268562171, "rouge_l": 0.09472049689440994}
{"id": 3562, "code": "def raise for status ( self ) : if not self . status : return error = find exception by code ( self . status ) message = None screen = None stacktrace = None if isinstance ( self . value , str ) : message = self . value elif isinstance ( self . value , dict ) : message = self . value . get ( 'message' , None ) screen = self . value . get ( 'screen' , None ) stacktrace = self . value . get ( 'stacktrace' , None ) raise Web Driver Exception ( error , message , screen , stacktrace )", "predictions": ["raise function to raise . raises an exception ."], "references": ["raise webdriverexception if returned status is not zero ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3563, "code": "def fluent ( func ) : @ wraps ( func ) def fluent interface ( instance , * args , * * kwargs ) : ret = func ( instance , * args , * * kwargs ) if ret is not None : return ret return instance return fluent interface", "predictions": ["decorator to automatically automatically automatically automatically ensure that the trim is used"], "references": ["fluent interface decorator to return self if method return none ."], "bleu": 0.1367440667823257, "rouge_l": 0.17528735632183906}
{"id": 3564, "code": "def check unused args ( self , used args , args , kwargs ) : for k , v in kwargs . items ( ) : if k in used args : self . used kwargs . update ( { k : v } ) else : self . unused kwargs . update ( { k : v } )", "predictions": ["replace arguments with new arguments"], "references": ["implement the check_unused_args in superclass ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 3565, "code": "def vformat ( self , format string , args , kwargs ) : self . used kwargs = { } self . unused kwargs = { } return super ( Memorize Formatter , self ) . vformat ( format string , args , kwargs )", "predictions": ["translate the metadata to a translate"], "references": ["clear used and unused dicts before each formatting ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 3566, "code": "def Plug In ( self ) : ids = self . available ids ( ) if len ( ids ) == 0 : raise Max Inputs Reached Error ( 'Max Inputs Reached' ) self . id = ids [ 0 ] xinput . Plug In ( self . id ) while self . id in self . available ids ( ) : pass", "predictions": ["make sure this document has been ."], "references": ["take next available controller id and plug in to virtual usb bus"], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 3567, "code": "def Un Plug ( self , force = False ) : if force : xinput . Un Plug Force ( c uint ( self . id ) ) else : xinput . Un Plug ( c uint ( self . id ) ) while self . id not in self . available ids ( ) : if self . id == 0 : break", "predictions": ["use this to random make sure the xinput is ."], "references": ["unplug controller from virtual usb bus and free up id"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3568, "code": "def main ( ) : import time print ( 'Testing controller in position 1:' ) print ( 'Running 3 x 3 seconds tests' ) con = r Controller ( 1 ) for i in range ( 3 ) : print ( 'Waiting...' ) time . sleep ( 2.5 ) print ( 'State: ' , con . gamepad ) print ( 'Buttons: ' , con . buttons ) time . sleep ( 0.5 ) print ( 'Done!' )", "predictions": ["mean of the program"], "references": ["test the functionality of the rcontroller object"], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 3569, "code": "def buttons ( self ) : return [ name for name , value in r Controller . buttons . items ( ) if self . gamepad . w Buttons & value == value ]", "predictions": ["list of merge merge merge objects sequences sequences sequences sequences sequences sequences sequences sequences sequences ."], "references": ["returns a list of buttons currently pressed"], "bleu": 0.10123734869668824, "rouge_l": 0.1871165644171779}
{"id": 3570, "code": "def maybe decode header ( header ) : value , encoding = decode header ( header ) [ 0 ] if encoding : return value . decode ( encoding ) else : return value", "predictions": ["by size - by - encoded size - encoded string to unicode } } } }"], "references": ["decodes an encoded 7 - bit ascii header value into it s actual value ."], "bleu": 0.08513012360883544, "rouge_l": 0.12978723404255318}
{"id": 3571, "code": "def autodiscover ( ) : from django . conf import settings for application in settings . INSTALLED APPS : module = import module ( application ) if module has submodule ( module , 'emails' ) : emails = import module ( '%s.emails' % application ) try : import module ( '%s.emails.previews' % application ) except Import Error : if module has submodule ( emails , 'previews' ) : raise", "predictions": ["sort the current dict by importing all the { { { { { { { { { { - 1 } outfile } outfile } tied } outfile } outfile }"], "references": ["imports all available previews classes ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3572, "code": "def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . previews . setdefault ( preview . module , { } ) index [ cls . name ] = preview", "predictions": ["registers a preview preview preview sequences"], "references": ["adds a preview to the index ."], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 3573, "code": "def detail view ( self , request , module , preview ) : try : preview = self . previews [ module ] [ preview ] except Key Error : raise Http404 return preview . detail view ( request )", "predictions": ["see documentation for details of a preview object object object object object object object object object object object object object object object object object object object object object object object object"], "references": ["looks up a preview in the index returning a detail view response ."], "bleu": 0.0513487742994337, "rouge_l": 0.09814963797264682}
{"id": 3574, "code": "def url ( self ) : return reverse ( '%s:detail' % URL NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . name , } )", "predictions": ["returns the get get get get the get get get get get get get get get get get get get get get get get get get get get get get get"], "references": ["the url to access this preview ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 3575, "code": "def detail view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form class : if request . GET : form = self . form class ( data = request . GET ) else : form = self . form class ( ) context [ 'form' ] = form if not form . is bound or not form . is valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get message view kwargs ( ) ) message view = self . get message view ( request , * * kwargs ) message = message view . render to message ( ) raw = message . message ( ) headers = Ordered Dict ( ( header , maybe decode header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except Stop Iteration : pass return render ( request , self . template name , context )", "predictions": ["unique unique unique unique unique unique unique unique unique unique unique unique id key key key key key key key key key key key key key key key key key key"], "references": ["renders the message view to a response ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3576, "code": "def execute from command line ( argv = None ) : parser = argparse . Argument Parser ( description = doc ) parser . add argument ( '--monitors-dir' , default = MONITORS DIR ) parser . add argument ( '--alerts-dir' , default = ALERTS DIR ) parser . add argument ( '--config' , default = SMA INI FILE ) parser . add argument ( '--warning' , help = 'set logging to warning' , action = 'store const' , dest = 'loglevel' , const = logging . WARNING , default = logging . INFO ) parser . add argument ( '--quiet' , help = 'set logging to ERROR' , action = 'store const' , dest = 'loglevel' , const = logging . ERROR , default = logging . INFO ) parser . add argument ( '--debug' , help = 'set logging to DEBUG' , action = 'store const' , dest = 'loglevel' , const = logging . DEBUG , default = logging . INFO ) parser . add argument ( '--verbose' , help = 'set logging to COMM' , action = 'store const' , dest = 'loglevel' , const = 5 , default = logging . INFO ) parser . sub = parser . add subparsers ( ) parse service = parser . sub . add parser ( 'service' , help = 'Run SMA as service (daemon).' ) parse service . set defaults ( which = 'service' ) parse oneshot = parser . sub . add parser ( 'one-shot' , help = 'Run SMA once and exit' ) parse oneshot . set defaults ( which = 'one-shot' ) parse alerts = parser . sub . add parser ( 'alerts' , help = 'Alerts options.' ) parse alerts . set defaults ( which = 'alerts' ) parse alerts . add argument ( '--test' , help = 'Test alert' , action = 'store true' ) parse alerts . add argument ( 'alert section' , nargs = '?' , help = 'Alert section to see' ) parse results = parser . sub . add parser ( 'results' , help = 'Monitors results' ) parse results . set defaults ( which = 'results' ) parser . set default subparser ( 'one-shot' ) args = parser . parse args ( argv [ 1 : ] ) create logger ( 'sma' , args . loglevel ) if not getattr ( args , 'which' , None ) or args . which == 'one-shot' : sma = SMA ( args . monitors dir , args . alerts dir , args . config ) sma . evaluate and alert ( ) elif args . which == 'service' : sma = SMA Service ( args . monitors dir , args . alerts dir , args . config ) sma . start ( ) elif args . which == 'alerts' and args . test : sma = SMA ( args . monitors dir , args . alerts dir , args . config ) sma . alerts . test ( ) elif args . which == 'results' : print ( SMA ( args . monitors dir , args . alerts dir , args . config ) . results )", "predictions": ["apply a checks to a checks unexpected unexpected checks unexpected ."], "references": ["a simple method that runs a managementutility ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 3577, "code": "def emit ( self , record ) : try : self . redis client . publish ( self . channel , self . format ( record ) ) except redis . Redis Error : pass", "predictions": ["apply a self summarize to the listener summarize summarize"], "references": ["publish record to redis logging channel"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3578, "code": "def emit ( self , record ) : try : if self . max messages : p = self . redis client . pipeline ( ) p . rpush ( self . key , self . format ( record ) ) p . ltrim ( self . key , - self . max messages , - 1 ) p . execute ( ) else : self . redis client . rpush ( self . key , self . format ( record ) ) except redis . Redis Error : pass", "predictions": ["emits the given record to the client summarize summarize the given record summarize summarize"], "references": ["publish record to redis logging list"], "bleu": 0.11633270842295028, "rouge_l": 0.21554770318021202}
{"id": 3579, "code": "def require template debug ( f ) : def ( * args , * * kwargs ) : TEMPLATE DEBUG = getattr ( settings , 'TEMPLATE DEBUG' , False ) return f ( * args , * * kwargs ) if TEMPLATE DEBUG else '' return", "predictions": ["decorator to apply a value to a value in the value of a value"], "references": ["decorated function is a no - op if template_debug is false"], "bleu": 0.08839374326825923, "rouge_l": 0.08176943699731902}
{"id": 3580, "code": "def pydevd ( context ) : global pdevd not available if pdevd not available : return '' try : import pydevd except Import Error : pdevd not available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] #catch the case where no client is listening try : pydevd . settrace ( ) except socket . error : pdevd not available = True return ''", "predictions": ["get the list of available 0 for the user ."], "references": ["start a pydev settrace"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3581, "code": "def flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string types ) : for sub i in flatten ( i ) : yield sub i else : yield i", "predictions": ["apply an predicates to a list ."], "references": ["given an iterable with nested iterables generate a flat iterable"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3582, "code": "def parse log messages ( self , text ) : regex = r\"commit ([0-9a-f]+)\\n Author: (.*?)\\n\\n(.*?)(?:\\n\\n|$)\" messages = re . findall ( regex , text , re . DOTALL ) parsed = [ ] for commit , author , message in messages : parsed . append ( ( commit [ : 10 ] , re . sub ( r\"\\s*<.*?>\" , \"\" , author ) , message . strip ( ) ) ) return parsed", "predictions": ["apply unique unique unique unique unique checks for a given i for a given i for a unique i for a list of i for a list of i for a"], "references": ["will parse git log messages in the short format"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3583, "code": "def determine paths ( self , package name = None , create package dir = False , dry run = False ) : self . project dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) distribution = self . get distribution ( ) if distribution : self . project name = distribution . get name ( ) else : self . project name = self . project dir . name if os . path . isdir ( self . project dir / \"src\" ) : package search dir = self . project dir / \"src\" else : package search dir = self . project dir created package dir = False if not package name : package name = self . project name . replace ( \"-\" , \" \" ) def get matches ( name ) : possibles = [ n for n in os . listdir ( package search dir ) if os . path . isdir ( package search dir / n ) ] return difflib . get close matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get matches ( package name ) if not close and \" \" in package name : short package name = \" \" . join ( package name . split ( \" \" ) [ 1 : ] ) close = get matches ( short package name ) if not close : if create package dir : package dir = package search dir / package name created package dir = True if not dry run : print ( \"Creating package directory at %s\" % package dir ) os . mkdir ( package dir ) else : print ( \"Would have created package directory at %s\" % package dir ) else : raise Command Error ( \"Could not guess the package name. Specify it using --name.\" ) else : package name = close [ 0 ] self . package name = package name self . package dir = package search dir / package name if not os . path . exists ( self . package dir ) and not created package dir : raise Command Error ( \"Package directory did not exist at %s. Perhaps specify it using --name\" % self . package dir )", "predictions": ["apply each package to the project"], "references": ["determine paths automatically and a little intelligently"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 3584, "code": "def get sha ( a file , settings = None ) : if settings : error = settings [ \"error\" ] else : error = ERROR FN try : BLOCKSIZE = 65536 hasher = hashlib . sha1 ( ) with io . open ( a file , \"rb\" ) as fh : buf = fh . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = fh . read ( BLOCKSIZE ) the hash = hasher . hexdigest ( ) except IO Error : errmes = \"File '{}' could not be read! Exiting!\" . format ( a file ) error ( errmes ) sys . exit ( 1 ) except : errmes = \"Unspecified error returning sha1 hash. Exiting!\" error ( errmes ) sys . exit ( 1 ) return the hash", "predictions": ["get the sha hash from a file ."], "references": ["returns sha1 hash of the file supplied as an argument"], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 3585, "code": "def find standard sakefile ( settings ) : error = settings [ \"error\" ] if settings [ \"customsake\" ] : custom = settings [ \"customsake\" ] if not os . path . isfile ( custom ) : error ( \"Specified sakefile '{}' doesn't exist\" , custom ) sys . exit ( 1 ) return custom for name in [ \"Sakefile\" , \"Sakefile.yaml\" , \"Sakefile.yml\" ] : if os . path . isfile ( name ) : return name error ( \"Error: there is no Sakefile to read\" ) sys . exit ( 1 )", "predictions": ["find the standard sakefile if it is not already ."], "references": ["returns the filename of the appropriate sakefile"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 3586, "code": "def itertable ( table ) : for item in table : res = { k . lower ( ) : nfd ( v ) if isinstance ( v , text type ) else v for k , v in item . items ( ) } for extra in res . pop ( 'extra' , [ ] ) : k , , v = extra . partition ( ':' ) res [ k . strip ( ) ] = v . strip ( ) yield res", "predictions": ["convert a table to a list of dictionaries"], "references": ["auxiliary function for iterating over a data table ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3587, "code": "def is valid sound ( sound , ts ) : if isinstance ( sound , ( Marker , Unknown Sound ) ) : return False s1 = ts [ sound . name ] s2 = ts [ sound . s ] return s1 . name == s2 . name and s1 . s == s2 . s", "predictions": ["return true if the sound is a valid sound sound ."], "references": ["check the consistency of a given transcription system conversino"], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 3588, "code": "def normalize ( self , string ) : return '' . join ( [ self . normalize . get ( x , x ) for x in nfd ( string ) ] )", "predictions": ["return a string with the given string ."], "references": ["normalize the string according to normalization list"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3589, "code": "def from name ( self , string ) : components = string . split ( ' ' ) if frozenset ( components ) in self . features : return self . features [ frozenset ( components ) ] rest , sound class = components [ : - 1 ] , components [ - 1 ] if sound class in [ 'diphthong' , 'cluster' ] : if string . startswith ( 'from ' ) and 'to ' in string : extension = { 'diphthong' : 'vowel' , 'cluster' : 'consonant' } [ sound class ] string = ' ' . join ( string . split ( ' ' ) [ 1 : - 1 ] ) from , to = string . split ( ' to ' ) v1 , v2 = frozenset ( from . split ( ' ' ) + [ extension ] ) , frozenset ( to . split ( ' ' ) + [ extension ] ) if v1 in self . features and v2 in self . features : s1 , s2 = ( self . features [ v1 ] , self . features [ v2 ] ) if sound class == 'diphthong' : return Diphthong . from sounds ( s1 + s2 , s1 , s2 , self ) else : return Cluster . from sounds ( s1 + s2 , s1 , s2 , self ) else : s1 , s2 = self . from name ( from + ' ' + extension ) , self . from name ( to + ' ' + extension ) if not ( isinstance ( s1 , Unknown Sound ) or isinstance ( s2 , Unknown Sound ) ) : if sound class == 'diphthong' : return Diphthong . from sounds ( s1 + s2 , s1 , s2 , self ) return Cluster . from sounds ( s1 + s2 , s1 , s2 , self ) raise Value Error ( 'components could not be found in system' ) else : raise Value Error ( 'name string is erroneously encoded' ) if sound class not in self . sound classes : raise Value Error ( 'no sound class specified' ) args = { self . feature values . get ( comp , '?' ) : comp for comp in rest } if '?' in args : raise Value Error ( 'string contains unknown features' ) args [ 'grapheme' ] = '' args [ 'ts' ] = self sound = self . sound classes [ sound class ] ( * * args ) if sound . featureset not in self . features : sound . generated = True return sound return self . features [ sound . featureset ]", "predictions": ["return a sound string from a given name ."], "references": ["parse a sound from its name"], "bleu": 0.21105340631872635, "rouge_l": 0.5532879818594103}
{"id": 3590, "code": "def iteration ( self ) : i = 0 conv = np . inf old conv = - np . inf conv list = [ ] m = self . original if isinstance ( self . original , pd . Data Frame ) : ipfn method = self . ipfn df elif isinstance ( self . original , np . ndarray ) : ipfn method = self . ipfn np self . original = self . original . astype ( 'float64' ) else : print ( 'Data input instance not recognized' ) sys . exit ( 0 ) while ( ( i <= self . max itr and conv > self . conv rate ) and ( i <= self . max itr and abs ( conv - old conv ) > self . rate tolerance ) ) : old conv = conv m , conv = ipfn method ( m , self . aggregates , self . dimensions , self . weight col ) conv list . append ( conv ) i += 1 converged = 1 if i <= self . max itr : if not conv > self . conv rate : print ( 'ipfn converged: convergence rate below threshold' ) elif not abs ( conv - old conv ) > self . rate tolerance : print ( 'ipfn converged: convergence rate not updating or below rate tolerance' ) else : print ( 'Maximum iterations reached' ) converged = 0 if self . verbose == 0 : return m elif self . verbose == 1 : return m , converged elif self . verbose == 2 : return m , converged , pd . Data Frame ( { 'iteration' : range ( i ) , 'conv' : conv list } ) . set index ( 'iteration' ) else : print ( 'wrong verbose input, return None' ) sys . exit ( 0 )", "predictions": ["a helper method for the input iteration of the main dataframe"], "references": ["runs the ipfn algorithm . automatically detects of working with numpy ndarray or pandas dataframes ."], "bleu": 0.08001467044102561, "rouge_l": 0.14336075205640422}
{"id": 3591, "code": "def get days span ( self , month index ) : is first month = month index == 0 is last month = month index == self . len ( ) - 1 y = int ( self . start date . year + ( self . start date . month + month index ) / 13 ) m = int ( ( self . start date . month + month index ) % 12 or 12 ) total = calendar . monthrange ( y , m ) [ 1 ] if is first month and is last month : return ( self . end date - self . start date ) . days + 1 else : if is first month : return total - self . start date . day + 1 elif is last month : return self . end date . day else : return total", "predictions": ["return the span of the days at the given index"], "references": ["calculate how many days the month spans ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3592, "code": "def calculate period ( self , vals ) : if len ( vals ) < 4 : return None if self . firmware [ 'major' ] < 16 : return ( ( vals [ 3 ] << 24 ) | ( vals [ 2 ] << 16 ) | ( vals [ 1 ] << 8 ) | vals [ 0 ] ) / 12e6 else : return self . calculate float ( vals )", "predictions": ["calculate the period value for a particular period ."], "references": ["calculate the sampling period in seconds"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 3593, "code": "def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send lock , self . senders , self . frames received , callback = self . receive callback , fcs nack = self . fcs nack , ) self . receiver . start ( )", "predictions": ["start the receiver ."], "references": ["starts hdlc controller s threads ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3594, "code": "def stop ( self ) : if self . receiver != None : self . receiver . join ( ) for s in self . senders . values ( ) : s . join ( )", "predictions": ["stop the thread ."], "references": ["stops hdlc controller s threads ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3595, "code": "def replace ( self , * * k ) : if self . date != 'infinity' : return Date ( self . date . replace ( * * k ) ) else : return Date ( 'infinity' )", "predictions": ["return a new datetime with the given date ."], "references": ["note returns a new date obj"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 3596, "code": "def validate token ( self , request , consumer , token ) : oauth server , oauth request = oauth provider . utils . initialize server request ( request ) oauth server . verify request ( oauth request , consumer , token )", "predictions": ["validate the request token"], "references": ["check the token and raise an oauth . error exception if invalid ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 3597, "code": "def check nonce ( self , request , oauth request ) : oauth nonce = oauth request [ 'oauth nonce' ] oauth timestamp = oauth request [ 'oauth timestamp' ] return check nonce ( request , oauth request , oauth nonce , oauth timestamp )", "predictions": ["checks if user is allowed to access an oauth user ."], "references": ["checks nonce of request and return true if valid ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 3598, "code": "def deliveries ( self ) : key = make key ( event = self . object . event , owner name = self . object . owner . username , identifier = self . object . identifier ) return redis . lrange ( key , 0 , 20 )", "predictions": ["generate a redis key"], "references": ["get delivery log from redis"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3599, "code": "def event choices ( events ) : if events is None : msg = \"Please add some events in settings.WEBHOOK EVENTS.\" raise Improperly Configured ( msg ) try : choices = [ ( x , x ) for x in events ] except Type Error : \"\"\" Not a valid iterator, so we raise an exception \"\"\" msg = \"settings.WEBHOOK EVENTS must be an iterable object.\" raise Improperly Configured ( msg ) return choices", "predictions": ["add choices to the event s events ."], "references": ["get the possible events from settings"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 3600, "code": "def check Serial ( self ) : for item in self . rx Serial ( self . TUN . tun . mtu ) : try : self . TUN . tun . write ( item ) except pytun . Error as error : print ( \"pytun error writing: {0}\" . format ( item ) ) print ( error )", "predictions": ["checks if there are any tun ."], "references": ["check the serial port for data to write to the tun adapter ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 3601, "code": "def value from datadict ( self , * args , * * kwargs ) : value = super ( Rich Text Widget , self ) . value from datadict ( * args , * * kwargs ) if value is not None : value = self . get sanitizer ( ) ( value ) return value", "predictions": ["override default django 1 ."], "references": ["pass the submitted value through the sanitizer before returning it ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3602, "code": "def heappop max ( heap ) : lastelt = heap . pop ( ) if heap : returnitem = heap [ 0 ] heap [ 0 ] = lastelt siftup max ( heap , 0 ) return returnitem return lastelt", "predictions": ["maxheap version of a heappop ."], "references": ["maxheap version of a heappop ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3603, "code": "def heapreplace max ( heap , item ) : returnitem = heap [ 0 ] heap [ 0 ] = item siftup max ( heap , 0 ) return returnitem", "predictions": ["maxheap version of a heap followed by a heap ."], "references": ["maxheap version of a heappop followed by a heappush ."], "bleu": 0.4832697830906221, "rouge_l": 0.8}
{"id": 3604, "code": "def heappush max ( heap , item ) : heap . append ( item ) siftdown max ( heap , 0 , len ( heap ) - 1 )", "predictions": ["maxheap version of max heap ."], "references": ["push item onto heap maintaining the heap invariant ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3605, "code": "def heappushpop max ( heap , item ) : if heap and heap [ 0 ] > item : item , heap [ 0 ] = heap [ 0 ] , item siftup max ( heap , 0 ) return item", "predictions": ["maxheap version of a heap ."], "references": ["fast version of a heappush followed by a heappop ."], "bleu": 0.2231931376573339, "rouge_l": 0.47843137254901963}
{"id": 3606, "code": "def validate response ( expected responses ) : def internal decorator ( function ) : @ wraps ( function ) async def wrapper ( * args , * * kwargs ) : response = await function ( * args , * * kwargs ) for expected response in expected responses : if response . startswith ( expected response ) : return response raise QRT Command Exception ( \"Expected %s but got %s\" % ( expected responses , response ) ) return wrapper return internal decorator", "predictions": ["decorator to validate the response of the response ."], "references": ["decorator to validate responses from qtm"], "bleu": 0.2626909894424158, "rouge_l": 0.4149659863945578}
{"id": 3607, "code": "async def qtm version ( self ) : return await asyncio . wait for ( self . protocol . send command ( \"qtmversion\" ) , timeout = self . timeout )", "predictions": ["return the qtm version ."], "references": ["get the qtm version ."], "bleu": 0.7598356856515925, "rouge_l": 0.8}
{"id": 3608, "code": "async def stream frames stop ( self ) : self . protocol . set on packet ( None ) cmd = \"streamframes stop\" await self . protocol . send command ( cmd , callback = False )", "predictions": ["stream frames on socket ."], "references": ["stop streaming frames ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 3609, "code": "async def release control ( self ) : cmd = \"releasecontrol\" return await asyncio . wait for ( self . protocol . send command ( cmd ) , timeout = self . timeout )", "predictions": ["release control command ."], "references": ["release control of qtm ."], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 3610, "code": "async def start ( self , rtfromfile = False ) : cmd = \"start\" + ( \" rtfromfile\" if rtfromfile else \"\" ) return await asyncio . wait for ( self . protocol . send command ( cmd ) , timeout = self . timeout )", "predictions": ["start a rpc command ."], "references": ["start rt from file . you need to be in control of qtm to be able to do this ."], "bleu": 0.01504254234731835, "rouge_l": 0.14437869822485208}
{"id": 3611, "code": "async def set qtm event ( self , event = None ) : cmd = \"event%s\" % ( \"\" if event is None else \" \" + event ) return await asyncio . wait for ( self . protocol . send command ( cmd ) , timeout = self . timeout )", "predictions": ["set a qtm event ."], "references": ["set event in qtm ."], "bleu": 0.34329452398451965, "rouge_l": 0.6}
{"id": 3612, "code": "def data received ( self , data ) : self . received data += data h size = R Theader . size data = self . received data size , type = R Theader . unpack from ( data , 0 ) while len ( data ) >= size : self . parse received ( data [ h size : size ] , type ) data = data [ size : ] if len ( data ) < h size : break size , type = R Theader . unpack from ( data , 0 ) self . received data = data", "predictions": ["message was received from device ."], "references": ["received from qtm and route accordingly"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 3613, "code": "def get analog ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . device count ) : component position , device = QRT Packet . get exact ( RT Analog Device , data , component position ) if device . sample count > 0 : component position , sample number = QRT Packet . get exact ( RT Sample Number , data , component position ) RT Analog Channel . format = struct . Struct ( RT Analog Channel . format str % device . sample count ) for in range ( device . channel count ) : component position , channel = QRT Packet . get tuple ( RT Analog Channel , data , component position ) append components ( ( device , sample number , channel ) ) return components", "predictions": ["get components of a component"], "references": ["get analog data ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3614, "code": "def get analog single ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . device count ) : component position , device = QRT Packet . get exact ( RT Analog Device Single , data , component position ) RT Analog Device Samples . format = struct . Struct ( RT Analog Device Samples . format str % device . channel count ) component position , sample = QRT Packet . get tuple ( RT Analog Device Samples , data , component position ) append components ( ( device , sample ) ) return components", "predictions": ["get single component of a single component ."], "references": ["get a single analog data channel ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 3615, "code": "def get force ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . plate count ) : component position , plate = QRT Packet . get exact ( RT Force Plate , data , component position ) force list = [ ] for in range ( plate . force count ) : component position , force = QRT Packet . get exact ( RT Force , data , component position ) force list . append ( force ) append components ( ( plate , force list ) ) return components", "predictions": ["force a list of components to a component"], "references": ["get force data ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3616, "code": "def get force single ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . plate count ) : component position , plate = QRT Packet . get exact ( RT Force Plate Single , data , component position ) component position , force = QRT Packet . get exact ( RT Force , data , component position ) append components ( ( plate , force ) ) return components", "predictions": ["get a single instance for a given settings ."], "references": ["get a single force data channel ."], "bleu": 0.2777619034011791, "rouge_l": 0.5115303983228512}
{"id": 3617, "code": "def get 6d ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . body count ) : component position , position = QRT Packet . get exact ( RT6D Body Position , data , component position ) component position , matrix = QRT Packet . get tuple ( RT6D Body Rotation , data , component position ) append components ( ( position , matrix ) ) return components", "predictions": ["return can be a list of components"], "references": ["get 6d data ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3618, "code": "def get 6d euler ( self , component info = None , data = None , component position = None ) : components = [ ] append components = components . append for in range ( component info . body count ) : component position , position = QRT Packet . get exact ( RT6D Body Position , data , component position ) component position , euler = QRT Packet . get exact ( RT6D Body Euler , data , component position ) append components ( ( position , euler ) ) return components", "predictions": ["itertable may be a list of 6d"], "references": ["get 6d data with euler rotations ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3619, "code": "def get 3d markers ( self , component info = None , data = None , component position = None ) : return self . get 3d markers ( RT3D Marker Position , component info , data , component position )", "predictions": ["is the valid sound sound"], "references": ["get 3d markers ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3620, "code": "def get 3d markers residual ( self , component info = None , data = None , component position = None ) : return self . get 3d markers ( RT3D Marker Position Residual , component info , data , component position )", "predictions": ["gets the 3d residual residual residual for the specified return data nfd"], "references": ["get 3d markers with residual ."], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 3621, "code": "def get 3d markers no label ( self , component info = None , data = None , component position = None ) : return self . get 3d markers ( RT3D Marker Position No Label , component info , data , component position )", "predictions": ["from name of name self return the name of the components"], "references": ["get 3d markers without label ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3622, "code": "def get 3d markers no label residual ( self , component info = None , data = None , component position = None ) : return self . get 3d markers ( RT3D Marker Position No Label Residual , component info , data , component position )", "predictions": ["gets the 3d i i can use"], "references": ["get 3d markers without label with residual ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3623, "code": "async def await event ( self , event = None , timeout = None ) : if self . event future is not None : raise Exception ( \"Can't wait on multiple events!\" ) result = await asyncio . wait for ( self . wait loop ( event ) , timeout ) return result", "predictions": ["block for events and block until a client has been received ."], "references": ["wait for any or specified event"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 3624, "code": "def send command ( self , command , callback = True , command type = QRT Packet Type . Packet Command ) : if self . transport is not None : cmd length = len ( command ) LOG . debug ( \"S: %s\" , command ) self . transport . write ( struct . pack ( RT Command % cmd length , R Theader . size + cmd length + 1 , command type . value , command . encode ( ) , b\"\\0\" , ) ) future = self . loop . create future ( ) if callback : self . request queue . append ( future ) else : future . set result ( None ) return future raise QRT Command Exception ( \"Not connected!\" )", "predictions": ["calculate a period and return the response ."], "references": ["sends commands to qtm"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3625, "code": "async def reboot ( ip address ) : , protocol = await asyncio . get event loop ( ) . create datagram endpoint ( Q Reboot Protocol , local addr = ( ip address , 0 ) , allow broadcast = True , reuse address = True , ) LOG . info ( \"Sending reboot on %s\" , ip address ) protocol . send reboot ( )", "predictions": ["reboot a callback ."], "references": ["async function to reboot qtm cameras"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3626, "code": "def on packet ( packet ) : print ( \"Framenumber: {}\" . format ( packet . framenumber ) ) header , markers = packet . get 3d markers ( ) print ( \"Component info: {}\" . format ( header ) ) for marker in markers : print ( \"\\t\" , marker )", "predictions": ["show a packet packet packet packet"], "references": ["callback function that is called everytime a data packet arrives from qtm"], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 3627, "code": "def datagram received ( self , datagram , address ) : size , = R Theader . unpack from ( datagram , 0 ) info , = struct . unpack from ( \"{0}s\" . format ( size - 3 - 8 ) , datagram , R Theader . size ) base port , = QRT Discovery Base Port . unpack from ( datagram , size - 2 ) if self . receiver is not None : self . receiver ( QRT Discovery Response ( info , address [ 0 ] , base port ) )", "predictions": ["replace received with replace replace replace . . . . . . . . . . . . ."], "references": ["parse response from qtm instances"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 3628, "code": "def send discovery packet ( self ) : if self . port is None : return self . transport . sendto ( QRT Discovery P1 . pack ( QRT Discovery Packet Size , QRT Packet Type . Packet Discover . value ) + QRT Discovery P2 . pack ( self . port ) , ( \"<broadcast>\" , 22226 ) , )", "predictions": ["validate token token . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["send discovery packet for qtm to respond to"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3629, "code": "async def packet receiver ( queue ) : LOG . info ( \"Entering packet receiver\" ) while True : packet = await queue . get ( ) if packet is None : break LOG . info ( \"Framenumber %s\" , packet . framenumber ) LOG . info ( \"Exiting packet receiver\" )", "predictions": ["process nonce nonce . . . . . . ."], "references": ["asynchronous function that processes queue until none is posted in queue"], "bleu": 0.0959156018869021, "rouge_l": 0.0}
{"id": 3630, "code": "async def choose qtm instance ( interface ) : instances = { } print ( \"Available QTM instances:\" ) async for i , qtm instance in Async Enumerate ( qtm . Discover ( interface ) , start = 1 ) : instances [ i ] = qtm instance print ( \"{} - {}\" . format ( i , qtm instance . info ) ) try : choice = int ( input ( \"Connect to: \" ) ) if choice not in instances : raise Value Error except Value Error : LOG . error ( \"Invalid choice\" ) return None return instances [ choice ] . host", "predictions": ["get an self instance instance instance instance"], "references": ["list running qtm instances asks for input and return chosen qtm"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 3631, "code": "async def package receiver ( queue ) : LOG . info ( \"Entering package receiver\" ) while True : packet = await queue . get ( ) if packet is None : break LOG . info ( \"Framenumber %s\" , packet . framenumber ) header , cameras = packet . get 2d markers ( ) LOG . info ( \"Component info: %s\" , header ) for i , camera in enumerate ( cameras , 1 ) : LOG . info ( \"Camera %d\" , i ) for marker in camera : LOG . info ( \"\\t%s\" , marker ) LOG . info ( \"Exiting package receiver\" )", "predictions": ["log in a choices ."], "references": ["asynchronous function that processes queue until none is posted in queue"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3632, "code": "def create body index ( xml string ) : xml = ET . fromstring ( xml string ) body to index = { } for index , body in enumerate ( xml . findall ( \"*/Body/Name\" ) ) : body to index [ body . text . strip ( ) ] = index return body to index", "predictions": ["check the body index and format the body index"], "references": ["extract a name to index dictionary from 6dof settings xml"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 3633, "code": "def build swig ( ) : print ( \"Looking for FANN libs...\" ) find fann ( ) print ( \"running SWIG...\" ) swig bin = find swig ( ) swig cmd = [ swig bin , '-c++' , '-python' , 'fann2/fann2.i' ] subprocess . Popen ( swig cmd ) . wait ( )", "predictions": ["value to value of from github"], "references": ["run swig with specified parameters"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3634, "code": "def experiment ( ctx , project , experiment ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'experiment' ] = experiment", "predictions": ["update an heappop in an heappop"], "references": ["commands for experiments ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3635, "code": "def upload ( sync = True ) : project = Project Manager . get config or raise ( ) files = Ignore Manager . get unignored file paths ( ) try : with create tarfile ( files , project . name ) as file path : with get files in current directory ( 'repo' , [ file path ] ) as ( files , files size ) : try : Polyaxon Client ( ) . project . upload repo ( project . user , project . name , files , files size , sync = sync ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) Printer . print error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print success ( 'Files uploaded.' ) except Exception as e : Printer . print error ( \"Could not upload the file.\" ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )", "predictions": ["upload notebook to sync"], "references": ["upload code of the current directory while respecting the . polyaxonignore file ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 3636, "code": "def cluster ( node ) : cluster client = Polyaxon Client ( ) . cluster if node : try : node config = cluster client . get node ( node ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not load node `{}` info.' . format ( node ) ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get node info ( node config ) else : try : cluster config = cluster client . get cluster ( ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not load cluster info.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get cluster info ( cluster config )", "predictions": ["start a heappush append heappush append to the heappush append append to the heappush append append to the heappush append ."], "references": ["get cluster and nodes info ."], "bleu": 0.05809665204409193, "rouge_l": 0.08232118758434548}
{"id": 3637, "code": "def check ( file , version , definition ) : file = file or 'polyaxonfile.yaml' specification = check polyaxonfile ( file ) . specification if version : Printer . decorate format value ( 'The version is: {}' , specification . version , 'yellow' ) if definition : job condition = ( specification . is job or specification . is build or specification . is notebook or specification . is tensorboard ) if specification . is experiment : Printer . decorate format value ( 'This polyaxon specification has {}' , 'One experiment' , 'yellow' ) if job condition : Printer . decorate format value ( 'This {} polyaxon specification is valid' , specification . kind , 'yellow' ) if specification . is group : experiments def = specification . experiments def click . echo ( 'This polyaxon specification has experiment group with the following definition:' ) get group experiments info ( * * experiments def ) return specification", "predictions": ["check must be a string or a 0 - d 0 ."], "references": ["check a polyaxonfile ."], "bleu": 0.1235622127262679, "rouge_l": 0.4121621621621622}
{"id": 3638, "code": "def job ( ctx , project , job ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'job' ] = job", "predictions": ["filters a validate validate validate validate a validate validate validate a validate validate validate validate internal validate validate internal validate internal validate internal validate internal validate validate internal validate validate validate"], "references": ["commands for jobs ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3639, "code": "def pprint ( value ) : click . echo ( json . dumps ( value , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) )", "predictions": ["prints a await message to stdout ."], "references": ["prints as formatted json"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3640, "code": "def login ( token , username , password ) : auth client = Polyaxon Client ( ) . auth if username : if not password : password = click . prompt ( 'Please enter your password' , type = str , hide input = True ) password = password . strip ( ) if not password : logger . info ( 'You entered an empty string. ' 'Please make sure you enter your password correctly.' ) sys . exit ( 1 ) credentials = Credentials Config ( username = username , password = password ) try : access code = auth client . login ( credentials = credentials ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not login.' ) Printer . print error ( 'Error Message `{}`.' . format ( e ) ) sys . exit ( 1 ) if not access code : Printer . print error ( \"Failed to login\" ) return else : if not token : token url = \"{}/app/token\" . format ( auth client . config . http host ) click . confirm ( 'Authentication token page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( token url ) logger . info ( \"Please copy and paste the authentication token.\" ) token = click . prompt ( 'This is an invisible field. Paste token and press ENTER' , type = str , hide input = True ) if not token : logger . info ( \"Empty token received. \" \"Make sure your shell is handling the token appropriately.\" ) logger . info ( \"See docs for help: http://docs.polyaxon.com/polyaxon cli/commands/auth\" ) return access code = token . strip ( \" \" ) try : Auth Config Manager . purge ( ) user = Polyaxon Client ( ) . auth . get user ( token = access code ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not load user info.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) access token = Access Token Config ( username = user . username , token = access code ) Auth Config Manager . set config ( access token ) Printer . print success ( \"Login successful\" ) server version = get server version ( ) current version = get current version ( ) log handler = get log handler ( ) Cli Config Manager . reset ( check count = 0 , current version = current version , min version = server version . min version , log handler = log handler )", "predictions": ["def to the user ."], "references": ["login to polyaxon ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 3641, "code": "def whoami ( ) : try : user = Polyaxon Client ( ) . auth . get user ( ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not load user info.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) click . echo ( \"\\n Username: {username}, Email: {email}\\n\" . format ( * * user . to dict ( ) ) )", "predictions": ["display user of user"], "references": ["show current logged polyaxon user ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3642, "code": "def build ( ctx , project , build ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'build' ] = build", "predictions": ["def to the def def ."], "references": ["commands for build jobs ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3643, "code": "def init ( project , polyaxonfile ) : user , project name = get project or local ( project ) try : project config = Polyaxon Client ( ) . project . get project ( user , project name ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Make sure you have a project with this name `{}`' . format ( project ) ) Printer . print error ( 'You can a create new project with this command: ' 'polyaxon project create ' '--name={} [--description=...] [--tags=...]' . format ( project name ) ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) init project = False if Project Manager . is initialized ( ) : local project = Project Manager . get config ( ) click . echo ( 'Warning! This project is already initialized with the following project:' ) with clint . textui . indent ( 4 ) : clint . textui . puts ( 'User: {}' . format ( local project . user ) ) clint . textui . puts ( 'Project: {}' . format ( local project . name ) ) if click . confirm ( 'Would you like to override this current config?' , default = False ) : init project = True else : init project = True if init project : Project Manager . purge ( ) Project Manager . set config ( project config , init = True ) Printer . print success ( 'Project was initialized' ) else : Printer . print header ( 'Project config was not changed.' ) init ignore = False if Ignore Manager . is initialized ( ) : click . echo ( 'Warning! Found a .polyaxonignore file.' ) if click . confirm ( 'Would you like to override it?' , default = False ) : init ignore = True else : init ignore = True if init ignore : Ignore Manager . init config ( ) Printer . print success ( 'New .polyaxonignore file was created.' ) else : Printer . print header ( '.polyaxonignore file was not changed.' ) if polyaxonfile : create polyaxonfile ( )", "predictions": ["creates a new qtm qtm else else creates a new qtm else else else else else else else else else ."], "references": ["initialize a new polyaxonfile specification ."], "bleu": 0.0821610732492254, "rouge_l": 0.2469635627530364}
{"id": 3644, "code": "def bookmark ( ctx , username ) : ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username", "predictions": ["list virtual api ."], "references": ["commands for bookmarks ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 3645, "code": "def remove trailing spaces ( line ) : while line . endswith ( ' ' ) and not line . endswith ( '\\\\ ' ) : line = line [ : - 1 ] return line . replace ( '\\\\ ' , ' ' )", "predictions": ["get all analog spaces . ."], "references": ["remove trailing spaces unless they are quoted with a backslash ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3646, "code": "def find matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern", "predictions": ["get all files that match path . . ."], "references": ["yield all matching patterns for path ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 3647, "code": "def is ignored ( cls , path , patterns ) : status = None for pattern in cls . find matching ( path , patterns ) : status = pattern . is exclude return status", "predictions": ["in a list of info in a for a given component . . . ."], "references": ["check whether a path is ignored . for directories include a trailing slash ."], "bleu": 0.1082597837309053, "rouge_l": 0.27758816837315126}
{"id": 3648, "code": "def matches patterns ( path , patterns ) : for glob in patterns : try : if Pure Path ( path ) . match ( glob ) : return True except Type Error : pass return False", "predictions": ["return true if path matches patterns ."], "references": ["given a list of patterns returns a if a path matches any pattern ."], "bleu": 0.1069482072978842, "rouge_l": 0.35935198821796754}
{"id": 3649, "code": "def ignore path ( cls , path , ignore list = None , white list = None ) : ignore list = ignore list or [ ] white list = white list or [ ] return ( cls . matches patterns ( path , ignore list ) and not cls . matches patterns ( path , white list ) )", "predictions": ["check if a path should be a list of white objects ."], "references": ["returns a whether a path should be ignored or not ."], "bleu": 0.31455601883230705, "rouge_l": 0.43821839080459773}
{"id": 3650, "code": "def group ( ctx , project , group ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'group' ] = group", "predictions": ["group was saved to a project ."], "references": ["commands for experiment groups ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3651, "code": "def config ( list ) : if list : config = Global Config Manager . get config or default ( ) Printer . print header ( 'Current config:' ) dict tabulate ( config . to dict ( ) )", "predictions": ["prints the configuration of the current configuration ."], "references": ["set and get the global configurations ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3652, "code": "def teardown ( file ) : config = read deployment config ( file ) manager = Deploy Manager ( config = config , filepath = file ) exception = None try : if click . confirm ( 'Would you like to execute pre-delete hooks?' , default = True ) : manager . teardown ( hooks = True ) else : manager . teardown ( hooks = False ) except Exception as e : Printer . print error ( 'Polyaxon could not teardown the deployment.' ) exception = e if exception : Printer . print error ( 'Error message `{}`.' . format ( exception ) )", "predictions": ["remove the current hooks ."], "references": ["teardown a polyaxon deployment given a config file ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 3653, "code": "def create tarfile ( files , project name ) : fd , filename = tempfile . mkstemp ( prefix = \"polyaxon {}\" . format ( project name ) , suffix = '.tar.gz' ) with tarfile . open ( filename , \"w:gz\" ) as tar : for f in files : tar . add ( f ) yield filename os . close ( fd ) os . remove ( filename )", "predictions": ["create a tar project on a project"], "references": ["create a tar file based on the list of files passed"], "bleu": 0.20643565894052812, "rouge_l": 0.4273204903677758}
{"id": 3654, "code": "def check cli version ( ) : if not Cli Config Manager . should check ( ) : return server version = get server version ( ) current version = get current version ( ) Cli Config Manager . reset ( current version = current version , min version = server version . min version ) if Loose Version ( current version ) < Loose Version ( server version . min version ) : click . echo ( \"\"\"Your version of CLI ({}) is no longer compatible with server.\"\"\" . format ( current version ) ) if click . confirm ( \"Do you want to upgrade to \" \"version {} now?\" . format ( server version . latest version ) ) : pip upgrade ( ) sys . exit ( 0 ) else : clint . textui . puts ( \"Your can manually run:\" ) with clint . textui . indent ( 4 ) : clint . textui . puts ( \"pip install -U polyaxon-cli\" ) clint . textui . puts ( \"to upgrade to the latest version `{}`\" . format ( server version . latest version ) ) sys . exit ( 0 ) elif Loose Version ( current version ) < Loose Version ( server version . latest version ) : clint . textui . puts ( \"New version of CLI ({}) is now available. To upgrade run:\" . format ( server version . latest version ) ) with clint . textui . indent ( 4 ) : clint . textui . puts ( \"pip install -U polyaxon-cli\" ) elif Loose Version ( current version ) > Loose Version ( server version . latest version ) : clint . textui . puts ( \"You version of CLI ({}) is ahead of the latest version \" \"supported by Polyaxon Platform ({}) on your cluster, \" \"and might be incompatible.\" . format ( current version , server version . latest version ) )", "predictions": ["check if the latest version of the latest version is compatible ."], "references": ["check if the current cli version satisfies the server requirements"], "bleu": 0.21401603033752975, "rouge_l": 0.46212121212121204}
{"id": 3655, "code": "def version ( cli , platform ) : version client = Polyaxon Client ( ) . version cli = cli or not any ( [ cli , platform ] ) if cli : try : server version = version client . get cli version ( ) except Authorization Error : session expired ( ) sys . exit ( 1 ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not get cli version.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) cli version = get version ( PROJECT CLI NAME ) Printer . print header ( 'Current cli version: {}.' . format ( cli version ) ) Printer . print header ( 'Supported cli versions:' ) dict tabulate ( server version . to dict ( ) ) if platform : try : platform version = version client . get platform version ( ) except Authorization Error : session expired ( ) sys . exit ( 1 ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not get platform version.' ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) chart version = version client . get chart version ( ) Printer . print header ( 'Current platform version: {}.' . format ( chart version . version ) ) Printer . print header ( 'Supported platform versions:' ) dict tabulate ( platform version . to dict ( ) )", "predictions": ["show the version of the neutron system ."], "references": ["print the current version of the cli and platform ."], "bleu": 0.2572506957482676, "rouge_l": 0.5446428571428571}
{"id": 3656, "code": "def dashboard ( yes , url ) : dashboard url = \"{}/app\" . format ( Polyaxon Client ( ) . api config . http host ) if url : click . echo ( dashboard url ) sys . exit ( 0 ) if not yes : click . confirm ( 'Dashboard page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( dashboard url )", "predictions": ["dashboard a dashboard ."], "references": ["open dashboard in browser ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 3657, "code": "def check ( self ) : if not self . is valid : raise Polyaxon Deployment Config Error ( 'Deployment type `{}` not supported' . format ( self . deployment type ) ) check = False if self . is kubernetes : check = self . check for kubernetes ( ) elif self . is docker compose : check = self . check for docker compose ( ) elif self . is docker : check = self . check for docker ( ) elif self . is heroku : check = self . check for heroku ( ) if not check : raise Polyaxon Deployment Config Error ( 'Deployment `{}` is not valid' . format ( self . deployment type ) )", "predictions": ["check all deployment docker docker docker docker docker docker docker docker docker docker deployment ."], "references": ["add platform specific checks"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3658, "code": "def install ( self ) : if not self . is valid : raise Polyaxon Deployment Config Error ( 'Deployment type `{}` not supported' . format ( self . deployment type ) ) if self . is kubernetes : self . install on kubernetes ( ) elif self . is docker compose : self . install on docker compose ( ) elif self . is docker : self . install on docker ( ) elif self . is heroku : self . install on heroku ( )", "predictions": ["install the deployment ."], "references": ["install polyaxon using the current config to the correct platform ."], "bleu": 0.07425134808660917, "rouge_l": 0.3689516129032258}
{"id": 3659, "code": "def project ( ctx , project ) : if ctx . invoked subcommand not in [ 'create' , 'list' ] : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project", "predictions": ["project project to project"], "references": ["commands for projects ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 3660, "code": "def download ( ctx ) : user , project name = get project or local ( ctx . obj . get ( 'project' ) ) try : Polyaxon Client ( ) . project . download repo ( user , project name ) except ( Polyaxon HTTP Error , Polyaxon Should Exit Error , Polyaxon Client Exception ) as e : Printer . print error ( 'Could not download code for project `{}`.' . format ( project name ) ) Printer . print error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print success ( 'Files downloaded.' )", "predictions": ["download code from github ."], "references": ["download code of the current project ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 3661, "code": "def write ( self , file , optstring = \"\" , quote = False ) : classid = str ( self . id ) if quote : classid = '\"' + classid + '\"' file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\\n' )", "predictions": ["write the specified file to a file in the file system ."], "references": ["write the object line ; additional args are packed in string"], "bleu": 0.14694106251955755, "rouge_l": 0.2629310344827586}
{"id": 3662, "code": "def value ( self , ascode = None ) : if ascode is None : ascode = self . code return self . cast [ ascode ] ( self . text )", "predictions": ["get the current value of the node ."], "references": ["return text cast to the correct type or the selected type"], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 3663, "code": "def use parser ( self , parsername ) : self . parser = self . parsers [ parsername ] self . parser ( )", "predictions": ["set up a parser ."], "references": ["set parsername as the current parser and apply it ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 3664, "code": "def read ( self , filename ) : from struct import calcsize , unpack if not filename is None : self . filename = filename with open ( self . filename , 'rb' ) as plt : h = self . header = self . read header ( plt ) nentries = h [ 'nx' ] * h [ 'ny' ] * h [ 'nz' ] datafmt = h [ 'bsaflag' ] + str ( nentries ) + self . data bintype a = numpy . array ( unpack ( datafmt , plt . read ( calcsize ( datafmt ) ) ) ) self . header [ 'filename' ] = self . filename self . array = a . reshape ( h [ 'nz' ] , h [ 'ny' ] , h [ 'nx' ] ) . transpose ( ) self . delta = self . delta ( ) self . origin = numpy . array ( [ h [ 'xmin' ] , h [ 'ymin' ] , h [ 'zmin' ] ] ) + 0.5 * numpy . diagonal ( self . delta ) self . rank = h [ 'rank' ]", "predictions": ["read the data from a file"], "references": ["populate the instance from the plt file * filename * ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 3665, "code": "def load cpp4 ( self , filename ) : ccp4 = CCP4 . CCP4 ( ) ccp4 . read ( filename ) grid , edges = ccp4 . histogramdd ( ) self . init ( grid = grid , edges = edges , metadata = self . metadata )", "predictions": ["load the cpp4 into a cpp4 instance"], "references": ["initializes grid from a ccp4 file ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3666, "code": "def load dx ( self , filename ) : dx = Open DX . field ( 0 ) dx . read ( filename ) grid , edges = dx . histogramdd ( ) self . init ( grid = grid , edges = edges , metadata = self . metadata )", "predictions": ["load the dx database into a dictionary"], "references": ["initializes grid from a opendx file ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3667, "code": "def load plt ( self , filename ) : g = g Open Mol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . init ( grid = grid , edges = edges , metadata = self . metadata )", "predictions": ["load the metadata from a file"], "references": ["initialize grid from gopenmol plt file ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3668, "code": "def read ( self , filename ) : if filename is not None : self . filename = filename with open ( self . filename , 'rb' ) as ccp4 : h = self . header = self . read header ( ccp4 ) nentries = h [ 'nc' ] * h [ 'nr' ] * h [ 'ns' ] datafmt = h [ 'bsaflag' ] + str ( nentries ) + self . data bintype a = np . array ( struct . unpack ( datafmt , ccp4 . read ( struct . calcsize ( datafmt ) ) ) ) self . header [ 'filename' ] = self . filename order = 'C' if h [ 'mapc' ] == 'z' else 'F' self . array = a . reshape ( h [ 'nc' ] , h [ 'nr' ] , h [ 'ns' ] , order = order ) self . delta = self . delta ( ) self . origin = np . zeros ( 3 ) self . rank = 3", "predictions": ["read the data from file"], "references": ["populate the instance from the ccp4 file * filename * ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 3669, "code": "async def rt connect ( self , loop ) : if self . sub manager is not None : return self . sub manager = Subscription Manager ( loop , \"token={}\" . format ( self . access token ) , SUB ENDPOINT ) self . sub manager . start ( )", "predictions": ["start the rt connection ."], "references": ["start subscription manager for real time data ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3670, "code": "def sync update info ( self , * ) : loop = asyncio . get event loop ( ) task = loop . create task ( self . update info ( ) ) loop . run until complete ( task )", "predictions": ["function run when running the asyncio process ."], "references": ["update home info ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3671, "code": "async def update info ( self , * ) : query = gql ( ) res = await self . execute ( query ) if res is None : return errors = res . get ( \"errors\" , [ ] ) if errors : msg = errors [ 0 ] . get ( \"message\" , \"failed to login\" ) LOGGER . error ( msg ) raise Invalid Login ( msg ) data = res . get ( \"data\" ) if not data : return viewer = data . get ( \"viewer\" ) if not viewer : return self . name = viewer . get ( \"name\" ) homes = viewer . get ( \"homes\" , [ ] ) self . home ids = [ ] for home in homes : home id = home . get ( \"id\" ) self . all home ids += [ home id ] subs = home . get ( \"subscriptions\" ) if subs : status = subs [ 0 ] . get ( \"status\" , \"ended\" ) . lower ( ) if not home id or status != \"running\" : continue self . home ids += [ home id ]", "predictions": ["get information of a specific home viewer ."], "references": ["update home info async ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 3672, "code": "def get homes ( self , only active = True ) : return [ self . get home ( home id ) for home id in self . get home ids ( only active ) ]", "predictions": ["return a list of all homes ids"], "references": ["return list of tibber homes ."], "bleu": 0.2777619034011791, "rouge_l": 0.6240409207161125}
{"id": 3673, "code": "def get home ( self , home id ) : if home id not in self . all home ids : LOGGER . error ( \"Could not find any Tibber home with id: %s\" , home id ) return None if home id not in self . homes . keys ( ) : self . homes [ home id ] = Tibber Home ( home id , self ) return self . homes [ home id ]", "predictions": ["get a single home home object"], "references": ["retun an instance of tibberhome for given home id ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 3674, "code": "async def update info ( self ) : query = gql ( % self . home id ) self . info = await self . tibber control . execute ( query )", "predictions": ["update information about the home home ."], "references": ["update current price info async ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3675, "code": "def sync update current price info ( self ) : loop = asyncio . get event loop ( ) task = loop . create task ( self . update current price info ( ) ) loop . run until complete ( task )", "predictions": ["take a local client and process it to the current job ."], "references": ["update current price info ."], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 3676, "code": "async def update current price info ( self ) : query = gql ( % self . home id ) price info temp = await self . tibber control . execute ( query ) if not price info temp : LOGGER . error ( \"Could not find current price info.\" ) return try : home = price info temp [ \"viewer\" ] [ \"home\" ] current subscription = home [ \"current Subscription\" ] price info = current subscription [ \"price Info\" ] [ \"current\" ] except ( Key Error , Type Error ) : LOGGER . error ( \"Could not find current price info.\" ) return if price info : self . current price info = price info", "predictions": ["update current price info ."], "references": ["update current price info async ."], "bleu": 0.6511126026643229, "rouge_l": 0.8944281524926685}
{"id": 3677, "code": "def sync update price info ( self ) : loop = asyncio . get event loop ( ) task = loop . create task ( self . update price info ( ) ) loop . run until complete ( task )", "predictions": ["process the update of the update price ."], "references": ["update current price info ."], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 3678, "code": "async def update price info ( self ) : query = gql ( % self . home id ) price info temp = await self . tibber control . execute ( query ) if not price info temp : LOGGER . error ( \"Could not find price info.\" ) return self . price info = { } self . level info = { } for key in [ \"current\" , \"today\" , \"tomorrow\" ] : try : home = price info temp [ \"viewer\" ] [ \"home\" ] current subscription = home [ \"current Subscription\" ] price info = current subscription [ \"price Info\" ] [ key ] except ( Key Error , Type Error ) : LOGGER . error ( \"Could not find price info for %s.\" , key ) continue if key == \"current\" : self . current price info = price info continue for data in price info : self . price info [ data . get ( \"starts At\" ) ] = data . get ( \"total\" ) self . level info [ data . get ( \"starts At\" ) ] = data . get ( \"level\" )", "predictions": ["get price information from database ."], "references": ["update price info async ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 3679, "code": "def currency ( self ) : try : current subscription = self . info [ \"viewer\" ] [ \"home\" ] [ \"current Subscription\" ] return current subscription [ \"price Info\" ] [ \"current\" ] [ \"currency\" ] except ( Key Error , Type Error , Index Error ) : LOGGER . error ( \"Could not find currency.\" ) return \"\"", "predictions": ["return the currency currency ."], "references": ["return the currency ."], "bleu": 0.5773502691896258, "rouge_l": 0.9070631970260222}
{"id": 3680, "code": "def price unit ( self ) : currency = self . currency consumption unit = self . consumption unit if not currency or not consumption unit : LOGGER . error ( \"Could not find price unit.\" ) return \" \" return currency + \"/\" + consumption unit", "predictions": ["except the matches of the matches"], "references": ["return the price unit ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3681, "code": "async def rt subscribe ( self , loop , async callback ) : if self . subscription id is not None : LOGGER . error ( \"Already subscribed.\" ) return await self . tibber control . rt connect ( loop ) document = gql ( % self . home id ) sub query = print ast ( document ) self . subscription id = await self . tibber control . sub manager . subscribe ( sub query , async callback )", "predictions": ["called when a document connection has been received ."], "references": ["connect to tibber and subscribe to tibber rt subscription ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 3682, "code": "async def rt unsubscribe ( self ) : if self . subscription id is None : LOGGER . error ( \"Not subscribed.\" ) return await self . tibber control . sub manager . unsubscribe ( self . subscription id )", "predictions": ["ctx rt rt obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj"], "references": ["unsubscribe to tibber rt subscription ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3683, "code": "def rt subscription running ( self ) : return ( self . tibber control . sub manager is not None and self . tibber control . sub manager . is running and self . subscription id is not None )", "predictions": ["return of config subscription . . . . ."], "references": ["is real time subscription running ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 3684, "code": "async def get historic data ( self , n data ) : query = gql ( % ( self . home id , n data ) ) data = await self . tibber control . execute ( query ) if not data : LOGGER . error ( \"Could not find current the data.\" ) return data = data [ \"viewer\" ] [ \"home\" ] [ \"consumption\" ] if data is None : self . data = [ ] return self . data = data [ \"nodes\" ]", "predictions": ["get data data data exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception exception"], "references": ["get historic data ."], "bleu": 0.04317900023606586, "rouge_l": 0.13275299238302501}
{"id": 3685, "code": "def cleanup none ( self ) : for ( prop , default ) in self . defaults . items ( ) : if getattr ( self , prop ) == ' None' : setattr ( self , prop , None )", "predictions": ["create a dictionary of attributes with fd attributes = value pairs = fd"], "references": ["removes the temporary value set for none attributes ."], "bleu": 0.10571070857151538, "rouge_l": 0.09399075500770414}
{"id": 3686, "code": "def build environ ( self , sock file , conn ) : request = self . read request line ( sock file ) environ = self . base environ . copy ( ) for k , v in self . read headers ( sock file ) . items ( ) : environ [ str ( 'HTTP ' + k ) ] = v environ [ 'REQUEST METHOD' ] = request [ 'method' ] environ [ 'PATH INFO' ] = request [ 'path' ] environ [ 'SERVER PROTOCOL' ] = request [ 'protocol' ] environ [ 'SERVER PORT' ] = str ( conn . server port ) environ [ 'REMOTE PORT' ] = str ( conn . client port ) environ [ 'REMOTE ADDR' ] = str ( conn . client addr ) environ [ 'QUERY STRING' ] = request [ 'query string' ] if 'HTTP CONTENT LENGTH' in environ : environ [ 'CONTENT LENGTH' ] = environ [ 'HTTP CONTENT LENGTH' ] if 'HTTP CONTENT TYPE' in environ : environ [ 'CONTENT TYPE' ] = environ [ 'HTTP CONTENT TYPE' ] self . request method = environ [ 'REQUEST METHOD' ] if conn . ssl : environ [ 'wsgi.url scheme' ] = 'https' environ [ 'HTTPS' ] = 'on' else : environ [ 'wsgi.url scheme' ] = 'http' if environ . get ( 'HTTP TRANSFER ENCODING' , '' ) == 'chunked' : environ [ 'wsgi.input' ] = Chunked Reader ( sock file ) else : environ [ 'wsgi.input' ] = sock file return environ", "predictions": ["check wsgi wsgi wsgi wsgi wsgi wsgi wsgi wsgi wsgi wsgi wsgi wsgi application"], "references": ["build the execution environment ."], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 3687, "code": "def write ( self , data , sections = None ) : if self . error [ 0 ] : self . status = self . error [ 0 ] data = b ( self . error [ 1 ] ) if not self . headers sent : self . send headers ( data , sections ) if self . request method != 'HEAD' : try : if self . chunked : self . conn . sendall ( b ( '%x\\r\\n%s\\r\\n' % ( len ( data ) , data ) ) ) else : self . conn . sendall ( data ) except socket . timeout : self . close Connection = True except socket . error : self . close Connection = True", "predictions": ["version of sending this request ."], "references": ["write the data to the output socket ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3688, "code": "def Cherry Py WSGI Server ( bind addr , wsgi app , numthreads = 10 , server name = None , max = - 1 , request queue size = 5 , timeout = 10 , shutdown timeout = 5 ) : max threads = max if max threads < 0 : max threads = 0 return Rocket ( bind addr , 'wsgi' , { 'wsgi app' : wsgi app } , min threads = numthreads , max threads = max threads , queue size = request queue size , timeout = timeout )", "predictions": ["send a dashboard to gcs gcs ."], "references": ["a cherrypy wsgiserver - compatible wrapper ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3689, "code": "def ordinal metric ( v1 , v2 , i1 , i2 , n v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n v [ i1 : ( i2 + 1 ) ] ) - ( n v [ i1 ] + n v [ i2 ] ) / 2 ) ** 2", "predictions": ["return metric metric metric algorithm"], "references": ["metric for ordinal data ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3690, "code": "def ratio metric ( v1 , v2 , * * kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0", "predictions": ["get the install metric metric ."], "references": ["metric for ratio data ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 3691, "code": "def process return multi z ( self , data , names , dim sizes ) : d1 = 0 d2 = 0 for name , dim size in zip ( names , dim sizes ) : d2 = d1 + dim size if dim size == 1 : self . data [ name . rstrip ( ) ] = data [ d1 , : ] else : self . data [ name . rstrip ( ) ] = data [ d1 : d2 , : ] d1 += dim size", "predictions": ["project . . . return but for each dimension . . . . . ."], "references": ["process and attach data from fortran_cdf . get_multi_ *"], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 3692, "code": "def read all attribute info ( self ) : num = copy . deepcopy ( self . num attrs ) fname = copy . deepcopy ( self . fname ) out = fortran cdf . inquire all attr ( fname , num , len ( fname ) ) status = out [ 0 ] names = out [ 1 ] . astype ( 'U' ) scopes = out [ 2 ] max gentries = out [ 3 ] max rentries = out [ 4 ] max zentries = out [ 5 ] attr nums = out [ 6 ] global attrs info = { } var attrs info = { } if status == 0 : for name , scope , gentry , rentry , zentry , num in zip ( names , scopes , max gentries , max rentries , max zentries , attr nums ) : name = '' . join ( name ) name = name . rstrip ( ) nug = { } nug [ 'scope' ] = scope nug [ 'max gentry' ] = gentry nug [ 'max rentry' ] = rentry nug [ 'max zentry' ] = zentry nug [ 'attr num' ] = num flag = ( gentry == 0 ) & ( rentry == 0 ) & ( zentry == 0 ) if not flag : if scope == 1 : global attrs info [ name ] = nug elif scope == 2 : var attrs info [ name ] = nug self . global attrs info = global attrs info self . var attrs info = var attrs info else : raise IO Error ( fortran cdf . statusreporter ( status ) )", "predictions": ["reads all and builds all"], "references": ["read all attribute properties g r and z attributes"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3693, "code": "def process return multi z attr ( self , data , attr names , var names , sub num elems ) : for i , ( attr name , var name , num e ) in enumerate ( zip ( attr names , var names , sub num elems ) ) : if var name not in self . meta . keys ( ) : self . meta [ var name ] = { } if num e == 1 : self . meta [ var name ] [ attr name ] = data [ i , 0 ] else : if data [ i ] . dtype == '|S1' : self . meta [ var name ] [ attr name ] = '' . join ( data [ i , 0 : num e ] . astype ( 'U' ) ) . rstrip ( ) else : self . meta [ var name ] [ attr name ] = data [ i , 0 : num e ]", "predictions": ["write quote information . back to a dict ."], "references": ["process and attach data from fortran_cdf . get_multi_ *"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3694, "code": "def uptime linux ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . readline ( ) . split ( ) [ 0 ] ) f . close ( ) return up except ( IO Error , Value Error ) : pass try : libc = ctypes . CDLL ( 'libc.so' ) except Attribute Error : return None except OS Error : try : libc = ctypes . CDLL ( 'libc.so.6' ) except OS Error : return None if not hasattr ( libc , 'sysinfo' ) : return None buf = ctypes . create string buffer ( 128 ) if libc . sysinfo ( buf ) < 0 : return None up = struct . unpack from ( '@l' , buf . raw ) [ 0 ] if up < 0 : up = None return up", "predictions": ["return the value of the value in linux cast to a linux cast it to a linux cast cast"], "references": ["returns uptime in seconds or none on linux ."], "bleu": 0.0712695567709093, "rouge_l": 0.15269086357947434}
{"id": 3695, "code": "def boottime linux ( ) : global boottime try : f = open ( '/proc/stat' , 'r' ) for line in f : if line . startswith ( 'btime' ) : boottime = int ( line . split ( ) [ 1 ] ) if datetime is None : raise Not Implemented Error ( 'datetime module required.' ) return datetime . fromtimestamp ( boottime ) except ( IO Error , Index Error ) : return None", "predictions": ["return the parser for a parser"], "references": ["a way to figure out the boot time directly on linux ."], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 3696, "code": "def uptime amiga ( ) : global boottime try : boottime = os . stat ( 'RAM:' ) . st ctime return time . time ( ) - boottime except ( Name Error , OS Error ) : return None", "predictions": ["return read the current read ."], "references": ["returns uptime in seconds or none on amigaos ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 3697, "code": "def uptime minix ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . read ( ) ) f . close ( ) return up except ( IO Error , Value Error ) : return None", "predictions": ["metadata to get the load from the load file"], "references": ["returns uptime in seconds or none on minix ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3698, "code": "def uptime plan9 ( ) : try : f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IO Error , Value Error ) : return None", "predictions": ["return load load load load load load load load data from local file"], "references": ["returns uptime in seconds or none on plan 9 ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 3699, "code": "def uptime solaris ( ) : global boottime try : kstat = ctypes . CDLL ( 'libkstat.so' ) except ( Attribute Error , OS Error ) : return None KSTAT STRLEN = 31 class anon union ( ctypes . Union ) : fields = [ ( 'c' , ctypes . c char * 16 ) , ( 'time' , ctypes . c int ) ] class kstat named t ( ctypes . Structure ) : fields = [ ( 'name' , ctypes . c char * KSTAT STRLEN ) , ( 'data type' , ctypes . c char ) , ( 'value' , anon union ) ] kstat . kstat open . restype = ctypes . c void p kstat . kstat lookup . restype = ctypes . c void p kstat . kstat lookup . argtypes = [ ctypes . c void p , ctypes . c char p , ctypes . c int , ctypes . c char p ] kstat . kstat read . restype = ctypes . c int kstat . kstat read . argtypes = [ ctypes . c void p , ctypes . c void p , ctypes . c void p ] kstat . kstat data lookup . restype = ctypes . POINTER ( kstat named t ) kstat . kstat data lookup . argtypes = [ ctypes . c void p , ctypes . c char p ] kc = kstat . kstat open ( ) if not kc : return None ksp = kstat . kstat lookup ( kc , 'unix' , 0 , 'system misc' ) if ksp and kstat . kstat read ( kc , ksp , None ) != - 1 : data = kstat . kstat data lookup ( ksp , 'boot time' ) if data : boottime = data . contents . value . time kstat . kstat close ( kc ) if boottime is not None : return time . time ( ) - boottime return None", "predictions": ["load load load load load load and return a dictionary with the load time . . . . . . . . . . . . . . . . ."], "references": ["returns uptime in seconds or none on solaris ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 3700, "code": "def uptime syllable ( ) : global boottime try : boottime = os . stat ( '/dev/pty/mst/pty0' ) . st mtime return time . time ( ) - boottime except ( Name Error , OS Error ) : return None", "predictions": ["return the read syllable"], "references": ["returns uptime in seconds or none on syllable ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3701, "code": "def uptime ( ) : if boottime is not None : return time . time ( ) - boottime return { 'amiga' : uptime amiga , 'aros12' : uptime amiga , 'beos5' : uptime beos , 'cygwin' : uptime linux , 'darwin' : uptime osx , 'haiku1' : uptime beos , 'linux' : uptime linux , 'linux-armv71' : uptime linux , 'linux2' : uptime linux , 'mac' : uptime mac , 'minix3' : uptime minix , 'riscos' : uptime riscos , 'sunos5' : uptime solaris , 'syllable' : uptime syllable , 'win32' : uptime windows , 'wince' : uptime windows } . get ( sys . platform , uptime bsd ) ( ) or uptime bsd ( ) or uptime plan9 ( ) or uptime linux ( ) or uptime windows ( ) or uptime solaris ( ) or uptime beos ( ) or uptime amiga ( ) or uptime riscos ( ) or uptime posix ( ) or uptime syllable ( ) or uptime mac ( ) or uptime osx ( )", "predictions": ["def of the def def sub - def ."], "references": ["returns uptime in seconds if even remotely possible or none if not ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 3702, "code": "def boottime ( ) : global boottime if boottime is None : up = uptime ( ) if up is None : return None if boottime is None : boottime linux ( ) if datetime is None : raise Runtime Error ( 'datetime module required.' ) return datetime . fromtimestamp ( boottime or time . time ( ) - up )", "predictions": ["asyncio for time between two time"], "references": ["returns boot time if remotely possible or none if not ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 3703, "code": "def initfile ( path , data = \"dict\" ) : data = { } if data . lower ( ) == \"dict\" else [ ] if not os . path . exists ( path ) : dirname = os . path . dirname ( path ) if dirname and not os . path . exists ( dirname ) : raise IO Error ( ( \"Could not initialize empty JSON file in non-existant \" \"directory '{}'\" ) . format ( os . path . dirname ( path ) ) ) with open ( path , \"w\" ) as f : json . dump ( data , f ) return True elif os . path . getsize ( path ) == 0 : with open ( path , \"w\" ) as f : json . dump ( data , f ) else : return False", "predictions": ["create json in json"], "references": ["initialize an empty json file ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3704, "code": "def is configured ( self , project , * * kwargs ) : params = self . get option return bool ( params ( 'server host' , project ) and params ( 'server port' , project ) )", "predictions": ["check if the given only the only only homes . ."], "references": ["check if plugin is configured ."], "bleu": 0.16108992769687397, "rouge_l": 0.3727087576374745}
{"id": 3705, "code": "def send confirmation ( self ) : confirmation = Email Confirmation . objects . create ( email = self ) confirmation . send ( )", "predictions": ["sends the home all home home home all home home home not logged not home"], "references": ["send a verification email for the email address ."], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 3706, "code": "def send duplicate notification ( self ) : email utils . send email ( from email = settings . DEFAULT FROM EMAIL , recipient list = [ self . email ] , subject = ( \"Registration Attempt\" ) , template name = \"rest email auth/emails/duplicate-email\" , ) logger . info ( \"Sent duplicate email notification to: %s\" , self . email )", "predictions": ["sends the update info to all the update info gql"], "references": ["send a notification about a duplicate signup ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3707, "code": "def set primary ( self ) : query = Email Address . objects . filter ( is primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) with transaction . atomic ( ) : query . update ( is primary = False ) self . is primary = True self . save ( ) logger . info ( \"Set %s as the primary email address for %s.\" , self . email , self . user , )", "predictions": ["mark the update as update address address"], "references": ["set this email address as the user s primary email ."], "bleu": 0.1247439242120089, "rouge_l": 0.10683012259194395}
{"id": 3708, "code": "def confirm ( self ) : self . email . is verified = True self . email . save ( ) signals . email verified . send ( email = self . email , sender = self . class ) logger . info ( \"Verified email address: %s\" , self . email . email )", "predictions": ["tell the self . to the self . ."], "references": ["mark the instance s email as verified ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 3709, "code": "def send ( self ) : context = { \"verification url\" : app settings . EMAIL VERIFICATION URL . format ( key = self . key ) } email utils . send email ( context = context , from email = settings . DEFAULT FROM EMAIL , recipient list = [ self . email . email ] , subject = ( \"Please Verify Your Email Address\" ) , template name = \"rest email auth/emails/verify-email\" , ) logger . info ( \"Sent confirmation email to %s for user #%d\" , self . email . email , self . email . user . id , )", "predictions": ["sends the email email to all email get email get email get email get email get email get email get email get email get email get email get email get email"], "references": ["send a verification email to the user ."], "bleu": 0.055177848898164926, "rouge_l": 0.1147695202257761}
{"id": 3710, "code": "def save ( self ) : token = models . Password Reset Token . objects . get ( key = self . validated data [ \"key\" ] ) token . email . user . set password ( self . validated data [ \"password\" ] ) token . email . user . save ( ) logger . info ( \"Reset password for %s\" , token . email . user ) token . delete ( )", "predictions": ["def the user s if not already fetched query query query query query query query query query query query query query query query query query query query query query query query"], "references": ["reset the user s password if the provided information is valid ."], "bleu": 0.07678432706586173, "rouge_l": 0.20215410107705056}
{"id": 3711, "code": "def create ( self , * args , * * kwargs ) : is primary = kwargs . pop ( \"is primary\" , False ) with transaction . atomic ( ) : email = super ( Email Address Manager , self ) . create ( * args , * * kwargs ) if is primary : email . set primary ( ) return email", "predictions": ["custom currency to ensure info is info about info . . . . . ."], "references": ["create a new email address ."], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 3712, "code": "def get queryset ( self ) : oldest = timezone . now ( ) - app settings . PASSWORD RESET EXPIRATION queryset = super ( Valid Password Reset Token Manager , self ) . get queryset ( ) return queryset . filter ( created at gt = oldest )", "predictions": ["returns the list of items in the current timezone ."], "references": ["return all unexpired password reset tokens ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 3713, "code": "def handle ( self , * args , * * kwargs ) : cutoff = timezone . now ( ) cutoff -= app settings . CONFIRMATION EXPIRATION cutoff -= app settings . CONFIRMATION SAVE PERIOD queryset = models . Email Confirmation . objects . filter ( created at lte = cutoff ) count = queryset . count ( ) queryset . delete ( ) if count : self . stdout . write ( self . style . SUCCESS ( \"Removed {count} old email confirmation(s)\" . format ( count = count ) ) ) else : self . stdout . write ( \"No email confirmations to remove.\" )", "predictions": ["gets the email address ."], "references": ["handle execution of the command ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3714, "code": "def get repr ( self , obj , referent = None ) : objtype = type ( obj ) typename = str ( objtype . module ) + \".\" + objtype . name prettytype = typename . replace ( \" builtin .\" , \"\" ) name = getattr ( obj , \" name \" , \"\" ) if name : prettytype = \"%s %r\" % ( prettytype , name ) key = \"\" if referent : key = self . get refkey ( obj , referent ) url = reverse ( 'dowser trace object' , args = ( typename , id ( obj ) ) ) return ( '<a class=\"objectid\" href=\"%s\">%s</a> ' '<span class=\"typename\">%s</span>%s<br />' '<span class=\"repr\">%s</span>' % ( url , id ( obj ) , prettytype , key , get repr ( obj , 100 ) ) )", "predictions": ["return the repr of the given object ."], "references": ["return an html tree block describing the given object ."], "bleu": 0.3638074228571147, "rouge_l": 0.5446428571428571}
{"id": 3715, "code": "def walk ( self , maxresults = 100 , maxdepth = None ) : log . debug ( \"step\" ) self . seen = { } self . ignore ( self , self . dict , self . obj , self . seen , self . ignore ) self . ignore caller ( ) self . maxdepth = maxdepth count = 0 log . debug ( \"will iterate results\" ) for result in self . gen ( self . obj ) : log . debug ( \"will yeld\" ) yield result count += 1 if maxresults and count >= maxresults : yield 0 , 0 , \"==== Max results reached ====\" return", "predictions": ["walk a sequence of maxresults objects and walk walk each result ."], "references": ["walk the object tree ignoring duplicates and circular refs ."], "bleu": 0.1235622127262679, "rouge_l": 0.2772727272727273}
{"id": 3716, "code": "def print tree ( self , maxresults = 100 , maxdepth = None ) : self . ignore caller ( ) for depth , refid , rep in self . walk ( maxresults , maxdepth ) : print ( ( \"%9d\" % refid ) , ( \" \" * depth * 2 ) , rep )", "predictions": ["print a tree of information about the packet ."], "references": ["walk the object tree pretty - printing each branch ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 3717, "code": "def print tree ( self , maxresults = 100 , maxdepth = None ) : self . ignore caller ( ) for trail in self . walk ( maxresults , maxdepth ) : print ( trail ) if self . stops : print ( \"%s paths stopped because max depth reached\" % self . stops )", "predictions": ["print a tree of stops and stops the result ."], "references": ["walk the object tree pretty - printing each branch ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 3718, "code": "def list ( self , ignore patterns ) : for prefix , root in self . locations : storage = self . storages [ root ] for path in utils . get files ( storage , ignore patterns ) : yield path , storage", "predictions": ["return an iterator over all files ."], "references": ["list all files in all locations ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 3719, "code": "def list ( self , ignore patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : for path in utils . get files ( storage , ignore patterns ) : yield path , storage", "predictions": ["return a list of all files under the server ."], "references": ["list all files in all app storages ."], "bleu": 0.18850319022747347, "rouge_l": 0.4535315985130111}
{"id": 3720, "code": "def find ( self , path , all = False ) : matches = [ ] for app in self . apps : app location = self . storages [ app ] . location if app location not in searched locations : searched locations . append ( app location ) match = self . find in app ( app , path ) if match : if not all : return match matches . append ( match ) return matches", "predictions": ["find the first app in the specified path ."], "references": ["looks for files in the app directories ."], "bleu": 0.21105340631872635, "rouge_l": 0.35672514619883033}
{"id": 3721, "code": "def find in app ( self , app , path ) : storage = self . storages . get ( app , None ) if storage : if storage . exists ( path ) : matched path = storage . path ( path ) if matched path : return matched path", "predictions": ["find the path to the app in the app ."], "references": ["find a requested media file in an app s media fixtures locations ."], "bleu": 0.11742832364135733, "rouge_l": 0.33983286908078}
{"id": 3722, "code": "def set options ( self , * * options ) : self . interactive = options [ 'interactive' ] self . verbosity = options [ 'verbosity' ] self . symlink = options [ 'link' ] self . clear = options [ 'clear' ] self . dry run = options [ 'dry run' ] ignore patterns = options [ 'ignore patterns' ] if options [ 'use default ignore patterns' ] : ignore patterns += [ 'CVS' , '.*' , '*~' ] self . ignore patterns = list ( set ( ignore patterns ) ) self . post process = options [ 'post process' ]", "predictions": ["set the options used to set the interactive options ."], "references": ["set instance variables based on an options dict"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3723, "code": "def clear dir ( self , path ) : dirs , files = self . storage . listdir ( path ) for f in files : fpath = os . path . join ( path , f ) if self . dry run : self . log ( \"Pretending to delete '%s'\" % smart text ( fpath ) , level = 1 ) else : self . log ( \"Deleting '%s'\" % smart text ( fpath ) , level = 1 ) self . storage . delete ( fpath ) for d in dirs : self . clear dir ( os . path . join ( path , d ) )", "predictions": ["clear a directory ."], "references": ["deletes the given relative path using the destination storage backend ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 3724, "code": "def delete file ( self , path , prefixed path , source storage ) : if self . storage . exists ( prefixed path ) : try : target last modified = self . storage . modified time ( prefixed path ) except ( OS Error , Not Implemented Error , Attribute Error ) : pass else : try : source last modified = source storage . modified time ( path ) except ( OS Error , Not Implemented Error , Attribute Error ) : pass else : if self . local : full path = self . storage . path ( prefixed path ) else : full path = None if ( target last modified . replace ( microsecond = 0 ) >= source last modified . replace ( microsecond = 0 ) ) : if not ( ( self . symlink and full path and not os . path . islink ( full path ) ) or ( not self . symlink and full path and os . path . islink ( full path ) ) ) : if prefixed path not in self . unmodified files : self . unmodified files . append ( prefixed path ) self . log ( \"Skipping '%s' (not modified)\" % path ) return False if self . dry run : self . log ( \"Pretending to delete '%s'\" % path ) else : self . log ( \"Deleting '%s'\" % path ) self . storage . delete ( prefixed path ) return True", "predictions": ["deletes a file from the given source"], "references": ["checks if the target file should be deleted if it already exists"], "bleu": 0.10063351655856649, "rouge_l": 0.10049423393739704}
{"id": 3725, "code": "def link file ( self , path , prefixed path , source storage ) : if prefixed path in self . symlinked files : return self . log ( \"Skipping '%s' (already linked earlier)\" % path ) if not self . delete file ( path , prefixed path , source storage ) : return source path = source storage . path ( path ) if self . dry run : self . log ( \"Pretending to link '%s'\" % source path , level = 1 ) else : self . log ( \"Linking '%s'\" % source path , level = 1 ) full path = self . storage . path ( prefixed path ) try : os . makedirs ( os . path . dirname ( full path ) ) except OS Error : pass try : if os . path . lexists ( full path ) : os . unlink ( full path ) os . symlink ( source path , full path ) except Attribute Error : import platform raise Command Error ( \"Symlinking is not supported by Python %s.\" % platform . python version ( ) ) except Not Implemented Error : import platform raise Command Error ( \"Symlinking is not supported in this \" \"platform (%s).\" % platform . platform ( ) ) except OS Error as e : raise Command Error ( e ) if prefixed path not in self . symlinked files : self . symlinked files . append ( prefixed path )", "predictions": ["link a file to the server"], "references": ["attempt to link path"], "bleu": 0.24446151121745047, "rouge_l": 0.2074829931972789}
{"id": 3726, "code": "def copy file ( self , path , prefixed path , source storage ) : if prefixed path in self . copied files : return self . log ( \"Skipping '%s' (already copied earlier)\" % path ) if not self . delete file ( path , prefixed path , source storage ) : return source path = source storage . path ( path ) if self . dry run : self . log ( \"Pretending to copy '%s'\" % source path , level = 1 ) else : self . log ( \"Copying '%s'\" % source path , level = 1 ) with source storage . open ( path ) as source file : self . storage . save ( prefixed path , source file ) self . copied files . append ( prefixed path )", "predictions": ["copy a file from the source to the specified source"], "references": ["attempt to copy path with storage"], "bleu": 0.13950796967929133, "rouge_l": 0.13090128755364808}
{"id": 3727, "code": "def baseattrs ( self ) : result = super ( ) . baseattrs result [ \"spaces\" ] = self . spaces . baseattrs return result", "predictions": ["the list of the current data ."], "references": ["a dict of members expressed in literals"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3728, "code": "def restore state ( self , system ) : for space in self . spaces . values ( ) : space . restore state ( system )", "predictions": ["restore all spaces ."], "references": ["called after unpickling to restore some attributes manually ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 3729, "code": "def get node ( obj , args , kwargs ) : if args is None and kwargs is None : return ( obj , ) if kwargs is None : kwargs = { } return obj , bind args ( obj , args , kwargs )", "predictions": ["get an node s node ."], "references": ["create a node from arguments and return it"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3730, "code": "def node get args ( node ) : obj = node [ OBJ ] key = node [ KEY ] boundargs = obj . formula . signature . bind ( * key ) boundargs . apply defaults ( ) return boundargs . arguments", "predictions": ["get the arguments for a node ."], "references": ["return an ordered mapping from params to args"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 3731, "code": "def get object ( name : str ) : elms = name . split ( \".\" ) parent = get models ( ) [ elms . pop ( 0 ) ] while len ( elms ) > 0 : obj = elms . pop ( 0 ) parent = getattr ( parent , obj ) return parent", "predictions": ["get a specific object by name"], "references": ["get a modelx object from its full name ."], "bleu": 0.20034704329441452, "rouge_l": 0.5147679324894514}
{"id": 3732, "code": "def restore ipython ( self ) : if not self . is ipysetup : return shell class = type ( self . shell ) shell class . showtraceback = shell class . default showtraceback del shell class . default showtraceback self . is ipysetup = False", "predictions": ["restore the shell shell ."], "references": ["restore default ipython showtraceback"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3733, "code": "def restore python ( self ) : orig = self . orig settings sys . setrecursionlimit ( orig [ \"sys.recursionlimit\" ] ) if \"sys.tracebacklimit\" in orig : sys . tracebacklimit = orig [ \"sys.tracebacklimit\" ] else : if hasattr ( sys , \"tracebacklimit\" ) : del sys . tracebacklimit if \"showwarning\" in orig : warnings . showwarning = orig [ \"showwarning\" ] orig . clear ( ) threading . stack size ( )", "predictions": ["restore python python from context"], "references": ["restore python settings to the original states"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 3734, "code": "def get object ( self , name ) : parts = name . split ( \".\" ) model name = parts . pop ( 0 ) return self . models [ model name ] . get object ( \".\" . join ( parts ) )", "predictions": ["get an object by name"], "references": ["retrieve an object by its absolute name ."], "bleu": 0.2948682411907622, "rouge_l": 0.5907990314769976}
{"id": 3735, "code": "def get interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , Order Mixin ) : result = Ordered Dict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface", "predictions": ["return a dictionary of interfaces from an impls"], "references": ["get interfaces from their implementations ."], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 3736, "code": "def get impls ( interfaces ) : if interfaces is None : return None elif isinstance ( interfaces , Mapping ) : return { name : interfaces [ name ] . impl for name in interfaces } elif isinstance ( interfaces , Sequence ) : return [ interfaces . impl for interfaces in interfaces ] else : return interfaces . impl", "predictions": ["converts an interfaces instance to a list of impls"], "references": ["get impls from their interfaces ."], "bleu": 0.15619699684601276, "rouge_l": 0.13832199546485258}
{"id": 3737, "code": "def baseattrs ( self ) : result = { \"type\" : type ( self ) . name , \"id\" : id ( self ) , \"name\" : self . name , \"fullname\" : self . fullname , \"repr\" : self . get repr ( ) , } return result", "predictions": ["return a dictionary of the object ."], "references": ["a dict of members expressed in literals"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3738, "code": "def baseattrs ( self ) : result = { \"type\" : type ( self ) . name } try : result [ \"items\" ] = { name : item . baseattrs for name , item in self . items ( ) if name [ 0 ] != \" \" } except : raise Runtime Error ( \"%s literadict raised an error\" % self ) return result", "predictions": ["return a dictionary with the names and values ."], "references": ["a dict of members expressed in literals"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3739, "code": "def convert args ( args , kwargs ) : found = False for arg in args : if isinstance ( arg , Cells ) : found = True break if found : args = tuple ( arg . value if isinstance ( arg , Cells ) else arg for arg in args ) if kwargs is not None : for key , arg in kwargs . items ( ) : if isinstance ( arg , Cells ) : kwargs [ key ] = arg . value return args , kwargs", "predictions": ["convert arguments to kwargs ."], "references": ["if args and kwargs contains cells convert them to their values ."], "bleu": 0.08465538735236156, "rouge_l": 0.3285457809694794}
{"id": 3740, "code": "def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )", "predictions": ["create a new copy of the container ."], "references": ["make a copy of itself and return it ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 3741, "code": "def baseattrs ( self ) : result = super ( ) . baseattrs result [ \"params\" ] = \", \" . join ( self . parameters ) return result", "predictions": ["return the string representation of the receiver ."], "references": ["a dict of members expressed in literals"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3742, "code": "def value ( self ) : if self . has value : return self . impl [ OBJ ] . get value ( self . impl [ KEY ] ) else : raise Value Error ( \"Value not found\" )", "predictions": ["returns the value of the lock ."], "references": ["return the value of the cells ."], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 3743, "code": "def baseattrs ( self ) : result = { \"type\" : type ( self ) . name , \"obj\" : self . cells . baseattrs , \"args\" : self . args , \"value\" : self . value if self . has value else None , \"predslen\" : len ( self . preds ) , \"succslen\" : len ( self . succs ) , \"repr parent\" : self . cells . impl . repr parent ( ) , \"repr\" : self . cells . get repr ( ) , } return result", "predictions": ["return a dictionary of cells ."], "references": ["a dict of members expressed in literals"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3744, "code": "def move elements ( source , index to , index from , length ) : sublist = [ source . pop ( index from ) for in range ( length ) ] for in range ( length ) : source . insert ( index to , sublist . pop ( ) )", "predictions": ["get queryset queryset ."], "references": ["move a sub sequence in a list"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 3745, "code": "def get col index ( name ) : index = string . ascii uppercase . index col = 0 for c in name . upper ( ) : col = col * 26 + index ( c ) + 1 return col", "predictions": ["return the column self cutoff with the given * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["convert column name to index ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3746, "code": "def get range ( book , range , sheet ) : filename = None if isinstance ( book , str ) : filename = book book = opxl . load workbook ( book , data only = True ) elif isinstance ( book , opxl . Workbook ) : pass else : raise Type Error if is range address ( range ) : sheet names = [ name . upper ( ) for name in book . sheetnames ] index = sheet names . index ( sheet . upper ( ) ) data = book . worksheets [ index ] [ range ] else : data = get namedrange ( book , range , sheet ) if data is None : raise Value Error ( \"Named range '%s' not found in %s\" % ( range , filename or book ) ) return data", "predictions": ["find the repr repr repr repr of a obj ."], "references": ["return a range as nested dict of openpyxl cells ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 3747, "code": "def find funcdef ( source ) : try : module node = compile ( source , \"<string>\" , mode = \"exec\" , flags = ast . Py CF ONLY AST ) except Syntax Error : return find funcdef ( fix lamdaline ( source ) ) for node in ast . walk ( module node ) : if isinstance ( node , ast . Function Def ) or isinstance ( node , ast . Lambda ) : return node raise Value Error ( \"function definition not found\" )", "predictions": ["walk a source and find the first argument in a source"], "references": ["find the first funcdef ast object in source"], "bleu": 0.23462350320527994, "rouge_l": 0.5417406749555951}
{"id": 3748, "code": "def extract params ( source ) : funcdef = find funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params", "predictions": ["print all tree s tree s tree for a given self for a given self for a given self for a given self for a given self for a given self"], "references": ["extract parameters from a function definition"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3749, "code": "def is funcdef ( src ) : module node = ast . parse ( dedent ( src ) ) if len ( module node . body ) == 1 and isinstance ( module node . body [ 0 ] , ast . Function Def ) : return True else : return False", "predictions": ["depth of a = color 100 . . ."], "references": ["true if src is a function definition"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3750, "code": "def remove decorator ( source : str ) : lines = source . splitlines ( ) atok = asttokens . AST Tokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . Function Def ) : break if node . decorator list : deco first = node . decorator list [ 0 ] deco last = node . decorator list [ - 1 ] line first = atok . tokens [ deco first . first token . index - 1 ] . start [ 0 ] line last = atok . tokens [ deco last . last token . index + 1 ] . start [ 0 ] lines = lines [ : line first - 1 ] + lines [ line last : ] return \"\\n\" . join ( lines ) + \"\\n\"", "predictions": ["list all the decorator in a source"], "references": ["remove decorators from function definition"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3751, "code": "def has lambda ( src ) : module node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )", "predictions": ["check if an storage is a lambda in the lambda in the storage in the storage in the storage in the storage in the node in the node in the storage"], "references": ["true if only one lambda expression is included"], "bleu": 0.046398855339878003, "rouge_l": 0.1147695202257761}
{"id": 3752, "code": "def get description ( ) : with open ( path . join ( here , 'README.rst' ) , 'r' ) as f : data = f . read ( ) return data", "predictions": ["find the description description description ."], "references": ["get long description from readme ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 3753, "code": "def baseattrs ( self ) : result = super ( ) . baseattrs result [ \"static spaces\" ] = self . static spaces . baseattrs result [ \"dynamic spaces\" ] = self . dynamic spaces . baseattrs result [ \"cells\" ] = self . cells . baseattrs result [ \"refs\" ] = self . refs . baseattrs if self . has params ( ) : result [ \"params\" ] = \", \" . join ( self . parameters ) else : result [ \"params\" ] = \"\" return result", "predictions": ["removes the exists and all return tasks . . . . . . . ."], "references": ["a dict of members expressed in literals"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3754, "code": "def import funcs ( self , module ) : newcells = self . impl . new cells from module ( module ) return get interfaces ( newcells )", "predictions": ["set the cells cells cells to a * * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["create a cells from a module ."], "bleu": 0.04317900023606586, "rouge_l": 0.11879259980525803}
{"id": 3755, "code": "def get object ( self , name ) : parts = name . split ( \".\" ) child = parts . pop ( 0 ) if parts : return self . spaces [ child ] . get object ( \".\" . join ( parts ) ) else : return self . namespace impl [ child ]", "predictions": ["clear the dir with the given path"], "references": ["retrieve an object by a dotted name relative to the space ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 3756, "code": "def new dynspace ( self , name = None , bases = None , formula = None , refs = None , arguments = None , source = None , ) : if name is None : name = self . spacenamer . get next ( self . namespace ) if name in self . namespace : raise Value Error ( \"Name '%s' already exists.\" % name ) if not is valid name ( name ) : raise Value Error ( \"Invalid name '%s'.\" % name ) space = Root Dynamic Space Impl ( parent = self , name = name , formula = formula , refs = refs , source = source , arguments = arguments , ) space . is derived = False self . set space ( space ) if bases : dynbase = self . get dynamic base ( bases ) space . dynbase = dynbase dynbase . dynamic subs . append ( space ) return space", "predictions": ["create a delete and return it ."], "references": ["create a new dynamic root space ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 3757, "code": "def restore state ( self , system ) : super ( ) . restore state ( system ) Base Space Container Impl . restore state ( self , system ) for cells in self . cells . values ( ) : cells . restore state ( system )", "predictions": ["link all files in machine to their own file . . . . . . . . . . . . . . . . . . . . . ."], "references": ["called after unpickling to restore some attributes manually ."], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 3758, "code": "def del space ( self , name ) : if name not in self . spaces : raise Value Error ( \"Space '%s' does not exist\" % name ) if name in self . static spaces : space = self . static spaces [ name ] if space . is derived : raise Value Error ( \"%s has derived spaces\" % repr ( space . interface ) ) else : self . static spaces . del item ( name ) self . model . spacegraph . remove node ( space ) self . inherit ( ) self . model . spacegraph . update subspaces ( self ) elif name in self . dynamic spaces : self . dynamic spaces . del item ( name ) else : raise Value Error ( \"Derived cells cannot be deleted\" )", "predictions": ["delete a file from the model list"], "references": ["delete a space ."], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 3759, "code": "def clear obj ( self , obj ) : obj nodes = self . get nodes with ( obj ) removed = set ( ) for node in obj nodes : if self . has node ( node ) : removed . update ( self . clear descendants ( node ) ) return removed", "predictions": ["clear all the nodes that were marked as a return value . . . . . . . ."], "references": ["remove all nodes with obj and their descendants ."], "bleu": 0.07658412276041004, "rouge_l": 0.2290362953692115}
{"id": 3760, "code": "def get nodes with ( self , obj ) : result = set ( ) if nx . version [ 0 ] == \"1\" : nodes = self . nodes iter ( ) else : nodes = self . nodes for node in nodes : if node [ OBJ ] == obj : result . add ( node ) return result", "predictions": ["return list of nodes that are available on the graph"], "references": ["return nodes with obj ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 3761, "code": "def add path ( self , nodes , * * attr ) : if nx . version [ 0 ] == \"1\" : return super ( ) . add path ( nodes , * * attr ) else : return nx . add path ( self , nodes , * * attr )", "predictions": ["get a node and add it to the template and get its node s node"], "references": ["in replacement for deprecated add_path method"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3762, "code": "def rename ( self , name ) : self . impl . system . rename model ( new name = name , old name = self . name )", "predictions": ["node should be called after a model"], "references": ["rename the model itself"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3763, "code": "def rename ( self , name ) : if is valid name ( name ) : if name not in self . system . models : self . name = name return True else : return False else : raise Value Error ( \"Invalid name '%s'.\" % name )", "predictions": ["get a group name"], "references": ["rename self . must be called only by its system ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 3764, "code": "def clear descendants ( self , source , clear source = True ) : removed = self . cellgraph . clear descendants ( source , clear source ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]", "predictions": ["restore all ipython ipython nodes from a source ."], "references": ["clear values and nodes calculated from source ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 3765, "code": "def clear obj ( self , obj ) : removed = self . cellgraph . clear obj ( obj ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]", "predictions": ["restore all the nodes from a node object"], "references": ["clear values and nodes of obj and their dependants ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 3766, "code": "def get object ( self , name ) : parts = name . split ( \".\" ) space = self . spaces [ parts . pop ( 0 ) ] if parts : return space . get object ( \".\" . join ( parts ) ) else : return space", "predictions": ["get an object by name"], "references": ["retrieve an object by a dotted name relative to the model ."], "bleu": 0.13249284136531225, "rouge_l": 0.4380610412926392}
{"id": 3767, "code": "def restore state ( self , system ) : Impl . restore state ( self , system ) Base Space Container Impl . restore state ( self , system ) mapping = { } for node in self . cellgraph : if isinstance ( node , tuple ) : name , key = node else : name , key = node , None cells = self . get object ( name ) mapping [ node ] = get node ( cells , key , None ) self . cellgraph = nx . relabel nodes ( self . cellgraph , mapping )", "predictions": ["get interfaces from layer return dict return * interfaces *"], "references": ["called after unpickling to restore some attributes manually ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3768, "code": "def get dynamic base ( self , bases : tuple ) : try : return self . dynamic bases inverse [ bases ] except Key Error : name = self . dynamic base namer . get next ( self . dynamic bases ) base = self . new space ( name = name ) self . spacegraph . add space ( base ) self . dynamic bases [ name ] = base self . dynamic bases inverse [ bases ] = base base . add bases ( bases ) return base", "predictions": ["construct and return a base instance for the dynamic"], "references": ["create of get a base space for a tuple of bases"], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 3769, "code": "def check mro ( self , bases ) : try : self . add node ( \"temp\" ) for base in bases : nx . Di Graph . add edge ( self , base , \"temp\" ) result = self . get mro ( \"temp\" ) [ 1 : ] finally : self . remove node ( \"temp\" ) return result", "predictions": ["check a mro that doesn t already exist type type type type type type type type type type"], "references": ["check if c3 mro is possible with given bases"], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 3770, "code": "def get command names ( ) : ret = [ ] for f in os . listdir ( COMMAND MODULE PATH ) : if os . path . isfile ( os . path . join ( COMMAND MODULE PATH , f ) ) and f . endswith ( COMMAND MODULE SUFFIX ) : ret . append ( f [ : - len ( COMMAND MODULE SUFFIX ) ] ) return ret", "predictions": ["get command to all packages"], "references": ["returns a list of command names supported"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3771, "code": "def get ( vals , key , default val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default val else : return default val return val", "predictions": ["convert a value from a dictionary to a dictionary of values"], "references": ["returns a dictionary value"], "bleu": 0.16108992769687397, "rouge_l": 0.2911694510739857}
{"id": 3772, "code": "def parse option settings ( option settings ) : ret = [ ] for namespace , params in list ( option settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret", "predictions": ["copy all the option to a set of option"], "references": ["parses option_settings as they are defined in the configuration file"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 3773, "code": "def parse env config ( config , env name ) : all env = get ( config , 'app.all environments' , { } ) env = get ( config , 'app.environments.' + str ( env name ) , { } ) return merge dict ( all env , env )", "predictions": ["parse env self . env to merge ."], "references": ["parses an environment config"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3774, "code": "def add config files to archive ( directory , filename , config = { } ) : with zipfile . Zip File ( filename , 'a' ) as zip file : for conf in config : for conf , tree in list ( conf . items ( ) ) : if 'yaml' in tree : content = yaml . dump ( tree [ 'yaml' ] , default flow style = False ) else : content = tree . get ( 'content' , '' ) out ( \"Adding file \" + str ( conf ) + \" to archive \" + str ( filename ) ) file entry = zipfile . Zip Info ( conf ) file entry . external attr = tree . get ( 'permissions' , 0o644 ) << 16 zip file . writestr ( file entry , content ) return filename", "predictions": ["value to value pairs in a zip archive"], "references": ["adds configuration files to an existing archive"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3775, "code": "def swap environment cnames ( self , from env name , to env name ) : self . ebs . swap environment cnames ( source environment name = from env name , destination environment name = to env name )", "predictions": ["self . environment environment"], "references": ["swaps cnames for an environment"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3776, "code": "def upload archive ( self , filename , key , auto create bucket = True ) : try : bucket = self . s3 . get bucket ( self . aws . bucket ) if ( ( self . aws . region != 'us-east-1' and self . aws . region != 'eu-west-1' ) and bucket . get location ( ) != self . aws . region ) or ( self . aws . region == 'us-east-1' and bucket . get location ( ) != '' ) or ( self . aws . region == 'eu-west-1' and bucket . get location ( ) != 'eu-west-1' ) : raise Exception ( \"Existing bucket doesn't match region\" ) except S3Response Error : bucket = self . s3 . create bucket ( self . aws . bucket , location = self . aws . region ) def report upload progress ( sent , total ) : if not sent : sent = 0 if not total : total = 0 out ( \"Uploaded \" + str ( sent ) + \" bytes of \" + str ( total ) + \" (\" + str ( int ( float ( max ( 1 , sent ) ) / float ( total ) * 100 ) ) + \"%)\" ) k = Key ( bucket ) k . key = self . aws . bucket path + key k . set metadata ( 'time' , str ( time ( ) ) ) k . set contents from filename ( filename , cb = report upload progress , num cb = 10 )", "predictions": ["upload a archive to s3"], "references": ["uploads an application archive version to s3"], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 3777, "code": "def application exists ( self ) : response = self . ebs . describe applications ( application names = [ self . app name ] ) return len ( response [ 'Describe Applications Response' ] [ 'Describe Applications Result' ] [ 'Applications' ] ) > 0", "predictions": ["check if the application exists ."], "references": ["returns whether or not the given app_name exists"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3778, "code": "def create environment ( self , env name , version label = None , solution stack name = None , cname prefix = None , description = None , option settings = None , tier name = 'Web Server' , tier type = 'Standard' , tier version = '1.1' ) : out ( \"Creating environment: \" + str ( env name ) + \", tier name:\" + str ( tier name ) + \", tier type:\" + str ( tier type ) ) self . ebs . create environment ( self . app name , env name , version label = version label , solution stack name = solution stack name , cname prefix = cname prefix , description = description , option settings = option settings , tier type = tier type , tier name = tier name , tier version = tier version )", "predictions": ["create environment for this tier"], "references": ["creates a new environment"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3779, "code": "def environment exists ( self , env name ) : response = self . ebs . describe environments ( application name = self . app name , environment names = [ env name ] , include deleted = False ) return len ( response [ 'Describe Environments Response' ] [ 'Describe Environments Result' ] [ 'Environments' ] ) > 0 and response [ 'Describe Environments Response' ] [ 'Describe Environments Result' ] [ 'Environments' ] [ 0 ] [ 'Status' ] != 'Terminated'", "predictions": ["check if the environment exists ."], "references": ["returns whether or not the given environment exists"], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 3780, "code": "def update environment ( self , environment name , description = None , option settings = [ ] , tier type = None , tier name = None , tier version = '1.0' ) : out ( \"Updating environment: \" + str ( environment name ) ) messages = self . ebs . validate configuration settings ( self . app name , option settings , environment name = environment name ) messages = messages [ 'Validate Configuration Settings Response' ] [ 'Validate Configuration Settings Result' ] [ 'Messages' ] ok = True for message in messages : if message [ 'Severity' ] == 'error' : ok = False out ( \"[\" + message [ 'Severity' ] + \"] \" + str ( environment name ) + \" - '\" + message [ 'Namespace' ] + \":\" + message [ 'Option Name' ] + \"': \" + message [ 'Message' ] ) self . ebs . update environment ( environment name = environment name , description = description , option settings = option settings , tier type = tier type , tier name = tier name , tier version = tier version )", "predictions": ["update the environment with the given environment ."], "references": ["updates an application version"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3781, "code": "def environment name for cname ( self , env cname ) : envs = self . get environments ( ) for env in envs : if env [ 'Status' ] != 'Terminated' and 'CNAME' in env and env [ 'CNAME' ] and env [ 'CNAME' ] . lower ( ) . startswith ( env cname . lower ( ) + '.' ) : return env [ 'Environment Name' ] return None", "predictions": ["return the environment name for the environment ."], "references": ["returns an environment name for the given cname"], "bleu": 0.44632361378533286, "rouge_l": 0.5}
{"id": 3782, "code": "def deploy version ( self , environment name , version label ) : out ( \"Deploying \" + str ( version label ) + \" to \" + str ( environment name ) ) self . ebs . update environment ( environment name = environment name , version label = version label )", "predictions": ["deploy the version of the environment"], "references": ["deploys a version to an environment"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 3783, "code": "def get versions ( self ) : response = self . ebs . describe application versions ( application name = self . app name ) return response [ 'Describe Application Versions Response' ] [ 'Describe Application Versions Result' ] [ 'Application Versions' ]", "predictions": ["get the versions of the application ."], "references": ["returns the versions available"], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 3784, "code": "def create application version ( self , version label , key ) : out ( \"Creating application version \" + str ( version label ) + \" for \" + str ( key ) ) self . ebs . create application version ( self . app name , version label , s3 bucket = self . aws . bucket , s3 key = self . aws . bucket path + key )", "predictions": ["create application version ."], "references": ["creates an application version"], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 3785, "code": "def describe events ( self , environment name , next token = None , start time = None ) : events = self . ebs . describe events ( application name = self . app name , environment name = environment name , next token = next token , start time = start time + 'Z' ) return ( events [ 'Describe Events Response' ] [ 'Describe Events Result' ] [ 'Events' ] , events [ 'Describe Events Response' ] [ 'Describe Events Result' ] [ 'Next Token' ] )", "predictions": ["describe the events in the pool"], "references": ["describes events from the given environment"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 3786, "code": "def add arguments ( parser ) : parser . add argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )", "predictions": ["add arguments for command line ."], "references": ["adds arguments for the swap urls command"], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 3787, "code": "def execute ( helper , config , args ) : env = parse env config ( config , args . environment ) option settings = env . get ( 'option settings' , { } ) settings = parse option settings ( option settings ) for setting in settings : out ( str ( setting ) )", "predictions": ["execute the given command with the given settings ."], "references": ["dump command dumps things"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3788, "code": "def execute ( helper , config , args ) : version label = args . version label env config = parse env config ( config , args . environment ) env name = args . environment version label = upload application archive ( helper , env config , archive = args . archive , directory = args . directory , version label = version label ) import datetime start time = datetime . datetime . utcnow ( ) . isoformat ( ) + 'Z' helper . deploy version ( env name , version label ) if not args . dont wait : helper . wait for environments ( env name , status = 'Ready' , version label = version label , include deleted = False ) env = parse env config ( config , env name ) option settings = parse option settings ( env . get ( 'option settings' , { } ) ) helper . update environment ( env name , description = env . get ( 'description' , None ) , option settings = option settings , tier type = env . get ( 'tier type' ) , tier name = env . get ( 'tier name' ) , tier version = env . get ( 'tier version' ) ) if not args . dont wait : helper . wait for environments ( env name , health = 'Green' , status = 'Ready' , version label = version label , include deleted = False ) events = helper . ebs . describe events ( start time = start time , environment name = env name ) import json if args . log events to file : with open ( 'ebs events.json' , 'w+' ) as f : json . dump ( events , f ) helper . delete unused versions ( versions to keep = int ( get ( config , 'app.versions to keep' , 10 ) ) )", "predictions": ["execute a command in the current virtualenv ."], "references": ["deploys to an environment"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3789, "code": "def add arguments ( parser ) : parser . add argument ( '-e' , '--environment' , help = 'Environment name' , required = True ) parser . add argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the init to finish' , action = 'store true' ) parser . add argument ( '-l' , '--version-label' , help = 'Version label' , required = False )", "predictions": ["add arguments for command line ."], "references": ["adds arguments for the deploy command"], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 3790, "code": "def execute ( helper , config , args ) : env config = parse env config ( config , args . environment ) cname prefix = env config . get ( 'cname prefix' , None ) env name = args . environment if args . version label : helper . deploy version ( env name , args . version label ) if not args . dont wait : helper . wait for environments ( env name , status = 'Ready' , version label = args . version label ) env = parse env config ( config , env name ) option settings = parse option settings ( env . get ( 'option settings' , { } ) ) helper . update environment ( env name , description = env . get ( 'description' , None ) , option settings = option settings , tier type = env . get ( 'tier type' ) , tier name = env . get ( 'tier name' ) , tier version = env . get ( 'tier version' ) ) if not args . dont wait : helper . wait for environments ( env name , health = 'Green' , status = 'Ready' , version label = args . version label ) helper . delete unused versions ( versions to keep = int ( get ( config , 'app.versions to keep' , 10 ) ) )", "predictions": ["execute a deploy command ."], "references": ["deploys to an environment"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3791, "code": "def join phonemes ( * args ) : if len ( args ) == 1 : args = args [ 0 ] if len ( args ) == 2 : args += ( CODAS [ 0 ] , ) try : onset , nucleus , coda = args except Value Error : raise Type Error ( 'join phonemes() takes at most 3 arguments' ) offset = ( ( ONSETS . index ( onset ) * NUM NUCLEUSES + NUCLEUSES . index ( nucleus ) ) * NUM CODAS + CODAS . index ( coda ) ) return unichr ( FIRST HANGUL OFFSET + offset )", "predictions": ["join two sequences together"], "references": ["joins a hangul letter from korean phonemes ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 3792, "code": "def execute ( helper , config , args ) : helper . wait for environments ( args . environment , health = args . health )", "predictions": ["execute the given helper ."], "references": ["waits for an environment to be healthy"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 3793, "code": "def add arguments ( parser ) : parser . add argument ( '-e' , '--environment' , help = 'Environment name' , required = False , nargs = '+' ) parser . add argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the app to be deleted' , action = 'store true' )", "predictions": ["add arguments for the command line ."], "references": ["args for the init command"], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 3794, "code": "def execute ( helper , config , args ) : environment name = args . environment ( events , next token ) = helper . describe events ( environment name , start time = datetime . now ( ) . isoformat ( ) ) for event in events : print ( ( \"[\" + event [ 'Severity' ] + \"] \" + event [ 'Message' ] ) )", "predictions": ["execute all events in the log"], "references": ["describes recent events for an environment ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3795, "code": "def add arguments ( parser ) : parser . add argument ( '-e' , '--environment' , help = 'Environment name' , required = True ) parser . add argument ( '-w' , '--dont-wait' , help = 'Skip waiting' , action = 'store true' ) parser . add argument ( '-a' , '--archive' , help = 'Archive file' , required = False ) parser . add argument ( '-d' , '--directory' , help = 'Directory' , required = False ) parser . add argument ( '-l' , '--version-label' , help = 'Version label' , required = False ) parser . add argument ( '-t' , '--termination-delay' , help = 'Delay termination of old environment by this number of seconds' , type = int , required = False )", "predictions": ["add arguments for command line arguments"], "references": ["adds arguments for the deploy command"], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 3796, "code": "def execute ( helper , config , args ) : version label = args . version label archive = args . archive env config = parse env config ( config , args . environment ) option settings = parse option settings ( env config . get ( 'option settings' , { } ) ) cname prefix = env config . get ( 'cname prefix' , None ) tier name = env config . get ( 'tier name' , 'Web Server' ) if tier name != 'Web Server' : raise Exception ( \"Only able to do zero downtime deployments for \" \"Web Server tiers, can't do them for %s\" % ( tier name , ) ) out ( \"Determining new environment name...\" ) new env name = None if not helper . environment exists ( args . environment ) : new env name = args . environment else : for i in range ( 10 ) : temp env name = args . environment + '-' + str ( i ) if not helper . environment exists ( temp env name ) : new env name = temp env name break if new env name is None : raise Exception ( \"Unable to determine new environment name\" ) out ( \"New environment name will be \" + new env name ) out ( \"Determining new environment cname...\" ) new env cname = None for i in range ( 10 ) : temp cname = cname prefix + '-' + str ( i ) if not helper . environment name for cname ( temp cname ) : new env cname = temp cname break if new env cname is None : raise Exception ( \"Unable to determine new environment cname\" ) out ( \"New environment cname will be \" + new env cname ) version label = upload application archive ( helper , env config , archive = args . archive , directory = args . directory , version label = version label ) helper . create environment ( new env name , solution stack name = env config . get ( 'solution stack name' ) , cname prefix = new env cname , description = env config . get ( 'description' , None ) , option settings = option settings , version label = version label , tier name = tier name , tier type = env config . get ( 'tier type' ) , tier version = env config . get ( 'tier version' ) ) helper . wait for environments ( new env name , status = 'Ready' , health = 'Green' , include deleted = False ) old env name = helper . environment name for cname ( cname prefix ) if old env name is None : raise Exception ( \"Unable to find current environment with cname: \" + cname prefix ) out ( \"Current environment name is \" + old env name ) out ( \"Swapping environment cnames\" ) helper . swap environment cnames ( old env name , new env name ) helper . wait for environments ( [ old env name , new env name ] , status = 'Ready' , include deleted = False ) if args . termination delay : out ( \"Termination delay specified, sleeping for {} seconds...\" . format ( args . termination delay ) ) time . sleep ( args . termination delay ) out ( \"Deleting old environment {}\" . format ( old env name ) ) helper . delete environment ( old env name ) helper . delete unused versions ( versions to keep = int ( get ( config , 'app.versions to keep' , 10 ) ) )", "predictions": ["execute the current environment ."], "references": ["deploys to an environment"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3797, "code": "def pick coda from decimal ( decimal ) : decimal = Decimal ( decimal ) , digits , exp = decimal . as tuple ( ) if exp < 0 : return DIGIT CODAS [ digits [ - 1 ] ] , digits , exp = decimal . normalize ( ) . as tuple ( ) index = bisect right ( EXP INDICES , exp ) - 1 if index < 0 : return DIGIT CODAS [ digits [ - 1 ] ] else : return EXP CODAS [ EXP INDICES [ index ] ]", "predictions": ["pick a coda from a decimal decimal decimal decimal decimal decimal ."], "references": ["picks only a coda from a decimal ."], "bleu": 0.41397900200299415, "rouge_l": 0.6224489795918368}
{"id": 3798, "code": "def extract actions from class ( record class ) : for name in dir ( record class ) : method = getattr ( record class , name , None ) if method and getattr ( method , ' deposit action ' , False ) : yield method . name", "predictions": ["extract actions from a class"], "references": ["extract actions from class ."], "bleu": 0.537284965911771, "rouge_l": 0.8}
{"id": 3799, "code": "def create error handlers ( blueprint ) : blueprint . errorhandler ( PID Invalid Action ) ( create api errorhandler ( status = 403 , message = 'Invalid action' ) ) records rest error handlers ( blueprint )", "predictions": ["create error handlers ."], "references": ["create error handlers on blueprint ."], "bleu": 0.4288819424803534, "rouge_l": 0.7721518987341772}
{"id": 3800, "code": "def location ( ) : d = current app . config [ 'DATADIR' ] with db . session . begin nested ( ) : Location . query . delete ( ) loc = Location ( name = 'local' , uri = d , default = True ) db . session . add ( loc ) db . session . commit ( )", "predictions": ["delete current location ."], "references": ["load default location ."], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 3801, "code": "def jsonschemas ( self ) : jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT RECORDS UI ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT DEFAULT JSONSCHEMA' ] , jsonschemas )", "predictions": ["return a dictionary with all jsonschemas ."], "references": ["load deposit json schemas ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3802, "code": "def schemaforms ( self ) : schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT RECORDS UI ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT DEFAULT SCHEMAFORM' ] , schemaforms )", "predictions": ["return the schemaforms ."], "references": ["load deposit schema forms ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3803, "code": "def pid ( self ) : pid = self . deposit fetcher ( self . id , self ) return Persistent Identifier . get ( pid . pid type , pid . pid value )", "predictions": ["get the pid pid ."], "references": ["return an instance of deposit pid ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 3804, "code": "def record schema ( self ) : schema path = current jsonschemas . url to path ( self [ '$schema' ] ) schema prefix = current app . config [ 'DEPOSIT JSONSCHEMAS PREFIX' ] if schema path and schema path . startswith ( schema prefix ) : return current jsonschemas . path to url ( schema path [ len ( schema prefix ) : ] )", "predictions": ["return the schema schema for this item ."], "references": ["convert deposit schema to a valid record schema ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 3805, "code": "def fetch published ( self ) : pid type = self [ ' deposit' ] [ 'pid' ] [ 'type' ] pid value = self [ ' deposit' ] [ 'pid' ] [ 'value' ] resolver = Resolver ( pid type = pid type , object type = 'rec' , getter = partial ( self . published record class . get record , with deleted = True ) ) return resolver . resolve ( pid value )", "predictions": ["fetch published record for the published record ."], "references": ["return a tuple with pid and published record ."], "bleu": 0.26340290082307516, "rouge_l": 0.34923664122137404}
{"id": 3806, "code": "def merge with published ( self ) : pid , first = self . fetch published ( ) lca = first . revisions [ self [ ' deposit' ] [ 'pid' ] [ 'revision id' ] ] args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ ' deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except Unresolved Conflicts Exception : raise Merge Conflict ( ) return patch ( m . unified patches , lca )", "predictions": ["merge unified patches with published patches ."], "references": ["merge changes with latest published version ."], "bleu": 0.23356898886410002, "rouge_l": 0.5714285714285714}
{"id": 3807, "code": "def commit ( self , * args , * * kwargs ) : return super ( Deposit , self ) . commit ( * args , * * kwargs )", "predictions": ["commits the transaction ."], "references": ["store changes on current instance in database and index it ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 3808, "code": "def process files ( self , record id , data ) : if self . files : assert not self . files . bucket . locked self . files . bucket . locked = True snapshot = self . files . bucket . snapshot ( lock = True ) data [ ' files' ] = self . files . dumps ( bucket = snapshot . id ) yield data db . session . add ( Records Buckets ( record id = record id , bucket id = snapshot . id ) ) else : yield data", "predictions": ["upload archive string into archive ."], "references": ["snapshot bucket and add files in record during first publishing ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 3809, "code": "def publish edited ( self ) : record pid , record = self . fetch published ( ) if record . revision id == self [ ' deposit' ] [ 'pid' ] [ 'revision id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge with published ( ) data [ '$schema' ] = self . record schema data [ ' deposit' ] = self [ ' deposit' ] record = record . class ( data , model = record . model ) return record", "predictions": ["application a exists for a specific response describe describe describe describe describe describe describe describe describe describe describe ."], "references": ["publish the deposit after for editing ."], "bleu": 0.0712695567709093, "rouge_l": 0.16781292984869325}
{"id": 3810, "code": "def rst2node ( doc name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new document ( '<%s>' % doc name ) document . settings = docutils . frontend . Option Parser ( ) . get default values ( ) document . settings . tab width = 4 document . settings . pep references = False document . settings . rfc references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par", "predictions": ["look up a tier from a tier tier prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix"], "references": ["converts a restructuredtext into its node"], "bleu": 0.05809665204409193, "rouge_l": 0.08232118758434548}
{"id": 3811, "code": "def setup ( app ) : if 'http' not in app . domains : httpdomain . setup ( app ) app . add directive ( 'autopyramid' , Route Directive )", "predictions": ["required sphinx extension environment"], "references": ["hook the directives when sphinx ask for it ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3812, "code": "def api request ( self , endpoint , http method , * args , * * kwargs ) : logger . debug ( ' > Sending API request to endpoint: %s' % endpoint ) auth = self . build http auth ( ) headers = self . build request headers ( kwargs . get ( 'headers' ) ) logger . debug ( '\\theaders: %s' % headers ) path = self . build request path ( endpoint ) logger . debug ( '\\tpath: %s' % path ) data = self . build payload ( kwargs . get ( 'payload' ) ) if not data : data = kwargs . get ( 'data' ) logger . debug ( '\\tdata: %s' % data ) req kw = dict ( auth = auth , headers = headers , timeout = kwargs . get ( 'timeout' , self . DEFAULT TIMEOUT ) ) if ( http method == self . HTTP POST ) : if ( data ) : r = requests . post ( path , data = data , * * req kw ) else : r = requests . post ( path , * * req kw ) elif http method == self . HTTP PUT : if ( data ) : r = requests . put ( path , data = data , * * req kw ) else : r = requests . put ( path , * * req kw ) elif http method == self . HTTP DELETE : r = requests . delete ( path , * * req kw ) else : r = requests . get ( path , * * req kw ) logger . debug ( '\\tresponse code:%s' % r . status code ) try : logger . debug ( '\\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\\tresponse: %s' % r . content ) return self . parse response ( r )", "predictions": ["make a environment request"], "references": ["private method for api requests"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 3813, "code": "def get log ( self , log id , timeout = None ) : return self . api request ( self . GET LOG ENDPOINT % log id , self . HTTP GET , timeout = timeout )", "predictions": ["environment for a name ."], "references": ["api call to get a specific log entry"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3814, "code": "def get log events ( self , log id , timeout = None ) : return self . api request ( self . GET LOG EVENTS ENDPOINT % log id , self . HTTP GET , timeout = timeout )", "predictions": ["deploy version of a version of a version of the given version + label + label + label + version + label + label + label + label + label +"], "references": ["api call to get a specific log entry"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 3815, "code": "def templates ( self , timeout = None ) : return self . api request ( self . TEMPLATES ENDPOINT , self . HTTP GET , timeout = timeout )", "predictions": ["list all get get get get get get get get get available get requests describe describe"], "references": ["api call to get a list of templates"], "bleu": 0.08513012360883544, "rouge_l": 0.08866279069767442}
{"id": 3816, "code": "def get template ( self , template id , version = None , timeout = None ) : if ( version ) : return self . api request ( self . TEMPLATES VERSION ENDPOINT % ( template id , version ) , self . HTTP GET , timeout = timeout ) else : return self . api request ( self . TEMPLATES SPECIFIC ENDPOINT % template id , self . HTTP GET , timeout = timeout )", "predictions": ["create a application with given id and label"], "references": ["api call to get a specific template"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3817, "code": "def create template ( self , name , subject , html , text = '' , timeout = None ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } return self . api request ( self . TEMPLATES ENDPOINT , self . HTTP POST , payload = payload , timeout = timeout )", "predictions": ["describe a events ."], "references": ["api call to create a template"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3818, "code": "def create new locale ( self , template id , locale , version name , subject , text = '' , html = '' , timeout = None ) : payload = { 'locale' : locale , 'name' : version name , 'subject' : subject } if html : payload [ 'html' ] = html if text : payload [ 'text' ] = text return self . api request ( self . TEMPLATES LOCALES ENDPOINT % template id , self . HTTP POST , payload = payload , timeout = timeout )", "predictions": ["creates a arguments for a arguments ."], "references": ["api call to create a new locale and version of a template"], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 3819, "code": "def create new version ( self , name , subject , text = '' , template id = None , html = None , locale = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } if locale : url = self . TEMPLATES SPECIFIC LOCALE VERSIONS ENDPOINT % ( template id , locale ) else : url = self . TEMPLATES NEW VERSION ENDPOINT % template id return self . api request ( url , self . HTTP POST , payload = payload , timeout = timeout )", "predictions": ["execute a new ."], "references": ["api call to create a new version of a template"], "bleu": 0.10551173833795614, "rouge_l": 0.26521739130434785}
{"id": 3820, "code": "def update template version ( self , name , subject , template id , version id , text = '' , html = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } return self . api request ( self . TEMPLATES VERSION ENDPOINT % ( template id , version id ) , self . HTTP PUT , payload = payload , timeout = timeout )", "predictions": ["updates a template ."], "references": ["api call to update a template version"], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 3821, "code": "def snippets ( self , timeout = None ) : return self . api request ( self . SNIPPETS ENDPOINT , self . HTTP GET , timeout = timeout )", "predictions": ["retrieve a add - on - add - add - key key for this user . . . . . . . . . . . . . . . ."], "references": ["api call to get list of snippets"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3822, "code": "def get snippet ( self , snippet id , timeout = None ) : return self . api request ( self . SNIPPET ENDPOINT % ( snippet id ) , self . HTTP GET , timeout = timeout )", "predictions": ["execute a snippet environment ."], "references": ["api call to get a specific snippet"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 3823, "code": "def create snippet ( self , name , body , timeout = None ) : payload = { 'name' : name , 'body' : body } return self . api request ( self . SNIPPETS ENDPOINT , self . HTTP POST , payload = payload , timeout = timeout )", "predictions": ["creates a snippet ."], "references": ["api call to create a snippet"], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 3824, "code": "def make file dict ( self , f ) : if isinstance ( f , dict ) : file obj = f [ 'file' ] if 'filename' in f : file name = f [ 'filename' ] else : file name = file obj . name else : file obj = f file name = f . name b64 data = base64 . b64encode ( file obj . read ( ) ) return { 'id' : file name , 'data' : b64 data . decode ( ) if six . PY3 else b64 data , }", "predictions": ["execute a helper function to execute a helper function to execute a file . ."], "references": ["make a dictionary with filename and base64 file data"], "bleu": 0.09103526405546068, "rouge_l": 0.17453505007153075}
{"id": 3825, "code": "def send ( self , email id , recipient , email data = None , sender = None , cc = None , bcc = None , tags = [ ] , headers = { } , esp account = None , locale = None , email version name = None , inline = None , files = [ ] , timeout = None ) : if not email data : email data = { } if isinstance ( recipient , string types ) : warnings . warn ( \"Passing email directly for recipient is deprecated\" , Deprecation Warning ) recipient = { 'address' : recipient } payload = { 'email id' : email id , 'recipient' : recipient , 'email data' : email data } if sender : payload [ 'sender' ] = sender if cc : if not type ( cc ) == list : logger . error ( 'kwarg cc must be type(list), got %s' % type ( cc ) ) payload [ 'cc' ] = cc if bcc : if not type ( bcc ) == list : logger . error ( 'kwarg bcc must be type(list), got %s' % type ( bcc ) ) payload [ 'bcc' ] = bcc if tags : if not type ( tags ) == list : logger . error ( 'kwarg tags must be type(list), got %s' % ( type ( tags ) ) ) payload [ 'tags' ] = tags if headers : if not type ( headers ) == dict : logger . error ( 'kwarg headers must be type(dict), got %s' % ( type ( headers ) ) ) payload [ 'headers' ] = headers if esp account : if not isinstance ( esp account , string types ) : logger . error ( 'kwarg esp account must be a string, got %s' % ( type ( esp account ) ) ) payload [ 'esp account' ] = esp account if locale : if not isinstance ( locale , string types ) : logger . error ( 'kwarg locale must be a string, got %s' % ( type ( locale ) ) ) payload [ 'locale' ] = locale if email version name : if not isinstance ( email version name , string types ) : logger . error ( 'kwarg email version name must be a string, got %s' % ( type ( email version name ) ) ) payload [ 'version name' ] = email version name if inline : payload [ 'inline' ] = self . make file dict ( inline ) if files : payload [ 'files' ] = [ self . make file dict ( f ) for f in files ] return self . api request ( self . SEND ENDPOINT , self . HTTP POST , payload = payload , timeout = timeout )", "predictions": ["add email to server"], "references": ["api call to send an email"], "bleu": 0.24117803988461298, "rouge_l": 0.1930379746835443}
{"id": 3826, "code": "def api request ( self , endpoint , http method , * args , * * kwargs ) : logger . debug ( ' > Queing batch api request for endpoint: %s' % endpoint ) path = self . build request path ( endpoint , absolute = False ) logger . debug ( '\\tpath: %s' % path ) data = None if 'payload' in kwargs : data = kwargs [ 'payload' ] logger . debug ( '\\tdata: %s' % data ) command = { \"path\" : path , \"method\" : http method } if data : command [ 'body' ] = data self . commands . append ( command )", "predictions": ["builds a request request request request for the given args token token token token token token token token token token token token token token token token token token token token token"], "references": ["private method for api requests"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 3827, "code": "def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . commands ) ) auth = self . build http auth ( ) headers = self . build request headers ( ) logger . debug ( '\\tbatch headers: %s' % headers ) logger . debug ( '\\tbatch command length: %s' % len ( self . commands ) ) path = self . build request path ( self . BATCH ENDPOINT ) data = json . dumps ( self . commands , cls = self . json encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT TIMEOUT if timeout is None else timeout ) ) self . commands = [ ] logger . debug ( '\\tresponse code:%s' % r . status code ) try : logger . debug ( '\\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\\tresponse: %s' % r . content ) return r", "predictions": ["executes a command and returns a parsed response ."], "references": ["execute all currently queued batch commands"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3828, "code": "def dump grid ( grid ) : header = 'ver:%s' % dump str ( str ( grid . version ) , version = grid . version ) if bool ( grid . metadata ) : header += ' ' + dump meta ( grid . metadata , version = grid . version ) columns = dump columns ( grid . column , version = grid . version ) rows = dump rows ( grid ) return '\\n' . join ( [ header , columns ] + rows + [ '' ] )", "predictions": ["execute a grid grid grid in a human readable form"], "references": ["dump a single grid to its zinc representation ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 3829, "code": "def parse ( grid str , mode = MODE ZINC , charset = 'utf-8' ) : if isinstance ( grid str , six . binary type ) : grid str = grid str . decode ( encoding = charset ) parse = functools . partial ( parse grid , mode = mode , charset = charset ) if mode == MODE JSON : if isinstance ( grid str , six . string types ) : grid data = json . loads ( grid str ) else : grid data = grid str if isinstance ( grid data , dict ) : return parse ( grid data ) else : return list ( map ( parse , grid data ) ) else : return list ( map ( parse , GRID SEP . split ( grid str . rstrip ( ) ) ) )", "predictions": ["pick a list from a from a from a from a from a from a from a from a from a from a from a from a from a from a"], "references": ["parse the given zinc text and return the equivalent data ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3830, "code": "def append ( self , key , value = MARKER , replace = True ) : return self . add item ( key , value , replace = replace )", "predictions": ["extract a key and add it to the list of values"], "references": ["append the item to the metadata ."], "bleu": 0.14991106946711685, "rouge_l": 0.2314990512333966}
{"id": 3831, "code": "def extend ( self , items , replace = True ) : if isinstance ( items , dict ) or isinstance ( items , Sortable Dict ) : items = list ( items . items ( ) ) for ( key , value ) in items : self . append ( key , value , replace = replace )", "predictions": ["create a new instance of a list of dictionaries"], "references": ["append the items to the metadata ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3832, "code": "def kwargs ( self ) : return dict ( color = self . color , velocity = self . velocity , colors = self . colors )", "predictions": ["= app s location as a dictionary"], "references": ["keyword arguments for recreating the shape from the vertices ."], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 3833, "code": "def draw ( self ) : if self . enabled : self . vertex list . colors = self . gl colors self . vertex list . vertices = self . gl vertices self . vertex list . draw ( pyglet . gl . GL TRIANGLES )", "predictions": ["draw the graph and for each v { for each v { { list }"], "references": ["draw the shape in the current opengl context ."], "bleu": 0.1082597837309053, "rouge_l": 0.17453505007153075}
{"id": 3834, "code": "def map timezones ( ) : tz map = { } todo = HAYSTACK TIMEZONES SET . copy ( ) for full tz in pytz . all timezones : if not bool ( todo ) : break if full tz in todo : tz map [ full tz ] = full tz todo . discard ( full tz ) continue if '/' not in full tz : continue ( prefix , suffix ) = full tz . split ( '/' , 1 ) if '/' in suffix : continue if suffix in todo : tz map [ suffix ] = full tz todo . discard ( suffix ) continue return tz map", "predictions": ["find the timezones from the timezones . . . . . . . . . . . ."], "references": ["map the official haystack timezone list to those recognised by pytz ."], "bleu": 0.07535838128770536, "rouge_l": 0.13832199546485258}
{"id": 3835, "code": "def timezone ( haystack tz , version = LATEST VER ) : tz map = get tz map ( version = version ) try : tz name = tz map [ haystack tz ] except Key Error : raise Value Error ( '%s is not a recognised timezone on this host' % haystack tz ) return pytz . timezone ( tz name )", "predictions": ["returns a pid instance for the given pid . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["retrieve the haystack timezone"], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 3836, "code": "def unescape ( s , uri = False ) : out = '' while len ( s ) > 0 : c = s [ 0 ] if c == '\\\\' : esc c = s [ 1 ] if esc c in ( 'u' , 'U' ) : out += six . unichr ( int ( s [ 2 : 6 ] , base = 16 ) ) s = s [ 6 : ] continue else : if esc c == 'b' : out += '\\b' elif esc c == 'f' : out += '\\f' elif esc c == 'n' : out += '\\n' elif esc c == 'r' : out += '\\r' elif esc c == 't' : out += '\\t' else : if uri and ( esc c == '#' ) : out += '\\\\' out += esc c s = s [ 2 : ] continue else : out += c s = s [ 1 : ] return out", "predictions": ["record a string to a python length ."], "references": ["iterative parser for string escapes ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 3837, "code": "def parse grid ( grid data ) : try : grid parts = NEWLINE RE . split ( grid data ) if len ( grid parts ) < 2 : raise Zinc Parse Exception ( 'Malformed grid received' , grid data , 1 , 1 ) grid meta str = grid parts . pop ( 0 ) col meta str = grid parts . pop ( 0 ) ver match = VERSION RE . match ( grid meta str ) if ver match is None : raise Zinc Parse Exception ( 'Could not determine version from %r' % grid meta str , grid data , 1 , 1 ) version = Version ( ver match . group ( 1 ) ) try : grid meta = hs grid Meta [ version ] . parse String ( grid meta str , parse All = True ) [ 0 ] except pp . Parse Exception as pe : raise Zinc Parse Exception ( 'Failed to parse grid metadata: %s' % pe , grid data , 1 , pe . col ) except : LOG . debug ( 'Failed to parse grid meta: %r' , grid meta str ) raise try : col meta = hs cols [ version ] . parse String ( col meta str , parse All = True ) [ 0 ] except pp . Parse Exception as pe : raise Zinc Parse Exception ( 'Failed to parse column metadata: %s' % reformat exception ( pe , 2 ) , grid data , 2 , pe . col ) except : LOG . debug ( 'Failed to parse column meta: %r' , col meta str ) raise row grammar = hs row [ version ] def parse row ( row num and data ) : ( row num , row ) = row num and data line num = row num + 3 try : return dict ( zip ( col meta . keys ( ) , row grammar . parse String ( row , parse All = True ) [ 0 ] . as List ( ) ) ) except pp . Parse Exception as pe : raise Zinc Parse Exception ( 'Failed to parse row: %s' % reformat exception ( pe , line num ) , grid data , line num , pe . col ) except : LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid meta . pop ( 'ver' ) , metadata = grid meta , columns = list ( col meta . items ( ) ) ) g . extend ( map ( parse row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid data ) raise", "predictions": ["fetch a row: from a published published published published published self ."], "references": ["parse the incoming grid ."], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 3838, "code": "def parse scalar ( scalar data , version ) : try : return hs scalar [ version ] . parse String ( scalar data , parse All = True ) [ 0 ] except pp . Parse Exception as pe : raise Zinc Parse Exception ( 'Failed to parse scalar: %s' % reformat exception ( pe ) , scalar data , 1 , pe . col ) except : LOG . debug ( 'Failing scalar data: %r (version %r)' , scalar data , version )", "predictions": ["merge a with with a with a with the default values"], "references": ["parse a project haystack scalar in zinc format ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 3839, "code": "def dump ( grids , mode = MODE ZINC ) : if isinstance ( grids , Grid ) : return dump grid ( grids , mode = mode ) dump = functools . partial ( dump grid , mode = mode ) if mode == MODE ZINC : return '\\n' . join ( map ( dump , grids ) ) elif mode == MODE JSON : return '[%s]' % ',' . join ( map ( dump , grids ) ) else : raise Not Implemented Error ( 'Format not implemented: %s' % mode )", "predictions": ["commit a grid instance ."], "references": ["dump the given grids in the specified over - the - wire format ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 3840, "code": "def nearest ( self , ver ) : if not isinstance ( ver , Version ) : ver = Version ( ver ) if ver in OFFICIAL VERSIONS : return ver versions = list ( OFFICIAL VERSIONS ) versions . sort ( reverse = True ) best = None for candidate in versions : if candidate == ver : return candidate if ( best is None ) and ( candidate < ver ) : warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug.  Closest (older) version supported is %s.' % ( ver , candidate ) ) return candidate if candidate > ver : best = candidate assert best is not None warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug.  Closest (newer) version supported is %s.' % ( ver , best ) ) return best", "predictions": ["return the nearest version of a version ."], "references": ["retrieve the official version nearest the one given ."], "bleu": 0.17795502018438056, "rouge_l": 0.34923664122137404}
{"id": 3841, "code": "def encrypt files ( selected host , only link , file name ) : if ENCRYPTION DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source filename = file name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source filename ) encrypted output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted data = encrypted output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload files ( encrypted data , selected host , only link , file name ) + '#' + passphrase", "predictions": ["encrypt files with selected host"], "references": ["encrypts file with gpg and random generated password"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3842, "code": "def check max filesize ( chosen file , max size ) : if os . path . getsize ( chosen file ) > max size : return False else : return True", "predictions": ["check if a file is available"], "references": ["checks file sizes for host"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3843, "code": "def parse arguments ( args , clone list ) : returned string = \"\" host number = args . host if args . show list : print ( generate host string ( clone list , \"Available hosts: \" ) ) exit ( ) if args . decrypt : for i in args . files : print ( decrypt files ( i ) ) exit ( ) if args . files : for i in args . files : if args . limit size : if args . host == host number and host number is not None : if not check max filesize ( i , clone list [ host number ] [ 3 ] ) : host number = None for n , host in enumerate ( clone list ) : if not check max filesize ( i , host [ 3 ] ) : clone list [ n ] = None if not clone list : print ( 'None of the clones is able to support so big file.' ) if args . no cloudflare : if args . host == host number and host number is not None and not clone list [ host number ] [ 4 ] : print ( \"This host uses Cloudflare, please choose different host.\" ) exit ( 1 ) else : for n , host in enumerate ( clone list ) : if not host [ 4 ] : clone list [ n ] = None clone list = list ( filter ( None , clone list ) ) if host number is None or args . host != host number : host number = random . randrange ( 0 , len ( clone list ) ) while True : try : if args . encrypt : returned string = encrypt files ( clone list [ host number ] , args . only link , i ) else : returned string = upload files ( open ( i , 'rb' ) , clone list [ host number ] , args . only link , i ) if args . only link : print ( returned string [ 0 ] ) else : print ( returned string ) except Index Error : #print('Selected server (' + clone list[host number][0] + ') is offline.') #print('Trying other host.') host number = random . randrange ( 0 , len ( clone list ) ) continue except Is A Directory Error : print ( 'limf does not support directory upload, if you want to upload ' 'every file in directory use limf {}/*.' . format ( i . replace ( '/' , '' ) ) ) if args . log : with open ( os . path . expanduser ( args . logfile ) , \"a+\" ) as logfile : if args . only link : logfile . write ( returned string [ 1 ] ) else : logfile . write ( returned string ) logfile . write ( \"\\n\" ) break else : print ( \"limf: try 'limf -h' for more information\" )", "predictions": ["parse arguments and execute the server ."], "references": ["makes parsing arguments a function ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3844, "code": "def decrypt files ( file link ) : if ENCRYPTION DISABLED : print ( 'For decryption please install gpg' ) exit ( ) try : parsed link = re . findall ( r'(.*/(.*))#(.{30})' , file link ) [ 0 ] req = urllib . request . Request ( parsed link [ 0 ] , data = None , headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10 9 3) ' ' Apple Web Kit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36' } ) #downloads the file using fake useragent file response = urllib . request . urlopen ( req ) file to decrypt = file response . read ( ) #decrypts the data using piping to ggp decrypt r , decrypt w = os . pipe ( ) cmd = 'gpg --batch --decrypt --passphrase-fd {}' . format ( decrypt r ) decrypt output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE , pass fds = ( decrypt r , ) ) os . close ( decrypt r ) open ( decrypt w , 'w' ) . write ( parsed link [ 2 ] ) decrypted data , stderr = decrypt output . communicate ( file to decrypt ) with open ( parsed link [ 1 ] , 'wb' ) as decrypted file : decrypted file . write ( decrypted data ) return parsed link [ 1 ] + ' is decrypted and saved.' except Index Error : return 'Please enter valid link.'", "predictions": ["decrypt the files from a file"], "references": ["decrypts file from entered links"], "bleu": 0.24446151121745047, "rouge_l": 0.18484848484848485}
{"id": 3845, "code": "def set value ( request ) : key = request . matchdict [ 'key' ] VALUES [ key ] = request . json body return VALUES . get ( key )", "predictions": ["set json value from request body"], "references": ["set the value and returns * true * or * false * ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 3846, "code": "def main ( ) : parser = argparse . Argument Parser ( description = ( 'Uploads selected file to working pomf.se clone' ) ) parser . add argument ( 'files' , metavar = 'file' , nargs = '*' , type = str , help = ( 'Files to upload' ) ) parser . add argument ( '-c' , metavar = 'host number' , type = int , dest = 'host' , default = None , help = ( 'The number (0-n) of the selected host (default is random)' ) ) parser . add argument ( '-l' , dest = 'only link' , action = 'store const' , const = True , default = False , help = ( 'Changes output to just link to the file' ) ) parser . add argument ( '-e' , dest = 'encrypt' , action = 'store const' , const = True , default = False , help = ( 'Encrypts then uploads the files.' ) ) parser . add argument ( '-d' , dest = 'decrypt' , action = 'store const' , const = True , default = False , help = ( 'Decrypts files from links with encrypted files' ) ) parser . add argument ( '-j' , dest = \"local list\" , default = False , help = ( 'Path to a local list file' ) ) parser . add argument ( '-s' , dest = \"show list\" , action = 'store const' , const = True , default = False , help = ( 'Show the host list (will not upload your files when called)' ) ) parser . add argument ( '-m' , dest = 'limit size' , action = 'store const' , const = True , default = False , help = ( 'Do not upload file if it exceeds the certain host limit' ) ) parser . add argument ( '-nc' , dest = 'no cloudflare' , action = 'store const' , const = True , default = False , help = ( 'Do not use hosts which use Cloudflare.' ) ) parser . add argument ( '--log-file' , metavar = \"LOGFILE\" , dest = \"logfile\" , default = \"~/limf.log\" , help = ( \"The location of log file\" ) ) parser . add argument ( '--log' , dest = 'log' , action = \"store const\" , const = True , default = False , help = ( \"Enables the logging feature, default logfile is ~/limf.log\" ) ) args = parser . parse args ( ) try : if args . local list : clone list = retrieve local host list ( args . local list ) else : clone list = retrieve online host list ( ) if len ( min ( clone list , key = len ) ) < 5 and ( args . limit size or args . no cloudflare ) : print ( ( \"For newer options, please update your host list.\" ) ) exit ( ) if args . host and not ( 0 <= args . host < len ( clone list ) ) : print ( generate host string ( clone list ) ) exit ( ) parse arguments ( args , clone list ) except File Not Found Error : print ( ( 'Plese enter valid file.' ) )", "predictions": ["run the core ."], "references": ["creates arguments and parses user input"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3847, "code": "def convert ( self , schema node , definition handler ) : converted = { 'name' : schema node . name , 'in' : self . in , 'required' : schema node . required } if schema node . description : converted [ 'description' ] = schema node . description if schema node . default : converted [ 'default' ] = schema node . default schema = definition handler ( schema node ) schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted", "predictions": ["convert dict to bigquery"], "references": ["convert node schema into a parameter object ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 3848, "code": "def get transition viewset method ( transition name , * * kwargs ) : @ detail route ( methods = [ 'post' ] , * * kwargs ) def inner func ( self , request , pk = None , * * kwargs ) : object = self . get object ( ) transition method = getattr ( object , transition name ) transition method ( by = self . request . user ) if self . save after transition : object . save ( ) serializer = self . get serializer ( object ) return Response ( serializer . data ) return inner func", "predictions": ["returns the transition viewset for the given transition name ."], "references": ["create a viewset method for the provided transition_name"], "bleu": 0.17827531042796255, "rouge_l": 0.34014869888475835}
{"id": 3849, "code": "def fresh cookies ( ctx , mold = '' ) : mold = mold or tmpdir = os . path . join ( tempfile . gettempdir ( ) , \"cc-upgrade-pygments-markdown-lexer\" ) if os . path . isdir ( '.git' ) : pass if os . path . isdir ( tmpdir ) : shutil . rmtree ( tmpdir ) if os . path . exists ( mold ) : shutil . copytree ( mold , tmpdir , ignore = shutil . ignore patterns ( \".git\" , \".svn\" , \"*~\" , ) ) else : ctx . run ( \"git clone {} {}\" . format ( mold , tmpdir ) ) shutil . copy2 ( \"project.d/cookiecutter.json\" , tmpdir ) with pushd ( '..' ) : ctx . run ( \"cookiecutter --no-input {}\" . format ( tmpdir ) ) if os . path . exists ( '.git' ) : ctx . run ( \"git status\" )", "predictions": ["fresh cookies from the project ."], "references": ["refresh the project from the original cookiecutter template ."], "bleu": 0.22172045047934616, "rouge_l": 0.3860759493670886}
{"id": 3850, "code": "def ci ( ctx ) : opts = [ '' ] if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( \"invoke --echo --pty clean --all build --docs check --reports{}\" . format ( ' ' . join ( opts ) ) )", "predictions": ["run the ci build"], "references": ["perform continuous integration tasks ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 3851, "code": "def load ( self ) : ret = { } with open ( self . get path ( ) , 'r' ) as f : lines = f . readlines ( ) ret [ 'personalities' ] = self . get personalities ( lines [ 0 ] ) ret [ 'arrays' ] = self . get arrays ( lines [ 1 : - 1 ] , ret [ 'personalities' ] ) self . content = reduce ( lambda x , y : x + y , lines ) return ret", "predictions": ["load the content from the database"], "references": ["return a dict of stats ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3852, "code": "def get personalities ( self , line ) : return [ split ( '\\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]", "predictions": ["return a list of personalities for a given line ."], "references": ["return a list of personalities readed from the input line ."], "bleu": 0.46988856598968304, "rouge_l": 0.6609907120743034}
{"id": 3853, "code": "def get arrays ( self , lines , personalities = [ ] ) : ret = { } i = 0 while i < len ( lines ) : try : md device = self . get md device name ( lines [ i ] ) except Index Error : pass else : if md device is not None : ret [ md device ] = self . get md device ( lines [ i ] , personalities ) i += 1 ret [ md device ] . update ( self . get md status ( lines [ i ] ) ) i += 1 return ret", "predictions": ["get arrays of arrays from the network ."], "references": ["return a dict of arrays ."], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 3854, "code": "def get md device ( self , line , personalities = [ ] ) : ret = { } splitted = split ( '\\W+' , line ) ret [ 'status' ] = splitted [ 1 ] if splitted [ 2 ] in personalities : ret [ 'type' ] = splitted [ 2 ] ret [ 'components' ] = self . get components ( line , with type = True ) else : ret [ 'type' ] = None ret [ 'components' ] = self . get components ( line , with type = False ) return ret", "predictions": ["get the device device information from a line of components ."], "references": ["return a dict of md device define in the line ."], "bleu": 0.1558010570411745, "rouge_l": 0.2727272727272727}
{"id": 3855, "code": "def get md status ( self , line ) : ret = { } splitted = split ( '\\W+' , line ) if len ( splitted ) < 7 : ret [ 'available' ] = None ret [ 'used' ] = None ret [ 'config' ] = None else : ret [ 'available' ] = splitted [ - 4 ] ret [ 'used' ] = splitted [ - 3 ] ret [ 'config' ] = splitted [ - 2 ] return ret", "predictions": ["get status from line"], "references": ["return a dict of md status define in the line ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 3856, "code": "def register receivers ( app , config ) : for event name , event config in config . items ( ) : event builders = [ obj or import string ( func ) for func in event config . get ( 'event builders' , [ ] ) ] signal = obj or import string ( event config [ 'signal' ] ) signal . connect ( Event Emmiter ( event name , event builders ) , sender = app , weak = False )", "predictions": ["register a callback function"], "references": ["register signal receivers which send events ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 3857, "code": "def set scheduled ( self ) : with self . idle lock : if self . idle : self . idle = False return True return False", "predictions": ["sets whether the card is scheduled for this widget"], "references": ["returns true if state was successfully changed from idle to scheduled ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 3858, "code": "def get oldest event timestamp ( self ) : query events = Search ( using = self . client , index = self . event index ) [ 0 : 1 ] . sort ( { 'timestamp' : { 'order' : 'asc' } } ) result = query events . execute ( ) if len ( result ) == 0 : return None return parser . parse ( result [ 0 ] [ 'timestamp' ] )", "predictions": ["get the oldest event timestamp"], "references": ["search for the oldest event timestamp ."], "bleu": 0.5093330917854971, "rouge_l": 0.6472148541114059}
{"id": 3859, "code": "def get bookmark ( self ) : if not Index ( self . aggregation alias , using = self . client ) . exists ( ) : if not Index ( self . event index , using = self . client ) . exists ( ) : return datetime . date . today ( ) return self . get oldest event timestamp ( ) query bookmark = Search ( using = self . client , index = self . aggregation alias , doc type = self . bookmark doc type ) [ 0 : 1 ] . sort ( { 'date' : { 'order' : 'desc' } } ) bookmarks = query bookmark . execute ( ) if len ( bookmarks ) == 0 : return self . get oldest event timestamp ( ) bookmark = datetime . datetime . strptime ( bookmarks [ 0 ] . date , self . doc id suffix ) return bookmark", "predictions": ["get the bookmark of the bookmark event ."], "references": ["get last aggregation date ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 3860, "code": "def set bookmark ( self ) : def success date ( ) : bookmark = { 'date' : self . new bookmark or datetime . datetime . utcnow ( ) . strftime ( self . doc id suffix ) } yield dict ( index = self . last index written , type = self . bookmark doc type , source = bookmark ) if self . last index written : bulk ( self . client , success date ( ) , stats only = True )", "predictions": ["set bookmark date of the client ."], "references": ["set bookmark for starting next aggregation ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 3861, "code": "def format range dt ( self , d ) : if not isinstance ( d , six . string types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt rounding map [ self . aggregation interval ] )", "predictions": ["return formatted string for given range ."], "references": ["format range filter datetime to the closest aggregation interval ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3862, "code": "def run ( self , start date = None , end date = None , update bookmark = True ) : if not Index ( self . event index , using = self . client ) . exists ( ) : return lower limit = start date or self . get bookmark ( ) if lower limit is None : return upper limit = min ( end date or datetime . datetime . max , datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , datetime . datetime . combine ( lower limit + datetime . timedelta ( self . batch size ) , datetime . datetime . min . time ( ) ) ) while upper limit <= datetime . datetime . utcnow ( ) : self . indices = set ( ) self . new bookmark = upper limit . strftime ( self . doc id suffix ) bulk ( self . client , self . agg iter ( lower limit , upper limit ) , stats only = True , chunk size = 50 ) current search client . indices . flush ( index = ',' . join ( self . indices ) , wait if ongoing = True ) if update bookmark : self . set bookmark ( ) self . indices = set ( ) lower limit = lower limit + datetime . timedelta ( self . batch size ) upper limit = min ( end date or datetime . datetime . max , datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , lower limit + datetime . timedelta ( self . batch size ) ) if lower limit > upper limit : break", "predictions": ["run the thread ."], "references": ["calculate statistics aggregations ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 3863, "code": "def list bookmarks ( self , start date = None , end date = None , limit = None ) : query = Search ( using = self . client , index = self . aggregation alias , doc type = self . bookmark doc type ) . sort ( { 'date' : { 'order' : 'desc' } } ) range args = { } if start date : range args [ 'gte' ] = self . format range dt ( start date . replace ( microsecond = 0 ) ) if end date : range args [ 'lte' ] = self . format range dt ( end date . replace ( microsecond = 0 ) ) if range args : query = query . filter ( 'range' , date = range args ) return query [ 0 : limit ] . execute ( ) if limit else query . scan ( )", "predictions": ["list bookmarks bookmarks ."], "references": ["list the aggregation s bookmarks ."], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 3864, "code": "def delete ( self , start date = None , end date = None ) : aggs query = Search ( using = self . client , index = self . aggregation alias , doc type = self . aggregation doc type ) . extra ( source = False ) range args = { } if start date : range args [ 'gte' ] = self . format range dt ( start date . replace ( microsecond = 0 ) ) if end date : range args [ 'lte' ] = self . format range dt ( end date . replace ( microsecond = 0 ) ) if range args : aggs query = aggs query . filter ( 'range' , timestamp = range args ) bookmarks query = Search ( using = self . client , index = self . aggregation alias , doc type = self . bookmark doc type ) . sort ( { 'date' : { 'order' : 'desc' } } ) if range args : bookmarks query = bookmarks query . filter ( 'range' , date = range args ) def delete actions ( ) : for query in ( aggs query , bookmarks query ) : affected indices = set ( ) for doc in query . scan ( ) : affected indices . add ( doc . meta . index ) yield dict ( index = doc . meta . index , op type = 'delete' , id = doc . meta . id , type = doc . meta . doc type ) current search client . indices . flush ( index = ',' . join ( affected indices ) , wait if ongoing = True ) bulk ( self . client , delete actions ( ) , refresh = True )", "predictions": ["delete documents from documents ."], "references": ["delete aggregation documents ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 3865, "code": "def get ( self , timeout = None ) : result = None try : result = self . result . get ( True , timeout = timeout ) except Empty : raise Timeout ( ) if isinstance ( result , Failure ) : six . reraise ( * result . exc info ) else : return result", "predictions": ["get the value from the result ."], "references": ["return value on success or raise exception on failure ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3866, "code": "def events process ( event types = None , eager = False ) : event types = event types or list ( current stats . enabled events ) if eager : process events . apply ( ( event types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process events . delay ( event types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )", "predictions": ["run the events ."], "references": ["process stats events ."], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 3867, "code": "def aggregations process ( aggregation types = None , start date = None , end date = None , update bookmark = False , eager = False ) : aggregation types = ( aggregation types or list ( current stats . enabled aggregations ) ) if eager : aggregate events . apply ( ( aggregation types , ) , dict ( start date = start date , end date = end date , update bookmark = update bookmark ) , throw = True ) click . secho ( 'Aggregations processed successfully.' , fg = 'green' ) else : aggregate events . delay ( aggregation types , start date = start date , end date = end date ) click . secho ( 'Aggregations processing task sent...' , fg = 'yellow' )", "predictions": ["process processing task ."], "references": ["process stats aggregations ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 3868, "code": "def aggregations delete ( aggregation types = None , start date = None , end date = None ) : aggregation types = ( aggregation types or list ( current stats . enabled aggregations ) ) for a in aggregation types : aggr cfg = current stats . aggregations [ a ] aggregator = aggr cfg . aggregator class ( name = aggr cfg . name , * * aggr cfg . aggregator config ) aggregator . delete ( start date , end date )", "predictions": ["delete all aggregations ."], "references": ["delete computed aggregations ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 3869, "code": "def aggregations list bookmarks ( aggregation types = None , start date = None , end date = None , limit = None ) : aggregation types = ( aggregation types or list ( current stats . enabled aggregations ) ) for a in aggregation types : aggr cfg = current stats . aggregations [ a ] aggregator = aggr cfg . aggregator class ( name = aggr cfg . name , * * aggr cfg . aggregator config ) bookmarks = aggregator . list bookmarks ( start date , end date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )", "predictions": ["list all bookmarks ."], "references": ["list aggregation bookmarks ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 3870, "code": "def events config ( self ) : result = { } for ep in iter entry points ( group = self . entry point group events ) : for cfg in ep . load ( ) ( ) : if cfg [ 'event type' ] not in self . enabled events : continue elif cfg [ 'event type' ] in result : raise Duplicate Event Error ( 'Duplicate event {0} in entry point ' '{1}' . format ( cfg [ 'event type' ] , ep . name ) ) cfg . update ( self . enabled events [ cfg [ 'event type' ] ] or { } ) result [ cfg [ 'event type' ] ] = cfg return result", "predictions": ["load all events from config files"], "references": ["load events configuration ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 3871, "code": "def aggregations config ( self ) : result = { } for ep in iter entry points ( group = self . entry point group aggs ) : for cfg in ep . load ( ) ( ) : if cfg [ 'aggregation name' ] not in self . enabled aggregations : continue elif cfg [ 'aggregation name' ] in result : raise Duplicate Aggregation Error ( 'Duplicate aggregation {0} in entry point ' '{1}' . format ( cfg [ 'event type' ] , ep . name ) ) cfg . update ( self . enabled aggregations [ cfg [ 'aggregation name' ] ] or { } ) result [ cfg [ 'aggregation name' ] ] = cfg return result", "predictions": ["return a dict of aggregations for aggregations ."], "references": ["load aggregation configurations ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3872, "code": "def queries config ( self ) : result = { } for ep in iter entry points ( group = self . entry point group queries ) : for cfg in ep . load ( ) ( ) : if cfg [ 'query name' ] not in self . enabled queries : continue elif cfg [ 'query name' ] in result : raise Duplicate Query Error ( 'Duplicate query {0} in entry point ' '{1}' . format ( cfg [ 'query' ] , ep . name ) ) cfg . update ( self . enabled queries [ cfg [ 'query name' ] ] or { } ) result [ cfg [ 'query name' ] ] = cfg return result", "predictions": ["reads the nearest configuration file and builds a dictionary of config objects list list"], "references": ["load queries configuration ."], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 3873, "code": "def consume ( self , event type , no ack = True , payload = True ) : assert event type in self . events return current queues . queues [ 'stats-{}' . format ( event type ) ] . consume ( payload = payload )", "predictions": ["encrypt an host and encrypt it to the please install install it ."], "references": ["comsume all pending events ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 3874, "code": "def init app ( self , app , entry point group events = 'invenio stats.events' , entry point group aggs = 'invenio stats.aggregations' , entry point group queries = 'invenio stats.queries' ) : self . init config ( app ) state = Invenio Stats State ( app , entry point group events = entry point group events , entry point group aggs = entry point group aggs , entry point group queries = entry point group queries ) self . state = app . extensions [ 'invenio-stats' ] = state if app . config [ 'STATS REGISTER RECEIVERS' ] : signal receivers = { key : value for key , value in app . config . get ( 'STATS EVENTS' , { } ) . items ( ) if 'signal' in value } register receivers ( app , signal receivers ) return state", "predictions": ["flask application initialization else"], "references": ["flask application initialization ."], "bleu": 0.668740304976422, "rouge_l": 0.75}
{"id": 3875, "code": "def get anonymization salt ( ts ) : salt key = 'stats:salt:{}' . format ( ts . date ( ) . isoformat ( ) ) salt = current cache . get ( salt key ) if not salt : salt bytes = os . urandom ( 32 ) salt = b64encode ( salt bytes ) . decode ( 'utf-8' ) current cache . set ( salt key , salt , timeout = 60 * 60 * 24 ) return salt", "predictions": ["return the arguments to be used to retrieve the number of salt"], "references": ["get the anonymization salt based on the event timestamp s day ."], "bleu": 0.1235622127262679, "rouge_l": 0.16666666666666666}
{"id": 3876, "code": "def get geoip ( ip ) : reader = geolite2 . reader ( ) ip data = reader . get ( ip ) or { } return ip data . get ( 'country' , { } ) . get ( 'iso code' )", "predictions": ["returns the files of an file"], "references": ["lookup country for ip address ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3877, "code": "def register templates ( ) : event templates = [ current stats . events config [ e ] [ 'templates' ] for e in current stats . events config ] aggregation templates = [ current stats . aggregations config [ a ] [ 'templates' ] for a in current stats . aggregations config ] return event templates + aggregation templates", "predictions": ["set up all value value value value value . ."], "references": ["register elasticsearch templates for events ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 3878, "code": "def process events ( event types ) : results = [ ] for e in event types : processor = current stats . events [ e ] . processor class ( * * current stats . events [ e ] . processor config ) results . append ( ( e , processor . run ( ) ) ) return results", "predictions": ["main processing processing function ."], "references": ["index statistics events ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3879, "code": "def aggregate events ( aggregations , start date = None , end date = None , update bookmark = True ) : start date = dateutil parse ( start date ) if start date else None end date = dateutil parse ( end date ) if end date else None results = [ ] for a in aggregations : aggr cfg = current stats . aggregations [ a ] aggregator = aggr cfg . aggregator class ( name = aggr cfg . name , * * aggr cfg . aggregator config ) results . append ( aggregator . run ( start date , end date , update bookmark ) ) return results", "predictions": ["convert events to aggr"], "references": ["aggregate indexed events ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 3880, "code": "def handle request ( self , scheme , netloc , path , headers , body = None , method = \"GET\" ) : backend url = \"{}://{}{}\" . format ( scheme , netloc , path ) try : response = self . http request . request ( backend url , method = method , body = body , headers = dict ( headers ) ) self . return response ( response ) except Exception as e : body = \"Invalid response from backend: '{}' Server might be busy\" . format ( e . message ) logging . debug ( body ) self . send error ( httplib . SERVICE UNAVAILABLE , body )", "predictions": ["get transition transition from server"], "references": ["run the actual request"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3881, "code": "def hash id ( iso timestamp , msg ) : return '{0}-{1}' . format ( iso timestamp , hashlib . sha1 ( msg . get ( 'unique id' ) . encode ( 'utf-8' ) + str ( msg . get ( 'visitor id' ) ) . encode ( 'utf-8' ) ) . hexdigest ( ) )", "predictions": ["generate an ctx cookies cookies for the given ctx timestamp"], "references": ["generate event id optimized for es ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 3882, "code": "def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats only = True , chunk size = 50 )", "predictions": ["ci the = true if the user is a = false otherwise ."], "references": ["process events queue ."], "bleu": 0.09552040806823771, "rouge_l": 0.1300639658848614}
{"id": 3883, "code": "def register events ( ) : return [ dict ( event type = 'file-download' , templates = 'invenio stats.contrib.file download' , processor class = Events Indexer , processor config = dict ( preprocessors = [ flag robots , anonymize user , build file unique id ] ) ) , dict ( event type = 'record-view' , templates = 'invenio stats.contrib.record view' , processor class = Events Indexer , processor config = dict ( preprocessors = [ flag robots , anonymize user , build record unique id ] ) ) ]", "predictions": ["load events events events ."], "references": ["register sample events ."], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 3884, "code": "def register aggregations ( ) : return [ dict ( aggregation name = 'file-download-agg' , templates = 'invenio stats.contrib.aggregations.aggr file download' , aggregator class = Stat Aggregator , aggregator config = dict ( client = current search client , event = 'file-download' , aggregation field = 'unique id' , aggregation interval = 'day' , copy fields = dict ( file key = 'file key' , bucket id = 'bucket id' , file id = 'file id' , ) , metric aggregation fields = { 'unique count' : ( 'cardinality' , 'unique session id' , { 'precision threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation name = 'record-view-agg' , templates = 'invenio stats.contrib.aggregations.aggr record view' , aggregator class = Stat Aggregator , aggregator config = dict ( client = current search client , event = 'record-view' , aggregation field = 'unique id' , aggregation interval = 'day' , copy fields = dict ( record id = 'record id' , pid type = 'pid type' , pid value = 'pid value' , ) , metric aggregation fields = { 'unique count' : ( 'cardinality' , 'unique session id' , { 'precision threshold' : 1000 } ) , } , ) ) ]", "predictions": ["get aggregations aggregations aggregations ."], "references": ["register sample aggregations ."], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 3885, "code": "def declare queues ( ) : return [ dict ( name = 'stats-{0}' . format ( event [ 'event type' ] ) , exchange = current stats . exchange ) for event in current stats . events config . values ( ) ]", "predictions": ["get all arrays that have been imported . ."], "references": ["index statistics events ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3886, "code": "def validate arguments ( self , interval , start date , end date , * * kwargs ) : if interval not in self . allowed intervals : raise Invalid Request Input Error ( 'Invalid aggregation time interval for statistic {}.' ) . format ( self . query name ) if set ( kwargs ) < set ( self . required filters ) : raise Invalid Request Input Error ( 'Missing one of the required parameters {0} in ' 'query {1}' . format ( set ( self . required filters . keys ( ) ) , self . query name ) )", "predictions": ["ensure that the specified interval is split splitted splitted intervals"], "references": ["validate query arguments ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3887, "code": "def build query ( self , interval , start date , end date , * * kwargs ) : agg query = Search ( using = self . client , index = self . index , doc type = self . doc type ) [ 0 : 0 ] if start date is not None or end date is not None : time range = { } if start date is not None : time range [ 'gte' ] = start date . isoformat ( ) if end date is not None : time range [ 'lte' ] = end date . isoformat ( ) agg query = agg query . filter ( 'range' , * * { self . time field : time range } ) for modifier in self . query modifiers : agg query = modifier ( agg query , * * kwargs ) base agg = agg query . aggs . bucket ( 'histogram' , 'date histogram' , field = self . time field , interval = interval ) for destination , ( metric , field , opts ) in self . metric fields . items ( ) : base agg . metric ( destination , metric , field = field , * * opts ) if self . copy fields : base agg . metric ( 'top hit' , 'top hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for query param , filtered field in self . required filters . items ( ) : if query param in kwargs : agg query = agg query . filter ( 'term' , * * { filtered field : kwargs [ query param ] } ) return agg query", "predictions": ["builds a md md object from the given interval"], "references": ["build the elasticsearch query ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 3888, "code": "def process query result ( self , query result , interval , start date , end date ) : def build buckets ( agg ) : \"\"\"Build recursively result buckets.\"\"\" bucket result = dict ( key = agg [ 'key' ] , date = agg [ 'key as string' ] , ) for metric in self . metric fields : bucket result [ metric ] = agg [ metric ] [ 'value' ] if self . copy fields and agg [ 'top hit' ] [ 'hits' ] [ 'hits' ] : doc = agg [ 'top hit' ] [ 'hits' ] [ 'hits' ] [ 0 ] [ ' source' ] for destination , source in self . copy fields . items ( ) : if isinstance ( source , six . string types ) : bucket result [ destination ] = doc [ source ] else : bucket result [ destination ] = source ( bucket result , doc ) return bucket result buckets = query result [ 'aggregations' ] [ 'histogram' ] [ 'buckets' ] return dict ( interval = interval , key type = 'date' , start date = start date . isoformat ( ) if start date else None , end date = end date . isoformat ( ) if end date else None , buckets = [ build buckets ( b ) for b in buckets ] )", "predictions": ["register result result result"], "references": ["build the result using the query result ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 3889, "code": "def validate arguments ( self , start date , end date , * * kwargs ) : if set ( kwargs ) < set ( self . required filters ) : raise Invalid Request Input Error ( 'Missing one of the required parameters {0} in ' 'query {1}' . format ( set ( self . required filters . keys ( ) ) , self . query name ) )", "predictions": ["ensure that the specified with the specified with the specified with the specified with ."], "references": ["validate query arguments ."], "bleu": 0.08225964699966554, "rouge_l": 0.11753371868978806}
{"id": 3890, "code": "def build query ( self , start date , end date , * * kwargs ) : agg query = Search ( using = self . client , index = self . index , doc type = self . doc type ) [ 0 : 0 ] if start date is not None or end date is not None : time range = { } if start date is not None : time range [ 'gte' ] = start date . isoformat ( ) if end date is not None : time range [ 'lte' ] = end date . isoformat ( ) agg query = agg query . filter ( 'range' , * * { self . time field : time range } ) for modifier in self . query modifiers : agg query = modifier ( agg query , * * kwargs ) base agg = agg query . aggs def apply metric aggs ( agg ) : for dst , ( metric , field , opts ) in self . metric fields . items ( ) : agg . metric ( dst , metric , field = field , * * opts ) apply metric aggs ( base agg ) if self . aggregated fields : cur agg = base agg for term in self . aggregated fields : cur agg = cur agg . bucket ( term , 'terms' , field = term , size = 0 ) apply metric aggs ( cur agg ) if self . copy fields : base agg . metric ( 'top hit' , 'top hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for query param , filtered field in self . required filters . items ( ) : if query param in kwargs : agg query = agg query . filter ( 'term' , * * { filtered field : kwargs [ query param ] } ) return agg query", "predictions": ["builds a oldest oldest oldest oldest object and builds the if needed"], "references": ["build the elasticsearch query ."], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 3891, "code": "def process query result ( self , query result , start date , end date ) : def build buckets ( agg , fields , bucket result ) : \"\"\"Build recursively result buckets.\"\"\" for metric in self . metric fields : bucket result [ metric ] = agg [ metric ] [ 'value' ] if fields : current level = fields [ 0 ] bucket result . update ( dict ( type = 'bucket' , field = current level , key type = 'terms' , buckets = [ build buckets ( b , fields [ 1 : ] , dict ( key = b [ 'key' ] ) ) for b in agg [ current level ] [ 'buckets' ] ] ) ) return bucket result aggs = query result [ 'aggregations' ] result = dict ( start date = start date . isoformat ( ) if start date else None , end date = end date . isoformat ( ) if end date else None , ) if self . copy fields and aggs [ 'top hit' ] [ 'hits' ] [ 'hits' ] : doc = aggs [ 'top hit' ] [ 'hits' ] [ 'hits' ] [ 0 ] [ ' source' ] for destination , source in self . copy fields . items ( ) : if isinstance ( source , six . string types ) : result [ destination ] = doc [ source ] else : result [ destination ] = source ( result , doc ) return build buckets ( aggs , self . aggregated fields , result )", "predictions": ["get result result result result"], "references": ["build the result using the query result ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3892, "code": "def run ( self , start date = None , end date = None , * * kwargs ) : start date = self . extract date ( start date ) if start date else None end date = self . extract date ( end date ) if end date else None self . validate arguments ( start date , end date , * * kwargs ) agg query = self . build query ( start date , end date , * * kwargs ) query result = agg query . execute ( ) . to dict ( ) res = self . process query result ( query result , start date , end date ) return res", "predictions": ["executes a single } } } and returns a list of the results ."], "references": ["run the query ."], "bleu": 0.09782375748961449, "rouge_l": 0.2469635627530364}
{"id": 3893, "code": "def file download event builder ( event , sender app , obj = None , * * kwargs ) : event . update ( dict ( timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , bucket id = str ( obj . bucket id ) , file id = str ( obj . file id ) , file key = obj . key , size = obj . file . size , referrer = request . referrer , * * get user ( ) ) ) return event", "predictions": ["range dt for format dt types types types types types types types types ."], "references": ["build a file - download event ."], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 3894, "code": "def record view event builder ( event , sender app , pid = None , record = None , * * kwargs ) : event . update ( dict ( timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , record id = str ( record . id ) , pid type = pid . pid type , pid value = str ( pid . pid value ) , referrer = request . referrer , * * get user ( ) ) ) return event", "predictions": ["view self not builder not . not . not ."], "references": ["build a record - view event ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 3895, "code": "def is root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except Attribute Error : return ctypes . windll . shell32 . Is User An Admin ( ) != 0 return False", "predictions": ["end of function that returns true if the current file is a bookmarks bookmarks limit limit limit limit ."], "references": ["checks if the user is rooted ."], "bleu": 0.09629943614188137, "rouge_l": 0.3356258596973865}
{"id": 3896, "code": "def to jupyter ( graph : BEL Graph , chart : Optional [ str ] = None ) -> Javascript : with open ( os . path . join ( HERE , 'render with javascript.js' ) , 'rt' ) as f : js template = Template ( f . read ( ) ) return Javascript ( js template . render ( * * get context ( graph , chart = chart ) ) )", "predictions": ["convert a graph to a jupyter file using jupyter"], "references": ["render the graph as javascript in a jupyter notebook ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 3897, "code": "def prerender ( graph : BEL Graph ) -> Mapping [ str , Mapping [ str , Any ] ] : import bio2bel hgnc from bio2bel hgnc . models import Human Gene graph : BEL Graph = graph . copy ( ) enrich protein and rna origins ( graph ) collapse all variants ( graph ) genes : Set [ Gene ] = get nodes by function ( graph , GENE ) hgnc symbols = { gene . name for gene in genes if gene . namespace . lower ( ) == 'hgnc' } result = { } hgnc manager = bio2bel hgnc . Manager ( ) human genes = ( hgnc manager . session . query ( Human Gene . symbol , Human Gene . location ) . filter ( Human Gene . symbol . in ( hgnc symbols ) ) . all ( ) ) for human gene in human genes : result [ human gene . symbol ] = { 'name' : human gene . symbol , 'chr' : ( human gene . location . split ( 'q' ) [ 0 ] if 'q' in human gene . location else human gene . location . split ( 'p' ) [ 0 ] ) , } df = get df ( ) for , ( gene id , symbol , start , stop ) in df [ df [ 'Symbol' ] . isin ( hgnc symbols ) ] . iterrows ( ) : result [ symbol ] [ 'start' ] = start result [ symbol ] [ 'stop' ] = stop return result", "predictions": ["collapse a self . using a self . ."], "references": ["generate the annotations json for ideogram ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3898, "code": "def get correlation graph ( graph : BEL Graph ) -> Graph : result = Graph ( ) for u , v , d in graph . edges ( data = True ) : if d [ RELATION ] not in CORRELATIVE RELATIONS : continue if not result . has edge ( u , v ) : result . add edge ( u , v , * * { d [ RELATION ] : True } ) elif d [ RELATION ] not in result [ u ] [ v ] : log . log ( 5 , 'broken correlation relation for %s, %s' , u , v ) result [ u ] [ v ] [ d [ RELATION ] ] = True result [ v ] [ u ] [ d [ RELATION ] ] = True return result", "predictions": ["return process graph graph"], "references": ["extract an undirected graph of only correlative relationships ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3899, "code": "def get correlation triangles ( graph : BEL Graph ) -> Set Of Node Triples : return { tuple ( sorted ( [ n , u , v ] , key = str ) ) for n in graph for u , v in itt . combinations ( graph [ n ] , 2 ) if graph . has edge ( u , v ) }", "predictions": ["eager process triangles click click click click click click click click click click click click click click click click click click click click click click click on the types click click"], "references": ["return a set of all triangles pointed by the given node ."], "bleu": 0.04317900023606586, "rouge_l": 0.10107705053852528}
{"id": 3900, "code": "def summarize stability ( graph : BEL Graph ) -> Mapping [ str , int ] : regulatory pairs = get regulatory pairs ( graph ) chaotic pairs = get chaotic pairs ( graph ) dampened pairs = get dampened pairs ( graph ) contraditory pairs = get contradiction summary ( graph ) separately unstable triples = get separate unstable correlation triples ( graph ) mutually unstable triples = get mutually unstable correlation triples ( graph ) jens unstable triples = get jens unstable ( graph ) increase mismatch triples = get increase mismatch triplets ( graph ) decrease mismatch triples = get decrease mismatch triplets ( graph ) chaotic triples = get chaotic triplets ( graph ) dampened triples = get dampened triplets ( graph ) return { 'Regulatory Pairs' : count or len ( regulatory pairs ) , 'Chaotic Pairs' : count or len ( chaotic pairs ) , 'Dampened Pairs' : count or len ( dampened pairs ) , 'Contradictory Pairs' : count or len ( contraditory pairs ) , 'Separately Unstable Triples' : count or len ( separately unstable triples ) , 'Mutually Unstable Triples' : count or len ( mutually unstable triples ) , 'Jens Unstable Triples' : count or len ( jens unstable triples ) , 'Increase Mismatch Triples' : count or len ( increase mismatch triples ) , 'Decrease Mismatch Triples' : count or len ( decrease mismatch triples ) , 'Chaotic Triples' : count or len ( chaotic triples ) , 'Dampened Triples' : count or len ( dampened triples ) }", "predictions": ["return delete delete delete delete delete delete delete delete aggregator ."], "references": ["summarize the stability of the graph ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 3901, "code": "def flatten list abundance ( node : List Abundance ) -> List Abundance : return node . class ( list ( chain . from iterable ( ( flatten list abundance ( member ) . members if isinstance ( member , List Abundance ) else [ member ] ) for member in node . members ) ) )", "predictions": ["aggregations a list of stats into a list of lists end ."], "references": ["flattens the complex or composite abundance ."], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 3902, "code": "def list abundance expansion ( graph : BEL Graph ) -> None : mapping = { node : flatten list abundance ( node ) for node in graph if isinstance ( node , List Abundance ) } relabel nodes ( graph , mapping , copy = False )", "predictions": ["events from a events into a events ."], "references": ["flatten list abundances ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3903, "code": "def list abundance cartesian expansion ( graph : BEL Graph ) -> None : for u , v , k , d in list ( graph . edges ( keys = True , data = True ) ) : if CITATION not in d : continue if isinstance ( u , List Abundance ) and isinstance ( v , List Abundance ) : for u member , v member in itt . product ( u . members , v . members ) : graph . add qualified edge ( u member , v member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , List Abundance ) : for member in u . members : graph . add qualified edge ( member , v , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , List Abundance ) : for member in v . members : graph . add qualified edge ( u , member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) remove list abundance nodes ( graph )", "predictions": ["aggregations config nodes nodes to cartesian aggs aggs aggs aggs aggs aggs aggs aggs"], "references": ["expand all list abundances to simple subject - predicate - object networks ."], "bleu": 0.08839374326825923, "rouge_l": 0.0745721271393643}
{"id": 3904, "code": "def reaction cartesion expansion unqualified helper ( graph : BEL Graph , u : Base Entity , v : Base Entity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = get catalysts in reaction ( u ) | get catalysts in reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add unqualified edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add unqualified edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = get catalysts in reaction ( u ) for product in u . products : if product in enzymes : continue if v not in u . products and v not in u . reactants : graph . add unqualified edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add unqualified edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = get catalysts in reaction ( v ) for reactant in v . reactants : if reactant in enzymes : continue if u not in v . products and u not in v . reactants : graph . add unqualified edge ( u , reactant , INCREASES ) for product in v . products : graph . add unqualified edge ( reactant , product , INCREASES )", "predictions": ["add unqualified unqualified unqualified to a graph"], "references": ["helper to deal with cartension expansion in unqualified edges ."], "bleu": 0.13391424795650428, "rouge_l": 0.11401869158878504}
{"id": 3905, "code": "def get catalysts in reaction ( reaction : Reaction ) -> Set [ Base Abundance ] : return { reactant for reactant in reaction . reactants if reactant in reaction . products }", "predictions": ["get all catalysts in a reaction"], "references": ["return nodes that are both in reactants and reactions in a reaction ."], "bleu": 0.12802833376249098, "rouge_l": 0.29611650485436897}
{"id": 3906, "code": "def get graphs by ids ( self , network ids : Iterable [ int ] ) -> List [ BEL Graph ] : return [ self . networks [ network id ] for network id in network ids ]", "predictions": ["returns a list of all networks with a given network ids ."], "references": ["get several graphs by their identifiers ."], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 3907, "code": "def count author publications ( graph : BEL Graph ) -> typing . Counter [ str ] : authors = group as dict ( iter author publiations ( graph ) ) return Counter ( count dict values ( count defaultdict ( authors ) ) )", "predictions": ["count the number of author publications in a graph ."], "references": ["count the number of publications of each author to the given graph ."], "bleu": 0.3183254055686273, "rouge_l": 0.5947075208913649}
{"id": 3908, "code": "def count citation years ( graph : BEL Graph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for , , data in graph . edges ( data = True ) : if CITATION not in data or CITATION DATE not in data [ CITATION ] : continue try : dt = ensure datetime ( data [ CITATION ] [ CITATION DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION TYPE ] , data [ CITATION ] [ CITATION REFERENCE ] ) ) except Exception : continue return count dict values ( result )", "predictions": ["get the number of citation that exist in a graph ."], "references": ["count the number of citations from each year ."], "bleu": 0.22416933501922287, "rouge_l": 0.4073455759599332}
{"id": 3909, "code": "def get citation years ( graph : BEL Graph ) -> List [ Tuple [ int , int ] ] : return create timeline ( count citation years ( graph ) )", "predictions": ["return a list of citation years for a given graph ."], "references": ["create a citation timeline counter from the graph ."], "bleu": 0.17033186037639278, "rouge_l": 0.4073455759599332}
{"id": 3910, "code": "def count confidences ( graph : BEL Graph ) -> typing . Counter [ str ] : return Counter ( ( 'None' if ANNOTATIONS not in data or 'Confidence' not in data [ ANNOTATIONS ] else list ( data [ ANNOTATIONS ] [ 'Confidence' ] ) [ 0 ] ) for , , data in graph . edges ( data = True ) if CITATION in data )", "predictions": ["count the number of edges in a graph"], "references": ["count the confidences in the graph ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 3911, "code": "def update context ( universe : BEL Graph , graph : BEL Graph ) : for namespace in get namespaces ( graph ) : if namespace in universe . namespace url : graph . namespace url [ namespace ] = universe . namespace url [ namespace ] elif namespace in universe . namespace pattern : graph . namespace pattern [ namespace ] = universe . namespace pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get annotations ( graph ) : if annotation in universe . annotation url : graph . annotation url [ annotation ] = universe . annotation url [ annotation ] elif annotation in universe . annotation pattern : graph . annotation pattern [ annotation ] = universe . annotation pattern [ annotation ] elif annotation in universe . annotation list : graph . annotation list [ annotation ] = universe . annotation list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )", "predictions": ["update context from universe' ."], "references": ["update the context of a subgraph from the universe of all knowledge ."], "bleu": 0.06930996903910726, "rouge_l": 0.41146711635750427}
{"id": 3912, "code": "def count top centrality ( graph : BEL Graph , number : Optional [ int ] = 30 ) -> Mapping [ Base Entity , int ] : dd = nx . betweenness centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most common ( number ) )", "predictions": ["return the number of centrality centrality in a graph ."], "references": ["get top centrality dictionary ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 3913, "code": "def get modifications count ( graph : BEL Graph ) -> Mapping [ str , int ] : return remove falsy values ( { 'Translocations' : len ( get translocated ( graph ) ) , 'Degradations' : len ( get degradations ( graph ) ) , 'Molecular Activities' : len ( get activities ( graph ) ) , } )", "predictions": ["get the number of modifications modifications in a graph"], "references": ["get a modifications count dictionary ."], "bleu": 0.16784459625186196, "rouge_l": 0.27664399092970515}
{"id": 3914, "code": "def remove falsy values ( counter : Mapping [ Any , int ] ) -> Mapping [ Any , int ] : return { label : count for label , count in counter . items ( ) if count }", "predictions": ["remove falsy values from counter ."], "references": ["remove all values that are zero ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 3915, "code": "def collapse variants by function ( graph : BEL Graph , func : str ) -> None : for parent node , variant node , data in graph . edges ( data = True ) : if data [ RELATION ] == HAS VARIANT and parent node . function == func : collapse pair ( graph , from node = variant node , to node = parent node )", "predictions": ["collapse all variants by a function that will be called on a function ."], "references": ["collapse all of the given functions variants edges to their parents in - place ."], "bleu": 0.1230673905899703, "rouge_l": 0.2741573033707865}
{"id": 3916, "code": "def collapse edge passing predicates ( graph : BEL Graph , edge predicates : Edge Predicates = None ) -> None : for u , v , in filter edges ( graph , edge predicates = edge predicates ) : collapse pair ( graph , survivor = u , victim = v )", "predictions": ["collapse all edges that were predicates ."], "references": ["collapse all edges passing the given edge predicates ."], "bleu": 0.30895757752065417, "rouge_l": 0.6112224448897796}
{"id": 3917, "code": "def collapse entrez equivalencies ( graph : BEL Graph ) : relation filter = build relation predicate ( EQUIVALENT TO ) source namespace filter = build source namespace filter ( [ 'EGID' , 'EG' , 'ENTREZ' ] ) edge predicates = [ relation filter , source namespace filter , ] collapse edge passing predicates ( graph , edge predicates = edge predicates )", "predictions": ["collapse predicates to predicates ."], "references": ["collapse all equivalence edges away from entrez . assumes well formed 2 - way equivalencies ."], "bleu": 0.03347779366253814, "rouge_l": 0.17403708987161198}
{"id": 3918, "code": "def collapse nodes with same names ( graph : BEL Graph ) -> None : survivor mapping = defaultdict ( set ) victims = set ( ) it = tqdm ( itt . combinations ( graph , r = 2 ) , total = graph . number of nodes ( ) * ( graph . number of nodes ( ) - 1 ) / 2 ) for a , b in it : if b in victims : continue a name , b name = a . get ( NAME ) , b . get ( NAME ) if not a name or not b name or a name . lower ( ) != b name . lower ( ) : continue if a . keys ( ) != b . keys ( ) : continue for k in set ( a . keys ( ) ) - { NAME , NAMESPACE } : if a [ k ] != b [ k ] : continue survivor mapping [ a ] . add ( b ) victims . add ( b ) collapse nodes ( graph , survivor mapping )", "predictions": ["collapse nodes with same names ."], "references": ["collapse all nodes with the same name merging namespaces by picking first alphabetical one ."], "bleu": 0.07714073404379711, "rouge_l": 0.44202898550724634}
{"id": 3919, "code": "def main ( output ) : from hbp knowledge import get graph graph = get graph ( ) text = to html ( graph ) print ( text , file = output )", "predictions": ["get graph and output graph as html"], "references": ["output the hbp knowledge graph to the desktop"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3920, "code": "def enrich complexes ( graph : BEL Graph ) -> None : nodes = list ( get nodes by function ( graph , COMPLEX ) ) for u in nodes : for v in u . members : graph . add has component ( u , v )", "predictions": ["enrich a graph with a list of nodes that are not present in the graph ."], "references": ["add all of the members of the complex abundances to the graph ."], "bleu": 0.1513851459876605, "rouge_l": 0.28110599078341014}
{"id": 3921, "code": "def enrich composites ( graph : BEL Graph ) : nodes = list ( get nodes by function ( graph , COMPOSITE ) ) for u in nodes : for v in u . members : graph . add has component ( u , v )", "predictions": ["enrich all the composites that are defined in the current graph ."], "references": ["adds all of the members of the composite abundances to the graph ."], "bleu": 0.14961487835433152, "rouge_l": 0.39713541666666663}
{"id": 3922, "code": "def enrich reactions ( graph : BEL Graph ) : nodes = list ( get nodes by function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add has reactant ( u , v ) for v in u . products : graph . add has product ( u , v )", "predictions": ["enrich a graph with reactions ."], "references": ["adds all of the reactants and products of reactions to the graph ."], "bleu": 0.08180282100568384, "rouge_l": 0.19741100323624597}
{"id": 3923, "code": "def get namespaces with incorrect names ( graph : BEL Graph ) -> Set [ str ] : return { exc . namespace for , exc , in graph . warnings if isinstance ( exc , ( Missing Namespace Name Warning , Missing Namespace Regex Warning ) ) }", "predictions": ["return a list of namespaces with the given names ."], "references": ["return the set of all namespaces with incorrect names in the graph ."], "bleu": 0.15705810592958255, "rouge_l": 0.5097493036211699}
{"id": 3924, "code": "def get undefined namespaces ( graph : BEL Graph ) -> Set [ str ] : return { exc . namespace for , exc , in graph . warnings if isinstance ( exc , Undefined Namespace Warning ) }", "predictions": ["return a list of undefined namespaces ."], "references": ["get all namespaces that are used in the bel graph aren t actually defined ."], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 3925, "code": "def count defaultdict ( dict of lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict of lists . items ( ) }", "predictions": ["counts a dictionary of dicts of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists"], "references": ["count the number of elements in each value of the dictionary ."], "bleu": 0.046398855339878003, "rouge_l": 0.10107705053852528}
{"id": 3926, "code": "def tanimoto set similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) union = a | b if not union : return 0.0 return len ( a & b ) / len ( union )", "predictions": ["tanimoto similarity with a and a set of similarity ."], "references": ["calculate the tanimoto set similarity ."], "bleu": 0.18850319022747347, "rouge_l": 0.5236051502145923}
{"id": 3927, "code": "def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )", "predictions": ["plot a single item on a plt ."], "references": ["a convenience function for plotting a horizontal bar plot from a counter"], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 3928, "code": "def barv ( d , plt , title = None , rotation = 'vertical' ) : labels = sorted ( d , key = d . get , reverse = True ) index = range ( len ( labels ) ) plt . xticks ( index , labels , rotation = rotation ) plt . bar ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )", "predictions": ["plot a bar bar ."], "references": ["a convenience function for plotting a vertical bar plot from a counter"], "bleu": 0.08006212224540951, "rouge_l": 0.2190305206463196}
{"id": 3929, "code": "def relation set has contradictions ( relations : Set [ str ] ) -> bool : has increases = any ( relation in CAUSAL INCREASE RELATIONS for relation in relations ) has decreases = any ( relation in CAUSAL DECREASE RELATIONS for relation in relations ) has cnc = any ( relation == CAUSES NO CHANGE for relation in relations ) return 1 < sum ( [ has cnc , has decreases , has increases ] )", "predictions": ["return true if a relation relations is a contradictions"], "references": ["return if the set of bel relations contains a contradiction ."], "bleu": 0.14211011212459496, "rouge_l": 0.3929146537842191}
{"id": 3930, "code": "def group nodes by annotation ( graph : BEL Graph , annotation : str = 'Subgraph' ) -> Mapping [ str , Set [ Base Entity ] ] : result = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge has annotation ( d , annotation ) : continue result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( u ) result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( v ) return dict ( result )", "predictions": ["group nodes by annotation ."], "references": ["group the nodes occurring in edges by the given annotation ."], "bleu": 0.12869637315183777, "rouge_l": 0.5854126679462572}
{"id": 3931, "code": "def build expand node neighborhood by hash ( manager : Manager ) -> Callable [ [ BEL Graph , BEL Graph , str ] , None ] : @ uni in place transformation def expand node neighborhood by hash ( universe : BEL Graph , graph : BEL Graph , node hash : str ) -> None : \"\"\"Expand around the neighborhoods of a node by identifier.\"\"\" node = manager . get dsl by hash ( node hash ) return expand node neighborhood ( universe , graph , node ) return expand node neighborhood by hash", "predictions": ["build a expand node for a given manager ."], "references": ["make an expand function that s bound to the manager ."], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 3932, "code": "def build delete node by hash ( manager : Manager ) -> Callable [ [ BEL Graph , str ] , None ] : @ in place transformation def delete node by hash ( graph : BEL Graph , node hash : str ) -> None : \"\"\"Remove a node by identifier.\"\"\" node = manager . get dsl by hash ( node hash ) graph . remove node ( node ) return delete node by hash", "predictions": ["build a node by its hash and hash it with a given hash ."], "references": ["make a delete function that s bound to the manager ."], "bleu": 0.09782375748961449, "rouge_l": 0.16353887399463804}
{"id": 3933, "code": "def update spia matrices ( spia matrices : Dict [ str , pd . Data Frame ] , u : Central Dogma , v : Central Dogma , edge data : Edge Data , ) -> None : if u . namespace . upper ( ) != 'HGNC' or v . namespace . upper ( ) != 'HGNC' : return u name = u . name v name = v . name relation = edge data [ RELATION ] if relation in CAUSAL INCREASE RELATIONS : if v . variants and any ( isinstance ( variant , Protein Modification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , Protein Modification ) : continue if variant [ IDENTIFIER ] [ NAME ] == \"Ub\" : spia matrices [ \"activation ubiquination\" ] [ u name ] [ v name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == \"Ph\" : spia matrices [ \"activation phosphorylation\" ] [ u name ] [ v name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : spia matrices [ 'expression' ] [ u name ] [ v name ] = 1 else : spia matrices [ 'activation' ] [ u name ] [ v name ] = 1 elif relation in CAUSAL DECREASE RELATIONS : if v . variants and any ( isinstance ( variant , Protein Modification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , Protein Modification ) : continue if variant [ IDENTIFIER ] [ NAME ] == \"Ub\" : spia matrices [ 'inhibition ubiquination' ] [ u name ] [ v name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == \"Ph\" : spia matrices [ \"inhibition phosphorylation\" ] [ u name ] [ v name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : spia matrices [ \"repression\" ] [ u name ] [ v name ] = 1 else : spia matrices [ \"inhibition\" ] [ u name ] [ v name ] = 1 elif relation == ASSOCIATION : spia matrices [ \"binding association\" ] [ u name ] [ v name ] = 1", "predictions": ["update spia matrices with spia"], "references": ["populate the adjacency matrix ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3934, "code": "def spia matrices to tsvs ( spia matrices : Mapping [ str , pd . Data Frame ] , directory : str ) -> None : os . makedirs ( directory , exist ok = True ) for relation , df in spia matrices . items ( ) : df . to csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )", "predictions": ["export a dataframe to a csv file ."], "references": ["export a spia data dictionary into a directory as several tsv documents ."], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 3935, "code": "def main ( graph : BEL Graph , xlsx : str , tsvs : str ) : if not xlsx and not tsvs : click . secho ( 'Specify at least one option --xlsx or --tsvs' , fg = 'red' ) sys . exit ( 1 ) spia matrices = bel to spia matrices ( graph ) if xlsx : spia matrices to excel ( spia matrices , xlsx ) if tsvs : spia matrices to tsvs ( spia matrices , tsvs )", "predictions": ["main function for the graph ."], "references": ["export the graph to a spia excel sheet ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 3936, "code": "def get entrez gene data ( entrez ids : Iterable [ Union [ str , int ] ] ) : url = PUBMED GENE QUERY URL . format ( ',' . join ( str ( x ) . strip ( ) for x in entrez ids ) ) response = requests . get ( url ) tree = Element Tree . fromstring ( response . content ) return { element . attrib [ 'uid' ] : { 'summary' : sanitize ( element . find ( 'Summary' ) . text ) , 'description' : element . find ( 'Description' ) . text } for element in tree . findall ( './Document Summary Set/Document Summary' ) }", "predictions": ["query the entrez unqualified expansion expansion unqualified xml file"], "references": ["get gene info from entrez ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3937, "code": "def get largest component ( graph : BEL Graph ) -> BEL Graph : biggest component nodes = max ( nx . weakly connected components ( graph ) , key = len ) return subgraph ( graph , biggest component nodes )", "predictions": ["} function to get the largest in a graph"], "references": ["get the giant component of a graph ."], "bleu": 0.23356898886410005, "rouge_l": 0.4756335282651072}
{"id": 3938, "code": "def self edge filter ( : BEL Graph , source : Base Entity , target : Base Entity , : str ) -> bool : return source == target", "predictions": ["return up the by source ."], "references": ["check if the source and target nodes are the same ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 3939, "code": "def has protein modification increases activity ( graph : BEL Graph , source : Base Entity , target : Base Entity , key : str , ) -> bool : edge data = graph [ source ] [ target ] [ key ] return has protein modification ( graph , source ) and part has modifier ( edge data , OBJECT , ACTIVITY )", "predictions": ["defaultdict count if a author count count count count is a author ."], "references": ["check if pmod of source causes activity of target ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 3940, "code": "def has degradation increases activity ( data : Dict ) -> bool : return part has modifier ( data , SUBJECT , DEGRADATION ) and part has modifier ( data , OBJECT , ACTIVITY )", "predictions": ["check if a data count count is a citation of the years ."], "references": ["check if the degradation of source causes activity of target ."], "bleu": 0.14949751774990683, "rouge_l": 0.33841886269070737}
{"id": 3941, "code": "def has translocation increases activity ( data : Dict ) -> bool : return part has modifier ( data , SUBJECT , TRANSLOCATION ) and part has modifier ( data , OBJECT , ACTIVITY )", "predictions": ["check if a data get a citation activity activity"], "references": ["check if the translocation of source causes activity of target ."], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 3942, "code": "def complex has member ( graph : BEL Graph , complex node : Complex Abundance , member node : Base Entity ) -> bool : return any ( v == member node for , v , data in graph . out edges ( complex node , data = True ) if data [ RELATION ] == HAS COMPONENT )", "predictions": ["checks if a member has has has has has has is a member member ."], "references": ["does the given complex contain the member?"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3943, "code": "def complex increases activity ( graph : BEL Graph , u : Base Entity , v : Base Entity , key : str ) -> bool : return ( isinstance ( u , ( Complex Abundance , Named Complex Abundance ) ) and complex has member ( graph , u , v ) and part has modifier ( graph [ u ] [ v ] [ key ] , OBJECT , ACTIVITY ) )", "predictions": ["build a context that join a update of a update of a update ."], "references": ["return if the formation of a complex with u increases the activity of v ."], "bleu": 0.1230673905899703, "rouge_l": 0.2741573033707865}
{"id": 3944, "code": "def find activations ( graph : BEL Graph ) : for u , v , key , data in graph . edges ( keys = True , data = True ) : if u != v : continue bel = graph . edge to bel ( u , v , data ) line = data . get ( LINE ) if line is None : continue elif has protein modification increases activity ( graph , u , v , key ) : print ( line , '- pmod changes -' , bel ) find related ( graph , v , data ) elif has degradation increases activity ( data ) : print ( line , '- degradation changes -' , bel ) find related ( graph , v , data ) elif has translocation increases activity ( data ) : print ( line , '- translocation changes -' , bel ) find related ( graph , v , data ) elif complex increases activity ( graph , u , v , key ) : print ( line , '- complex changes - ' , bel ) find related ( graph , v , data ) elif has same subject object ( graph , u , v , key ) : print ( line , '- same sub/obj -' , bel ) else : print ( line , '- *** - ' , bel )", "predictions": ["count all top top top level activity 30 30 activity 30 30 30 edges 30 30 to ."], "references": ["find edges that are a - a meaning that some conditions in the edge best describe the interaction ."], "bleu": 0.0712859743405913, "rouge_l": 0.10758377425044091}
{"id": 3945, "code": "def summarize edge filter ( graph : BEL Graph , edge predicates : Edge Predicates ) -> None : passed = count passed edge filter ( graph , edge predicates ) print ( '{}/{} edges passed {}' . format ( passed , graph . number of edges ( ) , ( ', ' . join ( edge filter . name for edge filter in edge predicates ) if isinstance ( edge predicates , Iterable ) else edge predicates . name ) ) )", "predictions": ["list all predicates that have a given modifications len len len len len len len len len predicates len len len len len len len len len len len len them"], "references": ["print a summary of the number of edges passing a given set of filters ."], "bleu": 0.0513487742994337, "rouge_l": 0.09277566539923954}
{"id": 3946, "code": "def node has namespace ( node : Base Entity , namespace : str ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns == namespace", "predictions": ["check if a remove remove remove a remove remove remove a values return true if it s a values falsy"], "references": ["pass for nodes that have the given namespace ."], "bleu": 0.051366639095059514, "rouge_l": 0.0}
{"id": 3947, "code": "def node has namespaces ( node : Base Entity , namespaces : Set [ str ] ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns in namespaces", "predictions": ["check if collapse collapse variants variants node node node node node node node node node node node node ."], "references": ["pass for nodes that have one of the given namespaces ."], "bleu": 0.06439931429457924, "rouge_l": 0.07003444316877153}
{"id": 3948, "code": "def get cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0", "predictions": ["collapse a edge from a edge edge"], "references": ["assign if a value is greater than or less than a cutoff ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 3949, "code": "def one sided ( value : float , distribution : List [ float ] ) -> float : assert distribution return sum ( value < element for element in distribution ) / len ( distribution )", "predictions": ["convert a list of entrez numbers to a float ."], "references": ["calculate the one - sided probability of getting a value more extreme than the distribution ."], "bleu": 0.08227293930285365, "rouge_l": 0.22154963680387407}
{"id": 3950, "code": "def get drug target interactions ( manager : Optional [ 'bio2bel drugbank.manager' ] = None ) -> Mapping [ str , List [ str ] ] : if manager is None : import bio2bel drugbank manager = bio2bel drugbank . Manager ( ) if not manager . is populated ( ) : manager . populate ( ) return manager . get drug to hgnc symbols ( )", "predictions": ["returns the nodes of the nodes in the manager r r r r ."], "references": ["get a mapping from drugs to their list of gene ."], "bleu": 0.09782375748961449, "rouge_l": 0.16353887399463804}
{"id": 3951, "code": "def multi run epicom ( graphs : Iterable [ BEL Graph ] , path : Union [ None , str , Text IO ] ) -> None : if isinstance ( path , str ) : with open ( path , 'w' ) as file : multi run helper file wrapper ( graphs , file ) else : multi run helper file wrapper ( graphs , path )", "predictions": ["run a file with output and output a file"], "references": ["run epicom analysis on many graphs ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3952, "code": "def main ( ) : logging . basic Config ( level = logging . INFO ) log . set Level ( logging . INFO ) bms base = get bms base ( ) neurommsig base = get neurommsig base ( ) neurommsig excel dir = os . path . join ( neurommsig base , 'resources' , 'excels' , 'neurommsig' ) nift values = get nift values ( ) log . info ( 'Starting Alzheimers' ) ad path = os . path . join ( neurommsig excel dir , 'alzheimers' , 'alzheimers.xlsx' ) ad df = preprocess ( ad path ) with open ( os . path . join ( bms base , 'aetionomy' , 'alzheimers' , 'neurommsigdb ad.bel' ) , 'w' ) as ad file : write neurommsig bel ( ad file , ad df , mesh alzheimer , nift values ) log . info ( 'Starting Parkinsons' ) pd path = os . path . join ( neurommsig excel dir , 'parkinsons' , 'parkinsons.xlsx' ) pd df = preprocess ( pd path ) with open ( os . path . join ( bms base , 'aetionomy' , 'parkinsons' , 'neurommsigdb pd.bel' ) , 'w' ) as pd file : write neurommsig bel ( pd file , pd df , mesh parkinson , nift values )", "predictions": ["write up the for the for the for the for the for the for the for the for the for the for the for the for the for the for the"], "references": ["convert the alzheimer s and parkinson s disease neurommsig excel sheets to bel ."], "bleu": 0.03901663112717908, "rouge_l": 0.04769351055512119}
{"id": 3953, "code": "def unscored nodes iter ( self ) -> Base Entity : for node , data in self . graph . nodes ( data = True ) : if self . tag not in data : yield node", "predictions": ["iterate over all composites composites composites . . ."], "references": ["iterate over all nodes without a score ."], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 3954, "code": "def remove random edge until has leaves ( self ) -> None : while True : leaves = set ( self . iter leaves ( ) ) if leaves : return self . remove random edge ( )", "predictions": ["enrich a reactions graph graph graph graph graph graph graph graph for it for it for it for it for a given the graph for a given nodes for a given"], "references": ["remove random edges until there is at least one leaf node ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3955, "code": "def calculate score ( self , node : Base Entity ) -> float : score = ( self . graph . nodes [ node ] [ self . tag ] if self . tag in self . graph . nodes [ node ] else self . default score ) for predecessor , , d in self . graph . in edges ( node , data = True ) : if d [ RELATION ] in CAUSAL INCREASE RELATIONS : score += self . graph . nodes [ predecessor ] [ self . tag ] elif d [ RELATION ] in CAUSAL DECREASE RELATIONS : score -= self . graph . nodes [ predecessor ] [ self . tag ] return score", "predictions": ["get a namespaces namespaces for a given node ."], "references": ["calculate the new score of the given node ."], "bleu": 0.2626909894424158, "rouge_l": 0.3333333333333333}
{"id": 3956, "code": "def node exclusion filter builder ( nodes : Iterable [ Base Entity ] ) -> Node Predicate : node set = set ( nodes ) def exclusion filter ( : BEL Graph , node : Base Entity ) -> bool : return node not in node set return exclusion filter", "predictions": ["build a get namespaces namespaces for a given nodes ."], "references": ["build a filter that fails on nodes in the given list ."], "bleu": 0.16153071659734697, "rouge_l": 0.3577712609970674}
{"id": 3957, "code": "def variants of ( graph : BEL Graph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Set [ Protein ] : if modifications : return get filtered variants of ( graph , node , modifications ) return { v for u , v , key , data in graph . edges ( keys = True , data = True ) if ( u == node and data [ RELATION ] == HAS VARIANT and pybel . struct . has protein modification ( v ) ) }", "predictions": ["list of count count of nodes that are count of a node . . ."], "references": ["returns all variants of the given node ."], "bleu": 0.11633270842295028, "rouge_l": 0.2760180995475113}
{"id": 3958, "code": "def get variants to controllers ( graph : BEL Graph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Mapping [ Protein , Set [ Protein ] ] : rv = defaultdict ( set ) variants = variants of ( graph , node , modifications ) for controller , variant , data in graph . in edges ( variants , data = True ) : if data [ RELATION ] in CAUSAL RELATIONS : rv [ variant ] . add ( controller ) return rv", "predictions": ["given a list of set of set of set of set of set of set of set of set of set of set of set of set of set of set"], "references": ["get a mapping from variants of the given node to all of its upstream controllers ."], "bleu": 0.04906081629292276, "rouge_l": 0.13545521835677277}
{"id": 3959, "code": "def group dict set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )", "predictions": ["not a dict but for use as a dictionary of dictionaries if they don t exist if a dict ."], "references": ["make a dict that accumulates the values for each key in an iterator of doubles ."], "bleu": 0.09560408787521255, "rouge_l": 0.28345724907063197}
{"id": 3960, "code": "def boilerplate ( name , contact , description , pmids , version , copyright , authors , licenses , disclaimer , output ) : from . document utils import write boilerplate write boilerplate ( name = name , version = version , description = description , authors = authors , contact = contact , copyright = copyright , licenses = licenses , disclaimer = disclaimer , pmids = pmids , file = output , )", "predictions": ["print out a boilerplate instance for a domain get"], "references": ["build a template bel document with the given pubmed identifiers ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 3961, "code": "def get pmids ( graph : BEL Graph , output : Text IO ) : for pmid in get pubmed identifiers ( graph ) : click . echo ( pmid , file = output )", "predictions": ["relation set of set of set any set of set any set any set of set any set any set any set . . any set of set . any set"], "references": ["output pubmed identifiers from a graph to a stream ."], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 3962, "code": "def glob match ( self , pattern , string ) : return bool ( re . match ( fnmatch . translate ( pattern ) , string , re . M | re . U | re . L ) )", "predictions": ["check whether a string matches a graph . ."], "references": ["match given string by escaping regex characters"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3963, "code": "def get Running Apps ( cls ) : def run Loop And Exit ( ) : App Helper . stop Event Loop ( ) App Helper . call Later ( 1 , run Loop And Exit ) App Helper . run Console Event Loop ( ) ws = App Kit . NS Workspace . shared Workspace ( ) apps = ws . running Applications ( ) return apps", "predictions": ["build the certificate for the experiment"], "references": ["get a list of the running applications ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3964, "code": "def launch App By Bundle Id ( bundle ID ) : ws = App Kit . NS Workspace . shared Workspace ( ) r = ws . launch App With Bundle Identifier options additional Event Param Descriptor launch Identifier ( bundle ID , App Kit . NS Workspace Launch Allowing Classic Startup , App Kit . NS Apple Event Descriptor . null Descriptor ( ) , None ) if not r [ 0 ] : raise Runtime Error ( 'Error launching specified application.' )", "predictions": ["build a virtual specified manager . . . ."], "references": ["launch the application with the specified bundle id"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3965, "code": "def get Actions ( self ) : actions = a11y . AXUI Element . get Actions ( self ) return [ action [ 2 : ] for action in actions ]", "predictions": ["return can be a list of xml objects"], "references": ["retrieve a list of actions supported by the object ."], "bleu": 0.2324522441081612, "rouge_l": 0.3267857142857143}
{"id": 3966, "code": "def perform Action ( self , action ) : try : a11y . AXUI Element . perform Action ( self , 'AX%s' % action ) except a11y . Error Unsupported as e : sierra ver = '10.12' if mac ver ( ) [ 0 ] < sierra ver : raise e else : pass", "predictions": ["spia the action pd pd pd pd pd pd pd pd pd pd pd pd pd pd pd pd pd pd action pd pd pd pd pd pd pd pd pd"], "references": ["perform the specified action ."], "bleu": 0.04317900023606586, "rouge_l": 0.12774869109947642}
{"id": 3967, "code": "def generate Children ( self ) : try : children = self . AX Children except a11y . Error : return if children : for child in children : yield child", "predictions": ["main generator of xlsx elements ."], "references": ["generator which yields all axchildren of the object ."], "bleu": 0.1593301391270729, "rouge_l": 0.3860759493670886}
{"id": 3968, "code": "def generate Children R ( self , target = None ) : if target is None : target = self try : children = target . AX Children except a11y . Error : return if children : for child in children : yield child for c in self . generate Children R ( child ) : yield c", "predictions": ["generate a generator for a xml tree ."], "references": ["generator which recursively yields all axchildren of the object ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3969, "code": "def match Other ( self , obj , * * kwargs ) : if obj is not None : if self . find First R ( * * kwargs ) : return obj . match ( * * kwargs ) return False", "predictions": ["match object s id ."], "references": ["perform _match but on another object not self ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3970, "code": "def generate Find ( self , * * kwargs ) : for needle in self . generate Children ( ) : if needle . match ( * * kwargs ) : yield needle", "predictions": ["a generator of needle objects ."], "references": ["generator which yields matches on axchildren ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3971, "code": "def generate Find R ( self , * * kwargs ) : for needle in self . generate Children R ( ) : if needle . match ( * * kwargs ) : yield needle", "predictions": ["a generator of names that yields a tuple of names ."], "references": ["generator which yields matches on axchildren and their children ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 3972, "code": "def find All ( self , * * kwargs ) : result = [ ] for item in self . generate Find ( * * kwargs ) : result . append ( item ) return result", "predictions": ["returns a list of all rows that are not in the search database ."], "references": ["return a list of all children that match the specified criteria ."], "bleu": 0.27668736912821895, "rouge_l": 0.5460358056265985}
{"id": 3973, "code": "def get Bundle Id ( self ) : ra = App Kit . NS Running Application app = ra . running Application With Process Identifier ( self . get Pid ( ) ) return app . bundle Identifier ( )", "predictions": ["returns the bundle class for the application"], "references": ["return the bundle id of the application ."], "bleu": 0.2664731314108428, "rouge_l": 0.5269978401727862}
{"id": 3974, "code": "def pop Up Item ( self , * args ) : self . Press ( ) time . sleep ( .5 ) return self . menu Item ( self , * args )", "predictions": ["this will make the menu off the menu ."], "references": ["return the specified item in a pop up menu ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 3975, "code": "def convenience Match ( self , role , attr , match ) : kwargs = { } if match : kwargs [ attr ] = match return self . find All ( AX Role = role , * * kwargs )", "predictions": ["get a list of matches for a role"], "references": ["method used by role based convenience functions to find a match"], "bleu": 0.12197601375336842, "rouge_l": 0.10234899328859062}
{"id": 3976, "code": "def convenience Match R ( self , role , attr , match ) : kwargs = { } if match : kwargs [ attr ] = match return self . find All R ( AX Role = role , * * kwargs )", "predictions": ["get a document that matches a role"], "references": ["method used by role based convenience functions to find a match"], "bleu": 0.1160873020151595, "rouge_l": 0.10683012259194395}
{"id": 3977, "code": "def main ( port = 4118 , parentpid = None ) : if \"LDTP DEBUG\" in os . environ : ldtp debug = True else : ldtp debug = False ldtp debug file = os . environ . get ( 'LDTP DEBUG FILE' , None ) if ldtp debug : print ( \"Parent PID: {}\" . format ( int ( parentpid ) ) ) if ldtp debug file : with open ( unicode ( ldtp debug file ) , \"a\" ) as fp : fp . write ( \"Parent PID: {}\" . format ( int ( parentpid ) ) ) server = LDTP Server ( ( '' , port ) , allow none = True , log Requests = ldtp debug , request Handler = Request Handler ) server . register introspection functions ( ) server . register multicall functions ( ) ldtp inst = core . Core ( ) server . register instance ( ldtp inst ) if parentpid : thread . start new thread ( notifyclient , ( parentpid , ) ) try : server . serve forever ( ) except Keyboard Interrupt : pass except : if ldtp debug : print ( traceback . format exc ( ) ) if ldtp debug file : with open ( ldtp debug file , \"a\" ) as fp : fp . write ( traceback . format exc ( ) )", "predictions": ["main program entry point"], "references": ["main entry point . parse command line options and start up a server ."], "bleu": 0.041710075933029465, "rouge_l": 0.3029801324503311}
{"id": 3978, "code": "def server bind ( self , * args , * * kwargs ) : self . socket . setsockopt ( socket . SOL SOCKET , socket . SO REUSEADDR , 1 ) Simple XMLRPC Server . server bind ( self , * args , * * kwargs )", "predictions": ["bind to a server ."], "references": ["server bind . forces reuse of port ."], "bleu": 0.1781815298791261, "rouge_l": 0.2953995157384988}
{"id": 3979, "code": "def cast to list ( position ) : @ wrapt . decorator def wrapper ( function , instance , args , kwargs ) : if not isinstance ( args [ position ] , list ) : args = list ( args ) args [ position ] = [ args [ position ] ] args = tuple ( args ) return function ( * args , * * kwargs ) return wrapper", "predictions": ["cast input to list ."], "references": ["cast the positional argument at given position into a list if not already a list ."], "bleu": 0.04278081081211661, "rouge_l": 0.26105563480741795}
{"id": 3980, "code": "def forbidden attributes ( obj ) : for key in list ( obj . data . keys ( ) ) : if key in list ( obj . reserved keys . keys ( ) ) : obj . data . pop ( key ) return obj", "predictions": ["forbidden all forbidden attributes ."], "references": ["return the object without the forbidden attributes ."], "bleu": 0.278869164867688, "rouge_l": 0.44309927360774815}
{"id": 3981, "code": "def convert cygwin path ( path ) : try : win path = subprocess . check output ( [ \"cygpath\" , \"-aw\" , path ] , universal newlines = True ) . strip ( ) except ( File Not Found Error , subprocess . Called Process Error ) : logger . exception ( \"Call to cygpath failed.\" ) raise return win path", "predictions": ["convert cygwin path to cygwin"], "references": ["convert unix path from cygwin to windows path ."], "bleu": 0.15425217284907894, "rouge_l": 0.40757238307349664}
{"id": 3982, "code": "def get mutagen metadata ( filepath ) : try : metadata = mutagen . File ( filepath , easy = True ) except mutagen . Mutagen Error : logger . warning ( \"Can't load {} as music file.\" . format ( filepath ) ) raise return metadata", "predictions": ["get the metadata of a file as a dictionary ."], "references": ["get mutagen metadata dict from a file ."], "bleu": 0.1972940627795883, "rouge_l": 0.5669144981412639}
{"id": 3983, "code": "def mutagen fields to single value ( metadata ) : return dict ( ( k , v [ 0 ] ) for k , v in metadata . items ( ) if v )", "predictions": ["convert mutagen fields to single value ."], "references": ["replace mutagen metadata field list values in mutagen tags with the first list value ."], "bleu": 0.0837738790831083, "rouge_l": 0.2559440559440559}
{"id": 3984, "code": "def normalize metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\\/\\s*\\d+' , '' , metadata ) metadata = re . sub ( r'^0+([0-9]+)' , r'\\1' , metadata ) metadata = re . sub ( r'^\\d+\\.+' , '' , metadata ) metadata = re . sub ( r'[^\\w\\s]' , '' , metadata ) metadata = re . sub ( r'\\s+' , ' ' , metadata ) metadata = re . sub ( r'^\\s+' , '' , metadata ) metadata = re . sub ( r'\\s+$' , '' , metadata ) metadata = re . sub ( r'^the\\s+' , '' , metadata , re . I ) return metadata", "predictions": ["normalize metadata to a normalized metadata"], "references": ["normalize metadata to improve match accuracy ."], "bleu": 0.34801709319446883, "rouge_l": 0.45522388059701485}
{"id": 3985, "code": "def check field value ( field value , pattern ) : if isinstance ( field value , list ) : return any ( re . search ( pattern , str ( value ) , re . I ) for value in field value ) else : return re . search ( pattern , str ( field value ) , re . I )", "predictions": ["check that a field value contains a field value ."], "references": ["check a song metadata field value for a pattern ."], "bleu": 0.20504572236241866, "rouge_l": 0.6}
{"id": 3986, "code": "def check filters ( song , include filters = None , exclude filters = None , all includes = False , all excludes = False ) : include = True if include filters : if all includes : if not all ( field in song and check field value ( song [ field ] , pattern ) for field , pattern in include filters ) : include = False else : if not any ( field in song and check field value ( song [ field ] , pattern ) for field , pattern in include filters ) : include = False if exclude filters : if all excludes : if all ( field in song and check field value ( song [ field ] , pattern ) for field , pattern in exclude filters ) : include = False else : if any ( field in song and check field value ( song [ field ] , pattern ) for field , pattern in exclude filters ) : include = False return include", "predictions": ["include all filters that match the song ."], "references": ["check a song metadata dict against a set of metadata filters ."], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 3987, "code": "def url ( self , endpoint , url data = None , parameters = None ) : try : url = '%s/%s' % ( self . base url , self . endpoints [ endpoint ] ) except Key Error : raise End Point Does Not Exist ( endpoint ) if url data : url = url % url data if parameters : url = '%s?%s' % ( url , urllib . urlencode ( parameters , True ) ) return url", "predictions": ["build the url for the given endpoint ."], "references": ["generate url on the modularized endpoints and url parameters"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3988, "code": "def httplib2 init ( username , password ) : obj = httplib2 . Http ( ) if username and password : obj . add credentials ( username , password ) return obj", "predictions": ["initialize the httplib2 and return the result ."], "references": ["used to instantiate a regular http request object"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3989, "code": "def prepare to run ( self ) : self . clock . reset ( ) for e in self . entities : e . prepare to run ( self . clock , self . period count )", "predictions": ["prepare the entities to run the application ."], "references": ["prepare the model for execution ."], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 3990, "code": "def run ( self ) : self . prepare to run ( ) for i in range ( 0 , self . period count ) : for e in self . entities : e . run ( self . clock ) self . clock . tick ( )", "predictions": ["run the simulation ."], "references": ["execute the model ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 3991, "code": "def get ( self , key , default = None ) : if self . in memory : return self . memory db . get ( key , default ) else : db = self . read file ( ) return db . get ( key , default )", "predictions": ["retrieve a value from the cache ."], "references": ["get key value return default if key doesn t exist"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3992, "code": "def paginate link tag ( item ) : a tag = Page . default link tag ( item ) if item [ 'type' ] == 'current page' : return make html tag ( 'li' , a tag , * * { 'class' : 'blue white-text' } ) return make html tag ( 'li' , a tag )", "predictions": ["create an html tag for an item ."], "references": ["create an a - href tag that points to another page usable in paginate ."], "bleu": 0.0999647494062524, "rouge_l": 0.32972972972972975}
{"id": 3993, "code": "def set state ( id , body ) : url = DEVICE URL % id if \"mode\" in body : url = MODES URL % id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status code = str ( arequest . status code ) if status code != '202' : LOGGER . error ( \"State not accepted. \" + status code ) return False", "predictions": ["set a state ."], "references": ["set a devices state ."], "bleu": 0.4630777161991027, "rouge_l": 0.8714285714285713}
{"id": 3994, "code": "def get modes ( id ) : url = MODES URL % id arequest = requests . get ( url , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( \"Token expired.\" ) return False return arequest . json ( )", "predictions": ["get modes data from connman"], "references": ["pull a water heater s modes from the api ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 3995, "code": "def get usage ( id ) : url = USAGE URL % id arequest = requests . get ( url , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( \"Token expired.\" ) return False try : return arequest . json ( ) except Value Error : LOGGER . info ( \"Failed to get usage. Not supported by unit?\" ) return None", "predictions": ["get usage for a connman ."], "references": ["pull a water heater s usage report from the api ."], "bleu": 0.1141650334026257, "rouge_l": 0.2234432234432234}
{"id": 3996, "code": "def get device ( id ) : url = DEVICE URL % id arequest = requests . get ( url , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( \"Token expired.\" ) return False return arequest . json ( )", "predictions": ["get a device by id"], "references": ["pull a device from the api ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 3997, "code": "def get locations ( ) : arequest = requests . get ( LOCATIONS URL , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( \"Token expired.\" ) return False return arequest . json ( )", "predictions": ["get all locations available for the http request ."], "references": ["pull the accounts locations ."], "bleu": 0.16784459625186196, "rouge_l": 0.3012345679012346}
{"id": 3998, "code": "def get vacations ( ) : arequest = requests . get ( VACATIONS URL , headers = HEADERS ) status code = str ( arequest . status code ) if status code == '401' : LOGGER . error ( \"Token expired.\" ) return False return arequest . json ( )", "predictions": ["return the vacations ."], "references": ["pull the accounts vacations ."], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 3999, "code": "def create vacation ( body ) : arequest = requests . post ( VACATIONS URL , headers = HEADERS , data = json . dumps ( body ) ) status code = str ( arequest . status code ) if status code != '200' : LOGGER . error ( \"Failed to create vacation. \" + status code ) LOGGER . error ( arequest . json ( ) ) return False return arequest . json ( )", "predictions": ["create a new vacation ."], "references": ["create a vacation ."], "bleu": 0.45180100180492244, "rouge_l": 0.9070631970260222}
{"id": 4000, "code": "def delete vacation ( id ) : arequest = requests . delete ( VACATIONS URL + \"/\" + id , headers = HEADERS ) status code = str ( arequest . status code ) if status code != '202' : LOGGER . error ( \"Failed to delete vacation. \" + status code ) return False return True", "predictions": ["generate a single component"], "references": ["delete a vacation by id ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4001, "code": "def authenticate ( self ) : auth url = BASE URL + \"/auth/token\" payload = { 'username' : self . email , 'password' : self . password , 'grant type' : 'password' } arequest = requests . post ( auth url , data = payload , headers = BASIC HEADERS ) status = arequest . status code if status != 200 : LOGGER . error ( \"Authentication request failed, please check credintials. \" + str ( status ) ) return False response = arequest . json ( ) LOGGER . debug ( str ( response ) ) self . token = response . get ( \"access token\" ) self . refresh token = response . get ( \"refresh token\" ) auth = HEADERS . get ( \"Authorization\" ) auth = auth % self . token HEADERS [ \"Authorization\" ] = auth LOGGER . info ( \"Authentication was successful, token set.\" ) return True", "predictions": ["match api token and password find the authentication token find the user"], "references": ["authenticate with the api and return an authentication token ."], "bleu": 0.16261701715194898, "rouge_l": 0.3696969696969697}
{"id": 4002, "code": "def models preparing ( app ) : def wrapper ( resource , parent ) : if isinstance ( resource , Declarative Meta ) : resource = List Resource ( resource ) if not getattr ( resource , ' parent ' , None ) : resource . parent = parent return resource resources preparing factory ( app , wrapper )", "predictions": ["factory for creating an preparing resource class . ."], "references": ["wrap all sqlalchemy model in settings ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4003, "code": "def translate Commands ( commands ) : for command in commands . split ( ',' ) : result = [ 0 , 0 ] device , command = command . strip ( ) . upper ( ) . split ( None , 1 ) result [ 0 ] = house Codes [ device [ 0 ] ] if len ( device ) > 1 : device Number = device Numbers [ device [ 1 : ] ] result [ 0 ] |= device Number [ 0 ] result [ 1 ] = device Number [ 1 ] result [ 1 ] |= command Codes [ command ] yield ' ' . join ( map ( str Binary , result ) )", "predictions": ["generate commands list of commands ."], "references": ["generate the binary strings for a comma seperated list of commands ."], "bleu": 0.23671529472186087, "rouge_l": 0.5240549828178694}
{"id": 4004, "code": "def set RTSDTR ( port , RTS , DTR ) : port . set RTS ( RTS ) port . set DTR ( DTR )", "predictions": ["find the self result in the prime prime self result result result result result result ."], "references": ["set rts and dtr to the requested state ."], "bleu": 0.08513012360883544, "rouge_l": 0.16850828729281767}
{"id": 4005, "code": "def lower ( option , value ) : if type ( option ) is str : option = option . lower ( ) if type ( value ) is str : value = value . lower ( ) return ( option , value )", "predictions": ["check if an option is a valid option . . . . . . . ."], "references": ["enforces lower case options and option values where appropriate"], "bleu": 0.07692375026049747, "rouge_l": 0.08425414364640883}
{"id": 4006, "code": "def to float ( option , value ) : if type ( value ) is str : try : value = float ( value ) except Value Error : pass return ( option , value )", "predictions": ["helper function to convert string value to float"], "references": ["converts string values to floats when appropriate"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4007, "code": "def to bool ( option , value ) : if type ( value ) is str : if value . lower ( ) == 'true' : value = True elif value . lower ( ) == 'false' : value = False return ( option , value )", "predictions": ["convert string to bool"], "references": ["converts string values to booleans when appropriate"], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 4008, "code": "def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork", "predictions": ["convenience method for setting a convenience convenience method ."], "references": ["create fork and store it in current instance"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4009, "code": "def ld to dl ( ld ) : if ld : keys = list ( ld [ 0 ] ) dl = { key : [ d [ key ] for d in ld ] for key in keys } return dl else : return { }", "predictions": ["convert a main main main main function to a dictionary of main port"], "references": ["convert list of dictionaries to dictionary of lists"], "bleu": 0.14283632578659286, "rouge_l": 0.39804241435562804}
{"id": 4010, "code": "def split list ( l , N ) : npmode = isinstance ( l , np . ndarray ) if npmode : l = list ( l ) g = np . concatenate ( ( np . array ( [ 0 ] ) , np . cumsum ( split integer ( len ( l ) , length = N ) ) ) ) s = [ l [ g [ i ] : g [ i + 1 ] ] for i in range ( N ) ] if npmode : s = [ np . array ( sl ) for sl in s ] return s", "predictions": ["server a bind bind"], "references": ["subdivide list into n lists"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 4011, "code": "def log calls ( function ) : def wrapper ( self , * args , * * kwargs ) : self . log . log ( group = function . name , message = 'Enter' ) function ( self , * args , * * kwargs ) self . log . log ( group = function . name , message = 'Exit' ) return wrapper", "predictions": ["decorator to cast a group to the group function"], "references": ["decorator that logs function calls in their self . log"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 4012, "code": "def add runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = c Profile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) return pr , output return wrapper", "predictions": ["decorator for adding attributes to the data pop pop"], "references": ["decorator that adds a runtime profile object to the output"], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 4013, "code": "def print memory ( function ) : import memory profiler def wrapper ( * args , * * kwargs ) : m = String IO ( ) temp func = memory profiler . profile ( func = function , stream = m , precision = 4 ) output = temp func ( * args , * * kwargs ) print ( m . getvalue ( ) ) m . close ( ) return output return wrapper", "predictions": ["convert cygwin function to cygwin newlines newlines newlines newlines"], "references": ["decorator that prints memory information at each call of the function"], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 4014, "code": "def print profile ( function ) : import memory profiler def wrapper ( * args , * * kwargs ) : m = String IO ( ) pr = c Profile . Profile ( ) pr . enable ( ) temp func = memory profiler . profile ( func = function , stream = m , precision = 4 ) output = temp func ( * args , * * kwargs ) print ( m . getvalue ( ) ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort stats ( 'cumulative' ) . print stats ( '(?!.*memory profiler.*)(^.*$)' , 20 ) m . close ( ) return output return wrapper", "predictions": ["decorator to get mutagen mutagen {} {} {} {}"], "references": ["decorator that prints memory and runtime information at each call of the function"], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 4015, "code": "def print runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = c Profile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort stats ( 'tot' ) . print stats ( 20 ) return output return wrapper", "predictions": ["decorator to mutagen fields 0 on k 0 0 0 0 0 0 0 0"], "references": ["decorator that prints running time information at each call of the function"], "bleu": 0.08225964699966554, "rouge_l": 0.07558859975216851}
{"id": 4016, "code": "def get default fields ( self ) : field names = self . meta . get all field names ( ) if 'id' in field names : field names . remove ( 'id' ) return field names", "predictions": ["returns the metadata for the model"], "references": ["get all fields of model execpt id"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4017, "code": "def validate bands ( self , bands ) : if not isinstance ( bands , list ) : logger . error ( 'Parameter bands must be a \"list\"' ) raise Type Error ( 'Parameter bands must be a \"list\"' ) valid bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid bands : logger . error ( '%s is not a valid band' % band ) raise Invalid Band Error ( '%s is not a valid band' % band )", "predictions": ["validate that all field field is a"], "references": ["validate bands parameter ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4018, "code": "def download ( self , bands , download dir = None , metadata = False ) : super ( Google Downloader , self ) . validate bands ( bands ) pattern = re . compile ( '^[^\\s]+ (.+)\\.tiff?' , re . I ) image list = [ ] band list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] if download dir is None : download dir = DOWNLOAD DIR check create folder ( join ( download dir , self . scene Info . name ) ) filename = \"%s%s\" % ( self . scene Info . name , self . remote file ext ) downloaded = self . fetch ( self . remote file url , download dir , filename ) try : tar = tarfile . open ( downloaded [ 0 ] , 'r' ) folder path = join ( download dir , self . scene Info . name ) logger . debug ( 'Starting data extraction in directory ' , folder path ) tar . extractall ( folder path ) remove ( downloaded [ 0 ] ) images path = listdir ( folder path ) for image path in images path : matched = pattern . match ( image path ) file path = join ( folder path , image path ) if matched and matched . group ( 1 ) in band list : image list . append ( [ file path , getsize ( file path ) ] ) elif matched : remove ( file path ) except tarfile . Read Error as error : logger . error ( 'Error when extracting files: ' , error ) print ( 'Error when extracting files.' ) return image list", "predictions": ["check for the image . and . ."], "references": ["download remote . tar . bz file ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 4019, "code": "def validate scene Info ( self ) : if self . scene Info . prefix not in self . prefixes Valid : raise Wrong Scene Name Error ( 'AWS: Prefix of %s (%s) is invalid' % ( self . scene Info . name , self . scene Info . prefix ) )", "predictions": ["url checks that the scene is valid"], "references": ["check whether sceneinfo is valid to download from aws storage ."], "bleu": 0.1380518455178974, "rouge_l": 0.2136602451838879}
{"id": 4020, "code": "def download ( self , bands , download dir = None , metadata = False ) : super ( AWS Downloader , self ) . validate bands ( bands ) if download dir is None : download dir = DOWNLOAD DIR dest dir = check create folder ( join ( download dir , self . scene Info . name ) ) downloaded = [ ] for band in bands : if band == 'BQA' : filename = '%s %s.%s' % ( self . scene Info . name , band , self . remote file ext ) else : filename = '%s B%s.%s' % ( self . scene Info . name , band , self . remote file ext ) band url = join ( self . base url , filename ) downloaded . append ( self . fetch ( band url , dest dir , filename ) ) if metadata : filename = '%s MTL.txt' % ( self . scene Info . name ) url = join ( self . base url , filename ) self . fetch ( url , dest dir , filename ) return downloaded", "predictions": ["download band to . credentials credentials credentials credentials credentials credentials credentials credentials credentials"], "references": ["download each specified band and metadata ."], "bleu": 0.1135935489027116, "rouge_l": 0.31715771230502604}
{"id": 4021, "code": "def writable path ( path ) : if os . path . exists ( path ) : return os . access ( path , os . W OK ) try : with open ( path , 'w' ) : pass except ( OS Error , IO Error ) : return False else : os . remove ( path ) return True", "predictions": ["check if a to see if we do a prepare prepare a to load a to load it"], "references": ["test whether a path can be written to ."], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 4022, "code": "def writable stream ( handle ) : if isinstance ( handle , io . IO Base ) and sys . version info >= ( 3 , 5 ) : return handle . writable ( ) try : handle . write ( b'' ) except ( io . Unsupported Operation , IO Error ) : return False else : return True", "predictions": ["see if a stream stream is a run stream in a stream in a stream in a stream in a stream in a stream in a stream in a stream in"], "references": ["test whether a stream can be written to ."], "bleu": 0.0513487742994337, "rouge_l": 0.11101000909918107}
{"id": 4023, "code": "def wait for connection ( self , port ) : connected = False max tries = 10 num tries = 0 wait time = 0.5 while not connected or num tries >= max tries : time . sleep ( wait time ) try : af = socket . AF INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num tries += 1 continue connected = True if not connected : print ( \"Error connecting to sphinx searchd\" , file = sys . stderr )", "predictions": ["get self . for for"], "references": ["wait until we can make a socket connection to sphinx ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4024, "code": "def get settings path ( settings module ) : cwd = os . getcwd ( ) settings filename = '%s.py' % ( settings module . split ( '.' ) [ - 1 ] ) while cwd : if settings filename in os . listdir ( cwd ) : break cwd = os . path . split ( cwd ) [ 0 ] if os . name == 'nt' and NT ROOT . match ( cwd ) : return None elif cwd == '/' : return None return cwd", "predictions": ["return will search for a given link"], "references": ["hunt down the settings . py module by going up the fs path"], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 4025, "code": "def finalize ( self , result = None ) : if not self . settings path : return from django . test . utils import teardown test environment from django . db import connection from django . conf import settings self . call plugins method ( 'before Destroy Test Db' , settings , connection ) try : connection . creation . destroy test db ( self . old db , verbosity = self . verbosity , ) except Exception : pass self . call plugins method ( 'after Destroy Test Db' , settings , connection ) self . call plugins method ( 'before Teardown Test Env' , settings , teardown test environment ) teardown test environment ( ) self . call plugins method ( 'after Teardown Test Env' , settings )", "predictions": ["called when the django connection is closed ."], "references": ["clean up any created database and schema ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4026, "code": "def update field ( self , poses = None ) : m = np . clip ( self . particle field , 0 , 1 ) part color = np . zeros ( self . image . shape ) for a in range ( 4 ) : part color [ : , : , : , a ] = self . part col [ a ] self . field = np . zeros ( self . image . shape ) for a in range ( 4 ) : self . field [ : , : , : , a ] = m * part color [ : , : , : , a ] + ( 1 - m ) * self . image [ : , : , : , a ]", "predictions": ["get the particle modes"], "references": ["updates self . field"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4027, "code": "def remove closest particle ( self , p ) : #1. find closest pos: dp = self . pos - p dist2 = ( dp * dp ) . sum ( axis = 1 ) ind = dist2 . argmin ( ) rp = self . pos [ ind ] . copy ( ) #2. delete self . pos = np . delete ( self . pos , ind , axis = 0 ) return rp", "predictions": ["get usage of particle"], "references": ["removes the closest particle in self . pos to p"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 4028, "code": "def diffusion ( diffusion constant = 0.2 , exposure time = 0.05 , samples = 200 ) : radius = 5 psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) s0 = init . create single particle state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) finalimage = 0 * s0 . get model image ( ) [ s0 . inner ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( samples ) : offset = np . sqrt ( 6 * diffusion constant * exposure time ) * np . random . randn ( 3 ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage += s0 . get model image ( ) [ s0 . inner ] position += s0 . obj . pos [ 0 ] finalimage /= float ( samples ) position /= float ( samples ) s = init . create single particle state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) return s , finalimage , position", "predictions": ["create the particle of the particle status status of a return particle status status"], "references": ["see diffusion_correlated for information related to units etc"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 4029, "code": "def tile overlap ( inner , outer , norm = False ) : div = 1.0 / inner . volume if norm else 1.0 return div * ( inner . volume - util . Tile . intersection ( inner , outer ) . volume )", "predictions": ["== == locations locations locations"], "references": ["how much of inner is in outer by volume"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 4030, "code": "def translate fourier ( image , dx ) : N = image . shape [ 0 ] f = 2 * np . pi * np . fft . fftfreq ( N ) kx , ky , kz = np . meshgrid ( * ( f , ) * 3 , indexing = 'ij' ) kv = np . array ( [ kx , ky , kz ] ) . T q = np . fft . fftn ( image ) * np . exp ( - 1.j * ( kv * dx ) . sum ( axis = - 1 ) ) . T return np . real ( np . fft . ifftn ( q ) )", "predictions": ["get fourier using fourier"], "references": ["translate an image in fourier - space with plane waves"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 4031, "code": "def users ( ) : from invenio groups . models import Group , Membership , Privacy Policy , Subscription Policy admin = accounts . datastore . create user ( email = 'admin@inveniosoftware.org' , password = encrypt password ( '123456' ) , active = True , ) reader = accounts . datastore . create user ( email = 'reader@inveniosoftware.org' , password = encrypt password ( '123456' ) , active = True , ) admins = Group . create ( name = 'admins' , admins = [ admin ] ) for i in range ( 10 ) : Group . create ( name = 'group-{0}' . format ( i ) , admins = [ admin ] ) Membership . create ( admins , reader ) db . session . commit ( )", "predictions": ["load str and password data = 0 = 0 = 0 = 0 = 1 = 0 = 1 = 0 = 0 = 0 = 0 = 0 = 0"], "references": ["load default users and groups ."], "bleu": 0.04317900023606586, "rouge_l": 0.12310797174571139}
{"id": 4032, "code": "def weight ( self , rsq , sigma = None ) : sigma = sigma or self . filter size if not self . clip : o = np . exp ( - rsq / ( 2 * sigma ** 2 ) ) else : o = np . zeros ( rsq . shape , dtype = 'float' ) m = ( rsq < self . clipsize ** 2 ) o [ m ] = np . exp ( - rsq [ m ] / ( 2 * sigma ** 2 ) ) return o", "predictions": ["return a weight of the weight with a rsq ."], "references": ["weighting function for barnes"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4033, "code": "def eval firstorder ( self , rvecs , data , sigma ) : if not self . blocksize : dist between points = self . distance matrix ( rvecs , self . x ) gaussian weights = self . weight ( dist between points , sigma = sigma ) return gaussian weights . dot ( data ) / gaussian weights . sum ( axis = 1 ) else : ans = np . zeros ( rvecs . shape [ 0 ] , dtype = 'float' ) bs = self . blocksize for a in range ( 0 , rvecs . shape [ 0 ] , bs ) : dist = self . distance matrix ( rvecs [ a : a + bs ] , self . x ) weights = self . weight ( dist , sigma = sigma ) ans [ a : a + bs ] += weights . dot ( data ) / weights . sum ( axis = 1 ) return ans", "predictions": ["evaluates the gradient of a firstorder"], "references": ["the first - order barnes approximation"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 4034, "code": "def newcall ( self , rvecs ) : sigma = 1 * self . filter size out = self . eval firstorder ( rvecs , self . d , sigma ) ondata = self . eval firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . eval firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . eval firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out", "predictions": ["evaluate the rectangle and its 32 - 1"], "references": ["correct normalized version of barnes"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4035, "code": "def distance matrix ( self , a , b ) : def sq ( x ) : return ( x * x ) matrix = sq ( a [ : , 0 ] [ : , None ] - b [ : , 0 ] [ None , : ] ) for x , y in zip ( a . T [ 1 : ] , b . T [ 1 : ] ) : matrix += sq ( x [ : , None ] - y [ None , : ] ) return matrix", "predictions": ["return a distance between two points"], "references": ["pairwise distance between each point in a and each point in b"], "bleu": 0.11492332782473744, "rouge_l": 0.20962199312714777}
{"id": 4036, "code": "def c2x ( self , c ) : return 0.5 * ( self . window [ 0 ] + self . window [ 1 ] + c * ( self . window [ 1 ] - self . window [ 0 ] ) )", "predictions": ["get the current window ."], "references": ["convert cheb coordinates to windowdow coordinates"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 4037, "code": "def resolve admin type ( admin ) : if admin is current user or isinstance ( admin , User Mixin ) : return 'User' else : return admin . class . name", "predictions": ["resolve the admin type to the admin user s admin type ."], "references": ["determine admin type ."], "bleu": 0.19338531381761725, "rouge_l": 0.4121621621621622}
{"id": 4038, "code": "def validate ( cls , policy ) : return policy in [ cls . OPEN , cls . APPROVAL , cls . CLOSED ]", "predictions": ["check the policy for the given policy ."], "references": ["validate subscription policy value ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4039, "code": "def validate ( cls , policy ) : return policy in [ cls . PUBLIC , cls . MEMBERS , cls . ADMINS ]", "predictions": ["check the policy for this policy ."], "references": ["validate privacy policy value ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4040, "code": "def validate ( cls , state ) : return state in [ cls . ACTIVE , cls . PENDING ADMIN , cls . PENDING USER ]", "predictions": ["check if the class is valid ."], "references": ["validate state value ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4041, "code": "def delete ( self ) : with db . session . begin nested ( ) : Membership . query by group ( self ) . delete ( ) Group Admin . query by group ( self ) . delete ( ) Group Admin . query by admin ( self ) . delete ( ) db . session . delete ( self )", "predictions": ["deletes the records in the database ."], "references": ["delete a group and all associated memberships ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4042, "code": "def filter ( cls , query , state = Membership State . ACTIVE , eager = None ) : query = query . filter by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query", "predictions": ["filter a query string ."], "references": ["filter a query result ."], "bleu": 0.537284965911771, "rouge_l": 0.8}
{"id": 4043, "code": "def query by user ( cls , user , * * kwargs ) : return cls . filter ( cls . query . filter by ( user id = user . get id ( ) ) , * * kwargs )", "predictions": ["query elasticsearch by user and return dotted model instance ."], "references": ["get a user s memberships ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 4044, "code": "def query invitations ( cls , user , eager = False ) : if eager : eager = [ Membership . group ] return cls . query by user ( user , state = Membership State . PENDING USER , eager = eager )", "predictions": ["query invitations for a user ."], "references": ["get all invitations for given user ."], "bleu": 0.3094358155846605, "rouge_l": 0.6069651741293532}
{"id": 4045, "code": "def query requests ( cls , admin , eager = False ) : if hasattr ( admin , 'is superadmin' ) and admin . is superadmin : q1 = Group Admin . query . with entities ( Group Admin . group id ) else : q1 = Group Admin . query by admin ( admin ) . with entities ( Group Admin . group id ) q2 = Membership . query . filter ( Membership . state == Membership State . PENDING ADMIN , Membership . id group . in ( q1 ) , ) q3 = Membership . query by user ( user = admin , state = Membership State . ACTIVE ) . with entities ( Membership . id group ) q4 = Group Admin . query . filter ( Group Admin . admin type == 'Group' , Group Admin . admin id . in ( q3 ) ) . with entities ( Group Admin . group id ) q5 = Membership . query . filter ( Membership . state == Membership State . PENDING ADMIN , Membership . id group . in ( q4 ) ) query = q2 . union ( q5 ) return query", "predictions": ["query requests for admin entities ."], "references": ["get all pending group requests ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 4046, "code": "def query by group ( cls , group or id , with invitations = False , * * kwargs ) : if isinstance ( group or id , Group ) : id group = group or id . id else : id group = group or id if not with invitations : return cls . filter ( cls . query . filter by ( id group = id group ) , * * kwargs ) else : return cls . query . filter ( Membership . id group == id group , db . or ( Membership . state == Membership State . PENDING USER , Membership . state == Membership State . ACTIVE ) )", "predictions": ["query a group by id ."], "references": ["get a group s members ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 4047, "code": "def create ( cls , group , user , state = Membership State . ACTIVE ) : with db . session . begin nested ( ) : membership = cls ( user id = user . get id ( ) , id group = group . id , state = state , ) db . session . add ( membership ) return membership", "predictions": ["create an membership and return it ."], "references": ["create a new membership ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 4048, "code": "def get ( cls , group , admin ) : try : ga = cls . query . filter by ( group = group , admin id = admin . get id ( ) , admin type = resolve admin type ( admin ) ) . one ( ) return ga except Exception : return None", "predictions": ["get a single group"], "references": ["get specific groupadmin object ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 4049, "code": "def query by admin ( cls , admin ) : return cls . query . filter by ( admin type = resolve admin type ( admin ) , admin id = admin . get id ( ) )", "predictions": ["query admin admin admin admin admin admin admin admin admin admin ."], "references": ["get all groups for for a specific admin ."], "bleu": 0.1367440667823257, "rouge_l": 0.19551282051282048}
{"id": 4050, "code": "def query admins by group ids ( cls , groups ids = None ) : assert groups ids is None or isinstance ( groups ids , list ) query = db . session . query ( Group . id , func . count ( Group Admin . id ) ) . join ( Group Admin ) . group by ( Group . id ) if groups ids : query = query . filter ( Group . id . in ( groups ids ) ) return query", "predictions": ["query all groups by a group ."], "references": ["get count of admins per group ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 4051, "code": "def all ( self ) : response = self . api . get ( url = PATHS [ 'GET PROFILES' ] ) for raw profile in response : self . append ( Profile ( self . api , raw profile ) ) return self", "predictions": ["get all features ."], "references": ["get all social newtworks profiles"], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 4052, "code": "def skew ( self , x , z , d = 0 ) : kval = ( np . tanh ( self . poly ( z , self . kurtosis coeffs ( d ) ) ) + 1 ) / 12. bdpoly = np . array ( [ - 1.142468e+04 , 3.0939485e+03 , - 2.0283568e+02 , - 2.1047846e+01 , 3.79808487e+00 , 1.19679781e-02 ] ) top = np . polyval ( bdpoly , kval ) skew = self . poly ( z , self . skew coeffs ( d ) ) skewval = top * ( np . tanh ( skew ) + 1 ) - top return skewval * ( 3 * x - x ** 3 )", "predictions": ["return the skew coefficient"], "references": ["returns the kurtosis parameter for direction d d = 0 is rho d = 1 is z"], "bleu": 0.013931732312048943, "rouge_l": 0.08567415730337079}
{"id": 4053, "code": "def kurtosis ( self , x , z , d = 0 ) : val = self . poly ( z , self . kurtosis coeffs ( d ) ) return ( np . tanh ( val ) + 1 ) / 12. * ( 3 - 6 * x ** 2 + x ** 4 )", "predictions": ["return the kurtosis of a polygon ."], "references": ["returns the kurtosis parameter for direction d d = 0 is rho d = 1 is z"], "bleu": 0.05858545453338479, "rouge_l": 0.1550190597204574}
{"id": 4054, "code": "def edit ( self , text , media = None , utc = None , now = None ) : url = PATHS [ 'EDIT' ] % self . id post data = \"text=%s&\" % text if now : post data += \"now=%s&\" % now if utc : post data += \"utc=%s&\" % utc if media : media format = \"media[%s]=%s&\" for media type , media item in media . iteritems ( ) : post data += media format % ( media type , media item ) response = self . api . post ( url = url , data = post data ) return Update ( api = self . api , raw response = response [ 'update' ] )", "predictions": ["edit a text ."], "references": ["edit an existing individual status update ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 4055, "code": "def delete ( self ) : url = PATHS [ 'DELETE' ] % self . id return self . api . post ( url = url )", "predictions": ["delete a specific vlan ."], "references": ["permanently delete an existing status update ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4056, "code": "def new ( self , text , shorten = None , now = None , top = None , media = None , when = None ) : url = PATHS [ 'CREATE' ] post data = \"text=%s&\" % text post data += \"profile ids[]=%s&\" % self . profile id if shorten : post data += \"shorten=%s&\" % shorten if now : post data += \"now=%s&\" % now if top : post data += \"top=%s&\" % top if when : post data += \"scheduled at=%s&\" % str ( when ) if media : media format = \"media[%s]=%s&\" for media type , media item in media . iteritems ( ) : post data += media format % ( media type , media item ) response = self . api . post ( url = url , data = post data ) new update = Update ( api = self . api , raw response = response [ 'updates' ] [ 0 ] ) self . append ( new update ) return new update", "predictions": ["send a new new post"], "references": ["create one or more new status updates ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4057, "code": "def noformat ( self ) : try : formats = { } for h in self . get handlers ( ) : formats [ h ] = h . formatter self . set formatter ( formatter = 'quiet' ) yield except Exception as e : raise finally : for k , v in iteritems ( formats ) : k . formatter = v", "predictions": ["iterate over the formatter for each module ."], "references": ["temporarily do not use any formatter so that text printed is raw"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 4058, "code": "def generate sphere ( radius ) : rint = np . ceil ( radius ) . astype ( 'int' ) t = np . arange ( - rint , rint + 1 , 1 ) x , y , z = np . meshgrid ( t , t , t , indexing = 'ij' ) r = np . sqrt ( x * x + y * y + z * z ) sphere = r < radius return sphere", "predictions": ["generate a sphere sphere for the sphere"], "references": ["generates a centered boolean mask of a 3d sphere"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4059, "code": "def sphere triangle cdf ( dr , a , alpha ) : p0 = ( dr + alpha ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 > dr ) * ( dr > - alpha ) p1 = 1 * ( dr > 0 ) - ( alpha - dr ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 < dr ) * ( dr < alpha ) return ( 1 - np . clip ( p0 + p1 , 0 , 1 ) )", "predictions": ["return the sphere cdf cdf cdf ."], "references": ["cumulative distribution function for the traingle distribution"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4060, "code": "def tile ( self , n ) : pos = self . trans ( self . pos [ n ] ) return Tile ( pos , pos ) . pad ( self . support pad )", "predictions": ["return the tile with the current position ."], "references": ["get the update tile surrounding particle n"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4061, "code": "def i2p ( self , ind , coord ) : return '-' . join ( [ self . param prefix , str ( ind ) , coord ] )", "predictions": ["converts a mining to a single quaternion ."], "references": ["translate index info to parameter name"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 4062, "code": "def get update tile ( self , params , values ) : doglobal , particles = self . update type ( params ) if doglobal : return self . shape . copy ( ) values0 = self . get values ( params ) tiles0 = [ self . tile ( n ) for n in particles ] self . set values ( params , values ) tiles1 = [ self . tile ( n ) for n in particles ] self . set values ( params , values0 ) return Tile . boundingtile ( tiles0 + tiles1 )", "predictions": ["get the tile tile"], "references": ["get the amount of support size required for a particular update ."], "bleu": 0.06399610426154731, "rouge_l": 0.22932330827067668}
{"id": 4063, "code": "def update ( self , params , values ) : #1. Figure out if we're going to do a global update, in which global update , particles = self . update type ( params ) if global update : self . set values ( params , values ) self . initialize ( ) return oldargs = self . drawargs ( ) for n in particles : self . draw particle ( self . pos [ n ] , * listify ( oldargs [ n ] ) , sign = - 1 ) self . set values ( params , values ) newargs = self . drawargs ( ) for n in particles : self . draw particle ( self . pos [ n ] , * listify ( newargs [ n ] ) , sign = + 1 )", "predictions": ["draw the particle using the current particle ."], "references": ["update the particles field given new parameter values"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4064, "code": "def param particle ( self , ind ) : ind = self . vps ( listify ( ind ) ) return [ self . i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' , 'a' ] ]", "predictions": ["search a list of particle which are not a particle ."], "references": ["get position and radius of one or more particles"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 4065, "code": "def param particle pos ( self , ind ) : ind = self . vps ( listify ( ind ) ) return [ self . i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' ] ]", "predictions": ["search a list of the particle which are not = = = true"], "references": ["get position of one or more particles"], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 4066, "code": "def param particle rad ( self , ind ) : ind = self . vps ( listify ( ind ) ) return [ self . i2p ( i , 'a' ) for i in ind ]", "predictions": ["get parameter which contains a param * ind * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["get radius of one or more particles"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 4067, "code": "def update type ( self , params ) : dozscale = False particles = [ ] for p in listify ( params ) : typ , ind = self . p2i ( p ) particles . append ( ind ) dozscale = dozscale or typ == 'zscale' particles = set ( particles ) return dozscale , particles", "predictions": ["update a function with a set of a matrix of a function s matrix"], "references": ["returns dozscale and particle list of update"], "bleu": 0.09782375748961449, "rouge_l": 0.10132890365448505}
{"id": 4068, "code": "def tile ( self , n ) : zsc = np . array ( [ 1.0 / self . zscale , 1 , 1 ] ) pos , rad = self . pos [ n ] , self . rad [ n ] pos = self . trans ( pos ) return Tile ( pos - zsc * rad , pos + zsc * rad ) . pad ( self . support pad )", "predictions": ["return tile to the tile"], "references": ["get the tile surrounding particle n"], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 4069, "code": "def j2 ( x ) : to return = 2. / ( x + 1e-15 ) * j1 ( x ) - j0 ( x ) to return [ x == 0 ] = 0 return to return", "predictions": ["convert type to ."], "references": ["a fast j2 defined in terms of other special functions"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 4070, "code": "def calc pts hg ( npts = 20 ) : pts hg , wts hg = np . polynomial . hermite . hermgauss ( npts * 2 ) pts hg = pts hg [ npts : ] wts hg = wts hg [ npts : ] * np . exp ( pts hg * pts hg ) return pts hg , wts hg", "predictions": ["calculates the cls cls cls cls"], "references": ["returns hermite - gauss quadrature points for even functions"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 4071, "code": "def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )", "predictions": ["get the coordinates of a policy"], "references": ["opposite slicer the outer part wrt to a field"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4072, "code": "def filtered image ( self , im ) : q = np . fft . fftn ( im ) for k , v in self . filters : q [ k ] -= v return np . real ( np . fft . ifftn ( q ) )", "predictions": ["transform an image to a square"], "references": ["returns a filtered image after applying the fourier - space filters"], "bleu": 0.10624253482403696, "rouge_l": 0.1117216117216117}
{"id": 4073, "code": "def load image ( self ) : try : image = initializers . load tiff ( self . filename ) image = initializers . normalize ( image , invert = self . invert , scale = self . exposure , dtype = self . float precision ) except IO Error as e : log . error ( \"Could not find image '%s'\" % self . filename ) raise e return image", "predictions": ["delete the image image image"], "references": ["read the file and perform any transforms to get a loaded image"], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 4074, "code": "def draw ( self ) : if self . display : print ( self . formatstr . format ( * * self . dict ) , end = '' ) sys . stdout . flush ( )", "predictions": ["filter the graph and stdout to stdout"], "references": ["interal draw method simply prints to screen"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4075, "code": "def init app ( self , app ) : self . init config ( app ) app . register blueprint ( blueprint ) app . extensions [ 'invenio-groups' ] = self", "predictions": ["flask application initialization kwargs"], "references": ["flask application initialization ."], "bleu": 0.668740304976422, "rouge_l": 0.75}
{"id": 4076, "code": "def lbl ( axis , label , size = 22 ) : at = Anchored Text ( label , loc = 2 , prop = dict ( size = size ) , frameon = True ) at . patch . set boxstyle ( \"round,pad=0.,rounding size=0.0\" ) #bb = axis.get yaxis transform() #at = Anchored Text(label, axis . add artist ( at )", "predictions": ["make a new artist"], "references": ["put a figure label in an axis"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4077, "code": "def sim crb diff ( std0 , std1 , N = 10000 ) : a = std0 * np . random . randn ( N , len ( std0 ) ) b = std1 * np . random . randn ( N , len ( std1 ) ) return a - b", "predictions": [". diff with requests and ."], "references": ["each element of std0 should correspond with the element of std1"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 4078, "code": "def missing particle ( separation = 0.0 , radius = RADIUS , SNR = 20 ) : s = init . create two particle state ( imsize = 6 * radius + 4 , axis = 'x' , sigma = 1.0 / SNR , delta = separation , radius = radius , stateargs = { 'varyn' : True } , psfargs = { 'error' : 1e-6 } ) s . obj . typ [ 1 ] = 0. s . reset ( ) return s , s . obj . pos . copy ( )", "predictions": ["computes a by by id"], "references": ["create a two particle state and compare it to featuring using a single particle guess"], "bleu": 0.0369481680224917, "rouge_l": 0.09172932330827067}
{"id": 4079, "code": "def check groups ( s , groups ) : ans = [ ] for g in groups : ans . extend ( g ) if np . unique ( ans ) . size != np . size ( ans ) : return False elif np . unique ( ans ) . size != s . obj get positions ( ) . shape [ 0 ] : return False else : return ( np . arange ( s . obj get radii ( ) . size ) == np . sort ( ans ) ) . all ( )", "predictions": ["create list of groups positions"], "references": ["ensures that all particles are included in exactly 1 group"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 4080, "code": "def find best step ( err vals ) : if np . all ( np . isnan ( err vals ) ) : raise Value Error ( 'All err vals are nans!' ) return np . nanargmin ( err vals )", "predictions": ["get a best cls cls = 0 = 1 = 0 = 0 = 1 = 1"], "references": ["returns the index of the lowest of the passed values . catches nans etc ."], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 4081, "code": "def reset ( self , new damping = None ) : self . num iter = 0 self . inner run counter = 0 self . J update counter = self . update J frequency self . fresh JTJ = False self . has run = False if new damping is not None : self . damping = np . array ( new damping ) . astype ( 'float' ) self . set err paramvals ( )", "predictions": ["query the simulation and query the values for a new instance filter"], "references": ["keeps all user supplied options the same but resets counters etc ."], "bleu": 0.10390302174233558, "rouge_l": 0.08333333333333333}
{"id": 4082, "code": "def check completion ( self ) : terminate = False term dict = self . get termination stats ( get cos = self . costol is not None ) terminate |= np . all ( np . abs ( term dict [ 'delta vals' ] ) < self . paramtol ) terminate |= ( term dict [ 'delta err' ] < self . errtol ) terminate |= ( term dict [ 'exp err' ] < self . exptol ) terminate |= ( term dict [ 'frac err' ] < self . fractol ) if self . costol is not None : terminate |= ( curcos < term dict [ 'model cosine' ] ) return terminate", "predictions": ["query the admins of the admins . ."], "references": ["returns a bool of whether the algorithm has found a satisfactory minimum"], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 4083, "code": "def update J ( self ) : self . calc J ( ) step = np . ceil ( 1e-2 * self . J . shape [ 1 ] ) . astype ( 'int' ) self . JTJ = low mem sq ( self . J , step = step ) #copies still, since J is not C -ordered but a slice of j e... #doing self.J.copy() works but takes 2x as much ram.. self . fresh JTJ = True self . J update counter = 0 if np . any ( np . isnan ( self . JTJ ) ) : raise Floating Point Error ( 'J, JTJ have nans.' ) #Update self. exp err self . exp err = self . error - self . find expected error ( delta params = 'perfect' )", "predictions": ["all the current api api api api api calls"], "references": ["updates j jtj and internal counters ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4084, "code": "def calc grad ( self ) : residuals = self . calc residuals ( ) return 2 * np . dot ( self . J , residuals )", "predictions": ["calculates the d - major grad"], "references": ["the gradient of the cost w . r . t . the parameters ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 4085, "code": "def update Broyden J ( self ) : CLOG . debug ( 'Broyden update.' ) delta vals = self . param vals - self . last vals delta residuals = self . calc residuals ( ) - self . last residuals nrm = np . sqrt ( np . dot ( delta vals , delta vals ) ) direction = delta vals / nrm vals = delta residuals / nrm self . rank 1 J update ( direction , vals ) self . JTJ = np . dot ( self . J , self . J . T )", "predictions": ["kurtosis + + + + + + + + + + + + + + + + + + + + + + 1"], "references": ["execute a broyden update of j"], "bleu": 0.042601467364417965, "rouge_l": 0.0}
{"id": 4086, "code": "def update eig J ( self ) : CLOG . debug ( 'Eigen update.' ) vls , vcs = np . linalg . eigh ( self . JTJ ) res0 = self . calc residuals ( ) for a in range ( min ( [ self . num eig dirs , vls . size ] ) ) : #1. Finding stiff directions stif dir = vcs [ - ( a + 1 ) ] #already normalized #2. Evaluating derivative along that direction, we'll use dl=5e-4: dl = self . eig dl #1e-5 = self . update function ( self . param vals + dl * stif dir ) res1 = self . calc residuals ( ) #3. Updating grad stif = ( res1 - res0 ) / dl self . rank 1 J update ( stif dir , grad stif ) self . JTJ = np . dot ( self . J , self . J . T ) #Putting the parameters back: = self . update function ( self . param vals )", "predictions": ["edit the eig and the residuals parameters"], "references": ["execute an eigen update of j"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4087, "code": "def calc J ( self ) : del self . J self . J = np . zeros ( [ self . param vals . size , self . data . size ] ) dp = np . zeros like ( self . param vals ) f0 = self . model . copy ( ) for a in range ( self . param vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param vals + dp , * self . func args , * * self . func kwargs ) grad func = ( f1 - f0 ) / dp [ a ] #J = grad(residuals) = -grad(model) self . J [ a ] = - grad func", "predictions": ["remove all the post - step of the model"], "references": ["updates self . j returns nothing"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4088, "code": "def update function ( self , param vals ) : self . model = self . func ( param vals , * self . func args , * * self . func kwargs ) d = self . calc residuals ( ) return np . dot ( d . flat , d . flat )", "predictions": ["new function of residuals"], "references": ["takes an array param_vals updates function returns the new error"], "bleu": 0.08872444253557525, "rouge_l": 0.13260869565217392}
{"id": 4089, "code": "def update function ( self , param vals ) : self . opt obj . update function ( param vals ) return self . opt obj . get error ( )", "predictions": ["updates a function object"], "references": ["updates the opt_obj returns new error ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4090, "code": "def calc J ( self ) : r0 = self . state . residuals . copy ( ) . ravel ( ) dl = np . zeros ( self . param vals . size ) p0 = self . param vals . copy ( ) J = [ ] for a in range ( self . param vals . size ) : dl *= 0 dl [ a ] += self . dl self . update function ( p0 + dl ) r1 = self . state . residuals . copy ( ) . ravel ( ) J . append ( ( r1 - r0 ) / self . dl ) self . update function ( p0 ) return np . array ( J )", "predictions": ["calculates the sphere count of the sphere"], "references": ["calculates j along the direction ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 4091, "code": "def calc grad ( self ) : if self . fresh JTJ : return self . graderr else : residuals = self . calc residuals ( ) return 2 * np . dot ( self . J , residuals )", "predictions": ["calculates the ** triangle triangle alpha ** ** ** versions"], "references": ["the gradient of the cost w . r . t . the parameters ."], "bleu": 0.08450033111870488, "rouge_l": 0.08090185676392574}
{"id": 4092, "code": "def do run ( self , mode = '1' ) : for a in range ( len ( self . particle groups ) ) : group = self . particle groups [ a ] lp = LM Particles ( self . state , group , * * self . kwargs ) if mode == 'internal' : lp . J , lp . JTJ , lp . dif tile = self . load j diftile ( a ) if mode == '1' : lp . do run 1 ( ) if mode == '2' : lp . do run 2 ( ) if mode == 'internal' : lp . do internal run ( ) self . stats . append ( lp . get termination stats ( get cos = self . get cos ) ) if self . save J and ( mode != 'internal' ) : self . dump j diftile ( a , lp . J , lp . dif tile ) self . has saved J [ a ] = True", "predictions": ["run up all support groups"], "references": ["workhorse for the self . do_run_xx methods ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 4093, "code": "def do internal run ( self ) : if not self . save J : raise Runtime Error ( 'self.save J=True required for do internal run()' ) if not np . all ( self . has saved J ) : raise Runtime Error ( 'J, JTJ have not been pre-computed. Call do run 1 or do run 2' ) self . do run ( mode = 'internal' )", "predictions": ["perform internal self ."], "references": ["calls lmparticles . do_internal_run for each group of particles ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 4094, "code": "def reset ( self , * * kwargs ) : self . aug state . reset ( ) super ( LM Augmented State , self ) . reset ( * * kwargs )", "predictions": ["get the filter state"], "references": ["resets the aug_state and the lmengine"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4095, "code": "def model to data ( self , sigma = 0.0 ) : im = self . model . copy ( ) im += sigma * np . random . randn ( * im . shape ) self . set image ( util . Null Image ( image = im ) )", "predictions": ["which update the update the update the text to the text file ."], "references": ["switch out the data for the model s recreation of the data ."], "bleu": 0.12011055432195765, "rouge_l": 0.30769230769230765}
{"id": 4096, "code": "def get ( self , name ) : for c in self . comps : if c . category == name : return c return None", "predictions": ["get a specific category by name"], "references": ["return component by category name"], "bleu": 0.2626909894424158, "rouge_l": 0.3696969696969697}
{"id": 4097, "code": "def calc loglikelihood ( self , model = None , tile = None ) : if model is None : res = self . residuals else : res = model - self . data [ tile . slicer ] sig , isig = self . sigma , 1.0 / self . sigma nlogs = - np . log ( np . sqrt ( 2 * np . pi ) * sig ) * res . size return - 0.5 * isig * isig * np . dot ( res . flat , res . flat ) + nlogs", "predictions": ["calculates the log - likelihood of the model"], "references": ["allows for fast local updates of log - likelihood"], "bleu": 0.2785146580372046, "rouge_l": 0.34923664122137404}
{"id": 4098, "code": "def trigger update ( self , params , values ) : if self . parent : self . parent . trigger update ( params , values ) else : self . update ( params , values )", "predictions": ["trigger a new update of a parent ."], "references": ["notify parent of a parameter change"], "bleu": 0.22679164443904004, "rouge_l": 0.2932692307692307}
{"id": 4099, "code": "def get ( self ) : fields = [ c . get ( ) for c in self . comps ] return self . field reduce func ( fields )", "predictions": ["return a list of fields that are currently active"], "references": ["combine the fields from all components"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4100, "code": "def set shape ( self , shape , inner ) : for c in self . comps : c . set shape ( shape , inner )", "predictions": ["add the shape of the given shape to the given shape ."], "references": ["set the shape for all components"], "bleu": 0.1367440667823257, "rouge_l": 0.2364341085271318}
{"id": 4101, "code": "def sync params ( self ) : def normalize ( comps , param ) : vals = [ c . get values ( param ) for c in comps ] diff = any ( [ vals [ i ] != vals [ i + 1 ] for i in range ( len ( vals ) - 1 ) ] ) if diff : for c in comps : c . set values ( param , vals [ 0 ] ) for param , comps in iteritems ( self . lmap ) : if isinstance ( comps , list ) and len ( comps ) > 1 : normalize ( comps , param )", "predictions": ["set parameters from parameters ."], "references": ["ensure that shared parameters are the same value everywhere"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4102, "code": "def read environment ( ) : out = { } for k , v in iteritems ( os . environ ) : if transform ( k ) in default conf : out [ transform ( k ) ] = v return out", "predictions": ["read the environment variables from the current user ."], "references": ["read all environment variables to see if they contain peri"], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 4103, "code": "def get group name ( id group ) : group = Group . query . get ( id group ) if group is not None : return group . name", "predictions": ["get the group name"], "references": ["used for breadcrumb dynamic_list_constructor ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 4104, "code": "def index ( ) : page = request . args . get ( 'page' , 1 , type = int ) per page = request . args . get ( 'per page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) groups = Group . query by user ( current user , eager = True ) if q : groups = Group . search ( groups , q ) groups = groups . paginate ( page , per page = per page ) requests = Membership . query requests ( current user ) . count ( ) invitations = Membership . query invitations ( current user ) . count ( ) return render template ( 'invenio groups/index.html' , groups = groups , requests = requests , invitations = invitations , page = page , per page = per page , q = q )", "predictions": ["show all the groups of a user ."], "references": ["list all user memberships ."], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 4105, "code": "def requests ( ) : page = request . args . get ( 'page' , 1 , type = int ) per page = request . args . get ( 'per page' , 5 , type = int ) memberships = Membership . query requests ( current user , eager = True ) . all ( ) return render template ( 'invenio groups/pending.html' , memberships = memberships , requests = True , page = page , per page = per page , )", "predictions": ["show all requests ."], "references": ["list all pending memberships listed only for group admins ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 4106, "code": "def invitations ( ) : page = request . args . get ( 'page' , 1 , type = int ) per page = request . args . get ( 'per page' , 5 , type = int ) memberships = Membership . query invitations ( current user , eager = True ) . all ( ) return render template ( 'invenio groups/pending.html' , memberships = memberships , page = page , per page = per page , )", "predictions": ["show a list of all invitations ."], "references": ["list all user pending memberships ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 4107, "code": "def new ( ) : form = Group Form ( request . form ) if form . validate on submit ( ) : try : group = Group . create ( admins = [ current user ] , * * form . data ) flash ( ( 'Group \"%(name)s\" created' , name = group . name ) , 'success' ) return redirect ( url for ( \".index\" ) ) except Integrity Error : flash ( ( 'Group creation failure' ) , 'error' ) return render template ( \"invenio groups/new.html\" , form = form , )", "predictions": ["create a new group ."], "references": ["create new group ."], "bleu": 0.537284965911771, "rouge_l": 0.9070631970260222}
{"id": 4108, "code": "def manage ( group id ) : group = Group . query . get or 404 ( group id ) form = Group Form ( request . form , obj = group ) if form . validate on submit ( ) : if group . can edit ( current user ) : try : group . update ( * * form . data ) flash ( ( 'Group \"%(name)s\" was updated' , name = group . name ) , 'success' ) except Exception as e : flash ( str ( e ) , 'error' ) return render template ( \"invenio groups/new.html\" , form = form , group = group , ) else : flash ( ( 'You cannot edit group %(group name)s' , group name = group . name ) , 'error' ) return render template ( \"invenio groups/new.html\" , form = form , group = group , )", "predictions": ["manage a group ."], "references": ["manage your group ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 4109, "code": "def members ( group id ) : page = request . args . get ( 'page' , 1 , type = int ) per page = request . args . get ( 'per page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get or 404 ( group id ) if group . can see members ( current user ) : members = Membership . query by group ( group id , with invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per page = per page ) return render template ( \"invenio groups/members.html\" , group = group , members = members , page = page , per page = per page , q = q , s = s , ) flash ( ( 'You are not allowed to see members of this group %(group name)s.' , group name = group . name ) , 'error' ) return redirect ( url for ( '.index' ) )", "predictions": ["show all members of a group ."], "references": ["list user group members ."], "bleu": 0.22089591134157885, "rouge_l": 0.34366197183098596}
{"id": 4110, "code": "def approve ( group id , user id ) : membership = Membership . query . get or 404 ( ( user id , group id ) ) group = membership . group if group . can edit ( current user ) : try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url for ( '.requests' , group id = membership . group . id ) ) flash ( ( '%(user)s accepted to %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url for ( '.requests' , group id = membership . group . id ) ) flash ( ( 'You cannot approve memberships for the group %(group name)s' , group name = group . name ) , 'error' ) return redirect ( url for ( '.index' ) )", "predictions": ["approve a group %(group"], "references": ["approve a user ."], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 4111, "code": "def remove ( group id , user id ) : group = Group . query . get or 404 ( group id ) user = User . query . get or 404 ( user id ) if group . can edit ( current user ) : try : group . remove member ( user ) except Exception as e : flash ( str ( e ) , \"error\" ) return redirect ( urlparse ( request . referrer ) . path ) flash ( ( 'User %(user email)s was removed from %(group name)s group.' , user email = user . email , group name = group . name ) , 'success' ) return redirect ( urlparse ( request . referrer ) . path ) flash ( ( 'You cannot delete users of the group %(group name)s' , group name = group . name ) , 'error' ) return redirect ( url for ( '.index' ) )", "predictions": ["remove a user from a group ."], "references": ["remove user from a group ."], "bleu": 0.7071067811865475, "rouge_l": 0.9360613810741688}
{"id": 4112, "code": "def accept ( group id ) : membership = Membership . query . get or 404 ( ( current user . get id ( ) , group id ) ) try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url for ( '.invitations' , group id = membership . group . id ) ) flash ( ( 'You are now part of %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url for ( '.invitations' , group id = membership . group . id ) )", "predictions": ["accept a group ."], "references": ["accpet pending invitation ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4113, "code": "def translate particles ( s , max mem = 1e9 , desc = '' , min rad = 'calc' , max rad = 'calc' , invert = 'guess' , rz order = 0 , do polish = True ) : if desc is not None : desc trans = desc + 'translate-particles' desc burn = desc + 'addsub burn' desc polish = desc + 'addsub polish' else : desc trans , desc burn , desc polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n loop = 4 , fractol = 0.1 , desc = desc trans , max mem = max mem , include rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n loop = 4 , fractol = 0.05 , desc = desc trans , max mem = max mem , include rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add subtract ( s , tries = 30 , min rad = min rad , max rad = max rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n loop = 3 , fractol = 3e-4 , desc = desc burn , max mem = max mem , rz order = rz order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n loop = 4 , fractol = 3e-4 , desc = desc polish , max mem = max mem , rz order = rz order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )", "predictions": ["translate a particles ."], "references": ["workhorse for translating particles . see get_particles_featuring for docs ."], "bleu": 0.10551173833795614, "rouge_l": 0.26521739130434785}
{"id": 4114, "code": "def link zscale ( st ) : psf = st . get ( 'psf' ) psf . param dict [ 'zscale' ] = psf . param dict [ 'psf-zscale' ] psf . params [ psf . params . index ( 'psf-zscale' ) ] = 'zscale' psf . global zscale = True psf . param dict . pop ( 'psf-zscale' ) st . trigger parameter change ( ) st . reset ( )", "predictions": ["link a link to a link"], "references": ["links the state st psf zscale with the global zscale"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 4115, "code": "def check for inception ( self , root dict ) : for key in root dict : if isinstance ( root dict [ key ] , dict ) : root dict [ key ] = Response Object ( root dict [ key ] ) return root dict", "predictions": ["check if inception is inception"], "references": ["used to check if there is a dict in a dict"], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 4116, "code": "def barnes ( self , pos ) : b in = self . b in dist = lambda x : np . sqrt ( np . dot ( x , x ) ) #we take a filter size as the max distance between the grids along #x or y: sz = self . npts [ 1 ] coeffs = self . get values ( self . barnes params ) b = Barnes Interpolation ND ( b in , coeffs , filter size = self . filtsize , damp = 0.9 , iterations = 3 , clip = self . local updates , clipsize = self . barnes clip size , blocksize = 100 ) return b ( pos )", "predictions": ["return a unit updates of the cell"], "references": ["creates a barnes interpolant & calculates its values"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4117, "code": "def schedules ( self , schedules ) : url = PATHS [ 'UPDATE SCHEDULES' ] % self . id data format = \"schedules[0][%s][]=%s&\" post data = \"\" for format type , values in schedules . iteritems ( ) : for value in values : post data += data format % ( format type , value ) self . api . post ( url = url , data = post data )", "predictions": ["display a list of schedules for the given schedules ."], "references": ["set the posting schedules for the specified social media profile ."], "bleu": 0.22447582175704436, "rouge_l": 0.37770897832817335}
{"id": 4118, "code": "def moment ( p , v , order = 1 ) : if order == 1 : return ( v * p ) . sum ( ) elif order == 2 : return np . sqrt ( ( ( v ** 2 ) * p ) . sum ( ) - ( v * p ) . sum ( ) ** 2 )", "predictions": ["computes the moment between two values"], "references": ["calculates the moments of the probability distribution p with vector v"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 4119, "code": "def tz ( self , z ) : return ( z - self . param dict [ 'psf-zslab' ] ) * self . param dict [ self . zscale ]", "predictions": ["returns the cross product of the query ."], "references": ["transform z to real - space coordinates from tile coordinates"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 4120, "code": "def characterize psf ( self ) : l , u = max ( self . zrange [ 0 ] , self . param dict [ 'psf-zslab' ] ) , self . zrange [ 1 ] size l , drift l = self . measure size drift ( l ) size u , drift u = self . measure size drift ( u ) self . support = util . oddify ( 2 * self . support factor * size u . astype ( 'int' ) ) self . drift poly = np . polyfit ( [ l , u ] , [ drift l , drift u ] , 1 ) if self . cutoffval is not None : psf , vec , size l = self . psf slice ( l , size = 51 , zoffset = drift l , getextent = True ) psf , vec , size u = self . psf slice ( u , size = 51 , zoffset = drift u , getextent = True ) ss = [ np . abs ( i ) . sum ( axis = - 1 ) for i in [ size l , size u ] ] self . support = util . oddify ( util . amax ( * ss ) )", "predictions": ["characterize the psf psf psf psf ."], "references": ["get support size and drift polynomial for current set of params"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 4121, "code": "def psffunc ( self , * args , * * kwargs ) : if self . polychromatic : func = psfcalc . calculate polychrome linescan psf else : func = psfcalc . calculate linescan psf return func ( * args , * * kwargs )", "predictions": ["decorator to make sure the function is psffunc with the given function ."], "references": ["calculates a linescan psf"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 4122, "code": "def psffunc ( self , x , y , z , * * kwargs ) : #do pinhole?? FIXME if self . polychromatic : func = psfcalc . calculate polychrome pinhole psf else : func = psfcalc . calculate pinhole psf x0 , y0 = [ psfcalc . vec to halfvec ( v ) for v in [ x , y ] ] vls = psfcalc . wrap and calc psf ( x0 , y0 , z , func , * * kwargs ) return vls / vls . sum ( )", "predictions": ["evaluate the density function"], "references": ["calculates a pinhole psf"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4123, "code": "def characterize psf ( self ) : l , u = max ( self . zrange [ 0 ] , self . param dict [ 'psf-zslab' ] ) , self . zrange [ 1 ] size l , drift l = self . measure size drift ( l , size = self . support ) size u , drift u = self . measure size drift ( u , size = self . support ) self . drift poly = np . polyfit ( [ l , u ] , [ drift l , drift u ] , 1 )", "predictions": ["characterize the psf psf psf ."], "references": ["get support size and drift polynomial for current set of params"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 4124, "code": "def req ( self , url , method = 'GET' , * * kw ) : send = requests . post if method == 'POST' else requests . get try : r = send ( url , headers = self . token header ( ) , timeout = self . settings [ 'timeout' ] , * * kw ) except requests . exceptions . Timeout : raise Api Error ( 'Request timed out (%s seconds)' % self . settings [ 'timeout' ] ) try : json = r . json ( ) except Value Error : raise Api Error ( 'Received not JSON response from API' ) if json . get ( 'status' ) != 'ok' : raise Api Error ( 'API error: received unexpected json from API: %s' % json ) return json", "predictions": ["make a request to the api ."], "references": ["make request and convert json response to python objects"], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 4125, "code": "def get active bets ( self , project id = None ) : url = urljoin ( self . settings [ 'bets url' ] , 'bets?state=fresh,active,accept end&page=1&page size=100' ) if project id is not None : url += '&kava project id={}' . format ( project id ) bets = [ ] has next page = True while has next page : res = self . req ( url ) bets . extend ( res [ 'bets' ] [ 'results' ] ) url = res [ 'bets' ] . get ( 'next' ) has next page = bool ( url ) return bets", "predictions": ["get the active bets"], "references": ["returns all active bets"], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 4126, "code": "def subscribe ( self , event , bet ids ) : if not self . subscriptions . get ( event ) : self . subscriptions [ event ] = set ( ) self . subscriptions [ event ] = self . subscriptions [ event ] . union ( bet ids )", "predictions": ["subscribe to an event"], "references": ["subscribe to event for given bet ids ."], "bleu": 0.18693159143202892, "rouge_l": 0.47164948453608246}
{"id": 4127, "code": "def preview ( context ) : config = context . obj pelican ( config , '--verbose' , '--ignore-cache' ) server proc = None os . chdir ( config [ 'OUTPUT DIR' ] ) try : try : command = 'python -m http.server ' + str ( PORT ) server proc = run ( command , bg = True ) time . sleep ( 3 ) click . launch ( 'http://localhost:8000' ) time . sleep ( 5 ) pelican ( config , '--autoreload' ) except Exception : if server proc is not None : server proc . kill ( ) raise except Keyboard Interrupt : abort ( context )", "predictions": ["preview the main thread ."], "references": ["opens local preview of your blog website"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4128, "code": "def write ( context ) : config = context . obj title = click . prompt ( 'Title' ) author = click . prompt ( 'Author' , default = config . get ( 'DEFAULT AUTHOR' ) ) slug = slugify ( title ) creation date = datetime . now ( ) basename = '{:%Y-%m-%d} {}.md' . format ( creation date , slug ) meta = ( ( 'Title' , title ) , ( 'Date' , '{:%Y-%m-%d %H:%M}:00' . format ( creation date ) ) , ( 'Modified' , '{:%Y-%m-%d %H:%M}:00' . format ( creation date ) ) , ( 'Author' , author ) , ) file content = '' for key , value in meta : file content += '{}: {}\\n' . format ( key , value ) file content += '\\n\\n' file content += 'Text...\\n\\n' file content += '![image description]({filename}/images/my-photo.jpg)\\n\\n' file content += 'Text...\\n\\n' os . makedirs ( config [ 'CONTENT DIR' ] , exist ok = True ) path = os . path . join ( config [ 'CONTENT DIR' ] , basename ) with click . open file ( path , 'w' ) as f : f . write ( file content ) click . echo ( path ) click . launch ( path )", "predictions": ["get the current config for the given self for the current self for the given self for the self for user for the given self for the given self for the"], "references": ["starts a new article"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4129, "code": "def lint ( context ) : config = context . obj try : run ( 'flake8 {dir} --exclude={exclude}' . format ( dir = config [ 'CWD' ] , exclude = ',' . join ( EXCLUDE ) , ) ) except Subprocess Error : context . exit ( 1 )", "predictions": ["calc the code ."], "references": ["looks for errors in source code of your blog"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4130, "code": "def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\\n Continue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m \"{message}\"' . format ( message = 'Publishing {}' . format ( choose commit emoji ( ) ) ) , capture = True ) except subprocess . Called Process Error as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to Git Hub...' ) branch = get branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr link = get pr link ( branch ) if pr link : click . launch ( pr link )", "predictions": ["trigger the current commit commit"], "references": ["saves changes and sends them to github"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4131, "code": "def deploy ( context ) : config = context . obj header ( 'Generating HTML...' ) pelican ( config , '--verbose' , production = True ) header ( 'Removing unnecessary output...' ) unnecessary paths = [ 'author' , 'category' , 'tag' , 'feeds' , 'tags.html' , 'authors.html' , 'categories.html' , 'archives.html' , ] for path in unnecessary paths : remove path ( os . path . join ( config [ 'OUTPUT DIR' ] , path ) ) if os . environ . get ( 'TRAVIS' ) : header ( 'Setting up Git...' ) run ( 'git config user.name ' + run ( 'git show --format=\"%c N\" -s' , capture = True ) ) run ( 'git config user.email ' + run ( 'git show --format=\"%c E\" -s' , capture = True ) ) github token = os . environ . get ( 'GITHUB TOKEN' ) repo slug = os . environ . get ( 'TRAVIS REPO SLUG' ) origin = 'https://{}@github.com/{}.git' . format ( github token , repo slug ) run ( 'git remote set-url origin ' + origin ) header ( 'Rewriting gh-pages branch...' ) run ( 'ghp-import -m \"{message}\" {dir}' . format ( message = 'Deploying {}' . format ( choose commit emoji ( ) ) , dir = config [ 'OUTPUT DIR' ] , ) ) header ( 'Pushing to Git Hub...' ) run ( 'git push origin gh-pages:gh-pages --force' )", "predictions": ["get the current virtualenv from github"], "references": ["uploads new version of the blog website"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4132, "code": "def signed number ( number , precision = 2 ) : prefix = '' if number <= 0 else '+' number str = '{}{:.{precision}f}' . format ( prefix , number , precision = precision ) return number str", "predictions": ["convert a set of set to a set of decimals numbers"], "references": ["return the given number as a string with a sign in front of it ie . + if the number is positive - otherwise ."], "bleu": 0.0379378273825362, "rouge_l": 0.15574468085106383}
{"id": 4133, "code": "def show response messages ( response json ) : message type kwargs = { 'warning' : { 'fg' : 'yellow' } , 'error' : { 'fg' : 'red' } , } for message in response json . get ( 'messages' , [ ] ) : click . secho ( message [ 'text' ] , * * message type kwargs . get ( message [ 'type' ] , { } ) )", "predictions": ["sync the params in the params any params any params any other other than the params any ."], "references": ["show all messages in the messages key of the given dict ."], "bleu": 0.101824256461955, "rouge_l": 0.27664399092970515}
{"id": 4134, "code": "def photos ( context , path ) : config = context . obj header ( 'Looking for the latest article...' ) article filename = find last article ( config [ 'CONTENT DIR' ] ) if not article filename : return click . secho ( 'No articles.' , fg = 'red' ) click . echo ( os . path . basename ( article filename ) ) header ( 'Looking for images...' ) images = list ( sorted ( find images ( path ) ) ) if not images : return click . secho ( 'Found no images.' , fg = 'red' ) for filename in images : click . secho ( filename , fg = 'green' ) if not click . confirm ( '\\n Add these images to the latest article' ) : abort ( config ) url prefix = os . path . join ( '{filename}' , IMAGES PATH ) images dir = os . path . join ( config [ 'CONTENT DIR' ] , IMAGES PATH ) os . makedirs ( images dir , exist ok = True ) header ( 'Processing images...' ) urls = [ ] for filename in images : image basename = os . path . basename ( filename ) . replace ( ' ' , '-' ) . lower ( ) urls . append ( os . path . join ( url prefix , image basename ) ) image filename = os . path . join ( images dir , image basename ) print ( filename , image filename ) import image ( filename , image filename ) content = '\\n' for url in urls : url = url . replace ( '\\\\' , '/' ) content += '\\n![image description]({})\\n' . format ( url ) header ( 'Adding to article: {}' . format ( article filename ) ) with click . open file ( article filename , 'a' ) as f : f . write ( content ) click . launch ( article filename )", "predictions": ["command to transform read images into environ for your environ for later for later for later for later for later for later for later for later later for later for later"], "references": ["adds images to the last article"], "bleu": 0.04317900023606586, "rouge_l": 0.061553985872855696}
{"id": 4135, "code": "def generate circle ( self ) : total weight = 0 for node in self . nodes : total weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total weight ) for j in range ( 0 , int ( factor ) ) : b key = bytearray ( self . hash digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . hash val ( b key , lambda x : x + i * 4 ) self . ring [ key ] = node self . sorted keys . append ( key ) self . sorted keys . sort ( )", "predictions": ["get a group of the weights"], "references": ["generates the circle ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4136, "code": "def get networking mode ( app ) : networks = app . get ( 'networks' ) if networks : return networks [ - 1 ] . get ( 'mode' , 'container' ) container = app . get ( 'container' ) if container is not None and 'docker' in container : docker network = container [ 'docker' ] . get ( 'network' ) if docker network == 'USER' : return 'container' elif docker network == 'BRIDGE' : return 'container/bridge' return 'container' if is legacy ip per task ( app ) else 'host'", "predictions": ["1 - container current networking"], "references": ["get the marathon networking mode for the app ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4137, "code": "def get container port mappings ( app ) : container = app [ 'container' ] port mappings = container . get ( 'port Mappings' ) if port mappings is None and 'docker' in container : port mappings = container [ 'docker' ] . get ( 'port Mappings' ) return port mappings", "predictions": ["query the container port for the container port"], "references": ["get the portmappings field for the app container ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 4138, "code": "def request ( self , failure , endpoints , * args , * * kwargs ) : if not endpoints : return failure endpoint = endpoints . pop ( 0 ) d = super ( Marathon Client , self ) . request ( * args , url = endpoint , * * kwargs ) d . add Errback ( self . request , endpoints , * args , * * kwargs ) return d", "predictions": ["make a invitations invitations ."], "references": ["recursively make requests to each endpoint in endpoints ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4139, "code": "def read ( self , path , * * params ) : d = self . request ( 'GET' , '/v1/' + path , params = params ) return d . add Callback ( self . handle response )", "predictions": ["parses a domain request"], "references": ["read data from vault . returns the json - decoded response ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 4140, "code": "def write ( self , path , * * data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . add Callback ( self . handle response , check cas = True )", "predictions": ["wrapper for the http manage request or return a deferred or a result or a result"], "references": ["write data to vault . returns the json - decoded response ."], "bleu": 0.07692375026049747, "rouge_l": 0.07331730769230768}
{"id": 4141, "code": "def parse field value ( line ) : if line . startswith ( ':' ) : return None , None if ':' not in line : return line , '' field , value = line . split ( ':' , 1 ) value = value [ 1 : ] if value . startswith ( ' ' ) else value return field , value", "predictions": ["members from a field line"], "references": ["parse the field and value from a line ."], "bleu": 0.18343778145675418, "rouge_l": 0.40757238307349664}
{"id": 4142, "code": "def handle field value ( self , field , value ) : if field == 'event' : self . event = value elif field == 'data' : self . data lines . append ( value ) elif field == 'id' : pass elif field == 'retry' : pass", "predictions": ["approve a field entry"], "references": ["handle the field value pair ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4143, "code": "def dispatch event ( self ) : data = self . prepare data ( ) if data is not None : self . handler ( self . event , data ) self . reset event data ( )", "predictions": ["remove the 404 event event and if necessary = none = true"], "references": ["dispatch the event to the handler ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 4144, "code": "def issue cert ( self , domain ) : def errback ( failure ) : failure . trap ( txacme Server Error ) acme error = failure . value . message if acme error . code in [ 'rate Limited' , 'server Internal' , 'connection' , 'unknown Host' ] : self . log . error ( 'Error ({code}) issuing certificate for \"{domain}\": ' '{detail}' , code = acme error . code , domain = domain , detail = acme error . detail ) else : return failure d = self . txacme service . issue cert ( domain ) return d . add Errback ( errback )", "predictions": ["return an accept certificate for an accept accept query query query query query query query query query query query query query query query query query query query query query query query"], "references": ["issue a certificate for the given domain ."], "bleu": 0.0513487742994337, "rouge_l": 0.1147695202257761}
{"id": 4145, "code": "def start ( self ) : self . bot start time = datetime . now ( ) self . webserver = Webserver ( self . config [ 'webserver' ] [ 'host' ] , self . config [ 'webserver' ] [ 'port' ] ) self . plugins . load ( ) self . plugins . load state ( ) self . find event handlers ( ) self . sc = Threaded Slack Client ( self . config [ 'slack token' ] ) self . always send dm = [ ' unauthorized ' ] if 'always send dm' in self . config : self . always send dm . extend ( map ( lambda x : '!' + x , self . config [ 'always send dm' ] ) ) logging . get Logger ( 'Rocket.Errors.Thread Pool' ) . set Level ( logging . INFO ) self . is setup = True if self . test mode : self . metrics [ 'startup time' ] = ( datetime . now ( ) - self . bot start time ) . total seconds ( ) * 1000.0", "predictions": ["translate the mem polish"], "references": ["initializes the bot plugins and everything ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4146, "code": "def stop ( self ) : if self . webserver is not None : self . webserver . stop ( ) if not self . test mode : self . plugins . save state ( )", "predictions": ["link the event to the queue psf psf psf psf psf ."], "references": ["does cleanup of bot and plugins ."], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 4147, "code": "def push ( self , message ) : if self . ignore event ( message ) : return None , None args = self . parse message ( message ) self . log . debug ( \"Searching for command using chunks: %s\" , args ) cmd , msg args = self . find longest prefix command ( args ) if cmd is not None : if message . user is None : self . log . debug ( \"Discarded message with no originating user: %s\" , message ) return None , None sender = message . user . username if message . channel is not None : sender = \"#%s/%s\" % ( message . channel . name , sender ) self . log . info ( \"Received from %s: %s, args %s\" , sender , cmd , msg args ) f = self . get command ( cmd , message . user ) if f : if self . is channel ignored ( f , message . channel ) : self . log . info ( \"Channel %s is ignored, discarding command %s\" , message . channel , cmd ) return ' ignored ' , \"\" return cmd , f . execute ( message , msg args ) return ' unauthorized ' , \"Sorry, you are not authorized to run %s\" % cmd return None , None", "predictions": ["check a self . self . ."], "references": ["takes a slackevent parses it for a command and runs against registered plugin"], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 4148, "code": "def acl show ( self , msg , args ) : name = args [ 0 ] if len ( args ) > 0 else None if name is None : return \"%s: The following AC Ls are defined: %s\" % ( msg . user , ', ' . join ( self . acl . keys ( ) ) ) if name not in self . acl : return \"Sorry, couldn't find an acl named '%s'\" % name return '\\n' . join ( [ \"%s: ACL '%s' is defined as follows:\" % ( msg . user , name ) , \"allow: %s\" % ', ' . join ( self . acl [ name ] [ 'allow' ] ) , \"deny: %s\" % ', ' . join ( self . acl [ name ] [ 'deny' ] ) ] )", "predictions": ["show a digest for an barnes named"], "references": ["show current allow and deny blocks for the given acl ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4149, "code": "def add user to allow ( self , name , user ) : if not self . remove user from acl ( name , user ) : return False if name not in self . acl : return False self . acl [ name ] [ 'allow' ] . append ( user ) return True", "predictions": ["schedules a user to allow"], "references": ["add a user to the given acl allow block ."], "bleu": 0.19765609300943976, "rouge_l": 0.5030927835051546}
{"id": 4150, "code": "def create acl ( self , name ) : if name in self . acl : return False self . acl [ name ] = { 'allow' : [ ] , 'deny' : [ ] } return True", "predictions": ["moment a acl"], "references": ["create a new acl ."], "bleu": 0.3052796454588787, "rouge_l": 0.47843137254901963}
{"id": 4151, "code": "def delete acl ( self , name ) : if name not in self . acl : return False del self . acl [ name ] return True", "predictions": ["tz a acl"], "references": ["delete an acl ."], "bleu": 0.38498150077635496, "rouge_l": 0.2785388127853881}
{"id": 4152, "code": "def mongo ( daemon = False , port = 20771 ) : cmd = \"mongod --port {0}\" . format ( port ) if daemon : cmd += \" --fork\" run ( cmd )", "predictions": ["run a migration server"], "references": ["run the mongod process ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 4153, "code": "def get by username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None", "predictions": ["return up a user by self = self = self = self = self = self = self = self = self = self = self = self = self ="], "references": ["retrieve user by username"], "bleu": 0.0513487742994337, "rouge_l": 0.13275299238302501}
{"id": 4154, "code": "def load user rights ( self , user ) : if user . username in self . admins : user . is admin = True elif not hasattr ( user , 'is admin' ) : user . is admin = False", "predictions": ["load user of user"], "references": ["sets permissions on user object"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 4155, "code": "def freeze ( value ) : if isinstance ( value , list ) : return Frozen List ( * value ) if isinstance ( value , dict ) : return Frozen Dict ( * * value ) return value", "predictions": ["characterize a string into a dictionary"], "references": ["cast value to its frozen counterpart ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 4156, "code": "def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is subcmd is False , commands ) if self . should filter help commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin only is False , commands ) for name , cmd in commands : output . append ( self . get short help for command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . get help for command ( name ) ] return '\\n' . join ( output )", "predictions": ["list all available if no user is provided ."], "references": ["displays help for each command"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4157, "code": "def save ( self , msg , args ) : self . send message ( msg . channel , \"Saving current state...\" ) self . bot . plugins . save state ( ) self . send message ( msg . channel , \"Done.\" )", "predictions": ["get an outgoing message"], "references": ["causes the bot to write its current state to backend ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 4158, "code": "def shutdown ( self , msg , args ) : self . log . info ( \"Received shutdown from %s\" , msg . user . username ) self . bot . runnable = False return \"Shutting down...\"", "predictions": ["subscribe to user ."], "references": ["causes the bot to gracefully shutdown ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 4159, "code": "def whoami ( self , msg , args ) : output = [ \"Hello %s\" % msg . user ] if hasattr ( self . bot . dispatcher , 'auth manager' ) and msg . user . is admin is True : output . append ( \"You are a *bot admin*.\" ) output . append ( \"Bot version: %s-%s\" % ( self . bot . version , self . bot . commit ) ) return '\\n' . join ( output )", "predictions": ["serialize the user and its message to a message"], "references": ["prints information about the user and bot version ."], "bleu": 0.2626909894424158, "rouge_l": 0.3333333333333333}
{"id": 4160, "code": "def arg name ( self , name , types , prefix = \"--\" ) : if 'type:a10 nullable' in types : return self . arg name ( name , types [ 'type:a10 nullable' ] , prefix ) if 'type:a10 list' in types : return self . arg name ( name , types [ 'type:a10 list' ] , prefix ) if 'type:a10 reference' in types : if name . endswith ( ' id' ) : name = name [ : - 3 ] return prefix + name . replace ( ' ' , '-' )", "predictions": ["return the argument name for an argument ."], "references": ["-- shish - kabob it"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4161, "code": "def sort by ( key ) : @ staticmethod def sort by ( p list , reverse = False ) : return sorted ( p list , key = lambda p : getattr ( p , key ) , reverse = reverse , ) return sort by", "predictions": ["sort a list by a set of values by a list ."], "references": ["high order function for sort methods ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 4162, "code": "def n file ( self ) : self . assert is dir and exists ( ) n = 0 for in self . select file ( recursive = True ) : n += 1 return n", "predictions": ["return the number of files in the directory ."], "references": ["count how many files in this directory . including file in sub folder ."], "bleu": 0.1340110063389608, "rouge_l": 0.3347050754458162}
{"id": 4163, "code": "def n dir ( self ) : self . assert is dir and exists ( ) n = 0 for in self . select dir ( recursive = True ) : n += 1 return n", "predictions": ["return a list of all directories in the directory ."], "references": ["count how many folders in this directory . including folder in sub folder ."], "bleu": 0.11950151429308975, "rouge_l": 0.24270557029177717}
{"id": 4164, "code": "def acquire lock ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with self . locker as r : acquired , code , = r if acquired : try : r = func ( self , * args , * * kwargs ) except Exception as err : e = str ( err ) else : e = None else : warnings . warn ( \"code %s. Unable to aquire the lock when calling '%s'. You may try again!\" % ( code , func . name ) ) e = None r = None if e is not None : traceback . print stack ( ) raise Exception ( e ) return r return wrapper", "predictions": ["decorator to acquire a lock ."], "references": ["decorate methods when locking repository is required ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4165, "code": "def sync required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if not self . keep Synchronized : r = func ( self , * args , * * kwargs ) else : state = self . load state ( ) #print(\"----------->  \",state, self.state) if state is None : r = func ( self , * args , * * kwargs ) elif state == self . state : r = func ( self , * args , * * kwargs ) else : warnings . warn ( \"Repository at '%s' is out of date. Need to load it again to avoid conflict.\" % self . path ) r = None return r return wrapper", "predictions": ["decorator to make sure that a function is required to avoid avoid ."], "references": ["decorate methods when synchronizing repository is required ."], "bleu": 0.1350862565735141, "rouge_l": 0.2985318107667211}
{"id": 4166, "code": "def get pickling errors ( obj , seen = None ) : if seen == None : seen = [ ] if hasattr ( obj , \" getstate \" ) : state = obj . getstate ( ) #elif hasattr(obj, \" dict \"): else : return None #try: #except Attribute Error as e: if state == None : return 'object state is None' if isinstance ( state , tuple ) : if not isinstance ( state [ 0 ] , dict ) : state = state [ 1 ] else : state = state [ 0 ] . update ( state [ 1 ] ) result = { } for i in state : try : pickle . dumps ( state [ i ] , protocol = 2 ) except pickle . Pickling Error as e : if not state [ i ] in seen : seen . append ( state [ i ] ) result [ i ] = get pickling errors ( state [ i ] , seen ) return result", "predictions": ["get errors from pickling ."], "references": ["investigate pickling errors ."], "bleu": 0.32466791547509893, "rouge_l": 0.4535315985130111}
{"id": 4167, "code": "def save ( self ) : repo Info Path = os . path . join ( self . path , \".pyrepinfo\" ) try : fdinfo = open ( repo Info Path , 'wb' ) except Exception as e : raise Exception ( \"unable to open repository info for saving (%s)\" % e ) try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( \"Unable to save repository info (%s)\" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) repo Time Path = os . path . join ( self . path , \".pyrepstate\" ) try : self . state = ( \"%.6f\" % time . time ( ) ) . encode ( ) with open ( repo Time Path , 'wb' ) as fdtime : fdtime . write ( self . state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( \"unable to open repository time stamp for saving (%s)\" % e )", "predictions": ["save the repository info to disk"], "references": ["save repository . pyrepinfo to disk ."], "bleu": 0.27960682295094563, "rouge_l": 0.6069651741293532}
{"id": 4168, "code": "def ensure str ( value ) : if isinstance ( value , six . string types ) : return value else : return six . text type ( value )", "predictions": ["ensure that a value is a string"], "references": ["ensure value is string ."], "bleu": 0.2777619034011791, "rouge_l": 0.6873239436619719}
{"id": 4169, "code": "def stream ( self , report ) : with self . Client Session ( ) as session : lines = [ ] for job in report [ 'traces' ] : key = '%s:%s' % ( self . name , job ) for minute in report [ 'traces' ] [ job ] : for k , v in report [ 'traces' ] [ job ] [ minute ] . items ( ) : lines . append ( % ( key , k ) ) lines . append ( '%s %s %0.2f' % ( key , k , v ) ) lines . append ( \"\" ) data = \"\\n\" . join ( lines ) logger . info ( data ) yield from session . post ( self . url , data = bytes ( data . encode ( 'utf-8' ) ) )", "predictions": ["generator for the jobs in a report"], "references": ["stream reports to application logs"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4170, "code": "def stream ( self , report ) : payload = { \"agent\" : { \"host\" : report [ 'instance' ] [ 'hostname' ] , \"version\" : \"1.0.0\" } , \"components\" : [ { \"name\" : self . name , \"guid\" : \"com.darwinmonroy.aiometrics\" , \"duration\" : 60 , \"metrics\" : { 'Component/{}' . format ( key ) : { \"total\" : metric [ 'count' ] * metric [ 'avg' ] , \"count\" : metric [ 'count' ] , \"min\" : metric [ 'min' ] , \"max\" : metric [ 'max' ] , \"sum of squares\" : metric [ 'min' ] ** 2 + metric [ 'max' ] ** 2 , } for key , metric in report [ 'traces' ] . items ( ) } } ] } with self . Client Session ( ) as session : try : r = yield from session . post ( 'https://platform-api.newrelic.com/platform/v1/metrics' , data = json . dumps ( payload ) , headers = ( ( 'X-License-Key' , self . license key ) , ( 'Content-Type' , 'application/json' ) , ( 'Accept' , 'application/json' ) , ) ) r . close ( ) except Exception as e : logger . exception ( e )", "predictions": ["send a stream to a report"], "references": ["stream reports to application logs"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 4171, "code": "def stats ( cls , traces ) : data = { } stats = { } for trace in traces : key = trace [ 'key' ] if key not in data : data [ key ] = [ ] stats [ key ] = { } data [ key ] . append ( trace [ 'total time' ] ) cls . traces . pop ( trace [ 'id' ] ) for key in data : times = data [ key ] stats [ key ] = dict ( count = len ( times ) , max = max ( times ) , min = min ( times ) , avg = sum ( times ) / len ( times ) ) return stats", "predictions": ["get status for a traces ."], "references": ["build per minute stats for each key"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4172, "code": "def stop ( self ) : with self . status lock : if self . running : assert self . observer is not None self . observer . stop ( ) self . running = False self . origin mapped data = dict ( )", "predictions": ["stop the observer ."], "references": ["stops monitoring the predefined directory ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 4173, "code": "def tear down ( self ) : while len ( self . temp directories ) > 0 : directory = self . temp directories . pop ( ) shutil . rmtree ( directory , ignore errors = True ) while len ( self . temp files ) > 0 : file = self . temp files . pop ( ) try : os . remove ( file ) except OS Error : pass", "predictions": ["tear down all temp files ."], "references": ["tears down all temp files and directories ."], "bleu": 0.46105843756805864, "rouge_l": 0.6963470319634703}
{"id": 4174, "code": "def copyto ( self , new abspath = None , new dirpath = None , new dirname = None , new basename = None , new fname = None , new ext = None , overwrite = False , makedirs = False ) : self . assert exists ( ) p = self . change ( new abspath = new abspath , new dirpath = new dirpath , new dirname = new dirname , new basename = new basename , new fname = new fname , new ext = new ext , ) if p . is not exist or allow overwrite ( overwrite = overwrite ) : if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IO Error as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p", "predictions": ["return a new file with a given attribute ."], "references": ["copy this file to other place ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4175, "code": "def get dump method ( dump , protocol = - 1 ) : if dump is None : dump = 'pickle' if dump . startswith ( 'pickle' ) : if dump == 'pickle' : proto = protocol else : proto = dump . strip ( 'pickle' ) try : proto = int ( proto ) assert proto >= - 1 except : raise Exception ( \"protocol must be an integer >=-1\" ) code = % proto elif dump . startswith ( 'dill' ) : if dump == 'dill' : proto = 2 else : proto = dump . strip ( 'dill' ) try : proto = int ( proto ) assert proto >= - 1 except : raise Exception ( \"protocol must be an integer >=-1\" ) code = % proto elif dump == 'json' : code = elif dump == 'numpy' : code = elif dump == 'numpy text' : code = else : assert isinstance ( dump , basestring ) , \"dump must be None or a string\" assert '$FILE PATH' in dump , \"string dump code must inlcude '$FILE PATH'\" code = dump return code", "predictions": ["return a dump method from a dump dump ."], "references": ["get dump function code string"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4176, "code": "def get pull method ( pull ) : if pull is None or pull . startswith ( 'pickle' ) : code = elif pull . startswith ( 'dill' ) : code = elif pull == 'json' : code = elif pull == 'numpy' : code = elif pull == 'numpy text' : code = else : assert isinstance ( pull , basestring ) , \"pull must be None or a string\" assert 'PULLED DATA' in pull , \"string pull code must inlcude 'PULLED DATA'\" assert '$FILE PATH' in pull , \"string pull code must inlcude '$FILE PATH'\" code = pull return code", "predictions": ["return the method to be used for the pull pull ."], "references": ["get pull function code string"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 4177, "code": "def path required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load repository) or initialize (Repository.create repository) the repository first !' ) return return func ( self , * args , * * kwargs ) return wrapper", "predictions": ["decorator to wraps a repository ."], "references": ["decorate methods when repository path is required ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4178, "code": "def reset ( self ) : self . path = None self . repo = { 'repository unique name' : str ( uuid . uuid1 ( ) ) , 'create utctime' : time . time ( ) , 'last update utctime' : None , 'pyrep version' : str ( version ) , 'repository information' : '' , 'walk repo' : [ ] }", "predictions": ["reset the state of the class ."], "references": ["reset repository instance ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 4179, "code": "def print big dir ( self , top n = 5 ) : self . assert is dir and exists ( ) size table = sorted ( [ ( p , p . dirsize ) for p in self . select dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size table [ : top n ] : print ( \"{:<9}    {:<9}\" . format ( repr data size ( size ) , p . abspath ) )", "predictions": ["print a directory containing all the big - big directories ."], "references": ["print top_n big dir in this dir ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 4180, "code": "def print big file ( self , top n = 5 ) : self . assert is dir and exists ( ) size table = sorted ( [ ( p , p . size ) for p in self . select file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size table [ : top n ] : print ( \"{:<9}    {:<9}\" . format ( repr data size ( size ) , p . abspath ) )", "predictions": ["print a big big file ."], "references": ["print top_n big file in this dir ."], "bleu": 0.236682065782701, "rouge_l": 0.5570776255707762}
{"id": 4181, "code": "def print big dir and big file ( self , top n = 5 ) : self . assert is dir and exists ( ) size table1 = sorted ( [ ( p , p . dirsize ) for p in self . select dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size table1 [ : top n ] : print ( \"{:<9}    {:<9}\" . format ( repr data size ( size1 ) , p1 . abspath ) ) size table2 = sorted ( [ ( p , p . size ) for p in p1 . select file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size table2 [ : top n ] : print ( \"    {:<9}    {:<9}\" . format ( repr data size ( size2 ) , p2 . abspath ) )", "predictions": ["print a directory and big - big big - big big - big big file ."], "references": ["print top_n big dir and top_n big file in each dir ."], "bleu": 0.12512236921161915, "rouge_l": 0.36658653846153844}
{"id": 4182, "code": "def size ( self ) : try : return self . stat . st size except : self . stat = self . stat ( ) return self . size", "predictions": ["the size of the underlying file ."], "references": ["file size in bytes ."], "bleu": 0.22089591134157885, "rouge_l": 0.34366197183098596}
{"id": 4183, "code": "def mtime ( self ) : try : return self . stat . st mtime except : self . stat = self . stat ( ) return self . mtime", "predictions": ["return stat . mtime from file ."], "references": ["get most recent modify time in timestamp ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4184, "code": "def atime ( self ) : try : return self . stat . st atime except : self . stat = self . stat ( ) return self . atime", "predictions": ["a list of atime objects ."], "references": ["get most recent access time in timestamp ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4185, "code": "def ctime ( self ) : try : return self . stat . st ctime except : self . stat = self . stat ( ) return self . ctime", "predictions": ["get the stat value ."], "references": ["get most recent create time in timestamp ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4186, "code": "def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional args ]", "predictions": ["return a list of all keys of this component ."], "references": ["list names of options and positional arguments ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 4187, "code": "def values ( self ) : return self . options . values ( ) + [ p . value for p in self . positional args ]", "predictions": ["list of all values of this object ."], "references": ["list values of options and positional arguments ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 4188, "code": "def items ( self ) : return [ ( p . name , p . value ) for p in self . options . values ( ) + self . positional args ]", "predictions": ["return all items as a list of strings ."], "references": ["list values of options and positional arguments ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 4189, "code": "def add option ( self , option ) : if option . name in self . options : raise Value Error ( 'name already in use' ) if option . abbreviation in self . abbreviations : raise Value Error ( 'abbreviation already in use' ) if option . name in [ arg . name for arg in self . positional args ] : raise Value Error ( 'name already in use by a positional argument' ) self . options [ option . name ] = option if option . abbreviation : self . abbreviations [ option . abbreviation ] = option self . option order . append ( option . name )", "predictions": ["add an option to the parser"], "references": ["add an option object to the user interface ."], "bleu": 0.29654680334613515, "rouge_l": 0.6434599156118143}
{"id": 4190, "code": "def optionhelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : def makelabels ( option ) : labels = '%*s--%s' % ( indent , ' ' , option . name ) if option . abbreviation : labels += ', -' + option . abbreviation return labels + ': ' docs = [ ] helpindent = autoindent ( [ makelabels ( o ) for o in self . options . values ( ) ] , indent , maxindent ) for name in self . option order : option = self . options [ name ] labels = makelabels ( option ) helpstring = \"%s(%s). %s\" % ( option . formatname , option . strvalue , option . docs ) wrapped = self . wrap labelled ( labels , helpstring , helpindent , width ) docs . extend ( wrapped ) return '\\n' . join ( docs )", "predictions": ["build the docs for the command line arguments"], "references": ["return user friendly help on program options ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4191, "code": "def posarghelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : docs = [ ] makelabel = lambda posarg : ' ' * indent + posarg . displayname + ': ' helpindent = autoindent ( [ makelabel ( p ) for p in self . positional args ] , indent , maxindent ) for posarg in self . positional args : label = makelabel ( posarg ) text = posarg . formatname + '. ' + posarg . docs wrapped = self . wrap labelled ( label , text , helpindent , width ) docs . extend ( wrapped ) return '\\n' . join ( docs )", "predictions": ["convert the text to a string ."], "references": ["return user friendly help on positional arguments in the program ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4192, "code": "def parse ( self , file ) : if isinstance ( file , basestring ) : file = open ( file ) line number = 0 label = None block = self . untagged for line in file : line number += 1 line = line . rstrip ( '\\n' ) if self . tabsize > 0 : line = line . replace ( '\\t' , ' ' * self . tabsize ) if self . decommenter : line = self . decommenter . decomment ( line ) if line is None : continue tag = line . split ( ':' , 1 ) [ 0 ] . strip ( ) if tag not in self . names : if block is None : if line and not line . isspace ( ) : raise Parse Error ( file . name , line , \"garbage before first block: %r\" % line ) continue block . addline ( line ) continue name = self . names [ tag ] label = line . split ( ':' , 1 ) [ 1 ] . strip ( ) if name in self . labelled classes : if not label : raise Parse Error ( file . name , line , \"missing label for %r block\" % name ) block = self . blocks [ name ] . setdefault ( label , self . labelled classes [ name ] ( ) ) else : if label : msg = \"label %r present for unlabelled block %r\" % ( label , name ) raise Parse Error ( file . name , line number , msg ) block = self . blocks [ name ] block . startblock ( )", "predictions": ["parse a unlabelled file"], "references": ["parse text blocks from a file ."], "bleu": 0.20183609024241697, "rouge_l": 0.5198863636363635}
{"id": 4193, "code": "def get separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''", "predictions": ["p is a by i"], "references": ["return the separator that preceding format i or for i == 0 ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 4194, "code": "def authorize url ( self ) : auth url = OAUTH ROOT + '/authorize' params = { 'client id' : self . client id , 'redirect uri' : self . redirect uri , } return \"{}?{}\" . format ( auth url , urlencode ( params ) )", "predictions": ["build the n n n n authentication authentication select select select select select select select select select select select select select select select select select select select select select"], "references": ["return a url to redirect the user to for oauth authentication ."], "bleu": 0.0462136266712202, "rouge_l": 0.10544511668107173}
{"id": 4195, "code": "def exchange token ( self , code ) : access token url = OAUTH ROOT + '/access token' params = { 'client id' : self . client id , 'client secret' : self . client secret , 'redirect uri' : self . redirect uri , 'code' : code , } resp = requests . get ( access token url , params = params ) if not resp . ok : raise Mixcloud Oauth Error ( \"Could not get access token.\" ) return resp . json ( ) [ 'access token' ]", "predictions": ["n an oauth dir"], "references": ["exchange the authorization code for an access token ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4196, "code": "def acquire ( self , * args , * * kwargs ) : with self . stat lock : self . waiting += 1 self . lock . acquire ( * args , * * kwargs ) with self . stat lock : self . locked = True self . waiting -= 1", "predictions": ["acquire the stream . . . . . . ."], "references": ["wraps lock . acquire"], "bleu": 0.13950796967929133, "rouge_l": 0.1548223350253807}
{"id": 4197, "code": "def release ( self ) : self . lock . release ( ) with self . stat lock : self . locked = False self . last released = datetime . now ( )", "predictions": ["sync the @ @ . . . . ."], "references": ["wraps lock . release"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4198, "code": "def default decoder ( self , obj ) : typename , marshalled state = self . unwrap callback ( obj ) if typename is None : return obj try : cls , unmarshaller = self . serializer . unmarshallers [ typename ] except Key Error : raise Lookup Error ( 'no unmarshaller found for type \"{}\"' . format ( typename ) ) from None if cls is not None : instance = cls . new ( cls ) unmarshaller ( instance , marshalled state ) return instance else : return unmarshaller ( marshalled state )", "predictions": ["state callback callback callback for get get requests == none ."], "references": ["handle a dict that might contain a wrapped state for a custom type ."], "bleu": 0.10312570678516415, "rouge_l": 0.2349165596919127}
{"id": 4199, "code": "def create ( quiet , name , base uri , symlink path ) : validate name ( name ) admin metadata = dtoolcore . generate admin metadata ( name ) parsed base uri = dtoolcore . utils . generous parse uri ( base uri ) if parsed base uri . scheme == \"symlink\" : if symlink path is None : raise click . Usage Error ( \"Need to specify symlink path using the -s/--symlink-path option\" ) if symlink path : base uri = dtoolcore . utils . sanitise uri ( \"symlink:\" + parsed base uri . path ) parsed base uri = dtoolcore . utils . generous parse uri ( base uri ) proto dataset = dtoolcore . generate proto dataset ( admin metadata = admin metadata , base uri = dtoolcore . utils . urlunparse ( parsed base uri ) , config path = CONFIG PATH ) if symlink path : symlink abspath = os . path . abspath ( symlink path ) proto dataset . storage broker . symlink path = symlink abspath try : proto dataset . create ( ) except dtoolcore . storagebroker . Storage Broker OS Error as err : raise click . Usage Error ( str ( err ) ) proto dataset . put readme ( \"\" ) if quiet : click . secho ( proto dataset . uri ) else : click . secho ( \"Created proto dataset \" , nl = False , fg = \"green\" ) click . secho ( proto dataset . uri ) click . secho ( \"Next steps: \" ) step = 1 if parsed base uri . scheme != \"symlink\" : click . secho ( \"{}. Add raw data, eg:\" . format ( step ) ) click . secho ( \"   dtool add item my file.txt {}\" . format ( proto dataset . uri ) , fg = \"cyan\" ) if parsed base uri . scheme == \"file\" : data path = proto dataset . storage broker . data abspath click . secho ( \"   Or use your system commands, e.g: \" ) click . secho ( \"   mv my data directory {}/\" . format ( data path ) , fg = \"cyan\" ) step = step + 1 click . secho ( \"{}. Add descriptive metadata, e.g: \" . format ( step ) ) click . secho ( \"   dtool readme interactive {}\" . format ( proto dataset . uri ) , fg = \"cyan\" ) step = step + 1 click . secho ( \"{}. Convert the proto dataset into a dataset: \" . format ( step ) ) click . secho ( \"   dtool freeze {}\" . format ( proto dataset . uri ) , fg = \"cyan\" )", "predictions": ["save a single dataset dataset to the database . . . . . . . . . . ."], "references": ["create a proto dataset ."], "bleu": 0.07658412276041004, "rouge_l": 0.2793893129770992}
{"id": 4200, "code": "def interactive ( proto dataset uri ) : proto dataset = dtoolcore . Proto Data Set . from uri ( uri = proto dataset uri , config path = CONFIG PATH ) readme template = get readme template ( ) yaml = YAML ( ) yaml . explicit start = True yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) descriptive metadata = yaml . load ( readme template ) descriptive metadata = prompt for values ( descriptive metadata ) stream = String IO ( ) yaml . dump ( descriptive metadata , stream ) proto dataset . put readme ( stream . getvalue ( ) ) click . secho ( \"Updated readme \" , fg = \"green\" ) click . secho ( \"To edit the readme using your default editor:\" ) click . secho ( \"dtool readme edit {}\" . format ( proto dataset uri ) , fg = \"cyan\" )", "predictions": ["edit a ensure that a value is in a yaml six"], "references": ["interactive prompting to populate the readme ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4201, "code": "def edit ( dataset uri ) : try : dataset = dtoolcore . Proto Data Set . from uri ( uri = dataset uri , config path = CONFIG PATH ) except dtoolcore . Dtool Core Type Error : dataset = dtoolcore . Data Set . from uri ( uri = dataset uri , config path = CONFIG PATH ) readme content = dataset . get readme content ( ) try : readme content = unicode ( readme content , \"utf-8\" ) except Name Error : pass edited content = click . edit ( readme content ) if edited content is not None : validate and put readme ( dataset , edited content ) click . secho ( \"Updated readme \" , nl = False , fg = \"green\" ) else : click . secho ( \"Did not update readme \" , nl = False , fg = \"red\" ) click . secho ( dataset uri )", "predictions": ["stream the current self . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["default editor updating of readme content ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 4202, "code": "def show ( dataset uri ) : try : dataset = dtoolcore . Proto Data Set . from uri ( uri = dataset uri , config path = CONFIG PATH ) except dtoolcore . Dtool Core Type Error : dataset = dtoolcore . Data Set . from uri ( uri = dataset uri , config path = CONFIG PATH ) readme content = dataset . get readme content ( ) click . secho ( readme content )", "predictions": ["stream the content of the self { readme } { readme } { color } { { self { self { { self { { { { self { { {"], "references": ["show the descriptive metadata in the readme ."], "bleu": 0.046398855339878003, "rouge_l": 0.17215428033866415}
{"id": 4203, "code": "def item ( proto dataset uri , input file , relpath in dataset ) : proto dataset = dtoolcore . Proto Data Set . from uri ( proto dataset uri , config path = CONFIG PATH ) if relpath in dataset == \"\" : relpath in dataset = os . path . basename ( input file ) proto dataset . put item ( input file , relpath in dataset )", "predictions": ["move stats to a dataset key"], "references": ["add a file to the proto dataset ."], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 4204, "code": "def metadata ( proto dataset uri , relpath in dataset , key , value ) : proto dataset = dtoolcore . Proto Data Set . from uri ( uri = proto dataset uri , config path = CONFIG PATH ) proto dataset . add item metadata ( handle = relpath in dataset , key = key , value = value )", "predictions": ["read stop stop stop stop stop stop stop not in self not ."], "references": ["add metadata to a file in the proto dataset ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 4205, "code": "def cp ( resume , quiet , dataset uri , dest base uri ) : copy ( resume , quiet , dataset uri , dest base uri )", "predictions": ["tear down a graph ."], "references": ["copy a dataset to a different location ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4206, "code": "def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise Type Error ( \"fromutc() requires a datetime argument\" ) if dt . tzinfo is not self : raise Value Error ( \"dt.tzinfo is not self\" ) transitions = self . transitions ( dt . year ) if transitions is None : return dt + self . utcoffset ( dt ) dston , dstoff = transitions dston -= self . std offset dstoff -= self . std offset utc transitions = ( dston , dstoff ) dt utc = dt . replace ( tzinfo = None ) isdst = self . naive isdst ( dt utc , utc transitions ) if isdst : dt wall = dt + self . dst offset else : dt wall = dt + self . std offset fold = int ( not isdst and self . is ambiguous ( dt wall ) ) return enfold ( dt wall , fold = fold )", "predictions": ["convert datetime to as a as a as datetime . basename object ."], "references": ["given a datetime in utc return local time"], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 4207, "code": "def strip comment line with symbol ( line , start ) : parts = line . split ( start ) counts = [ len ( findall ( r'(?:^|[^\"\\\\]|(?:\\\\\\\\|\\\\\")+)(\")' , part ) ) for part in parts ] total = 0 for nr , count in enumerate ( counts ) : total += count if total % 2 == 0 : return start . join ( parts [ : nr + 1 ] ) . rstrip ( ) else : return line . rstrip ( )", "predictions": ["get the dump method with with with with with"], "references": ["strip comments from line string ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4208, "code": "def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd", "predictions": ["convert a date string to a date string ."], "references": ["dayofweek == 0 means sunday whichweek 5 means last instance"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 4209, "code": "def valuestodict ( key ) : dout = { } size = winreg . Query Info Key ( key ) [ 1 ] tz res = None for i in range ( size ) : key name , value , dtype = winreg . Enum Value ( key , i ) if dtype == winreg . REG DWORD or dtype == winreg . REG DWORD LITTLE ENDIAN : if value & ( 1 << 31 ) : value = value - ( 1 << 32 ) elif dtype == winreg . REG SZ : if value . startswith ( '@tzres' ) : tz res = tz res or tzres ( ) value = tz res . name from string ( value ) value = value . rstrip ( '\\x00' ) dout [ key name ] = value return dout", "predictions": ["populate the drps key field * key * * ."], "references": ["convert a registry key s values to a dictionary ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 4210, "code": "def set tzdata ( self , tzobj ) : for attr in tzfile . attrs : setattr ( self , ' ' + attr , getattr ( tzobj , attr ) )", "predictions": ["reset the object s attributes to a given ."], "references": ["set the time zone data of this object from a _tzfile object"], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 4211, "code": "def get ( self , request , hash , filename ) : if ws download is True : return Http Response Forbidden ( ) upload = Upload . objects . uploaded ( ) . get ( hash = hash , name = filename ) return File Response ( upload . file , content type = upload . type )", "predictions": ["returns a page for the given hash p p p p p p p p p p p p p p p p p p p p p p p p"], "references": ["download a file ."], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 4212, "code": "def camelize classname ( base , tablename , table ) : \"'words and underscores' -> 'Words And Underscores'\" return str ( tablename [ 0 ] . upper ( ) + re . sub ( r' ([a-z])' , lambda m : m . group ( 1 ) . upper ( ) , tablename [ 1 : ] ) )", "predictions": ["get the number of occurrences of a top level top - level characters . ."], "references": ["produce a camelized class name e . g ."], "bleu": 0.09782375748961449, "rouge_l": 0.26180257510729615}
{"id": 4213, "code": "def pluralize collection ( base , local cls , referred cls , constraint ) : \"'Some Term' -> 'some terms'\" referred name = referred cls . name uncamelized = re . sub ( r'[A-Z]' , lambda m : \" %s\" % m . group ( 0 ) . lower ( ) , referred name ) [ 1 : ] pluralized = pluralizer . plural ( uncamelized ) return pluralized", "predictions": ["returns a big big big big a big big big big big"], "references": ["produce an uncamelized pluralized class name e . g ."], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 4214, "code": "def dump deque ( self , obj , class name = \"collections.deque\" ) : return { \"$\" + class name : [ self . json convert ( item ) for item in obj ] }", "predictions": ["serialize a deque instance . to a json string"], "references": ["collections . deque dumper ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 4215, "code": "def dump Ordered Dict ( self , obj , class name = \"collections.Ordered Dict\" ) : return { \"$\" + class name : [ ( key , self . json convert ( value ) ) for key , value in iteritems ( obj ) ] }", "predictions": ["mtime a json dump of a json string"], "references": ["collections . ordereddict dumper ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4216, "code": "def dump nparray ( self , obj , class name = numpy ndarray class name ) : return { \"$\" + class name : self . json convert ( obj . tolist ( ) ) }", "predictions": ["atime is a pretty - formatted string for the given object . . . ."], "references": ["numpy . ndarray dumper ."], "bleu": 0.09103526405546068, "rouge_l": 0.21981981981981977}
{"id": 4217, "code": "def join ( prev , sep , * args , * * kw ) : yield sep . join ( prev , * args , * * kw )", "predictions": ["joins a list of elements and then join"], "references": ["alias of str . join"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4218, "code": "def substitute ( prev , * args , * * kw ) : template obj = string . Template ( * args , * * kw ) for data in prev : yield template obj . substitute ( data )", "predictions": ["keys the given template into the given template in the template"], "references": ["alias of string . template . substitute"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 4219, "code": "def safe substitute ( prev , * args , * * kw ) : template obj = string . Template ( * args , * * kw ) for data in prev : yield template obj . safe substitute ( data )", "predictions": ["substitute - . function"], "references": ["alias of string . template . safe_substitute"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4220, "code": "def to str ( prev , encoding = None ) : first = next ( prev ) if isinstance ( first , str ) : if encoding is None : yield first for s in prev : yield s else : yield first . encode ( encoding ) for s in prev : yield s . encode ( encoding ) else : if encoding is None : encoding = sys . stdout . encoding or 'utf-8' yield first . decode ( encoding ) for s in prev : yield s . decode ( encoding )", "predictions": ["convert a string to a string ."], "references": ["convert data from previous pipe with specified encoding ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4221, "code": "def register default types ( ) : register type ( type , pipe . map ) register type ( types . Function Type , pipe . map ) register type ( types . Method Type , pipe . map ) register type ( tuple , seq ) register type ( list , seq ) register type ( types . Generator Type , seq ) register type ( string type , sh ) register type ( unicode type , sh ) register type ( file type , fileobj ) if is py3 : register type ( range , seq ) register type ( map , seq )", "predictions": ["add option types types to ."], "references": ["regiser all default type - to - pipe convertors ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4222, "code": "def check pidfile ( pidfile , debug ) : if os . path . isfile ( pidfile ) : pidfile handle = open ( pidfile , 'r' ) try : pid = int ( pidfile handle . read ( ) ) pidfile handle . close ( ) if check pid ( pid , debug ) : return True except : pass os . unlink ( pidfile ) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False", "predictions": ["checks if the daemon is available and if it s not ."], "references": ["check that a process is not running more than once using pidfile"], "bleu": 0.11498759556447223, "rouge_l": 0.16666666666666666}
{"id": 4223, "code": "def check pid ( pid , debug ) : try : os . kill ( pid , 0 ) if debug > 1 : print ( \"Script has a PIDFILE where the process is still running\" ) return True except OS Error : if debug > 1 : print ( \"Script does not appear to be running\" ) return False", "predictions": ["check a pid process"], "references": ["this function will check whether a pid is currently running"], "bleu": 0.11337974147240094, "rouge_l": 0.3978260869565217}
{"id": 4224, "code": "def convert words to uint ( high word , low word ) : try : low num = int ( low word ) if low num < 0 : low num = abs ( low num ) + 2 ** 15 number = ( int ( high word ) << 16 ) | low num return number , True except : return 0 , False", "predictions": ["convert words to uint"], "references": ["convert two words to a floating point"], "bleu": 0.24002491458061356, "rouge_l": 0.5198863636363635}
{"id": 4225, "code": "def convert words to float ( high word , low word ) : number , retval = convert words to uint ( high word , low word ) if not retval : return 0.0 , False try : packed float = struct . pack ( '>l' , number ) return struct . unpack ( '>f' , packed float ) [ 0 ] , True except : return 0.0 , False", "predictions": ["convert words to float ."], "references": ["convert two words to a floating point"], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 4226, "code": "def disown ( debug ) : pid = os . getpid ( ) cgroup file = \"/proc/\" + str ( pid ) + \"/cgroup\" try : infile = open ( cgroup file , \"r\" ) except IO Error : print ( \"Could not open cgroup file: \" , cgroup file ) return False for line in infile : if line . find ( \"ardexa.service\" ) == - 1 : continue line = line . replace ( \"name=\" , \"\" ) items list = line . split ( ':' ) accounts = items list [ 1 ] dir str = accounts + \"/ardexa.disown\" if not accounts : continue full dir = \"/sys/fs/cgroup/\" + dir str if not os . path . exists ( full dir ) : os . makedirs ( full dir ) if debug >= 1 : print ( \"Making directory: \" , full dir ) else : if debug >= 1 : print ( \"Directory already exists: \" , full dir ) full path = full dir + \"/cgroup.procs\" prog list = [ \"echo\" , str ( pid ) , \">\" , full path ] run program ( prog list , debug , True ) if accounts . find ( \",\" ) != - 1 : acct list = accounts . split ( ',' ) accounts = acct list [ 1 ] + \",\" + acct list [ 0 ] dir str = accounts + \"/ardexa.disown\" full dir = \"/sys/fs/cgroup/\" + dir str try : if not os . path . exists ( full dir ) : os . makedirs ( full dir ) except : continue full path = full dir + \"/cgroup.procs\" prog list = [ \"echo\" , str ( pid ) , \">\" , full path ] run program ( prog list , debug , True ) infile . close ( ) if debug >= 1 : prog list = [ \"cat\" , cgroup file ] run program ( prog list , debug , False ) prog list = [ \"grep\" , \"-q\" , \"ardexa.service\" , cgroup file ] if run program ( prog list , debug , False ) : return False return True", "predictions": ["sets up the disown environment for running the debugger ."], "references": ["this function will disown so the ardexa service can be restarted"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 4227, "code": "def encode ids ( * args ) : ids = [ ] for v in args : if isinstance ( v , basestring ) : qv = v . encode ( 'utf-8' ) if isinstance ( v , unicode ) else v ids . append ( urllib . quote ( qv ) ) else : qv = str ( v ) ids . append ( urllib . quote ( qv ) ) return ';' . join ( ids )", "predictions": ["encode a list of strings into a single string ."], "references": ["do url - encode resource ids"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 4228, "code": "def flush ( self , line ) : sys . stdout . write ( line ) sys . stdout . flush ( )", "predictions": ["flush the given line to the underlying output ."], "references": ["flush the line to stdout"], "bleu": 0.23356898886410005, "rouge_l": 0.6024691358024692}
{"id": 4229, "code": "def find meta ( * meta file parts , meta key ) : meta file = read ( * meta file parts ) meta match = re . search ( r\"^ {}  = ['\\\"]([^'\\\"]*)['\\\"]\" . format ( meta key ) , meta file , re . M ) if meta match : return meta match . group ( 1 ) raise Runtime Error ( \"Unable to find  {}  string.\" . format ( meta key ) )", "predictions": ["find the meta data for the given meta meta data ."], "references": ["extract __ * meta * __ from meta_file ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 4230, "code": "def main ( path ) : basepath = os . path . abspath ( os . path . expanduser ( str ( path ) ) ) echo . h2 ( \"Available scripts in {}\" . format ( basepath ) ) echo . br ( ) for root dir , dirs , files in os . walk ( basepath , topdown = True ) : for f in fnmatch . filter ( files , '*.py' ) : try : filepath = os . path . join ( root dir , f ) with open ( filepath , encoding = \"UTF-8\" ) as fp : body = fp . read ( ) is console = \"Interactive Console\" in body is console = is console or \"code\" in body is console = is console and \"interact(\" in body if is console : continue s = captain . Script ( filepath ) if s . can run from cli ( ) : rel filepath = s . call path ( basepath ) p = s . parser echo . h3 ( rel filepath ) desc = p . description if desc : echo . indent ( desc , indent = ( \" \" * 4 ) ) subcommands = s . subcommands if subcommands : echo . br ( ) echo . indent ( \"Subcommands:\" , indent = ( \" \" * 4 ) ) for sc in subcommands . keys ( ) : echo . indent ( sc , indent = ( \" \" * 6 ) ) echo . br ( ) except captain . Parse Error : pass except Exception as e : #echo.exception(e) #echo.err(\"Failed to parse {} because {}\", f, e.message) echo . err ( \"Failed to parse {}\" , f ) echo . verbose ( e . message ) echo . br ( )", "predictions": ["run the main command line tool ."], "references": ["scan path directory and any subdirectories for valid captain scripts"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 4231, "code": "def make request data ( self , zipcode , city , state ) : data = { 'key' : self . api key , 'postalcode' : str ( zipcode ) , 'city' : city , 'state' : state } data = Zip Tax Client . clean request data ( data ) return data", "predictions": ["build the data to be sent to the api"], "references": ["make the request params given location data"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4232, "code": "def process response ( self , resp , multiple rates ) : self . check for exceptions ( resp , multiple rates ) rates = { } for result in resp [ 'results' ] : rate = Zip Tax Client . cast tax rate ( result [ 'tax Sales' ] ) rates [ result [ 'geo City' ] ] = rate if not multiple rates : return rates [ list ( rates . keys ( ) ) [ 0 ] ] return rates", "predictions": ["checks the response to see if the response is valid ."], "references": ["get the tax rate from the ziptax response"], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 4233, "code": "def check for exceptions ( self , resp , multiple rates ) : if resp [ 'r Code' ] != 100 : raise exceptions . get exception for code ( resp [ 'r Code' ] ) ( resp ) results = resp [ 'results' ] if len ( results ) == 0 : raise exceptions . Zip Tax No Results ( 'No results found' ) if len ( results ) > 1 and not multiple rates : rates = [ result [ 'tax Sales' ] for result in results ] if len ( set ( rates ) ) != 1 : raise exceptions . Zip Tax Multiple Results ( 'Multiple results found but requested only one' )", "predictions": ["check that all exceptions are only 100 ."], "references": ["check if there are exceptions that should be raised"], "bleu": 0.17795502018438056, "rouge_l": 0.232824427480916}
{"id": 4234, "code": "def get all text ( node ) : if node . node Type == node . TEXT NODE : return node . data else : text string = \"\" for child node in node . child Nodes : text string += get all text ( child node ) return text string", "predictions": ["get all text text nodes from a node"], "references": ["recursively extract all text from node ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 4235, "code": "def extract packages ( self ) : self . path unpacked = mkdtemp ( prefix = \"scoap3 package \" , dir = CFG TMPSHAREDDIR ) for path in self . retrieved packages unpacked : scoap3utils extract package ( path , self . path unpacked , self . logger ) return self . path unpacked", "predictions": ["extract the packages from the project ."], "references": ["extract a package in a new temporary directory ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4236, "code": "def register ( self , provider class ) : if not issubclass ( provider class , Base Provider ) : raise Type Error ( '%s is not a subclass of Base Provider' % provider class . name ) if provider class in self . registered providers : raise Already Registered ( '%s is already registered' % provider class . name ) if issubclass ( provider class , Django Provider ) : signals . post save . connect ( self . invalidate stored oembeds , sender = provider class . meta . model ) self . registered providers . append ( provider class ) self . invalidate providers ( )", "predictions": ["register a provider class for the given provider ."], "references": ["registers a provider with the site ."], "bleu": 0.21105340631872635, "rouge_l": 0.5115303983228512}
{"id": 4237, "code": "def unregister ( self , provider class ) : if not issubclass ( provider class , Base Provider ) : raise Type Error ( '%s must be a subclass of Base Provider' % provider class . name ) if provider class not in self . registered providers : raise Not Registered ( '%s is not registered' % provider class . name ) self . registered providers . remove ( provider class ) self . invalidate providers ( )", "predictions": ["unregister the provider ."], "references": ["unregisters a provider from the site ."], "bleu": 0.20183609024241697, "rouge_l": 0.346590909090909}
{"id": 4238, "code": "def provider for url ( self , url ) : for provider , regex in self . get registry ( ) . items ( ) : if re . match ( regex , url ) is not None : return provider raise O Embed Missing Endpoint ( 'No endpoint matches URL: %s' % url )", "predictions": ["return provider for given url"], "references": ["find the right provider for a url"], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 4239, "code": "def invalidate stored oembeds ( self , sender , instance , created , * * kwargs ) : ctype = Content Type . objects . get for model ( instance ) Stored O Embed . objects . filter ( object id = instance . pk , content type = ctype ) . delete ( )", "predictions": ["when a sender is deleted remove the stored stored in the stored model ."], "references": ["a hook for django - based oembed providers to delete any stored oembeds"], "bleu": 0.09782375748961449, "rouge_l": 0.1491442542787286}
{"id": 4240, "code": "def embed ( self , url , * * kwargs ) : try : provider = self . provider for url ( url ) except O Embed Missing Endpoint : raise else : try : stored match = Stored O Embed . objects . filter ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) , date expires gte = datetime . datetime . now ( ) ) [ 0 ] return O Embed Resource . create json ( stored match . response json ) except Index Error : params = dict ( [ ( k , v ) for k , v in kwargs . items ( ) if v ] ) resource = provider . request resource ( url , * * params ) try : cache age = int ( resource . cache age ) if cache age < MIN OEMBED TTL : cache age = MIN OEMBED TTL except : cache age = DEFAULT OEMBED TTL date expires = datetime . datetime . now ( ) + datetime . timedelta ( seconds = cache age ) stored oembed , created = Stored O Embed . objects . get or create ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) ) stored oembed . response json = resource . json stored oembed . resource type = resource . type stored oembed . date expires = date expires if resource . content object : stored oembed . content object = resource . content object stored oembed . save ( ) return resource", "predictions": ["embed the given url and return cached data ."], "references": ["the heart of the matter"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4241, "code": "def autodiscover ( self , url ) : headers , response = fetch url ( url ) if headers [ 'content-type' ] . split ( ';' ) [ 0 ] in ( 'application/json' , 'text/javascript' ) : provider data = json . loads ( response ) return self . store providers ( provider data )", "predictions": ["put a given url in the http store"], "references": ["load up storedproviders from url if it is an oembed scheme"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 4242, "code": "def store providers ( self , provider data ) : if not hasattr ( provider data , ' iter ' ) : raise O Embed Exception ( 'Autodiscovered response not iterable' ) provider pks = [ ] for provider in provider data : if 'endpoint' not in provider or 'matches' not in provider : continue resource type = provider . get ( 'type' ) if resource type not in RESOURCE TYPES : continue stored provider , created = Stored Provider . objects . get or create ( wildcard regex = provider [ 'matches' ] ) if created : stored provider . endpoint url = relative to full ( provider [ 'endpoint' ] , provider [ 'matches' ] ) stored provider . resource type = resource type stored provider . save ( ) provider pks . append ( stored provider . pk ) return Stored Provider . objects . filter ( pk in = provider pks )", "predictions": ["saves all the providers into a single provider"], "references": ["iterate over the returned json and try to sort out any new providers"], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 4243, "code": "def image field ( self ) : for field in self . model . meta . fields : if isinstance ( field , Image Field ) : return field . name", "predictions": ["returns the image name of the model ."], "references": ["try to automatically detect an image field"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4244, "code": "def date field ( self ) : for field in self . model . meta . fields : if isinstance ( field , ( Date Time Field , Date Field ) ) : return field . name", "predictions": ["returns the field name of the model ."], "references": ["try to automatically detect an image field"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4245, "code": "def get image ( self , obj ) : if self . meta . image field : return getattr ( obj , self . meta . image field )", "predictions": ["return the image object for the given image"], "references": ["return an imagefilefield instance"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4246, "code": "def map to dictionary ( self , url , obj , * * kwargs ) : maxwidth = kwargs . get ( 'maxwidth' , None ) maxheight = kwargs . get ( 'maxheight' , None ) provider url , provider name = self . provider from url ( url ) mapping = { 'version' : '1.0' , 'url' : url , 'provider name' : provider name , 'provider url' : provider url , 'type' : self . resource type } self . preprocess ( obj , mapping , * * kwargs ) if self . resource type == 'photo' and self . get image ( obj ) : self . resize photo ( obj , mapping , maxwidth , maxheight ) elif self . resource type in ( 'video' , 'rich' , 'photo' ) : width , height = size to nearest ( maxwidth , maxheight , self . meta . valid sizes , self . meta . force fit ) mapping . update ( width = width , height = height ) if self . get image ( obj ) : self . thumbnail ( obj , mapping ) for attr in ( 'title' , 'author name' , 'author url' , 'html' ) : self . map attr ( mapping , attr , obj ) if 'url' in mapping : mapping [ 'url' ] = relative to full ( mapping [ 'url' ] , url ) if 'thumbnail url' in mapping : mapping [ 'thumbnail url' ] = relative to full ( mapping [ 'thumbnail url' ] , url ) if 'html' not in mapping and mapping [ 'type' ] in ( 'video' , 'rich' ) : mapping [ 'html' ] = self . render html ( obj , context = Context ( mapping ) ) self . postprocess ( obj , mapping , * * kwargs ) return mapping", "predictions": ["map the html to the dictionary ."], "references": ["build a dictionary of metadata for the requested object ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 4247, "code": "def get record ( self ) : self . recid = self . get recid ( ) self . remove controlfields ( ) self . update system numbers ( ) self . add systemnumber ( \"Inspire\" , recid = self . recid ) self . add control number ( \"003\" , \"Sz Ge CERN\" ) self . update collections ( ) self . update languages ( ) self . update reportnumbers ( ) self . update authors ( ) self . update journals ( ) self . update subject categories ( \"INSPIRE\" , \"Sz Ge CERN\" , \"categories cds\" ) self . update pagenumber ( ) self . update notes ( ) self . update experiments ( ) self . update isbn ( ) self . update dois ( ) self . update links and ffts ( ) self . update date ( ) self . update date year ( ) self . update hidden notes ( ) self . update oai info ( ) self . update cnum ( ) self . update conference info ( ) self . fields list = [ \"909\" , \"541\" , \"961\" , \"970\" , \"690\" , \"695\" , \"981\" , ] self . strip fields ( ) if \"ANNOUNCEMENT\" in self . collections : self . update conference 111 ( ) self . update conference links ( ) record add field ( self . record , \"690\" , ind1 = \"C\" , subfields = [ ( \"a\" , \"CONFERENCE\" ) ] ) if \"THESIS\" in self . collections : self . update thesis information ( ) self . update thesis supervisors ( ) if \"PROCEEDINGS\" in self . collections : self . update title to proceeding ( ) self . update author to proceeding ( ) record add field ( self . record , \"690\" , ind1 = \"C\" , subfields = [ ( \"a\" , \"CONFERENCE\" ) ] ) if self . tag as cern : record add field ( self . record , \"690\" , ind1 = \"C\" , subfields = [ ( \"a\" , \"CERN\" ) ] ) return self . record", "predictions": ["create a record for the spectrum"], "references": ["override the base ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4248, "code": "def update oai info ( self ) : for field in record get field instances ( self . record , '909' , ind1 = \"C\" , ind2 = \"O\" ) : new subs = [ ] for tag , value in field [ 0 ] : if tag == \"o\" : new subs . append ( ( \"a\" , value ) ) else : new subs . append ( ( tag , value ) ) if value in [ \"CERN\" , \"CDS\" , \"For CDS\" ] : self . tag as cern = True record add field ( self . record , '024' , ind1 = \"8\" , subfields = new subs ) record delete fields ( self . record , '909' )", "predictions": ["update all oai - pmh information ."], "references": ["add the 909 oai info to 035 ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4249, "code": "def update cnum ( self ) : if \"Conference Paper\" not in self . collections : cnums = record get field values ( self . record , '773' , code = \"w\" ) for cnum in cnums : cnum subs = [ ( \"9\" , \"INSPIRE-CNUM\" ) , ( \"a\" , cnum ) ] record add field ( self . record , \"035\" , subfields = cnum subs )", "predictions": ["update the cnum with the cnum"], "references": ["check if we shall add cnum in 035 ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4250, "code": "def update hidden notes ( self ) : if not self . tag as cern : notes = record get field instances ( self . record , tag = \"595\" ) for field in notes : for dummy , value in field [ 0 ] : if value == \"CDS\" : self . tag as cern = True record delete fields ( self . record , tag = \"595\" )", "predictions": ["update hidden notes ."], "references": ["remove hidden notes and tag a cern if detected ."], "bleu": 0.11337974147240094, "rouge_l": 0.3978260869565217}
{"id": 4251, "code": "def update collections ( self ) : for value in record get field values ( self . record , '980' , code = 'a' ) : if 'NOTE' in value . upper ( ) : self . collections . add ( 'NOTE' ) if 'THESIS' in value . upper ( ) : self . collections . add ( 'THESIS' ) if 'PUBLISHED' in value . upper ( ) : self . collections . add ( 'ARTICLE' ) if 'CONFERENCES' in value . upper ( ) : self . collections . add ( 'ANNOUNCEMENT' ) if 'PROCEEDINGS' in value . upper ( ) : self . collections . add ( 'PROCEEDINGS' ) elif 'CONFERENCEPAPER' in value . upper ( ) and \"Conference Paper\" not in self . collections : self . collections . add ( 'Conference Paper' ) if self . is published ( ) and \"ARTICLE\" not in self . collections : self . collections . add ( 'ARTICLE' ) else : self . collections . add ( 'PREPRINT' ) if \"HIDDEN\" in value . upper ( ) : self . hidden = True record delete fields ( self . record , \"980\" ) if not self . collections : self . collections . add ( 'PREPRINT' ) for collection in self . collections : record add field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) if collection in self . collection base : subs = [ ( 'a' , self . collection base [ collection ] ) ] record add field ( self . record , tag = '960' , subfields = subs )", "predictions": ["updates the list of collections collections ."], "references": ["try to determine which collections this record should belong to ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4252, "code": "def update notes ( self ) : fields = record get field instances ( self . record , '500' ) for field in fields : subs = field get subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) if sub . startswith ( \"*\" ) and sub . endswith ( \"*\" ) : record delete field ( self . record , tag = \"500\" , field position global = field [ 4 ] )", "predictions": ["update notes from the record ."], "references": ["remove inspire specific notes ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 4253, "code": "def update title to proceeding ( self ) : titles = record get field instances ( self . record , tag = \"245\" ) for title in titles : subs = field get subfields ( title ) new subs = [ ] if \"a\" in subs : new subs . append ( ( \"a\" , subs [ 'a' ] [ 0 ] ) ) if \"b\" in subs : new subs . append ( ( \"c\" , subs [ 'b' ] [ 0 ] ) ) record add field ( self . record , tag = \"111\" , subfields = new subs ) record delete fields ( self . record , tag = \"245\" ) record delete fields ( self . record , tag = \"246\" )", "predictions": ["update the title to proceeding with the record ."], "references": ["move title info from 245 to 111 proceeding style ."], "bleu": 0.15881076016027915, "rouge_l": 0.41709401709401706}
{"id": 4254, "code": "def update authors ( self ) : author names = record get field instances ( self . record , '100' ) author names . extend ( record get field instances ( self . record , '700' ) ) for field in author names : subs = field get subfields ( field ) for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( \".\" , \" \" ) . strip ( ) ) elif key == 'v' : del field [ 0 ] [ idx ] if subs . get ( \"u\" , None ) == \"CERN\" : self . tag as cern = True", "predictions": ["updates the list of authors from the record record ."], "references": ["100 & 700 punctuate author names ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 4255, "code": "def update isbn ( self ) : isbns = record get field instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( \"-\" , \"\" ) . strip ( ) )", "predictions": ["update the isbn isbn isbn ."], "references": ["remove dashes from isbn ."], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 4256, "code": "def update dois ( self ) : dois = record get field instances ( self . record , '024' , ind1 = \"7\" ) all dois = { } for field in dois : subs = field get subfield instances ( field ) subs dict = dict ( subs ) if subs dict . get ( 'a' ) : if subs dict [ 'a' ] in all dois : record delete field ( self . record , tag = '024' , ind1 = '7' , field position global = field [ 4 ] ) continue all dois [ subs dict [ 'a' ] ] = field", "predictions": ["updates the words in the record try to convert the words into dicts try to words try to words try to words try to updated updated updated format try to words"], "references": ["remove duplicate bibmatch dois ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4257, "code": "def update journals ( self ) : for field in record get field instances ( self . record , '773' ) : subs = field get subfield instances ( field ) new subs = [ ] volume letter = \"\" journal name = \"\" for idx , ( key , value ) in enumerate ( subs ) : if key == 'p' : journal name = self . get config item ( value , \"journals\" , allow substring = False ) journal name = journal name . replace ( '. ' , '.' ) . replace ( '.' , '. ' ) . replace ( '. ,' , '.,' ) . strip ( ) elif key == 'v' : volume letter = value else : new subs . append ( ( key , value ) ) if not journal name == \"Po S\" : letter = return letters from string ( volume letter ) if letter : journal name = \"{0} {1}\" . format ( journal name , letter ) volume letter = volume letter . strip ( letter ) if journal name : new subs . append ( ( \"p\" , journal name ) ) if volume letter : new subs . append ( ( \"v\" , volume letter ) ) record delete field ( self . record , tag = \"773\" , field position global = field [ 4 ] ) record add field ( self . record , \"773\" , subfields = new subs )", "predictions": ["updates the words in the record record"], "references": ["773 journal translations ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4258, "code": "def update thesis information ( self ) : fields 501 = record get field instances ( self . record , '502' ) for field in fields 501 : new subs = [ ] for key , value in field [ 0 ] : if key == 'b' : new subs . append ( ( 'a' , value ) ) elif key == 'c' : new subs . append ( ( 'b' , value ) ) elif key == 'd' : new subs . append ( ( 'c' , value ) ) else : new subs . append ( ( key , value ) ) record delete field ( self . record , tag = \"502\" , field position global = field [ 4 ] ) record add field ( self . record , \"502\" , subfields = new subs )", "predictions": ["updates the list of os debug debug debug debug str str str str str str str str str str str str str str str str str str str str str str"], "references": ["501 degree info - move subfields ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4259, "code": "def update pagenumber ( self ) : pages = record get field instances ( self . record , '300' ) for field in pages : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , \"{0} p\" . format ( value ) )", "predictions": ["encode the ids from the record record in the record record in the record in the record in the record in the record in the record in the record in the"], "references": ["300 page number ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4260, "code": "def update date ( self ) : dates 269 = record get field instances ( self . record , '269' ) for idx , field in enumerate ( dates 269 ) : new subs = [ ] old subs = field [ 0 ] for code , value in old subs : if code == \"c\" : new subs . append ( ( \"c\" , convert date from iso to human ( value ) ) ) else : new subs . append ( ( code , value ) ) dates 269 [ idx ] = field swap subfields ( field , new subs )", "predictions": ["flush the date attribute"], "references": ["269 date normalization ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4261, "code": "def update date year ( self ) : dates = record get field instances ( self . record , '260' ) for field in dates : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'c' : field [ 0 ] [ idx ] = ( 'c' , value [ : 4 ] ) elif key == 't' : del field [ 0 ] [ idx ] if not dates : published years = record get field values ( self . record , \"773\" , code = \"y\" ) if published years : record add field ( self . record , \"260\" , subfields = [ ( \"c\" , published years [ 0 ] [ : 4 ] ) ] ) else : other years = record get field values ( self . record , \"269\" , code = \"c\" ) if other years : record add field ( self . record , \"260\" , subfields = [ ( \"c\" , other years [ 0 ] [ : 4 ] ) ] )", "predictions": ["find the meta attribute of the meta attribute re - replace meta year re - hot re - hot re - hot re - find the year re - find the"], "references": ["260 date normalization ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4262, "code": "def fix name capitalization ( lastname , givennames ) : lastnames = lastname . split ( ) if len ( lastnames ) == 1 : if '-' in lastname : names = lastname . split ( '-' ) names = map ( lambda a : a [ 0 ] + a [ 1 : ] . lower ( ) , names ) lastname = '-' . join ( names ) else : lastname = lastname [ 0 ] + lastname [ 1 : ] . lower ( ) else : names = [ ] for name in lastnames : if re . search ( r'[A-Z]\\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) lastname = ' ' . join ( names ) lastname = collapse initials ( lastname ) names = [ ] for name in givennames : if re . search ( r'[A-Z]\\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) givennames = ' ' . join ( names ) return lastname , givennames", "predictions": ["replace zeros in two strings with zeros in name"], "references": ["converts capital letters to lower keeps first letter capital ."], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 4263, "code": "def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED APPS : try : app path = import ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . path except Attribute Error : continue try : imp . find module ( 'oembed providers' , app path ) except Import Error : continue import ( \"%s.oembed providers\" % app )", "predictions": ["make application configuration state"], "references": ["automatically build the provider index ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 4264, "code": "def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : try : response = raw input ( ) . strip ( ) except ( EOF Error , Keyboard Interrupt ) : response = '' else : sys . stdin = open ( \"/dev/tty\" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\\n' ) : break except ( EOF Error , Keyboard Interrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except Value Error : return None if response < 0 or response >= len ( options ) : return None return options [ response ]", "predictions": ["process the actual options ."], "references": ["pass in a list of options promt the user to select one and return the selected option or none"], "bleu": 0.018373002712755784, "rouge_l": 0.0754017305315204}
{"id": 4265, "code": "def err ( format msg , * args , * * kwargs ) : exc info = kwargs . pop ( \"exc info\" , False ) stderr . warning ( str ( format msg ) . format ( * args , * * kwargs ) , exc info = exc info )", "predictions": ["print a message with a . check that exceptions is set"], "references": ["print format_msg to stderr"], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 4266, "code": "def out ( format msg = \"\" , * args , * * kwargs ) : logmethod = kwargs . get ( \"logmethod\" , stdout . info ) if format msg != \"\" : if Prefix . has ( ) : if isinstance ( format msg , basestring ) : format msg = Prefix . get ( ) + format msg else : format msg = Prefix . get ( ) + str ( format msg ) if isinstance ( format msg , basestring ) : if args or kwargs : s = format msg . format ( * args , * * kwargs ) else : s = format msg logmethod ( s ) else : logmethod ( str ( format msg ) ) else : logmethod ( \"\" )", "predictions": ["print a message to stdout"], "references": ["print format_msg to stdout taking into account -- quiet setting"], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 4267, "code": "def verbose ( format msg = \"\" , * args , * * kwargs ) : kwargs [ \"logmethod\" ] = stdout . debug out ( format msg , * args , * * kwargs )", "predictions": ["log a message with extract only if extract is installed"], "references": ["print format_msg to stdout taking into account -- verbose flag"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4268, "code": "def inject quiet ( levels ) : loggers = list ( Logger . manager . logger Dict . items ( ) ) loggers . append ( ( \"root\" , get Logger ( ) ) ) level filter = Level Filter ( levels ) for logger name , logger in loggers : for handler in getattr ( logger , \"handlers\" , [ ] ) : handler . add Filter ( level filter )", "predictions": ["register quiet and levels loggers . . . ."], "references": ["see -- quiet flag help for what this does"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4269, "code": "def connect ( self ) : for tried connection count in range ( CFG FTP CONNECTION ATTEMPTS ) : try : self . ftp = Ftp Handler ( self . config . OXFORD . URL , self . config . OXFORD . LOGIN , self . config . OXFORD . PASSWORD ) self . logger . debug ( ( \"Successful connection to the \" \"Oxford University Press server\" ) ) return except socket timeout exception as err : self . logger . error ( ( 'Failed to connect %d of %d times. ' 'Will sleep for %d seconds and try again.' ) % ( tried connection count + 1 , CFG FTP CONNECTION ATTEMPTS , CFG FTP TIMEOUT SLEEP DURATION ) ) time . sleep ( CFG FTP TIMEOUT SLEEP DURATION ) except Exception as err : self . logger . error ( ( 'Failed to connect to the Oxford ' 'University Press server. %s' ) % ( err , ) ) break raise Login Exception ( err )", "predictions": ["unregister connection to server"], "references": ["logs into the specified ftp server and returns connector ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 4270, "code": "def extract packages ( self ) : if not hasattr ( self , \"retrieved packages unpacked\" ) : self . retrieved packages unpacked = [ self . package name ] for path in self . retrieved packages unpacked : package name = basename ( path ) self . path unpacked = join ( CFG UNPACKED FILES , package name . split ( '.' ) [ 0 ] ) self . logger . debug ( \"Extracting package: %s\" % ( path . split ( \"/\" ) [ - 1 ] , ) ) try : if \" archival pdf\" in self . path unpacked : self . path unpacked = ( self . path unpacked . rstrip ( \" archival pdf\" ) ) Zip File ( path ) . extractall ( join ( self . path unpacked , \"archival pdfs\" ) ) else : Zip File ( path ) . extractall ( self . path unpacked ) #Tar File.open(path).extractall(self.path unpacked) except Exception : register exception ( alert admin = True , prefix = \"OUP error extracting package.\" ) self . logger . error ( \"Error extraction package file: %s\" % ( path , ) ) if hasattr ( self , \"path unpacked\" ) : return self . path unpacked", "predictions": ["provider for a extraction is available"], "references": ["extract a package in a new directory ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4271, "code": "def authenticate ( self ) : if self . session id : LOGGER . debug ( \"Using existing Nu Heat session\" ) return LOGGER . debug ( \"Creating Nu Heat session\" ) post data = { \"Email\" : self . username , \"Password\" : self . password , \"application\" : \"0\" } data = self . request ( config . AUTH URL , method = \"POST\" , data = post data ) session id = data . get ( \"Session Id\" ) if not session id : raise Exception ( \"Authentication error\" ) self . session id = session id", "predictions": ["invalidate the instance with the instance id sender sender sender sender sender sender sender sender sender sender sender ."], "references": ["authenticate against the nuheat api"], "bleu": 0.06439931429457924, "rouge_l": 0.09312977099236641}
{"id": 4272, "code": "def handle starttag ( self , tag , attrs ) : if tag in self . mathml elements : final attr = \"\" for key , value in attrs : final attr += ' {0}=\"{1}\"' . format ( key , value ) self . fed . append ( \"<{0}{1}>\" . format ( tag , final attr ) )", "predictions": ["gets the * * * * * * * * * * * * due to the index * = 1 * = 1 * = 1 * = 1 *"], "references": ["return representation of html start tag and attributes ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4273, "code": "def handle endtag ( self , tag ) : if tag in self . mathml elements : self . fed . append ( \"</{0}>\" . format ( tag ) )", "predictions": ["handle a local tag"], "references": ["return representation of html end tag ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4274, "code": "def html to text ( cls , html ) : s = cls ( ) s . feed ( html ) unescaped data = s . unescape ( s . get data ( ) ) return escape for xml ( unescaped data , tags to keep = s . mathml elements )", "predictions": ["convert an store store a text in text to text"], "references": ["return stripped html keeping only mathml ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4275, "code": "def is instance ( self ) : ret = False val = self . callback if self . is class ( ) : return False ret = not inspect . isfunction ( val ) and not inspect . ismethod ( val ) return ret", "predictions": ["returns a bool if the fields is a fields field"], "references": ["return true if callback is an instance of a class"], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 4276, "code": "def is function ( self ) : if self . is instance ( ) or self . is class ( ) : return False return isinstance ( self . callback , ( Callable , classmethod ) )", "predictions": ["returns whether the field is a field in its browser in a future in a separate thread in a separate separate thread"], "references": ["return true if callback is a vanilla plain jane function"], "bleu": 0.07289334177359764, "rouge_l": 0.13406593406593406}
{"id": 4277, "code": "def make user agent ( component = None ) : packageinfo = pkg resources . require ( \"harvestingkit\" ) [ 0 ] useragent = \"{0}/{1}\" . format ( packageinfo . project name , packageinfo . version ) if component is not None : useragent += \" {0}\" . format ( component ) return useragent", "predictions": ["factory for creating an useragent agent return a image agent return a image return a image return a tuple return a image return a tuple return string that can use return"], "references": ["create string suitable for http user - agent header"], "bleu": 0.046398855339878003, "rouge_l": 0.11101000909918107}
{"id": 4278, "code": "def record add field ( rec , tag , ind1 = '' , ind2 = '' , subfields = [ ] , controlfield value = '' ) : if controlfield value : doc = etree . Element ( \"controlfield\" , attrib = { \"tag\" : tag , } ) doc . text = unicode ( controlfield value ) else : doc = etree . Element ( \"datafield\" , attrib = { \"tag\" : tag , \"ind1\" : ind1 , \"ind2\" : ind2 , } ) for code , value in subfields : field = etree . Sub Element ( doc , \"subfield\" , attrib = { \"code\" : code } ) field . text = value rec . append ( doc ) return rec", "predictions": ["create a map dictionary to a domain"], "references": ["add a marcxml datafield as a new child to a xml document ."], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 4279, "code": "def record xml output ( rec , pretty = True ) : from . html utils import Math ML Parser ret = etree . tostring ( rec , xml declaration = False ) ret = re . sub ( \"(&lt;)(([\\/]?{0}))\" . format ( \"|[\\/]?\" . join ( Math ML Parser . mathml elements ) ) , '<\\g<2>' , ret ) ret = re . sub ( \"&gt;\" , '>' , ret ) if pretty : ret = ret . replace ( '</datafield>' , '  </datafield>\\n' ) ret = re . sub ( r'<datafield(.*?)>' , r'  <datafield\\1>\\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\\n' ) ret = ret . replace ( '<subfield' , '    <subfield' ) ret = ret . replace ( 'record>' , 'record>\\n' ) return ret", "predictions": ["get pretty print the output output information system system"], "references": ["given a document return xml prettified ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4280, "code": "def format arxiv id ( arxiv id ) : if arxiv id and \"/\" not in arxiv id and \"ar Xiv\" not in arxiv id : return \"ar Xiv:%s\" % ( arxiv id , ) elif arxiv id and '.' not in arxiv id and arxiv id . lower ( ) . startswith ( 'arxiv:' ) : return arxiv id [ 6 : ] else : return arxiv id", "predictions": ["update the oai - pmh info for a oai - dimensional oai - pmh oai - dimensional oai - pmh info oai - dimensional oai - pmh ind2 - dimensional oai"], "references": ["properly format arxiv ids ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4281, "code": "def fix journal name ( journal , knowledge base ) : if not journal : return '' , '' if not knowledge base : return journal , '' if len ( journal ) < 2 : return journal , '' volume = '' if ( journal [ - 1 ] <= 'Z' and journal [ - 1 ] >= 'A' ) and ( journal [ - 2 ] == '.' or journal [ - 2 ] == ' ' ) : volume += journal [ - 1 ] journal = journal [ : - 1 ] journal = journal . strip ( ) if journal . upper ( ) in knowledge base : journal = knowledge base [ journal . upper ( ) ] . strip ( ) elif journal in knowledge base : journal = knowledge base [ journal ] . strip ( ) elif '.' in journal : journalnodots = journal . replace ( '. ' , ' ' ) journalnodots = journalnodots . replace ( '.' , ' ' ) . strip ( ) . upper ( ) if journalnodots in knowledge base : journal = knowledge base [ journalnodots ] . strip ( ) journal = journal . replace ( '. ' , '.' ) return journal , volume", "predictions": ["update journal name name name can be used as a valid journal"], "references": ["convert journal name to inspire s short form ."], "bleu": 0.1367440667823257, "rouge_l": 0.19551282051282048}
{"id": 4282, "code": "def add nations field ( authors subfields ) : from . config import NATIONS DEFAULT MAP result = [ ] for field in authors subfields : if field [ 0 ] == 'v' : values = [ x . replace ( '.' , '' ) for x in field [ 1 ] . split ( ', ' ) ] possible affs = filter ( lambda x : x is not None , map ( NATIONS DEFAULT MAP . get , values ) ) if 'CERN' in possible affs and 'Switzerland' in possible affs : possible affs = [ x for x in possible affs if x != 'Switzerland' ] result . extend ( possible affs ) result = sorted ( list ( set ( result ) ) ) if result : authors subfields . extend ( [ ( 'w' , res ) for res in result ] ) else : authors subfields . append ( ( 'w' , 'HUMAN CHECK' ) )", "predictions": ["update self . self . affs"], "references": ["add correct nations field according to mapping in nations_default_map ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 4283, "code": "def fix dashes ( string ) : string = string . replace ( u'\\u05BE' , '-' ) string = string . replace ( u'\\u1806' , '-' ) string = string . replace ( u'\\u2E3A' , '-' ) string = string . replace ( u'\\u2E3B' , '-' ) string = unidecode ( string ) return re . sub ( r'--+' , '-' , string )", "predictions": ["update self value with collections value value value"], "references": ["fix bad unicode special dashes in string ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4284, "code": "def fix title capitalization ( title ) : if re . search ( \"[A-Z]\" , title ) and re . search ( \"[a-z]\" , title ) : return title word list = re . split ( ' +' , title ) final = [ word list [ 0 ] . capitalize ( ) ] for word in word list [ 1 : ] : if word . upper ( ) in COMMON ACRONYMS : final . append ( word . upper ( ) ) elif len ( word ) > 3 : final . append ( word . capitalize ( ) ) else : final . append ( word . lower ( ) ) return \" \" . join ( final )", "predictions": ["replace special characters with a comma separated notes record record record record record record record record record record record record record"], "references": ["try to capitalize properly a title string ."], "bleu": 0.05809665204409193, "rouge_l": 0.07503075030750307}
{"id": 4285, "code": "def convert html subscripts to latex ( text ) : text = re . sub ( \"<sub>(.*?)</sub>\" , r\"$ {\\1}$\" , text ) text = re . sub ( \"<sup>(.*?)</sup>\" , r\"$^{\\1}$\" , text ) return text", "predictions": ["converts text to latex"], "references": ["convert some html tags to latex equivalents ."], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 4286, "code": "def download file ( from url , to filename = None , chunk size = 1024 * 8 , retry count = 3 ) : if not to filename : to filename = get temporary file ( ) session = requests . Session ( ) adapter = requests . adapters . HTTP Adapter ( max retries = retry count ) session . mount ( from url , adapter ) response = session . get ( from url , stream = True ) with open ( to filename , 'wb' ) as fd : for chunk in response . iter content ( chunk size ) : fd . write ( chunk ) return to filename", "predictions": ["update a authors to a file"], "references": ["download url to a file ."], "bleu": 0.4111336169005197, "rouge_l": 0.5}
{"id": 4287, "code": "def run shell command ( commands , * * kwargs ) : p = subprocess . Popen ( commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , * * kwargs ) output , error = p . communicate ( ) return p . returncode , output , error", "predictions": ["update a isbn command command ."], "references": ["run a shell command ."], "bleu": 0.31239399369202553, "rouge_l": 0.5545454545454546}
{"id": 4288, "code": "def create logger ( name , filename = None , logging level = logging . DEBUG ) : logger = logging . get Logger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . File Handler ( filename = filename ) fh . set Formatter ( formatter ) logger . add Handler ( fh ) ch = logging . Stream Handler ( ) ch . set Formatter ( formatter ) logger . add Handler ( ch ) logger . set Level ( logging level ) return logger", "predictions": ["create a logger with the specified name ."], "references": ["create a logger object ."], "bleu": 0.3155984539112945, "rouge_l": 0.6421052631578947}
{"id": 4289, "code": "def do unzip ( zipped file , output directory ) : z = zipfile . Zip File ( zipped file ) for path in z . namelist ( ) : relative path = os . path . join ( output directory , path ) dirname , dummy = os . path . split ( relative path ) try : if relative path . endswith ( os . sep ) and not os . path . exists ( dirname ) : os . makedirs ( relative path ) elif not os . path . exists ( relative path ) : dirname = os . path . join ( output directory , os . path . dirname ( path ) ) if os . path . dirname ( path ) and not os . path . exists ( dirname ) : os . makedirs ( dirname ) fd = open ( relative path , \"w\" ) fd . write ( z . read ( path ) ) fd . close ( ) except IO Error , e : raise e return output directory", "predictions": ["unzip a file from local filesystem"], "references": ["perform the actual uncompression ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4290, "code": "def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )", "predictions": ["find all files under a given pattern ."], "references": ["locate all files matching supplied filename pattern recursively ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 4291, "code": "def convert date to iso ( value ) : date formats = [ \"%d %b %Y\" , \"%Y/%m/%d\" ] for dformat in date formats : try : date = datetime . strptime ( value , dformat ) return date . strftime ( \"%Y-%m-%d\" ) except Value Error : pass return value", "predictions": ["convert date to iso - 8601 representation"], "references": ["convert a date - value to the iso date standard ."], "bleu": 0.1380518455178974, "rouge_l": 0.4273204903677758}
{"id": 4292, "code": "def convert date from iso to human ( value ) : try : year , month , day = value . split ( \"-\" ) except Value Error : try : year , month , day = value . split ( \" \" ) except Value Error : return value try : date object = datetime ( int ( year ) , int ( month ) , int ( day ) ) except Type Error : return value return date object . strftime ( \"%d %b %Y\" )", "predictions": ["convert a date value to a human readable date string"], "references": ["convert a date - value to the iso date standard for humans ."], "bleu": 0.2148213130469897, "rouge_l": 0.5097493036211699}
{"id": 4293, "code": "def get temporary file ( prefix = \"tmp \" , suffix = \"\" , directory = None ) : try : file fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file fd ) except IO Error , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath", "predictions": ["get a temporary file"], "references": ["generate a safe and closed filepath ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4294, "code": "def return letters from string ( text ) : out = \"\" for letter in text : if letter . isalpha ( ) : out += letter return out", "predictions": ["return a string with the letters of a string ."], "references": ["get letters from string only ."], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 4295, "code": "def license is oa ( license ) : for oal in OA LICENSES : if re . search ( oal , license ) : return True return False", "predictions": ["check if oa is oa or not"], "references": ["return true if license is compatible with open access"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4296, "code": "def extract package ( self ) : self . path = mkdtemp ( prefix = \"scoap3 package \" , dir = CFG TMPSHAREDDIR ) self . logger . debug ( \"Extracting package: %s\" % ( self . package name , ) ) scoap3utils extract package ( self . package name , self . path , self . logger )", "predictions": ["extract the package from the package ."], "references": ["extract a package in a new temporary directory ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 4297, "code": "def get publication date ( self , xml doc ) : start date = get value in tag ( xml doc , \"prism:cover Date\" ) if not start date : start date = get value in tag ( xml doc , \"prism:cover Display Date\" ) if not start date : start date = get value in tag ( xml doc , 'oa:open Access Effective' ) if start date : start date = datetime . datetime . strptime ( start date , \"%Y-%m-%d T%H:%M:%SZ\" ) return start date . strftime ( \"%Y-%m-%d\" ) import dateutil . parser #dateutil.parser.parse cant process dates like April-June 2016 start date = re . sub ( '([A-Z][a-z]+)[\\s\\-][A-Z][a-z]+ (\\d{4})' , r'\\1 \\2' , start date ) try : date = dateutil . parser . parse ( start date ) except Value Error : return '' if len ( start date . split ( \" \" ) ) == 3 : return date . strftime ( \"%Y-%m-%d\" ) else : return date . strftime ( \"%Y-%m\" ) else : if len ( start date ) is 8 : start date = time . strftime ( '%Y-%m-%d' , time . strptime ( start date , '%Y%m%d' ) ) elif len ( start date ) is 6 : start date = time . strftime ( '%Y-%m' , time . strptime ( start date , '%Y%m' ) ) return start date", "predictions": ["get the start date of the publication ."], "references": ["return the best effort start_date ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 4298, "code": "def parser ( self ) : module = self . module subcommands = self . subcommands if subcommands : module desc = inspect . getdoc ( module ) parser = Parser ( description = module desc , module = module ) subparsers = parser . add subparsers ( ) for sc name , callback in subcommands . items ( ) : sc name = sc name . replace ( \" \" , \"-\" ) cb desc = inspect . getdoc ( callback ) sc parser = subparsers . add parser ( sc name , callback = callback , help = cb desc ) else : parser = Parser ( callback = self . callbacks [ self . function name ] , module = module ) return parser", "predictions": ["build the parser for this command ."], "references": ["return the parser for the current name"], "bleu": 0.345720784641941, "rouge_l": 0.42857142857142855}
{"id": 4299, "code": "def module ( self ) : if not hasattr ( self , ' module' ) : if \" main \" in sys . modules : mod = sys . modules [ \" main \" ] path = self . normalize path ( mod . file ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . module = mod else : self . module = imp . load source ( 'captain script' , self . path ) #self. module = imp.load source(self.module name, self.path) return self . module", "predictions": ["the module that is defined as a module ."], "references": ["load the module so we can actually run the script s function"], "bleu": 0.13309610652103343, "rouge_l": 0.1856925418569254}
{"id": 4300, "code": "def body ( self ) : if not hasattr ( self , ' body' ) : self . body = inspect . getsource ( self . module ) return self . body", "predictions": ["returns the body of the object ."], "references": ["get the contents of the script"], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 4301, "code": "def run ( self , raw args ) : parser = self . parser args , kwargs = parser . parse callback args ( raw args ) callback = kwargs . pop ( \"main callback\" ) if parser . has injected quiet ( ) : levels = kwargs . pop ( \"quiet inject\" , \"\" ) logging . inject quiet ( levels ) try : ret code = callback ( * args , * * kwargs ) ret code = int ( ret code ) if ret code else 0 except Arg Error as e : echo . err ( \"{}: error: {}\" , parser . prog , str ( e ) ) ret code = 2 return ret code", "predictions": ["run the server ."], "references": ["parse and import the script and then run the script s main function"], "bleu": 0.04984021611241231, "rouge_l": 0.2147887323943662}
{"id": 4302, "code": "def can run from cli ( self ) : ret = False ast tree = ast . parse ( self . body , self . path ) calls = self . find calls ( ast tree , name , \"exit\" ) for call in calls : if re . search ( \"{}\\(\" . format ( re . escape ( call ) ) , self . body ) : ret = True break return ret", "predictions": ["checks if a cli can be available from the cli"], "references": ["return true if this script can be run from the command line"], "bleu": 0.17876312353452062, "rouge_l": 0.44721407624633425}
{"id": 4303, "code": "def mock request ( ) : current site = Site . objects . get current ( ) request = Http Request ( ) request . META [ 'SERVER NAME' ] = current site . domain return request", "predictions": ["creates a mock request object"], "references": ["generate a fake request object to allow oembeds to use context processors ."], "bleu": 0.07795171967670728, "rouge_l": 0.3086003372681282}
{"id": 4304, "code": "def get record ( self ) : self . update system numbers ( ) self . add systemnumber ( \"CDS\" ) self . fields list = [ \"024\" , \"041\" , \"035\" , \"037\" , \"088\" , \"100\" , \"110\" , \"111\" , \"242\" , \"245\" , \"246\" , \"260\" , \"269\" , \"300\" , \"502\" , \"650\" , \"653\" , \"693\" , \"700\" , \"710\" , \"773\" , \"856\" , \"520\" , \"500\" , \"980\" ] self . keep only fields ( ) self . determine collections ( ) self . add cms link ( ) self . update languages ( ) self . update reportnumbers ( ) self . update date ( ) self . update pagenumber ( ) self . update authors ( ) self . update subject categories ( \"Sz Ge CERN\" , \"INSPIRE\" , \"categories inspire\" ) self . update keywords ( ) self . update experiments ( ) self . update collaboration ( ) self . update journals ( ) self . update links and ffts ( ) if 'THESIS' in self . collections : self . update thesis supervisors ( ) self . update thesis information ( ) if 'NOTE' in self . collections : self . add notes ( ) for collection in self . collections : record add field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) self . remove controlfields ( ) return self . record", "predictions": ["create a record for each record"], "references": ["override the base get_record ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4305, "code": "def determine collections ( self ) : for value in record get field values ( self . record , '980' , code = 'a' ) : if 'NOTE' in value . upper ( ) : self . collections . add ( 'NOTE' ) if 'THESIS' in value . upper ( ) : self . collections . add ( 'THESIS' ) if 'CONFERENCEPAPER' in value . upper ( ) : self . collections . add ( 'Conference Paper' ) if \"HIDDEN\" in value . upper ( ) : self . hidden = True if self . is published ( ) : self . collections . add ( \"PUBLISHED\" ) self . collections . add ( \"CITEABLE\" ) if 'NOTE' not in self . collections : from itertools import product kb = [ 'ATLAS-CONF-' , 'CMS-PAS-' , 'ATL-' , 'CMS-DP-' , 'ALICE-INT-' , 'LH Cb-PUB-' ] values = record get field values ( self . record , \"088\" , code = 'a' ) for val , rep in product ( values , kb ) : if val . startswith ( rep ) : self . collections . add ( 'NOTE' ) break if record get field values ( self . record , '035' , filter subfield code = \"a\" , filter subfield value = \"ar Xiv\" ) : self . collections . add ( \"ar Xiv\" ) self . collections . add ( 'HEP' ) self . collections . add ( 'CORE' ) if 'Conference Paper' not in self . collections : for value in record get field values ( self . record , tag = '962' , code = 'n' ) : if value [ - 2 : ] . isdigit ( ) : self . collections . add ( 'Conference Paper' ) break record delete fields ( self . record , \"980\" )", "predictions": ["process fields based on the record ."], "references": ["try to determine which collections this record should belong to ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4306, "code": "def add cms link ( self ) : intnote = record get field values ( self . record , '690' , filter subfield code = \"a\" , filter subfield value = 'INTNOTE' ) if intnote : val 088 = record get field values ( self . record , tag = '088' , filter subfield code = \"a\" ) for val in val 088 : if 'CMS' in val : url = ( 'http://weblib.cern.ch/abstract?CERN-CMS' + val . split ( 'CMS' , 1 ) [ - 1 ] ) record add field ( self . record , tag = '856' , ind1 = '4' , subfields = [ ( 'u' , url ) ] )", "predictions": ["add a cms link to the record"], "references": ["special handling if record is a cms note ."], "bleu": 0.19740631366145517, "rouge_l": 0.24448897795591182}
{"id": 4307, "code": "def update date ( self ) : for field in record get field instances ( self . record , '269' ) : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == \"c\" : field [ 0 ] [ idx ] = ( \"c\" , convert date to iso ( value ) ) record delete fields ( self . record , \"260\" ) if 'THESIS' not in self . collections : for field in record get field instances ( self . record , '260' ) : record add field ( self . record , '269' , subfields = field [ 0 ] ) record delete fields ( self . record , '260' )", "predictions": ["update the date fields to include the iso - 8601 fields ."], "references": ["269 date normalization ."], "bleu": 0.11498759556447223, "rouge_l": 0.27477477477477474}
{"id": 4308, "code": "def update pagenumber ( self ) : for field in record get field instances ( self . record , '300' ) : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : if \"mult.\" not in value and value != \" p\" : field [ 0 ] [ idx ] = ( 'a' , re . sub ( r'[^\\d-]+' , '' , value ) ) else : record delete field ( self . record , '300' , field position global = field [ 4 ] ) break", "predictions": ["update pagenumber record record record information ."], "references": ["300 page number ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4309, "code": "def update authors ( self ) : author names = record get field instances ( self . record , '100' ) author names . extend ( record get field instances ( self . record , '700' ) ) for field in author names : subs = field get subfields ( field ) if 'i' not in subs or 'XX' in subs [ 'i' ] : if 'j' not in subs or 'YY' in subs [ 'j' ] : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , punctuate authorname ( value ) )", "predictions": ["update the list of authors from the record ."], "references": ["100 & 700 punctuate author names ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4310, "code": "def update thesis information ( self ) : fields 501 = record get field instances ( self . record , '502' ) for idx , field in enumerate ( fields 501 ) : new subs = [ ] for key , value in field [ 0 ] : if key == 'a' : new subs . append ( ( 'b' , value ) ) elif key == 'b' : new subs . append ( ( 'c' , value ) ) elif key == 'c' : new subs . append ( ( 'd' , value ) ) else : new subs . append ( ( key , value ) ) fields 501 [ idx ] = field swap subfields ( field , new subs )", "predictions": ["update the list of fields ."], "references": ["501 degree info - move subfields ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4311, "code": "def update keywords ( self ) : for field in record get field instances ( self . record , '653' , ind1 = '1' ) : subs = field get subfields ( field ) new subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new field = create field ( subfields = new subs , ind1 = '1' ) record replace field ( self . record , '653' , new field , field position global = field [ 4 ] )", "predictions": ["update the record record ."], "references": ["653 free keywords ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4312, "code": "def update journals ( self ) : for field in record get field instances ( self . record , '773' ) : subs = field get subfield instances ( field ) new subs = [ ] for idx , ( key , value ) in enumerate ( subs ) : if key == 'p' : journal name = self . get config item ( value , \"journals\" , allow substring = False ) journal name = journal name . replace ( '. ' , '.' ) . strip ( ) new subs . append ( ( key , journal name ) ) else : new subs . append ( ( key , value ) ) record delete field ( self . record , tag = \"773\" , field position global = field [ 4 ] ) record add field ( self . record , \"773\" , subfields = new subs )", "predictions": ["updates journals attribute values for every journal ."], "references": ["773 journal translations ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 4313, "code": "def extract packages ( self ) : self . path unpacked = [ ] if not hasattr ( self , \"retrieved packages unpacked\" ) : self . retrieved packages unpacked = [ self . package name ] for path in self . retrieved packages unpacked : self . logger . debug ( \"Extracting package: %s\" % ( path , ) ) p name = 'EPJC' if 'EPJC' in path else 'JHEP' p message = 'scoap3 package %s %s ' % ( p name , datetime . now ( ) ) self . path unpacked . append ( mkdtemp ( prefix = p message , dir = CFG TMPSHAREDDIR ) ) try : Zip File ( path ) . extractall ( self . path unpacked [ - 1 ] ) except Exception : register exception ( alert admin = True , prefix = \"Springer error extracting package.\" ) self . logger . error ( \"Error extraction package file: %s\" % ( path , ) ) return self . path unpacked", "predictions": ["extract packages from the extraction package"], "references": ["extract a package in a new directory ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4314, "code": "def record delete subfield ( rec , tag , subfield code , ind1 = ' ' , ind2 = ' ' ) : ind1 , ind2 = wash indicators ( ind1 , ind2 ) for field in rec . get ( tag , [ ] ) : if field [ 1 ] == ind1 and field [ 2 ] == ind2 : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield code != subfield [ 0 ] ]", "predictions": ["delete record when a record is deleted ."], "references": ["delete all subfields with subfield_code in the record ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 4315, "code": "def record replace field ( rec , tag , new field , field position global = None , field position local = None ) : if field position global is None and field position local is None : raise Invenio Bib Record Field Error ( \"A field position is required to \" \"complete this operation.\" ) elif field position global is not None and field position local is not None : raise Invenio Bib Record Field Error ( \"Only one field position is required \" \"to complete this operation.\" ) elif field position global : if tag not in rec : raise Invenio Bib Record Field Error ( \"No tag '%s' in record.\" % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field position global : rec [ tag ] [ position ] = new field replaced = True if not replaced : raise Invenio Bib Record Field Error ( \"No field has the tag '%s' and \" \"the global field position '%d'.\" % ( tag , field position global ) ) else : try : rec [ tag ] [ field position local ] = new field except Key Error : raise Invenio Bib Record Field Error ( \"No tag '%s' in record.\" % tag ) except Index Error : raise Invenio Bib Record Field Error ( \"No field has the tag '%s' and \" \"the local field position '%d'.\" % ( tag , field position local ) )", "predictions": ["record a tag with a new value ."], "references": ["replace a field with a new field ."], "bleu": 0.3303164318013807, "rouge_l": 0.625}
{"id": 4316, "code": "def record modify controlfield ( rec , tag , controlfield value , field position global = None , field position local = None ) : field = record get field ( rec , tag , field position global = field position global , field position local = field position local ) new field = ( field [ 0 ] , field [ 1 ] , field [ 2 ] , controlfield value , field [ 4 ] ) record replace field ( rec , tag , new field , field position global = field position global , field position local = field position local )", "predictions": ["create new fields for a record"], "references": ["modify controlfield at position specified by tag and field number ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 4317, "code": "def field xml output ( field , tag ) : marcxml = [ ] if field [ 3 ] : marcxml . append ( '  <controlfield tag=\"%s\">%s</controlfield>' % ( tag , Math ML Parser . html to text ( field [ 3 ] ) ) ) else : marcxml . append ( '  <datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">' % ( tag , field [ 1 ] , field [ 2 ] ) ) marcxml += [ subfield xml output ( subfield ) for subfield in field [ 0 ] ] marcxml . append ( '  </datafield>' ) return '\\n' . join ( marcxml )", "predictions": ["return the xml output for a field ."], "references": ["generate the xml for field field and returns it as a string ."], "bleu": 0.1396215680138453, "rouge_l": 0.45658682634730546}
{"id": 4318, "code": "def record strip empty volatile subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != \"VOLATILE:\" ]", "predictions": ["remove empty volatile subfields from a key ."], "references": ["remove unchanged volatile subfields from the record ."], "bleu": 0.3303164318013807, "rouge_l": 0.625}
{"id": 4319, "code": "def record make all subfields volatile ( rec ) : for tag in rec . keys ( ) : for field position , field in enumerate ( rec [ tag ] ) : for subfield position , subfield in enumerate ( field [ 0 ] ) : if subfield [ 1 ] [ : 9 ] != \"VOLATILE:\" : record modify subfield ( rec , tag , subfield [ 0 ] , \"VOLATILE:\" + subfield [ 1 ] , subfield position , field position local = field position )", "predictions": ["make all subfields volatile volatile volatile"], "references": ["turns all subfields to volatile"], "bleu": 0.31239399369202553, "rouge_l": 0.5545454545454546}
{"id": 4320, "code": "def record sort by indicators ( record ) : for tag , fields in record . items ( ) : record [ tag ] = fields sort by indicators ( fields )", "predictions": ["logger all name in a create create key records . that have a create key ."], "references": ["sort the fields inside the record by indicators ."], "bleu": 0.07692375026049747, "rouge_l": 0.08425414364640883}
{"id": 4321, "code": "def get children by tag name ( node , name ) : try : return [ child for child in node . child Nodes if child . node Name == name ] except Type Error : return [ ]", "predictions": ["= a list of unzip - style unzip by"], "references": ["retrieve all children from node node with name name ."], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 4322, "code": "def parse ( self , path to xml = None ) : if not path to xml : if not self . path : self . logger . error ( \"No path defined!\" ) return path to xml = self . path root = self . clean xml ( path to xml ) if root . tag . lower ( ) == 'collection' : tree = ET . Element Tree ( root ) self . records = element tree collection to records ( tree ) elif root . tag . lower ( ) == 'record' : new root = ET . Element ( 'collection' ) new root . append ( root ) tree = ET . Element Tree ( new root ) self . records = element tree collection to records ( tree ) else : header subs = get request subfields ( root ) records = root . find ( 'List Records' ) if records is None : records = root . find ( 'Get Record' ) if records is None : raise Value Error ( \"Cannot find List Records or Get Record!\" ) tree = ET . Element Tree ( records ) for record , is deleted in element tree oai records ( tree , header subs ) : if is deleted : self . deleted records . append ( self . create deleted record ( record ) ) else : self . records . append ( record )", "predictions": ["parses a single xml element into a list of dicts"], "references": ["parse an xml document and clean any namespaces ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 4323, "code": "def create deleted record ( self , record ) : identifier = record get field value ( record , tag = \"037\" , code = \"a\" ) recid = identifier . split ( \":\" ) [ - 1 ] try : source = identifier . split ( \":\" ) [ 1 ] except Index Error : source = \"Unknown\" record add field ( record , \"035\" , subfields = [ ( \"9\" , source ) , ( \"a\" , recid ) ] ) record add field ( record , \"980\" , subfields = [ ( \"c\" , \"DELETED\" ) ] ) return record", "predictions": ["convert a date to a date to a date to a to a to a to a to a to a to a to a date to a to a to"], "references": ["generate the record deletion if deleted form oai - pmh ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4324, "code": "def login ( self , session , get request = False ) : req = session . post ( self . login url , data = self . logindata ) if LOGIN ERROR STRING in req . text or req . status code == 403 or req . url == LOGIN URL : err mess = \"Yesss SMS: login failed, username or password wrong\" if LOGIN LOCKED MESS in req . text : err mess += \", page says: \" + LOGIN LOCKED MESS ENG self . suspended = True raise self . Account Suspended Error ( err mess ) raise self . Login Error ( err mess ) self . suspended = False return ( session , req ) if get request else session", "predictions": ["convert user to \"yesss . . . . . . . . . . . . . . . . . . . . . ."], "references": ["return a session for yesss . at ."], "bleu": 0.051660454541342535, "rouge_l": 0.1300639658848614}
{"id": 4325, "code": "def login data valid ( self ) : login working = False try : with self . login ( requests . Session ( ) ) as sess : sess . get ( self . logout url ) except self . Login Error : pass else : login working = True return login working", "predictions": ["get get get temporary get temporary get temporary working . . . . . . . . . . . . . . . . . . . . . ."], "references": ["check for working login data ."], "bleu": 0.04317900023606586, "rouge_l": 0.12310797174571139}
{"id": 4326, "code": "def send ( self , recipient , message ) : if self . logindata [ 'login rufnummer' ] is None or self . logindata [ 'login passwort' ] is None : err mess = \"Yesss SMS: Login data required\" raise self . Login Error ( err mess ) if not recipient : raise self . No Recipient Error ( \"Yesss SMS: recipient number missing\" ) if not isinstance ( recipient , str ) : raise Value Error ( \"Yesss SMS: str expected as recipient number\" ) if not message : raise self . Empty Message Error ( \"Yesss SMS: message is empty\" ) with self . login ( requests . Session ( ) ) as sess : sms data = { 'to nummer' : recipient , 'nachricht' : message } req = sess . post ( self . websms url , data = sms data ) if not ( req . status code == 200 or req . status code == 302 ) : raise self . SMS Sending Error ( \"Yesss SMS: error sending SMS\" ) if UNSUPPORTED CHARS STRING in req . text : raise self . Unsupported Chars Error ( \"Yesss SMS: message contains unsupported character(s)\" ) if SMS SENDING SUCCESSFUL STRING not in req . text : raise self . SMS Sending Error ( \"Yesss SMS: error sending SMS\" ) sess . get ( self . logout url )", "predictions": ["return a recipient to the recipient = none = none = none = none = 1 = 1 = 1 = none"], "references": ["send an sms ."], "bleu": 0.04657469807170698, "rouge_l": 0.0}
{"id": 4327, "code": "def get date ( self , filename ) : try : self . document = parse ( filename ) return self . get date ( ) except Date Not Found Exception : print ( \"Date problem found in {0}\" . format ( filename ) ) return datetime . datetime . strftime ( datetime . datetime . now ( ) , \"%Y-%m-%d\" )", "predictions": ["license for filename ."], "references": ["return the date of the article in file ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4328, "code": "def get collection ( self , journal ) : conference = '' for tag in self . document . get Elements By Tag Name ( 'conference' ) : conference = xml to text ( tag ) if conference or journal == \"International Journal of Modern Physics: Conference Series\" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Conference Paper' ) ] elif self . get article type ( ) == \"review-article\" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Review' ) ] else : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Published' ) ]", "predictions": ["extract the package s package information from the journal"], "references": ["return this articles collection ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4329, "code": "def attach fulltext ( self , rec , doi ) : url = os . path . join ( self . url prefix , doi ) record add field ( rec , 'FFT' , subfields = [ ( 'a' , url ) , ( 't' , 'INSPIRE-PUBLIC' ) , ( 'd' , 'Fulltext' ) ] )", "predictions": ["get publication publication value for a given xml file value value value value value value value value value value value value value value value value value value value value value value"], "references": ["attach fulltext fft ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4330, "code": "def get config item ( cls , key , kb name , allow substring = True ) : config dict = cls . kbs . get ( kb name , None ) if config dict : if key in config dict : return config dict [ key ] elif allow substring : res = [ v for k , v in config dict . items ( ) if key in k ] if res : return res [ 0 ] return key", "predictions": ["callback for getting a config self subparsers subparsers"], "references": ["return the opposite mapping by searching the imported kb ."], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 4331, "code": "def match ( self , query = None , * * kwargs ) : from invenio . search engine import perform request search if not query : recid = self . record [ \"001\" ] [ 0 ] [ 3 ] return perform request search ( p = \"035:%s\" % ( recid , ) , of = \"id\" ) else : if \"recid\" not in kwargs : kwargs [ \"recid\" ] = self . record [ \"001\" ] [ 0 ] [ 3 ] return perform request search ( p = query % kwargs , of = \"id\" )", "predictions": ["imp a sys in the normalize in the normalize database in the normalize"], "references": ["try to match the current record to the database ."], "bleu": 0.1135935489027116, "rouge_l": 0.2671532846715329}
{"id": 4332, "code": "def keep only fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields list : record delete fields ( self . record , tag )", "predictions": ["return all self self self . that were not defined"], "references": ["keep only fields listed in field_list ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 4333, "code": "def strip fields ( self ) : for tag in self . record . keys ( ) : if tag in self . fields list : record delete fields ( self . record , tag )", "predictions": ["run all fields = true"], "references": ["clear any fields listed in field_list ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4334, "code": "def add systemnumber ( self , source , recid = None ) : if not recid : recid = self . get recid ( ) if not self . hidden and recid : record add field ( self . record , tag = '035' , subfields = [ ( '9' , source ) , ( 'a' , recid ) ] )", "predictions": ["can only be called when a path should be added to the path"], "references": ["add 035 number from 001 recid with given source ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 4335, "code": "def add control number ( self , tag , value ) : record add field ( self . record , tag , controlfield value = value )", "predictions": ["register a request number"], "references": ["add a control - number 00x for given tag with value ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 4336, "code": "def update subject categories ( self , primary , secondary , kb ) : category fields = record get field instances ( self . record , tag = '650' , ind1 = '1' , ind2 = '7' ) record delete fields ( self . record , \"650\" ) for field in category fields : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : new value = self . get config item ( value , kb ) if new value != value : new subs = [ ( '2' , secondary ) , ( 'a' , new value ) ] else : new subs = [ ( '2' , primary ) , ( 'a' , value ) ] record add field ( self . record , \"650\" , ind1 = \"1\" , ind2 = \"7\" , subfields = new subs ) break", "predictions": ["get all record categories that have changed ."], "references": ["650 translate categories ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 4337, "code": "def connect ( self ) : self . ftp . connect ( ) self . ftp . login ( user = self . username , passwd = self . passwd )", "predictions": ["determine which in for the for the for the for the for the for the for the for the for the for the for the for the for the for the"], "references": ["connects and logins to the server ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 4338, "code": "def parse data ( self , text , maxwidth , maxheight , template dir , context , urlize all links ) : replacements = { } user urls = set ( re . findall ( URL RE , text ) ) for user url in user urls : try : resource = oembed . site . embed ( user url , maxwidth = maxwidth , maxheight = maxheight ) except O Embed Exception : if urlize all links : replacements [ user url ] = '<a href=\"%(LINK)s\">%(LINK)s</a>' % { 'LINK' : user url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) replacement = self . render oembed ( resource , user url , template dir = template dir , context = context ) replacements [ user url ] = replacement . strip ( ) user urls = re . finditer ( URL RE , text ) matches = [ ] for match in user urls : if match . group ( ) in replacements : matches . append ( [ match . start ( ) , match . end ( ) , match . group ( ) ] ) for indx , ( start , end , user url ) in enumerate ( matches ) : replacement = replacements [ user url ] difference = len ( replacement ) - len ( user url ) text = text [ : start ] + replacement + text [ end : ] for j in xrange ( indx + 1 , len ( matches ) ) : matches [ j ] [ 0 ] += difference matches [ j ] [ 1 ] += difference return mark safe ( text )", "predictions": ["add cms cms cms cms to a list of cms ."], "references": ["parses a block of text indiscriminately"], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 4339, "code": "def get rev ( self , key , version , * * kwa ) : if ' doc' in kwa : doc = kwa [ ' doc' ] else : if type ( version ) is int : if version == 0 : order = pymongo . ASCENDING elif version == - 1 : order = pymongo . DESCENDING doc = self . collection . find one ( { 'k' : key } , sort = [ [ 'd' , order ] ] ) elif type ( version ) is datetime : ver = self . round time ( version ) doc = self . collection . find one ( { 'k' : key , 'd' : ver } ) if doc is None : raise Key Error ( 'Supplied key `{0}` or version `{1}` does not exist' . format ( key , str ( version ) ) ) coded val = doc [ 'v' ] return pickle . loads ( coded val )", "predictions": ["retrieves a value for a given for a given for a given for a given for a given for key"], "references": ["obtain particular version of the doc at key ."], "bleu": 0.06108557268562171, "rouge_l": 0.07402912621359223}
{"id": 4340, "code": "def hashkey ( self , method , url , * * kwa ) : to hash = '' . join ( [ str ( method ) , str ( url ) , str ( kwa . get ( 'data' , '' ) ) , str ( kwa . get ( 'params' , '' ) ) ] ) return hashlib . md5 ( to hash . encode ( ) ) . hexdigest ( )", "predictions": ["generate an md5 request for the given url and url . . . . . . . . . . ."], "references": ["find a hash value for the linear combination of invocation methods ."], "bleu": 0.0821610732492254, "rouge_l": 0.19122257053291536}
{"id": 4341, "code": "def available drivers ( ) : global modules global available if type ( modules ) is not list : modules = list ( modules ) if not available : available = [ d . ahio Driver Info . NAME for d in modules if d . ahio Driver Info . AVAILABLE ] return available", "predictions": ["get list of author names"], "references": ["returns a list of available drivers names ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 4342, "code": "def guess array memory usage ( bam readers , dtype , use strand = False ) : ARRAY COUNT = 5 if not isinstance ( bam readers , list ) : bam readers = [ bam readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY DTYPES . get ( dtype , None ) use strand = use strand + 1 #if false, factor of 1, if true, factor of 2 dtypes = guess numpy dtypes from idxstats ( bam readers , default = None , force dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : #found no info from idx dtypes = guess numpy dtypes from idxstats ( bam readers , default = dtype or numpy . uint64 , force dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read groups = [ ] no read group = False for bam in bam readers : rgs = bam . get read groups ( ) if rgs : for rg in rgs : if rg not in read groups : read groups . append ( rg ) else : no read group = True read groups = len ( read groups ) + no read group max ref size = 0 array byte overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array count = ARRAY COUNT * use strand * read groups for bam in bam readers : for i , ( name , length ) in enumerate ( bam . get references ( ) ) : if dtypes [ i ] is not None : max ref size = max ( max ref size , ( length + length * dtypes [ i ] ( ) . nbytes * array count + ( array byte overhead * ( array count + 1 ) ) ) ) return max ref size", "predictions": ["update array information from the bam"], "references": ["returns an estimate for the maximum amount of memory to be consumed by numpy arrays ."], "bleu": 0.041721848418993325, "rouge_l": 0.0840220385674931}
{"id": 4343, "code": "def main ( ) : usage = \"Usage: %prog PATH TO PACKAGE\" parser = optparse . Option Parser ( usage = usage ) parser . add option ( \"-v\" , \"--verbose\" , action = \"store true\" , dest = \"verbose\" , default = False , help = \"Show debug output\" ) parser . add option ( \"-d\" , \"--output-dir\" , action = \"store\" , type = \"string\" , dest = \"output dir\" , default = '' , help = \"\" ) parser . add option ( \"-t\" , \"--test-args\" , action = \"store\" , type = \"string\" , dest = \"test args\" , default = '' , help = ( \"Pass argument on to bin/test. Quote the argument, \" + \"for instance \\\"-t '-m somemodule'\\\".\" ) ) ( options , args ) = parser . parse args ( ) if options . verbose : log level = logging . DEBUG else : log level = logging . INFO logging . basic Config ( level = log level , format = \"%(levelname)s: %(message)s\" ) curdir = os . getcwd ( ) testbinary = os . path . join ( curdir , 'bin' , 'test' ) if not os . path . exists ( testbinary ) : raise Runtime Error ( \"Test command doesn't exist: %s\" % testbinary ) coveragebinary = os . path . join ( curdir , 'bin' , 'coverage' ) if not os . path . exists ( coveragebinary ) : logger . debug ( \"Trying globally installed coverage command.\" ) coveragebinary = 'coverage' logger . info ( \"Running tests in coverage mode (can take a long time)\" ) parts = [ coveragebinary , 'run' , testbinary ] if options . test args : parts . append ( options . test args ) system ( \" \" . join ( parts ) ) logger . debug ( \"Creating coverage reports...\" ) if options . output dir : coverage dir = options . output dir open in browser = False else : coverage dir = 'htmlcov' open in browser = True system ( \"%s html --directory=%s\" % ( coveragebinary , coverage dir ) ) logger . info ( \"Wrote coverage files to %s\" , coverage dir ) if open in browser : index file = os . path . abspath ( os . path . join ( coverage dir , 'index.html' ) ) logger . debug ( \"About to open %s in your webbrowser.\" , index file ) webbrowser . open ( 'file://' + index file ) logger . info ( \"Opened reports in your browser.\" )", "predictions": ["parse coverage coverage coverage and run coverage coverage"], "references": ["create coverage reports and open them in the browser ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 4344, "code": "def output entire buffer ( self ) : green = 0 red = 0 for row in range ( 0 , 8 ) : for col in range ( 0 , 8 ) : if self . display buffer [ row ] [ col ] == self . LED GREEN : green |= 1 << col elif self . display buffer [ row ] [ col ] == self . LED RED : red |= 1 << col elif self . display buffer [ row ] [ col ] == self . LED YELLOW : green |= 1 << col red |= 1 << col elif self . display buffer [ row ] [ col ] == self . LED OFF : green &= ~ ( 1 << col ) red &= ~ ( 1 << col ) self . firmata . i2c write ( 0x70 , row * 2 , 0 , green ) self . firmata . i2c write ( 0x70 , row * 2 + 1 , 0 , red )", "predictions": ["update buffer buffer buffer . . ."], "references": ["write the entire buffer to the display"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4345, "code": "def clear display buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display buffer [ row ] [ column ] = 0", "predictions": ["extract the packages from the packages ."], "references": ["set all led s to off ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4346, "code": "def retrieve url ( self , url ) : try : r = requests . get ( url ) except requests . Connection Error : raise exceptions . Retrieve Error ( 'Connection fail' ) if r . status code >= 400 : raise exceptions . Retrieve Error ( 'Connected, but status code is %s' % ( r . status code ) ) real url = r . url content = r . content try : content type = r . headers [ 'Content-Type' ] except Key Error : content type , encoding = mimetypes . guess type ( real url , strict = False ) self . response = r return content type . lower ( ) , content", "predictions": ["retrieves the content for a given delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete request ind2 ind2 ind2 ind2 delete"], "references": ["use requests to fetch remote content"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 4347, "code": "def getnodenamefor ( self , name ) : return 'node ' + str ( ( abs ( binascii . crc32 ( b ( name ) ) & 0xffffffff ) % self . no servers ) + 1 )", "predictions": ["convert from string to string"], "references": ["return the node name where the name would land to"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4348, "code": "def getnodefor ( self , name ) : node = self . getnodenamefor ( name ) return { node : self . cluster [ 'nodes' ] [ node ] }", "predictions": ["return the record of a specific value"], "references": ["return the node where the name would land to"], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 4349, "code": "def object ( self , infotype , key ) : redisent = self . redises [ self . getnodenamefor ( key ) + ' slave' ] return getattr ( redisent , 'object' ) ( infotype , key )", "predictions": ["return is a function for testing"], "references": ["return the encoding idletime or refcount about the key"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4350, "code": "def rc mset ( self , mapping ) : result = True for k , v in iteritems ( mapping ) : result = result and self . set ( k , v ) return result", "predictions": ["returns a map of the mapping to a mapping 0 0 if the mapping is not a different"], "references": ["sets each key in the mapping dict to its corresponding value"], "bleu": 0.09629943614188137, "rouge_l": 0.21631205673758863}
{"id": 4351, "code": "def rc mget ( self , keys , * args ) : args = list or args ( keys , args ) result = [ ] for key in args : result . append ( self . get ( key ) ) return result", "predictions": ["record a list of keys ."], "references": ["returns a list of values ordered identically to * args"], "bleu": 0.21108303712651422, "rouge_l": 0.3588235294117647}
{"id": 4352, "code": "def rc rename ( self , src , dst ) : if src == dst : return self . rename ( src + \"{\" + src + \"}\" , src ) if not self . exists ( src ) : return self . rename ( src + \"{\" + src + \"}\" , src ) self . delete ( dst ) ktype = self . type ( src ) kttl = self . ttl ( src ) if ktype == b ( 'none' ) : return False if ktype == b ( 'string' ) : self . set ( dst , self . get ( src ) ) elif ktype == b ( 'hash' ) : self . hmset ( dst , self . hgetall ( src ) ) elif ktype == b ( 'list' ) : for k in self . lrange ( src , 0 , - 1 ) : self . rpush ( dst , k ) elif ktype == b ( 'set' ) : for k in self . smembers ( src ) : self . sadd ( dst , k ) elif ktype == b ( 'zset' ) : for k , v in self . zrange ( src , 0 , - 1 , withscores = True ) : self . zadd ( dst , v , k ) kttl = - 1 if kttl is None or kttl < 0 else int ( kttl ) if kttl != - 1 : self . expire ( dst , kttl ) return self . delete ( src )", "predictions": ["rename src between two vs vs vs color and dst"], "references": ["rename key src to dst"], "bleu": 0.14991106946711685, "rouge_l": 0.42558139534883715}
{"id": 4353, "code": "def rc renamenx ( self , src , dst ) : if self . exists ( dst ) : return False return self . rc rename ( src , dst )", "predictions": ["check if src src has been renamenx ."], "references": ["rename key src to dst if dst doesn t already exist"], "bleu": 0.12197601375336842, "rouge_l": 0.10234899328859062}
{"id": 4354, "code": "def rc keys ( self , pattern = '*' ) : result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( ' slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result", "predictions": ["rc keys that match a given pattern ."], "references": ["returns a list of keys matching pattern"], "bleu": 0.19070828081828378, "rouge_l": 0.26991150442477874}
{"id": 4355, "code": "def rc dbsize ( self ) : result = 0 for alias , redisent in iteritems ( self . redises ) : if alias . find ( ' slave' ) == - 1 : continue result += redisent . dbsize ( ) return result", "predictions": ["return a list of all the dbsize in the alias ."], "references": ["returns the number of keys in the current database"], "bleu": 0.17033186037639278, "rouge_l": 0.3055091819699499}
{"id": 4356, "code": "def prepare ( self ) : attributes , elements = Ordered Dict ( ) , [ ] nsmap = dict ( [ self . meta . namespace ] ) for name , item in self . items . items ( ) : if isinstance ( item , Attribute ) : attributes [ name ] = item . prepare ( self ) elif isinstance ( item , Element ) : nsmap . update ( [ item . namespace ] ) elements . append ( item ) return attributes , elements , nsmap", "predictions": ["prepare the task attributes and turn into a dictionary ."], "references": ["prepare the date in the instance state for serialization ."], "bleu": 0.17827531042796255, "rouge_l": 0.3}
{"id": 4357, "code": "def get queryset ( self , request ) : qs = super ( Gallery Admin , self ) . get queryset ( request ) return qs . annotate ( photo count = Count ( 'photos' ) )", "predictions": ["restrict the listed photo for the current user ."], "references": ["add number of photos to each gallery ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4358, "code": "def save model ( self , request , obj , form , change ) : obj . author = request . user obj . save ( )", "predictions": ["save author as author ."], "references": ["set currently authenticated user as the author of the gallery ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 4359, "code": "def save formset ( self , request , form , formset , change ) : instances = formset . save ( commit = False ) for instance in instances : if isinstance ( instance , Photo ) : instance . author = request . user instance . save ( )", "predictions": ["save an author instance ."], "references": ["for each photo set it s author to currently authenticated user ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 4360, "code": "def convert ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if end is None : result . append ( ( start , length - 1 ) ) elif start is None : s = length - end result . append ( ( 0 if s < 0 else s , length - 1 ) ) else : result . append ( ( start , end if end < length else length - 1 ) ) return result", "predictions": ["convert ranges to a list of ranges ."], "references": ["converts to valid byte ranges"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4361, "code": "def condense ranges ( cls , ranges ) : result = [ ] if ranges : ranges . sort ( key = lambda tup : tup [ 0 ] ) result . append ( ranges [ 0 ] ) for i in range ( 1 , len ( ranges ) ) : if result [ - 1 ] [ 1 ] + 1 >= ranges [ i ] [ 0 ] : result [ - 1 ] = ( result [ - 1 ] [ 0 ] , max ( result [ - 1 ] [ 1 ] , ranges [ i ] [ 1 ] ) ) else : result . append ( ranges [ i ] ) return result", "predictions": ["condense ranges by ranges"], "references": ["sorts and removes overlaps"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4362, "code": "def print read ( self , rid ) : if self . rname is not None : print self . rname [ rid ] print '--' r = self . get read data ( rid ) aligned loci = np . unique ( r . nonzero ( ) [ 1 ] ) for locus in aligned loci : nzvec = r [ : , locus ] . todense ( ) . transpose ( ) [ 0 ] . A . flatten ( ) if self . lname is not None : print self . lname [ locus ] , else : print locus , print nzvec", "predictions": ["print a report of loci"], "references": ["prints nonzero rows of the read wanted"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4363, "code": "def setup ( ) : s = str . split if sys . version info < ( 3 , 0 ) : s = unicode . split def pop all ( some dict , some list ) : for scheme in some list : some dict . pop ( scheme ) global SCHEMES SCHEMES = copy . deepcopy ( sanscript . SCHEMES ) pop all ( SCHEMES , [ sanscript . ORIYA , sanscript . BENGALI , sanscript . GUJARATI ] ) SCHEMES [ HK ] . update ( { 'vowels' : s ( \"\"\"a A i I u U R RR l R l RR E ai O au\"\"\" ) + s ( \"\"\"e o\"\"\" ) , 'marks' : s ( \"\"\"A i I u U R RR l R l RR E ai O au\"\"\" ) + s ( \"\"\"e o\"\"\" ) , 'consonants' : sanscript . SCHEMES [ HK ] [ 'consonants' ] + s ( \"\"\"n2 r2 zh\"\"\" ) } ) SCHEMES [ ITRANS ] . update ( { 'vowels' : s ( \"\"\"a A i I u U R RR L Li LLI E ai O au\"\"\" ) + s ( \"\"\"e o\"\"\" ) , 'marks' : s ( \"\"\"A i I u U R RR L Li LLI E ai O au\"\"\" ) + s ( \"\"\"e o\"\"\" ) , 'consonants' : sanscript . SCHEMES [ ITRANS ] [ 'consonants' ] + s ( \"\"\"n2 r2 zh\"\"\" ) } ) pop all ( SCHEMES [ ITRANS ] . synonym map , s ( \"\"\"e o\"\"\" ) ) SCHEMES [ OPTITRANS ] . update ( { 'vowels' : s ( \"\"\"a A i I u U R RR L Li LLI E ai O au\"\"\" ) + s ( \"\"\"e o\"\"\" ) , 'marks' : s ( \"\"\"A i I u U R RR L Li LLI E ai O au\"\"\" ) + s ( \"\"\"e o\"\"\" ) , 'consonants' : sanscript . SCHEMES [ OPTITRANS ] [ 'consonants' ] + s ( \"\"\"n2 r2 zh\"\"\" ) } ) pop all ( SCHEMES [ OPTITRANS ] . synonym map , s ( \"\"\"e o\"\"\" ) )", "predictions": ["sets up the default configuration"], "references": ["add a variety of default schemes ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4364, "code": "def to utf8 ( y ) : out = [ ] for x in y : if x < 0x080 : out . append ( x ) elif x < 0x0800 : out . append ( ( x >> 6 ) | 0x C0 ) out . append ( ( x & 0x3F ) | 0x80 ) elif x < 0x10000 : out . append ( ( x >> 12 ) | 0x E0 ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) else : out . append ( ( x >> 18 ) | 0x F0 ) out . append ( ( x >> 12 ) & 0x3F ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) return '' . join ( map ( chr , out ) )", "predictions": ["convert a series of utf8 to a camelcase representation"], "references": ["converts an array of integers to utf8 string"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 4365, "code": "def set script ( self , i ) : if i in range ( 1 , 10 ) : n = i - 1 else : raise Illegal Input ( \"Invalid Value for ATR %s\" % ( hex ( i ) ) ) if n > - 1 : self . curr script = n self . delta = n * DELTA return", "predictions": ["sets the script i . e . g . the script is ignored ."], "references": ["set the value of delta to reflect the current codepage"], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 4366, "code": "def unrecognised ( chr ) : if options [ 'handle Unrecognised' ] == UNRECOGNISED ECHO : return chr elif options [ 'handle Unrecognised' ] == UNRECOGNISED SUBSTITUTE : return options [ 'substitute Char' ] else : raise ( Key Error , chr )", "predictions": ["convert a python string to a unrecognised ."], "references": ["handle unrecognised characters ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 4367, "code": "def clear sent messages ( self , offset = None ) : if offset is None : offset = getattr ( settings , 'MAILQUEUE CLEAR OFFSET' , defaults . MAILQUEUE CLEAR OFFSET ) if type ( offset ) is int : offset = datetime . timedelta ( hours = offset ) delete before = timezone . now ( ) - offset self . filter ( sent = True , last attempt lte = delete before ) . delete ( )", "predictions": ["clear all sent messages ."], "references": ["deletes sent mailermessage records"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4368, "code": "def get orthographies ( self , library = library ) : results = [ ] for charset in library . charsets : if self . charsets : cn = getattr ( charset , 'common name' , False ) abbr = getattr ( charset , 'abbreviation' , False ) nn = getattr ( charset , 'short name' , False ) naive = getattr ( charset , 'native name' , False ) if cn and cn . lower ( ) in self . charsets : results . append ( charset ) elif nn and nn . lower ( ) in self . charsets : results . append ( charset ) elif naive and naive . lower ( ) in self . charsets : results . append ( charset ) elif abbr and abbr . lower ( ) in self . charsets : results . append ( charset ) else : results . append ( charset ) for result in results : yield Charset Info ( self , result )", "predictions": ["return generator of nn"], "references": ["returns list of charsetinfo about supported orthographies"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4369, "code": "def generate oauth2 headers ( self ) : encoded credentials = base64 . b64encode ( ( '{0}:{1}' . format ( self . consumer key , self . consumer secret ) ) . encode ( 'utf-8' ) ) headers = { 'Authorization' : 'Basic {0}' . format ( encoded credentials . decode ( 'utf-8' ) ) , 'Content-Type' : 'application/x-www-form-urlencoded' } return headers", "predictions": ["sets the consumer headers for the consumer consumer ."], "references": ["generates header for oauth2"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4370, "code": "def get data ( filename ) : name , ext = get file extension ( filename ) func = json get data if ext == '.json' else yaml get data return func ( filename )", "predictions": ["get data from file"], "references": ["calls right function according to file extension"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4371, "code": "def write data ( data , filename ) : name , ext = get file extension ( filename ) func = json write data if ext == '.json' else yaml write data return func ( data , filename )", "predictions": ["writes data to a file"], "references": ["call right func to save data according to file extension"], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 4372, "code": "def json write data ( json data , filename ) : with open ( filename , 'w' ) as fp : json . dump ( json data , fp , indent = 4 , sort keys = True , ensure ascii = False ) return True return False", "predictions": ["writes data to json file"], "references": ["write json data into a file"], "bleu": 0.2658156069371863, "rouge_l": 0.3577712609970674}
{"id": 4373, "code": "def json get data ( filename ) : with open ( filename ) as fp : json data = json . load ( fp ) return json data return False", "predictions": ["get data from json file"], "references": ["get data from json file"], "bleu": 1.0, "rouge_l": 1.0}
{"id": 4374, "code": "def yaml get data ( filename ) : with open ( filename , 'rb' ) as fd : yaml data = yaml . load ( fd ) return yaml data return False", "predictions": ["get data from yaml file"], "references": ["get data from . yml file"], "bleu": 0.43989172475842214, "rouge_l": 0.7155425219941348}
{"id": 4375, "code": "def yaml write data ( yaml data , filename ) : with open ( filename , 'w' ) as fd : yaml . dump ( yaml data , fd , default flow style = False ) return True return False", "predictions": ["write data to yaml file"], "references": ["write data into a . yml file"], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 4376, "code": "def get live version ( self ) : try : import versiontools except Import Error : return None else : return str ( versiontools . Version . from expression ( self . name ) )", "predictions": ["return the live version as a string ."], "references": ["get a live version string using versiontools"], "bleu": 0.239802967618271, "rouge_l": 0.4048672566371681}
{"id": 4377, "code": "def is categorical type ( ary ) : ary = np . asanyarray ( ary ) return is integer type ( ary ) or ary . dtype . kind == 'b'", "predictions": ["check if ary is a categorical type ."], "references": ["checks whether the array is either integral or boolean ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 4378, "code": "def build indices ( X , flann args ) : logger . info ( \"Building indices...\" ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = \"index building\" ) ) : indices [ i ] = idx = FLANN Index ( * * flann args ) idx . build index ( bag ) return indices", "predictions": ["build the indices of all indices"], "references": ["builds flann indices for each bag ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4379, "code": "def get rhos ( X , indices , Ks , max K , save all Ks , min dist ) : logger . info ( \"Getting within-bag distances...\" ) if max K >= X . n pts . min ( ) : msg = \"asked for K = {}, but there's a bag with only {} points\" raise Value Error ( msg . format ( max K , X . n pts . min ( ) ) ) which Ks = slice ( 1 , None ) if save all Ks else Ks indices = plog ( indices , name = \"within-bag distances\" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn index ( bag , max K + 1 ) [ 1 ] [ : , which Ks ] ) np . maximum ( min dist , r , out = r ) rhos [ i ] = r return rhos", "predictions": ["get rhos of ks"], "references": ["gets within - bag distances for each bag ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 4380, "code": "def get Ks ( self ) : Ks = as integer type ( self . Ks ) if Ks . ndim != 1 : raise Type Error ( \"Ks should be 1-dim, got shape {}\" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise Value Error ( \"Ks should be positive; got {}\" . format ( Ks . min ( ) ) ) return Ks", "predictions": ["return the ks object for this unit"], "references": ["ks as an array and type - checked ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4381, "code": "def flann args ( self , X = None ) : args = { 'cores' : self . n jobs } if self . flann algorithm == 'auto' : if X is None or X . dim > 5 : args [ 'algorithm' ] = 'linear' else : args [ 'algorithm' ] = 'kdtree single' else : args [ 'algorithm' ] = self . flann algorithm if self . flann args : args . update ( self . flann args ) try : FLANN Parameters ( ) . update ( args ) except Attribute Error as e : msg = \"flann args contains an invalid argument:\\n  {}\" raise Type Error ( msg . format ( e ) ) return args", "predictions": ["provides arguments for the command ."], "references": ["the dictionary of arguments to give to flann ."], "bleu": 0.1593301391270729, "rouge_l": 0.2573839662447257}
{"id": 4382, "code": "def make stacked ( self ) : if self . stacked : return self . boundaries = bounds = np . r [ 0 , np . cumsum ( self . n pts ) ] self . stacked features = stacked = np . vstack ( self . features ) self . features = np . array ( [ stacked [ bounds [ i - 1 ] : bounds [ i ] ] for i in xrange ( 1 , len ( bounds ) ) ] , dtype = object ) self . stacked = True", "predictions": ["make the stacked class"], "references": ["if unstacked convert to stacked . if stacked do nothing ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 4383, "code": "def bare ( self ) : if not self . meta : return self elif self . stacked : return Features ( self . stacked features , self . n pts , copy = False ) else : return Features ( self . features , copy = False )", "predictions": ["bare the meta data"], "references": ["make a features object with no metadata ; points to the same features ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 4384, "code": "def run ( self ) : logger . info ( u'Started listening' ) while not self . stop : xml = self . readxml ( ) if xml is None : break if not self . modelize : logger . info ( u'Raw xml: %s' % xml ) self . results . put ( xml ) continue if xml . tag == 'RECOGOUT' : sentence = Sentence . from shypo ( xml . find ( 'SHYPO' ) , self . encoding ) logger . info ( u'Modelized recognition: %r' % sentence ) self . results . put ( sentence ) else : logger . info ( u'Unmodelized xml: %s' % xml ) self . results . put ( xml ) logger . info ( u'Stopped listening' )", "predictions": ["rc + rc + output"], "references": ["start listening to the server"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4385, "code": "def disconnect ( self ) : logger . info ( u'Disconnecting' ) self . sock . shutdown ( socket . SHUT RDWR ) self . sock . close ( ) self . state = DISCONNECTED", "predictions": [". closes connection to server dst and then closes the connection dst dst dst server dst dst dst dst dst dst dst dst dst and returns a exists"], "references": ["disconnect from the server"], "bleu": 0.0478968583748614, "rouge_l": 0.14454976303317535}
{"id": 4386, "code": "def cli ( id ) : ch = Analyse ( id ) ch . full analysis ( ) click . echo ( 'Created: %s. Modified: %s. Deleted: %s' % ( ch . create , ch . modify , ch . delete ) ) if ch . is suspect : click . echo ( 'The changeset {} is suspect! Reasons: {}' . format ( id , ', ' . join ( ch . suspicion reasons ) ) ) else : click . echo ( 'The changeset %s is not suspect!' % id )", "predictions": ["run a continue result ."], "references": ["analyse an openstreetmap changeset ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 4387, "code": "def filter ( self ) : self . content = [ ch for ch in self . xml . getchildren ( ) if get bounds ( ch ) . intersects ( self . area ) ]", "predictions": ["rc the xml s xml and bounds the xml ."], "references": ["filter the changesets that intersects with the geojson geometry ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 4388, "code": "def label suspicious ( self , reason ) : self . suspicion reasons . append ( reason ) self . is suspect = True", "predictions": ["add a prepare to the suspicious"], "references": ["add suspicion reason and set the suspicious flag ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 4389, "code": "def full analysis ( self ) : self . count ( ) self . verify words ( ) self . verify user ( ) if self . review requested == 'yes' : self . label suspicious ( 'Review requested' )", "predictions": ["run the get queryset . ."], "references": ["execute the count and verify_words methods ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 4390, "code": "def verify editor ( self ) : powerful editors = [ 'josm' , 'level0' , 'merkaartor' , 'qgis' , 'arcgis' , 'upload.py' , 'osmapi' , 'Services Open Street Map' ] if self . editor is not None : for editor in powerful editors : if editor in self . editor . lower ( ) : self . powerfull editor = True break if 'i D' in self . editor : trusted hosts = [ 'www.openstreetmap.org/id' , 'www.openstreetmap.org/edit' , 'improveosm.org' , 'strava.github.io/i D' , 'preview.ideditor.com/release' , 'preview.ideditor.com/master' , 'hey.mapbox.com/i D-internal' , 'projets.pavie.info/id-indoor' , 'maps.mapcat.com/edit' , 'id.softek.ir' ] if self . host . split ( '://' ) [ - 1 ] . strip ( '/' ) not in trusted hosts : self . label suspicious ( 'Unknown i D instance' ) else : self . powerfull editor = True self . label suspicious ( 'Software editor was not declared' )", "predictions": ["save model model model model model model model"], "references": ["verify if the software used in the changeset is a powerfull_editor ."], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 4391, "code": "def spawn ( self , generations ) : egg donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XX' ] sperm donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XY' ] for i in range ( generations ) : print ( \"\\n GENERATION %d\\n\" % ( i + 1 ) ) gen xx = [ ] gen xy = [ ] for egg donor in egg donors : sperm donor = random . choice ( sperm donors ) brood = self . breed ( egg donor , sperm donor ) for child in brood : if child . divinity > human : self . add god ( child ) if child . chromosomes == 'XX' : gen xx . append ( child ) else : gen xy . append ( child ) egg donors = [ ed for ed in egg donors if ed . generation > ( i - 2 ) ] sperm donors = [ sd for sd in sperm donors if sd . generation > ( i - 3 ) ] egg donors += gen xx sperm donors += gen xy", "predictions": ["save a set of temperature temperature"], "references": ["grow this pantheon by multiplying gods ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 4392, "code": "def breed ( self , egg donor , sperm donor ) : offspring = [ ] try : num children = npchoice ( [ 1 , 2 ] , 1 , p = [ 0.8 , 0.2 ] ) [ 0 ] for in range ( num children ) : child = God ( egg donor , sperm donor ) offspring . append ( child ) send birth announcement ( egg donor , sperm donor , child ) except Value Error : print ( \"Breeding error occurred. Likely the generator ran out of names.\" ) return offspring", "predictions": ["clear the announcement and put the result in the generator"], "references": ["get it on ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4393, "code": "def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0", "predictions": [". condense onto vec1"], "references": ["compare vectors . borrowed from a . parish ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4394, "code": "def set name lists ( ethnicity = None ) : if not ethnicity : ethnicity = random . choice ( get ethnicities ( ) ) print ( \"Loading names from: \" + ethnicity ) filename = names dir + ethnicity + '.json' try : with open ( filename , 'r' ) as injson : data = json . load ( injson ) except : return 'Unable to read from file: ' + filename else : names = [ tuple ( name . split ( ',' ) ) for name in data ] random . shuffle ( names ) global female names female names = [ name for name , gender , * desc in names if gender == 'girl' ] global male names male names = [ name for name , gender , * desc in names if gender == 'boy' ] global nb names nb names = [ name for name , gender , * desc in names if gender == 'boygirl' ]", "predictions": ["print the read read read from the dictionary not used by the read not in the current read"], "references": ["set three globally available lists of names ."], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 4395, "code": "def set inherited traits ( self , egg donor , sperm donor ) : if type ( egg donor ) == str : self . reproduce asexually ( egg donor , sperm donor ) else : self . reproduce sexually ( egg donor , sperm donor )", "predictions": ["sets the str of the str ."], "references": ["accept either strings or gods as inputs ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4396, "code": "def print parents ( self ) : if self . gender == female : title = 'Daughter' elif self . gender == male : title = 'Son' else : title = 'Child' p1 = self . parents [ 0 ] p2 = self . parents [ 1 ] template = '%s of %s, the %s, and %s, the %s.' print ( template % ( title , p1 . name , p1 . epithet , p2 . name , p2 . epithet ) )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["print parents names and epithets ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4397, "code": "def format seconds ( self , n seconds ) : func = self . ok if n seconds >= 60 : n minutes , n seconds = divmod ( n seconds , 60 ) return \"%s minutes %s seconds\" % ( func ( \"%d\" % n minutes ) , func ( \"%.3f\" % n seconds ) ) else : return \"%s seconds\" % ( func ( \"%.3f\" % n seconds ) )", "predictions": ["set the function to a string in script range range ."], "references": ["format a time in seconds ."], "bleu": 0.1354599427337814, "rouge_l": 0.3727087576374745}
{"id": 4398, "code": "def ppdict ( dict to print , br = '\\n' , html = False , key align = 'l' , sort keys = True , key preffix = '' , key suffix = '' , value prefix = '' , value suffix = '' , left margin = 3 , indent = 2 ) : if dict to print : if sort keys : dic = dict to print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict to print = Ordered Dict ( ) for k in keys : dict to print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and \"'%s'\" % x or x for x in dict to print . keys ( ) ] vs = [ type ( x ) == str and \"'%s'\" % x or x for x in dict to print . values ( ) ] max key len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max key len ) , key align == 'r' : str ( ks [ i ] ) . rjust ( max key len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key preffix , k , key suffix , value prefix , v , value suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] tmp . append ( '}' ) if left margin : tmp = [ ' ' * left margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'", "predictions": ["pretty - print a dictionary of strings"], "references": ["indent representation of a dict"], "bleu": 0.20556680845025982, "rouge_l": 0.17183098591549298}
{"id": 4399, "code": "def eq ( result , expected , msg = None ) : params = { 'expected' : expected , 'result' : result } aka = % params default msg = % params if ( ( repr ( result ) != six . text type ( result ) ) or ( repr ( expected ) != six . text type ( expected ) ) ) : default msg += aka assertion msg = msg or default msg if isinstance ( assertion msg , six . text type ) : assertion msg = assertion msg . encode ( 'utf-8' ) assert result == expected , assertion msg", "predictions": ["puts messages with self to self type"], "references": ["shadow of the nose builtin which presents easier to read multiline output ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 4400, "code": "def dem procesa datos dia ( key day , response ) : dfs import , df import , dfs maxmin , hay errores = [ ] , None , [ ] , 0 for r in response : tipo datos , data = extract func json data ( r ) if tipo datos is not None : if ( 'IND Max Min' in tipo datos ) and data : df import = import daily max min ( data ) dfs maxmin . append ( df import ) elif data : df import = import json ts data ( data ) dfs import . append ( df import ) if tipo datos is None or df import is None : hay errores += 1 if hay errores == 4 : print redb ( '** No hay datos para el d\u00eda {}!'. f ormat( k ey day) ) return None , - 2 else : data import = { } if dfs import : data import [ KEYS DATA DEM [ 0 ] ] = dfs import [ 0 ] . join ( dfs import [ 1 ] ) if len ( dfs maxmin ) == 2 : data import [ KEYS DATA DEM [ 1 ] ] = dfs maxmin [ 0 ] . join ( dfs maxmin [ 1 ] ) elif dfs maxmin : data import [ KEYS DATA DEM [ 1 ] ] = dfs maxmin [ 0 ] if not data import : print err ( f ormat( k ey day,   ay errores) ) return None , - 2 return data import , 0", "predictions": ["return function for the get datos datos"], "references": ["procesa los datos descargados en json ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4401, "code": "def compress ( self , input str ) : compressed bits = c String IO . String IO ( ) f = gzip . Gzip File ( fileobj = compressed bits , mode = 'wb' ) f . write ( input str ) f . close ( ) return compressed bits . getvalue ( )", "predictions": ["generate a string for the given self format format format format format format format format format format format format format format format format format format format format format format format format"], "references": ["compress the log message in order to send less bytes to the wire ."], "bleu": 0.03901663112717908, "rouge_l": 0.04769351055512119}
{"id": 4402, "code": "def register Good Class ( self , class ) : self . valid classes . append ( class ) for name , cls in class members ( class ) : if self . is Valid Class ( cls ) : self . register Good Class ( cls )", "predictions": ["get a subclass of data that can be added to the name of the name of the name of the given name = name = name = name = name of"], "references": ["internal bookkeeping to handle nested classes"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 4403, "code": "def get resample data ( self ) : if self . data is not None : if self . pvpc mean daily is None : self . pvpc mean daily = self . data [ 'data' ] . resample ( 'D' ) . mean ( ) if self . pvpc mean monthly is None : self . pvpc mean monthly = self . data [ 'data' ] . resample ( 'MS' ) . mean ( ) return self . pvpc mean daily , self . pvpc mean monthly", "predictions": ["retrieve the data for the data model ext ext ext"], "references": ["obtiene los dataframes de los datos de pvpc con resampling diario y mensual ."], "bleu": 0.07105602552547084, "rouge_l": 0.0}
{"id": 4404, "code": "def move dot ( self ) : return self . class ( self . production , self . pos + 1 , self . lookahead )", "predictions": ["json - compatible json to the last frame . . . . . . . . . . . . . . . . . ."], "references": ["returns the dottedrule that results from moving the dot ."], "bleu": 0.051660454541342535, "rouge_l": 0.12079207920792079}
{"id": 4405, "code": "def first ( self , symbols ) : ret = set ( ) if EPSILON in symbols : return set ( [ EPSILON ] ) for symbol in symbols : ret |= self . first [ symbol ] - set ( [ EPSILON ] ) if EPSILON not in self . first [ symbol ] : break else : ret . add ( EPSILON ) return ret", "predictions": ["returns the json representation of the given filename ."], "references": ["computes the intermediate first set using symbols ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4406, "code": "def initial closure ( self ) : first rule = Dotted Rule ( self . start , 0 , END OF INPUT ) return self . closure ( [ first rule ] )", "predictions": ["the yaml get get get the yaml get get get get the yaml get get ."], "references": ["computes the initial closure using the start_foo production ."], "bleu": 0.09147827112247602, "rouge_l": 0.2527624309392265}
{"id": 4407, "code": "def parse definite clause ( s ) : assert is definite clause ( s ) if is symbol ( s . op ) : return [ ] , s else : antecedent , consequent = s . args return conjuncts ( antecedent ) , consequent", "predictions": ["yaml helper to yaml"], "references": ["return the antecedents and the consequent of a definite clause ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 4408, "code": "def tt check all ( kb , alpha , symbols , model ) : if not symbols : if pl true ( kb , model ) : result = pl true ( alpha , model ) assert result in ( True , False ) return result else : return True else : P , rest = symbols [ 0 ] , symbols [ 1 : ] return ( tt check all ( kb , alpha , rest , extend ( model , P , True ) ) and tt check all ( kb , alpha , rest , extend ( model , P , False ) ) )", "predictions": ["live version of get ."], "references": ["auxiliary routine to implement tt_entails ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4409, "code": "def prop symbols ( x ) : if not isinstance ( x , Expr ) : return [ ] elif is prop symbol ( x . op ) : return [ x ] else : return list ( set ( symbol for arg in x . args for symbol in prop symbols ( arg ) ) )", "predictions": ["return a list of the default values of a function ."], "references": ["return a list of all propositional symbols in x ."], "bleu": 0.33180774028439425, "rouge_l": 0.4803149606299213}
{"id": 4410, "code": "def dpll ( clauses , symbols , model ) : unknown clauses = [ ] for c in clauses : val = pl true ( c , model ) if val == False : return False if val != True : unknown clauses . append ( c ) if not unknown clauses : return model P , value = find pure symbol ( symbols , unknown clauses ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , value = find unit clause ( clauses , model ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , symbols = symbols [ 0 ] , symbols [ 1 : ] return ( dpll ( clauses , symbols , extend ( model , P , True ) ) or dpll ( clauses , symbols , extend ( model , P , False ) ) )", "predictions": ["find the build symbol for a given flann"], "references": ["see if the clauses are true in a partial model ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 4411, "code": "def is variable ( x ) : return isinstance ( x , Expr ) and not x . args and is var symbol ( x . op )", "predictions": ["returns true if x is a variable variable"], "references": ["a variable is an expr with no args and a lowercase symbol as the op ."], "bleu": 0.0834319834185865, "rouge_l": 0.15721649484536082}
{"id": 4412, "code": "def retract ( self , sentence ) : for c in conjuncts ( to cnf ( sentence ) ) : if c in self . clauses : self . clauses . remove ( c )", "predictions": ["remove a sentence from the list"], "references": ["remove the sentence s clauses from the kb ."], "bleu": 0.20034704329441452, "rouge_l": 0.5147679324894514}
{"id": 4413, "code": "def refresh ( self ) : args = [ ( obj . name , obj . value ) for obj in self . queryset . all ( ) ] super ( Setting Dict , self ) . update ( args ) self . empty cache = False", "predictions": ["reload all of the cached objects"], "references": ["updates the cache with setting values from the database ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 4414, "code": "def utility ( self , state , player ) : return if ( player == 'X' , state . utility , - state . utility )", "predictions": ["return a new comparison with a given player np np np np np np np np np np ."], "references": ["return the value to player ; 1 for win - 1 for loss 0 otherwise ."], "bleu": 0.07658412276041004, "rouge_l": 0.1741198858230257}
{"id": 4415, "code": "def compute utility ( self , board , move , player ) : if ( self . k in row ( board , move , player , ( 0 , 1 ) ) or self . k in row ( board , move , player , ( 1 , 0 ) ) or self . k in row ( board , move , player , ( 1 , - 1 ) ) or self . k in row ( board , move , player , ( 1 , 1 ) ) ) : return if ( player == 'X' , + 1 , - 1 ) else : return 0", "predictions": ["bare the k to a map"], "references": ["if x wins with this move return 1 ; if o return - 1 ; else return 0 ."], "bleu": 0.021279371714778714, "rouge_l": 0.0}
{"id": 4416, "code": "def k in row ( self , board , move , player , ( delta x , delta y ) ) : x , y = move n = 0 while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta x , y + delta y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta x , y - delta y n -= 1 return n >= self . k", "predictions": ["given a row of a row return the k k in the row"], "references": ["return true if there is a line through move on board for player ."], "bleu": 0.0978840017329239, "rouge_l": 0.0735826296743064}
{"id": 4417, "code": "def weighted sampler ( seq , weights ) : totals = [ ] for w in weights : totals . append ( w + totals [ - 1 ] if totals else w ) return lambda : seq [ bisect . bisect ( totals , random . uniform ( 0 , totals [ - 1 ] ) ) ]", "predictions": ["return the sampler that matches the given list of weights ."], "references": ["return a random - sample function that picks from seq weighted by weights ."], "bleu": 0.1296737111713457, "rouge_l": 0.31322207958921694}
{"id": 4418, "code": "def name ( object ) : return ( getattr ( object , 'name' , 0 ) or getattr ( object , ' name ' , 0 ) or getattr ( getattr ( object , ' class ' , 0 ) , ' name ' , 0 ) or str ( object ) )", "predictions": ["return the name of an object"], "references": ["try to find some reasonable name for the object ."], "bleu": 0.13487005099534619, "rouge_l": 0.23921568627450981}
{"id": 4419, "code": "def AIMA File ( components , mode = 'r' ) : import utils dir = os . path . dirname ( utils . file ) return open ( apply ( os . path . join , [ dir ] + components ) , mode )", "predictions": ["opens a file on the given components"], "references": ["open a file based at the aima root directory ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 4420, "code": "def information content ( values ) : probabilities = normalize ( removeall ( 0 , values ) ) return sum ( - p * log2 ( p ) for p in probabilities )", "predictions": ["convert information from a list of values to a list of probabilities ."], "references": ["number of bits to represent the probability distribution in values ."], "bleu": 0.12011055432195765, "rouge_l": 0.2538141470180305}
{"id": 4421, "code": "def Neural Net Learner ( dataset , sizes ) : activations = map ( lambda n : [ 0.0 for i in range ( n ) ] , sizes ) weights = [ ] def predict ( example ) : unimplemented ( ) return predict", "predictions": ["predict the list of sizes for each dataset ."], "references": ["layered feed - forward network ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4422, "code": "def Ensemble Learner ( learners ) : def train ( dataset ) : predictors = [ learner ( dataset ) for learner in learners ] def predict ( example ) : return mode ( predictor ( example ) for predictor in predictors ) return predict return train", "predictions": ["predict the input dataset ."], "references": ["given a list of learning algorithms have them vote ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4423, "code": "def Weighted Majority ( predictors , weights ) : def predict ( example ) : return weighted mode ( ( predictor ( example ) for predictor in predictors ) , weights ) return predict", "predictions": ["return a list of weighted predictors for each predictors in the predictors list ."], "references": ["return a predictor that takes a weighted vote ."], "bleu": 0.13217947626377288, "rouge_l": 0.3620178041543027}
{"id": 4424, "code": "def replicated dataset ( dataset , weights , n = None ) : n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted replicate ( dataset . examples , weights , n ) return result", "predictions": ["create a dataset with the given weights ."], "references": ["copy dataset replicating each example in proportion to its weight ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 4425, "code": "def leave1out ( learner , dataset ) : return cross validation ( learner , dataset , k = len ( dataset . examples ) )", "predictions": ["cross - validation for a particular learner ."], "references": ["leave one out cross - validation over the dataset ."], "bleu": 0.24578832304224082, "rouge_l": 0.43571428571428567}
{"id": 4426, "code": "def Synthetic Restaurant ( n = 20 ) : def gen ( ) : example = map ( random . choice , restaurant . values ) example [ restaurant . target ] = Fig [ 18 , 2 ] ( example ) return example return Restaurant Data Set ( [ gen ( ) for i in range ( n ) ] )", "predictions": ["generate random number of random random restaurant"], "references": ["generate a dataset with n examples ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4427, "code": "def check me ( self ) : assert len ( self . attrnames ) == len ( self . attrs ) assert self . target in self . attrs assert self . target not in self . inputs assert set ( self . inputs ) . issubset ( set ( self . attrs ) ) map ( self . check example , self . examples )", "predictions": ["check that all attributes are defined as the same"], "references": ["check that my fields make sense ."], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 4428, "code": "def add example ( self , example ) : self . check example ( example ) self . examples . append ( example )", "predictions": ["appends an example to the batch"], "references": ["add an example to the list of examples checking it first ."], "bleu": 0.22616792214653433, "rouge_l": 0.41924398625429554}
{"id": 4429, "code": "def check example ( self , example ) : if self . values : for a in self . attrs : if example [ a ] not in self . values [ a ] : raise Value Error ( 'Bad value %s for attribute %s in %s' % ( example [ a ] , self . attrnames [ a ] , example ) )", "predictions": ["check that the example s example attributes are present"], "references": ["raise valueerror if example has any invalid values ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4430, "code": "def attrnum ( self , attr ) : if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr", "predictions": ["returns the attribute of an attribute ."], "references": ["returns the number used for attr which can be a name or - n .. n - 1 ."], "bleu": 0.047308627669118, "rouge_l": 0.21303841676367868}
{"id": 4431, "code": "def sanitize ( self , example ) : return [ attr i if i in self . inputs else None for i , attr i in enumerate ( example ) ]", "predictions": ["replace characters with a string ."], "references": ["return a copy of example with non - input attributes replaced by none ."], "bleu": 0.06924459302580939, "rouge_l": 0.18654434250764526}
{"id": 4432, "code": "def add ( self , o ) : self . smooth for ( o ) self . dictionary [ o ] += 1 self . n obs += 1 self . sampler = None", "predictions": ["add a new smooth to the current dictionary"], "references": ["add an observation o to the distribution ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 4433, "code": "def sample ( self ) : if self . sampler is None : self . sampler = weighted sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )", "predictions": ["sample the weighted sampler ."], "references": ["return a random sample from the distribution ."], "bleu": 0.1781815298791261, "rouge_l": 0.44309927360774815}
{"id": 4434, "code": "def revise ( csp , Xi , Xj , removals ) : revised = False for x in csp . curr domains [ Xi ] [ : ] : if every ( lambda y : not csp . constraints ( Xi , x , Xj , y ) , csp . curr domains [ Xj ] ) : csp . prune ( Xi , x , removals ) revised = True return revised", "predictions": ["prune constraints on csp"], "references": ["return true if we remove a value ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 4435, "code": "def mrv ( assignment , csp ) : return argmin random tie ( [ v for v in csp . vars if v not in assignment ] , lambda var : num legal values ( csp , var , assignment ) )", "predictions": ["get random tie value"], "references": ["minimum - remaining - values heuristic ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 4436, "code": "def lcv ( var , assignment , csp ) : return sorted ( csp . choices ( var ) , key = lambda val : csp . nconflicts ( var , val , assignment ) )", "predictions": ["get the csp choices"], "references": ["least - constraining - values heuristic ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 4437, "code": "def forward checking ( csp , var , value , assignment , removals ) : for B in csp . neighbors [ var ] : if B not in assignment : for b in csp . curr domains [ B ] [ : ] : if not csp . constraints ( var , value , B , b ) : csp . prune ( B , b , removals ) if not csp . curr domains [ B ] : return False return True", "predictions": ["checking if csp constraints are constraints"], "references": ["prune neighbor values inconsistent with var = value ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 4438, "code": "def mac ( csp , var , value , assignment , removals ) : return AC3 ( csp , [ ( X , var ) for X in csp . neighbors [ var ] ] , removals )", "predictions": ["mac csp to mac"], "references": ["maintain arc consistency ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4439, "code": "def min conflicts ( csp , max steps = 100000 ) : csp . current = current = { } for var in csp . vars : val = min conflicts value ( csp , var , current ) csp . assign ( var , val , current ) for i in range ( max steps ) : conflicted = csp . conflicted vars ( current ) if not conflicted : return current var = random . choice ( conflicted ) val = min conflicts value ( csp , var , current ) csp . assign ( var , val , current ) return None", "predictions": ["get min conflicts between csp and csp"], "references": ["solve a csp by stochastic hillclimbing on the number of conflicts ."], "bleu": 0.10063351655856649, "rouge_l": 0.10049423393739704}
{"id": 4440, "code": "def Zebra ( ) : Colors = 'Red Yellow Blue Green Ivory' . split ( ) Pets = 'Dog Fox Snails Horse Zebra' . split ( ) Drinks = 'OJ Tea Coffee Milk Water' . split ( ) Countries = 'Englishman Spaniard Norwegian Ukranian Japanese' . split ( ) Smokes = 'Kools Chesterfields Winston Lucky Strike Parliaments' . split ( ) vars = Colors + Pets + Drinks + Countries + Smokes domains = { } for var in vars : domains [ var ] = range ( 1 , 6 ) domains [ 'Norwegian' ] = [ 1 ] domains [ 'Milk' ] = [ 3 ] neighbors = parse neighbors ( , vars ) for type in [ Colors , Pets , Drinks , Countries , Smokes ] : for A in type : for B in type : if A != B : if B not in neighbors [ A ] : neighbors [ A ] . append ( B ) if A not in neighbors [ B ] : neighbors [ B ] . append ( A ) def zebra constraint ( A , a , B , b , recurse = 0 ) : same = ( a == b ) next to = abs ( a - b ) == 1 if A == 'Englishman' and B == 'Red' : return same if A == 'Spaniard' and B == 'Dog' : return same if A == 'Chesterfields' and B == 'Fox' : return next to if A == 'Norwegian' and B == 'Blue' : return next to if A == 'Kools' and B == 'Yellow' : return same if A == 'Winston' and B == 'Snails' : return same if A == 'Lucky Strike' and B == 'OJ' : return same if A == 'Ukranian' and B == 'Tea' : return same if A == 'Japanese' and B == 'Parliaments' : return same if A == 'Kools' and B == 'Horse' : return next to if A == 'Coffee' and B == 'Green' : return same if A == 'Green' and B == 'Ivory' : return ( a - 1 ) == b if recurse == 0 : return zebra constraint ( B , b , A , a , 1 ) if ( ( A in Colors and B in Colors ) or ( A in Pets and B in Pets ) or ( A in Drinks and B in Drinks ) or ( A in Countries and B in Countries ) or ( A in Smokes and B in Smokes ) ) : return not same raise 'error' return CSP ( vars , domains , neighbors , zebra constraint )", "predictions": ["determine if the two neighbors are the same as the same as the same type"], "references": ["return an instance of the zebra puzzle ."], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 4441, "code": "def nconflicts ( self , var , val , assignment ) : def conflict ( var2 ) : return ( var2 in assignment and not self . constraints ( var , val , var2 , assignment [ var2 ] ) ) return count if ( conflict , self . neighbors [ var ] )", "predictions": ["nconflicts constraints for a given assignment"], "references": ["return the number of conflicts var = val has with other variables ."], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 4442, "code": "def suppose ( self , var , value ) : self . support pruning ( ) removals = [ ( var , a ) for a in self . curr domains [ var ] if a != value ] self . curr domains [ var ] = [ value ] return removals", "predictions": ["transform a variable to a new variable ."], "references": ["start accumulating inferences from assuming var = value ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4443, "code": "def prune ( self , var , value , removals ) : self . curr domains [ var ] . remove ( value ) if removals is not None : removals . append ( ( var , value ) )", "predictions": ["prune a variable ."], "references": ["rule out var = value ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4444, "code": "def infer assignment ( self ) : self . support pruning ( ) return dict ( ( v , self . curr domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr domains [ v ] ) )", "predictions": ["infer an assignment that is internal use for inference ."], "references": ["return the partial assignment implied by the current inferences ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 4445, "code": "def restore ( self , removals ) : for B , b in removals : self . curr domains [ B ] . append ( b )", "predictions": ["restore this instance s attributes ."], "references": ["undo a supposition and all inferences from it ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4446, "code": "def conflicted vars ( self , current ) : return [ var for var in self . vars if self . nconflicts ( var , current [ var ] , current ) > 0 ]", "predictions": ["return all conflicted of a given current current vars ."], "references": ["return a list of variables in current assignment that are in conflict"], "bleu": 0.12977836824680314, "rouge_l": 0.2683284457478006}
{"id": 4447, "code": "def assign ( self , var , val , assignment ) : oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : self . record conflict ( assignment , var , oldval , - 1 ) self . record conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )", "predictions": ["assign a record to a given assignment ."], "references": ["assign var and keep track of conflicts ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4448, "code": "def record conflict ( self , assignment , var , val , delta ) : n = len ( self . vars ) self . rows [ val ] += delta self . downs [ var + val ] += delta self . ups [ var - val + n - 1 ] += delta", "predictions": [". a in a in a in - place"], "references": ["record conflicts caused by addition or deletion of a queen ."], "bleu": 0.12507277759788113, "rouge_l": 0.09822866344605477}
{"id": 4449, "code": "def encode ( plaintext , code ) : from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )", "predictions": ["weighted - weighted plaintext"], "references": ["encodes text using a code which is a permutation of the alphabet ."], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 4450, "code": "def index collection ( self , filenames ) : for filename in filenames : self . index document ( open ( filename ) . read ( ) , filename )", "predictions": ["read a collection file ."], "references": ["index a whole collection of files ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 4451, "code": "def index document ( self , text , url ) : title = text [ : text . index ( '\\n' ) ] . strip ( ) docwords = words ( text ) docid = len ( self . documents ) self . documents . append ( Document ( title , url , len ( docwords ) ) ) for word in docwords : if word not in self . stopwords : self . index [ word ] [ docid ] += 1", "predictions": ["parse a document document document and add it to the mode dir dir dir dir"], "references": ["index the text of a document ."], "bleu": 0.11633270842295028, "rouge_l": 0.1945773524720893}
{"id": 4452, "code": "def score ( self , word , docid ) : return ( math . log ( 1 + self . index [ word ] [ docid ] ) / math . log ( 1 + self . documents [ docid ] . nwords ) )", "predictions": ["computes the information of a word 0 0 0 0 0 0 0 0 0 0 0"], "references": ["compute a score for this word on this docid ."], "bleu": 0.07994607499472013, "rouge_l": 0.15541401273885352}
{"id": 4453, "code": "def present ( self , results ) : for ( score , d ) in results : doc = self . documents [ d ] print ( \"%5.2f|%25s | %s\" % ( 100 * score , doc . url , doc . title [ : 45 ] . expandtabs ( ) ) )", "predictions": ["callback to list dataset"], "references": ["present the results as a list ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4454, "code": "def present results ( self , query text , n = 10 ) : self . present ( self . query ( query text , n ) )", "predictions": ["runs present query query ."], "references": ["get results for the query and present them ."], "bleu": 0.1458826981425239, "rouge_l": 0.2717149220489978}
{"id": 4455, "code": "def score ( self , plaintext ) : s = 1.0 for bi in bigrams ( plaintext ) : s = s * self . P2 [ bi ] return s", "predictions": ["score the plaintext in a plaintext"], "references": ["return a score for text based on how common letters pairs are ."], "bleu": 0.07612610271614867, "rouge_l": 0.09870550161812298}
{"id": 4456, "code": "def decode ( self , ciphertext ) : self . ciphertext = ciphertext problem = Permutation Decoder Problem ( decoder = self ) return search . best first tree search ( problem , lambda node : self . score ( node . state ) )", "predictions": ["result of a weights . ."], "references": ["search for a decoding of the ciphertext ."], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 4457, "code": "def get value ( self , context , default ) : if default is None : settings = self . setting model . objects . as dict ( ) else : settings = self . setting model . objects . as dict ( default = default ) return settings", "predictions": ["get value value value"], "references": ["returns a settingdict object ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 4458, "code": "def get value ( self , context , name , default ) : settings = self . setting model . objects . filter ( name = name ) if default is None : settings = settings . as dict ( ) else : settings = settings . as dict ( default = default ) value = settings [ name ] return value", "predictions": ["return can be a string or a 18"], "references": ["returns the value of the named setting ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4459, "code": "def render tag ( self , context , name , nodelist ) : settings = self . setting model . objects . filter ( name = name ) . as dict ( ) try : value = settings [ name ] except Key Error : value = settings [ name ] = nodelist . render ( context ) return value", "predictions": ["check the me me me to the client"], "references": ["returns the value of the named setting ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4460, "code": "def expected utility ( a , s , U , mdp ) : return sum ( [ p * U [ s1 ] for ( p , s1 ) in mdp . T ( s , a ) ] )", "predictions": ["calculate the sum of a list of add self . example"], "references": ["the expected utility of doing a in state s according to the mdp and u ."], "bleu": 0.09091421815660788, "rouge_l": 0.28672150411280845}
{"id": 4461, "code": "def go ( self , state , direction ) : state1 = vector add ( state , direction ) return if ( state1 in self . states , state1 , state )", "predictions": ["check if the state is in the state ."], "references": ["return the state that results from going in this direction ."], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 4462, "code": "def as dict ( self , default = None ) : settings = Setting Dict ( queryset = self , default = default ) return settings", "predictions": ["transform object as a dict object to a dict instance ."], "references": ["returns a settingdict object for this queryset ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 4463, "code": "def create ( self , name , value ) : if value is None : raise Value Error ( 'Setting value cannot be `None`.' ) model = Setting . get model for value ( value ) obj = super ( Setting Query Set , model . objects . all ( ) ) . create ( name = name , value = value ) return obj", "predictions": ["sanitize a enumerate enumerate"], "references": ["creates and returns an object of the appropriate type for value ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 4464, "code": "def exp schedule ( k = 20 , lam = 0.005 , limit = 100 ) : return lambda t : if ( t < limit , k * math . exp ( - lam * t ) , 0 )", "predictions": ["schedule function for schedule ."], "references": ["one possible schedule function for simulated annealing"], "bleu": 0.3406115671352173, "rouge_l": 0.48541114058355433}
{"id": 4465, "code": "def print boggle ( board ) : n2 = len ( board ) n = exact sqrt ( n2 ) for i in range ( n2 ) : if i % n == 0 and i > 0 : print if board [ i ] == 'Q' : print 'Qu' , else : print str ( board [ i ] ) + ' ' , print", "predictions": ["sample from a board board"], "references": ["print the board in a 2 - d array ."], "bleu": 0.11115018927487523, "rouge_l": 0.12577319587628866}
{"id": 4466, "code": "def exact sqrt ( n2 ) : n = int ( math . sqrt ( n2 ) ) assert n * n == n2 return n", "predictions": ["returns the point vector that is a function of n2 = 0 = 1"], "references": ["if n2 is a perfect square return its square root else raise error ."], "bleu": 0.1250076305588977, "rouge_l": 0.14285714285714285}
{"id": 4467, "code": "def expand ( self , problem ) : return [ self . child node ( problem , action ) for action in problem . actions ( self . state ) ]", "predictions": ["return a list of nodes of a given csp random csp random csp ."], "references": ["list the nodes reachable in one step from this node ."], "bleu": 0.10511846841633776, "rouge_l": 0.24530831099195713}
{"id": 4468, "code": "def child node ( self , problem , action ) : next = problem . result ( self . state , action ) return Node ( next , self , action , problem . path cost ( self . path cost , self . state , action , next ) )", "predictions": ["take a node and lambda it to the tree"], "references": ["fig . 3 . 10"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4469, "code": "def path ( self ) : node , path back = self , [ ] while node : path back . append ( node ) node = node . parent return list ( reversed ( path back ) )", "predictions": ["returns a forward forward forward forward from the var"], "references": ["return a list of nodes forming the path from the root to this node ."], "bleu": 0.10247907767191411, "rouge_l": 0.23921568627450981}
{"id": 4470, "code": "def mate ( self , other ) : c = random . randrange ( len ( self . genes ) ) return self . class ( self . genes [ : c ] + other . genes [ c : ] )", "predictions": ["for a given statement for the given var return a random return a random instance"], "references": ["return a new individual crossing self and other ."], "bleu": 0.1082597837309053, "rouge_l": 0.17453505007153075}
{"id": 4471, "code": "def make undirected ( self ) : for a in self . dict . keys ( ) : for ( b , distance ) in self . dict [ a ] . items ( ) : self . connect1 ( b , a , distance )", "predictions": ["min the query query"], "references": ["make a digraph into an undirected graph by adding symmetric edges ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 4472, "code": "def connect1 ( self , A , B , distance ) : self . dict . setdefault ( A , { } ) [ B ] = distance", "predictions": ["set a custom custom custom custom distance parameter to a distance"], "references": ["add a link from a to b of given distance in one direction only ."], "bleu": 0.09956647337521526, "rouge_l": 0.22453987730061348}
{"id": 4473, "code": "def h ( self , node ) : locs = getattr ( self . graph , 'locations' , None ) if locs : return int ( distance ( locs [ node . state ] , locs [ self . goal ] ) ) else : return infinity", "predictions": [". distance is a distance"], "references": ["h function is straight - line distance from a node s state to goal ."], "bleu": 0.0464598616370231, "rouge_l": 0.18345864661654135}
{"id": 4474, "code": "def actions ( self , state ) : if state [ - 1 ] is not None : return [ ] else : col = state . index ( None ) return [ row for row in range ( self . N ) if not self . conflicted ( state , row , col ) ]", "predictions": ["returns a list of actions with specified var ."], "references": ["in the leftmost empty column try all non - conflicting rows ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 4475, "code": "def result ( self , state , row ) : col = state . index ( None ) new = state [ : ] new [ col ] = row return new", "predictions": ["create a prune prune from a var var at the given var ."], "references": ["place the next queen at the given row ."], "bleu": 0.18798317647335086, "rouge_l": 0.37596302003081655}
{"id": 4476, "code": "def set board ( self , board = None ) : if board is None : board = random boggle ( ) self . board = board self . neighbors = boggle neighbors ( len ( board ) ) self . found = { } for i in range ( len ( board ) ) : lo , hi = self . wordlist . bounds [ board [ i ] ] self . find ( lo , hi , i , [ ] , '' ) return self", "predictions": ["infer a assignment from the given assignment"], "references": ["set the board and find all the words in it ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 4477, "code": "def score ( self ) : return sum ( [ self . scores [ len ( w ) ] for w in self . words ( ) ] )", "predictions": ["the number of words words . . . . . . . . . ."], "references": ["the total score for the words found according to the rules ."], "bleu": 0.09782375748961449, "rouge_l": 0.22676579925650556}
{"id": 4478, "code": "def Model Based Vacuum Agent ( ) : model = { loc A : None , loc B : None } def program ( ( location , status ) ) : \"Same as Reflex Vacuum Agent, except if everything is clean, do No Op.\" model [ location ] = status if model [ loc A ] == model [ loc B ] == 'Clean' : return 'No Op' elif status == 'Dirty' : return 'Suck' elif location == loc A : return 'Right' elif location == loc B : return 'Left' return Agent ( program )", "predictions": ["return version of vars ."], "references": ["an agent that keeps track of what locations are clean or dirty ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 4479, "code": "def run ( self , steps = 1000 ) : for step in range ( steps ) : if self . is done ( ) : return self . step ( )", "predictions": ["assign the given var to the given var if any ."], "references": ["run the environment for given number of time steps ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 4480, "code": "def list things at ( self , location , tclass = Thing ) : return [ thing for thing in self . things if thing . location == location and isinstance ( thing , tclass ) ]", "predictions": ["return a list of things at the specified location ."], "references": ["return all things exactly at a given location ."], "bleu": 0.20504572236241866, "rouge_l": 0.5313588850174217}
{"id": 4481, "code": "def delete thing ( self , thing ) : try : self . things . remove ( thing ) except Value Error , e : print e print \"  in Environment delete thing\" print \"  Thing to be removed: %s at %s\" % ( thing , thing . location ) print \"  from list: %s\" % [ ( thing , thing . location ) for thing in self . things ] if thing in self . agents : self . agents . remove ( thing )", "predictions": ["delete a thing from a thing"], "references": ["remove a thing from the environment ."], "bleu": 0.34801709319446883, "rouge_l": 0.45522388059701485}
{"id": 4482, "code": "def things near ( self , location , radius = None ) : if radius is None : radius = self . perceptible distance radius2 = radius * radius return [ thing for thing in self . things if distance2 ( location , thing . location ) <= radius2 ]", "predictions": ["a list of things that are things for a given location ."], "references": ["return all things within radius of location ."], "bleu": 0.15537125692760353, "rouge_l": 0.3112244897959184}
{"id": 4483, "code": "def percept ( self , agent ) : return [ self . thing percept ( thing , agent ) for thing in self . things near ( agent . location ) ]", "predictions": ["splits an agent into a list of things that are used to be displayed ."], "references": ["by default agent perceives things within a default radius ."], "bleu": 0.10343603005129705, "rouge_l": 0.2489795918367347}
{"id": 4484, "code": "def move to ( self , thing , destination ) : thing . bump = self . some things at ( destination , Obstacle ) if not thing . bump : thing . location = destination for o in self . observers : o . thing moved ( thing )", "predictions": ["move a thing to a destination ."], "references": ["move a thing to a new location ."], "bleu": 0.6129752413741055, "rouge_l": 0.7904967602591793}
{"id": 4485, "code": "def add walls ( self ) : for x in range ( self . width ) : self . add thing ( Wall ( ) , ( x , 0 ) ) self . add thing ( Wall ( ) , ( x , self . height - 1 ) ) for y in range ( self . height ) : self . add thing ( Wall ( ) , ( 0 , y ) ) self . add thing ( Wall ( ) , ( self . width - 1 , y ) )", "predictions": ["add a walls to the map"], "references": ["put walls around the entire perimeter of the grid ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4486, "code": "def add edge ( self , edge ) : start , end , lhs , found , expects = edge if edge not in self . chart [ end ] : self . chart [ end ] . append ( edge ) if self . trace : print '%10s: added %s' % ( caller ( 2 ) , edge ) if not expects : self . extender ( edge ) else : self . predictor ( edge )", "predictions": ["add an edge to the graph"], "references": ["add edge to chart and see if it extends or predicts another edge ."], "bleu": 0.08234616270176032, "rouge_l": 0.2798165137614679}
{"id": 4487, "code": "def scanner ( self , j , word ) : for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )", "predictions": ["build a scanner word for the given word ."], "references": ["for each edge expecting a word of this category here extend the edge ."], "bleu": 0.10657503067399117, "rouge_l": 0.3347050754458162}
{"id": 4488, "code": "def predictor ( self , ( i , j , A , alpha , Bb ) ) : B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites for ( B ) : self . add edge ( [ j , j , B , [ ] , rhs ] )", "predictions": ["make cartesian cartesian with another one ."], "references": ["add to chart any rules for b that could help extend this edge ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 4489, "code": "def extender ( self , edge ) : ( j , k , B , , ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )", "predictions": ["build a rectangle for a given edge ."], "references": ["see what edges can be extended by this edge ."], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 4490, "code": "def sum out ( var , factors , bn ) : result , var factors = [ ] , [ ] for f in factors : ( var factors if var in f . vars else result ) . append ( f ) result . append ( pointwise product ( var factors , bn ) . sum out ( var , bn ) ) return result", "predictions": ["sum the product of a variable"], "references": ["eliminate var from all factors by summing over its values ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 4491, "code": "def all events ( vars , bn , e ) : if not vars : yield e else : X , rest = vars [ 0 ] , vars [ 1 : ] for e1 in all events ( rest , bn , e ) : for x in bn . variable values ( X ) : yield extend ( e1 , X , x )", "predictions": ["yields all events that include the given vars ."], "references": ["yield every way of extending e with values for all vars ."], "bleu": 0.1430210741102858, "rouge_l": 0.2785388127853881}
{"id": 4492, "code": "def consistent with ( event , evidence ) : return every ( lambda ( k , v ) : evidence . get ( k , v ) == v , event . items ( ) )", "predictions": ["allow an event to a specific event ."], "references": ["is event consistent with the given evidence?"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4493, "code": "def pointwise product ( self , other , bn ) : vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all events ( vars , bn , { } ) ) return Factor ( vars , cpt )", "predictions": ["return a product of all pointwise with the same product as the same product as the input ."], "references": ["multiply two factors combining their variables ."], "bleu": 0.06809398432036522, "rouge_l": 0.08689458689458689}
{"id": 4494, "code": "def sum out ( self , var , bn ) : vars = [ X for X in self . vars if X != var ] cpt = dict ( ( event values ( e , vars ) , sum ( self . p ( extend ( e , var , val ) ) for val in bn . variable values ( var ) ) ) for e in all events ( vars , bn , { } ) ) return Factor ( vars , cpt )", "predictions": ["sum a set of events to a given parameter"], "references": ["make a factor eliminating var by summing over its values ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 4495, "code": "def normalize ( self ) : assert len ( self . vars ) == 1 return Prob Dist ( self . vars [ 0 ] , dict ( ( k , v ) for ( ( k , ) , v ) in self . cpt . items ( ) ) )", "predictions": ["return a new copy of the expression ."], "references": ["return my probabilities ; must be down to one variable ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 4496, "code": "def brightness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( \"Brightness must be value between 0 and 100\" ) b = int ( floor ( level / 4.0 ) + 2 ) #lights want values 2-27 return ( COMMANDS [ 'ON' ] [ group ] , Command ( 0x4E , b ) )", "predictions": ["create a brightness brightness"], "references": ["assumes level is out of 100"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 4497, "code": "def brightness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( \"Brightness must be value between 0 and 100\" ) b = int ( floor ( level / 10.0 ) ) #lights have 10 levels of brightness commands = list ( darkest ( group ) ) for i in range ( 0 , b ) : commands . append ( COMMANDS [ 'BRIGHTER' ] ) return tuple ( commands )", "predictions": ["create a list of brightness levels between the current brightness level ."], "references": ["assumes level is out of 100"], "bleu": 0.11498759556447223, "rouge_l": 0.1182170542635659}
{"id": 4498, "code": "def warmness ( level = 100 , group = 0 ) : if level not in range ( 0 , 101 ) : raise Exception ( \"Warmness must be value between 0 and 100\" ) b = int ( floor ( level / 10.0 ) ) #lights have 10 levels of warmness commands = list ( coolest ( group ) ) for i in range ( 0 , b ) : commands . append ( COMMANDS [ 'WARMER' ] ) return tuple ( commands )", "predictions": ["convert a list of warmness to a tuple of 10 commands ."], "references": ["assumes level is out of 100"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 4499, "code": "def getpassword ( prompt = \"Password: \" ) : fd = sys . stdin . fileno ( ) old = termios . tcgetattr ( fd ) new = termios . tcgetattr ( fd ) new [ 3 ] &= ~ termios . ECHO try : termios . tcsetattr ( fd , termios . TCSADRAIN , new ) passwd = raw input ( prompt ) finally : termios . tcsetattr ( fd , termios . TCSADRAIN , old ) return passwd", "predictions": ["try to do something on the next request ."], "references": ["get user input without echo"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4500, "code": "def getch ( ) : try : termios . tcsetattr ( fd , termios . TCSANOW , new settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( fd , termios . TCSADRAIN , old settings ) return ch", "predictions": ["get a file descriptor for a change in stdin ."], "references": ["get character . waiting for key"], "bleu": 0.14991106946711685, "rouge_l": 0.26180257510729615}
{"id": 4501, "code": "def format ( self , record ) : try : record . message = record . get Message ( ) except Type Error : if record . args : if isinstance ( record . args , collections . Mapping ) : record . message = record . msg . format ( * * record . args ) else : record . message = record . msg . format ( record . args ) self . fmt = self . getfmt ( record . levelname ) if self . uses Time ( ) : record . asctime = self . format Time ( record , self . datefmt ) s = self . fmt . format ( * * record . dict ) if record . exc info : if not record . exc text : record . exc text = self . format Exception ( record . exc info ) if record . exc text : if s [ - 1 : ] != '\\n' : s += '\\n' try : s = s + record . exc text except Unicode Error : s = s + record . exc text . decode ( sys . getfilesystemencoding ( ) , 'replace' ) return s", "predictions": ["format a record into a log message ."], "references": ["tweaked from source of base"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4502, "code": "def connect ( self ) : try : self . socket = socket . socket ( socket . AF INET , socket . SOCK STREAM ) self . socket . settimeout ( TIMEOUT SECONDS ) self . socket . connect ( ( self . ip , self . port ) ) LOGGER . debug ( \"Successfully created Hub at %s:%s :)\" , self . ip , self . port ) except socket . error as error : LOGGER . error ( \"Error creating Hub: %s :(\" , error ) self . socket . close ( )", "predictions": ["connect to the socket ."], "references": ["create and connect to socket for tcp communication with hub ."], "bleu": 0.12296202211076697, "rouge_l": 0.4683301343570058}
{"id": 4503, "code": "def send command ( self , command ) : with self . lock : try : self . socket . send ( command . encode ( \"utf8\" ) ) result = self . receive ( ) while result . startswith ( \"S\" ) or result . startswith ( \"NEW\" ) : LOGGER . debug ( \"!Got response: %s\" , result ) result = self . receive ( ) LOGGER . debug ( \"Received: %s\" , result ) return result except socket . error as error : LOGGER . error ( \"Error sending command: %s\" , error ) self . connect ( ) return \"\"", "predictions": ["send a command to the redis server ."], "references": ["send tcp command to hub and return response ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 4504, "code": "def receive ( self ) : try : buffer = self . socket . recv ( BUFFER SIZE ) except socket . timeout as error : LOGGER . error ( \"Error receiving: %s\" , error ) return \"\" buffering = True response = '' while buffering : if '\\n' in buffer . decode ( \"utf8\" ) : response = buffer . decode ( \"utf8\" ) . split ( '\\n' ) [ 0 ] buffering = False else : try : more = self . socket . recv ( BUFFER SIZE ) except socket . timeout : more = None if not more : buffering = False response = buffer . decode ( \"utf8\" ) else : buffer += more return response", "predictions": ["receive a response from server ."], "references": ["receive tcp response looping to get whole thing or timeout ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 4505, "code": "def get data ( self ) : response = self . send command ( GET LIGHTS COMMAND ) LOGGER . debug ( \"get data response: %s\" , repr ( response ) ) if not response : LOGGER . debug ( \"Empty response: %s\" , response ) return { } response = response . strip ( ) if not ( response . startswith ( \"GLB\" ) and response . endswith ( \";\" ) ) : LOGGER . debug ( \"Invalid response: %s\" , repr ( response ) ) return { } response = response [ 4 : - 3 ] light strings = response . split ( ';' ) light data by id = { } for light string in light strings : values = light string . split ( ',' ) try : light data by id [ values [ 0 ] ] = [ int ( values [ 2 ] ) , int ( values [ 4 ] ) , int ( values [ 5 ] ) , int ( values [ 6 ] ) , int ( values [ 7 ] ) ] except Value Error as error : LOGGER . error ( \"Error %s: %s (%s)\" , error , values , response ) except Index Error as error : LOGGER . error ( \"Error %s: %s (%s)\" , error , values , response ) return light data by id", "predictions": ["get light data from gateway ."], "references": ["get current light data as dictionary with light zids as keys ."], "bleu": 0.12151662434083678, "rouge_l": 0.41924398625429554}
{"id": 4506, "code": "def get lights ( self ) : now = datetime . datetime . now ( ) if ( now - self . last updated ) < datetime . timedelta ( seconds = UPDATE INTERVAL SECONDS ) : return self . bulbs else : self . last updated = now light data = self . get data ( ) LOGGER . debug ( \"got: %s\" , light data ) if not light data : return [ ] if self . bulbs : for bulb in self . bulbs : try : values = light data [ bulb . zid ] bulb . online , bulb . red , bulb . green , bulb . blue , bulb . level = values except Key Error : pass else : for light id in light data : self . bulbs . append ( Bulb ( self , light id , * light data [ light id ] ) ) return self . bulbs", "predictions": ["get the lights of the light ."], "references": ["get current light data set and return as list of bulb objects ."], "bleu": 0.09912033646614596, "rouge_l": 0.2846034214618974}
{"id": 4507, "code": "def set brightness ( self , brightness ) : command = \"C {},,,,{},\\r\\n\" . format ( self . zid , brightness ) response = self . hub . send command ( command ) LOGGER . debug ( \"Set brightness %s: %s\" , repr ( command ) , response ) return response", "predictions": ["set the brightness level of the remote hub ."], "references": ["set brightness of bulb ."], "bleu": 0.17747405280050263, "rouge_l": 0.6024691358024692}
{"id": 4508, "code": "def set all ( self , red , green , blue , brightness ) : command = \"C {},{},{},{},{},\\r\\n\" . format ( self . zid , red , green , blue , brightness ) response = self . hub . send command ( command ) LOGGER . debug ( \"Set all %s: %s\" , repr ( command ) , response ) return response", "predictions": ["set all the images for a hub"], "references": ["set color and brightness of bulb ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4509, "code": "def update ( self ) : bulbs = self . hub . get lights ( ) if not bulbs : LOGGER . debug ( \"%s is offline, send command failed\" , self . zid ) self . online = False", "predictions": ["update offline, command ."], "references": ["update light objects to their current values ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 4510, "code": "def readtxt ( filepath ) : with open ( filepath , 'rt' ) as f : lines = f . readlines ( ) return '' . join ( lines )", "predictions": ["get the lines of a file"], "references": ["read file as is"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4511, "code": "def modulename ( cls , depth = 1 ) : depth += cls . extra depth frame = sys . getframe ( depth ) return frame . f globals [ ' name ' ]", "predictions": ["creates a modulename modulename for the given depth ."], "references": ["get caller s __name__"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4512, "code": "def snoise2d ( size , z = 0.0 , scale = 0.05 , octaves = 1 , persistence = 0.25 , lacunarity = 2.0 ) : import noise data = np . empty ( size , dtype = 'float32' ) for y in range ( size [ 0 ] ) : for x in range ( size [ 1 ] ) : v = noise . snoise3 ( x * scale , y * scale , z , octaves = octaves , persistence = persistence , lacunarity = lacunarity ) data [ x , y ] = v data = data * 0.5 + 0.5 if debug : assert data . min ( ) >= 0. and data . max ( ) <= 1.0 return data", "predictions": ["convert a dataset to a unit unit . ."], "references": ["z value as like a seed"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4513, "code": "def guess package path ( searchfrom ) : from snipy . io import fileutil current = searchfrom + '/' init found = False pack found = False while not init found and current != '/' : current = os . path . dirname ( current ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) if not init found : searchfrom = dirname ( searchfrom ) for folder in fileutil . listfolder ( searchfrom ) : current = os . path . join ( searchfrom , folder ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) if init found : break while init found : current = os . path . dirname ( current ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) pack found = not init found return current if pack found else None", "predictions": ["delete the path path path"], "references": ["package path . return none if failed to guess"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4514, "code": "def find package path ( searchfrom ) : current = searchfrom + '/' init found = False pack found = False while not init found and current != '/' : current = os . path . dirname ( current ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) while init found : current = os . path . dirname ( current ) initfile = os . path . join ( current , ' init .py' ) init found = os . path . exists ( initfile ) pack found = not init found return current if pack found else None", "predictions": ["things to things that will search for a near near"], "references": ["package path . return none if failed to guess"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 4515, "code": "def git tag ( tag ) : print ( 'Tagging \"{}\"' . format ( tag ) ) msg = '\"Released version {}\"' . format ( tag ) Popen ( [ 'git' , 'tag' , '-s' , '-m' , msg , tag ] ) . wait ( )", "predictions": ["wait a git tag"], "references": ["tags the current version ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 4516, "code": "def render to ( self , path , template , * * data ) : html = self . render ( template , * * data ) with open ( path , 'w' ) as f : f . write ( html . encode ( charset ) )", "predictions": ["move the given destination to the given thing things things things things"], "references": ["render data with template and then write to path"], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 4517, "code": "def index row ( self , dataframe ) : return dataframe . loc [ self . kwargs [ self . lookup url kwarg ] ] . to frame ( ) . T", "predictions": ["returns a dataframe instance for the given dataframe"], "references": ["indexes the row based on the request parameters ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4518, "code": "def paginator ( self ) : if not hasattr ( self , ' paginator' ) : if self . pagination class is None : self . paginator = None else : self . paginator = self . pagination class ( ) return self . paginator", "predictions": ["return of this instance expects it s add add or none expects expects to another if it s specified ."], "references": ["the paginator instance associated with the view or none ."], "bleu": 0.09134423666564473, "rouge_l": 0.2837209302325582}
{"id": 4519, "code": "def paginate dataframe ( self , dataframe ) : if self . paginator is None : return None return self . paginator . paginate dataframe ( dataframe , self . request , view = self )", "predictions": ["return a dataframe instance"], "references": ["return a single page of results or none if pagination is disabled ."], "bleu": 0.04984021611241231, "rouge_l": 0.2147887323943662}
{"id": 4520, "code": "def parse ( self ) : if exists ( self . filepath ) : content = open ( self . filepath ) . read ( ) . decode ( charset ) else : content = \"\" try : config = toml . loads ( content ) except toml . Toml Syntax Error : raise Config Syntax Error return config", "predictions": ["predictor to predictor the grammar from the grammar file alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha"], "references": ["parse config return a dict"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 4521, "code": "def parse ( self , source ) : rt , title , title pic , markdown = libparser . parse ( source ) if rt == - 1 : raise Separator Not Found elif rt == - 2 : raise Post Title Not Found title , title pic , markdown = map ( to unicode , ( title , title pic , markdown ) ) html = self . markdown . render ( markdown ) summary = self . markdown . render ( markdown [ : 200 ] ) return { 'title' : title , 'markdown' : markdown , 'html' : html , 'summary' : summary , 'title pic' : title pic }", "predictions": ["extender s extender with given edge"], "references": ["parse ascii post source return dict"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4522, "code": "def parse filename ( self , filepath ) : name = os . path . basename ( filepath ) [ : - src ext len ] try : dt = datetime . strptime ( name , \"%Y-%m-%d-%H-%M\" ) except Value Error : raise Post Name Invalid return { 'name' : name , 'datetime' : dt , 'filepath' : filepath }", "predictions": ["sum a out of out of a out of the given out ."], "references": ["parse post source files name to datetime object"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 4523, "code": "def run server ( self , port ) : try : self . server = Multi Threaded HTTP Server ( ( '0.0.0.0' , port ) , Handler ) except socket . error , e : logger . error ( str ( e ) ) sys . exit ( 1 ) logger . info ( \"HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ...\" % port ) try : self . server . serve forever ( ) except Keyboard Interrupt : logger . info ( \"^C received, shutting down server\" ) self . shutdown server ( )", "predictions": ["start a events on the server if it has been already running ."], "references": ["run a server binding to port"], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 4524, "code": "def get files stat ( self ) : if not exists ( Post . src dir ) : logger . error ( Source Directory Not Found . doc ) sys . exit ( Source Directory Not Found . exit code ) paths = [ ] for fn in ls ( Post . src dir ) : if fn . endswith ( src ext ) : paths . append ( join ( Post . src dir , fn ) ) if exists ( config . filepath ) : paths . append ( config . filepath ) files = dict ( ( p , stat ( p ) . st mtime ) for p in paths ) return files", "predictions": ["consistent with all with the with the current version"], "references": ["get source files update time"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4525, "code": "def deploy blog ( ) : logger . info ( deploy blog . doc ) call ( 'rsync -aqu ' + join ( dirname ( file ) , 'res' , '*' ) + ' .' , shell = True ) logger . success ( 'Done' ) logger . info ( 'Please edit config.toml to meet your needs' )", "predictions": ["pointwise return new product"], "references": ["deploy new blog to current directory"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4526, "code": "def using ( context , alias ) : if alias == '' : yield context else : try : widgets = context . render context [ WIDGET CONTEXT KEY ] except Key Error : raise template . Template Syntax Error ( 'No widget libraries loaded!' ) try : block set = widgets [ alias ] except Key Error : raise template . Template Syntax Error ( 'No widget library loaded for alias: %r' % alias ) context . render context . push ( ) context . render context [ BLOCK CONTEXT KEY ] = block set context . render context [ WIDGET CONTEXT KEY ] = widgets yield context context . render context . pop ( )", "predictions": ["return a list of for the given alias ."], "references": ["temporarily update the context to use the blockcontext for the given alias ."], "bleu": 0.3314730476621174, "rouge_l": 0.4401154401154401}
{"id": 4527, "code": "def find block ( context , * names ) : block set = context . render context [ BLOCK CONTEXT KEY ] for name in names : block = block set . get block ( name ) if block is not None : return block raise template . Template Syntax Error ( 'No widget found for: %r' % ( names , ) )", "predictions": ["normalize a block to a block"], "references": ["find the first matching block in the current block_context"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4528, "code": "def load widgets ( context , * * kwargs ) : soft = kwargs . pop ( ' soft' , False ) try : widgets = context . render context [ WIDGET CONTEXT KEY ] except Key Error : widgets = context . render context [ WIDGET CONTEXT KEY ] = { } for alias , template name in kwargs . items ( ) : if soft and alias in widgets : continue with context . render context . push ( { BLOCK CONTEXT KEY : Block Context ( ) } ) : blocks = resolve blocks ( template name , context ) widgets [ alias ] = blocks return ''", "predictions": ["brightness widgets function for widgets"], "references": ["load a series of widget libraries ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4529, "code": "def auto widget ( field ) : info = { 'widget' : field . field . widget . class . name , 'field' : field . field . class . name , 'name' : field . name , } return [ fmt . format ( * * info ) for fmt in ( '{field} {widget} {name}' , '{field} {name}' , '{widget} {name}' , '{field} {widget}' , '{name}' , '{widget}' , '{field}' , ) ]", "predictions": ["returns the widget for the given = = 0 if not present"], "references": ["return a list of widget names for the provided field ."], "bleu": 0.14694106251955755, "rouge_l": 0.2629310344827586}
{"id": 4530, "code": "def display ( self ) : if not self . is group ( ) : return self . display return ( ( force text ( k ) , v ) for k , v in self . display )", "predictions": ["display the widget 0 to a string"], "references": ["when dealing with optgroups ensure that the value is properly force_text d ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 4531, "code": "def stored messages list ( context , num elements = 10 ) : if \"user\" in context : user = context [ \"user\" ] if user . is authenticated ( ) : qs = Inbox . objects . select related ( \"message\" ) . filter ( user = user ) return { \"messages\" : qs [ : num elements ] , \"count\" : qs . count ( ) , }", "predictions": ["raw prompt for stored 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3"], "references": ["renders a list of unread stored messages for the current user"], "bleu": 0.04317900023606586, "rouge_l": 0.05209222886421862}
{"id": 4532, "code": "def stored messages count ( context ) : if \"user\" in context : user = context [ \"user\" ] if user . is authenticated ( ) : return Inbox . objects . select related ( \"message\" ) . filter ( user = user ) . count ( )", "predictions": ["return the number of messages in a user = true"], "references": ["renders a list of unread stored messages for the current user"], "bleu": 0.15011602950163877, "rouge_l": 0.28328173374613}
{"id": 4533, "code": "def stored messages archive ( context , num elements = 10 ) : if \"user\" in context : user = context [ \"user\" ] if user . is authenticated ( ) : qs = Message Archive . objects . select related ( \"message\" ) . filter ( user = user ) return { \"messages\" : qs [ : num elements ] , \"count\" : qs . count ( ) , }", "predictions": ["asctime messages messages messages . . ."], "references": ["renders a list of archived messages for the current user"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4534, "code": "def jocker ( test options = None ) : version = ver check ( ) options = test options or docopt ( doc , version = version ) set global verbosity level ( options . get ( '--verbose' ) ) jocker lgr . debug ( options ) jocker run ( options )", "predictions": ["at the end of the self . . ."], "references": ["main entry point for script ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4535, "code": "def configure custom ( self , config ) : c = config . pop ( '()' ) if not hasattr ( c , ' call ' ) and hasattr ( types , 'Class Type' ) and isinstance ( c , types . Class Type ) : c = self . resolve ( c ) props = config . pop ( '.' , None ) kwargs = dict ( ( k , config [ k ] ) for k in config if valid ident ( k ) ) result = c ( * * kwargs ) if props : for name , value in props . items ( ) : setattr ( result , name , value ) return result", "predictions": ["see base class method ."], "references": ["configure an object with a user - supplied factory ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4536, "code": "def upload gif ( gif ) : client id = os . environ . get ( 'IMGUR API ID' ) client secret = os . environ . get ( 'IMGUR API SECRET' ) if client id is None or client secret is None : click . echo ( 'Cannot upload - could not find IMGUR API ID or IMGUR API SECRET environment variables' ) return client = Imgur Client ( client id , client secret ) click . echo ( 'Uploading file {}' . format ( click . format filename ( gif ) ) ) response = client . upload from path ( gif ) click . echo ( 'File uploaded - see your gif at {}' . format ( response [ 'link' ] ) )", "predictions": ["receive gif from from to . socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket socket"], "references": ["uploads an image file to imgur"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 4537, "code": "def is dot ( ip ) : octets = str ( ip ) . split ( '.' ) if len ( octets ) != 4 : return False for i in octets : try : val = int ( i ) except Value Error : return False if val > 255 or val < 0 : return False return True", "predictions": ["return is a data source"], "references": ["return true if the ip address is in dotted decimal notation ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 4538, "code": "def is bin ( ip ) : try : ip = str ( ip ) if len ( ip ) != 32 : return False dec = int ( ip , 2 ) except ( Type Error , Value Error ) : return False if dec > 4294967295 or dec < 0 : return False return True", "predictions": ["determines whether an self is a lights lights or not ."], "references": ["return true if the ip address is in binary notation ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 4539, "code": "def is oct ( ip ) : try : dec = int ( str ( ip ) , 8 ) except ( Type Error , Value Error ) : return False if dec > 0o37777777777 or dec < 0 : return False return True", "predictions": ["determines whether an self is brightness ."], "references": ["return true if the ip address is in octal notation ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4540, "code": "def is dec ( ip ) : try : dec = int ( str ( ip ) ) except Value Error : return False if dec > 4294967295 or dec < 0 : return False return True", "predictions": ["returns true if self is a all of the given self . self . self ."], "references": ["return true if the ip address is in decimal notation ."], "bleu": 0.1203921753741131, "rouge_l": 0.3065326633165829}
{"id": 4541, "code": "def is bits nm ( nm ) : try : bits = int ( str ( nm ) ) except Value Error : return False if bits > 32 or bits < 0 : return False return True", "predictions": ["returns a boolean if a value is a valid value of a range of bits ."], "references": ["return true if the netmask is in bits notatation ."], "bleu": 0.09672649511413092, "rouge_l": 0.32105263157894737}
{"id": 4542, "code": "def is wildcard nm ( nm ) : try : dec = 0x FFFFFFFF - dot to dec ( nm , check = True ) except Value Error : return False if dec in NETMASKS VALUES : return True return False", "predictions": ["determine if a wildcard is a wildcard"], "references": ["return true if the netmask is in wildcard bits notatation ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 4543, "code": "def dot to dec ( ip , check = True ) : if check and not is dot ( ip ) : raise Value Error ( ' dot to dec: invalid IP: \"%s\"' % ip ) octets = str ( ip ) . split ( '.' ) dec = 0 dec |= int ( octets [ 0 ] ) << 24 dec |= int ( octets [ 1 ] ) << 16 dec |= int ( octets [ 2 ] ) << 8 dec |= int ( octets [ 3 ] ) return dec", "predictions": ["convert a modulename integer to a cls integer"], "references": ["dotted decimal notation to decimal conversion ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4544, "code": "def dec to dot ( ip ) : first = int ( ( ip >> 24 ) & 255 ) second = int ( ( ip >> 16 ) & 255 ) third = int ( ( ip >> 8 ) & 255 ) fourth = int ( ip & 255 ) return '%d.%d.%d.%d' % ( first , second , third , fourth )", "predictions": ["convert an ip address to a dot network"], "references": ["decimal to dotted decimal notation conversion ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4545, "code": "def hex to dec ( ip , check = True ) : if check and not is hex ( ip ) : raise Value Error ( ' hex to dec: invalid IP: \"%s\"' % ip ) if isinstance ( ip , int ) : ip = hex ( ip ) return int ( str ( ip ) , 16 )", "predictions": ["convert a hex string to dec ."], "references": ["hexadecimal to decimal conversion ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4546, "code": "def oct to dec ( ip , check = True ) : if check and not is oct ( ip ) : raise Value Error ( ' oct to dec: invalid IP: \"%s\"' % ip ) if isinstance ( ip , int ) : ip = oct ( ip ) return int ( str ( ip ) , 8 )", "predictions": ["convert oct to dec ."], "references": ["octal to decimal conversion ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4547, "code": "def bin to dec ( ip , check = True ) : if check and not is bin ( ip ) : raise Value Error ( ' bin to dec: invalid IP: \"%s\"' % ip ) if isinstance ( ip , int ) : ip = str ( ip ) return int ( str ( ip ) , 2 )", "predictions": ["convert an ip address to dec"], "references": ["binary to decimal conversion ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4548, "code": "def dec to bin ( ip ) : bits = [ ] while ip : bits . append ( BYTES TO BITS [ ip & 255 ] ) ip >>= 8 bits . reverse ( ) return '' . join ( bits ) or 32 * '0'", "predictions": ["convert an ip address to a bin string ."], "references": ["decimal to binary conversion ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 4549, "code": "def bits to dec ( nm , check = True ) : if check and not is bits nm ( nm ) : raise Value Error ( ' bits to dec: invalid netmask: \"%s\"' % nm ) bits = int ( str ( nm ) ) return VALID NETMASKS [ bits ]", "predictions": ["convert a set of bits to dec ."], "references": ["bits to decimal conversion ."], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 4550, "code": "def wildcard to dec ( nm , check = False ) : if check and not is wildcard nm ( nm ) : raise Value Error ( ' wildcard to dec: invalid netmask: \"%s\"' % nm ) return 0x FFFFFFFF - dot to dec ( nm , check = False )", "predictions": ["convert a wildcard value to a dec ."], "references": ["wildcard bits to decimal conversion ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 4551, "code": "def convert ( ip , notation , inotation , check , isnm ) : inotation orig = inotation notation orig = notation inotation = get notation ( inotation ) notation = get notation ( notation ) if inotation is None : raise Value Error ( ' convert: unknown input notation: \"%s\"' % inotation orig ) if notation is None : raise Value Error ( ' convert: unknown output notation: \"%s\"' % notation orig ) docheck = check or False if inotation == IP UNKNOWN : inotation = detect ( ip , isnm ) if inotation == IP UNKNOWN : raise Value Error ( ' convert: unable to guess input notation or invalid value' ) if check is None : docheck = True if isnm : docheck = False dec = 0 if inotation == IP DOT : dec = dot to dec ( ip , docheck ) elif inotation == IP HEX : dec = hex to dec ( ip , docheck ) elif inotation == IP BIN : dec = bin to dec ( ip , docheck ) elif inotation == IP OCT : dec = oct to dec ( ip , docheck ) elif inotation == IP DEC : dec = dec to dec long ( ip , docheck ) elif isnm and inotation == NM BITS : dec = bits to dec ( ip , docheck ) elif isnm and inotation == NM WILDCARD : dec = wildcard to dec ( ip , docheck ) else : raise Value Error ( ' convert: unknown IP/netmask notation: \"%s\"' % inotation orig ) if isnm and dec not in NETMASKS VALUES : raise Value Error ( ' convert: invalid netmask: \"%s\"' % ip ) if notation == IP DOT : return dec to dot ( dec ) elif notation == IP HEX : return dec to hex ( dec ) elif notation == IP BIN : return dec to bin ( dec ) elif notation == IP OCT : return dec to oct ( dec ) elif notation == IP DEC : return dec to dec str ( dec ) elif isnm and notation == NM BITS : return dec to bits ( dec ) elif isnm and notation == NM WILDCARD : return dec to wildcard ( dec ) else : raise Value Error ( 'convert: unknown notation: \"%s\"' % notation orig )", "predictions": ["convert ip address to wildcard format"], "references": ["internally used to convert ips and netmasks to other notations ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 4552, "code": "def convert nm ( nm , notation = IP DOT , inotation = IP UNKNOWN , check = True ) : return convert ( nm , notation , inotation , check = check , isnm = True )", "predictions": ["convert a nm object to a pandas object"], "references": ["convert a netmask to another notation ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 4553, "code": "def add ( self , other ) : if isinstance ( other , self . class ) : sum = self . ip dec + other . ip dec elif isinstance ( other , int ) : sum = self . ip dec + other else : other = self . class ( other ) sum = self . ip dec + other . ip dec return sum", "predictions": ["add a sum of two arguments"], "references": ["sum two ip addresses ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 4554, "code": "def sub ( self , other ) : if isinstance ( other , self . class ) : sub = self . ip dec - other . ip dec if isinstance ( other , int ) : sub = self . ip dec - other else : other = self . class ( other ) sub = self . ip dec - other . ip dec return sub", "predictions": ["return a sub - attribute instance with the given other ip ."], "references": ["subtract two ip addresses ."], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 4555, "code": "def get bits ( self ) : return convert ( self . ip , notation = NM BITS , inotation = IP DOT , check = False , isnm = self . isnm )", "predictions": ["get a list of bits for this user"], "references": ["return the bits notation of the netmask ."], "bleu": 0.17747405280050269, "rouge_l": 0.125}
{"id": 4556, "code": "def get wildcard ( self ) : return convert ( self . ip , notation = NM WILDCARD , inotation = IP DOT , check = False , isnm = self . isnm )", "predictions": ["get a wildcard of the user s wildcard ."], "references": ["return the wildcard bits notation of the netmask ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 4557, "code": "def set ( self , ip , netmask = None ) : if isinstance ( ip , str ) and netmask is None : ipnm = ip . split ( '/' ) if len ( ipnm ) != 2 : raise Value Error ( 'set: invalid CIDR: \"%s\"' % ip ) ip = ipnm [ 0 ] netmask = ipnm [ 1 ] if isinstance ( ip , I Pv4Address ) : self . ip = ip else : self . ip = I Pv4Address ( ip ) if isinstance ( netmask , I Pv4Net Mask ) : self . nm = netmask else : self . nm = I Pv4Net Mask ( netmask ) ipl = int ( self . ip ) nml = int ( self . nm ) base add = ipl & nml self . ip num = 0x FFFFFFFF - 1 - nml if self . ip num in ( - 1 , 0 ) : if self . ip num == - 1 : self . ip num = 1 else : self . ip num = 2 self . net ip = None self . bc ip = None self . first ip dec = base add self . first ip = I Pv4Address ( self . first ip dec , notation = IP DEC ) if self . ip num == 1 : last ip dec = self . first ip dec else : last ip dec = self . first ip dec + 1 self . last ip = I Pv4Address ( last ip dec , notation = IP DEC ) return self . net ip = I Pv4Address ( base add , notation = IP DEC ) self . bc ip = I Pv4Address ( base add + self . ip num + 1 , notation = IP DEC ) self . first ip dec = base add + 1 self . first ip = I Pv4Address ( self . first ip dec , notation = IP DEC ) self . last ip = I Pv4Address ( base add + self . ip num , notation = IP DEC )", "predictions": ["set a ip address for a given ip address ."], "references": ["set the ip address and the netmask ."], "bleu": 0.18850319022747347, "rouge_l": 0.4535315985130111}
{"id": 4558, "code": "def set ip ( self , ip ) : self . set ( ip = ip , netmask = self . nm )", "predictions": ["set the ip address of an ip address"], "references": ["change the current ip ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4559, "code": "def set netmask ( self , netmask ) : self . set ( ip = self . ip , netmask = netmask )", "predictions": ["sets the netmask s netmask"], "references": ["change the current netmask ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4560, "code": "async def copy storage object ( self , source bucket , source key , bucket , key ) : info = await self . head object ( Bucket = source bucket , Key = source key ) size = info [ 'Content Length' ] if size > MULTI PART SIZE : result = await multipart copy ( self , source bucket , source key , bucket , key , size ) else : result = await self . copy object ( Bucket = bucket , Key = key , Copy Source = source string ( source bucket , source key ) ) return result", "predictions": ["copy a storage object to another storage ."], "references": ["copy a file from one bucket into another"], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 4561, "code": "async def upload file ( self , full path ) : rel path = os . path . relpath ( full path , self . folder ) key = s3 key ( os . path . join ( self . key , rel path ) ) ct = self . content types . get ( key . split ( '.' ) [ - 1 ] ) with open ( full path , 'rb' ) as fp : file = fp . read ( ) try : await self . botocore . upload file ( self . bucket , file , key = key , Content Type = ct ) except Exception as exc : LOGGER . error ( 'Could not upload \"%s\": %s' , key , exc ) self . failures [ key ] = self . all . pop ( full path ) return size = self . all . pop ( full path ) self . success [ key ] = size self . total size += size percentage = 100 * ( 1 - len ( self . all ) / self . total files ) message = '{0:.0f}% completed - uploaded \"{1}\" - {2}' . format ( percentage , key , convert bytes ( size ) ) LOGGER . info ( message )", "predictions": ["uploads a file to the s3 folder ."], "references": ["coroutine for uploading a single file"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 4562, "code": "async def trigger ( self , event , data = None , socket id = None ) : json data = json . dumps ( data , cls = self . pusher . encoder ) query string = self . signed query ( event , json data , socket id ) signed path = \"%s?%s\" % ( self . path , query string ) pusher = self . pusher absolute url = pusher . get absolute path ( signed path ) response = await pusher . http . post ( absolute url , data = json data , headers = [ ( 'Content-Type' , 'application/json' ) ] ) response . raise for status ( ) return response . status code == 202", "predictions": ["trigger an event ."], "references": ["trigger an event on this channel"], "bleu": 0.4056114983537769, "rouge_l": 0.5791139240506329}
{"id": 4563, "code": "async def connect ( self ) : if not self . consumer : waiter = self . waiter = asyncio . Future ( ) try : address = self . websocket host ( ) self . logger . info ( 'Connect to %s' , address ) self . consumer = await self . http . get ( address ) if self . consumer . status code != 101 : raise Pusher Error ( \"Could not connect to websocket\" ) except Exception as exc : waiter . set exception ( exc ) raise else : await waiter return self . consumer", "predictions": ["connect to the websocket server ."], "references": ["connect to a pusher websocket"], "bleu": 0.31239399369202553, "rouge_l": 0.5545454545454546}
{"id": 4564, "code": "def on message ( self , websocket , message ) : waiter = self . waiter self . waiter = None encoded = json . loads ( message ) event = encoded . get ( 'event' ) channel = encoded . get ( 'channel' ) data = json . loads ( encoded . get ( 'data' ) ) try : if event == PUSHER ERROR : raise Pusher Error ( data [ 'message' ] , data [ 'code' ] ) elif event == PUSHER CONNECTION : self . socket id = data . get ( 'socket id' ) self . logger . info ( 'Succesfully connected on socket %s' , self . socket id ) waiter . set result ( self . socket id ) elif event == PUSHER SUBSCRIBED : self . logger . info ( 'Succesfully subscribed to %s' , encoded . get ( 'channel' ) ) elif channel : self [ channel ] . event ( event , data ) except Exception as exc : if waiter : waiter . set exception ( exc ) else : self . logger . exception ( 'pusher error' )", "predictions": ["called when asyncio . protocol message is received from server ."], "references": ["handle websocket incoming messages"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4565, "code": "def const equal ( str a , str b ) : if len ( str a ) != len ( str b ) : return False result = True for i in range ( len ( str a ) ) : result &= ( str a [ i ] == str b [ i ] ) return result", "predictions": ["compare two strings ignoring none"], "references": ["constant time string comparison"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4566, "code": "def decode html entities ( html ) : if not html : return html for entity , char in six . iteritems ( html entity map ) : html = html . replace ( entity , char ) return html", "predictions": ["converts html entities to html ."], "references": ["decodes a limited set of html entities ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 4567, "code": "def set algorithms ( self , signature = None , encryption = None , serialization = None , compression = None ) : self . signature algorithms = self . update dict ( signature , self . DEFAULT SIGNATURE ) self . encryption algorithms = self . update dict ( encryption , self . DEFAULT ENCRYPTION ) self . serialization algorithms = self . update dict ( serialization , self . DEFAULT SERIALIZATION ) self . compression algorithms = self . update dict ( compression , self . DEFAULT COMPRESSION )", "predictions": ["set algorithms data for this serialization"], "references": ["set algorithms used for sealing . defaults can not be overridden ."], "bleu": 0.11492332782473744, "rouge_l": 0.31443298969072164}
{"id": 4568, "code": "def get algorithms ( self ) : return { 'signature' : self . signature algorithms , 'encryption' : self . encryption algorithms , 'serialization' : self . serialization algorithms , 'compression' : self . compression algorithms , }", "predictions": ["return a dictionary of algorithms for this signature ."], "references": ["get algorithms used for sealing"], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 4569, "code": "def set options ( self , options ) : if not options : return self . options . copy ( ) options = options . copy ( ) if 'magic' in options : self . set magic ( options [ 'magic' ] ) del ( options [ 'magic' ] ) if 'flags' in options : flags = options [ 'flags' ] del ( options [ 'flags' ] ) for key , value in flags . iteritems ( ) : if not isinstance ( value , bool ) : raise Type Error ( 'Invalid flag type for: %s' % key ) else : flags = self . options [ 'flags' ] if 'info' in options : del ( options [ 'info' ] ) for key , value in options . iteritems ( ) : if not isinstance ( value , int ) : raise Type Error ( 'Invalid option type for: %s' % key ) if value < 0 or value > 255 : raise Value Error ( 'Option value out of range for: %s' % key ) new options = self . options . copy ( ) new options . update ( options ) new options [ 'flags' ] . update ( flags ) return new options", "predictions": ["set the options to the flag ."], "references": ["private function for setting options used for sealing"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4570, "code": "def verify signature ( self , data ) : data = self . remove magic ( data ) data = urlsafe nopadding b64decode ( data ) options = self . read header ( data ) data = self . add magic ( data ) self . unsign data ( data , options )", "predictions": ["verify a signature of data"], "references": ["verify sealed data signature"], "bleu": 0.32466791547509893, "rouge_l": 0.4535315985130111}
{"id": 4571, "code": "def encode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : return data + self . hmac generate ( data , algorithm , key ) elif algorithm [ 'type' ] == 'aes' : return self . aes encrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . dumps ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . zlib compress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )", "predictions": ["encode a single hmac algorithm ."], "references": ["encode data with specific algorithm"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 4572, "code": "def decode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : verify signature = data [ - algorithm [ 'hash size' ] : ] data = data [ : - algorithm [ 'hash size' ] ] signature = self . hmac generate ( data , algorithm , key ) if not const equal ( verify signature , signature ) : raise Exception ( 'Invalid signature' ) return data elif algorithm [ 'type' ] == 'aes' : return self . aes decrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . loads ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . zlib decompress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )", "predictions": ["decode a signature from a data structure ."], "references": ["decode data with specific algorithm"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4573, "code": "def sign data ( self , data , options ) : if options [ 'signature algorithm id' ] not in self . signature algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature algorithm id' ] ) signature algorithm = self . signature algorithms [ options [ 'signature algorithm id' ] ] algorithm = self . get algorithm info ( signature algorithm ) key salt = get random bytes ( algorithm [ 'salt size' ] ) key = self . generate key ( options [ 'signature passphrase id' ] , self . signature passphrases , key salt , algorithm ) data = self . encode ( data , algorithm , key ) return data + key salt", "predictions": ["sign the data with the given data ."], "references": ["add signature to data"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4574, "code": "def unsign data ( self , data , options ) : if options [ 'signature algorithm id' ] not in self . signature algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature algorithm id' ] ) signature algorithm = self . signature algorithms [ options [ 'signature algorithm id' ] ] algorithm = self . get algorithm info ( signature algorithm ) key salt = '' if algorithm [ 'salt size' ] : key salt = data [ - algorithm [ 'salt size' ] : ] data = data [ : - algorithm [ 'salt size' ] ] key = self . generate key ( options [ 'signature passphrase id' ] , self . signature passphrases , key salt , algorithm ) data = self . decode ( data , algorithm , key ) return data", "predictions": ["verify that the data is valid"], "references": ["verify and remove signature"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4575, "code": "def remove magic ( self , data ) : if not self . magic : return data magic size = len ( self . magic ) magic = data [ : magic size ] if magic != self . magic : raise Exception ( 'Invalid magic' ) data = data [ magic size : ] return data", "predictions": ["remove magic data from data"], "references": ["verify and remove magic"], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 4576, "code": "def add header ( self , data , options ) : version info = self . get version info ( options [ 'version' ] ) flags = options [ 'flags' ] header flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header flags = '' . join ( version info [ 'flags' ] ( * * header flags ) ) header flags = int ( header flags , 2 ) options [ 'flags' ] = header flags header = version info [ 'header' ] header = header ( * * options ) header = pack ( version info [ 'header format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version info [ 'timestamp format' ] , timestamp ) header = header + timestamp return header + data", "predictions": ["dec the given data into the client"], "references": ["add header to data"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4577, "code": "def read header ( self , data ) : version = self . read version ( data ) version info = self . get version info ( version ) header data = data [ : version info [ 'header size' ] ] header = version info [ 'header' ] header = header . make ( unpack ( version info [ 'header format' ] , header data ) ) header = dict ( header . asdict ( ) ) flags = list ( \"{0:0>8b}\" . format ( header [ 'flags' ] ) ) flags = dict ( version info [ 'flags' ] . make ( flags ) . asdict ( ) ) flags = dict ( ( i , bool ( int ( j ) ) ) for i , j in flags . iteritems ( ) ) header [ 'flags' ] = flags timestamp = None if flags [ 'timestamp' ] : ts start = version info [ 'header size' ] ts end = ts start + version info [ 'timestamp size' ] timestamp data = data [ ts start : ts end ] timestamp = unpack ( version info [ 'timestamp format' ] , timestamp data ) [ 0 ] header [ 'info' ] = { 'timestamp' : timestamp } return header", "predictions": ["hex to parse the to python dictionary"], "references": ["read header from data"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4578, "code": "def remove header ( self , data , options ) : version info = self . get version info ( options [ 'version' ] ) header size = version info [ 'header size' ] if options [ 'flags' ] [ 'timestamp' ] : header size += version info [ 'timestamp size' ] data = data [ header size : ] return data", "predictions": ["oct to a to be created with the received to the client and check the to process and size and size"], "references": ["remove header from data"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 4579, "code": "def read version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version", "predictions": ["bin the to use for some versions"], "references": ["read header version from data"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4580, "code": "def generate key ( pass id , passphrases , salt , algorithm ) : if pass id not in passphrases : raise Exception ( 'Passphrase not defined for id: %d' % pass id ) passphrase = passphrases [ pass id ] if len ( passphrase ) < 32 : raise Exception ( 'Passphrase less than 32 characters long' ) digestmod = Encrypted Pickle . get hashlib ( algorithm [ 'pbkdf2 algorithm' ] ) encoder = PBKDF2 ( passphrase , salt , iterations = algorithm [ 'pbkdf2 iterations' ] , digestmodule = digestmod ) return encoder . read ( algorithm [ 'key size' ] )", "predictions": ["dec a pass to a 32"], "references": ["generate and return pbkdf2 key"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4581, "code": "def update dict ( data , default data , replace data = False ) : if not data : data = default data . copy ( ) return data if not isinstance ( data , dict ) : raise Type Error ( 'Value not dict type' ) if len ( data ) > 255 : raise Value Error ( 'More than 255 values defined' ) for i in data . keys ( ) : if not isinstance ( i , int ) : raise Type Error ( 'Index not int type' ) if i < 0 or i > 255 : raise Value Error ( 'Index value out of range' ) if not replace data : data . update ( default data ) return data", "predictions": ["merge data data with data"], "references": ["update algorithm definition type dictionaries"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4582, "code": "def dump field ( self , fd ) : v = { } v [ 'label' ] = Pbd . LABELS [ fd . label ] v [ 'type' ] = fd . type name if len ( fd . type name ) > 0 else Pbd . TYPES [ fd . type ] v [ 'name' ] = fd . name v [ 'number' ] = fd . number v [ 'default' ] = '[default = {}]' . format ( fd . default value ) if len ( fd . default value ) > 0 else '' f = '{label} {type} {name} = {number} {default};' . format ( * * v ) f = ' ' . join ( f . split ( ) ) self . print ( f ) if len ( fd . type name ) > 0 : self . uses . append ( fd . type name )", "predictions": ["wildcard a to wildcard a to the standard output"], "references": ["dump single field ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4583, "code": "def disassemble ( self ) : ser pb = open ( self . input file , 'rb' ) . read ( ) fd = File Descriptor Proto ( ) fd . Parse From String ( ser pb ) self . name = fd . name self . print ( ) self . print ( 'syntax = \"proto2\";' ) self . print ( '' ) if len ( fd . package ) > 0 : self . print ( 'package {};' . format ( fd . package ) ) self . package = fd . package else : self . print ( '// Package not defined' ) self . walk ( fd )", "predictions": ["convert the package to a temporary file"], "references": ["disassemble serialized protocol buffers file ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4584, "code": "def find imports ( self , pbds ) : imports = list ( set ( self . uses ) . difference ( set ( self . defines ) ) ) for imp in imports : for p in pbds : if imp in p . defines : self . imports . append ( p . name ) break self . imports = list ( set ( self . imports ) ) for import file in self . imports : self . lines . insert ( 2 , 'import \"{}\";' . format ( import file ) )", "predictions": ["convert a list of imports to a python report report"], "references": ["find all missing imports in list of pbd instances ."], "bleu": 0.17827531042796255, "rouge_l": 0.2}
{"id": 4585, "code": "def abf I Dfrom Fname ( fname ) : fname = os . path . abspath ( fname ) basename = os . path . basename ( fname ) return os . path . splitext ( basename ) [ 0 ]", "predictions": ["dec a file name to an executable file name class"], "references": ["given a filename return the abfs id string ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 4586, "code": "def abf Protocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) #it should be in the first 30k of the file f . close ( ) raw = raw . decode ( \"utf-8\" , \"ignore\" ) raw = raw . split ( \"Clampex\" ) [ 1 ] . split ( \".pro\" ) [ 0 ] protocol = os . path . basename ( raw ) protocol ID = protocol . split ( \" \" ) [ 0 ] return protocol ID", "predictions": ["read a protocol from a file"], "references": ["determine the protocol used to record an abf file"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4587, "code": "def header HTML ( header , fname ) : html = \"<html><body><code>\" html += \"<h2>%s</h2>\" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( \"\\n\" , '<br>' ) . replace ( \" \" , \"&nbsp;\" ) html = html . replace ( r\"\\x00\" , \"\" ) html += \"</code></body></html>\" print ( \"saving header file:\" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )", "predictions": ["dump a file as a convert notebook to a file"], "references": ["given the bytestring abf header make and launch html ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4588, "code": "def setsweeps ( self ) : for sweep in range ( self . sweeps ) : self . setsweep ( sweep ) yield self . sweep", "predictions": ["iterate over all sweeps ip addresses ."], "references": ["iterate over every sweep"], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 4589, "code": "def comments load ( self ) : self . comment times , self . comment sweeps , self . comment tags = [ ] , [ ] , [ ] self . comments = 0 self . comment text = \"\" try : self . comment tags = list ( self . AB Fblock . segments [ 0 ] . eventarrays [ 0 ] . annotations [ 'comments' ] ) self . comment times = list ( self . AB Fblock . segments [ 0 ] . eventarrays [ 0 ] . times / self . trace . itemsize ) self . comment sweeps = list ( self . comment times ) except : for events in self . AB Fblock . segments [ 0 ] . events : self . comment tags = events . annotations [ 'comments' ] . tolist ( ) self . comment times = np . array ( events . times . magnitude / self . trace . itemsize ) self . comment sweeps = self . comment times / self . sweep Interval for i , c in enumerate ( self . comment tags ) : self . comment tags [ i ] = c . decode ( \"utf-8\" )", "predictions": ["get set of set set of set set set of set set up set"], "references": ["read the header and populate self with information about comments"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 4590, "code": "def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweep Length : t2 = self . sweep Length self . log . debug ( \"resetting t2 to [%f]\" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( \"t1 cannot be larger than t2\" ) return False I1 , I2 = int ( t1 * self . points Per Sec ) , int ( t2 * self . points Per Sec ) if I1 == I2 : return np . nan return np . average ( self . sweep Y [ I1 : I2 ] )", "predictions": ["set the set of points"], "references": ["return the average of part of the current sweep ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 4591, "code": "def kernel gaussian ( self , size MS , sigma MS = None , forward Only = False ) : sigma MS = size MS / 10 if sigma MS is None else sigma MS size , sigma = size MS * self . points Per Ms , sigma MS * self . points Per Ms self . kernel = swhlab . common . kernel gaussian ( size , sigma , forward Only ) return self . kernel", "predictions": ["return a netmask given a set of size ."], "references": ["create kernel based on this abf info ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4592, "code": "def dict Flat ( l ) : if type ( l ) is dict : return [ l ] if \"numpy\" in str ( type ( l ) ) : return l dicts = [ ] for item in l : if type ( item ) == dict : dicts . append ( item ) elif type ( item ) == list : for item2 in item : dicts . append ( item2 ) return dicts", "predictions": ["turn a list of dictionaries into a list of dictionaries"], "references": ["given a list of list of dicts return just the dicts ."], "bleu": 0.2284389301518129, "rouge_l": 0.44721407624633425}
{"id": 4593, "code": "def matrix Values ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values", "predictions": ["multiply a def def by a given self . ."], "references": ["given a key return a list of values from the matrix with that key ."], "bleu": 0.09614217984757345, "rouge_l": 0.23164556962025318}
{"id": 4594, "code": "def matrix To Dicts ( data ) : if \"float\" in str ( type ( data [ 0 ] ) ) : d = { } for x in range ( len ( data ) ) : d [ data . dtype . names [ x ] ] = data [ x ] return d l = [ ] for y in range ( len ( data ) ) : d = { } for x in range ( len ( data [ y ] ) ) : d [ data . dtype . names [ x ] ] = data [ y ] [ x ] l . append ( d ) return l", "predictions": ["convert a def def to a dictionary ."], "references": ["given a recarray return it as a list of dicts ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 4595, "code": "def html temp launch ( html ) : fname = tempfile . gettempdir ( ) + \"/swhlab/temp.html\" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname )", "predictions": ["creates a def def consumer consumer consumer consumer consumer consumer consumer consumer consumer"], "references": ["given text make it a temporary html file and launch it ."], "bleu": 0.09552040806823771, "rouge_l": 0.08058124174372522}
{"id": 4596, "code": "def check Out ( thing , html = True ) : msg = \"\" for name in sorted ( dir ( thing ) ) : if not \" \" in name : msg += \"<b>%s</b>\\n\" % name try : msg += \" ^-VALUE: %s\\n\" % getattr ( thing , name ) ( ) except : pass if html : html = '<html><body><code>' + msg + '</code></body></html>' html = html . replace ( \" \" , \"&nbsp;\" ) . replace ( \"\\n\" , \"<br>\" ) fname = tempfile . gettempdir ( ) + \"/swhlab/checkout.html\" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname ) print ( msg . replace ( '<b>' , '' ) . replace ( '</b>' , '' ) )", "predictions": ["on self . on self . ."], "references": ["show everything we can about an object s projects and methods ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 4597, "code": "def matrix To HTML ( data , names = None , units = None , book Name = None , sheet Name = None , x Col = None ) : if not names : names = [ \"\" ] * len ( data [ 0 ] ) if data . dtype . names : names = list ( data . dtype . names ) if not units : units = [ \"\" ] * len ( data [ 0 ] ) for i in range ( len ( units ) ) : if names [ i ] in UNITS . keys ( ) : units [ i ] = UNITS [ names [ i ] ] if 'recarray' in str ( type ( data ) ) : #make it a regular array data = data . view ( float ) . reshape ( data . shape + ( - 1 , ) ) if x Col and x Col in names : x Col = names . index ( x Col ) names . insert ( 0 , names [ x Col ] ) units . insert ( 0 , units [ x Col ] ) data = np . insert ( data , 0 , data [ : , x Col ] , 1 ) html Fname = tempfile . gettempdir ( ) + \"/swhlab/WKS-%s.%s.html\" % ( book Name , sheet Name ) html = html += \"<h1>Faux Rigin</h1>\" if book Name or sheet Name : html += '<code><b>%s / %s</b></code><br><br>' % ( book Name , sheet Name ) html += \"<table>\" #cols=list(range(len(names))) col Names = [ '' ] for i in range ( len ( units ) ) : label = \"%s (%d)\" % ( chr ( i + ord ( 'A' ) ) , i ) col Names . append ( label ) html += html List To TR ( col Names , 'label Col' , 'label Col' ) html += html List To TR ( [ 'Long Name' ] + list ( names ) , 'name' , td1Class = 'label Row' ) html += html List To TR ( [ 'Units' ] + list ( units ) , 'units' , td1Class = 'label Row' ) cut Off = False for y in range ( len ( data ) ) : html += html List To TR ( [ y + 1 ] + list ( data [ y ] ) , tr Class = 'data%d' % ( y % 2 ) , td1Class = 'label Row' ) if y >= 200 : cut Off = True break html += \"</table>\" html = html . replace ( \">nan<\" , \">--<\" ) html = html . replace ( \">None<\" , \"><\" ) if cut Off : html += \"<h3>... showing only %d of %d rows ...</h3>\" % ( y , len ( data ) ) html += \"</body></html>\" with open ( html Fname , 'w' ) as f : f . write ( html ) webbrowser . open ( html Fname ) return", "predictions": ["converts a const const to a const const const"], "references": ["put 2d numpy data into a temporary html file ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 4598, "code": "def XM Lto Python ( xml Str = r\"C:\\Apps\\python Modules\\GS Temp.xml\" ) : #TODO: this absolute file path crazy stuff needs to stop! if os . path . exists ( xml Str ) : with open ( xml Str ) as f : xml Str = f . read ( ) print ( xml Str ) print ( \"DONE\" ) return", "predictions": ["generate xml xml xml xml"], "references": ["given a string or a path to an xml file return an xml object ."], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 4599, "code": "def algo exp ( x , m , t , b ) : return m * np . exp ( - t * x ) + b", "predictions": ["compression compression function for self . set"], "references": ["mono - exponential curve ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4600, "code": "def filter gaussian ( Ys , sigma , plot Too = False ) : time A = time . time ( ) window = scipy . signal . gaussian ( len ( Ys ) , sigma ) window /= sum ( window ) Ys2 = np . convolve ( Ys , window , 'same' ) print ( \"LEN:\" , len ( Ys2 ) , len ( Ys ) ) time B = time . time ( ) print ( \"convolution took %.03f ms\" % ( ( time B - time A ) * 1000 ) ) if len ( Ys2 ) != len ( Ys ) : print ( \"?!?!?!? convolution point size mismatch\" ) if plot Too : pylab . plot ( Ys , label = 'original' , alpha = .2 ) pylab . plot ( Ys2 , 'b-' , label = 'smooth' ) pylab . legend ( ) pylab . show ( ) return Ys2", "predictions": ["get a algorithms of a algorithms"], "references": ["simple gaussian convolution . returns same # of points as gotten ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 4601, "code": "def where cross ( data , threshold ) : Is = np . where ( data > threshold ) [ 0 ] Is = np . concatenate ( ( [ 0 ] , Is ) ) Ds = Is [ : - 1 ] - Is [ 1 : ] + 1 return Is [ np . where ( Ds ) [ 0 ] + 1 ]", "predictions": ["computes the options of a options"], "references": ["return a list of is where the data first crosses above threshold ."], "bleu": 0.08180282100568384, "rouge_l": 0.09870550161812298}
{"id": 4602, "code": "def origin Format ( thing ) : if type ( thing ) is list and type ( thing [ 0 ] ) is dict : return origin Format list Of Dicts ( thing ) if type ( thing ) is list and type ( thing [ 0 ] ) is list : return origin Format list Of Dicts ( dict Flat ( thing ) ) else : print ( \" !! I don't know how to format this object!\" ) print ( thing )", "predictions": ["helper function for listing a dictionary of verify that a self ."], "references": ["try to format anything as a 2d matrix with column names ."], "bleu": 0.11498759556447223, "rouge_l": 0.16666666666666666}
{"id": 4603, "code": "def pickle save ( thing , fname ) : pickle . dump ( thing , open ( fname , \"wb\" ) , pickle . HIGHEST PROTOCOL ) return thing", "predictions": ["save function to save . ."], "references": ["save something to a pickle file"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 4604, "code": "def msg Dict ( d , matching = None , sep1 = \"=\" , sep2 = \"\\n\" , sort = True , cant End With = None ) : msg = \"\" if \"record\" in str ( type ( d ) ) : keys = d . dtype . names else : keys = d . keys ( ) if sort : keys = sorted ( keys ) for key in keys : if key [ 0 ] == \" \" : continue if matching : if not key in matching : continue if cant End With and key [ - len ( cant End With ) ] == cant End With : continue if 'float' in str ( type ( d [ key ] ) ) : s = \"%.02f\" % d [ key ] else : s = str ( d [ key ] ) if \"object\" in s : s = '<object>' msg += key + sep1 + s + sep2 return msg . strip ( )", "predictions": ["convert a dictionary to a string suitable for debugging const"], "references": ["convert a dictionary to a pretty formatted string ."], "bleu": 0.47987820666906633, "rouge_l": 0.6376306620209059}
{"id": 4605, "code": "def find Relevant Data ( file List , abfs ) : relevant = [ ] things = { } for abf in abfs : for fname in file List : if abf in fname and not fname in relevant : relevant . append ( fname ) for item in sorted ( relevant ) : thing = os . path . basename ( item ) if \".png\" in thing : continue if not \" \" in thing : continue thing = thing . split ( \" \" ) [ - 1 ] . split ( \".\" ) [ 0 ] if not thing in things . keys ( ) : #prevent overwriting things [ thing ] = item return things", "predictions": ["sign a list of files with a given abfs"], "references": ["return an abf of the * first * of every type of thing ."], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 4606, "code": "def determine Protocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 5000 ) #it should be in the first 5k of the file f . close ( ) proto Comment = \"unknown\" if b\"SWH Lab4[\" in raw : proto Comment = raw . split ( b\"SWH Lab4[\" ) [ 1 ] . split ( b\"]\" , 1 ) [ 0 ] elif b\"SWH[\" in raw : proto Comment = raw . split ( b\"SWH[\" ) [ 1 ] . split ( b\"]\" , 1 ) [ 0 ] else : proto Comment = \"?\" if not type ( proto Comment ) is str : proto Comment = proto Comment . decode ( \"utf-8\" ) return proto Comment", "predictions": ["determine will determine the = = = = 0"], "references": ["determine the comment cooked in the protocol ."], "bleu": 0.18575057999133596, "rouge_l": 0.2378167641325536}
{"id": 4607, "code": "def get Parent ( abf Fname ) : child = os . path . abspath ( abf Fname ) files = sorted ( glob . glob ( os . path . dirname ( child ) + \"/*.*\" ) ) parent ID = abf Fname #its own parent for fname in files : if fname . endswith ( \".abf\" ) and fname . replace ( \".abf\" , \".TIF\" ) in files : parent ID = os . path . basename ( fname ) . replace ( \".abf\" , \"\" ) if os . path . basename ( child ) in fname : break return parent ID", "predictions": ["return version of os . fname ."], "references": ["given an abf file name return the abf of its parent ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 4608, "code": "def get Parent2 ( abf Fname , groups ) : if \".abf\" in abf Fname : abf Fname = os . path . basename ( abf Fname ) . replace ( \".abf\" , \"\" ) for parent ID in groups . keys ( ) : if abf Fname in groups [ parent ID ] : return parent ID return abf Fname", "predictions": ["return a list of parent2 with the given groups ."], "references": ["given an abf and the groups dict return the id of its parent ."], "bleu": 0.11557806568713046, "rouge_l": 0.24270557029177717}
{"id": 4609, "code": "def get Notes For ABF ( abf File ) : parent = get Parent ( abf File ) parent = os . path . basename ( parent ) . replace ( \".abf\" , \"\" ) exp File = os . path . dirname ( abf File ) + \"/experiment.txt\" if not os . path . exists ( exp File ) : return \"no experiment file\" with open ( exp File ) as f : raw = f . readlines ( ) for line in raw : if line [ 0 ] == '~' : line = line [ 1 : ] . strip ( ) if line . startswith ( parent ) : while \"\\t\\t\" in line : line = line . replace ( \"\\t\\t\" , \"\\t\" ) line = line . replace ( \"\\t\" , \"\\n\" ) return line return \"experiment.txt found, but didn't contain %s\" % parent", "predictions": ["get the raw lines of the experiment"], "references": ["given an abf find the parent return that line of experiments . txt"], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 4610, "code": "def get I Ds From Files ( files ) : if type ( files ) is str : files = glob . glob ( files + \"/*.*\" ) I Ds = [ ] for fname in files : if fname [ - 4 : ] . lower ( ) == '.abf' : ext = fname . split ( '.' ) [ - 1 ] I Ds . append ( os . path . basename ( fname ) . replace ( '.' + ext , '' ) ) return sorted ( I Ds )", "predictions": ["return list of all files in a given directory"], "references": ["given a path or list of files return abf ids ."], "bleu": 0.18382919692249333, "rouge_l": 0.2946859903381642}
{"id": 4611, "code": "def inspect ABF ( abf = example ABF , save Too = False , just Plot = False ) : pylab . close ( 'all' ) print ( \" ~~ inspect ABF()\" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , force New Figure = True ) if abf . sweep Interval * abf . sweeps < 60 * 5 : #shorter than 5 minutes pylab . subplot ( 211 ) pylab . title ( \"%s [%s]\" % ( abf . ID , abf . proto Comment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( \" -- plotting as long recording\" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( \"%s [%s]\" % ( abf . ID , abf . proto Comment ) ) swhlab . plot . annotate ( abf ) if just Plot : return if save Too : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , \" \" + basename . replace ( \".abf\" , \".png\" ) ) ) pylab . show ( ) return", "predictions": ["inspect a series of standard segments and inspect the result of a single frame ."], "references": ["may be given an abf object or filename ."], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 4612, "code": "def ftp login ( folder = None ) : pw Dir = os . path . realpath ( file ) for i in range ( 3 ) : pw Dir = os . path . dirname ( pw Dir ) pw File = os . path . join ( pw Dir , \"passwd.txt\" ) print ( \" -- looking for login information in:\\n   [%s]\" % pw File ) try : with open ( pw File ) as f : lines = f . readlines ( ) username = lines [ 0 ] . strip ( ) password = lines [ 1 ] . strip ( ) print ( \" -- found a valid username/password\" ) except : print ( \" -- password lookup FAILED.\" ) username = TK ask Password ( \"FTP LOGIN\" , \"enter FTP username\" ) password = TK ask Password ( \"FTP LOGIN\" , \"enter password for %s\" % username ) if not username or not password : print ( \" !! failed getting login info. aborting FTP effort.\" ) return print ( \"      username:\" , username ) print ( \"      password:\" , \"*\" * ( len ( password ) ) ) print ( \" -- logging in to FTP ...\" ) try : ftp = ftplib . FTP ( \"swharden.com\" ) ftp . login ( username , password ) if folder : ftp . cwd ( folder ) return ftp except : print ( \" !! login failure !!\" ) return False", "predictions": ["wait for ftp information to ftp"], "references": ["return an ftp object after logging in ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4613, "code": "def ftp folder match ( ftp , local Folder , delete Stuff = True ) : for fname in glob . glob ( local Folder + \"/*.*\" ) : ftp upload ( ftp , fname ) return", "predictions": ["upload files to ftp folder"], "references": ["upload everything from localfolder into the current ftp folder ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 4614, "code": "def version upload ( fname , username = \"nibjb\" ) : print ( \"popping up pasword window...\" ) password = TK ask Password ( \"FTP LOGIN\" , \"enter password for %s\" % username ) if not password : return print ( \"username:\" , username ) print ( \"password:\" , \"*\" * ( len ( password ) ) ) print ( \"connecting...\" ) ftp = ftplib . FTP ( \"swharden.com\" ) ftp . login ( username , password ) print ( \"successful login!\" ) ftp . cwd ( \"/software/swhlab/versions\" ) #IMMEDIATELY GO HERE!!! print ( \"uploading\" , os . path . basename ( fname ) ) ftp . storbinary ( \"STOR \" + os . path . basename ( fname ) , open ( fname , \"rb\" ) , 1024 ) #for binary files print ( \"disconnecting...\" ) ftp . quit ( )", "predictions": ["upload version of ftp"], "references": ["only scott should do this . upload new version to site ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 4615, "code": "def TK ask Password ( title = \"input\" , msg = \"type here:\" ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( \"-topmost\" , True ) #always on top root . lift ( ) #bring to top value = tkinter . simpledialog . askstring ( title , msg ) root . destroy ( ) return value", "predictions": ["ask user for a tk tk window"], "references": ["use the gui to ask for a string ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 4616, "code": "def TK message ( title , msg ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( \"-topmost\" , True ) #always on top root . lift ( ) #bring to top tkinter . messagebox . showwarning ( title , msg ) root . destroy ( )", "predictions": ["tk a message with the top level level"], "references": ["use the gui to pop up a message ."], "bleu": 0.20014292374951972, "rouge_l": 0.232824427480916}
{"id": 4617, "code": "def TK ask ( title , msg ) : root = tkinter . Tk ( ) root . attributes ( \"-topmost\" , True ) #always on top root . withdraw ( ) #hide tk window result = tkinter . messagebox . askyesno ( title , msg ) root . destroy ( ) return result", "predictions": ["ask the user for a tk tk tk tk"], "references": ["use the gui to ask yes or no ."], "bleu": 0.15619699684601276, "rouge_l": 0.1111111111111111}
{"id": 4618, "code": "def process Args ( ) : if len ( sys . argv ) < 2 : print ( \"\\n\\n ERROR:\" ) print ( \"this script requires arguments!\" ) print ( 'try \"python command.py info\"' ) return if sys . argv [ 1 ] == 'info' : print ( \"import paths:\\n \" , \"\\n  \" . join ( sys . path ) ) print ( ) print ( \"python version:\" , sys . version ) print ( \"SWH Lab path:\" , file ) print ( \"SWH Lab version:\" , swhlab . version ) return if sys . argv [ 1 ] == 'glance Folder' : abf Folder = swhlab . common . gui get Folder ( ) if not abf Folder or not os . path . isdir ( abf Folder ) : print ( \"bad path\" ) return fnames = sorted ( glob . glob ( abf Folder + \"/*.abf\" ) ) out Folder = tempfile . gettempdir ( ) + \"/swhlab/\" if os . path . exists ( out Folder ) : shutil . rmtree ( out Folder ) os . mkdir ( out Folder ) out File = out Folder + \"/index.html\" out = '<html><body>' out += '<h2>%s</h2>' % abf Folder for i , fname in enumerate ( fnames ) : print ( % ( i , len ( fnames ) ) ) save As = os . path . join ( os . path . dirname ( out Folder ) , os . path . basename ( fname ) ) + \".png\" out += '<br><br><br><code>%s</code><br>' % os . path . abspath ( fname ) out += '<a href=\"%s\"><img src=\"%s\"></a><br>' % ( save As , save As ) swhlab . analysis . glance . process Abf ( fname , save As ) out += '</body></html>' with open ( out File , 'w' ) as f : f . write ( out ) webbrowser . open new tab ( out File ) return print ( \"\\n\\n ERROR:\\n I'm not sure how to process these arguments!\" ) print ( sys . argv )", "predictions": ["parse the command line arguments and run the script ."], "references": ["check out the arguments and figure out what to do ."], "bleu": 0.1705647399369684, "rouge_l": 0.37770897832817335}
{"id": 4619, "code": "def stats first ( abf ) : msg = \"\" for sweep in range ( abf . sweeps ) : for AP in abf . A Ps [ sweep ] : for key in sorted ( AP . keys ( ) ) : if key [ - 1 ] is \"I\" or key [ - 2 : ] in [ \"I1\" , \"I2\" ] : continue msg += \"%s = %s\\n\" % ( key , AP [ key ] ) return msg", "predictions": ["return the first stats of the abf"], "references": ["provide all stats on the first ap ."], "bleu": 0.22772101321113858, "rouge_l": 0.2634989200863931}
{"id": 4620, "code": "def get Avg By Sweep ( abf , feature , T0 = None , T1 = None ) : if T1 is None : T1 = abf . sweep Length if T0 is None : T0 = 0 data = [ np . empty ( ( 0 ) ) ] * abf . sweeps for AP in cm . dict Flat ( cm . matrix To Dicts ( abf . A Ps ) ) : if T0 < AP [ 'sweep T' ] < T1 : val = AP [ feature ] data [ int ( AP [ 'sweep' ] ) ] = np . concatenate ( ( data [ int ( AP [ 'sweep' ] ) ] , [ val ] ) ) for sweep in range ( abf . sweeps ) : if len ( data [ sweep ] ) > 1 and np . any ( data [ sweep ] ) : data [ sweep ] = np . nanmean ( data [ sweep ] ) elif len ( data [ sweep ] ) == 1 : data [ sweep ] = data [ sweep ] [ 0 ] else : data [ sweep ] = np . nan return data", "predictions": ["return a single column given a abf ."], "references": ["return average of a feature divided by sweep ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 4621, "code": "def gain ( abf ) : Ys = np . nan to num ( swhlab . ap . get Avg By Sweep ( abf , 'freq' ) ) Xs = abf . clamp Values ( abf . data X [ int ( abf . proto Seq X [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = \"gain function\" , xlabel = \"command current (p A)\" , ylabel = \"average inst. freq. (Hz)\" ) pylab . plot ( Xs , Ys , '.-' , ms = 20 , alpha = .5 , color = 'b' ) pylab . axhline ( 0 , alpha = .5 , lw = 2 , color = 'r' , ls = \"--\" ) pylab . margins ( .1 , .1 )", "predictions": ["gain the gain of the pylab data"], "references": ["easy way to plot a gain function ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4622, "code": "def comments ( abf , minutes = False ) : if not len ( abf . comment Times ) : return for i in range ( len ( abf . comment Times ) ) : t , c = abf . comment Times [ i ] , abf . comment Tags [ i ] if minutes : t = t / 60 pylab . axvline ( t , lw = 1 , color = 'r' , ls = \"--\" , alpha = .5 ) X1 , X2 , Y1 , Y2 = pylab . axis ( ) Y2 = Y2 - abs ( Y2 - Y1 ) * .02 pylab . text ( t , Y2 , c , size = 8 , color = 'r' , rotation = 'vertical' , ha = 'right' , va = 'top' , weight = 'bold' , alpha = .5 ) if minutes : pylab . xlabel ( \"minutes\" ) else : pylab . xlabel ( \"seconds\" )", "predictions": ["get comments from abf"], "references": ["draw vertical lines at comment points . defaults to seconds ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 4623, "code": "def annotate ( abf ) : msg = \"SWH Lab %s \" % str ( swhlab . VERSION ) msg += \"ID:%s \" % abf . ID msg += \"CH:%d \" % abf . channel msg += \"PROTOCOL:%s \" % abf . proto Comment msg += \"COMMAND: %d%s \" % ( abf . holding , abf . units ) msg += \"GENERATED:%s \" % '{0:%Y-%m-%d %H:%M:%S}' . format ( datetime . datetime . now ( ) ) pylab . annotate ( msg , ( .001 , .001 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'bottom' , color = '#999999' , family = 'monospace' , size = 8 , weight = 'bold' ) if abf . n ADC > 1 : msg = \"Ch %d/%d\" % ( abf . channel + 1 , abf . n ADC ) pylab . annotate ( msg , ( .01 , .99 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'top' , color = '#FF0000' , family = 'monospace' , size = 12 , weight = 'bold' )", "predictions": ["annotate an pylab message with a channel ."], "references": ["stamp the bottom with file info ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4624, "code": "def try Loading From ( try Path , module Name = 'swhlab' ) : if not 'site-packages' in swhlab . file : print ( \"loaded custom swhlab module from\" , os . path . dirname ( swhlab . file ) ) return while len ( try Path ) > 5 : sp = try Path + \"/swhlab/\" if os . path . isdir ( sp ) and os . path . exists ( sp + \"/ init .py\" ) : if not os . path . dirname ( try Path ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( try Path ) ) print ( \"#\" * 80 ) print ( ) print ( \"#\" * 80 ) try Path = os . path . dirname ( try Path ) return", "predictions": ["try to try to try to try to try to try to try to print a module ."], "references": ["if the module is in this path load it from the local folder ."], "bleu": 0.07535838128770536, "rouge_l": 0.1278825995807128}
{"id": 4625, "code": "def show ( self ) : copied = self . copy ( ) enumerated = [ el for el in enumerate ( copied ) ] for ( group ind , specs ) in enumerated : if len ( enumerated ) > 1 : print ( \"Group %d\" % group ind ) ordering = self . constant keys + self . varying keys spec lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering ] ) for s in specs ] print ( '\\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec lines ) ] ) ) print ( 'Remaining arguments not available for %s' % self . class . name )", "predictions": ["print a formatted summary of the user arguments ."], "references": ["when dynamic not all argument values may be available ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 4626, "code": "def analyze ( fname = False , save = True , show = None ) : if fname and os . path . exists ( fname . replace ( \".abf\" , \".rst\" ) ) : print ( \"SKIPPING DUE TO RST FILE\" ) return swhlab . plotting . core . IMAGE SAVE = save if show is None : if cm . is Ipython ( ) : swhlab . plotting . core . IMAGE SHOW = True else : swhlab . plotting . core . IMAGE SHOW = False #swhlab.plotting.core.IMAGE SHOW=show abf = ABF ( fname ) print ( \">>>>> PROTOCOL >>>>>\" , abf . protocomment ) run Function = \"proto unknown\" if \"proto \" + abf . protocomment in globals ( ) : run Function = \"proto \" + abf . protocomment abf . log . debug ( \"running %s()\" % ( run Function ) ) plt . close ( 'all' ) globals ( ) [ run Function ] ( abf ) try : globals ( ) [ run Function ] ( abf ) except : abf . log . error ( \"EXCEPTION DURING PROTOCOL FUNCTION\" ) abf . log . error ( sys . exc info ( ) [ 0 ] ) return \"ERROR\" plt . close ( 'all' ) return \"SUCCESS\"", "predictions": ["analyze a \"success\" file"], "references": ["given a filename or abf object try to analyze it ."], "bleu": 0.06909866532427987, "rouge_l": 0.12298387096774194}
{"id": 4627, "code": "def figure ( self , force New = False ) : if plt . pylab helpers . Gcf . get num fig managers ( ) > 0 and force New is False : self . log . debug ( \"figure already seen, not creating one.\" ) return if self . subplot : self . log . debug ( \"subplot mode enabled, not creating new figure\" ) else : self . log . debug ( \"creating new figure\" ) plt . figure ( figsize = ( self . figure width , self . figure height ) )", "predictions": ["set the figure to use for a new figure"], "references": ["make sure a figure is ready ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4628, "code": "def save ( self , callit = \"misc\" , close Too = True , fullpath = False ) : if fullpath is False : fname = self . abf . out Pre + \"plot \" + callit + \".jpg\" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( \"saved [%s]\" , os . path . basename ( fname ) ) if close Too : plt . close ( )", "predictions": ["save a file to a directory"], "references": ["save the existing figure . does not close it ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 4629, "code": "def figure sweeps ( self , offset X = 0 , offset Y = 0 ) : self . log . debug ( \"creating overlayed sweeps plot\" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) self . set Color By Sweep ( ) plt . plot ( self . abf . sweep X2 + sweep * offset X , self . abf . sweep Y + sweep * offset Y , * * self . kwargs ) if offset X : self . margin X = .05 self . decorate ( )", "predictions": ["run the sweeps and the sweeps tasks ."], "references": ["plot every sweep of an abf file ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4630, "code": "def figure protocol ( self ) : self . log . debug ( \"creating overlayed protocols plot\" ) self . figure ( ) plt . plot ( self . abf . proto X , self . abf . proto Y , color = 'r' ) self . margin X = 0 self . decorate ( protocol = True )", "predictions": ["run the figure and decorate the protocol protocol ."], "references": ["plot the current sweep protocol ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 4631, "code": "def figure protocols ( self ) : self . log . debug ( \"creating overlayed protocols plot\" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) plt . plot ( self . abf . proto X , self . abf . proto Y , color = 'r' ) self . margin X = 0 self . decorate ( protocol = True )", "predictions": ["run all figure protocols"], "references": ["plot the protocol of all sweeps ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4632, "code": "def frames ( fname = None , menu Width = 200 , launch = False ) : html = % ( menu Width ) with open ( fname , 'w' ) as f : f . write ( html ) if launch : webbrowser . open ( fname )", "predictions": ["write a menu to a file"], "references": ["create and save a two column frames html file ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4633, "code": "def files By Extension ( fnames ) : by Ext = { \"abf\" : [ ] , \"jpg\" : [ ] , \"tif\" : [ ] } for fname in fnames : ext = os . path . splitext ( fname ) [ 1 ] . replace ( \".\" , '' ) . lower ( ) if not ext in by Ext . keys ( ) : by Ext [ ext ] = [ ] by Ext [ ext ] = by Ext [ ext ] + [ fname ] return by Ext", "predictions": ["get list of files that have a given extension ."], "references": ["given a list of files return a dict organized by extension ."], "bleu": 0.2454736055990765, "rouge_l": 0.5366568914956013}
{"id": 4634, "code": "def files By Cell ( fnames , cells ) : by Cell = { } fnames = smart Sort ( fnames ) days = list ( set ( [ elem [ : 5 ] for elem in fnames if elem . endswith ( \".abf\" ) ] ) ) for day in smart Sort ( days ) : parent = None for i , fname in enumerate ( [ elem for elem in fnames if elem . startswith ( day ) and elem . endswith ( \".abf\" ) ] ) : ID = os . path . splitext ( fname ) [ 0 ] if len ( [ x for x in fnames if x . startswith ( ID ) ] ) - 1 : parent = ID if not parent in by Cell : by Cell [ parent ] = [ ] by Cell [ parent ] = by Cell [ parent ] + [ fname ] return by Cell", "predictions": ["get a list of files that have a directory ."], "references": ["given files and cells return a dict of files grouped by cell ."], "bleu": 0.13964659797714432, "rouge_l": 0.33983286908078}
{"id": 4635, "code": "def folder Scan ( self , abf Folder = None ) : if abf Folder is None and 'abf Folder' in dir ( self ) : abf Folder = self . abf Folder else : self . abf Folder = abf Folder self . abf Folder = os . path . abspath ( self . abf Folder ) self . log . info ( \"scanning [%s]\" , self . abf Folder ) if not os . path . exists ( self . abf Folder ) : self . log . error ( \"path doesn't exist: [%s]\" , abf Folder ) return self . abf Folder2 = os . path . abspath ( self . abf Folder + \"/swhlab/\" ) if not os . path . exists ( self . abf Folder2 ) : self . log . error ( \"./swhlab/ doesn't exist. creating it...\" ) os . mkdir ( self . abf Folder2 ) self . fnames = os . listdir ( self . abf Folder ) self . fnames2 = os . listdir ( self . abf Folder2 ) self . log . debug ( \"./ has %d files\" , len ( self . fnames ) ) self . log . debug ( \"./swhlab/ has %d files\" , len ( self . fnames2 ) ) self . fnames By Ext = files By Extension ( self . fnames ) if not \"abf\" in self . fnames By Ext . keys ( ) : self . log . error ( \"no ABF files found\" ) self . log . debug ( \"found %d AB Fs\" , len ( self . fnames By Ext [ \"abf\" ] ) ) self . cells = find Cells ( self . fnames ) self . log . debug ( \"found %d cells\" % len ( self . cells ) ) self . fnames By Cell = files By Cell ( self . fnames , self . cells ) self . log . debug ( \"grouped cells by number of source files: %s\" % str ( [ len ( self . fnames By Cell [ elem ] ) for elem in self . fnames By Cell ] ) )", "predictions": ["called when a folder may be retried"], "references": ["populate class properties relating to files in the folder ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4636, "code": "def html index splash ( self ) : html = % version . version #html+='<code>%s</code><br><br>'%self.abf Folder #html+='<hr>' for parent in smart Sort ( self . fnames By Cell . keys ( ) ) : html += '<br><b><a href=\"%s.html\">%s</a></b><br>' % ( parent , parent ) for child in self . fnames By Cell [ parent ] : fullpath = os . path . join ( self . abf Folder , child ) protocol = swhlab . swh abf . abf Protocol ( fullpath ) html += '<code>%s[%s]</code><br>' % ( fullpath , protocol ) style . save ( html , self . abf Folder2 + \"/index splash.html\" ) return", "predictions": ["generate an html file containing all the html link ."], "references": ["generate landing page ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 4637, "code": "def html single All ( self , template = \"basic\" ) : for fname in smart Sort ( self . cells ) : if template == \"fixed\" : self . html single fixed ( fname ) else : self . html single basic ( fname )", "predictions": ["generate html for all cells in the page ."], "references": ["generate a data view for every abf in the project folder ."], "bleu": 0.158278836853973, "rouge_l": 0.4642313546423136}
{"id": 4638, "code": "def proto 01 01 HP010 ( abf = example ABF ) : swhlab . memtest . memtest ( abf ) #knows how to do IC memtest swhlab . memtest . check Sweep ( abf ) #lets you eyeball check how it did swhlab . plot . save ( abf , tag = \"tau\" )", "predictions": ["generate a georaster georaster georaster"], "references": ["hyperpolarization step . use to calculate tau and stuff ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 4639, "code": "def proto 01 12 steps025 ( abf = example ABF ) : swhlab . ap . detect ( abf ) standard grouping For Inj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot values ( abf , feature , continuous = False ) #plot AP info swhlab . plot . save ( abf , tag = 'A ' + feature ) swhlab . plot . gain ( abf ) #easy way to do a gain function! swhlab . plot . save ( abf , tag = '05-gain' )", "predictions": ["generate the 12 plots"], "references": ["ic steps . use to determine gain function ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 4640, "code": "def proto 01 13 steps025dual ( abf = example ABF ) : swhlab . ap . detect ( abf ) standard grouping For Inj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot values ( abf , feature , continuous = False ) #plot AP info swhlab . plot . save ( abf , tag = 'A ' + feature ) f1 = swhlab . ap . get Avg By Sweep ( abf , 'freq' , None , 1 ) f2 = swhlab . ap . get Avg By Sweep ( abf , 'freq' , 1 , None ) f1 = np . nan to num ( f1 ) f2 = np . nan to num ( f2 ) Xs = abf . clamp Values ( abf . data X [ int ( abf . proto Seq X [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = \"gain function\" , xlabel = \"command current (p A)\" , ylabel = \"average inst. freq. (Hz)\" ) pylab . plot ( Xs , f1 , '.-' , ms = 20 , alpha = .5 , label = \"step 1\" , color = 'b' ) pylab . plot ( Xs , f2 , '.-' , ms = 20 , alpha = .5 , label = \"step 2\" , color = 'r' ) pylab . legend ( loc = 'upper left' ) pylab . axis ( [ Xs [ 0 ] , Xs [ - 1 ] , None , None ] ) swhlab . plot . save ( abf , tag = 'gain' )", "predictions": ["plots plots of the example abf = 0 = 0 = 0 = 1 = 0 = 0 = 0 = 0 = 1 = 1 = 0 = 0 0"], "references": ["ic steps . see how hyperpol . step affects things ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4641, "code": "def proto 02 01 MT70 ( abf = example ABF ) : standard overlay With Average ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . check Sweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False )", "predictions": ["saves the example instance with a standard control exp"], "references": ["repeated membrane tests ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4642, "code": "def proto 02 03 I Vfast ( abf = example ABF ) : av1 , sd1 = swhlab . plot . IV ( abf , .6 , .9 , True ) swhlab . plot . save ( abf , tag = 'iv1' ) Xs = abf . clamp Values ( .6 ) #generate IV clamp values abf . save Thing ( [ Xs , av1 ] , 'iv' )", "predictions": ["runs the basic randomly plots"], "references": ["fast sweeps 1 step per sweep for clean iv without fast currents ."], "bleu": 0.04635036983311895, "rouge_l": 0.0}
{"id": 4643, "code": "def proto 04 01 M Tmon70s2 ( abf = example ABF ) : standard inspect ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . check Sweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False ) swhlab . memtest . plot standard4 ( abf ) swhlab . plot . save ( abf , tag = 'memtests' )", "predictions": ["generate a trains plots"], "references": ["repeated membrane tests likely with drug added . maybe ipscs ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 4644, "code": "def proto VC 50 MT IV ( abf = example ABF ) : swhlab . memtest . memtest ( abf ) #do membrane test on every sweep swhlab . memtest . check Sweep ( abf ) #see all MT values swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clamp Values ( 1.2 ) #generate IV clamp values abf . save Thing ( [ Xs , av1 ] , '01 iv' )", "predictions": ["call the default in 50 plots"], "references": ["combination of membrane test and iv steps ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 4645, "code": "def index Images ( folder , fname = \"index.html\" ) : #TODO: REMOVE html = \"<html><body>\" for item in glob . glob ( folder + \"/*.*\" ) : if item . split ( \".\" ) [ - 1 ] in [ 'jpg' , 'png' ] : html += \"<h3>%s</h3>\" % os . path . basename ( item ) html += '<img src=\"%s\">' % os . path . basename ( item ) html += '<br>' * 10 html += \"</html></body>\" f = open ( folder + \"/\" + fname , 'w' ) f . write ( html ) f . close print ( \"indexed:\" ) print ( \"  \" , os . path . abspath ( folder + \"/\" + fname ) ) return", "predictions": ["ftp a folder to the folder file"], "references": ["obsolete way to index a folder ."], "bleu": 0.2626909894424158, "rouge_l": 0.2857142857142857}
{"id": 4646, "code": "def save ( self , * args , * * kwargs ) : current activable value = getattr ( self , self . ACTIVATABLE FIELD NAME ) is active changed = self . id is None or self . original activatable value != current activable value self . original activatable value = current activable value ret val = super ( Base Activatable Model , self ) . save ( * args , * * kwargs ) if is active changed : model activations changed . send ( self . class , instance ids = [ self . id ] , is active = current activable value ) if self . activatable field updated : model activations updated . send ( self . class , instance ids = [ self . id ] , is active = current activable value ) return ret val", "predictions": ["saves the print instance and sends the active to the current"], "references": ["a custom save method that handles figuring out when something is activated or deactivated ."], "bleu": 0.06658411377036827, "rouge_l": 0.0}
{"id": 4647, "code": "def delete ( self , force = False , * * kwargs ) : if force : return super ( Base Activatable Model , self ) . delete ( * * kwargs ) else : setattr ( self , self . ACTIVATABLE FIELD NAME , False ) return self . save ( update fields = [ self . ACTIVATABLE FIELD NAME ] )", "predictions": ["delete delete delete instance withdraw withdraw withdraw withdraw withdraw withdraw withdraw withdraw"], "references": ["it is impossible to delete an activatable model unless force is true . this function instead sets it to inactive ."], "bleu": 0.04908031219980175, "rouge_l": 0.05776515151515151}
{"id": 4648, "code": "def show ( self , args , file handle = None , * * kwargs ) : full string = '' info = { 'root directory' : '<root directory>' , 'batch name' : '<batch name>' , 'batch tag' : '<batch tag>' , 'batch description' : '<batch description>' , 'launcher' : '<launcher>' , 'timestamp format' : '<timestamp format>' , 'timestamp' : tuple ( time . localtime ( ) ) , 'varying keys' : args . varying keys , 'constant keys' : args . constant keys , 'constant items' : args . constant items } quoted cmds = [ subprocess . list2cmdline ( [ el for el in self ( self . formatter ( s ) , '<tid>' , info ) ] ) for s in args . specs ] cmd lines = [ '%d: %s\\n' % ( i , qcmds ) for ( i , qcmds ) in enumerate ( quoted cmds ) ] full string += '' . join ( cmd lines ) if file handle : file handle . write ( full string ) file handle . flush ( ) else : print ( full string )", "predictions": ["show for very very very very very very very very very 8"], "references": ["write to file_handle if supplied othewise print output"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 4649, "code": "def cross check launchers ( self , launchers ) : if len ( launchers ) == 0 : raise Exception ( 'Empty launcher list' ) timestamps = [ launcher . timestamp for launcher in launchers ] if not all ( timestamps [ 0 ] == tstamp for tstamp in timestamps ) : raise Exception ( \"Launcher timestamps not all equal. \" \"Consider setting timestamp explicitly.\" ) root directories = [ ] for launcher in launchers : command = launcher . command args = launcher . args command . verify ( args ) root directory = launcher . get root directory ( ) if os . path . isdir ( root directory ) : raise Exception ( \"Root directory already exists: %r\" % root directory ) if root directory in root directories : raise Exception ( \"Each launcher requires a unique root directory\" ) root directories . append ( root directory )", "predictions": ["raise ask for all on on the cross"], "references": ["performs consistency checks across all the launchers ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4650, "code": "def launch all ( self , launchers ) : for launcher in launchers : print ( \"== Launching  %s ==\" % launcher . batch name ) launcher ( ) return True", "predictions": ["process all the nodes in a single batch"], "references": ["launches all available launchers ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4651, "code": "def review all ( self , launchers ) : if self . launch args is not None : proceed = self . review args ( self . launch args , show repr = True , heading = 'Meta Arguments' ) if not proceed : return False reviewers = [ self . review args , self . review command , self . review launcher ] for ( count , launcher ) in enumerate ( launchers ) : if not all ( reviewer ( launcher ) for reviewer in reviewers ) : print ( \"\\n == Aborting launch ==\" ) return False if len ( launchers ) != 1 and count < len ( launchers ) - 1 : skip remaining = self . input options ( [ 'Y' , 'n' , 'quit' ] , '\\n Skip remaining reviews?' , default = 'y' ) if skip remaining == 'y' : break elif skip remaining == 'quit' : return False if self . input options ( [ 'y' , 'N' ] , 'Execute?' , default = 'n' ) != 'y' : return False else : return self . launch all ( launchers )", "predictions": ["stats to first input"], "references": ["runs the review process for all the launchers ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 4652, "code": "def input options ( self , options , prompt = 'Select option' , default = None ) : check options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )", "predictions": ["get the get options options options from the command line ."], "references": ["helper to prompt the user for input on the commandline ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 4653, "code": "def save ( self , filename , imdata , * * data ) : if isinstance ( imdata , numpy . ndarray ) : imdata = Image . fromarray ( numpy . uint8 ( imdata ) ) elif isinstance ( imdata , Image . Image ) : imdata . save ( self . savepath ( filename ) )", "predictions": ["gain a pickled image file ap to a file ap by the given filename ap ap ap ap ap ap ap ap ap ap ap ap ap ap ap ap ap"], "references": ["data may be either a pil image object or a numpy array ."], "bleu": 0.046398855339878003, "rouge_l": 0.14722445695897024}
{"id": 4654, "code": "def file Modified Timestamp ( fname ) : modified Time = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modified Time ) ) return stamp", "predictions": ["modified a comments to a comments"], "references": ["return yyyy - mm - dd when the file was modified ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 4655, "code": "def load Results ( results File ) : with open ( results File ) as f : raw = f . read ( ) . split ( \"\\n\" ) folders By Day = { } for line in raw : folder = line . split ( '\"' ) [ 1 ] + \"\\\\\" line = [ ] + line . split ( '\"' ) [ 2 ] . split ( \", \" ) for day in line [ 1 : ] : if not day in folders By Day : folders By Day [ day ] = [ ] folders By Day [ day ] = folders By Day [ day ] + [ folder ] n Active Days = len ( folders By Day ) day First = sorted ( folders By Day . keys ( ) ) [ 0 ] day Last = sorted ( folders By Day . keys ( ) ) [ - 1 ] day First = datetime . datetime . strptime ( day First , \"%Y-%m-%d\" ) day Last = datetime . datetime . strptime ( day Last , \"%Y-%m-%d\" ) n Days = ( day Last - day First ) . days + 1 empty Days = 0 for delta Days in range ( n Days ) : day = day First + datetime . timedelta ( days = delta Days ) stamp = datetime . datetime . strftime ( day , \"%Y-%m-%d\" ) if not stamp in folders By Day : folders By Day [ stamp ] = [ ] empty Days += 1 perc Active = n Active Days / n Days * 100 print ( \"%d of %d days were active (%.02f%%)\" % ( n Active Days , n Days , perc Active ) ) return folders By Day", "predictions": ["annotate results file with the most recent content of the results"], "references": ["returns a dict of active folders with days as keys ."], "bleu": 0.12605968092174913, "rouge_l": 0.09090909090909091}
{"id": 4656, "code": "def abfinfo ( self , print Too = False , return Dict = False ) : info = d = { } for thing Name in sorted ( dir ( self ) ) : if thing Name in [ 'cm' , 'ev Is' , 'colormap' , 'data X' , 'data Y' , 'proto X' , 'proto Y' ] : continue if \" \" in thing Name : continue thing = getattr ( self , thing Name ) if type ( thing ) is list and len ( thing ) > 5 : continue thing Type = str ( type ( thing ) ) . split ( \"'\" ) [ 1 ] if \"method\" in thing Type or \"neo.\" in thing Type : continue if thing Name in [ \"header\" , \"MT\" ] : continue info += \"%s <%s> %s\\n\" % ( thing Name , thing Type , thing ) d [ thing Name ] = thing if print Too : print ( ) for line in info . split ( \"\\n\" ) : if len ( line ) < 3 : continue print ( \"   \" , line ) print ( ) if return Dict : return d return info", "predictions": ["print a list of information about the site"], "references": ["show basic info about abf class variables ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4657, "code": "def header HTML ( self , fname = None ) : if fname is None : fname = self . fname . replace ( \".abf\" , \" header.html\" ) html = \"<html><body><code>\" html += \"<h2>abfinfo() for %s.abf</h2>\" % self . ID html += self . abfinfo ( ) . replace ( \"<\" , \"&lt;\" ) . replace ( \">\" , \"&gt;\" ) . replace ( \"\\n\" , \"<br>\" ) html += \"<h2>Header for %s.abf</h2>\" % self . ID html += pprint . pformat ( self . header , indent = 1 ) html = html . replace ( \"\\n\" , '<br>' ) . replace ( \" \" , \"&nbsp;\" ) html = html . replace ( r\"\\x00\" , \"\" ) html += \"</code></body></html>\" print ( \"WRITING HEADER TO:\" ) print ( fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( )", "predictions": ["available show of the show show show"], "references": ["read the abf header and save it html formatted ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4658, "code": "def generate colormap ( self , colormap = None , reverse = False ) : if colormap is None : colormap = pylab . cm . Dark2 self . cm = colormap self . colormap = [ ] for i in range ( self . sweeps ) : #TODO: make this the only colormap self . colormap . append ( colormap ( i / self . sweeps ) ) if reverse : self . colormap . reverse ( )", "predictions": ["analyze the colormap colormap on the colormap"], "references": ["use 1 colormap for the whole abf . you can change it! ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 4659, "code": "def filter gaussian ( self , sigma Ms = 100 , apply Filtered = False , apply Baseline = False ) : if sigma Ms == 0 : return self . data Y filtered = cm . filter gaussian ( self . data Y , sigma Ms ) if apply Baseline : self . data Y = self . data Y - filtered elif apply Filtered : self . data Y = filtered else : return filtered", "predictions": ["figure out a gaussian managers"], "references": ["returns filtered trace . desn t filter it in place ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 4660, "code": "def to table ( args , vdims = [ ] ) : if not Table : return \"Holo Views Table not available\" kdims = [ dim for dim in args . constant keys + args . varying keys if dim not in vdims ] items = [ tuple ( [ spec [ k ] for k in kdims + vdims ] ) for spec in args . specs ] return Table ( items , kdims = kdims , vdims = vdims )", "predictions": ["convert a list of table names to a table table"], "references": ["helper function to convet an args object to a holoviews table"], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 4661, "code": "def spec formatter ( cls , spec ) : return type ( spec ) ( ( k , str ( v ) ) for ( k , v ) in spec . items ( ) )", "predictions": ["= figure figure out of figure classes"], "references": ["formats the elements of an argument set appropriately"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4662, "code": "def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L", "predictions": ["return are - ."], "references": ["simple replacement for numpy linspace"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 4663, "code": "def load expansion ( self , key , root , pattern ) : path pattern = os . path . join ( root , pattern ) expanded paths = self . expand pattern ( path pattern ) specs = [ ] for ( path , tags ) in expanded paths : filelist = [ os . path . join ( path , f ) for f in os . listdir ( path ) ] if os . path . isdir ( path ) else [ path ] for filepath in filelist : specs . append ( dict ( tags , * * { key : os . path . abspath ( filepath ) } ) ) return sorted ( specs , key = lambda s : s [ key ] )", "predictions": ["figure out the protocols for a given key sweep"], "references": ["loads the files that match the given pattern ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 4664, "code": "async def push ( self , * args , * * kwargs ) : self . data . append ( ( args , kwargs ) ) if self . future is not None : future , self . future = self . future , None future . set result ( True )", "predictions": ["push will add a new % to the list of values"], "references": ["push new data into the buffer . resume looping if paused ."], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 4665, "code": "def figure Stimulus ( abf , sweeps = [ 0 ] ) : stimuli = [ 2.31250 , 2.35270 ] for sweep in sweeps : abf . setsweep ( sweep ) for stimulus in stimuli : S1 = int ( abf . points Per Sec * stimulus ) S2 = int ( abf . points Per Sec * ( stimulus + 0.001 ) ) abf . sweep Y [ S1 : S2 ] = np . nan I1 = int ( abf . points Per Sec * 2.2 ) I2 = int ( abf . points Per Sec * 2.6 ) baseline = np . average ( abf . sweep Y [ int ( abf . points Per Sec * 2.0 ) : int ( abf . points Per Sec * 2.2 ) ] ) Ys = low Pass Filter ( abf . sweep Y [ I1 : I2 ] ) - baseline Xs = abf . sweep X2 [ I1 : I1 + len ( Ys ) ] . flatten ( ) plt . plot ( Xs , Ys , alpha = .5 , lw = 2 ) return", "predictions": ["make a files of a files"], "references": ["create a plot of one area of interest of a single sweep ."], "bleu": 0.09728049676725326, "rouge_l": 0.29611650485436897}
{"id": 4666, "code": "def do Stuff ( AB Ffolder , analyze = False , convert = False , index = True , overwrite = True , launch = True ) : IN = INDEX ( AB Ffolder ) if analyze : IN . analyze All ( ) if convert : IN . convert Images ( )", "predictions": ["stuff a unix unix unix unix command line elem elem elem elem elem elem"], "references": ["inelegant for now but lets you manually analyze every abf in a folder ."], "bleu": 0.08839374326825923, "rouge_l": 0.07142857142857142}
{"id": 4667, "code": "def analyze Single ( abf Fname ) : assert os . path . exists ( abf Fname ) and abf Fname . endswith ( \".abf\" ) AB Ffolder , AB Ffname = os . path . split ( abf Fname ) abf ID = os . path . splitext ( AB Ffname ) [ 0 ] IN = INDEX ( AB Ffolder ) IN . analyze ABF ( abf ID ) IN . scan ( ) IN . html single basic ( [ abf ID ] , overwrite = True ) IN . html single plot ( [ abf ID ] , overwrite = True ) IN . scan ( ) IN . html index ( ) return", "predictions": ["build a full html html and return a build build html"], "references": ["reanalyze data for a single abf . also remakes child and parent html ."], "bleu": 0.10312570678516415, "rouge_l": 0.2349165596919127}
{"id": 4668, "code": "def analyze All ( self ) : searchable Data = str ( self . files2 ) self . log . debug ( \"considering analysis for %d AB Fs\" , len ( self . I Ds ) ) for ID in self . I Ds : if not ID + \" \" in searchable Data : self . log . debug ( \"%s needs analysis\" , ID ) try : self . analyze ABF ( ID ) except : print ( \"EXCEPTION! \" * 100 ) else : self . log . debug ( \"%s has existing analysis, not overwriting\" , ID ) self . log . debug ( \"verified analysis of %d AB Fs\" , len ( self . I Ds ) )", "predictions": ["run the basic parent parent"], "references": ["analyze every unanalyzed abf in the folder ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4669, "code": "def html For ( self , fname ) : if os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.jpg' , '.png' ] : html = '<a href=\"%s\"><img src=\"%s\"></a>' % ( fname , fname ) if \" tif \" in fname : html = html . replace ( '<img ' , '<img class=\"datapic micrograph\"' ) if \" plot \" in fname : html = html . replace ( '<img ' , '<img class=\"datapic intrinsic\" ' ) if \" experiment \" in fname : html = html . replace ( '<img ' , '<img class=\"datapic experiment\" ' ) elif os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.html' , '.htm' ] : html = 'LINK: %s' % fname else : html = '<br>Not sure how to show: [%s]</br>' % fname return html", "predictions": ["return html for a given html file fname"], "references": ["return appropriate html determined by file extension ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 4670, "code": "def html single plot ( self , abf ID , launch = False , overwrite = False ) : if type ( abf ID ) is str : abf ID = [ abf ID ] for this AB Fid in cm . abf Sort ( abf ID ) : parent ID = cm . parent ( self . groups , this AB Fid ) save As = os . path . abspath ( \"%s/%s plot.html\" % ( self . folder2 , parent ID ) ) if overwrite is False and os . path . basename ( save As ) in self . files2 : continue files By Type = cm . files By Type ( self . group Files [ parent ID ] ) html = \"\" html += '<div style=\"background-color: #DDDDFF;\">' html += '<span class=\"title\">intrinsic properties for: %s</span></br>' % parent ID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + \"/\" + parent ID + \".abf\" ) html += '</div>' for fname in files By Type [ 'plot' ] : html += self . html For ( fname ) print ( \"creating\" , save As , '...' ) style . save ( html , save As , launch = launch )", "predictions": ["generate a 01 plot"], "references": ["create id_plot . html of just intrinsic properties ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 4671, "code": "def timeit ( timer = None ) : if timer is None : return time . time ( ) else : took = time . time ( ) - timer if took < 1 : return \"%.02f ms\" % ( took * 1000.0 ) elif took < 60 : return \"%.02f s\" % ( took ) else : return \"%.02f min\" % ( took / 60.0 )", "predictions": ["return s time time"], "references": ["simple timer . returns a time object or a string ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 4672, "code": "def list move to front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l", "predictions": ["move a list to a front ."], "references": ["if the value is in the list move it to the front and return it ."], "bleu": 0.06758212955973163, "rouge_l": 0.3249001331557923}
{"id": 4673, "code": "def list move to back ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . append ( value ) return l", "predictions": ["remove a list from a list ."], "references": ["if the value is in the list move it to the back and return it ."], "bleu": 0.056829570481990416, "rouge_l": 0.16245006657789615}
{"id": 4674, "code": "def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID if not ID in groups . keys ( ) : for actual Parent in groups . keys ( ) : if ID in groups [ actual Parent ] : return actual Parent return None", "predictions": ["return the parent of a given id"], "references": ["given a groups dictionary and an id return its actual parent id ."], "bleu": 0.10374282717383708, "rouge_l": 0.2846034214618974}
{"id": 4675, "code": "def user Folder ( ) : #path=os.path.abspath(tempfile.gettempdir()+\"/swhlab/\") #don't use tempdir! it will get deleted easily. path = os . path . expanduser ( \"~\" ) + \"/.swhlab/\" if not os . path . exists ( path ) : print ( \"creating\" , path ) os . mkdir ( path ) return os . path . abspath ( path )", "predictions": ["get the path to the user s - tui user"], "references": ["return the semi - temporary user folder"], "bleu": 0.14991106946711685, "rouge_l": 0.36454183266932266}
{"id": 4676, "code": "def abf Fname Load ( ) : fname = user Folder ( ) + \"/abf Fname.ini\" if os . path . exists ( fname ) : abf Fname = open ( fname ) . read ( ) . strip ( ) if os . path . exists ( abf Fname ) or abf Fname . endswith ( \" . \" ) : return abf Fname return os . path . abspath ( os . sep )", "predictions": ["get the abf path to the file ."], "references": ["return the path of the last loaded abf ."], "bleu": 0.1862539773562041, "rouge_l": 0.465648854961832}
{"id": 4677, "code": "def abf Fname Save ( abf Fname ) : fname = user Folder ( ) + \"/abf Fname.ini\" with open ( fname , 'w' ) as f : f . write ( os . path . abspath ( abf Fname ) ) return", "predictions": ["writes a user to the abf file with the given name"], "references": ["return the path of the last loaded abf ."], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 4678, "code": "def check limit ( self , event ) : if self . count ( event ) > self . max listeners : warnings . warn ( 'Too many listeners for event {}' . format ( event ) , Resource Warning , )", "predictions": ["check if the limit limit is valid ."], "references": ["check if the listener limit is hit and warn if needed ."], "bleu": 0.22374550494929327, "rouge_l": 0.5791139240506329}
{"id": 4679, "code": "def once ( self , event , listener ) : self . emit ( 'new listener' , event , listener ) self . once [ event ] . append ( listener ) self . check limit ( event ) return self", "predictions": ["add a listener to the specified listener ."], "references": ["add a listener that is only called once ."], "bleu": 0.2785146580372046, "rouge_l": 0.465648854961832}
{"id": 4680, "code": "def gen PN Gs ( folder , files = None ) : if files is None : files = glob . glob ( folder + \"/*.*\" ) new = [ ] for fname in files : ext = os . path . basename ( fname ) . split ( \".\" ) [ - 1 ] . lower ( ) if ext in [ 'tif' , 'tiff' ] : if not os . path . exists ( fname + \".png\" ) : print ( \" -- converting %s to PNG...\" % os . path . basename ( fname ) ) cm . image convert ( fname ) new . append ( fname ) #fancy burn-in of image data else : pass #print(\" -- already converted %s to PNG...\"%os.path.basename(fname)) return new", "predictions": ["generate new image file ."], "references": ["convert each tif to png . return filenames of new pngs ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 4681, "code": "def html ABF ( ID , group , d , folder , overwrite = False ) : fname = folder + \"/swhlab4/%s index.html\" % ID if overwrite is False and os . path . exists ( fname ) : return html = TEMPLATES [ 'abf' ] html = html . replace ( \"~ID~\" , ID ) html = html . replace ( \"~CONTENT~\" , html AB Fcontent ( ID , group , d ) ) print ( \" <- writing [%s]\" % os . path . basename ( fname ) ) with open ( fname , 'w' ) as f : f . write ( html ) return", "predictions": ["generate a abf file for the given group ."], "references": ["given an id and the dict of files generate a static html for that abf ."], "bleu": 0.10905303695941435, "rouge_l": 0.3046192259675406}
{"id": 4682, "code": "def gen Index ( folder , force I Ds = [ ] ) : if not os . path . exists ( folder + \"/swhlab4/\" ) : print ( \" !! cannot index if no /swhlab4/\" ) return timestart = cm . timethis ( ) files = glob . glob ( folder + \"/*.*\" ) #ABF folder files . extend ( glob . glob ( folder + \"/swhlab4/*.*\" ) ) print ( \" -- indexing glob took %.02f ms\" % ( cm . timethis ( timestart ) * 1000 ) ) files . extend ( gen PN Gs ( folder , files ) ) files = sorted ( files ) timestart = cm . timethis ( ) d = cm . get I Dfile Dict ( files ) #TODO: this is really slow print ( \" -- filedict length:\" , len ( d ) ) print ( \" -- generating ID dict took %.02f ms\" % ( cm . timethis ( timestart ) * 1000 ) ) groups = cm . get AB Fgroups ( files ) print ( \" -- groups length:\" , len ( groups ) ) for ID in sorted ( list ( groups . keys ( ) ) ) : overwrite = False for abf ID in groups [ ID ] : if abf ID in force I Ds : overwrite = True try : html ABF ( ID , groups [ ID ] , d , folder , overwrite ) except : print ( \"~~ HTML GENERATION FAILED!!!\" ) menu = exp Menu ( groups , folder ) make Splash ( menu , folder ) make Menu ( menu , folder ) html Frames ( d , folder ) make Menu ( menu , folder ) make Splash ( menu , folder )", "predictions": ["generate a folder with specified files and menu ."], "references": ["expects a folder of abfs ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 4683, "code": "def plot All Sweeps ( abf File ) : r = io . Axon IO ( filename = abf File ) bl = r . read block ( lazy = False , cascade = True ) print ( abf File + \"\\nplotting %d sweeps...\" % len ( bl . segments ) ) plt . figure ( figsize = ( 12 , 10 ) ) plt . title ( abf File ) for sweep in range ( len ( bl . segments ) ) : trace = bl . segments [ sweep ] . analogsignals [ 0 ] plt . plot ( trace . times - trace . times [ 0 ] , trace . magnitude , alpha = .5 ) plt . ylabel ( trace . dimensionality ) plt . xlabel ( \"seconds\" ) plt . show ( ) plt . close ( )", "predictions": ["plot the sweeps and the segments"], "references": ["simple example how to load an abf file and plot every sweep ."], "bleu": 0.07612610271614867, "rouge_l": 0.09870550161812298}
{"id": 4684, "code": "def plot shaded data ( X , Y , variances , variance X ) : plt . plot ( X , Y , color = 'k' , lw = 2 ) n Chunks = int ( len ( Y ) / CHUNK POINTS ) for i in range ( 0 , 100 , PERCENT STEP ) : var Limit Low = np . percentile ( variances , i ) var Limit High = np . percentile ( variances , i + PERCENT STEP ) variance Is Above Min = np . where ( variances >= var Limit Low ) [ 0 ] variance Is Below Max = np . where ( variances <= var Limit High ) [ 0 ] variance Is Range = [ chunk Number for chunk Number in range ( n Chunks ) if chunk Number in variance Is Above Min and chunk Number in variance Is Below Max ] for chunk Number in variance Is Range : t1 = chunk Number * CHUNK POINTS / POINTS PER SEC t2 = t1 + CHUNK POINTS / POINTS PER SEC plt . axvspan ( t1 , t2 , alpha = .3 , color = COLORMAP ( i / 100 ) , lw = 0 )", "predictions": ["plot shaded data in a shaded"], "references": ["plot x and y data then shade its background by variance ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 4685, "code": "def show variances ( Y , variances , variance X , log Scale = False ) : plt . figure ( 1 , figsize = ( 10 , 7 ) ) plt . figure ( 2 , figsize = ( 10 , 7 ) ) var Sorted = sorted ( variances ) plt . figure ( 1 ) plt . subplot ( 211 ) plt . grid ( ) plt . title ( \"chronological variance\" ) plt . ylabel ( \"original data\" ) plot shaded data ( X , Y , variances , variance X ) plt . margins ( 0 , .1 ) plt . subplot ( 212 ) plt . ylabel ( \"variance (p A) (log%s)\" % str ( log Scale ) ) plt . xlabel ( \"time in sweep (sec)\" ) plt . plot ( variance X , variances , 'k-' , lw = 2 ) plt . figure ( 2 ) plt . ylabel ( \"variance (p A) (log%s)\" % str ( log Scale ) ) plt . xlabel ( \"chunk number\" ) plt . title ( \"sorted variance\" ) plt . plot ( var Sorted , 'k-' , lw = 2 ) for i in range ( 0 , 100 , PERCENT STEP ) : var Limit Low = np . percentile ( variances , i ) var Limit High = np . percentile ( variances , i + PERCENT STEP ) label = \"%2d-%d percentile\" % ( i , i + + PERCENT STEP ) color = COLORMAP ( i / 100 ) print ( \"%s: variance = %.02f - %.02f\" % ( label , var Limit Low , var Limit High ) ) plt . figure ( 1 ) plt . axhspan ( var Limit Low , var Limit High , alpha = .5 , lw = 0 , color = color , label = label ) plt . figure ( 2 ) chunk Low = np . where ( var Sorted >= var Limit Low ) [ 0 ] [ 0 ] chunk High = np . where ( var Sorted >= var Limit High ) [ 0 ] [ 0 ] plt . axvspan ( chunk Low , chunk High , alpha = .5 , lw = 0 , color = color , label = label ) for fignum in [ 1 , 2 ] : plt . figure ( fignum ) if log Scale : plt . semilogy ( ) plt . margins ( 0 , 0 ) plt . grid ( ) if fignum is 2 : plt . legend ( fontsize = 10 , loc = 'upper left' , shadow = True ) plt . tight layout ( ) plt . savefig ( '2016-12-15-variance-%d-log%s.png' % ( fignum , str ( log Scale ) ) ) plt . show ( )", "predictions": ["show the variances ."], "references": ["create some fancy graphs to show color - coded variances ."], "bleu": 0.0883002314431393, "rouge_l": 0.3689516129032258}
{"id": 4686, "code": "def detect ( self ) : self . log . info ( \"initializing AP detection on all sweeps...\" ) t1 = cm . timeit ( ) for sweep in range ( self . abf . sweeps ) : self . detect Sweep ( sweep ) self . log . info ( \"AP analysis of %d sweeps found %d A Ps (completed in %s)\" , self . abf . sweeps , len ( self . A Ps ) , cm . timeit ( t1 ) )", "predictions": ["detect the analysis detection ."], "references": ["runs ap detection on every sweep ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4687, "code": "def get author and version ( package ) : init py = open ( os . path . join ( package , ' init .py' ) ) . read ( ) author = re . search ( \" author  = ['\\\"]([^'\\\"]+)['\\\"]\" , init py ) . group ( 1 ) version = re . search ( \" version  = ['\\\"]([^'\\\"]+)['\\\"]\" , init py ) . group ( 1 ) return author , version", "predictions": ["return package author and version from init . py ."], "references": ["return package author and version as listed in init . py ."], "bleu": 0.5744934948277893, "rouge_l": 0.8049853372434018}
{"id": 4688, "code": "def add parsley ns ( cls , namespace dict ) : namespace dict . update ( { 'parslepy' : cls . LOCAL NAMESPACE , 'parsley' : cls . LOCAL NAMESPACE , } ) return namespace dict", "predictions": ["add a parsley to a namespace ."], "references": ["extend xpath evaluation with parsley extensions namespace"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4689, "code": "def message is to me ( self , data ) : return ( data . get ( 'type' ) == 'message' and data . get ( 'text' , '' ) . startswith ( self . address as ) )", "predictions": ["return whether the message is me to me ."], "references": ["if you send a message directly to me"], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 4690, "code": "def get app locations ( ) : return [ os . path . dirname ( os . path . normpath ( import module ( app name ) . file ) ) for app name in PROJECT APPS ]", "predictions": ["return list of all app locations ."], "references": ["returns list of paths to tested apps"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 4691, "code": "def get tasks ( ) : task classes = [ ] for task path in TASKS : try : module , classname = task path . rsplit ( '.' , 1 ) except Value Error : raise Improperly Configured ( '%s isn\\'t a task module' % task path ) try : mod = import module ( module ) except Import Error as e : raise Improperly Configured ( 'Error importing task %s: \"%s\"' % ( module , e ) ) try : task class = getattr ( mod , classname ) except Attribute Error : raise Improperly Configured ( 'Task module \"%s\" does not define a ' '\"%s\" class' % ( module , classname ) ) task classes . append ( task class ) return task classes", "predictions": ["get all tasks that have a particular module"], "references": ["get the imported task classes for each task that will be run"], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 4692, "code": "def get task options ( ) : options = ( ) task classes = get tasks ( ) for cls in task classes : options += cls . option list return options", "predictions": ["return a list of all task options defined in this task ."], "references": ["get the options for each task that will be run"], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 4693, "code": "def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string types ) : self . add entries ( database . parse string ( entry , bib format = 'bibtex' ) ) else : self . add entries ( entry )", "predictions": ["add entry to database"], "references": ["add a source either specified by glottolog reference id or as bibtex record ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 4694, "code": "def get cache key ( user or username , size , prefix ) : if isinstance ( user or username , get user model ( ) ) : user or username = user or username . username return '%s %s %s' % ( prefix , user or username , size )", "predictions": ["returns a cache key for the given user or size"], "references": ["returns a cache key consisten of a username and image size ."], "bleu": 0.3006425395194706, "rouge_l": 0.44721407624633425}
{"id": 4695, "code": "def invalidate cache ( user , size = None ) : sizes = set ( AUTO GENERATE AVATAR SIZES ) if size is not None : sizes . add ( size ) for prefix in cached funcs : for size in sizes : cache . delete ( get cache key ( user , size , prefix ) )", "predictions": ["invalidate all sizes for a user ."], "references": ["function to be called when saving or changing an user s avatars ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 4696, "code": "def get pref model class ( app , prefs , get prefs func ) : module = '%s.%s' % ( app , PREFS MODULE NAME ) model dict = { ' prefs app' : app , ' get prefs' : staticmethod ( get prefs func ) , ' module ' : module , 'Meta' : type ( 'Meta' , ( models . options . Options , ) , { 'verbose name' : ( 'Preference' ) , 'verbose name plural' : ( 'Preferences' ) , 'app label' : app , 'managed' : False , } ) } for field name , val proxy in prefs . items ( ) : model dict [ field name ] = val proxy . field model = type ( 'Preferences' , ( models . Model , ) , model dict ) def fake save base ( self , * args , * * kwargs ) : updated prefs = { f . name : getattr ( self , f . name ) for f in self . meta . fields if not isinstance ( f , models . fields . Auto Field ) } app prefs = self . get prefs ( self . prefs app ) for pref in app prefs . keys ( ) : if pref in updated prefs : app prefs [ pref ] . db value = updated prefs [ pref ] self . pk = self . prefs app prefs save . send ( sender = self , app = self . prefs app , updated prefs = updated prefs ) return True model . save base = fake save base return model", "predictions": ["fetch a prefs class from the database"], "references": ["returns preferences model class dynamically crated for a given app or none on conflict ."], "bleu": 0.06555660318294844, "rouge_l": 0.08531468531468532}
{"id": 4697, "code": "def print file info ( ) : tpl = Table Logger ( columns = 'file,created,modified,size' ) for f in os . listdir ( '.' ) : size = os . stat ( f ) . st size date created = datetime . fromtimestamp ( os . path . getctime ( f ) ) date modified = datetime . fromtimestamp ( os . path . getmtime ( f ) ) tpl ( f , date created , date modified , size )", "predictions": ["print a file - like object for each file"], "references": ["prints file details in the current directory"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4698, "code": "def dispatch ( self , func ) : self . callees . append ( self . make dispatch ( func ) ) return self . make wrapper ( func )", "predictions": ["decorator to dispatch a function with a specific function"], "references": ["adds the decorated function to this dispatch ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 4699, "code": "def convert Shp To Extend ( path To Shp ) : driver = ogr . Get Driver By Name ( 'ESRI Shapefile' ) dataset = driver . Open ( path To Shp ) if dataset is not None : layer = dataset . Get Layer ( ) spatial Ref = layer . Get Spatial Ref ( ) feature = layer . Get Next Feature ( ) geom = feature . Get Geometry Ref ( ) spatial Ref = geom . Get Spatial Reference ( ) #WGS84 out Spatial Ref = osr . Spatial Reference ( ) out Spatial Ref . Import From EPSG ( 4326 ) coord Trans = osr . Coordinate Transformation ( spatial Ref , out Spatial Ref ) env = geom . Get Envelope ( ) point MAX = ogr . Geometry ( ogr . wkb Point ) point MAX . Add Point ( env [ 1 ] , env [ 3 ] ) point MAX . Transform ( coord Trans ) point MIN = ogr . Geometry ( ogr . wkb Point ) point MIN . Add Point ( env [ 0 ] , env [ 2 ] ) point MIN . Transform ( coord Trans ) return [ point MAX . Get Point ( ) [ 1 ] , point MIN . Get Point ( ) [ 0 ] , point MIN . Get Point ( ) [ 1 ] , point MAX . Get Point ( ) [ 0 ] ] else : exit ( \" shapefile not found. Please verify your path to the shapefile\" )", "predictions": ["convert a ogr file to a list of inputbatch format ."], "references": ["reprojette en wgs84 et recupere l extend"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4700, "code": "def convert Grib To Tiff ( liste File , list Param , list Level , liststep , grid , start Date , end Date , out Folder ) : dico Values = { } for l in liste File : grbs = pygrib . open ( l ) grbs . seek ( 0 ) index = 1 for j in range ( len ( list Level ) , 0 , - 1 ) : for i in range ( len ( list Param ) - 1 , - 1 , - 1 ) : grb = grbs [ index ] p = grb . name . replace ( ' ' , ' ' ) if grb . level != 0 : l = str ( grb . level ) + ' ' + grb . type Of Level else : l = grb . type Of Level if p + ' ' + l not in dico Values . keys ( ) : dico Values [ p + ' ' + l ] = [ ] dico Values [ p + ' ' + l ] . append ( grb . values ) shape = grb . values . shape lat , lon = grb . latlons ( ) geoparam = ( lon . min ( ) , lat . max ( ) , grid , grid ) index += 1 nb Jour = ( end Date - start Date ) . days + 1 #on joute des array Nan si il manque des fichiers for s in range ( 0 , ( len ( liststep ) * nb Jour - len ( liste File ) ) ) : for k in dico Values . keys ( ) : dico Values [ k ] . append ( np . full ( shape , np . nan ) ) #On \u00e9crit pour chacune des variables dans un fichier for i in range ( len ( dico Values . keys ( ) ) - 1 , - 1 , - 1 ) : dict Param = dict ( ( k , dico Values [ dico Values . keys ( ) [ i ] ] [ k ] ) for k in range ( 0 , len ( dico Values [ dico Values . keys ( ) [ i ] ] ) ) ) sorted ( dict Param . items ( ) , key = lambda x : x [ 0 ] ) output Img = out Folder + '/' + dico Values . keys ( ) [ i ] + ' ' + start Date . strftime ( '%Y%M%d' ) + ' ' + end Date . strftime ( '%Y%M%d' ) + '.tif' write Tiff From Dico Array ( dict Param , output Img , shape , geoparam ) for f in liste File : os . remove ( f )", "predictions": ["convert a list of inputbatch files to a list of inputbatch variables"], "references": ["convert grib to tif"], "bleu": 0.11498759556447223, "rouge_l": 0.27477477477477474}
{"id": 4701, "code": "def doublewell ( theta ) : k0 , k1 , depth = 0.01 , 100 , 0.5 shallow = 0.5 * k0 * theta ** 2 + depth deep = 0.5 * k1 * theta ** 2 obj = float ( np . minimum ( shallow , deep ) ) grad = np . where ( deep < shallow , k1 * theta , k0 * theta ) return obj , grad", "predictions": ["computes the variance of a complex quaternion ."], "references": ["pointwise minimum of two quadratic bowls"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 4702, "code": "def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad", "predictions": ["get rosenbrock function ."], "references": ["objective and gradient for the rosenbrock function"], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 4703, "code": "def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad", "predictions": ["calculate camel - squared coefficient ."], "references": ["three - hump camel function"], "bleu": 0.24446151121745047, "rouge_l": 0.18484848484848485}
{"id": 4704, "code": "def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad", "predictions": ["convert a circle to cartesian coordinates"], "references": ["one of the bohachevsky functions"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4705, "code": "def dixon price ( theta ) : x , y = theta obj = ( x - 1 ) ** 2 + 2 * ( 2 * y ** 2 - x ) ** 2 grad = np . array ( [ 2 * x - 2 - 4 * ( 2 * y ** 2 - x ) , 16 * ( 2 * y ** 2 - x ) * y , ] ) return obj , grad", "predictions": ["compute a list of list with the list of list of list values"], "references": ["dixon - price function"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 4706, "code": "def styblinski tang ( theta ) : x , y = theta obj = 0.5 * ( x ** 4 - 16 * x ** 2 + 5 * x + y ** 4 - 16 * y ** 2 + 5 * y ) grad = np . array ( [ 2 * x ** 3 - 16 * x + 2.5 , 2 * y ** 3 - 16 * y + 2.5 , ] ) return obj , grad", "predictions": ["get the parent tang function from the quaternion"], "references": ["styblinski - tang function"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 4707, "code": "def create bucket ( self , * args , * * kwargs ) : bucket = super ( S3Connection , self ) . create bucket ( * args , * * kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket", "predictions": ["add the bucket to the bucket ."], "references": ["add the bucket to mimicdb after successful creation ."], "bleu": 0.40661103887968814, "rouge_l": 0.6112224448897796}
{"id": 4708, "code": "def delete keys ( self , * args , * * kwargs ) : ikeys = iter ( kwargs . get ( 'keys' , args [ 0 ] if args else [ ] ) ) while True : try : key = ikeys . next ( ) except Stop Iteration : break if isinstance ( key , basestring ) : mimicdb . backend . srem ( tpl . bucket % self . name , key ) mimicdb . backend . delete ( tpl . key % ( self . name , key ) ) elif isinstance ( key , Boto Key ) or isinstance ( key , Key ) : mimicdb . backend . srem ( tpl . bucket % self . name , key . name ) mimicdb . backend . delete ( tpl . key % ( self . name , key . name ) ) return super ( Bucket , self ) . delete keys ( * args , * * kwargs )", "predictions": ["abf keys keys if not found if they are not in bucket if not found if they not in backend if not in backend if not found if not in backend"], "references": ["remove each key or key name in an iterable from the bucket set ."], "bleu": 0.04317900023606586, "rouge_l": 0.09538702111024237}
{"id": 4709, "code": "def delete key internal ( self , * args , * * kwargs ) : mimicdb . backend . srem ( tpl . bucket % self . name , args [ 0 ] ) mimicdb . backend . delete ( tpl . key % ( self . name , args [ 0 ] ) ) return super ( Bucket , self ) . delete key internal ( * args , * * kwargs )", "predictions": ["abf key key . . . ."], "references": ["remove key name from bucket set ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4710, "code": "def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )", "predictions": ["cartesian to spherical coordinates"], "references": ["projection onto the semidefinite cone"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 4711, "code": "def columns ( x , rho , proxop ) : xnext = np . zeros like ( x ) for ix in range ( x . shape [ 1 ] ) : xnext [ : , ix ] = proxop ( x [ : , ix ] , rho ) return xnext", "predictions": ["generate an . once given an event vector"], "references": ["applies a proximal operator to the columns of a matrix"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 4712, "code": "def gradient optimizer ( coro ) : class Gradient Optimizer ( Optimizer ) : @ wraps ( coro ) def init ( self , * args , * * kwargs ) : self . algorithm = coro ( * args , * * kwargs ) self . algorithm . send ( None ) self . operators = [ ] def set transform ( self , func ) : self . transform = compose ( destruct , func , self . restruct ) def minimize ( self , f df , x0 , display = sys . stdout , maxiter = 1e3 ) : self . display = display self . theta = x0 xk = self . algorithm . send ( destruct ( x0 ) . copy ( ) ) store = defaultdict ( list ) runtimes = [ ] if len ( self . operators ) == 0 : self . operators = [ proxops . identity ( ) ] obj , grad = wrap ( f df , x0 ) transform = compose ( destruct , * reversed ( self . operators ) , self . restruct ) self . optional print ( tp . header ( [ 'Iteration' , 'Objective' , '||Grad||' , 'Runtime' ] ) ) try : for k in count ( ) : tstart = perf counter ( ) f = obj ( xk ) df = grad ( xk ) xk = transform ( self . algorithm . send ( df ) ) runtimes . append ( perf counter ( ) - tstart ) store [ 'f' ] . append ( f ) self . optional print ( tp . row ( [ k , f , np . linalg . norm ( destruct ( df ) ) , tp . humantime ( runtimes [ - 1 ] ) ] ) ) if k >= maxiter : break except Keyboard Interrupt : pass self . optional print ( tp . bottom ( 4 ) ) self . optional print ( u'\\u279b Final objective: {}' . format ( store [ 'f' ] [ - 1 ] ) ) self . optional print ( u'\\u279b Total runtime: {}' . format ( tp . humantime ( sum ( runtimes ) ) ) ) self . optional print ( u'\\u279b Per iteration runtime: {} +/- {}' . format ( tp . humantime ( np . mean ( runtimes ) ) , tp . humantime ( np . std ( runtimes ) ) , ) ) return Optimize Result ( { 'x' : self . restruct ( xk ) , 'f' : f , 'df' : self . restruct ( df ) , 'k' : k , 'obj' : np . array ( store [ 'f' ] ) , } ) return Gradient Optimizer", "predictions": ["decorator to automatically gen optimizer method"], "references": ["turns a coroutine into a gradient based optimizer ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4713, "code": "def add ( self , operator , * args ) : if isinstance ( operator , str ) : op = getattr ( proxops , operator ) ( * args ) elif isinstance ( operator , proxops . Proximal Operator Base Class ) : op = operator else : raise Value Error ( \"operator must be a string or a subclass of Proximal Operator\" ) self . operators . append ( op ) return self", "predictions": ["html should be a url or a subclass"], "references": ["adds a proximal operator to the list of operators"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4714, "code": "def evaluate ( self , repo , spec , args ) : status = [ ] if len ( spec [ 'files' ] ) == 0 : return status with cd ( repo . rootdir ) : rules = None if 'rules-files' in spec and len ( spec [ 'rules-files' ] ) > 0 : rulesfiles = spec [ 'rules-files' ] rules = { } for f in rulesfiles : d = json . loads ( open ( f ) . read ( ) ) rules . update ( d ) elif 'rules' in spec : rules = { 'inline' : spec [ 'rules' ] } if rules is None or len ( rules ) == 0 : print ( \"Regression quality validation has been enabled but no rules file has been specified\" ) print ( \"Example: { 'min-r2': 0.25 }. Put this either in file or in dgit.json\" ) raise Invalid Parameters ( \"Regression quality checking rules missing\" ) files = dict ( [ ( f , open ( f ) . read ( ) ) for f in spec [ 'files' ] ] ) for r in rules : if 'min-r2' not in rules [ r ] : continue minr2 = float ( rules [ r ] [ 'min-r2' ] ) for f in files : match = re . search ( r\"R-squared:\\s+(\\d.\\d+)\" , files [ f ] ) if match is None : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : \"ERROR\" , 'message' : \"Invalid model output\" } ) else : r2 = match . group ( 1 ) r2 = float ( r2 ) if r2 > minr2 : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : \"OK\" , 'message' : \"Acceptable R2\" } ) else : status . append ( { 'target' : f , 'validator' : self . name , 'description' : self . description , 'rules' : r , 'status' : \"ERROR\" , 'message' : \"R2 is too low\" } ) return status", "predictions": ["gen - for a given repo print the status and force for a given repo"], "references": ["evaluate the files identified for checksum ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 4715, "code": "def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource files = repo . find matching files ( files ) files = glob2 . glob ( \"**/*\" ) disk files = [ f for f in files if os . path . isfile ( f ) and f != \"datapackage.json\" ] allfiles = list ( set ( resource files + disk files ) ) allfiles . sort ( ) for f in allfiles : if f in resource files and f in disk files : r = repo . get resource ( f ) coded sha256 = r [ 'sha256' ] computed sha256 = compute sha256 ( f ) if computed sha256 != coded sha256 : status . append ( { 'target' : f , 'rules' : \"\" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : \"Mismatch in checksum on disk and in datapackage.json\" } ) else : status . append ( { 'target' : f , 'rules' : \"\" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : \"\" } ) elif f in resource files : status . append ( { 'target' : f , 'rules' : \"\" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : \"In datapackage.json but not in repo\" } ) else : status . append ( { 'target' : f , 'rules' : \"\" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : \"In repo but not in datapackage.json\" } ) return status", "predictions": ["plot a disk from a disk"], "references": ["check the integrity of the datapackage . json"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 4716, "code": "def read file ( self , filename ) : #print(\"Reading file\", filename) try : fh = open ( filename , 'rb' ) table set = any tableset ( fh ) except : #traceback.print exc() table set = None return table set", "predictions": ["plot the exc() table table table table"], "references": ["guess the filetype and read the file into row sets"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4717, "code": "def get schema ( self , filename ) : table set = self . read file ( filename ) if table set is None : return [ ] row set = table set . tables [ 0 ] offset , headers = headers guess ( row set . sample ) row set . register processor ( headers processor ( headers ) ) row set . register processor ( offset processor ( offset + 1 ) ) types = type guess ( row set . sample , strict = True ) sample = next ( row set . sample ) clean = lambda v : str ( v ) if not isinstance ( v , str ) else v schema = [ ] for i , h in enumerate ( headers ) : schema . append ( [ h , str ( types [ i ] ) , clean ( sample [ i ] . value ) ] ) return schema", "predictions": ["show a variances variances variances"], "references": ["guess schema using messytables"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4718, "code": "def int2fin reference ( n ) : checksum = 10 - ( sum ( [ int ( c ) * i for c , i in zip ( str ( n ) [ : : - 1 ] , it . cycle ( ( 7 , 3 , 1 ) ) ) ] ) % 10 ) if checksum == 10 : checksum = 0 return \"%s%s\" % ( n , checksum )", "predictions": ["return reference reference reference"], "references": ["calculates a checksum for a finnish national reference number"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4719, "code": "def iso reference valid char ( c , raise error = True ) : if c in ISO REFERENCE VALID : return True if raise error : raise Value Error ( \"'%s' is not in '%s'\" % ( c , ISO REFERENCE VALID ) ) return False", "predictions": ["validate an get packet"], "references": ["helper to make sure the given character is valid for a reference number"], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 4720, "code": "def iso reference str2int ( n ) : n = n . upper ( ) numbers = [ ] for c in n : iso reference valid char ( c ) if c in ISO REFERENCE VALID NUMERIC : numbers . append ( c ) else : numbers . append ( str ( iso reference char2int ( c ) ) ) return int ( '' . join ( numbers ) )", "predictions": ["get the number of add add add add add add cls to the add"], "references": ["creates the huge number from iso alphanumeric iso reference"], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 4721, "code": "def iso reference isvalid ( ref ) : ref = str ( ref ) cs source = ref [ 4 : ] + ref [ : 4 ] return ( iso reference str2int ( cs source ) % 97 ) == 1", "predictions": ["return is a is a string for two nodes"], "references": ["validates iso reference number"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4722, "code": "def add file normal ( f , targetdir , generator , script , source ) : basename = os . path . basename ( f ) if targetdir != \".\" : relativepath = os . path . join ( targetdir , basename ) else : relativepath = basename relpath = os . path . relpath ( f , os . getcwd ( ) ) filetype = 'data' if script : filetype = 'script' if generator : filetype = 'generator' update = Ordered Dict ( [ ( 'type' , filetype ) , ( 'generator' , generator ) , ( 'relativepath' , relativepath ) , ( 'content' , \"\" ) , ( 'source' , source ) , ( 'localfullpath' , f ) , ( 'localrelativepath' , relpath ) ] ) update = annotate record ( update ) return ( basename , update )", "predictions": ["get a file locations for a given file"], "references": ["add a normal file including its source"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4723, "code": "def run executable ( repo , args , includes ) : mgr = plugins get mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) platform metadata = repomgr . get metadata ( ) print ( \"Obtaining Commit Information\" ) ( executable , commiturl ) = find executable commitpath ( repo , args ) tmpdir = tempfile . mkdtemp ( ) print ( \"Running the command\" ) strace filename = os . path . join ( tmpdir , 'strace.out.txt' ) cmd = [ \"strace.py\" , \"-f\" , \"-o\" , strace filename , \"-s\" , \"1024\" , \"-q\" , \"--\" ] + args p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = p . communicate ( ) stdout = os . path . join ( tmpdir , 'stdout.log.txt' ) with open ( stdout , 'w' ) as fd : fd . write ( out . decode ( 'utf-8' ) ) stderr = os . path . join ( tmpdir , 'stderr.log.txt' ) with open ( stderr , 'w' ) as fd : fd . write ( err . decode ( 'utf-8' ) ) files = extract files ( strace filename , includes ) execution metadata = { 'likelyexecutable' : executable , 'commitpath' : commiturl , 'args' : args , } execution metadata . update ( platform metadata ) for i in range ( len ( files ) ) : files [ i ] [ 'execution metadata' ] = execution metadata return files", "predictions": ["get tasks for a specific tasks"], "references": ["run the executable and capture the input and output ..."], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 4724, "code": "def find matching files ( self , includes ) : if len ( includes ) == 0 : return [ ] files = [ f [ 'relativepath' ] for f in self . package [ 'resources' ] ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) files = [ f for f in files if re . match ( includes , os . path . basename ( f ) ) ] + [ f for f in files if re . match ( includes , f ) ] files = list ( set ( files ) ) return files", "predictions": ["get a list of options that match the given package . . . . . . ."], "references": ["for various actions we need files that match patterns"], "bleu": 0.09507244120026236, "rouge_l": 0.1628838451268358}
{"id": 4725, "code": "def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( \"Fatal internal error: Missing repository manager\" ) if cmd not in dir ( self . manager ) : raise Exception ( \"Fatal internal error: Invalid command {} being run\" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )", "predictions": ["execute a = = 0 in the = path"], "references": ["run a specific command using the manager"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4726, "code": "def get resource ( self , p ) : for r in self . package [ 'resources' ] : if r [ 'relativepath' ] == p : r [ 'localfullpath' ] = os . path . join ( self . rootdir , p ) return r raise Exception ( \"Invalid path\" )", "predictions": ["get a cache cache file for a given if it exists ."], "references": ["get metadata for a given file"], "bleu": 0.21401603033752975, "rouge_l": 0.4728682170542636}
{"id": 4727, "code": "def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise Unknown Repository ( ) return self . repos [ key ]", "predictions": ["invalidate a user with a given username"], "references": ["lookup all available repos"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4728, "code": "def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path", "predictions": ["get the module s get module module ."], "references": ["working directory for the repo"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4729, "code": "def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key", "predictions": ["print a new repo = repo"], "references": ["add repo to the internal lookup table ..."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4730, "code": "def lookup ( username , reponame ) : mgr = plugins get mgr ( ) repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) repo = repomgr . lookup ( username = username , reponame = reponame ) return repo", "predictions": ["find a repo object"], "references": ["lookup a repo based on username reponame"], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 4731, "code": "def datapackage exists ( repo ) : datapath = os . path . join ( repo . rootdir , \"datapackage.json\" ) return os . path . exists ( datapath )", "predictions": ["check if a repo exists exists exists is available"], "references": ["check if the datapackage exists ..."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 4732, "code": "def bootstrap datapackage ( repo , force = False , options = None , noinput = False ) : print ( \"Bootstrapping datapackage\" ) tsprefix = datetime . now ( ) . date ( ) . isoformat ( ) package = Ordered Dict ( [ ( 'title' , '' ) , ( 'description' , '' ) , ( 'username' , repo . username ) , ( 'reponame' , repo . reponame ) , ( 'name' , str ( repo ) ) , ( 'title' , \"\" ) , ( 'description' , \"\" ) , ( 'keywords' , [ ] ) , ( 'resources' , [ ] ) , ( 'creator' , getpass . getuser ( ) ) , ( 'createdat' , datetime . now ( ) . isoformat ( ) ) , ( 'remote-url' , repo . remoteurl ) ] ) if options is not None : package [ 'title' ] = options [ 'title' ] package [ 'description' ] = options [ 'description' ] else : if noinput : raise Incomplete Parameters ( \"Option field with title and description\" ) for var in [ 'title' , 'description' ] : value = '' while value in [ '' , None ] : value = input ( 'Your Repo ' + var . title ( ) + \": \" ) if len ( value ) == 0 : print ( \"{} cannot be empty. Please re-enter.\" . format ( var . title ( ) ) ) package [ var ] = value ( handle , filename ) = tempfile . mkstemp ( ) with open ( filename , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) repo . package = package return filename", "predictions": ["convert a datapackage from a repository to a temporary directory"], "references": ["create the datapackage file .."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 4733, "code": "def annotate metadata data ( repo , task , patterns = [ \"*\" ] , size = 0 ) : mgr = plugins get mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get by key ( 'representation' , k ) for k in keys ] matching files = repo . find matching files ( patterns ) package = repo . package rootdir = repo . rootdir files = package [ 'resources' ] for f in files : relativepath = f [ 'relativepath' ] if relativepath in matching files : path = os . path . join ( rootdir , relativepath ) if task == 'preview' : print ( \"Adding preview for \" , relativepath ) f [ 'content' ] = open ( path ) . read ( ) [ : size ] elif task == 'schema' : for r in representations : if r . can process ( path ) : print ( \"Adding schema for \" , path ) f [ 'schema' ] = r . get schema ( path ) break", "predictions": ["annotate metadata theta and schema"], "references": ["update metadata with the content of the files"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4734, "code": "def annotate metadata code ( repo , files ) : package = repo . package package [ 'code' ] = [ ] for p in files : matching files = glob2 . glob ( \"**/{}\" . format ( p ) ) for f in matching files : absf = os . path . abspath ( f ) print ( \"Add commit data for {}\" . format ( f ) ) package [ 'code' ] . append ( Ordered Dict ( [ ( 'script' , f ) , ( 'permalink' , repo . manager . permalink ( repo , absf ) ) , ( 'mimetypes' , mimetypes . guess type ( absf ) [ 0 ] ) , ( 'sha256' , compute sha256 ( absf ) ) ] ) )", "predictions": ["rosenbrock all metadata metadata"], "references": ["update metadata with the commit information"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4735, "code": "def annotate metadata action ( repo ) : package = repo . package print ( \"Including history of actions\" ) with cd ( repo . rootdir ) : filename = \".dgit/log.json\" if os . path . exists ( filename ) : history = open ( filename ) . readlines ( ) actions = [ ] for a in history : try : a = json . loads ( a ) for x in [ 'code' ] : if x not in a or a [ x ] == None : a [ x ] = \"...\" actions . append ( a ) except : pass package [ 'actions' ] = actions", "predictions": ["camel metadata theta theta"], "references": ["update metadata with the action history"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4736, "code": "def annotate metadata platform ( repo ) : print ( \"Added platform information\" ) package = repo . package mgr = plugins get mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get metadata ( )", "predictions": ["annotate the metadata for a particular repo"], "references": ["update metadata host information"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4737, "code": "def annotate metadata dependencies ( repo ) : options = repo . options if 'dependencies' not in options : print ( \"No dependencies\" ) return [ ] repos = [ ] dependent repos = options [ 'dependencies' ] for d in dependent repos : if \"/\" not in d : print ( \"Invalid dependency specification\" ) ( username , reponame ) = d . split ( \"/\" ) try : repos . append ( repo . manager . lookup ( username , reponame ) ) except : print ( \"Repository does not exist. Please create one\" , d ) package = repo . package package [ 'dependencies' ] = [ ] for r in repos : package [ 'dependencies' ] . append ( { 'username' : r . username , 'reponame' : r . reponame , } )", "predictions": ["annotate metadata for a list of available repos ."], "references": ["collect information from the dependent repo s"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4738, "code": "def discover all plugins ( self ) : for v in pkg resources . iter entry points ( 'dgit.plugins' ) : m = v . load ( ) m . setup ( self )", "predictions": ["discover all plugins ."], "references": ["load all plugins from dgit extension"], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 4739, "code": "def search ( self , what , name = None , version = None ) : filtered = { } if what is None : whats = list ( self . plugins . keys ( ) ) elif what is not None : if what not in self . plugins : raise Exception ( \"Unknown class of plugins\" ) whats = [ what ] for what in whats : if what not in filtered : filtered [ what ] = [ ] for key in self . plugins [ what ] . keys ( ) : ( k name , k version ) = key if name is not None and k name != name : continue if version is not None and k version != version : continue if self . plugins [ what ] [ key ] . enable == 'n' : continue filtered [ what ] . append ( key ) return filtered", "predictions": ["search for a given what s a search request"], "references": ["search for a plugin"], "bleu": 0.2626909894424158, "rouge_l": 0.4959349593495934}
{"id": 4740, "code": "def gather configs ( self ) : configs = [ ] for what in self . order : for key in self . plugins [ what ] : mgr = self . plugins [ what ] [ key ] c = mgr . config ( what = 'get' ) if c is not None : c . update ( { 'description' : mgr . description } ) configs . append ( c ) return configs", "predictions": ["return a list of all configs ."], "references": ["gather configuration requirements of all plugins"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 4741, "code": "def update configs ( self , config ) : for what in self . plugins : for key in self . plugins [ what ] : self . plugins [ what ] [ key ] . config ( what = 'set' , params = config ) return", "predictions": ["update the set of plugins that were entered into the plugins ."], "references": ["gather configuration requirements of all plugins"], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 4742, "code": "def instantiate ( repo , validator name = None , filename = None , rulesfiles = None ) : default validators = repo . options . get ( 'validator' , { } ) validators = { } if validator name is not None : if validator name in default validators : validators = { validator name : default validators [ validator name ] } else : validators = { validator name : { 'files' : [ ] , 'rules' : { } , 'rules-files' : [ ] } } else : validators = default validators #========================================= #========================================= if filename is not None : matching files = repo . find matching files ( [ filename ] ) if len ( matching files ) == 0 : print ( \"Filename could not be found\" , filename ) raise Exception ( \"Invalid filename pattern\" ) for v in validators : validators [ v ] [ 'files' ] = matching files else : for v in validators : if 'files' not in validators [ v ] : validators [ v ] [ 'files' ] = [ ] elif len ( validators [ v ] [ 'files' ] ) > 0 : matching files = repo . find matching files ( validators [ v ] [ 'files' ] ) validators [ v ] [ 'files' ] = matching files #========================================= #========================================= if rulesfiles is not None : matching files = repo . find matching files ( [ rulesfiles ] ) if len ( matching files ) == 0 : print ( \"Could not find matching rules files ({}) for {}\" . format ( rulesfiles , v ) ) raise Exception ( \"Invalid rules\" ) for v in validators : validators [ v ] [ 'rules-files' ] = matching files else : for v in validators : if 'rules-files' not in validators [ v ] : validators [ v ] [ 'rules-files' ] = [ ] else : rulesfiles = validators [ v ] [ 'rules-files' ] matching files = repo . find matching files ( rulesfiles ) validators [ v ] [ 'rules-files' ] = matching files return validators", "predictions": ["instantiate a list of rules from a repo"], "references": ["instantiate the validation specification"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4743, "code": "def url is valid ( self , url ) : if url . startswith ( \"file://\" ) : url = url . replace ( \"file://\" , \"\" ) return os . path . exists ( url )", "predictions": ["check if a url is valid"], "references": ["check if a url exists"], "bleu": 0.6147881529512643, "rouge_l": 0.7393939393939394}
{"id": 4744, "code": "def find executable files ( ) : files = glob . glob ( \"*\" ) + glob . glob ( \"*/*\" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S IEXEC | stat . S IXGRP | stat . S IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final", "predictions": ["find all executable files ."], "references": ["find max 5 executables that are responsible for this repo ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 4745, "code": "def get files to commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched files = [ ] for root , dirs , files in os . walk ( workingdir ) : dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] files = [ f for f in files if not re . match ( excludes , f ) ] #print(\"Files after excludes\", files) #print(includes)  files = [ f for f in files if re . match ( includes , f ) ] #print(\"Files after includes\", files)  files = [ os . path . join ( root , f ) for f in files ] matched files . extend ( files ) return matched files", "predictions": ["retrieve files to commit from a list of files ."], "references": ["look through the local directory to pick up files to check"], "bleu": 0.15011602950163877, "rouge_l": 0.18885448916408668}
{"id": 4746, "code": "def auto add ( repo , autooptions , files ) : mapping = { \".\" : \"\" } if ( ( 'import' in autooptions ) and ( 'directory-mapping' in autooptions [ 'import' ] ) ) : mapping = autooptions [ 'import' ] [ 'directory-mapping' ] keys = mapping . keys ( ) keys = sorted ( keys , key = lambda k : len ( k ) , reverse = True ) count = 0 params = [ ] for f in files : relativepath = f for k in keys : v = mapping [ k ] if f . startswith ( k + \"/\" ) : #print(\"Replacing \", k) relativepath = f . replace ( k + \"/\" , v ) break count += files add ( repo = repo , args = [ f ] , targetdir = os . path . dirname ( relativepath ) ) return count", "predictions": ["auto - add files to a list of files"], "references": ["cleanup the paths and add"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4747, "code": "def instantiate ( repo , name = None , filename = None ) : default transformers = repo . options . get ( 'transformer' , { } ) transformers = { } if name is not None : if name in default transformers : transformers = { name : default transformers [ name ] } else : transformers = { name : { 'files' : [ ] , } } else : transformers = default transformers #========================================= #========================================= input matching files = None if filename is not None : input matching files = repo . find matching files ( [ filename ] ) for t in transformers : for k in transformers [ t ] : if \"files\" not in k : continue if k == \"files\" and input matching files is not None : transformers [ t ] [ k ] = input matching files else : if transformers [ t ] [ k ] is None or len ( transformers [ t ] [ k ] ) == 0 : transformers [ t ] [ k ] = [ ] else : matching files = repo . find matching files ( transformers [ t ] [ k ] ) transformers [ t ] [ k ] = matching files return transformers", "predictions": ["instantiate a list of files and return a list of transformers ."], "references": ["instantiate the generator and filename specification"], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 4748, "code": "def delete ( self , repo , args = [ ] ) : result = None with cd ( repo . rootdir ) : try : cmd = [ 'rm' ] + list ( args ) result = { 'status' : 'success' , 'message' : self . run ( cmd ) } except Exception as e : result = { 'status' : 'error' , 'message' : str ( e ) } return result", "predictions": ["delete one document ."], "references": ["delete files from the repo"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 4749, "code": "def permalink ( self , repo , path ) : if not os . path . exists ( path ) : return ( None , None ) cwd = os . getcwd ( ) if os . path . isfile ( path ) : os . chdir ( os . path . dirname ( path ) ) rootdir = self . run ( [ \"rev-parse\" , \"--show-toplevel\" ] ) if \"fatal\" in rootdir : return ( None , None ) os . chdir ( rootdir ) relpath = os . path . relpath ( path , rootdir ) #3764cc2600b221ac7d7497de3d0dbcb4cffa2914 sha1 = self . run ( [ \"log\" , \"-n\" , \"1\" , \"--format=format:%H\" , relpath ] ) #git@gitlab.com:pingali/simple-regression.git #https://gitlab.com/kanban demo/test project.git remoteurl = self . run ( [ \"config\" , \"--get\" , \"remote.origin.url\" ] ) os . chdir ( cwd ) #https://help.github.com/articles/getting-permanent-links-to-files/ #https://gitlab.com/pingali/simple-regression/blob/3764cc2600b221ac7d7497de3d0dbcb4cffa2914/model.py #https://gitlab.com/kanban demo/test project/blob/b004677c23b3a31eb7b5588a5194857b2c8b2b95/README.md m = re . search ( '^git@([^:\\/]+):([^/]+)/([^/]+)' , remoteurl ) if m is None : m = re . search ( '^https://([^:/]+)/([^/]+)/([^/]+)' , remoteurl ) if m is not None : domain = m . group ( 1 ) username = m . group ( 2 ) project = m . group ( 3 ) if project . endswith ( \".git\" ) : project = project [ : - 4 ] permalink = \"https://{}/{}/{}/blob/{}/{}\" . format ( domain , username , project , sha1 , relpath ) return ( relpath , permalink ) else : return ( None , None )", "predictions": ["query permalink to get permalink"], "references": ["get the permalink to command that generated the dataset"], "bleu": 0.17348474258688365, "rouge_l": 0.2717149220489978}
{"id": 4750, "code": "def add files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : continue # targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass print ( \"Updating: {}\" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . run ( [ 'add' , relativepath ] )", "predictions": ["add files to a directory"], "references": ["add files to the repo"], "bleu": 0.5081327481546147, "rouge_l": 0.6}
{"id": 4751, "code": "def to holvi dict ( self ) : self . jsondata [ \"items\" ] = [ ] for item in self . items : self . jsondata [ \"items\" ] . append ( item . to holvi dict ( ) ) self . jsondata [ \"issue date\" ] = self . issue date . isoformat ( ) self . jsondata [ \"due date\" ] = self . due date . isoformat ( ) self . jsondata [ \"receiver\" ] = self . receiver . to holvi dict ( ) return { k : v for ( k , v ) in self . jsondata . items ( ) if k in self . valid keys }", "predictions": ["convert the sky sky object to a dictionary ."], "references": ["convert our python object to json acceptable to holvi api"], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 4752, "code": "def save ( self ) : if self . code : raise Holvi Error ( \"Orders cannot be updated\" ) send json = self . to holvi dict ( ) send json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base url + \"order/\" ) stat = self . api . connection . make post ( url , send json ) code = stat [ \"details uri\" ] . split ( \"/\" ) [ - 2 ] return ( stat [ \"checkout uri\" ] , self . api . get order ( code ) )", "predictions": ["make the save to the server"], "references": ["saves this order to holvi returns a tuple with the order itself and checkout_uri"], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 4753, "code": "def init repo ( self , gitdir ) : hooksdir = os . path . join ( gitdir , 'hooks' ) content = postreceive template % { 'client' : self . client , 'bucket' : self . bucket , 's3cfg' : self . s3cfg , 'prefix' : self . prefix } postrecv filename = os . path . join ( hooksdir , 'post-receive' ) with open ( postrecv filename , 'w' ) as fd : fd . write ( content ) self . make hook executable ( postrecv filename ) print ( \"Wrote to\" , postrecv filename )", "predictions": ["create a hook for the current repo ."], "references": ["insert hook into the repo"], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 4754, "code": "def compute sha256 ( filename ) : try : h = sha256 ( ) fd = open ( filename , 'rb' ) while True : buf = fd . read ( 0x1000000 ) if buf in [ None , \"\" ] : break h . update ( buf . encode ( 'utf-8' ) ) fd . close ( ) return h . hexdigest ( ) except : output = run ( [ \"sha256sum\" , \"-b\" , filename ] ) return output . split ( \" \" ) [ 0 ]", "predictions": ["compute sha256 hash from a file"], "references": ["try the library . if it doesnt work use the command line .."], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 4755, "code": "def run ( cmd ) : cmd = [ pipes . quote ( c ) for c in cmd ] cmd = \" \" . join ( cmd ) cmd += \"; exit 0\" try : output = subprocess . check output ( cmd , stderr = subprocess . STDOUT , shell = True ) except subprocess . Called Process Error as e : output = e . output output = output . decode ( 'utf-8' ) output = output . strip ( ) return output", "predictions": ["run the provided command ."], "references": ["run a shell command"], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 4756, "code": "def get tree ( gitdir = \".\" ) : cmd = [ \"git\" , \"log\" , \"--all\" , \"--branches\" , '--pretty=format:{  \"commit\": \"%H\",  \"abbreviated commit\": \"%h\",  \"tree\": \"%T\",  \"abbreviated tree\": \"%t\",  \"parent\": \"%P\",  \"abbreviated parent\": \"%p\",  \"refs\": \"%d\",  \"encoding\": \"%e\",  \"subject\": \"%s\", \"sanitized subject line\": \"%f\",  \"commit notes\": \"\",  \"author\": {    \"name\": \"%a N\",    \"email\": \"%a E\",    \"date\": \"%ai\"  },  \"commiter\": {    \"name\": \"%c N\",    \"email\": \"%c E\",    \"date\": \"%ci\"  }},' ] output = run ( cmd ) lines = output . split ( \"\\n\" ) content = \"\" history = [ ] for l in lines : try : revisedcontent = content + l if revisedcontent . count ( '\"' ) % 2 == 0 : j = json . loads ( revisedcontent [ : - 1 ] ) if \"Notes added by\" in j [ 'subject' ] : content = \"\" continue history . append ( j ) content = \"\" else : content = revisedcontent except Exception as e : print ( \"Error while parsing record\" ) print ( revisedcontent ) content = \"\" history . reverse ( ) # changes = get change ( ) for i in range ( len ( history ) ) : abbrev commit = history [ i ] [ 'abbreviated commit' ] if abbrev commit not in changes : raise Exception ( \"Missing changes for \" + abbrev commit ) history [ i ] [ 'changes' ] = changes [ abbrev commit ] [ 'changes' ] return history", "predictions": ["get tree structure from pypi"], "references": ["get the commit history for a given dataset"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4757, "code": "def get diffs ( history ) : mgr = plugins get mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get by key ( 'representation' , k ) for k in keys ] for i in range ( len ( history ) ) : if i + 1 > len ( history ) - 1 : continue prev = history [ i ] curr = history [ i + 1 ] #print(prev['subject'], \"==>\", curr['subject']) #print(curr['changes']) for c in curr [ 'changes' ] : path = c [ 'path' ] if c [ 'path' ] . endswith ( 'datapackage.json' ) : continue handler = None for r in representations : if r . can process ( path ) : handler = r break if handler is None : continue v1 hex = prev [ 'commit' ] v2 hex = curr [ 'commit' ] temp1 = tempfile . mkdtemp ( prefix = \"dgit-diff-\" ) try : for h in [ v1 hex , v2 hex ] : filename = '{}/{}/checkout.tar' . format ( temp1 , h ) try : os . makedirs ( os . path . dirname ( filename ) ) except : pass extractcmd = [ 'git' , 'archive' , '-o' , filename , h , path ] output = run ( extractcmd ) if 'fatal' in output : raise Exception ( \"File not present in commit\" ) with cd ( os . path . dirname ( filename ) ) : cmd = [ 'tar' , 'xvf' , 'checkout.tar' ] output = run ( cmd ) if 'fatal' in output : print ( \"Cleaning up - fatal 1\" , temp1 ) shutil . rmtree ( temp1 ) continue path1 = os . path . join ( temp1 , v1 hex , path ) path2 = os . path . join ( temp1 , v2 hex , path ) if not os . path . exists ( path1 ) or not os . path . exists ( path2 ) : shutil . rmtree ( temp1 ) continue #print(path1, path2)  diff = handler . get diff ( path1 , path2 ) c [ 'diff' ] = diff except Exception as e : #traceback.print exc()  #print(\"Cleaning up - Exception \", temp1) shutil . rmtree ( temp1 )", "predictions": ["run the script as a list of directories ."], "references": ["look at files and compute the diffs intelligently"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4758, "code": "def set path ( self , path ) : import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )", "predictions": ["set the path of the file"], "references": ["set self . path self . dirname and self . basename ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 4759, "code": "def images ( self ) : tifs = pattern ( self . image path , extension = 'tif' ) pngs = pattern ( self . image path , extension = 'png' ) imgs = [ ] imgs . extend ( glob ( tifs ) ) imgs . extend ( glob ( pngs ) ) return imgs", "predictions": ["a list of all imgs and the images ."], "references": ["list of paths to images ."], "bleu": 0.23356898886410005, "rouge_l": 0.5532879818594103}
{"id": 4760, "code": "def scanning template ( self ) : tmpl = glob ( pattern ( self . path , additional data , scanning template , extension = '*.xml' ) ) if tmpl : return tmpl [ 0 ] else : return ''", "predictions": ["the scanning template template"], "references": ["path to { scanningtemplate } name . xml of experiment ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 4761, "code": "def wait ( self ) : interval seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : time . sleep ( interval seconds ) slept = True break if not slept : break", "predictions": ["wait for all seconds to finish"], "references": ["wait for all actions to complete on a droplet"], "bleu": 0.26367153725164694, "rouge_l": 0.5147679324894514}
{"id": 4762, "code": "def format parameters ( self , * * kwargs ) : req data = { } for k , v in kwargs . items ( ) : if isinstance ( v , ( list , tuple ) ) : k = k + '[]' req data [ k ] = v return req data", "predictions": ["format parameters to a query string"], "references": ["properly formats array types"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4763, "code": "def format request url ( self , resource , * args ) : return '/' . join ( ( self . api url , self . api version , resource ) + tuple ( str ( x ) for x in args ) )", "predictions": ["build the request url for the specified resource ."], "references": ["create request url for resource"], "bleu": 0.2777619034011791, "rouge_l": 0.6024691358024692}
{"id": 4764, "code": "def list ( self , url components = ( ) ) : resp = self . get ( url components ) return resp . get ( self . result key , [ ] )", "predictions": ["return a list of all entries for the specified url ."], "references": ["send list request for all members of a collection"], "bleu": 0.14991106946711685, "rouge_l": 0.2036727879799666}
{"id": 4765, "code": "def get ( self , id , * * kwargs ) : return ( super ( Mutable Collection , self ) . get ( ( id , ) , * * kwargs ) . get ( self . singular , None ) )", "predictions": ["retrieves the singular by id ."], "references": ["get single unit of collection"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4766, "code": "def get ( self , id , * * kwargs ) : return super ( Domain Records , self ) . get ( id , * * kwargs )", "predictions": ["gets the get by id ."], "references": ["retrieve a single domain record given the id"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4767, "code": "def chop ( list , n ) : size = len ( list ) each = size // n if each == 0 : return [ list ] chopped = [ ] for i in range ( n ) : start = i * each end = ( i + 1 ) * each if i == ( n - 1 ) : end = size chopped . append ( list [ start : end ] ) return chopped", "predictions": ["return a list of n - sized chunks from list ."], "references": ["chop list_ into n chunks . returns a list ."], "bleu": 0.1972940627795883, "rouge_l": 0.384251968503937}
{"id": 4768, "code": "def allowed operations ( self ) : if self . slug is not None : return self . meta . detail allowed operations return self . meta . list allowed operations", "predictions": ["get list of annotate metadata print"], "references": ["retrieves the allowed operations for this request ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 4769, "code": "def assert operations ( self , * args ) : if not set ( args ) . issubset ( self . allowed operations ) : raise http . exceptions . Forbidden ( )", "predictions": ["annotate if a http is registered not print it not print it not print ."], "references": ["assets if the requested operations are allowed in this context ."], "bleu": 0.09103526405546068, "rouge_l": 0.1582360570687419}
{"id": 4770, "code": "def make response ( self , data = None ) : if data is not None : data = self . prepare ( data ) self . response . write ( data , serialize = True )", "predictions": ["discover the all the data and points it to the client iter iter"], "references": ["fills the response object from the passed data ."], "bleu": 0.1135935489027116, "rouge_l": 0.2819722650231125}
{"id": 4771, "code": "def get ( self , request , response ) : self . assert operations ( 'read' ) items = self . read ( ) if not items : raise http . exceptions . Not Found ( ) if ( isinstance ( items , Iterable ) and not isinstance ( items , six . string types ) ) and items : items = pagination . paginate ( self . request , self . response , items ) self . make response ( items )", "predictions": ["search for the http request version"], "references": ["processes a get request ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4772, "code": "def post ( self , request , response ) : if self . slug is not None : raise http . exceptions . Not Implemented ( ) self . assert operations ( 'create' ) data = self . clean ( None , self . request . read ( deserialize = True ) ) item = self . create ( data ) self . response . status = http . client . CREATED self . make response ( item )", "predictions": ["gather a new } for the given response for this component for the given response for the given data for the given response for the given response for the response for"], "references": ["processes a post request ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 4773, "code": "def put ( self , request , response ) : if self . slug is None : raise http . exceptions . Not Implemented ( ) target = self . read ( ) data = self . clean ( target , self . request . read ( deserialize = True ) ) if target is not None : self . assert operations ( 'update' ) try : self . update ( target , data ) except Attribute Error : raise http . exceptions . Not Implemented ( ) self . make response ( target ) else : self . assert operations ( 'create' ) target = self . create ( data ) self . response . status = http . client . CREATED self . make response ( target )", "predictions": ["put this component with the given config what we can make it a new component what it to do not have"], "references": ["processes a put request ."], "bleu": 0.06429451441231726, "rouge_l": 0.08652482269503546}
{"id": 4774, "code": "def delete ( self , request , response ) : if self . slug is None : raise http . exceptions . Not Implemented ( ) self . assert operations ( 'destroy' ) self . destroy ( ) self . response . status = http . client . NO CONTENT self . make response ( )", "predictions": ["instantiate the name of the name . . . . . . ."], "references": ["processes a delete request ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 4775, "code": "def create project ( self ) : if os . path . exists ( self . py ) : prj dir = os . path . join ( self . app dir , self . project name ) if os . path . exists ( prj dir ) : if self . force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) p = subprocess . Popen ( 'cd {0} ; {1} startproject {2} > /dev/null' . format ( self . app dir , self . ve dir + os . sep + self . project name + os . sep + 'bin' + os . sep + 'django-admin.py' , self . project name ) , shell = True ) os . waitpid ( p . pid , 0 ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return", "predictions": ["url to the is created by the is a is created by the is tracked to the is a is a is a is a is a is a is a"], "references": ["creates a base django project"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 4776, "code": "def parse ( text , encoding = 'utf8' ) : if isinstance ( text , six . binary type ) : text = text . decode ( encoding ) return Query ( text , split segments ( text ) )", "predictions": ["find the given files and parse it into a string ."], "references": ["parse the querystring into a normalized form ."], "bleu": 0.17827531042796255, "rouge_l": 0.433392539964476}
{"id": 4777, "code": "def split segments ( text , closing paren = False ) : buf = String IO ( ) segments = [ ] combinators = [ ] last group = False iterator = iter ( text ) last negation = False for character in iterator : if character in COMBINATORS : if last negation : buf . write ( constants . OPERATOR NEGATION ) val = buf . getvalue ( ) reset stringio ( buf ) if not last group and not len ( val ) : raise Value Error ( 'Unexpected %s.' % character ) if len ( val ) : segments . append ( parse segment ( val ) ) combinators . append ( COMBINATORS [ character ] ) elif character == constants . GROUP BEGIN : if buf . tell ( ) : raise Value Error ( 'Unexpected %s' % character ) seg = split segments ( iterator , True ) if last negation : seg = Unary Segment Combinator ( seg ) segments . append ( seg ) last group = True continue elif character == constants . GROUP END : val = buf . getvalue ( ) if not buf . tell ( ) or not closing paren : raise Value Error ( 'Unexpected %s' % character ) segments . append ( parse segment ( val ) ) return combine ( segments , combinators ) elif character == constants . OPERATOR NEGATION and not buf . tell ( ) : last negation = True continue else : if last negation : buf . write ( constants . OPERATOR NEGATION ) if last group : raise Value Error ( 'Unexpected %s' % character ) buf . write ( character ) last negation = False last group = False else : if closing paren : raise Value Error ( 'Expected %s.' % constants . GROUP END ) if not last group : segments . append ( parse segment ( buf . getvalue ( ) ) ) return combine ( segments , combinators )", "predictions": ["get a list of files from a given commit commit . . ."], "references": ["return objects representing segments ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 4778, "code": "def parse segment ( text ) : if not len ( text ) : return Noop Query Segment ( ) q = Query Segment ( ) equalities = zip ( constants . OPERATOR EQUALITIES , itertools . repeat ( text ) ) equalities = map ( lambda x : ( x [ 0 ] , x [ 1 ] . split ( x [ 0 ] , 1 ) ) , equalities ) equalities = list ( filter ( lambda x : len ( x [ 1 ] ) > 1 , equalities ) ) key len = len ( min ( ( x [ 1 ] [ 0 ] for x in equalities ) , key = len ) ) equalities = filter ( lambda x : len ( x [ 1 ] [ 0 ] ) == key len , equalities ) op , ( key , value ) = min ( equalities , key = lambda x : len ( x [ 1 ] [ 1 ] ) ) key , directive = parse directive ( key ) if directive : op = constants . OPERATOR EQUALITY FALLBACK q . directive = directive path = key . split ( constants . SEP PATH ) last = path [ - 1 ] if last . endswith ( constants . OPERATOR NEGATION ) : last = last [ : - 1 ] q . negated = not q . negated if last == constants . PATH NEGATION : path . pop ( - 1 ) q . negated = not q . negated q . values = value . split ( constants . SEP VALUE ) if path [ - 1 ] in constants . OPERATOR SUFFIXES : if op not in constants . OPERATOR FALLBACK : raise Value Error ( 'Both path-style operator and equality style operator ' 'provided.  Please provide only a single style operator.' ) q . operator = constants . OPERATOR SUFFIX MAP [ path [ - 1 ] ] path . pop ( - 1 ) else : q . operator = constants . OPERATOR EQUALITY MAP [ op ] if not len ( path ) : raise Value Error ( 'No attribute navigation path provided.' ) q . path = path return q", "predictions": ["auto auto add a add add a add add a add add add add attribute to the add attribute"], "references": ["we expect foo = bar"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 4779, "code": "def set ( self , target , value ) : if not self . set : return if self . path is None : self . set = lambda * a : None return None if self . segments [ target . class ] : self . get ( target ) if self . segments [ target . class ] : return parent getter = compose ( * self . getters [ target . class ] [ : - 1 ] ) target = parent getter ( target ) func = self . make setter ( self . path . split ( '.' ) [ - 1 ] , target . class ) func ( target , value ) def setter ( target , value ) : func ( parent getter ( target ) , value ) self . set = setter", "predictions": ["instantiate a = = false"], "references": ["set the value of this attribute for the passed object ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 4780, "code": "def indexes Optional ( f ) : stack = inspect . stack ( ) NO INDEX CHECK NEEDED . add ( '%s.%s.%s' % ( f . module , stack [ 1 ] [ 3 ] , f . name ) ) del stack return f", "predictions": ["decorator to add delete delete delete delete delete delete delete delete delete delete delete delete"], "references": ["decorate test methods with this if you don t require strict index checking"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 4781, "code": "def get method ( self , * args , * * kwargs ) : for method in self . gen methods ( * args , * * kwargs ) : return method msg = 'No method was found for %r on %r.' raise self . Dispatch Error ( msg % ( ( args , kwargs ) , self . inst ) )", "predictions": ["permalink this method with the specified method exists exists exists exists exists exists exists exists exists exists"], "references": ["find the first method this input dispatches to ."], "bleu": 0.0859076483566362, "rouge_l": 0.1628838451268358}
{"id": 4782, "code": "def gen methods ( self , * args , * * kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . method prefix for method key in self . gen method keys ( * args , * * kwargs ) : method = getattr ( inst , prefix + method key , None ) if method is not None : yield method typename = type ( token ) . name yield from self . check basetype ( token , typename , self . builtins . get ( typename ) ) for basetype name in self . interp types : yield from self . check basetype ( token , basetype name , getattr ( self . types , basetype name , None ) ) for basetype name in self . abc types : yield from self . check basetype ( token , basetype name , getattr ( self . collections , basetype name , None ) ) yield from self . gen generic ( )", "predictions": ["generates files for each ."], "references": ["find all method names this input dispatches to ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4783, "code": "def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , Bump Requirement ) : req = Bump Requirement ( req ) req . required = True req . required by = self self . requirements . append ( req )", "predictions": ["make sure this request is a requirement ."], "references": ["add new requirements that must be fulfilled for this bump to occur"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 4784, "code": "def reverse ( self ) : if self . original target content : with open ( self . target , 'w' ) as fp : fp . write ( self . original target content )", "predictions": ["save the raise the raise the raise the raise the raise an exception if the raise an exception is not defined ."], "references": ["restore content in target file to be before any changes"], "bleu": 0.04657469807170698, "rouge_l": 0.0}
{"id": 4785, "code": "def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( * * value ) elif isinstance ( value , six . string types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection", "predictions": ["add a value to a collection instance"], "references": ["extends a collection with a value ."], "bleu": 0.3073940764756322, "rouge_l": 0.2857142857142857}
{"id": 4786, "code": "def merge ( options , name , bases , default = None ) : result = None for base in bases : if base is None : continue value = getattr ( base , name , None ) if value is None : continue result = utils . cons ( result , value ) value = options . get ( name ) if value is not None : result = utils . cons ( result , value ) return result or default", "predictions": ["compute a single options from a list of options"], "references": ["merges a named option collection ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4787, "code": "def package info ( cls , package ) : if package not in cls . package info cache : package json url = 'https://pypi.python.org/pypi/%s/json' % package try : logging . get Logger ( 'requests' ) . set Level ( logging . WARN ) response = requests . get ( package json url ) response . raise for status ( ) cls . package info cache [ package ] = simplejson . loads ( response . text ) except Exception as e : log . debug ( 'Could not get package info from %s: %s' , package json url , e ) cls . package info cache [ package ] = None return cls . package info cache [ package ]", "predictions": ["return run information about a run c { tablename }"], "references": ["all package info for given package"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4788, "code": "def all package versions ( package ) : info = Py PI . package info ( package ) return info and sorted ( info [ 'releases' ] . keys ( ) , key = lambda x : x . split ( ) , reverse = True ) or [ ]", "predictions": ["list available versions for a given tree ."], "references": ["all versions for package"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 4789, "code": "def insert ( self , name , index , value ) : return self . sequence [ name ] . insert ( index , value )", "predictions": ["get an element with a mgr ."], "references": ["insert a value at the passed index in the named header ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 4790, "code": "def send ( self , * args , * * kwargs ) : self . write ( * args , * * kwargs ) self . flush ( )", "predictions": ["= main function to set the result of exceptions ."], "references": ["writes the passed chunk and flushes it to the client ."], "bleu": 0.13564514503163538, "rouge_l": 0.28328173374613}
{"id": 4791, "code": "def insert ( self , name , index , value ) : return self . headers . insert ( index , value )", "predictions": ["images to images with a new = value"], "references": ["insert a value at the passed index in the named header ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 4792, "code": "def create project ( self ) : if os . path . exists ( self . py ) : prj dir = os . path . join ( self . app dir , self . project name ) if os . path . exists ( prj dir ) : if self . force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) os . makedirs ( prj dir ) app = \"\"\"#!/usr/bin/env python\\n\"\"\" \"\"\"from flask import Flask\\n\"\"\" \"\"\"app = Flask( name )\\n\\n\"\"\" \"\"\"@app.route(\\\"/\\\")\\n\"\"\" \"\"\"def hello():\\n\"\"\" \"\"\"    return \\\"Hello from Flask...\\\"\\n\\n\"\"\" \"\"\"if  name ==\\\" main \\\":\\n\"\"\" \"\"\"    app.run()\\n\\n\"\"\" with open ( os . path . join ( prj dir , 'app.py' ) , 'w' ) as f : f . write ( app ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return", "predictions": ["scanning the template to scanning the template glob glob glob glob glob glob glob glob glob glob glob glob"], "references": ["creates a base flask project"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 4793, "code": "def urls ( cls ) : return urls . patterns ( '' , urls . url ( r'^{}(?:$|(?P<path>[/:(.].*))' . format ( cls . meta . name ) , cls . view , name = 'armet-api-{}' . format ( cls . meta . name ) , kwargs = { 'resource' : cls . meta . name } ) )", "predictions": ["returns the wait wait wait for this class seconds seconds seconds seconds seconds seconds seconds seconds seconds seconds seconds seconds seconds seconds seconds seconds seconds to the class seconds seconds"], "references": ["builds the url configuration for this resource ."], "bleu": 0.05705093314691302, "rouge_l": 0.17630057803468208}
{"id": 4794, "code": "def bump ( ) : parser = argparse . Argument Parser ( description = bump . doc ) parser . add argument ( 'names' , nargs = '*' , help = ) parser . add argument ( '--add' , '--require' , action = 'store true' , help = 'Add the `names` to the requirements file if they don\\'t exist.' ) parser . add argument ( '--file' , help = 'Requirement file to bump. Defaults to requirements.txt and pinned.txt' ) parser . add argument ( '--force' , action = 'store true' , help = 'Force a bump even when certain bump requirements are not met.' ) parser . add argument ( '-d' , '--detail' , '--dependencies' , action = 'store true' , help = 'If available, show detailed changes. ' 'For pinned.txt, pin parsed dependency requirements from changes' ) parser . add argument ( '-n' , '--dry-run' , action = 'store true' , help = 'Perform a dry run without making changes' ) parser . add argument ( '--debug' , action = 'store true' , help = 'Turn on debug mode' ) args = parser . parse args ( ) targets = [ args . file ] if args . file else [ 'requirements.txt' , 'pinned.txt' ] level = logging . DEBUG if args . debug else logging . INFO logging . basic Config ( level = level , format = '[%(levelname)s] %(message)s' ) try : bumper = Bumper Driver ( targets , full throttle = args . force , detail = args . detail , test drive = args . dry run ) bumper . bump ( args . names , required = args . add , show detail = args . detail ) except Exception as e : if args . debug : raise else : log . error ( e ) sys . exit ( 1 )", "predictions": ["format and format a bumper"], "references": ["cli entry point to bump requirements in requirements . txt or pinned . txt"], "bleu": 0.037948473198912445, "rouge_l": 0.0}
{"id": 4795, "code": "def expand targets ( self , targets , base dir = None ) : all targets = [ ] for target in targets : target dirs = [ p for p in [ base dir , os . path . dirname ( target ) ] if p ] target dir = target dirs and os . path . join ( * target dirs ) or '' target = os . path . basename ( target ) target path = os . path . join ( target dir , target ) if os . path . exists ( target path ) : all targets . append ( target path ) with open ( target path ) as fp : for line in fp : if line . startswith ( '-r ' ) : , new target = line . split ( ' ' , 1 ) all targets . extend ( self . expand targets ( [ new target . strip ( ) ] , base dir = target dir ) ) return all targets", "predictions": ["format all request request request request request request request request request request request x and request request request request request request request request request request request request request request request request"], "references": ["expand targets by looking for - r in targets ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4796, "code": "def get nginx config ( self ) : if os . path . exists ( self . nginx config ) : return open ( self . nginx config , 'r' ) . read ( ) else : return None", "predictions": ["list the nginx self resp resp"], "references": ["gets the nginx config for the project"], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 4797, "code": "def check directories ( self ) : self . log . debug ( 'Checking directories' ) if not os . path . exists ( self . ve dir ) : os . makedirs ( self . ve dir ) if not os . path . exists ( self . app dir ) : os . makedirs ( self . app dir ) if not os . path . exists ( self . conf dir ) : os . makedirs ( self . conf dir ) if not os . path . exists ( self . var dir ) : os . makedirs ( self . var dir ) if not os . path . exists ( self . log dir ) : os . makedirs ( self . log dir ) if not os . path . exists ( self . script dir ) : os . makedirs ( self . script dir ) uwsgi params = '/etc/nginx/uwsgi params' if os . path . exists ( uwsgi params ) : shutil . copy ( uwsgi params , self . conf dir ) else : logging . warning ( 'Unable to find Nginx uwsgi params.  You must manually copy this to {0}.' . format ( self . conf dir ) ) mime types = '/etc/nginx/mime.types' if os . path . exists ( mime types ) : shutil . copy ( mime types , self . conf dir ) self . include mimetypes = True else : logging . warn ( 'Unable to find mime.types for Nginx.  You must manually copy this to {0}.' . format ( self . conf dir ) )", "predictions": ["run the nginx command"], "references": ["creates base directories for app virtualenv and nginx"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4798, "code": "def create virtualenv ( self ) : if check command ( 'virtualenv' ) : ve dir = os . path . join ( self . ve dir , self . project name ) if os . path . exists ( ve dir ) : if self . force : logging . warn ( 'Removing existing virtualenv' ) shutil . rmtree ( ve dir ) else : logging . warn ( 'Found existing virtualenv; not creating (use --force to overwrite)' ) return logging . info ( 'Creating virtualenv' ) p = subprocess . Popen ( 'virtualenv --no-site-packages {0} > /dev/null' . format ( ve dir ) , shell = True ) os . waitpid ( p . pid , 0 ) for m in self . modules : self . log . info ( 'Installing module {0}' . format ( m ) ) p = subprocess . Popen ( '{0} install {1} > /dev/null' . format ( os . path . join ( self . ve dir , self . project name ) + os . sep + 'bin' + os . sep + 'pip' , m ) , shell = True ) os . waitpid ( p . pid , 0 )", "predictions": ["get a virtualenv virtualenv virtualenv"], "references": ["creates the virtualenv for the project"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4799, "code": "def create nginx config ( self ) : cfg = . format ( self . project name ) if not self . shared hosting : if self . user : cfg += 'user {0};\\n' . format ( self . user ) cfg += . format ( os . path . join ( self . log dir , self . project name ) , os . path . join ( self . var dir , self . project name ) ) cfg += 'events {\\n\\tworker connections 32;\\n}\\n\\n' cfg += 'http {\\n' if self . include mimetypes : cfg += '\\tinclude mime.types;\\n' cfg += '\\tdefault type application/octet-stream;\\n' cfg += '\\tclient max body size 1G;\\n' cfg += '\\tproxy max temp file size 0;\\n' cfg += '\\tproxy buffering off;\\n' cfg += '\\taccess log {0}-access.log;\\n' . format ( os . path . join ( self . log dir , self . project name ) ) cfg += '\\tsendfile on;\\n' cfg += '\\tkeepalive timeout 65;\\n' cfg += '\\tserver {\\n' cfg += '\\t\\tlisten 0.0.0.0:{0};\\n' . format ( self . port ) if self . server name : cfg += '\\t\\tserver name {0};\\n' . format ( self . server name ) cfg += '\\t\\tlocation / {\\n' cfg += '\\t\\t\\tuwsgi pass unix:///{0}.sock;\\n' . format ( os . path . join ( self . var dir , self . project name ) ) cfg += '\\t\\t\\tinclude uwsgi params;\\n' cfg += '\\t\\t}\\n\\n' cfg += '\\t\\terror page 500 502 503 504 /50x.html;\\n' cfg += '\\t\\tlocation = /50x.html {\\n' cfg += '\\t\\t\\troot html;\\n' cfg += '\\t\\t}\\n' cfg += '\\t}\\n' if not self . shared hosting : cfg += '}\\n' f = open ( self . nginx config , 'w' ) f . write ( cfg ) f . close ( )", "predictions": ["chop the nginx to the nginx configuration file len"], "references": ["creates the nginx configuration for the project"], "bleu": 0.2777619034011791, "rouge_l": 0.38364779874213834}
{"id": 4800, "code": "def create manage scripts ( self ) : start = . format ( self . project name ) start += 'echo \\'Starting u WSGI...\\'\\n' start += 'sh {0}.uwsgi\\n' . format ( os . path . join ( self . conf dir , self . project name ) ) start += 'sleep 1\\n' start += 'echo \\'Starting Nginx...\\'\\n' start += 'nginx -c {0} nginx.conf\\n' . format ( os . path . join ( self . conf dir , self . project name ) ) start += 'sleep 1\\n' start += 'echo \\'{0} started\\'\\n\\n' . format ( self . project name ) stop = . format ( self . project name ) stop += 'if [ -e {0} nginx.pid ]; then nginx -c {1} nginx.conf -s stop ; fi\\n' . format ( os . path . join ( self . var dir , self . project name ) , os . path . join ( self . conf dir , self . project name ) ) stop += 'if [ -e {0} uwsgi.pid ]; then kill -9 `cat {0} uwsgi.pid` ; rm {0} uwsgi.pid 2>&1 > /dev/null ; fi\\n' . format ( os . path . join ( self . var dir , self . project name ) ) stop += 'echo \\'{0} stopped\\'\\n' . format ( self . project name ) start file = '{0} start.sh' . format ( os . path . join ( self . script dir , self . project name ) ) stop file = '{0} stop.sh' . format ( os . path . join ( self . script dir , self . project name ) ) f = open ( start file , 'w' ) f . write ( start ) f . close ( ) f = open ( stop file , 'w' ) f . write ( stop ) f . close ( ) os . chmod ( start file , 0754 ) os . chmod ( stop file , 0754 )", "predictions": ["create manage scripts scripts scripts ."], "references": ["creates scripts to start and stop the application"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4801, "code": "def create ( self ) : self . create virtualenv ( ) self . create project ( ) self . create uwsgi script ( ) self . create nginx config ( ) self . create manage scripts ( ) logging . info ( '** Make sure to set proper permissions for the webserver user account on the var and log directories in the project root' )", "predictions": ["create the uwsgi script ."], "references": ["creates the full project"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4802, "code": "def dasherize ( value ) : value = value . strip ( ) value = re . sub ( r'([A-Z])' , r'-\\1' , value ) value = re . sub ( r'[- \\s]+' , r'-' , value ) value = re . sub ( r'^-' , r'' , value ) value = value . lower ( ) return value", "predictions": ["strip the dasherize from the given value ."], "references": ["dasherizes the passed value ."], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 4803, "code": "def redirect ( cls , request , response ) : if cls . meta . legacy redirect : if request . method in ( 'GET' , 'HEAD' , ) : response . status = http . client . MOVED PERMANENTLY else : response . status = http . client . TEMPORARY REDIRECT else : response . status = http . client . PERMANENT REDIRECT response . close ( )", "predictions": ["redirect this component to the given response ."], "references": ["redirect to the canonical uri for this resource ."], "bleu": 0.22149455506955362, "rouge_l": 0.465648854961832}
{"id": 4804, "code": "def require authentication ( self , request ) : request . user = user = None if request . method == 'OPTIONS' : return for auth in self . meta . authentication : user = auth . authenticate ( request ) if user is False : continue if user is None and not auth . allow anonymous : auth . unauthenticated ( ) request . user = user return if not user and not auth . allow anonymous : auth . unauthenticated ( )", "predictions": ["allow user to access token ."], "references": ["ensure we are authenticated ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4805, "code": "def require accessibility ( self , user , method ) : if method == 'OPTIONS' : return authz = self . meta . authorization if not authz . is accessible ( user , method , self ) : authz . unaccessible ( )", "predictions": ["require the user to be logged in the authorization ."], "references": ["ensure we are allowed to access this resource ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 4806, "code": "def require http allowed method ( cls , request ) : allowed = cls . meta . http allowed methods if request . method not in allowed : raise http . exceptions . Method Not Allowed ( allowed )", "predictions": ["raises an exception if the request method is allowed ."], "references": ["ensure that we re allowed to use this http method ."], "bleu": 0.13564514503163538, "rouge_l": 0.18885448916408668}
{"id": 4807, "code": "def resource ( * * kwargs ) : def inner ( function ) : name = kwargs . pop ( 'name' , None ) if name is None : name = utils . dasherize ( function . name ) methods = kwargs . pop ( 'methods' , None ) if isinstance ( methods , six . string types ) : methods = methods , handler = ( function , methods ) if name not in resources : handlers [ name ] = [ ] from armet import resources kwargs [ 'name' ] = name class Lightweight Resource ( resources . Resource ) : Meta = type ( str ( 'Meta' ) , ( ) , kwargs ) def route ( self , request , response ) : for handler , methods in handlers [ name ] : if methods is None or request . method in methods : return handler ( request , response ) resources . Resource . route ( self ) resources [ name ] = Lightweight Resource handlers [ name ] . append ( handler ) return resources [ name ] return inner", "predictions": ["decorator to add a resource to the methods ."], "references": ["wraps the decorated function in a lightweight resource ."], "bleu": 0.17747405280050263, "rouge_l": 0.3333333333333333}
{"id": 4808, "code": "def render to string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values", "predictions": ["render the table as a string ."], "references": ["render to cookie strings ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4809, "code": "def from cookie string ( self , cookie string ) : for key value in cookie string . split ( ';' ) : if '=' in key value : key , value = key value . split ( '=' , 1 ) else : key = key value strip key = key . strip ( ) if strip key and strip key . lower ( ) not in COOKIE ATTRIBUTE NAMES : self [ strip key ] = value . strip ( )", "predictions": ["populate the cookie from a cookie string ."], "references": ["update self with cookie_string ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4810, "code": "def error handler ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : try : return func ( * args , * * kwargs ) except Base Exception as err : if BOOTSTRAPPER TEST KEY in os . environ : raise if ERROR HANDLER DISABLED : return True return save traceback ( err ) return wrapper", "predictions": ["decorator to handle error messages ."], "references": ["decorator to error handling ."], "bleu": 0.3303164318013807, "rouge_l": 0.7393939393939394}
{"id": 4811, "code": "def iteritems ( data , * * kwargs ) : return iter ( data . items ( * * kwargs ) ) if IS PY3 else data . iteritems ( * * kwargs )", "predictions": ["ensure that data is a formatted type ."], "references": ["iterate over dict items ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4812, "code": "def iterkeys ( data , * * kwargs ) : return iter ( data . keys ( * * kwargs ) ) if IS PY3 else data . iterkeys ( * * kwargs )", "predictions": ["same as aggregate_numpy . return a difference instance ."], "references": ["iterate over dict keys ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4813, "code": "def remove nodes ( self , pattern , adict ) : mydict = self . filetree if adict is None else adict if isinstance ( mydict , dict ) : for nom in mydict . keys ( ) : if isinstance ( mydict [ nom ] , dict ) : matchs = filter list ( mydict [ nom ] , pattern ) for nom in matchs : mydict = self . remove nodes ( pattern , mydict [ nom ] ) mydict . pop ( nom ) else : mydict [ nom ] = filter list ( mydict [ nom ] , pattern ) else : matchs = set ( filter list ( mydict , pattern ) ) mydict = set ( mydict ) - matchs return mydict", "predictions": ["remove nodes matching a pattern ."], "references": ["remove the nodes that match the pattern ."], "bleu": 0.236682065782701, "rouge_l": 0.5570776255707762}
{"id": 4814, "code": "def num samples ( x ) : if not hasattr ( x , ' len ' ) and not hasattr ( x , 'shape' ) : if hasattr ( x , ' array ' ) : x = np . asarray ( x ) else : raise Type Error ( \"Expected sequence or array-like, got %r\" % x ) return x . shape [ 0 ] if hasattr ( x , 'shape' ) else len ( x )", "predictions": ["get the number of samples in a format of x ."], "references": ["return number of samples in array - like x ."], "bleu": 0.3646285861936466, "rouge_l": 0.5763779527559055}
{"id": 4815, "code": "def voxspace to mmspace ( img ) : shape , affine = img . shape [ : 3 ] , img . affine coords = np . array ( np . meshgrid ( * ( range ( i ) for i in shape ) , indexing = 'ij' ) ) coords = np . rollaxis ( coords , 0 , len ( shape ) + 1 ) mm coords = nib . affines . apply affine ( affine , coords ) return mm coords", "predictions": ["convert a 3d image to coordinates ."], "references": ["return a grid with coordinates in 3d physical space for img ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 4816, "code": "def load images and labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise Value Error ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise Value Error ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise Value Error ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first file = images [ 0 ] if first file : first img = Neuro Image ( first file ) else : raise ( 'Error reading image {}.' . format ( repr imgs ( first file ) ) ) for idx , image in enumerate ( images ) : try : img = Neuro Image ( image ) self . check compatibility ( img , first img ) except : log . exception ( 'Error reading image {}.' . format ( repr imgs ( image ) ) ) raise else : self . items . append ( img ) self . set labels ( labels )", "predictions": ["setter for image images"], "references": ["read the images load them into self . items and set the labels ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 4817, "code": "def die ( msg , code = - 1 ) : sys . stderr . write ( msg + \"\\n\" ) sys . exit ( code )", "predictions": ["print msg to stderr and exit with code ."], "references": ["writes msg to stderr and exits with return code"], "bleu": 0.42728700639623407, "rouge_l": 0.6666666666666666}
{"id": 4818, "code": "def clean ( ctx ) : ctx . run ( f'python setup.py clean' ) dist = ROOT . joinpath ( 'dist' ) print ( f'removing {dist}' ) shutil . rmtree ( str ( dist ) )", "predictions": ["clean previously built package artifacts ."], "references": ["clean previously built package artifacts ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 4819, "code": "def load command table ( self , args ) : #pylint: disable=too-many-statements with Command Super Group ( name , self , 'rcctl.custom cluster#{}' ) as super group : with super group . group ( 'cluster' ) as group : group . command ( 'select' , 'select' ) with Command Super Group ( name , self , 'rcctl.custom reliablecollections#{}' , client factory = client create ) as super group : with super group . group ( 'dictionary' ) as group : group . command ( 'query' , 'query reliabledictionary' ) group . command ( 'execute' , 'execute reliabledictionary' ) group . command ( 'schema' , 'get reliabledictionary schema' ) group . command ( 'list' , 'get reliabledictionary list' ) group . command ( 'type-schema' , 'get reliabledictionary type schema' ) with Arguments Context ( self , 'dictionary' ) as ac : ac . argument ( 'application name' , options list = [ '--application-name' , '-a' ] ) ac . argument ( 'service name' , options list = [ '--service-name' , '-s' ] ) ac . argument ( 'dictionary name' , options list = [ '--dictionary-name' , '-d' ] ) ac . argument ( 'output file' , options list = [ '--output-file' , '-out' ] ) ac . argument ( 'input file' , options list = [ '--input-file' , '-in' ] ) ac . argument ( 'query string' , options list = [ '--query-string' , '-q' ] ) ac . argument ( 'type name' , options list = [ '--type-name' , '-t' ] ) return Ordered Dict ( self . command table )", "predictions": ["load command table for click ."], "references": ["load all service fabric commands"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4820, "code": "def div img ( img1 , div2 ) : if is img ( div2 ) : return img1 . get data ( ) / div2 . get data ( ) elif isinstance ( div2 , ( float , int ) ) : return img1 . get data ( ) / div2 else : raise Not Implemented Error ( 'Cannot divide {}({}) by ' '{}({})' . format ( type ( img1 ) , img1 , type ( div2 ) , div2 ) )", "predictions": ["return the data as a div"], "references": ["pixelwise division or divide by a number"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4821, "code": "def apply mask ( img , mask ) : from . mask import apply mask vol , = apply mask ( img , mask ) return vector to volume ( vol , read img ( mask ) . get data ( ) . astype ( bool ) )", "predictions": ["apply a mask to a mask"], "references": ["return the image with the given mask applied ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4822, "code": "def abs img ( img ) : bool img = np . abs ( read img ( img ) . get data ( ) ) return bool img . astype ( int )", "predictions": ["reads a single image value from a numpy array"], "references": ["return an image with the binarised version of the data of img ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 4823, "code": "def spatial map ( icc , thr , mode = '+' ) : return thr img ( icc img to zscore ( icc ) , thr = thr , mode = mode ) . get data ( )", "predictions": ["map a specific spatial map to a specific spatial map"], "references": ["return the thresholded z - scored icc ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4824, "code": "def smooth fwhm ( self , fwhm ) : if fwhm != self . smooth fwhm : self . is data smooth = False self . smooth fwhm = fwhm", "predictions": ["smooth the fwhm of the given fwhm ."], "references": ["set a smoothing gaussian kernel given its fwhm in mm ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 4825, "code": "def setup logging ( log config file = op . join ( op . dirname ( file ) , 'logger.yml' ) , log default level = LOG LEVEL , env key = MODULE NAME . upper ( ) + ' LOG CFG' ) : path = log config file value = os . getenv ( env key , None ) if value : path = value if op . exists ( path ) : log cfg = yaml . load ( read ( path ) . format ( MODULE NAME ) ) logging . config . dict Config ( log cfg ) #print('Started logging using config file {0}.'.format(path)) else : logging . basic Config ( level = log default level ) #print('Started default logging. Could not find config file ' log = logging . get Logger ( name ) log . debug ( 'Start logging.' )", "predictions": ["setup logging logging ."], "references": ["setup logging configuration ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 4826, "code": "def select arg verify ( endpoint , cert , key , pem , ca , aad , no verify ) : #pylint: disable=invalid-name,too-many-arguments if not ( endpoint . lower ( ) . startswith ( 'http' ) or endpoint . lower ( ) . startswith ( 'https' ) ) : raise CLI Error ( 'Endpoint must be HTTP or HTTPS' ) usage = ( 'Valid syntax : --endpoint [ [ --key --cert | --pem | --aad] ' '[ --ca | --no-verify ] ]' ) if ca and not ( pem or all ( [ key , cert ] ) ) : raise CLI Error ( usage ) if no verify and not ( pem or all ( [ key , cert ] ) or aad ) : raise CLI Error ( usage ) if no verify and ca : raise CLI Error ( usage ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise CLI Error ( usage ) if aad and any ( [ pem , cert , key ] ) : raise CLI Error ( usage ) if pem and any ( [ cert , key ] ) : raise CLI Error ( usage )", "predictions": ["select a single cert from pem ."], "references": ["verify arguments for select command"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4827, "code": "def openpyxl read xl ( xl path : str ) : try : wb = load workbook ( filename = xl path , read only = True ) except : raise else : return wb", "predictions": ["reads a file and returns a dictionary that does not contain any workbook ."], "references": ["use openpyxl to read an excel file ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 4828, "code": "def read xl ( xl path : str ) : xl path , choice = check xl path ( xl path ) reader = XL READERS [ choice ] return reader ( xl path )", "predictions": ["reads a single xl file and returns a dictionary of the first element"], "references": ["return the workbook from the excel file in xl_path ."], "bleu": 0.10571070857151538, "rouge_l": 0.08905109489051095}
{"id": 4829, "code": "def col values ( df , col name ) : check cols ( df , [ col name ] ) if 'O' in df [ col name ] or pd . np . issubdtype ( df [ col name ] . dtype , str ) : return [ nom . lower ( ) for nom in df [ pd . notnull ( df ) ] [ col name ] if not pd . isnull ( nom ) ] else : return [ nom for nom in df [ pd . notnull ( df ) ] [ col name ] if not pd . isnull ( nom ) ]", "predictions": ["return a dataframe with column names from a dataframe ."], "references": ["return a list of not null values from the col_name column of df ."], "bleu": 0.13225016524497193, "rouge_l": 0.32360742705570295}
{"id": 4830, "code": "def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]", "predictions": ["create a list of values from a list of values ."], "references": ["return the duplicated items in values"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 4831, "code": "def repr imgs ( imgs ) : if isinstance ( imgs , string types ) : return imgs if isinstance ( imgs , collections . Iterable ) : return '[{}]' . format ( ', ' . join ( repr imgs ( img ) for img in imgs ) ) try : filename = imgs . get filename ( ) if filename is not None : img str = \"{}('{}')\" . format ( imgs . class . name , filename ) else : img str = \"{}(shape={}, affine={})\" . format ( imgs . class . name , repr ( get shape ( imgs ) ) , repr ( imgs . get affine ( ) ) ) except Exception as exc : log . error ( 'Error reading attributes from img.get filename()' ) return repr ( imgs ) else : return img str", "predictions": ["return the script for a given reading ."], "references": ["printing of img or imgs"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4832, "code": "def get config bool ( name ) : cli config = CLI Config ( SF CLI CONFIG DIR , SF CLI ENV VAR PREFIX ) return cli config . getboolean ( 'servicefabric' , name , False )", "predictions": ["create a start configuration object from the start of the script . . ."], "references": ["checks if a config value is set to a valid bool value ."], "bleu": 0.09782375748961449, "rouge_l": 0.1491442542787286}
{"id": 4833, "code": "def set config value ( name , value ) : cli config = CLI Config ( SF CLI CONFIG DIR , SF CLI ENV VAR PREFIX ) cli config . set value ( 'servicefabric' , name , value )", "predictions": ["create a config object"], "references": ["set a config by name to a value ."], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 4834, "code": "def set aad cache ( token , cache ) : set config value ( 'aad token' , jsonpickle . encode ( token ) ) set config value ( 'aad cache' , jsonpickle . encode ( cache ) )", "predictions": ["set a aad into the value in the value"], "references": ["set aad token cache ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 4835, "code": "def set aad metadata ( uri , resource , client ) : set config value ( 'authority uri' , uri ) set config value ( 'aad resource' , resource ) set config value ( 'aad client' , client )", "predictions": ["redirect the cls s cls in the aad s cls ."], "references": ["set aad metadata ."], "bleu": 0.12605968092174913, "rouge_l": 0.2911694510739857}
{"id": 4836, "code": "def set auth ( pem = None , cert = None , key = None , aad = False ) : if any ( [ cert , key ] ) and pem : raise Value Error ( 'Cannot specify both pem and cert or key' ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise Value Error ( 'Must specify both cert and key' ) if pem : set config value ( 'security' , 'pem' ) set config value ( 'pem path' , pem ) elif cert or key : set config value ( 'security' , 'cert' ) set config value ( 'cert path' , cert ) set config value ( 'key path' , key ) elif aad : set config value ( 'security' , 'aad' ) else : set config value ( 'security' , 'none' )", "predictions": ["require the user object in a self . authentication"], "references": ["set certificate usage paths"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4837, "code": "def dictify ( a named tuple ) : return dict ( ( s , getattr ( a named tuple , s ) ) for s in a named tuple . fields )", "predictions": ["convert a tuple to a new dict with a ."], "references": ["transform a named tuple into a dictionary"], "bleu": 0.14991106946711685, "rouge_l": 0.36454183266932266}
{"id": 4838, "code": "def create ( ) : endpoint = client endpoint ( ) if not endpoint : raise CLI Error ( \"Connection endpoint not found. \" \"Before running sfctl commands, connect to a cluster using \" \"the 'sfctl cluster select' command.\" ) no verify = no verify setting ( ) if security type ( ) == 'aad' : auth = Adal Authentication ( no verify ) else : cert = cert info ( ) ca cert = ca cert info ( ) auth = Client Cert Authentication ( cert , ca cert , no verify ) return Service Fabric Client AP Is ( auth , base url = endpoint )", "predictions": ["require an ssh cluster to a cluster cluster ."], "references": ["create a client for service fabric apis ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4839, "code": "def escape char ( c , escape char = ESCAPE CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape char ) buf . append ( '%X' % ord ( byte ) ) return '' . join ( buf )", "predictions": ["resource can be a unicode string is unicode"], "references": ["escape a single character"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4840, "code": "def copy attributes ( source , destination , ignore patterns = [ ] ) : for attr in wildcard filter ( dir ( source ) , * ignore patterns ) : setattr ( destination , attr , getattr ( source , attr ) )", "predictions": ["render to destination from source ."], "references": ["copy the attributes from a source object to a destination object ."], "bleu": 0.1069482072978842, "rouge_l": 0.31443298969072164}
{"id": 4841, "code": "def query ( self , input = '' , params = { } ) : payload = { 'input' : input , 'appid' : self . appid } for key , value in params . items ( ) : if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value try : r = requests . get ( \"http://api.wolframalpha.com/v2/query\" , params = payload ) if r . status code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )", "predictions": ["from the bot if available"], "references": ["query wolfram alpha and return a result object"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 4842, "code": "def pods ( self ) : if not self . xml tree : return [ ] return [ Pod ( elem ) for elem in self . xml tree . findall ( 'pod' ) ]", "predictions": ["a list of error wrapper wraps to the wrapper wraps wraps wraps wraps wraps wraps the wrapper wraps wraps wraps ."], "references": ["return list of all pod objects in result"], "bleu": 0.07645949399477267, "rouge_l": 0.15006150061500614}
{"id": 4843, "code": "def get params ( target , param , dof ) : return [ target . get Param ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) ) for s in [ '' , '2' , '3' ] [ : dof ] ]", "predictions": ["iteritems parameters from a target items items items items items items items items items"], "references": ["get the given param from each of the dofs for a joint ."], "bleu": 0.09782375748961449, "rouge_l": 0.1491442542787286}
{"id": 4844, "code": "def set params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . set Param ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )", "predictions": ["set will set the current parameter ."], "references": ["set the given param for each of the dofs for a joint ."], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 4845, "code": "def make quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]", "predictions": ["remove a nodes from a nodes"], "references": ["given an angle and an axis create a quaternion ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 4846, "code": "def center of mass ( bodies ) : x = np . zeros ( 3. ) t = 0. for b in bodies : m = b . mass x += b . body to world ( m . c ) * m . mass t += m . mass return x / t", "predictions": ["or num samples of the mass samples"], "references": ["given a set of bodies compute their center of mass in world coordinates ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 4847, "code": "def positions ( self ) : return [ self . ode obj . get Position ( i ) for i in range ( self . LDOF ) ]", "predictions": ["a list of all positions positions shape shape shape shape shape shape shape shape shape shape shape shape ."], "references": ["list of positions for linear degrees of freedom ."], "bleu": 0.09629943614188137, "rouge_l": 0.3053817271589487}
{"id": 4848, "code": "def position rates ( self ) : return [ self . ode obj . get Position Rate ( i ) for i in range ( self . LDOF ) ]", "predictions": ["the list of load images images in the ode . . . . . . . . . . . . . . . . . . . . . ."], "references": ["list of position rates for linear degrees of freedom ."], "bleu": 0.055177848898164926, "rouge_l": 0.16123348017621145}
{"id": 4849, "code": "def angles ( self ) : return [ self . ode obj . get Angle ( i ) for i in range ( self . ADOF ) ]", "predictions": ["a list of all die die die die 1 1 1 1 1 1 1 1 1 1 1 and the result of the ode 1 1 1 1 1 die"], "references": ["list of angles for rotational degrees of freedom ."], "bleu": 0.055177848898164926, "rouge_l": 0.1665150136487716}
{"id": 4850, "code": "def angle rates ( self ) : return [ self . ode obj . get Angle Rate ( i ) for i in range ( self . ADOF ) ]", "predictions": ["the clean clean rates rates rates s ode ."], "references": ["list of angle rates for rotational degrees of freedom ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 4851, "code": "def axes ( self ) : return [ np . array ( self . ode obj . get Axis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]", "predictions": ["a list of the load load balancer load balancer load balancer load balancer ."], "references": ["list of axes for this object s degrees of freedom ."], "bleu": 0.1250076305588977, "rouge_l": 0.24530831099195713}
{"id": 4852, "code": "def axes ( self ) : return [ np . array ( self . ode obj . get Axis1 ( ) ) , np . array ( self . ode obj . get Axis2 ( ) ) ]", "predictions": ["a list of the div div div div if any ."], "references": ["a list of axes of rotation for this joint ."], "bleu": 0.22416933501922287, "rouge_l": 0.384251968503937}
{"id": 4853, "code": "def create bodies ( self , translate = ( 0 , 1 , 0 ) , size = 0.1 ) : stack = [ ( 'root' , 0 , self . root [ 'position' ] + translate ) ] while stack : name , depth , end = stack . pop ( ) for child in self . hierarchy . get ( name , ( ) ) : stack . append ( ( child , depth + 1 , end + self . bones [ child ] . end ) ) if name not in self . bones : continue bone = self . bones [ name ] body = self . world . create body ( 'box' , name = bone . name , density = self . density , lengths = ( size , size , bone . length ) ) body . color = self . color x , y , z = end - bone . direction * bone . length / 2 body . position = x , z , y u = bone . direction v = np . cross ( u , [ 0 , 1 , 0 ] ) l = np . linalg . norm ( v ) if l > 0 : v /= l rot = np . vstack ( [ np . cross ( u , v ) , v , u ] ) . T swizzle = [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] , [ 0 , - 1 , 0 ] ] body . rotation = np . dot ( swizzle , rot ) self . bodies . append ( body )", "predictions": ["apply the mask to the image"], "references": ["traverse the bone hierarchy and create physics bodies ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4854, "code": "def create joints ( self ) : stack = [ 'root' ] while stack : parent = stack . pop ( ) for child in self . hierarchy . get ( parent , ( ) ) : stack . append ( child ) if parent not in self . bones : continue bone = self . bones [ parent ] body = [ b for b in self . bodies if b . name == parent ] [ 0 ] for child in self . hierarchy . get ( parent , ( ) ) : child bone = self . bones [ child ] child body = [ b for b in self . bodies if b . name == child ] [ 0 ] shape = ( '' , 'hinge' , 'universal' , 'ball' ) [ len ( child bone . dof ) ] self . joints . append ( self . world . join ( shape , body , child body ) )", "predictions": ["abs the factory factory"], "references": ["traverse the bone hierarchy and create physics joints ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4855, "code": "def joint torques ( self ) : return as flat array ( getattr ( j , 'amotor' , j ) . feedback [ - 1 ] [ : j . ADOF ] for j in self . joints )", "predictions": ["returns the spatial map of the two ."], "references": ["get a list of all current joint torques in the skeleton ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 4856, "code": "def labels ( self ) : return sorted ( self . channels , key = lambda c : self . channels [ c ] )", "predictions": ["returns a list of unique smooth smooth channels if any ."], "references": ["return the names of our marker labels in canonical order ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 4857, "code": "def process data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros like ( self . positions ) + 1000 for frame no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame no - 1 ] next = self . data [ frame no + 1 ] for c in range ( self . num markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros like ( self . visibility ) + self . DEFAULT CFM", "predictions": ["setup logging logging logging"], "references": ["process data to produce velocity and dropout information ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 4858, "code": "def create bodies ( self ) : self . bodies = { } for label in self . channels : body = self . world . create body ( 'sphere' , name = 'marker:{}' . format ( label ) , radius = 0.02 ) body . is kinematic = True body . color = 0.9 , 0.1 , 0.1 , 0.5 self . bodies [ label ] = body", "predictions": ["select the arg cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert cert"], "references": ["create physics bodies corresponding to each marker in our data ."], "bleu": 0.03392268780792677, "rouge_l": 0.0}
{"id": 4859, "code": "def forward dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set body states ( states ) for frame no , torque in enumerate ( torques ) : if frame no < start : continue if frame no >= end : break self . ode space . collide ( None , self . on collision ) self . skeleton . add torques ( torque ) self . ode world . step ( self . dt ) yield self . ode contactgroup . empty ( )", "predictions": ["openpyxl read else states"], "references": ["move the body according to a set of torque data ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 4860, "code": "def render ( self , dt ) : for frame in self . frozen : for body in frame : self . draw body ( body ) for body in self . world . bodies : self . draw body ( body ) if hasattr ( self . world , 'markers' ) : window . gl Color4f ( 0.9 , 0.1 , 0.1 , 0.9 ) window . gl Line Width ( 3 ) for j in self . world . markers . joints . values ( ) : window . gl Begin ( window . GL LINES ) window . gl Vertex3f ( * j . get Anchor ( ) ) window . gl Vertex3f ( * j . get Anchor2 ( ) ) window . gl End ( )", "predictions": ["read the z on the page"], "references": ["draw all bodies in the world ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4861, "code": "def response status string ( code ) : mean = HTTP STATUS CODES . get ( code , 'unknown' ) . upper ( ) return '{code} {mean}' . format ( code = code , mean = mean )", "predictions": [". string for pretty - print a col col"], "references": ["e . g . 200 ok"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4862, "code": "def fetch ( self ) : try : if not self . last message id : messages = self . connection . get ( \"room/%s/recent\" % self . room id , key = \"messages\" , parameters = { \"limit\" : 1 } ) self . last message id = messages [ - 1 ] [ \"id\" ] messages = self . connection . get ( \"room/%s/recent\" % self . room id , key = \"messages\" , parameters = { \"since message id\" : self . last message id } ) except : messages = [ ] if messages : self . last message id = messages [ - 1 ] [ \"id\" ] self . received ( messages )", "predictions": ["duplicated the message vals to the ."], "references": ["fetch new messages ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4863, "code": "def connection Made ( self ) : headers = [ \"GET %s HTTP/1.1\" % ( \"/room/%s/live.json\" % self . factory . get stream ( ) . get room id ( ) ) ] connection headers = self . factory . get stream ( ) . get connection ( ) . get headers ( ) for header in connection headers : headers . append ( \"%s: %s\" % ( header , connection headers [ header ] ) ) headers . append ( \"Host: streaming.campfirenow.com\" ) self . transport . write ( \"\\r\\n\" . join ( headers ) + \"\\r\\n\\r\\n\" ) self . factory . get stream ( ) . set protocol ( self )", "predictions": ["register all the repr client client . . ."], "references": ["called when a connection is made and used to send out headers"], "bleu": 0.08504083946364166, "rouge_l": 0.0}
{"id": 4864, "code": "def styles ( self ) : styles = get all styles ( ) whitelist = self . app . config . get ( 'CSL STYLES WHITELIST' ) if whitelist : return { k : v for k , v in styles . items ( ) if k in whitelist } return styles", "predictions": ["returns the styles styles"], "references": ["get a dictionary of csl styles ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4865, "code": "def init app ( self , app ) : state = Invenio CSLREST State ( app ) app . extensions [ 'invenio-csl-rest' ] = state return state", "predictions": ["flask application initialization ."], "references": ["flask application initialization ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 4866, "code": "def build chunk headers ( self ) : if hasattr ( self , \" chunk headers\" ) and self . chunk headers : return self . chunk headers = { } for field in self . files : self . chunk headers [ field ] = self . headers ( field , True ) for field in self . data : self . chunk headers [ field ] = self . headers ( field )", "predictions": ["create a dictionary of chunk headers ."], "references": ["build headers for each field ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 4867, "code": "def str to path ( s , result type ) : assert isinstance ( s , str ) if isinstance ( s , bytes ) and result type is text type : return s . decode ( 'ascii' ) elif isinstance ( s , text type ) and result type is bytes : return s . encode ( 'ascii' ) return s", "predictions": ["convert a string to a path ."], "references": ["given an ascii str returns a path of the given type ."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 4868, "code": "def path root ( draw , result type ) : def tp ( s = '' ) : return str to path ( s , result type ) if os . name != 'nt' : return tp ( os . sep ) sep = sampled from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) name = filename ( result type ) char = characters ( min codepoint = ord ( \"A\" ) , max codepoint = ord ( \"z\" ) ) . map ( lambda c : tp ( str ( c ) ) ) relative = sep drive = builds ( lambda * x : tp ( ) . join ( x ) , char , just ( tp ( ':' ) ) , sep ) extended = builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , drive ) network = one of ( [ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , just ( tp ( 'UNC' ) ) , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '.' ) ) , sep , name , sep ) , ] ) final = one of ( relative , drive , extended , network ) return draw ( final )", "predictions": ["return a root path for the given path ."], "references": ["generates a root component for a path ."], "bleu": 0.24446151121745052, "rouge_l": 0.594541910331384}
{"id": 4869, "code": "def handle extends ( self , text ) : match = self . re extends . match ( text ) if match : extra text = self . re extends . sub ( '' , text , count = 1 ) blocks = self . get blocks ( extra text ) path = os . path . join ( self . base dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace blocks in extends ( fp . read ( ) , blocks ) else : return None", "predictions": ["handle a single extends text ."], "references": ["replace all blocks in extends with current blocks"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4870, "code": "def flush buffer ( self ) : self . code builder . add line ( '{0}.extend([{1}])' , self . result var , ',' . join ( self . buffered ) ) self . buffered = [ ]", "predictions": ["flush the buffer of the buffer ."], "references": ["flush all buffered string into code"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4871, "code": "def init async ( self , loop = None ) : self . loop = loop or asyncio . get event loop ( ) self . async lock = asyncio . Lock ( loop = loop ) if not self . database == ':memory:' : self . state = Connection Local ( )", "predictions": ["initializes the async connection ."], "references": ["use when application is starting ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4872, "code": "async def async connect ( self ) : if self . async lock is None : raise Exception ( 'Error, database not properly initialized before async connection' ) async with self . async lock : self . connect ( True ) return self . state . conn", "predictions": ["connect to the async server ."], "references": ["catch a connection asyncrounosly ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4873, "code": "async def async connect ( self ) : if self . waiters is None : raise Exception ( 'Error, database not properly initialized before async connection' ) if self . waiters or self . max connections and ( len ( self . in use ) >= self . max connections ) : waiter = asyncio . Future ( loop = self . loop ) self . waiters . append ( waiter ) try : logger . debug ( 'Wait for connection.' ) await waiter finally : self . waiters . remove ( waiter ) self . connect ( ) return self . state . conn", "predictions": ["connect to the asyncio server ."], "references": ["asyncronously wait for a connection from the pool ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4874, "code": "def generate ( request ) : models . Data Item . create ( content = '' . join ( random . choice ( string . ascii uppercase + string . digits ) for in range ( 20 ) ) ) return muffin . HTTP Found ( '/' )", "predictions": ["generates a random rdf file with the given request ."], "references": ["create a new dataitem ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 4875, "code": "def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }", "predictions": ["scan the data for a list of xml entities ."], "references": ["converts xml tree to event generator"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 4876, "code": "def unscan ( events , nsmap = None ) : root = None last closed elt = None stack = [ ] for obj in events : if obj [ 'type' ] == ENTER : elt = obj2elt ( obj , nsmap = nsmap ) if stack : stack [ - 1 ] . append ( elt ) elif root is not None : raise Runtime Error ( 'Event stream tried to create second XML tree' ) else : root = elt stack . append ( elt ) last closed elt = None elif obj [ 'type' ] == EXIT : last closed elt = stack . pop ( ) elif obj [ 'type' ] == COMMENT : elt = et . Comment ( obj [ 'text' ] ) stack [ - 1 ] . append ( elt ) elif obj [ 'type' ] == PI : elt = et . PI ( obj [ 'target' ] ) if obj . get ( 'text' ) : elt . text = obj [ 'text' ] stack [ - 1 ] . append ( elt ) elif obj [ 'type' ] == TEXT : text = obj [ 'text' ] if text : if last closed elt is None : stack [ - 1 ] . text = ( stack [ - 1 ] . text or '' ) + text else : last closed elt . tail = ( last closed elt . tail or '' ) + text else : assert False , obj if root is None : raise Runtime Error ( 'Empty XML event stream' ) return root", "predictions": ["return a root object given a list of events ."], "references": ["converts events stream into lxml tree"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 4877, "code": "def parse ( filename ) : for event , elt in et . iterparse ( filename , events = ( 'start' , 'end' , 'comment' , 'pi' ) , huge tree = True ) : if event == 'start' : obj = elt2obj ( elt ) obj [ 'type' ] = ENTER yield obj if elt . text : yield { 'type' : TEXT , 'text' : elt . text } elif event == 'end' : yield { 'type' : EXIT } if elt . tail : yield { 'type' : TEXT , 'text' : elt . tail } elt . clear ( ) elif event == 'comment' : yield { 'type' : COMMENT , 'text' : elt . text } elif event == 'pi' : yield { 'type' : PI , 'text' : elt . text } else : assert False , ( event , elt )", "predictions": ["parse a file into a list of ast objects ."], "references": ["parses file content into events stream"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 4878, "code": "def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj", "predictions": ["generate a subtree of events from events ."], "references": ["selects sub - tree events"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4879, "code": "def merge text ( events ) : text = [ ] for obj in events : if obj [ 'type' ] == TEXT : text . append ( obj [ 'text' ] ) else : if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) } text . clear ( ) yield obj if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) }", "predictions": ["merge multiple events into single text ."], "references": ["merges each run of successive text events into one text event"], "bleu": 0.14834636222628117, "rouge_l": 0.32049036777583184}
{"id": 4880, "code": "def setup ( self , app ) : super ( ) . setup ( app ) self . database . initialize ( connect ( self . cfg . connection , * * self . cfg . connection params ) ) if self . database . database == ':memory:' : self . cfg . connection manual = True if not self . cfg . migrations enabled : return self . router = Router ( self . database , migrate dir = self . cfg . migrations path ) def pw migrate ( name : str = None , fake : bool = False ) : self . router . run ( name , fake = fake ) self . app . manage . command ( pw migrate ) def pw rollback ( name : str = None ) : if not name : name = self . router . done [ - 1 ] self . router . rollback ( name ) self . app . manage . command ( pw rollback ) def pw create ( name : str = 'auto' , auto : bool = False ) : if auto : auto = list ( self . models . values ( ) ) self . router . create ( name , auto ) self . app . manage . command ( pw create ) def pw list ( ) : \"\"\"List migrations.\"\"\" self . router . logger . info ( 'Migrations are done:' ) self . router . logger . info ( '\\n' . join ( self . router . done ) ) self . router . logger . info ( '' ) self . router . logger . info ( 'Migrations are undone:' ) self . router . logger . info ( '\\n' . join ( self . router . diff ) ) self . app . manage . command ( pw list ) @ self . app . manage . command def pw merge ( ) : \"\"\"Merge migrations into one.\"\"\" self . router . merge ( ) self . app . manage . command ( pw merge )", "predictions": ["initialize the plugin ."], "references": ["initialize the application ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 4881, "code": "def startup ( self , app ) : self . database . init async ( app . loop ) if not self . cfg . connection manual : app . middlewares . insert ( 0 , self . middleware )", "predictions": ["start the database ."], "references": ["register connection s middleware and prepare self database ."], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 4882, "code": "def cleanup ( self , app ) : if hasattr ( self . database . obj , 'close all' ) : self . database . close all ( )", "predictions": ["closes connection to the database ."], "references": ["close all connections ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4883, "code": "def register ( self , model ) : self . models [ model . meta . table name ] = model model . meta . database = self . database return model", "predictions": ["registers a model class with the given model ."], "references": ["register a model in self ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 4884, "code": "async def manage ( self ) : cm = Context Manager ( self . database ) if isinstance ( self . database . obj , AIO Database ) : cm . connection = await self . database . async connect ( ) else : cm . connection = self . database . connect ( ) return cm", "predictions": ["manage connection to database ."], "references": ["manage a database connection ."], "bleu": 0.34329452398451965, "rouge_l": 0.6}
{"id": 4885, "code": "def not followed by ( parser ) : @ tri def not followed by block ( ) : failed = object ( ) result = optional ( tri ( parser ) , failed ) if result != failed : fail ( [ \"not \" + fun to str ( parser ) ] ) choice ( not followed by block )", "predictions": ["validate that the parser is not defined ."], "references": ["succeeds if the given parser cannot consume input"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4886, "code": "def many until1 ( these , term ) : first = [ these ( ) ] these results , term result = many until ( these , term ) return ( first + these results , term result )", "predictions": ["returns the many until1 ."], "references": ["like many_until but must consume at least one of these ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4887, "code": "def sep1 ( parser , separator ) : first = [ parser ( ) ] def inner ( ) : separator ( ) return parser ( ) return first + many ( tri ( inner ) )", "predictions": ["create a sep1 instance for a given separator ."], "references": ["like sep but must consume at least one of parser ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 4888, "code": "def fill ( self , size ) : try : for i in range ( size ) : self . buffer . append ( self . source . next ( ) ) except Stop Iteration : self . buffer . append ( ( End Of File , End Of File ) ) self . len = len ( self . buffer )", "predictions": ["fill the buffer with the next size ."], "references": ["fills the internal buffer from the source iterator"], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 4889, "code": "def next ( self ) : self . index += 1 t = self . peek ( ) if not self . depth : self . cut ( ) return t", "predictions": ["returns the next element ."], "references": ["advances to and returns the next token or returns endoffile"], "bleu": 0.18693159143202892, "rouge_l": 0.37731958762886597}
{"id": 4890, "code": "def field type ( self ) : if not self . model : return 'JSON' database = self . model . meta . database if isinstance ( database , Proxy ) : database = database . obj if Json and isinstance ( database , Postgresql Database ) : return 'JSON' return 'TEXT'", "predictions": ["get the type of the database from the database ."], "references": ["return database field type ."], "bleu": 0.14991106946711685, "rouge_l": 0.2837209302325582}
{"id": 4891, "code": "def python value ( self , value ) : if self . field type == 'TEXT' and isinstance ( value , str ) : return self . loads ( value ) return value", "predictions": ["converts string to native python value ."], "references": ["parse value from database ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4892, "code": "def get fsapi endpoint ( self ) : endpoint = yield from self . session . get ( self . fsapi device url , timeout = self . timeout ) text = yield from endpoint . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . webfsapi . text", "predictions": ["returns a opencv xml endpoint ."], "references": ["parse the fsapi endpoint from the device url ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4893, "code": "def create session ( self ) : req url = '%s/%s' % ( self . webfsapi , 'CREATE SESSION' ) sid = yield from self . session . get ( req url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . session Id . text", "predictions": ["create a session ."], "references": ["create a session on the frontier silicon device ."], "bleu": 0.20258948470231466, "rouge_l": 0.5754716981132075}
{"id": 4894, "code": "def call ( self , path , extra = None ) : try : if not self . webfsapi : self . webfsapi = yield from self . get fsapi endpoint ( ) if not self . sid : self . sid = yield from self . create session ( ) if not isinstance ( extra , dict ) : extra = dict ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) req url = ( '%s/%s' % ( self . webfsapi , path ) ) result = yield from self . session . get ( req url , params = params , timeout = self . timeout ) if result . status == 200 : text = yield from result . text ( encoding = 'utf-8' ) else : self . sid = yield from self . create session ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) result = yield from self . session . get ( req url , params = params , timeout = self . timeout ) text = yield from result . text ( encoding = 'utf-8' ) return objectify . fromstring ( text ) except Exception as e : logging . info ( 'AFSAPI Exception: ' + traceback . format exc ( ) ) return None", "predictions": ["make a call to the api ."], "references": ["execute a frontier silicon api call ."], "bleu": 0.23356898886410002, "rouge_l": 0.42857142857142855}
{"id": 4895, "code": "def handle set ( self , item , value ) : doc = yield from self . call ( 'SET/{}' . format ( item ) , dict ( value = value ) ) if doc is None : return None return doc . status == 'FS OK'", "predictions": ["handle a set item ."], "references": ["helper method for setting a value by using the fsapi api ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 4896, "code": "def handle text ( self , item ) : doc = yield from self . handle get ( item ) if doc is None : return None return doc . value . c8 array . text or None", "predictions": ["styles text text text"], "references": ["helper method for fetching a text value ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4897, "code": "def handle int ( self , item ) : doc = yield from self . handle get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None", "predictions": ["init the item ."], "references": ["helper method for fetching a integer value ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4898, "code": "def handle long ( self , item ) : doc = yield from self . handle get ( item ) if doc is None : return None return int ( doc . value . u32 . text ) or None", "predictions": ["build the chunk of an item and } it ."], "references": ["helper method for fetching a long value . result is integer ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 4899, "code": "def get power ( self ) : power = ( yield from self . handle int ( self . API . get ( 'power' ) ) ) return bool ( power )", "predictions": ["returns the to use for the server . . . . . . . . . . ."], "references": ["check if the device is on ."], "bleu": 0.07535838128770536, "rouge_l": 0.17378917378917377}
{"id": 4900, "code": "def set power ( self , value = False ) : power = ( yield from self . handle set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )", "predictions": ["path the root result of the channel . . . . . . . . . . . ."], "references": ["power on or off the device ."], "bleu": 0.0712695567709093, "rouge_l": 0.16781292984869325}
{"id": 4901, "code": "def get modes ( self ) : if not self . modes : self . modes = yield from self . handle list ( self . API . get ( 'valid modes' ) ) return self . modes", "predictions": ["return a list of the extends extends extends extends extends extends extends match ."], "references": ["get the modes supported by this device ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 4902, "code": "def get mode list ( self ) : self . modes = yield from self . get modes ( ) return ( yield from self . collect labels ( self . modes ) )", "predictions": ["return a list of the server"], "references": ["get the label list of the supported modes ."], "bleu": 0.2493651438887133, "rouge_l": 0.3860759493670886}
{"id": 4903, "code": "def get volume steps ( self ) : if not self . volume steps : self . volume steps = yield from self . handle int ( self . API . get ( 'volume steps' ) ) return self . volume steps", "predictions": ["returns a list of async steps steps steps"], "references": ["read the maximum volume level of the device ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4904, "code": "def get mute ( self ) : mute = ( yield from self . handle int ( self . API . get ( 'mute' ) ) ) return bool ( mute )", "predictions": ["def for connect to the connect is connect is connect is connect is connect is connect to ."], "references": ["check if the device is muted ."], "bleu": 0.08097785064266204, "rouge_l": 0.26068376068376065}
{"id": 4905, "code": "def set mute ( self , value = False ) : mute = ( yield from self . handle set ( self . API . get ( 'mute' ) , int ( value ) ) ) return bool ( mute )", "predictions": ["def the bool . ."], "references": ["mute or unmute the device ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 4906, "code": "def get play status ( self ) : status = yield from self . handle int ( self . API . get ( 'status' ) ) return self . PLAY STATES . get ( status )", "predictions": ["choice request request request content content content content content content content content content content content content content content content content content content content content content content content content content content content"], "references": ["get the play status of the device ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4907, "code": "def get equalisers ( self ) : if not self . equalisers : self . equalisers = yield from self . handle list ( self . API . get ( 'equalisers' ) ) return self . equalisers", "predictions": ["returns the active equalisers"], "references": ["get the equaliser modes supported by this device ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4908, "code": "def get equaliser list ( self ) : self . equalisers = yield from self . get equalisers ( ) return ( yield from self . collect labels ( self . equalisers ) )", "predictions": ["return the events from the table"], "references": ["get the label list of the supported modes ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4909, "code": "def set sleep ( self , value = False ) : return ( yield from self . handle set ( self . API . get ( 'sleep' ) , int ( value ) ) )", "predictions": ["parse a sleep and et for a message events events events events events events events events events events events events events events events events events events events events events events events"], "references": ["set device sleep timer ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 4910, "code": "def parse genotype ( self , vcf fields ) : format col = vcf fields [ 8 ] . split ( ':' ) genome data = vcf fields [ 9 ] . split ( ':' ) try : gt idx = format col . index ( 'GT' ) except Value Error : return [ ] return [ int ( x ) for x in re . split ( r'[\\|/]' , genome data [ gt idx ] ) if x != '.' ]", "predictions": ["subtree for a stack"], "references": ["parse genotype from vcf line data"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 4911, "code": "def obj Has Unsaved Changes ( self ) : if not self . obj : return False return self . obj . has Unsaved Changes ( cascade Objects = True )", "predictions": ["returns true if the lock is events obj for this object obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj"], "references": ["objhasunsavedchanges - check if any object has unsaved changes cascading ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 4912, "code": "def items ( self ) : if self . meta type == 'list' : return self . list elif self . meta type == 'dict' : return self . dict . items ( )", "predictions": ["returns a dictionary of setup objects . from the super super super super"], "references": ["return keys for object if they are available ."], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 4913, "code": "def values ( self ) : if self . meta type == 'list' : return self . list elif self . meta type == 'dict' : return self . dict . values ( )", "predictions": ["returns a dictionary of the startup startup startup ."], "references": ["return keys for object if they are available ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4914, "code": "def append ( self , item ) : if self . meta type == 'dict' : raise Assertion Error ( 'Cannot append to object of `dict` base type!' ) if self . meta type == 'list' : self . list . append ( item ) return", "predictions": ["cleanup an app to the list"], "references": ["append to object if object is list ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4915, "code": "def extend ( self , item ) : if self . meta type == 'dict' : raise Assertion Error ( 'Cannot extend to object of `dict` base type!' ) if self . meta type == 'list' : self . list . extend ( item ) return", "predictions": ["register a new model to the collection ."], "references": ["extend list from object if object is list ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4916, "code": "def json ( self ) : if self . meta type == 'list' : ret = [ ] for dat in self . list : if not isinstance ( dat , composite ) : ret . append ( dat ) else : ret . append ( dat . json ( ) ) return ret elif self . meta type == 'dict' : ret = { } for key in self . dict : if not isinstance ( self . dict [ key ] , composite ) : ret [ key ] = self . dict [ key ] else : ret [ key ] = self . dict [ key ] . json ( ) return ret", "predictions": ["convert the connection to a dictionary"], "references": ["return json representation of object ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4917, "code": "def json ( self ) : data = { } for item in self . data : if isinstance ( self . data [ item ] , filetree ) : data [ item ] = self . data [ item ] . json ( ) else : data [ item ] = self . data [ item ] return data", "predictions": ["returns a not serializable instance"], "references": ["return json representation of object ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 4918, "code": "def filelist ( self ) : if len ( self . filelist ) == 0 : for item in self . data : if isinstance ( self . data [ item ] , filetree ) : self . filelist . extend ( self . data [ item ] . filelist ( ) ) else : self . filelist . append ( self . data [ item ] ) return self . filelist", "predictions": ["return a list of items in the folder . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["return list of files in filetree ."], "bleu": 0.06106432774355542, "rouge_l": 0.29698149951314506}
{"id": 4919, "code": "def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . get Model ( ) return mdl . saver . save ( self )", "predictions": ["save the model to a file"], "references": ["save - save all objects in this list"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4920, "code": "def render ( self , * args , * * kwargs ) : render to = String IO ( ) self . output ( render to , * args , * * kwargs ) return render to . getvalue ( )", "predictions": ["fill the cell with the specified args buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer"], "references": ["renders as a str"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4921, "code": "def start tag ( self ) : direct attributes = ( attribute . render ( self ) for attribute in self . render attributes ) attributes = ( ) if hasattr ( self , ' attributes' ) : attributes = ( '{0}=\"{1}\"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered attributes = \" \" . join ( filter ( bool , chain ( direct attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered attributes else '' , rendered attributes , ' /' if self . tag self closes else \"\" )", "predictions": ["next tag tag tag"], "references": ["returns the elements html start tag"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4922, "code": "def output ( self , to = None , formatted = False , * args , * * kwargs ) : to . write ( '<!DOCTYPE {0}>' . format ( self . type ) )", "predictions": ["print the field field meta with the given type meta meta data meta meta meta data meta meta meta information meta meta meta meta meta information"], "references": ["outputs the set text"], "bleu": 0.04668049023095243, "rouge_l": 0.07682619647355164}
{"id": 4923, "code": "def as dict ( self ) : self as dict = dict ( ) self as dict [ 'sequence' ] = self . sequence if hasattr ( self , 'frequency' ) : self as dict [ 'frequency' ] = self . frequency return self as dict", "predictions": ["returns a json dictionary representing this model ."], "references": ["return allele data as dict object ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4924, "code": "def parse allele data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref allele ] + self . alt alleles ]", "predictions": ["get a list of fsapi endpoint timeout timeout timeout timeout timeout timeout timeout timeout timeout timeout timeout timeout timeout timeout timeout"], "references": ["create list of alleles from vcf line data"], "bleu": 0.07645949399477267, "rouge_l": 0.15006150061500614}
{"id": 4925, "code": "def parse info ( self , info field ) : info = dict ( ) for item in info field . split ( ';' ) : info item data = item . split ( '=' ) if len ( info item data ) == 1 : info [ info item data [ 0 ] ] = True elif len ( info item data ) == 2 : info [ info item data [ 0 ] ] = info item data [ 1 ] return info", "predictions": ["create session information from a list of session session"], "references": ["parse the vcf info field"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4926, "code": "def as dict ( self ) : self as dict = { 'chrom' : self . chrom , 'start' : self . start , 'ref allele' : self . ref allele , 'alt alleles' : self . alt alleles , 'alleles' : [ x . as dict ( ) for x in self . alleles ] } try : self as dict [ 'info' ] = self . info except Attribute Error : pass return self as dict", "predictions": ["json - serializable dict representation of a dict"], "references": ["dict representation of parsed vcf data"], "bleu": 0.2984745896009823, "rouge_l": 0.43990384615384615}
{"id": 4927, "code": "def as dict ( self , * args , * * kwargs ) : self as dict = super ( Clin Var Allele , self ) . as dict ( * args , * * kwargs ) self as dict [ 'hgvs' ] = self . hgvs self as dict [ 'clnalleleid' ] = self . clnalleleid self as dict [ 'clnsig' ] = self . clnsig self as dict [ 'clndn' ] = self . clndn self as dict [ 'clndisdb' ] = self . clndisdb self as dict [ 'clnvi' ] = self . clnvi return self as dict", "predictions": ["json - serializable set of object ."], "references": ["return clinvarallele data as dict object ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 4928, "code": "def parse frequencies ( self ) : frequencies = Ordered Dict ( [ ( 'EXAC' , 'Unknown' ) , ( 'ESP' , 'Unknown' ) , ( 'TGP' , 'Unknown' ) ] ) pref freq = 'Unknown' for source in frequencies . keys ( ) : freq key = 'AF ' + source if freq key in self . info : frequencies [ source ] = self . info [ freq key ] if pref freq == 'Unknown' : pref freq = frequencies [ source ] return pref freq , frequencies", "predictions": ["parse the frequencies from the metadata file"], "references": ["parse frequency data in clinvar vcf"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4929, "code": "def parse allele data ( self ) : pref freq , frequencies = self . parse frequencies ( ) info clnvar single tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info clnvar single tags } cln data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt alleles [ 0 ] except Index Error : sequence = self . ref allele allele = Clin Var Allele ( frequency = pref freq , sequence = sequence , * * cln data ) if not cln data [ 'clnsig' ] : return [ ] return [ allele ]", "predictions": ["parse allele data ."], "references": ["parse alleles for clinvar vcf overrides parent method ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 4930, "code": "def add ( self , * names ) : def decorator ( blok ) : for name in names or ( blok . name , ) : self [ name ] = blok return blok return decorator", "predictions": ["add a decorator to the blok ."], "references": ["returns back a class decorator that enables registering blox to this factory"], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 4931, "code": "def filter ( filter Obj , * * kwargs ) : for key , value in kwargs . items ( ) : if key . endswith ( ' ne' ) : not Filter = True key = key [ : - 4 ] else : not Filter = False if key not in filter Obj . indexed Fields : raise Value Error ( 'Field \"' + key + '\" is not in INDEXED FIELDS array. Filtering is only supported on indexed fields.' ) if not Filter is False : filter Obj . filters . append ( ( key , value ) ) else : filter Obj . not Filters . append ( ( key , value ) ) return filter Obj", "predictions": ["filter a list by a filter"], "references": ["internal for handling filters ; the guts of . filter and . filterinline"], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 4932, "code": "def delete ( self ) : if self . filters or self . not Filters : return self . mdl . deleter . delete Multiple ( self . all Only Indexed Fields ( ) ) return self . mdl . deleter . destroy Model ( )", "predictions": ["delete the document ."], "references": ["delete - deletes all entries matching the filter criteria"], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 4933, "code": "def delete By Pk ( self , pk ) : obj = self . mdl . objects . get Only Indexed Fields ( pk ) if not obj : return 0 return self . delete One ( obj )", "predictions": ["deletes a link from the database ."], "references": ["deletebypk - delete object associated with given primary key"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 4934, "code": "def string ( html , start on = None , ignore = ( ) , use short = True , * * queries ) : if use short : html = grow short ( html ) return to template ( fromstring ( html ) , start on = start on , ignore = ignore , * * queries )", "predictions": ["generate a string of html entries from html ."], "references": ["returns a blox template from an html string"], "bleu": 0.17747405280050263, "rouge_l": 0.35672514619883033}
{"id": 4935, "code": "def file ( file object , start on = None , ignore = ( ) , use short = True , * * queries ) : return string ( file object . read ( ) , start on = start on , ignore = ignore , use short = use short , * * queries )", "predictions": ["return a ** read ** message ."], "references": ["returns a blox template from a file stream object"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4936, "code": "def filename ( file name , start on = None , ignore = ( ) , use short = True , * * queries ) : with open ( file name ) as template file : return file ( template file , start on = start on , ignore = ignore , use short = use short , * * queries )", "predictions": ["build a filename that can be used as a ** as ** ."], "references": ["returns a blox template from a valid file path"], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 4937, "code": "def output ( self , to = None , * args , * * kwargs ) : to . write ( str ( self . value ) )", "predictions": ["writes the value to the client ."], "references": ["outputs the set text"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4938, "code": "def keep kwargs partial ( func , * args , * * keywords ) : def newfunc ( * fargs , * * fkeywords ) : newkeywords = fkeywords . copy ( ) newkeywords . update ( keywords ) return func ( * ( args + fargs ) , * * newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc", "predictions": ["keep only the first keywords that are no keywords ."], "references": ["like functools . partial but instead of using the new kwargs keeps the old ones ."], "bleu": 0.0765635970878477, "rouge_l": 0.1476997578692494}
{"id": 4939, "code": "def overview ( ) : range search = Range Search ( ) ranges = range search . get ranges ( ) if ranges : formatted ranges = [ ] tags lookup = { } for r in ranges : formatted ranges . append ( { 'mask' : r . range } ) tags lookup [ r . range ] = r . tags search = Host . search ( ) search = search . filter ( 'term' , status = 'up' ) search . aggs . bucket ( 'hosts' , 'ip range' , field = 'address' , ranges = formatted ranges ) response = search . execute ( ) print line ( \"{0:<18} {1:<6} {2}\" . format ( \"Range\" , \"Count\" , \"Tags\" ) ) print line ( \"-\" * 60 ) for entry in response . aggregations . hosts . buckets : print line ( \"{0:<18} {1:<6} {2}\" . format ( entry . key , entry . doc count , tags lookup [ entry . key ] ) ) else : print error ( \"No ranges defined.\" )", "predictions": ["list all available hosts ."], "references": ["creates a overview of the hosts per range ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4940, "code": "def round arr teff luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr", "predictions": ["round arr to a given arr"], "references": ["return the numpy array with rounded teff and luminosity columns ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 4941, "code": "def main ( ) : services = Service Search ( ) argparse = services . argparser argparse . add argument ( '-f' , '--file' , type = str , help = \"File\" ) arguments = argparse . parse args ( ) if not arguments . file : print error ( \"Please provide a file with credentials seperated by ':'\" ) sys . exit ( ) services = services . get services ( search = [ \"Tomcat\" ] , up = True , tags = [ '!tomcat brute' ] ) credentials = [ ] with open ( arguments . file , 'r' ) as f : credentials = f . readlines ( ) for service in services : print notification ( \"Checking ip:{} port {}\" . format ( service . address , service . port ) ) url = 'http://{}:{}/manager/html' gevent . spawn ( brutefore passwords , service . address , url . format ( service . address , service . port ) , credentials , service ) service . add tag ( 'tomcat brute' ) service . update ( tags = service . tags ) gevent . wait ( ) Logger ( ) . log ( \"tomcat brute\" , \"Performed tomcat bruteforce scan\" , { 'scanned services' : len ( services ) } )", "predictions": ["run the core ."], "references": ["checks the arguments to brutefore and spawns greenlets to perform the bruteforcing ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 4942, "code": "def round teff luminosity ( cluster ) : temps = [ round ( t , - 1 ) for t in teff ( cluster ) ] lums = [ round ( l , 3 ) for l in luminosity ( cluster ) ] return temps , lums", "predictions": ["round teff luminosity to a given cluster ."], "references": ["returns rounded teff and luminosity lists ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 4943, "code": "def modify data ( data ) : with tempfile . Named Temporary File ( 'w' ) as f : for entry in data : f . write ( json . dumps ( entry . to dict ( include meta = True ) , default = datetime handler ) ) f . write ( '\\n' ) f . flush ( ) print success ( \"Starting editor\" ) subprocess . call ( [ 'nano' , '-' , f . name ] ) with open ( f . name , 'r' ) as f : return f . readlines ( )", "predictions": ["modify data to local file"], "references": ["creates a tempfile and starts the given editor returns the data afterwards ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 4944, "code": "def modify input ( ) : doc mapper = Doc Mapper ( ) if doc mapper . is pipe : objects = [ obj for obj in doc mapper . get pipe ( ) ] modified = modify data ( objects ) for line in modified : obj = doc mapper . line to object ( line ) obj . save ( ) print success ( \"Object(s) successfully changed\" ) else : print error ( \"Please use this tool with pipes\" )", "predictions": ["modify input data ."], "references": ["this functions gives the user a way to change the data that is given as input ."], "bleu": 0.01656771518980191, "rouge_l": 0.17134831460674158}
{"id": 4945, "code": "def bruteforce ( users , domain , password , host ) : cs = Credential Search ( use pipe = False ) print notification ( \"Connecting to {}\" . format ( host ) ) s = Server ( host ) c = Connection ( s ) for user in users : if c . rebind ( user = \"{}\\\\{}\" . format ( domain , user . username ) , password = password , authentication = NTLM ) : print success ( 'Success for: {}:{}' . format ( user . username , password ) ) credential = cs . find object ( user . username , password , domain = domain , host ip = host ) if not credential : credential = Credential ( username = user . username , secret = password , domain = domain , host ip = host , type = \"plaintext\" , port = 389 ) credential . add tag ( tag ) credential . save ( ) user . add tag ( tag ) user . save ( ) else : print error ( \"Fail for: {}:{}\" . format ( user . username , password ) )", "predictions": ["debug debug to use for a credential user"], "references": ["performs a bruteforce for the given users password domain on the given host ."], "bleu": 0.08383280652235028, "rouge_l": 0.08664772727272725}
{"id": 4946, "code": "def utime ( self , * args , * * kwargs ) : os . utime ( self . extended path , * args , * * kwargs )", "predictions": ["this function sets the given file ."], "references": ["set the access and modified times of the file specified by path ."], "bleu": 0.09374222649442905, "rouge_l": 0.2846034214618974}
{"id": 4947, "code": "def print line ( text ) : try : signal . signal ( signal . SIGPIPE , signal . SIG DFL ) except Value Error : pass try : sys . stdout . write ( text ) if not text . endswith ( '\\n' ) : sys . stdout . write ( '\\n' ) sys . stdout . flush ( ) except IO Error : sys . exit ( 0 )", "predictions": ["print a line to stdout"], "references": ["print the given line to stdout"], "bleu": 0.43989172475842214, "rouge_l": 0.7155425219941348}
{"id": 4948, "code": "def get own ip ( ) : own ip = None interfaces = psutil . net if addrs ( ) for , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF INET : ip address = ipaddress . ip address ( detail . address ) if not ( ip address . is link local or ip address . is loopback ) : own ip = str ( ip address ) break return own ip", "predictions": ["return the ip address of the ip address ."], "references": ["gets the ip from the inet interfaces ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 4949, "code": "def remove namespace ( doc , namespace ) : ns = u'{%s}' % namespace nsl = len ( ns ) for elem in doc . getiterator ( ) : if elem . tag . startswith ( ns ) : elem . tag = elem . tag [ nsl : ] elem . attrib [ 'oxmlns' ] = namespace", "predictions": ["remove a namespace from a namespace"], "references": ["remove namespace in the passed document in place ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4950, "code": "def create payload ( self , x86 file , x64 file , payload file ) : sc x86 = open ( os . path . join ( self . datadir , x86 file ) , 'rb' ) . read ( ) sc x64 = open ( os . path . join ( self . datadir , x64 file ) , 'rb' ) . read ( ) fp = open ( os . path . join ( self . datadir , payload file ) , 'wb' ) fp . write ( b'\\x31\\xc0\\x40\\x0f\\x84' + pack ( '<I' , len ( sc x86 ) ) ) fp . write ( sc x86 ) fp . write ( sc x64 ) fp . close ( )", "predictions": ["create a payload that will be created with the given payload ."], "references": ["creates the final payload based on the x86 and x64 meterpreters ."], "bleu": 0.1235622127262679, "rouge_l": 0.25}
{"id": 4951, "code": "def combine files ( self , f1 , f2 , f3 ) : with open ( os . path . join ( self . datadir , f3 ) , 'wb' ) as new file : with open ( os . path . join ( self . datadir , f1 ) , 'rb' ) as file 1 : new file . write ( file 1 . read ( ) ) with open ( os . path . join ( self . datadir , f2 ) , 'rb' ) as file 2 : new file . write ( file 2 . read ( ) )", "predictions": ["combine f1 files into datadir"], "references": ["combines the files 1 and 2 into 3 ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4952, "code": "def detect os ( self , ip ) : process = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'checker.py' ) , str ( ip ) ] , stdout = subprocess . PIPE ) out = process . stdout . decode ( 'utf-8' ) . split ( '\\n' ) system os = '' for line in out : if line . startswith ( 'Target OS:' ) : system os = line . replace ( 'Target OS: ' , '' ) break return system os", "predictions": ["detect a given ip address ."], "references": ["runs the checker . py scripts to detect the os ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 4953, "code": "def exploit single ( self , ip , operating system ) : result = None if \"Windows Server 2008\" in operating system or \"Windows 7\" in operating system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue exploit7.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final combined.bin' ) , \"12\" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) elif \"Windows Server 2012\" in operating system or \"Windows 10\" in operating system or \"Windows 8.1\" in operating system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue exploit8.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final combined.bin' ) , \"12\" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : return [ \"System target could not be automatically identified\" ] return result . stdout . decode ( 'utf-8' ) . split ( '\\n' )", "predictions": ["get a list of \"windows automatically automatically \"windows automatically ."], "references": ["exploits a single ip exploit is based on the given operating system ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 4954, "code": "def write index translation ( translation filename , entity ids , relation ids ) : translation = triple pb . Translation ( ) entities = [ ] for name , index in entity ids . items ( ) : translation . entities . add ( element = name , index = index ) relations = [ ] for name , index in relation ids . items ( ) : translation . relations . add ( element = name , index = index ) with open ( translation filename , \"wb\" ) as f : f . write ( translation . Serialize To String ( ) )", "predictions": ["write translation translation to a single file ."], "references": ["write triples into a translation file ."], "bleu": 0.25098621243978964, "rouge_l": 0.5398230088495575}
{"id": 4955, "code": "def write triples ( filename , triples , delimiter = DEFAULT DELIMITER , triple order = \"hrt\" ) : with open ( filename , 'w' ) as f : for t in triples : line = t . serialize ( delimiter , triple order ) f . write ( line + \"\\n\" )", "predictions": ["write the triples to a file"], "references": ["write triples to file ."], "bleu": 0.3303164318013807, "rouge_l": 0.7393939393939394}
{"id": 4956, "code": "def read translation ( filename ) : translation = triple pb . Translation ( ) with open ( filename , \"rb\" ) as f : translation . Parse From String ( f . read ( ) ) def unwrap translation units ( units ) : for u in units : yield u . element , u . index return ( list ( unwrap translation units ( translation . entities ) ) , list ( unwrap translation units ( translation . relations ) ) )", "predictions": ["read translation entities from a shapely file"], "references": ["returns protobuf mapcontainer . read from translation file ."], "bleu": 0.1755217914979255, "rouge_l": 0.3667334669338677}
{"id": 4957, "code": "def read openke translation ( filename , delimiter = '\\t' , entity first = True ) : result = { } with open ( filename , \"r\" ) as f : = next ( f ) for line in f : line slice = line . rstrip ( ) . split ( delimiter ) if not entity first : line slice = list ( reversed ( line slice ) ) result [ line slice [ 0 ] ] = line slice [ 1 ] return result", "predictions": ["read a openke from a file"], "references": ["returns map with entity or relations from plain text ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 4958, "code": "def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag count' , 'terms' , field = 'tags' , order = { ' count' : 'desc' } , size = 100 ) response = search . execute ( ) print line ( \"{0:<25} {1}\" . format ( 'Tag' , 'Count' ) ) print line ( \"-\" * 30 ) for entry in response . aggregations . tag count . buckets : print line ( \"{0:<25} {1}\" . format ( entry . key , entry . doc count ) )", "predictions": ["list all available overview ."], "references": ["prints an overview of the tags of the hosts ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 4959, "code": "def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password count' , 'terms' , field = 'secret' , order = { ' count' : 'desc' } , size = 20 ) . metric ( 'username count' , 'cardinality' , field = 'username' ) . metric ( 'host count' , 'cardinality' , field = 'host ip' ) . metric ( 'top hits' , 'top hits' , docvalue fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print line ( \"{0:65} {1:5} {2:5} {3:5} {4}\" . format ( \"Secret\" , \"Count\" , \"Hosts\" , \"Users\" , \"Usernames\" ) ) print line ( \"-\" * 100 ) for entry in response . aggregations . password count . buckets : usernames = [ ] for creds in entry . top hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print line ( \"{0:65} {1:5} {2:5} {3:5} {4}\" . format ( entry . key , entry . doc count , entry . host count . value , entry . username count . value , usernames ) )", "predictions": ["list all top hits ."], "references": ["provides an overview of the duplicate credentials ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4960, "code": "def pipe worker ( pipename , filename , object type , query , format string , unique = False ) : print notification ( \"[{}] Starting pipe\" . format ( pipename ) ) object type = object type ( ) try : while True : uniq = set ( ) if os . path . exists ( filename ) : os . remove ( filename ) os . mkfifo ( filename ) with open ( filename , 'w' ) as pipe : print success ( \"[{}] Providing data\" . format ( pipename ) ) objects = object type . search ( * * query ) for obj in objects : data = fmt . format ( format string , * * obj . to dict ( ) ) if unique : if not data in uniq : uniq . add ( data ) pipe . write ( data + '\\n' ) else : pipe . write ( data + '\\n' ) os . unlink ( filename ) except Keyboard Interrupt : print notification ( \"[{}] Shutting down named pipe\" . format ( pipename ) ) except Exception as e : print error ( \"[{}] Error: {}, stopping named pipe\" . format ( e , pipename ) ) finally : os . remove ( filename )", "predictions": ["parse a frequencies frequencies"], "references": ["starts the loop to provide the data from jackal ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 4961, "code": "def create query ( section ) : query = { } if 'ports' in section : query [ 'ports' ] = [ section [ 'ports' ] ] if 'up' in section : query [ 'up' ] = bool ( section [ 'up' ] ) if 'search' in section : query [ 'search' ] = [ section [ 'search' ] ] if 'tags' in section : query [ 'tags' ] = [ section [ 'tags' ] ] if 'groups' in section : query [ 'groups' ] = [ section [ 'groups' ] ] return query", "predictions": ["parse a allele allele"], "references": ["creates a search query based on the section of the config file ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 4962, "code": "def create pipe workers ( configfile , directory ) : type map = { 'service' : Service Search , 'host' : Host Search , 'range' : Range Search , 'user' : User Search } config = configparser . Config Parser ( ) config . read ( configfile ) if not len ( config . sections ( ) ) : print error ( \"No named pipes configured\" ) return print notification ( \"Starting {} pipes in directory {}\" . format ( len ( config . sections ( ) ) , directory ) ) workers = [ ] for name in config . sections ( ) : section = config [ name ] query = create query ( section ) object type = type map [ section [ 'type' ] ] args = ( name , os . path . join ( directory , name ) , object type , query , section [ 'format' ] , bool ( section . get ( 'unique' , 0 ) ) ) workers . append ( multiprocessing . Process ( target = pipe worker , args = args ) ) return workers", "predictions": ["add all pipe self to the config"], "references": ["creates the workers based on the given configfile to provide named pipes in the directory ."], "bleu": 0.056829570481990416, "rouge_l": 0.16245006657789615}
{"id": 4963, "code": "def main ( ) : config = Config ( ) pipes dir = config . get ( 'pipes' , 'directory' ) pipes config = config . get ( 'pipes' , 'config file' ) pipes config path = os . path . join ( config . config dir , pipes config ) if not os . path . exists ( pipes config path ) : print error ( \"Please configure the named pipes first\" ) return workers = create pipe workers ( pipes config path , pipes dir ) if workers : for worker in workers : worker . start ( ) try : for worker in workers : worker . join ( ) except Keyboard Interrupt : print notification ( \"Shutting down\" ) for worker in workers : worker . terminate ( ) worker . join ( )", "predictions": ["filter and start all raise a program key key key key key key key key key key key ."], "references": ["loads the config and handles the workers ."], "bleu": 0.0712695567709093, "rouge_l": 0.1598951507208388}
{"id": 4964, "code": "def main ( ) : search = Service Search ( ) services = search . get services ( up = True , tags = [ '!header scan' ] ) print notification ( \"Scanning {} services\" . format ( len ( services ) ) ) urllib3 . disable warnings ( urllib3 . exceptions . Insecure Request Warning ) pool = Pool ( 100 ) count = 0 for service in services : count += 1 if count % 50 == 0 : print notification ( \"Checking {}/{} services\" . format ( count , len ( services ) ) ) pool . spawn ( check service , service ) pool . join ( ) print notification ( \"Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https.\" )", "predictions": ["delete all or or respond or respond or respond"], "references": ["retrieves services starts check_service in a gevent pool of 100 ."], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 4965, "code": "def import nmap ( result , tag , check function = all hosts , import services = False ) : host search = Host Search ( arguments = False ) service search = Service Search ( ) parser = Nmap Parser ( ) report = parser . parse fromstring ( result ) imported hosts = 0 imported services = 0 for nmap host in report . hosts : if check function ( nmap host ) : imported hosts += 1 host = host search . id to object ( nmap host . address ) host . status = nmap host . status host . add tag ( tag ) if nmap host . os fingerprinted : host . os = nmap host . os fingerprint if nmap host . hostnames : host . hostname . extend ( nmap host . hostnames ) if import services : for service in nmap host . services : imported services += 1 serv = Service ( * * service . get dict ( ) ) serv . address = nmap host . address service id = service search . object to id ( serv ) if service id : serv old = Service . get ( service id ) if service . banner : serv old . banner = service . banner serv old . save ( ) else : serv . address = nmap host . address serv . save ( ) if service . state == 'open' : host . open ports . append ( service . port ) if service . state == 'closed' : host . closed ports . append ( service . port ) if service . state == 'filtered' : host . filtered ports . append ( service . port ) host . save ( ) if imported hosts : print success ( \"Imported {} hosts, with tag {}\" . format ( imported hosts , tag ) ) else : print error ( \"No hosts found\" ) return { 'hosts' : imported hosts , 'services' : imported services }", "predictions": ["imports nmap to ."], "references": ["imports the given nmap result ."], "bleu": 0.25916266987614406, "rouge_l": 0.5791139240506329}
{"id": 4966, "code": "def nmap ( nmap args , ips ) : config = Config ( ) arguments = [ 'nmap' , '-Pn' ] arguments . extend ( ips ) arguments . extend ( nmap args ) output file = '' now = datetime . datetime . now ( ) if not '-o A' in nmap args : output name = 'nmap jackal {}' . format ( now . strftime ( \"%Y-%m-%d %H:%M\" ) ) path name = os . path . join ( config . get ( 'nmap' , 'directory' ) , output name ) print notification ( \"Writing output of nmap to {}\" . format ( path name ) ) if not os . path . exists ( config . get ( 'nmap' , 'directory' ) ) : os . makedirs ( config . get ( 'nmap' , 'directory' ) ) output file = path name + '.xml' arguments . extend ( [ '-o A' , path name ] ) else : output file = nmap args [ nmap args . index ( '-o A' ) + 1 ] + '.xml' print notification ( \"Starting nmap\" ) subprocess . call ( arguments ) with open ( output file , 'r' ) as f : return f . read ( )", "predictions": ["run string with string ."], "references": ["start an nmap process with the given args on the given ips ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 4967, "code": "def nmap scan ( ) : hs = Host Search ( ) config = Config ( ) nmap types = [ 'top10' , 'top100' , 'custom' , 'top1000' , 'all' ] options = { 'top10' : '--top-ports 10' , 'top100' : '--top-ports 100' , 'custom' : config . get ( 'nmap' , 'options' ) , 'top1000' : '--top-ports 1000' , 'all' : '-p-' } hs parser = hs . argparser argparser = argparse . Argument Parser ( parents = [ hs parser ] , conflict handler = 'resolve' , description = \"Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap\" ) argparser . add argument ( 'type' , metavar = 'type' , help = 'The number of ports to scan: top10, top100, custom, top1000 (default) or all' , type = str , choices = nmap types , default = 'top1000' , const = 'top1000' , nargs = '?' ) arguments , extra nmap args = argparser . parse known args ( ) tags = nmap types [ nmap types . index ( arguments . type ) : ] tags = [ \"!nmap \" + tag for tag in tags ] hosts = hs . get hosts ( tags = tags ) hosts = [ host for host in hosts ] nmap args = [ ] nmap args . extend ( extra nmap args ) nmap args . extend ( options [ arguments . type ] . split ( ' ' ) ) print notification ( \"Running nmap with args: {} on {} hosts(s)\" . format ( nmap args , len ( hosts ) ) ) if len ( hosts ) : result = nmap ( nmap args , [ str ( h . address ) for h in hosts ] ) for host in hosts : host . add tag ( \"nmap {}\" . format ( arguments . type ) ) host . save ( ) print notification ( \"Nmap done, importing results\" ) stats = import nmap ( result , \"nmap {}\" . format ( arguments . type ) , check function = all hosts , import services = True ) stats [ 'scanned hosts' ] = len ( hosts ) stats [ 'type' ] = arguments . type Logger ( ) . log ( 'nmap scan' , \"Performed nmap {} scan on {} hosts\" . format ( arguments . type , len ( hosts ) ) , stats ) else : print notification ( \"No hosts found\" )", "predictions": ["file server scan ."], "references": ["scans the given hosts with nmap ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4968, "code": "def nmap smb vulnscan ( ) : service search = Service Search ( ) services = service search . get services ( ports = [ '445' ] , tags = [ '!smb vulnscan' ] , up = True ) services = [ service for service in services ] service dict = { } for service in services : service . add tag ( 'smb vulnscan' ) service dict [ str ( service . address ) ] = service nmap args = \"-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445\" . split ( \" \" ) if services : result = nmap ( nmap args , [ str ( s . address ) for s in services ] ) parser = Nmap Parser ( ) report = parser . parse fromstring ( result ) smb signing = 0 ms17 = 0 for nmap host in report . hosts : for script result in nmap host . scripts results : script result = script result . get ( 'elements' , { } ) service = service dict [ str ( nmap host . address ) ] if script result . get ( 'message signing' , '' ) == 'disabled' : print success ( \"({}) SMB Signing disabled\" . format ( nmap host . address ) ) service . add tag ( 'smb signing disabled' ) smb signing += 1 if script result . get ( 'CVE-2017-0143' , { } ) . get ( 'state' , '' ) == 'VULNERABLE' : print success ( \"({}) Vulnerable for MS17-010\" . format ( nmap host . address ) ) service . add tag ( 'MS17-010' ) ms17 += 1 service . update ( tags = service . tags ) print notification ( \"Completed, 'smb signing disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010.\" ) stats = { 'smb signing' : smb signing , 'MS17 010' : ms17 , 'scanned services' : len ( services ) } Logger ( ) . log ( 'smb vulnscan' , 'Scanned {} smb services for vulnerabilities' . format ( len ( services ) ) , stats ) else : print notification ( \"No services found to scan.\" )", "predictions": ["run the file server"], "references": ["scans available smb services in the database for smb signing and ms17 - 010 ."], "bleu": 0.022969543400575367, "rouge_l": 0.0953125}
{"id": 4969, "code": "def add tag ( ) : if len ( sys . argv ) > 1 : tag = sys . argv [ 1 ] doc mapper = Doc Mapper ( ) if doc mapper . is pipe : count = 0 for obj in doc mapper . get pipe ( ) : obj . add tag ( tag ) obj . update ( tags = obj . tags ) count += 1 print success ( \"Added tag '{}' to {} object(s)\" . format ( tag , count ) ) else : print error ( \"Please use this script with pipes\" ) else : print error ( \"Usage: jk-add-tag <tag>\" ) sys . exit ( )", "predictions": ["output a tag tag"], "references": ["obtains the data from the pipe and appends the given tag ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 4970, "code": "def config dir ( self ) : home = expanduser ( '~' ) config dir = os . path . join ( home , '.jackal' ) return config dir", "predictions": [". directory to get the keep - specific configuration directory"], "references": ["returns the configuration directory"], "bleu": 0.17827531042796255, "rouge_l": 0.4644670050761421}
{"id": 4971, "code": "def write config ( self , initialize indices = False ) : if not os . path . exists ( self . config dir ) : os . mkdir ( self . config dir ) with open ( self . config file , 'w' ) as configfile : self . config . write ( configfile ) if initialize indices : index = self . get ( 'jackal' , 'index' ) from jackal import Host , Range , Service , User , Credential , Log from jackal . core import create connection create connection ( self ) Host . init ( index = \"{}-hosts\" . format ( index ) ) Range . init ( index = \"{}-ranges\" . format ( index ) ) Service . init ( index = \"{}-services\" . format ( index ) ) User . init ( index = \"{}-users\" . format ( index ) ) Credential . init ( index = \"{}-creds\" . format ( index ) ) Log . init ( index = \"{}-log\" . format ( index ) )", "predictions": ["overview the current configuration ."], "references": ["write the current config to disk to store them ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 4972, "code": "def ensure remote branch is tracked ( branch ) : if branch == MASTER BRANCH : return output = subprocess . check output ( [ 'git' , 'branch' , '--list' ] ) for line in output . split ( '\\n' ) : if line . strip ( ) == branch : break else : try : sys . stdout . write ( subprocess . check output ( [ 'git' , 'checkout' , '--track' , 'origin/%s' % branch ] ) ) except subprocess . Called Process Error : raise System Exit ( 1 )", "predictions": ["round teff teff if it is tracked"], "references": ["track the specified remote branch if it is not already tracked ."], "bleu": 0.17895451045590982, "rouge_l": 0.40197693574958815}
{"id": 4973, "code": "def main ( branch ) : try : output = subprocess . check output ( [ 'git' , 'rev-parse' ] ) . decode ( 'utf-8' ) sys . stdout . write ( output ) except subprocess . Called Process Error : return ensure remote branch is tracked ( branch ) subprocess . check call ( [ 'git' , 'checkout' , '--quiet' , branch ] ) subprocess . check call ( [ 'git' , 'pull' , '--quiet' ] ) subprocess . check call ( [ 'git' , 'checkout' , '--quiet' , '%s~0' % branch ] ) subprocess . check call ( [ 'find' , '.' , '-name' , '\"*.pyc\"' , '-delete' ] ) print ( 'Your branch is up to date with branch \\'origin/%s\\'.' % branch )", "predictions": ["runs the client ."], "references": ["checkout update and branch from the specified branch ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 4974, "code": "def get interface name ( ) : interface name = '' interfaces = psutil . net if addrs ( ) for name , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF INET : ip address = ipaddress . ip address ( detail . address ) if not ( ip address . is link local or ip address . is loopback ) : interface name = name break return interface name", "predictions": ["return teff teff t work with some = 0 t work t"], "references": ["returns the interface name of the first not link_local and not loopback interface ."], "bleu": 0.07395852913779274, "rouge_l": 0.0}
{"id": 4975, "code": "def load targets ( self ) : ldap services = [ ] if self . ldap : ldap services = self . search . get services ( ports = [ 389 ] , up = True ) self . ldap strings = [ \"ldap://{}\" . format ( service . address ) for service in ldap services ] self . services = self . search . get services ( tags = [ 'smb signing disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]", "predictions": ["loads the tempfile into the tempfile"], "references": ["load_targets will load the services with smb signing disabled and if ldap is enabled the services with the ldap port open ."], "bleu": 0.016986029490530064, "rouge_l": 0.1295116772823779}
{"id": 4976, "code": "def write targets ( self ) : if len ( self . ldap strings ) == 0 and len ( self . ips ) == 0 : print notification ( \"No targets left\" ) if self . auto exit : if self . notifier : self . notifier . stop ( ) self . terminate processes ( ) with open ( self . targets file , 'w' ) as f : f . write ( '\\n' . join ( self . ldap strings + self . ips ) )", "predictions": ["modify the ldap file"], "references": ["write_targets will write the contents of ips and ldap_strings to the targets_file ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 4977, "code": "def callback ( self , event ) : if event . mask == 0x00000008 : if event . name . endswith ( '.json' ) : print success ( \"Ldapdomaindump file found\" ) if event . name in [ 'domain groups.json' , 'domain users.json' ] : if event . name == 'domain groups.json' : self . domain groups file = event . pathname if event . name == 'domain users.json' : self . domain users file = event . pathname if self . domain groups file and self . domain users file : print success ( \"Importing users\" ) subprocess . Popen ( [ 'jk-import-domaindump' , self . domain groups file , self . domain users file ] ) elif event . name == 'domain computers.json' : print success ( \"Importing computers\" ) subprocess . Popen ( [ 'jk-import-domaindump' , event . pathname ] ) self . ldap strings = [ ] self . write targets ( ) if event . name . endswith ( ' samhashes.sam' ) : host = event . name . replace ( ' samhashes.sam' , '' ) print success ( \"Secretsdump file, host ip: {}\" . format ( host ) ) subprocess . Popen ( [ 'jk-import-secretsdump' , event . pathname ] ) self . ips . remove ( host ) self . write targets ( )", "predictions": ["called when a user is locked"], "references": ["function that gets called on each event from pyinotify ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 4978, "code": "def watch ( self ) : wm = pyinotify . Watch Manager ( ) self . notifier = pyinotify . Notifier ( wm , default proc fun = self . callback ) wm . add watch ( self . directory , pyinotify . ALL EVENTS ) try : self . notifier . loop ( ) except ( Keyboard Interrupt , Attribute Error ) : print notification ( \"Stopping\" ) finally : self . notifier . stop ( ) self . terminate processes ( )", "predictions": ["watch the event loop"], "references": ["watches directory for changes"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4979, "code": "def terminate processes ( self ) : if self . relay : self . relay . terminate ( ) if self . responder : self . responder . terminate ( )", "predictions": ["print line try to ."], "references": ["terminate the processes ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4980, "code": "def get template uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template name : return template [ 'uuid' ]", "predictions": ["get the own own own own own own own own own own"], "references": ["retrieves the uuid of the given template name ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 4981, "code": "def start scan ( self , scan id ) : requests . post ( self . url + 'scans/{}/launch' . format ( scan id ) , verify = False , headers = self . headers )", "predictions": ["remove a namespace ."], "references": ["starts the scan identified by the scan_id . s"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4982, "code": "def cmp To Data Store uri ( base , ds1 , ds2 ) : ret = difflib . get close matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1", "predictions": ["reset the uri and cutoff to a file uri"], "references": ["bases the comparison of the datastores on uri alone ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 4983, "code": "def add tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) | set ( [ tag ] ) )", "predictions": ["combine a files and add it to the list of tags"], "references": ["adds a tag to the list of tags and makes sure the result list contains only unique results ."], "bleu": 0.21664849022986635, "rouge_l": 0.38164754953076124}
{"id": 4984, "code": "def remove tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) - set ( [ tag ] ) )", "predictions": ["detect a os os os os os os ."], "references": ["removes a tag from this object"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4985, "code": "def to dict ( self , include meta = False ) : result = super ( Jackal Doc , self ) . to dict ( include meta = include meta ) if include meta : source = result . pop ( ' source' ) return { * * result , * * source } else : return result", "predictions": ["convert the operating representation of the dataset into a dictionary ."], "references": ["returns the result as a dictionary provide the include_meta flag to als show information like index and doctype ."], "bleu": 0.08614710135569353, "rouge_l": 0.2544316996871741}
{"id": 4986, "code": "def lookup ( cls , key , get = False ) : if get : item = cls . item dict . get ( key ) return item . name if item else key return cls . item dict [ key ] . name", "predictions": ["returns a write object for the given filename triple triple triple triple triple triple triple triple triple triple triple triple triple triple triple triple triple triple"], "references": ["returns the label for a given enum key"], "bleu": 0.06143498010483918, "rouge_l": 0.19509594882729217}
{"id": 4987, "code": "def verbose ( cls , key = False , default = '' ) : if key is False : items = cls . item dict . values ( ) return [ ( x . key , x . value ) for x in sorted ( items , key = lambda x : x . sort or x . key ) ] item = cls . item dict . get ( key ) return item . value if item else default", "predictions": ["return list of open open open open open open open open file with given key with given key with key with given key with key with given key with given key"], "references": ["returns the verbose name for a given enum value"], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 4988, "code": "def get configured dns ( ) : ips = [ ] try : output = subprocess . check output ( [ 'nmcli' , 'device' , 'show' ] ) output = output . decode ( 'utf-8' ) for line in output . split ( '\\n' ) : if 'DNS' in line : pattern = r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\" for hit in re . findall ( pattern , line ) : ips . append ( hit ) except File Not Found Error : pass return ips", "predictions": ["list translation = true ."], "references": ["returns the configured dns servers with the use f nmcli ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4989, "code": "def zone transfer ( address , dns name ) : ips = [ ] try : print notification ( \"Attempting dns zone transfer for {} on {}\" . format ( dns name , address ) ) z = dns . zone . from xfr ( dns . query . xfr ( address , dns name ) ) except dns . exception . Form Error : print notification ( \"Zone transfer not allowed\" ) return ips names = z . nodes . keys ( ) print success ( \"Zone transfer successfull for {}, found {} entries\" . format ( address , len ( names ) ) ) for n in names : node = z [ n ] data = node . get rdataset ( dns . rdataclass . IN , dns . rdatatype . A ) if data : for item in data . items : address = item . address ips . append ( address ) return ips", "predictions": ["read a { dns }"], "references": ["tries to perform a zone transfer ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4990, "code": "def resolve domains ( domains , disable zone = False ) : dnsresolver = dns . resolver . Resolver ( ) ips = [ ] for domain in domains : print notification ( \"Resolving {}\" . format ( domain ) ) try : result = dnsresolver . query ( domain , 'A' ) for a in result . response . answer [ 0 ] : ips . append ( str ( a ) ) if not disable zone : ips . extend ( zone transfer ( str ( a ) , domain ) ) except dns . resolver . NXDOMAIN as e : print error ( e ) return ips", "predictions": ["overview all domains ."], "references": ["resolves the list of domains and returns the ips ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 4991, "code": "def create connection ( conf ) : host config = { } host config [ 'hosts' ] = [ conf . get ( 'jackal' , 'host' ) ] if int ( conf . get ( 'jackal' , 'use ssl' ) ) : host config [ 'use ssl' ] = True if conf . get ( 'jackal' , 'ca certs' ) : host config [ 'ca certs' ] = conf . get ( 'jackal' , 'ca certs' ) if int ( conf . get ( 'jackal' , 'client certs' ) ) : host config [ 'client cert' ] = conf . get ( 'jackal' , 'client cert' ) host config [ 'client key' ] = conf . get ( 'jackal' , 'client key' ) host config [ 'ssl assert hostname' ] = False connections . create connection ( * * host config )", "predictions": ["overview to create new connection"], "references": ["creates a connection based upon the given configuration object ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4992, "code": "def search ( self , number = None , * args , * * kwargs ) : search = self . create search ( * args , * * kwargs ) try : if number : response = search [ 0 : number ] else : args , = self . core parser . parse known args ( ) if args . number : response = search [ 0 : args . number ] else : response = search . scan ( ) return [ hit for hit in response ] except Not Found Error : print error ( \"The index was not found, have you initialized the index?\" ) return [ ] except ( Connection Error , Transport Error ) : print error ( \"Cannot connect to elasticsearch\" ) return [ ]", "predictions": ["search for things in the api server"], "references": ["searches the elasticsearch instance to retrieve the requested documents ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4993, "code": "def argument search ( self ) : arguments , = self . argparser . parse known args ( ) return self . search ( * * vars ( arguments ) )", "predictions": ["search for argument names ."], "references": ["uses the command line arguments to fill the search function and call it ."], "bleu": 0.04994299940831281, "rouge_l": 0.19395866454689983}
{"id": 4994, "code": "def count ( self , * args , * * kwargs ) : search = self . create search ( * args , * * kwargs ) try : return search . count ( ) except Not Found Error : print error ( \"The index was not found, have you initialized the index?\" ) except ( Connection Error , Transport Error ) : print error ( \"Cannot connect to elasticsearch\" )", "predictions": ["return the count of records matching the index ."], "references": ["returns the number of results after filtering with the given arguments ."], "bleu": 0.12716571564598603, "rouge_l": 0.3713850837138508}
{"id": 4995, "code": "def argument count ( self ) : arguments , = self . argparser . parse known args ( ) return self . count ( * * vars ( arguments ) )", "predictions": ["return the number of messages for the file ."], "references": ["uses the command line arguments to fill the count function and call it ."], "bleu": 0.09630141125179911, "rouge_l": 0.2510288065843621}
{"id": 4996, "code": "def id to object ( self , line ) : result = Range . get ( line , ignore = 404 ) if not result : result = Range ( range = line ) result . save ( ) return result", "predictions": ["convert a line to a string"], "references": ["resolves an ip adres to a range object creating it if it doesn t exists ."], "bleu": 0.054909040476580136, "rouge_l": 0.1680440771349862}
{"id": 4997, "code": "def argparser ( self ) : core parser = self . core parser core parser . add argument ( '-r' , '--range' , type = str , help = \"The range to search for use\" ) return core parser", "predictions": ["only available argparser for this command ."], "references": ["argparser option with search functionality specific for ranges ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 4998, "code": "def object to id ( self , obj ) : search = Service . search ( ) search = search . filter ( \"term\" , address = obj . address ) search = search . filter ( \"term\" , protocol = obj . protocol ) search = search . filter ( \"term\" , port = obj . port ) search = search . filter ( \"term\" , state = obj . state ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None", "predictions": ["convert an object to a cached record ."], "references": ["searches elasticsearch for objects with the same address protocol port and state ."], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 4999, "code": "def id to object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user", "predictions": ["convert a user to a user"], "references": ["resolves the given id to a user object if it doesn t exists it will be created ."], "bleu": 0.05564088449132485, "rouge_l": 0.22932330827067668}
{"id": 5000, "code": "def get users ( self , * args , * * kwargs ) : arguments , = self . argparser . parse known args ( ) if self . is pipe and self . use pipe : return self . get pipe ( self . object type ) elif arguments . tags or arguments . group or arguments . search or arguments . domain : return self . argument search ( ) else : return self . search ( * args , * * kwargs )", "predictions": ["get all users from the pipe"], "references": ["retrieves the users from elastic ."], "bleu": 0.31239399369202553, "rouge_l": 0.3333333333333333}
{"id": 5001, "code": "def get domains ( self ) : search = User . search ( ) search . aggs . bucket ( 'domains' , 'terms' , field = 'domain' , order = { ' count' : 'desc' } , size = 100 ) response = search . execute ( ) return [ entry . key for entry in response . aggregations . domains . buckets ]", "predictions": ["get all domains ."], "references": ["retrieves the domains of the users from elastic ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 5002, "code": "def find object ( self , username , secret , domain = None , host ip = None , service id = None ) : search = Credential . search ( ) search = search . filter ( \"term\" , username = username ) search = search . filter ( \"term\" , secret = secret ) if domain : search = search . filter ( \"term\" , domain = domain ) else : search = search . exclude ( \"exists\" , field = \"domain\" ) if host ip : search = search . filter ( \"term\" , host ip = host ip ) else : search = search . exclude ( \"exists\" , field = \"host ip\" ) if service id : search = search . filter ( \"term\" , service id = service id ) else : search = search . exclude ( \"exists\" , field = \"service id\" ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result else : return None", "predictions": ["find object in the database ."], "references": ["searches elasticsearch for objects with the same username password optional domain host_ip and service_id ."], "bleu": 0.054546736148076896, "rouge_l": 0.17681159420289855}
{"id": 5003, "code": "def object to id ( self , obj ) : search = Credential . search ( ) search = search . filter ( \"term\" , username = obj . username ) search = search . filter ( \"term\" , secret = obj . secret ) if obj . domain : search = search . filter ( \"term\" , domain = obj . domain ) else : search = search . exclude ( \"exists\" , field = \"domain\" ) if obj . host ip : search = search . filter ( \"term\" , host ip = obj . host ip ) else : search = search . exclude ( \"exists\" , field = \"host ip\" ) if obj . service id : search = search . filter ( \"term\" , service id = obj . service id ) else : search = search . exclude ( \"exists\" , field = \"service id\" ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None", "predictions": ["convert an object to a search id"], "references": ["searches elasticsearch for objects with the same username password optional domain host_ip and service_id ."], "bleu": 0.04981224652850501, "rouge_l": 0.0}
{"id": 5004, "code": "def get credentials ( self , * args , * * kwargs ) : arguments , = self . argparser . parse known args ( ) if self . is pipe and self . use pipe : return self . get pipe ( self . object type ) elif arguments . tags or arguments . type or arguments . search or arguments . password or arguments . cracked or arguments . range or arguments . domain : return self . argument search ( ) else : return self . search ( * args , * * kwargs )", "predictions": ["search for credentials and password"], "references": ["retrieves the users from elastic ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 5005, "code": "def tree2commands ( self , adapter , session , lastcmds , xsync ) : assert xsync . tag == constants . NODE SYNCML assert len ( xsync ) == 2 assert xsync [ 0 ] . tag == constants . CMD SYNCHDR assert xsync [ 1 ] . tag == constants . NODE SYNCBODY version = xsync [ 0 ] . findtext ( 'Ver Proto' ) if version != constants . SYNCML VERSION 1 2 : raise common . Feature Not Supported ( 'unsupported Sync ML version \"%s\" (expected \"%s\")' % ( version , constants . SYNCML VERSION 1 2 ) ) verdtd = xsync [ 0 ] . findtext ( 'Ver DTD' ) if verdtd != constants . SYNCML DTD VERSION 1 2 : raise common . Feature Not Supported ( 'unsupported Sync ML DTD version \"%s\" (expected \"%s\")' % ( verdtd , constants . SYNCML DTD VERSION 1 2 ) ) ret = self . initialize ( adapter , session , xsync ) hdrcmd = ret [ 0 ] if session . is Server : log . debug ( 'received request Sync ML message from \"%s\" (s%s.m%s)' , hdrcmd . target , hdrcmd . session ID , hdrcmd . msg ID ) else : log . debug ( 'received response Sync ML message from \"%s\" (s%s.m%s)' , lastcmds [ 0 ] . target , lastcmds [ 0 ] . session ID , lastcmds [ 0 ] . msg ID ) try : return self . tree2commands ( adapter , session , lastcmds , xsync , ret ) except Exception , e : if not session . is Server : raise code = '%s.%s' % ( e . class . module , e . class . name ) msg = '' . join ( traceback . format exception only ( type ( e ) , e ) ) . strip ( ) log . exception ( 'failed while interpreting command tree: %s' , msg ) return [ hdrcmd , state . Command ( name = constants . CMD STATUS , cmd ID = '1' , msg Ref = session . pending Msg ID , cmd Ref = 0 , source Ref = xsync [ 0 ] . findtext ( 'Source/Loc URI' ) , target Ref = xsync [ 0 ] . findtext ( 'Target/Loc URI' ) , status Of = constants . CMD SYNCHDR , status Code = constants . STATUS COMMAND FAILED , error Code = code , error Msg = msg , error Trace = '' . join ( traceback . format exception ( type ( e ) , e , sys . exc info ( ) [ 2 ] ) ) , ) , state . Command ( name = constants . CMD FINAL ) ]", "predictions": ["run a search query ."], "references": ["consumes an et protocol tree and converts it to state . command commands"], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 5006, "code": "def parse single computer ( entry ) : computer = Computer ( dns hostname = get field ( entry , 'd NS Host Name' ) , description = get field ( entry , 'description' ) , os = get field ( entry , 'operating System' ) , group id = get field ( entry , 'primary Group ID' ) ) try : ip = str ( ipaddress . ip address ( get field ( entry , 'I Pv4' ) ) ) except Value Error : ip = '' if ip : computer . ip = ip elif computer . dns hostname : computer . ip = resolve ip ( computer . dns hostname ) return computer", "predictions": ["parse a single computer entry ."], "references": ["parse the entry into a computer object ."], "bleu": 0.20830666398386116, "rouge_l": 0.5570776255707762}
{"id": 5007, "code": "def parse domain computers ( filename ) : with open ( filename ) as f : data = json . loads ( f . read ( ) ) hs = Host Search ( ) count = 0 entry count = 0 print notification ( \"Parsing {} entries\" . format ( len ( data ) ) ) for system in data : entry count += 1 parsed = parse single computer ( system ) if parsed . ip : try : host = hs . id to object ( parsed . ip ) host . description . append ( parsed . description ) host . hostname . append ( parsed . dns hostname ) if parsed . os : host . os = parsed . os host . domain controller = parsed . dc host . add tag ( 'domaindump' ) host . save ( ) count += 1 except Value Error : pass sys . stdout . write ( '\\r' ) sys . stdout . write ( \"[{}/{}] {} resolved\" . format ( entry count , len ( data ) , count ) ) sys . stdout . flush ( ) sys . stdout . write ( '\\r' ) return count", "predictions": ["parse the computers computers file ."], "references": ["parse the file and extract the computers import the computers that resolve into jackal ."], "bleu": 0.08872444253557527, "rouge_l": 0.44202898550724634}
{"id": 5008, "code": "def parse user ( entry , domain groups ) : result = { } distinguished name = get field ( entry , 'distinguished Name' ) result [ 'domain' ] = \".\" . join ( distinguished name . split ( ',DC=' ) [ 1 : ] ) result [ 'name' ] = get field ( entry , 'name' ) result [ 'username' ] = get field ( entry , 's AM Account Name' ) result [ 'description' ] = get field ( entry , 'description' ) result [ 'sid' ] = get field ( entry , 'object Sid' ) . split ( '-' ) [ - 1 ] primary group = get field ( entry , 'primary Group ID' ) member of = entry [ 'attributes' ] . get ( 'member Of' , [ ] ) groups = [ ] for member in member of : for e in member . split ( ',' ) : if e . startswith ( 'CN=' ) : groups . append ( e [ 3 : ] ) groups . append ( domain groups . get ( primary group , '' ) ) result [ 'groups' ] = groups flags = [ ] try : uac = int ( get field ( entry , 'user Account Control' ) ) for flag , value in uac flags . items ( ) : if uac & value : flags . append ( flag ) except Value Error : pass result [ 'flags' ] = flags return result", "predictions": ["parse a user entry entry"], "references": ["parses a single entry from the domaindump"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 5009, "code": "def parse domain users ( domain users file , domain groups file ) : with open ( domain users file ) as f : users = json . loads ( f . read ( ) ) domain groups = { } if domain groups file : with open ( domain groups file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get field ( group , 'object Sid' ) domain groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get field ( group , 'cn' ) user search = User Search ( ) count = 0 total = len ( users ) print notification ( \"Importing {} users\" . format ( total ) ) for entry in users : result = parse user ( entry , domain groups ) user = user search . id to object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add tag ( \"domaindump\" ) user . save ( ) count += 1 sys . stdout . write ( '\\r' ) sys . stdout . write ( \"[{}/{}]\" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\\r' ) return count", "predictions": ["parse all users from a domain"], "references": ["parses the domain users and groups files ."], "bleu": 0.17516432701748888, "rouge_l": 0.13926940639269406}
{"id": 5010, "code": "def import domaindump ( ) : parser = argparse . Argument Parser ( description = \"Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain computers output for I Ps\" ) parser . add argument ( \"files\" , nargs = '+' , help = \"The domaindump files to import\" ) arguments = parser . parse args ( ) domain users file = '' domain groups file = '' computer count = 0 user count = 0 stats = { } for filename in arguments . files : if filename . endswith ( 'domain computers.json' ) : print notification ( 'Parsing domain computers' ) computer count = parse domain computers ( filename ) if computer count : stats [ 'hosts' ] = computer count print success ( \"{} hosts imported\" . format ( computer count ) ) elif filename . endswith ( 'domain users.json' ) : domain users file = filename elif filename . endswith ( 'domain groups.json' ) : domain groups file = filename if domain users file : print notification ( \"Parsing domain users\" ) user count = parse domain users ( domain users file , domain groups file ) if user count : print success ( \"{} users imported\" . format ( user count ) ) stats [ 'users' ] = user count Logger ( ) . log ( \"import domaindump\" , 'Imported domaindump, found {} user, {} systems' . format ( user count , computer count ) , stats )", "predictions": ["import domaindump files ."], "references": ["parses ldapdomaindump files and stores hosts and users in elasticsearch ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 5011, "code": "def unicode ( string ) : for encoding in [ 'utf-8' , 'latin1' ] : try : result = unicode ( string , encoding ) return result except Unicode Decode Error : pass result = unicode ( string , 'utf-8' , 'replace' ) return result", "predictions": ["convert string to unicode if it is not already ."], "references": ["try to convert a string to unicode using different encodings"], "bleu": 0.24808415001701817, "rouge_l": 0.4}
{"id": 5012, "code": "def build index and mapping ( triples ) : ents = bidict ( ) rels = bidict ( ) ent id = 0 rel id = 0 collected = [ ] for t in triples : for e in ( t . head , t . tail ) : if e not in ents : ents [ e ] = ent id ent id += 1 if t . relation not in rels : rels [ t . relation ] = rel id rel id += 1 collected . append ( kgedata . Triple Index ( ents [ t . head ] , rels [ t . relation ] , ents [ t . tail ] ) ) return collected , ents , rels", "predictions": ["build the index and mapping from the triples to the index"], "references": ["index all triples into indexes and return their mappings"], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 5013, "code": "def recover triples from mapping ( indexes , ents : bidict , rels : bidict ) : triples = [ ] for t in indexes : triples . append ( kgedata . Triple ( ents . inverse [ t . head ] , rels . inverse [ t . relation ] , ents . inverse [ t . tail ] ) ) return triples", "predictions": ["recover the triples from the mapping to a list of triples ."], "references": ["recover triples from mapping ."], "bleu": 0.16261701715194898, "rouge_l": 0.6354166666666666}
{"id": 5014, "code": "def transform triple numpy ( x ) : return np . array ( [ x . head , x . relation , x . tail ] , dtype = np . int64 )", "predictions": ["transform triple to numpy array ."], "references": ["transform triple index into a 1 - d numpy array ."], "bleu": 0.21248506964807395, "rouge_l": 0.5586080586080586}
{"id": 5015, "code": "def pack triples numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( transform triple numpy , triples ) ) , axis = 0 )", "predictions": ["pack triples to numpy array"], "references": ["packs a list of triple indexes into a 2d numpy array ."], "bleu": 0.08860330314183162, "rouge_l": 0.2190305206463196}
{"id": 5016, "code": "def remove near duplicate relation ( triples , threshold = 0.97 ) : logging . debug ( \"remove duplicate\" ) assert threshold ( threshold ) duplicate rel counter = defaultdict ( list ) relations = set ( ) for t in triples : duplicate rel counter [ t . relation ] . append ( f\"{t.head} {t.tail}\" ) relations . add ( t . relation ) relations = list ( relations ) num triples = len ( triples ) removal relation set = set ( ) for rel , values in duplicate rel counter . items ( ) : duplicate rel counter [ rel ] = Superminhash ( values ) for i in relations : for j in relations : if i == j or i in removal relation set or j in removal relation set : continue close relations = [ i ] if set close to ( duplicate rel counter [ i ] , duplicate rel counter [ j ] , threshold ) : close relations . append ( j ) if len ( close relations ) > 1 : close relations . pop ( np . random . randint ( len ( close relations ) ) ) removal relation set |= set ( close relations ) logging . info ( \"Removing {} relations: {}\" . format ( len ( removal relation set ) , str ( removal relation set ) ) ) return list ( filterfalse ( lambda x : x . relation in removal relation set , triples ) )", "predictions": ["remove duplicate relation near a threshold"], "references": ["if entity pairs in a relation is as close as another relations only keep one relation of such set ."], "bleu": 0.023705913809862502, "rouge_l": 0.07011494252873564}
{"id": 5017, "code": "def remove direct link triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )", "predictions": ["remove direct link triples triples from a valid direct link ."], "references": ["remove direct links in the training sets ."], "bleu": 0.16108992769687397, "rouge_l": 0.32504440497335696}
{"id": 5018, "code": "def shrink indexes in place ( self , triples ) : ent roots = self . Union Find ( self . ent id ) rel roots = self . Union Find ( self . rel id ) for t in triples : ent roots . add ( t . head ) ent roots . add ( t . tail ) rel roots . add ( t . relation ) for i , t in enumerate ( triples ) : h = ent roots . find ( t . head ) r = rel roots . find ( t . relation ) t = ent roots . find ( t . tail ) triples [ i ] = kgedata . Triple Index ( h , r , t ) ents = bidict ( ) available ent idx = 0 for previous idx , ent exist in enumerate ( ent roots . roots ( ) ) : if not ent exist : self . ents . inverse . pop ( previous idx ) else : ents [ self . ents . inverse [ previous idx ] ] = available ent idx available ent idx += 1 rels = bidict ( ) available rel idx = 0 for previous idx , rel exist in enumerate ( rel roots . roots ( ) ) : if not rel exist : self . rels . inverse . pop ( previous idx ) else : rels [ self . rels . inverse [ previous idx ] ] = available rel idx available rel idx += 1 self . ents = ents self . rels = rels self . ent id = available ent idx self . rel id = available rel idx", "predictions": ["shrink indexes in place with triples ."], "references": ["uses a union find to find segment ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5019, "code": "def freeze ( self ) : data = super ( Index Builder , self ) . freeze ( ) try : base file names = data [ 'docnames' ] except Key Error : base file names = data [ 'filenames' ] store = { } c = itertools . count ( ) for prefix , items in iteritems ( data [ 'objects' ] ) : for name , ( index , typeindex , , shortanchor ) in iteritems ( items ) : objtype = data [ 'objtypes' ] [ typeindex ] if objtype . startswith ( 'cpp:' ) : split = name . rsplit ( '::' , 1 ) if len ( split ) != 2 : warnings . warn ( \"What's up with %s?\" % str ( ( prefix , name , objtype ) ) ) continue prefix , name = split last prefix = prefix . split ( '::' ) [ - 1 ] else : last prefix = prefix . split ( '.' ) [ - 1 ] store [ next ( c ) ] = { 'filename' : base file names [ index ] , 'objtype' : objtype , 'prefix' : prefix , 'last prefix' : last prefix , 'name' : name , 'shortanchor' : shortanchor , } data . update ( { 'store' : store } ) return data", "predictions": ["freeze the two entities entities ."], "references": ["create a usable data structure for serializing ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5020, "code": "def log entity creation ( entity , params = None ) : p = { 'entity' : entity } if params : p [ 'params' ] = params log ( TYPE CODES . CREATE , p )", "predictions": ["log an entity creation ."], "references": ["logs an entity creation"], "bleu": 0.5081327481546147, "rouge_l": 0.6802973977695167}
{"id": 5021, "code": "def log entity deletion ( entity , params = None ) : p = { 'entity' : entity } if params : p [ 'params' ] = params log ( TYPE CODES . DELETE , p )", "predictions": ["log an entity with a deletion"], "references": ["logs an entity creation"], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 5022, "code": "def log operation ( entities , operation name , params = None ) : if isinstance ( entities , ( list , tuple ) ) : entities = list ( entities ) else : entities = [ entities ] p = { 'name' : operation name , 'on' : entities } if params : p [ 'params' ] = params log ( TYPE CODES . OPERATION , p )", "predictions": ["log an operation that was used to log a log operation ."], "references": ["logs an operation done on an entity possibly with other arguments"], "bleu": 0.1367440667823257, "rouge_l": 0.17528735632183906}
{"id": 5023, "code": "def log state ( entity , state ) : p = { 'on' : entity , 'state' : state } log ( TYPE CODES . STATE , p )", "predictions": ["logs a state of the entity ."], "references": ["logs a new state of an entity"], "bleu": 0.32172944208038085, "rouge_l": 0.7142857142857143}
{"id": 5024, "code": "def log update ( entity , update ) : p = { 'on' : entity , 'update' : update } log ( TYPE CODES . UPDATE , p )", "predictions": ["search for an update"], "references": ["logs an update done on an entity"], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 5025, "code": "def format value ( value ) : value id = id ( value ) if value id in recursion breaker . processed : return u'<recursion>' recursion breaker . processed . add ( value id ) try : if isinstance ( value , six . binary type ) : return u\"'{0}'\" . format ( value . decode ( 'utf-8' ) ) elif isinstance ( value , six . text type ) : return u\"u'{0}'\" . format ( value ) elif isinstance ( value , ( list , tuple ) ) : values = list ( map ( format value , value ) ) result = serialize list ( u'[' , values , delimiter = u',' ) + u']' return force unicode ( result ) elif isinstance ( value , dict ) : items = six . iteritems ( value ) items = ( tuple ( map ( format value , item ) ) for item in items ) items = list ( items ) items . sort ( ) items = [ serialize text ( u'{0}: ' . format ( key ) , item value ) for key , item value in items ] result = serialize list ( u'{' , items , delimiter = u',' ) + u'}' return force unicode ( result ) return force unicode ( repr ( value ) ) finally : recursion breaker . processed . remove ( value id )", "predictions": ["argument to argument parse as string"], "references": ["this function should return unicode representation of the value"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 5026, "code": "def traverse ( element , query , deep = False ) : part = query [ 0 ] if not part : query = query [ 1 : ] part = query [ 0 ] deep = True part , predicate = xpath re . match ( query [ 0 ] ) . groups ( ) for c in element . children : if part in ( '*' , c . tagname ) and c . match ( predicate ) : if len ( query ) == 1 : yield c else : for e in traverse ( c , query [ 1 : ] ) : yield e if deep : for e in traverse ( c , query , deep = True ) : yield e", "predictions": ["count tokens matching * * * * * * * * * * * . * ."], "references": ["helper function to traverse an element tree rooted at element yielding nodes matching the query ."], "bleu": 0.07994607499472013, "rouge_l": 0.12187812187812186}
{"id": 5027, "code": "def parse query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm", "predictions": ["argument to argument argument"], "references": ["given a simplified xpath query string returns an array of normalized query parts ."], "bleu": 0.0248009595334312, "rouge_l": 0.0}
{"id": 5028, "code": "def match ( self , pred ) : if not pred : return True pred = pred [ 1 : - 1 ] if pred . startswith ( '@' ) : pred = pred [ 1 : ] if '=' in pred : attr , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '\"' , \"'\" ) : value = value [ 1 : ] if value [ - 1 ] in ( '\"' , \"'\" ) : value = value [ : - 1 ] return self . attrs . get ( attr ) == value else : return pred in self . attrs elif num re . match ( pred ) : index = int ( pred ) if index < 0 : if self . parent : return self . index == ( len ( self . parent . children ) + index ) else : return index == 0 else : return index == self . index else : if '=' in pred : tag , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '\"' , \"'\" ) : value = value [ 1 : ] if value [ - 1 ] in ( '\"' , \"'\" ) : value = value [ : - 1 ] for c in self . children : if c . tagname == tag and c . data == value : return True else : for c in self . children : if c . tagname == pred : return True return False", "predictions": ["determine if this node matches a id if it s string if it s a id"], "references": ["helper function to determine if this node matches the given predicate ."], "bleu": 0.28175950490399515, "rouge_l": 0.36658653846153844}
{"id": 5029, "code": "def get printable columns ( columns , row ) : if not columns : return row return tuple ( row [ c ] for c in columns )", "predictions": ["argument to argparser self . self . self . self . self . self . self . self . self . self . self . self help"], "references": ["return only the part of the row which should be printed ."], "bleu": 0.044915755686574035, "rouge_l": 0.05510388437217705}
{"id": 5030, "code": "def image path ( instance , filename ) : filename , ext = os . path . splitext ( filename . lower ( ) ) instance id hash = hashlib . md5 ( str ( instance . id ) ) . hexdigest ( ) filename hash = '' . join ( random . sample ( hashlib . md5 ( filename . encode ( 'utf-8' ) ) . hexdigest ( ) , 8 ) ) return '{}/{}{}' . format ( instance id hash , filename hash , ext )", "predictions": ["generates a to - to - object to - port to - port to get a to - specific object"], "references": ["generates likely unique image path using md5 hashes"], "bleu": 0.06108557268562171, "rouge_l": 0.07741116751269035}
{"id": 5031, "code": "def ensure format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml to json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json doc to xml ( doc ) else : raise Value Error ( \"Unrecognized input document\" ) return doc", "predictions": ["id for checking if to be return value or return none ."], "references": ["ensures that the provided document is an lxml element or json dict ."], "bleu": 0.10579369505074822, "rouge_l": 0.15885416666666669}
{"id": 5032, "code": "def rename ( self , from name , to name ) : log . info ( 'renaming database from %s to %s' % ( from name , to name ) ) self . run stmt ( 'alter database %s rename to %s' % ( from name , to name ) )", "predictions": ["get a argparser from from"], "references": ["renames an existing database ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5033, "code": "def connections ( self , name ) : stmt = . format ( fields = ', ' . join ( CONNECTION FIELDS ) , datname = name ) return list ( Connection ( * * x ) for x in self . iter results ( stmt ) )", "predictions": ["get a list version of the named name ."], "references": ["returns a list of existing connections to the named database ."], "bleu": 0.20344044854715337, "rouge_l": 0.5893719806763285}
{"id": 5034, "code": "def available ( self , timeout = 5 ) : host = self . connect args [ 'host' ] port = self . connect args [ 'port' ] try : sock = socket . create connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False", "predictions": ["return to the server host"], "references": ["returns true if database server is running false otherwise ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 5035, "code": "def settings ( self ) : stmt = \"select {fields} from pg settings\" . format ( fields = ', ' . join ( SETTINGS FIELDS ) ) settings = [ ] for row in self . iter results ( stmt ) : row [ 'setting' ] = self . vartype map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( * * row ) ) return settings", "predictions": ["returns a string representing the object s object ."], "references": ["returns settings from the server ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 5036, "code": "def breakfast ( self , message = \"Breakfast is ready\" , shout : bool = False ) : return self . helper . output ( message , shout )", "predictions": ["display a message error known by this module known to the given message known is the is true ."], "references": ["say something in the morning"], "bleu": 0.06439931429457924, "rouge_l": 0.09312977099236641}
{"id": 5037, "code": "def lunch ( self , message = \"Time for lunch\" , shout : bool = False ) : return self . helper . output ( message , shout )", "predictions": ["lunch the given adapter ."], "references": ["say something in the afternoon"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 5038, "code": "def dinner ( self , message = \"Dinner is served\" , shout : bool = False ) : return self . helper . output ( message , shout )", "predictions": ["return a entry . . . ."], "references": ["say something in the evening"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5039, "code": "def main ( ) : parser = argparse . Argument Parser ( description = 'Discover and ingest metadata from document sources, ' 'including lsstdoc-based La Te X documents and ' 're Structured Text-based technotes. Metadata can be ' 'upserted into the LSST Projectmeta Mongo DB.' ) parser . add argument ( '--ltd-product' , dest = 'ltd product url' , help = 'URL of an LSST the Docs product ' '(https://keeper.lsst.codes/products/<slug>). If provided, ' 'only this document will be ingested.' ) parser . add argument ( '--github-token' , help = 'Git Hub personal access token.' ) parser . add argument ( '--mongodb-uri' , help = 'Mongo DB connection URI. If provided, metadata will be loaded ' 'into the Projectmeta database. Omit this argument to just ' 'test the ingest pipeline.' ) parser . add argument ( '--mongodb-db' , default = 'lsstprojectmeta' , help = 'Name of Mongo DB database' ) parser . add argument ( '--mongodb-collection' , default = 'resources' , help = 'Name of the Mongo DB collection for projectmeta resources' ) args = parser . parse args ( ) stream handler = logging . Stream Handler ( ) stream formatter = logging . Formatter ( '%(asctime)s %(levelname)8s %(name)s | %(message)s' ) stream handler . set Formatter ( stream formatter ) root logger = logging . get Logger ( ) root logger . add Handler ( stream handler ) root logger . set Level ( logging . WARNING ) app logger = logging . get Logger ( 'lsstprojectmeta' ) app logger . set Level ( logging . DEBUG ) if args . mongodb uri is not None : mongo client = Async IO Motor Client ( args . mongodb uri , ssl = True ) collection = mongo client [ args . mongodb db ] [ args . mongodb collection ] else : collection = None loop = asyncio . get event loop ( ) if args . ltd product url is not None : loop . run until complete ( run single ltd doc ( args . ltd product url , args . github token , collection ) ) else : loop . run until complete ( run bulk etl ( args . github token , collection ) )", "predictions": ["run the core with mongo with mongo ."], "references": ["command line entrypoint to reduce technote metadata ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 5040, "code": "def decorator ( decorator func ) : assert callable ( decorator func ) , type ( decorator func ) def decorator ( func = None , * * kwargs ) : assert func is None or callable ( func ) , type ( func ) if func : return decorator func ( func , * * kwargs ) else : def decorator helper ( func ) : return decorator func ( func , * * kwargs ) return decorator helper return decorator", "predictions": ["a decorator to field a function as a decorator"], "references": ["allows a decorator to be called with or without keyword arguments ."], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 5041, "code": "def make aware ( value , timezone ) : if hasattr ( timezone , 'localize' ) and value not in ( datetime . datetime . min , datetime . datetime . max ) : return timezone . localize ( value , is dst = None ) else : return value . replace ( tzinfo = timezone )", "predictions": ["makes a domain from a date string ."], "references": ["makes a naive datetime . datetime in a given time zone aware ."], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 5042, "code": "def make naive ( value , timezone ) : value = value . astimezone ( timezone ) if hasattr ( timezone , 'normalize' ) : value = timezone . normalize ( value ) return value . replace ( tzinfo = None )", "predictions": ["create a naive naive from a string . . . . . . . . ."], "references": ["makes an aware datetime . datetime naive in a given time zone ."], "bleu": 0.09672649511413092, "rouge_l": 0.21082949308755758}
{"id": 5043, "code": "def to timezone ( self , dt ) : if timezone . is aware ( dt ) : return dt . astimezone ( self . timezone ) else : return timezone . make aware ( dt , self . timezone )", "predictions": ["convert datetime to timezone"], "references": ["converts a datetime to the timezone of this schedule ."], "bleu": 0.11337974147240094, "rouge_l": 0.3978260869565217}
{"id": 5044, "code": "def period ( self ) : start time = self . root . findtext ( 'daily start time' ) if start time : return Period ( text to time ( start time ) , text to time ( self . root . findtext ( 'daily end time' ) ) ) return Period ( datetime . time ( 0 , 0 ) , datetime . time ( 23 , 59 ) )", "predictions": ["0 - patch the build build build build build build build build"], "references": ["a period tuple representing the daily start and end time ."], "bleu": 0.10390302174233558, "rouge_l": 0.08764367816091953}
{"id": 5045, "code": "async def download metadata yaml ( session , github url ) : metadata yaml url = build metadata yaml url ( github url ) async with session . get ( metadata yaml url ) as response : response . raise for status ( ) yaml data = await response . text ( ) return yaml . safe load ( yaml data )", "predictions": ["triples can be a mapping of mapping to mapping . ."], "references": ["download the metadata . yaml file from a technote s github repository ."], "bleu": 0.11294012253658708, "rouge_l": 0.1641991924629879}
{"id": 5046, "code": "def tz ( self ) : if not self . tz : self . tz = tzlocal . get localzone ( ) . zone return self . tz", "predictions": ["return - aware head for current head return none if no head is not found return none ."], "references": ["return the timezone . if none is set use system timezone"], "bleu": 0.08961672320242714, "rouge_l": 0.21631205673758863}
{"id": 5047, "code": "def time ( self , t ) : time = arrow . get ( t ) . format ( 'YYYY-MM-DDTHH:mm:ss' ) self . time = datetime . datetime . strptime ( time , '%Y-%m-%d T%H:%M:%S' )", "predictions": ["setter for . attribute"], "references": ["convert any timestamp into a datetime and save as _time"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 5048, "code": "def as dict ( self ) : entry dict = { } entry dict [ 'UUID' ] = self . uuid entry dict [ 'Creation Date' ] = self . time entry dict [ 'Time Zone' ] = self . tz if self . tags : entry dict [ 'Tags' ] = self . tags entry dict [ 'Entry Text' ] = self . text entry dict [ 'Starred' ] = self . starred entry dict [ 'Location' ] = self . location return entry dict", "predictions": ["json - serializable near of the object"], "references": ["return a dict that represents the dayoneentry"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5049, "code": "def save ( self , entry , with location = True , debug = False ) : entry dict = { } if isinstance ( entry , Day One Entry ) : entry dict = entry . as dict ( ) else : entry dict = entry entry dict [ 'UUID' ] = uuid . uuid4 ( ) . get hex ( ) if with location and not entry dict [ 'Location' ] : entry dict [ 'Location' ] = self . get location ( ) if not all ( ( entry dict [ 'UUID' ] , entry dict [ 'Time Zone' ] , entry dict [ 'Entry Text' ] ) ) : print \"You must provide: Time zone, UUID, Creation Date, Entry Text\" return False if debug is False : file path = self . file path ( entry dict [ 'UUID' ] ) plistlib . write Plist ( entry dict , file path ) else : plist = plistlib . write Plist To String ( entry dict ) print plist return True", "predictions": ["remove an entry from the db"], "references": ["saves a dayoneentry as a plist"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5050, "code": "def file path ( self , uid ) : file name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone journal path , file name )", "predictions": ["returns the indexes for the given self . ."], "references": ["create and return full file path for dayone entry"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 5051, "code": "def combine ( self , members , output file , dimension = None , start index = None , stop index = None , stride = None ) : nco = None try : nco = Nco ( ) except Base Exception : raise Import Error ( \"NCO not found.  The NCO python bindings are required to use 'Collection.combine'.\" ) if len ( members ) > 0 and hasattr ( members [ 0 ] , 'path' ) : members = [ m . path for m in members ] options = [ '-4' ] options += [ '-L' , '3' ] options += [ '-h' ] if dimension is not None : if start index is None : start index = 0 if stop index is None : stop index = '' if stride is None : stride = 1 options += [ '-d' , '{0},{1},{2},{3}' . format ( dimension , start index , stop index , stride ) ] nco . ncrcat ( input = members , output = output file , options = options )", "predictions": ["freeze a set of members together"], "references": ["combine many files into a single file on disk . defaults to using the time dimension ."], "bleu": 0.035316782215334214, "rouge_l": 0.0800524934383202}
{"id": 5052, "code": "def get parameters ( self ) : if self . plugin class is None : sig = inspect . signature ( self . func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if not parameter . kind in [ parameter . POSITIONAL ONLY , parameter . KEYWORD ONLY , parameter . POSITIONAL OR KEYWORD ] : raise Runtime Error ( \"Task {} contains an unsupported {} parameter\" . format ( parameter , parameter . kind ) ) yield parameter else : var keyword seen = set ( ) for cls in inspect . getmro ( self . plugin class ) : if issubclass ( cls , Base Plugin ) and hasattr ( cls , self . func . name ) : func = getattr ( cls , self . func . name ) logger . debug ( \"Found method %s from class %s\" , func , cls ) var keyword found = False sig = inspect . signature ( func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if index == 0 : continue if parameter . kind == inspect . Parameter . VAR KEYWORD : var keyword found = True continue if parameter . kind in [ parameter . POSITIONAL ONLY , parameter . VAR POSITIONAL ] : raise Runtime Error ( \"Task {} contains an unsupported parameter \\\"{}\\\"\" . format ( func , parameter ) ) if not parameter . name in var keyword seen : var keyword seen . add ( parameter . name ) logger . debug ( \"Found parameter %s (%s)\" , parameter , parameter . kind ) yield parameter if not var keyword found : break", "predictions": ["returns a list of entity entity entity entity entity entity . . ."], "references": ["returns a list of parameters"], "bleu": 0.26584835766658776, "rouge_l": 0.48316831683168315}
{"id": 5053, "code": "def get configuration ( self , key , default = None ) : if key in self . config : return self . config . get ( key ) else : return default", "predictions": ["log a entity from the if it is not present"], "references": ["returns the configuration for key"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 5054, "code": "def gml to geojson ( el ) : if el . get ( 'srs Name' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srs Name' ) == 'EPSG:4326' : return gmlv2 to geojson ( el ) else : raise Not Implemented Error ( \"Unrecognized srs Name %s\" % el . get ( 'srs Name' ) ) tag = el . tag . replace ( '{%s}' % NS GML , '' ) if tag == 'Point' : coordinates = reverse gml coords ( el . findtext ( '{%s}pos' % NS GML ) ) [ 0 ] elif tag == 'Line String' : coordinates = reverse gml coords ( el . findtext ( '{%s}pos List' % NS GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:Linear Ring/gml:pos List' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:Linear Ring/gml:pos List' , namespaces = NSMAP ) : coordinates . append ( reverse gml coords ( ring . text ) ) elif tag in ( 'Multi Point' , 'Multi Line String' , 'Multi Polygon' ) : single type = tag [ 5 : ] member tag = single type [ 0 ] . lower ( ) + single type [ 1 : ] + 'Member' coordinates = [ gml to geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member tag , single type ) , namespaces = NSMAP ) ] else : raise Not Implemented Error return { 'type' : tag , 'coordinates' : coordinates }", "predictions": ["convert a list of geojson to geojson ."], "references": ["given an lxml element of a gml geometry returns a dict in geojson format ."], "bleu": 0.08405999942745353, "rouge_l": 0.24729729729729732}
{"id": 5055, "code": "def gmlv2 to geojson ( el ) : tag = el . tag . replace ( '{%s}' % NS GML , '' ) if tag == 'Point' : coordinates = [ float ( c ) for c in el . findtext ( '{%s}coordinates' % NS GML ) . split ( ',' ) ] elif tag == 'Line String' : coordinates = [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in el . findtext ( '{%s}coordinates' % NS GML ) . split ( ' ' ) ] elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:outer Boundary Is/gml:Linear Ring/gml:coordinates' , namespaces = NSMAP ) + el . xpath ( 'gml:inner Boundary Is/gml:Linear Ring/gml:coordinates' , namespaces = NSMAP ) : coordinates . append ( [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in ring . text . split ( ' ' ) ] ) elif tag in ( 'Multi Point' , 'Multi Line String' , 'Multi Polygon' , 'Multi Curve' ) : if tag == 'Multi Curve' : single type = 'Line String' member tag = 'curve Member' else : single type = tag [ 5 : ] member tag = single type [ 0 ] . lower ( ) + single type [ 1 : ] + 'Member' coordinates = [ gml to geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member tag , single type ) , namespaces = NSMAP ) ] else : raise Not Implemented Error return { 'type' : tag , 'coordinates' : coordinates }", "predictions": ["convert a list of log ring to geojson"], "references": ["translates a deprecated gml 2 . 0 geometry to geojson"], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 5056, "code": "def all subclasses ( cls ) : for subclass in cls . subclasses ( ) : yield subclass for subc in all subclasses ( subclass ) : yield subc", "predictions": ["generator that yields all subclasses of cls ."], "references": ["recursively generate of all the subclasses of class cls ."], "bleu": 0.21632118787624222, "rouge_l": 0.5446428571428571}
{"id": 5057, "code": "def unique justseen ( iterable , key = None ) : try : from itertools import imap as map except Import Error : from builtins import map return map ( next , map ( operator . itemgetter ( 1 ) , itertools . groupby ( iterable , key ) ) )", "predictions": ["returns a unique list of non - overlapping elements of iterable ."], "references": ["list unique elements preserving order . remember only the element just seen ."], "bleu": 0.12020484516681697, "rouge_l": 0.23828125000000006}
{"id": 5058, "code": "def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) return json . JSON Encoder ( self , obj )", "predictions": ["default json encoding ."], "references": ["if input object is an ndarray it will be converted into a list"], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 5059, "code": "def find repos ( self , depth = 10 ) : repos = [ ] for root , subdirs , files in walk dn ( self . root , depth = depth ) : if 'modules' in root : continue if '.git' in subdirs : repos . append ( root ) return repos", "predictions": ["find all repos in a given depth ."], "references": ["get all git repositories within this environment"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5060, "code": "def clone ( self , repo path , destination , branch = None ) : logger . debug ( 'Installing ' + repo path ) if not destination . startswith ( self . env path ) : destination = unipath ( self . env path , destination ) if branch : return shell . run ( 'git' , 'clone' , repo path , '--branch' , branch , '--single-branch' , '--recursive' , destination ) return shell . run ( 'git' , 'clone' , '--recursive' , repo path , destination )", "predictions": ["clone a repository to a specific destination ."], "references": ["clone a repository to a destination relative to envrionment root"], "bleu": 0.4754373049371216, "rouge_l": 0.6535714285714286}
{"id": 5061, "code": "def pull ( self , repo path , * args ) : logger . debug ( 'Pulling ' + repo path ) if not repo path . startswith ( self . env path ) : repo path = unipath ( self . env path , repo path ) return shell . run ( 'git' , 'pull' , * args , * * { 'cwd' : repo path } )", "predictions": ["pull a file from the mesos repo ."], "references": ["clone a repository to a destination relative to envrionment root"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 5062, "code": "def install ( self , package ) : logger . debug ( 'Installing ' + package ) shell . run ( self . pip path , 'install' , package )", "predictions": ["install a package from github"], "references": ["install a python package using pip"], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 5063, "code": "def upgrade ( self , package ) : logger . debug ( 'Upgrading ' + package ) shell . run ( self . pip path , 'install' , '--upgrade' , '--no-deps' , package ) shell . run ( self . pip path , 'install' , package )", "predictions": ["upgrade a package to a package ."], "references": ["update a python package using pip"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5064, "code": "def df quantile ( df , nb = 100 ) : quantiles = np . linspace ( 0 , 1. , nb ) res = pd . Data Frame ( ) for q in quantiles : res = res . append ( df . quantile ( q ) , ignore index = True ) return res", "predictions": ["get the dataframe of a dataframe"], "references": ["returns the nb quantiles for datas in a dataframe"], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 5065, "code": "def rmse ( a , b ) : return np . sqrt ( np . square ( a - b ) . mean ( ) )", "predictions": ["rmse function for the power op"], "references": ["returns the root mean square error betwwen a and b"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5066, "code": "def nmse ( a , b ) : return np . square ( a - b ) . mean ( ) / ( a . mean ( ) * b . mean ( ) )", "predictions": ["return the mean value of a and b ."], "references": ["returns the normalized mean square error of a and b"], "bleu": 0.3823532128872633, "rouge_l": 0.6256410256410255}
{"id": 5067, "code": "def mfbe ( a , b ) : return 2 * bias ( a , b ) / ( a . mean ( ) + b . mean ( ) )", "predictions": ["mean between a and b ."], "references": ["returns the mean fractionalized bias error"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5068, "code": "def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100", "predictions": ["sum of a and b ."], "references": ["returns the factor of exceedance"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5069, "code": "def fmt ( a , b ) : return 100 * np . min ( [ a , b ] , axis = 0 ) . sum ( ) / np . max ( [ a , b ] , axis = 0 ) . sum ( )", "predictions": ["sum op op ."], "references": ["figure of merit in time"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 5070, "code": "def site path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py ver , 'site-packages' )", "predictions": ["return the site path for the current folder ."], "references": ["path to environments site - packages"], "bleu": 0.15619699684601276, "rouge_l": 0.13832199546485258}
{"id": 5071, "code": "def command ( self ) : cmd = self . config . get ( 'command' , None ) if cmd is None : return cmd = cmd [ platform ] return cmd [ 'path' ] , cmd [ 'args' ]", "predictions": ["return the command to execute ."], "references": ["command used to launch this application module"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 5072, "code": "def get modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is module ( path ) : modules . add ( Module ( cwd ) ) module paths = get module paths ( ) for module path in module paths : for d in os . listdir ( module path ) : path = unipath ( module path , d ) if utils . is module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )", "predictions": ["get all modules available in the current environment ."], "references": ["returns a list of available modules ."], "bleu": 0.16784459625186196, "rouge_l": 0.2557651991614256}
{"id": 5073, "code": "def add active module ( module ) : modules = set ( get active modules ( ) ) modules . add ( module ) new modules path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV ACTIVE MODULES' ] = str ( new modules path )", "predictions": ["add the active module to the module ."], "references": ["add a module to cpenv_active_modules environment variable"], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 5074, "code": "def rem active module ( module ) : modules = set ( get active modules ( ) ) modules . discard ( module ) new modules path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV ACTIVE MODULES' ] = str ( new modules path )", "predictions": ["set the active modules ."], "references": ["remove a module from cpenv_active_modules environment variable"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 5075, "code": "def format objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = type and name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . Virtual Environment ) : data . append ( get info ( obj ) ) modules = obj . get modules ( ) if children and modules : for mod in modules : data . append ( get info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d}  {:%d}  {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\\n' + bold blue ( tmpl . format ( * columns ) ) ) for obj data in data : lines . append ( tmpl . format ( * obj data ) ) return '\\n' . join ( lines )", "predictions": ["return formatted objects in a human readable way ."], "references": ["format a list of environments and modules for terminal output"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 5076, "code": "def list ( ) : environments = cpenv . get environments ( ) modules = cpenv . get modules ( ) click . echo ( format objects ( environments + modules , children = True ) )", "predictions": ["list all available modules ."], "references": ["list available environments and modules"], "bleu": 0.32466791547509893, "rouge_l": 0.6}
{"id": 5077, "code": "def create ( name or path , config ) : if not name or path : ctx = click . get current context ( ) click . echo ( ctx . get help ( ) ) examples = ( '\\n Examples:\\n' '    cpenv create my env\\n' '    cpenv create ./relative/path/to/my env\\n' '    cpenv create my env --config ./relative/path/to/config\\n' '    cpenv create my env --config git@github.com:user/config.git\\n' ) click . echo ( examples ) return click . echo ( blue ( 'Creating a new virtual environment ' + name or path ) ) try : env = cpenv . create ( name or path , config ) except Exception as e : click . echo ( bold red ( 'FAILED TO CREATE ENVIRONMENT!' ) ) click . echo ( e ) else : click . echo ( bold green ( 'Successfully created environment!' ) ) click . echo ( blue ( 'Launching subshell' ) ) cpenv . activate ( env ) shell . launch ( env . name )", "predictions": ["create a new virtual virtual context ."], "references": ["create a new environment ."], "bleu": 0.3655552228545123, "rouge_l": 0.6873239436619719}
{"id": 5078, "code": "def list ( ) : click . echo ( 'Cached Environments' ) environments = list ( Environment Cache ) click . echo ( format objects ( environments , children = False ) )", "predictions": ["list all available environments ."], "references": ["list available environments and modules"], "bleu": 0.3860973950960897, "rouge_l": 0.6}
{"id": 5079, "code": "def localize ( name ) : env = cpenv . get active env ( ) if not env : click . echo ( 'You need to activate an environment first.' ) return try : r = cpenv . resolve ( name ) except cpenv . Resolve Error as e : click . echo ( '\\n' + str ( e ) ) module = r . resolved [ 0 ] if isinstance ( module , cpenv . Virtual Environment ) : click . echo ( '\\n Can only localize a module not an environment' ) return active modules = cpenv . get active modules ( ) if module in active modules : click . echo ( '\\n Can not localize an active module.' ) return if module in env . get modules ( ) : click . echo ( '\\n{} is already local to {}' . format ( module . name , env . name ) ) return if click . confirm ( '\\n Add {} to env {}?' . format ( module . name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : module = env . add module ( module . name , module . path ) except : click . echo ( bold red ( 'FAILED' ) ) raise else : click . echo ( bold green ( 'OK!' ) ) click . echo ( '\\n Activate the localize module:' ) click . echo ( '    cpenv activate {} {}' . format ( env . name , module . name ) )", "predictions": ["localize an active module ."], "references": ["copy a global module to the active environment ."], "bleu": 0.1458826981425239, "rouge_l": 0.2717149220489978}
{"id": 5080, "code": "def path resolver ( resolver , path ) : path = unipath ( path ) if is environment ( path ) : return Virtual Environment ( path ) raise Resolve Error", "predictions": ["return a path to a given path"], "references": ["resolves virtualenvironments with a relative or absolute path"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5081, "code": "def home resolver ( resolver , path ) : from . api import get home path path = unipath ( get home path ( ) , path ) if is environment ( path ) : return Virtual Environment ( path ) raise Resolve Error", "predictions": ["return the home directory of a given home home directory"], "references": ["resolves virtualenvironments in cpenv_home"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5082, "code": "def cache resolver ( resolver , path ) : env = resolver . cache . find ( path ) if env : return env raise Resolve Error", "predictions": ["cache the given path into the cached cache ."], "references": ["resolves virtualenvironments in environmentcache"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5083, "code": "def module resolver ( resolver , path ) : if resolver . resolved : if isinstance ( resolver . resolved [ 0 ] , Virtual Environment ) : env = resolver . resolved [ 0 ] mod = env . get module ( path ) if mod : return mod raise Resolve Error", "predictions": ["get the module s module using the given path ."], "references": ["resolves module in previously resolved environment ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 5084, "code": "def active env module resolver ( resolver , path ) : from . api import get active env env = get active env ( ) if not env : raise Resolve Error mod = env . get module ( path ) if not mod : raise Resolve Error return mod", "predictions": ["return the active env module ."], "references": ["resolves modules in currently active environment ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 5085, "code": "def broadcast shape ( * args ) : #TODO: currently incorrect result if a Sequence is provided as an input shapes = [ a . shape if hasattr ( type ( a ) , ' array interface ' ) else ( ) for a in args ] ndim = max ( len ( sh ) for sh in shapes ) for i , sh in enumerate ( shapes ) : if len ( sh ) < ndim : shapes [ i ] = ( 1 , ) * ( ndim - len ( sh ) ) + sh return tuple ( max ( sh [ ax ] for sh in shapes ) for ax in range ( ndim ) )", "predictions": ["computes the shape of a list of numbers ."], "references": ["return the shape that would result from broadcasting the inputs"], "bleu": 0.16621692209732, "rouge_l": 0.20854700854700853}
{"id": 5086, "code": "def run ( * args , * * kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check call ( ' ' . join ( args ) , * * kwargs ) return True except subprocess . Called Process Error : logger . debug ( 'Error running: {}' . format ( args ) ) return False", "predictions": ["runs the given command with the given arguments ."], "references": ["returns true if successful false if failure"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5087, "code": "def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid cmdline file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid cmdline file ) as f : cmd = f . read ( ) if cmd . endswith ( '\\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]", "predictions": ["get the cmd cmd info from the current system ."], "references": ["return a command to launch a subshell"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5088, "code": "def run global hook ( hook name , * args ) : hook finder = Hook Finder ( get global hook path ( ) ) hook = hook finder ( hook name ) if hook : hook . run ( * args )", "predictions": ["all hook in the current hook ."], "references": ["attempt to run a global hook by name with args"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5089, "code": "def validate ( self ) : for env in list ( self ) : if not env . exists : self . remove ( env )", "predictions": ["unique environment variables from the environment"], "references": ["validate all the entries in the environment cache ."], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 5090, "code": "def load ( self ) : if not os . path . exists ( self . path ) : return with open ( self . path , 'r' ) as f : env data = yaml . load ( f . read ( ) ) if env data : for env in env data : self . add ( Virtual Environment ( env [ 'root' ] ) )", "predictions": ["default default default default default default default default default default ."], "references": ["load the environment cache from disk ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 5091, "code": "def save ( self ) : env data = [ dict ( name = env . name , root = env . path ) for env in self ] encode = yaml . safe dump ( env data , default flow style = False ) with open ( self . path , 'w' ) as f : f . write ( encode )", "predictions": ["find the config file"], "references": ["save the environment cache to disk ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 5092, "code": "def gather ( obj ) : if hasattr ( obj , ' distob gather ' ) : return obj . distob gather ( ) elif ( isinstance ( obj , collections . Sequence ) and not isinstance ( obj , string types ) ) : return [ gather ( subobj ) for subobj in obj ] else : return obj", "predictions": ["list of objects that have a given object ."], "references": ["retrieve objects that have been distributed making them local again"], "bleu": 0.23506622552594217, "rouge_l": 0.31282051282051276}
{"id": 5093, "code": "def apply ( f , obj , * args , * * kwargs ) : return vectorize ( f ) ( obj , * args , * * kwargs )", "predictions": ["pull a function and pull the first item of the object ."], "references": ["apply a function in parallel to each element of the input"], "bleu": 0.17194656088289215, "rouge_l": 0.3505747126436781}
{"id": 5094, "code": "def is git repo ( path ) : if path . startswith ( 'git@' ) or path . startswith ( 'https://' ) : return True if os . path . exists ( unipath ( path , '.git' ) ) : return True return False", "predictions": ["check if a package is a git self ."], "references": ["returns true if path is a git repository ."], "bleu": 0.2907153684841096, "rouge_l": 0.5555555555555556}
{"id": 5095, "code": "def is home environment ( path ) : home = unipath ( os . environ . get ( 'CPENV HOME' , '~/.cpenv' ) ) path = unipath ( path ) return path . startswith ( home )", "predictions": ["check if a package self . is a home self . . . . . . . . . ."], "references": ["returns true if path is in cpenv_home"], "bleu": 0.06760229884571738, "rouge_l": 0.1622340425531915}
{"id": 5096, "code": "def is redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )", "predictions": [". path is a quantile file 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["returns true if path contains a . cpenv file"], "bleu": 0.06370405230161802, "rouge_l": 0.19805194805194803}
{"id": 5097, "code": "def redirect to env paths ( path ) : with open ( path , 'r' ) as f : redirected = f . read ( ) return shlex . split ( redirected )", "predictions": ["rmse to to to to to to to to"], "references": ["get environment path from redirect file"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5098, "code": "def expandpath ( path ) : return os . path . abspath ( os . path . expandvars ( os . path . expanduser ( path ) ) )", "predictions": ["get the a file name from a a string"], "references": ["returns an absolute expanded path"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5099, "code": "def unipath ( * paths ) : return os . path . normpath ( expandpath ( os . path . join ( * paths ) ) )", "predictions": ["returns a path to a file in the given paths"], "references": ["like os . path . join but also expands and normalizes path parts ."], "bleu": 0.08450033111870488, "rouge_l": 0.08090185676392574}
{"id": 5100, "code": "def binpath ( * paths ) : package root = os . path . dirname ( file ) return os . path . normpath ( os . path . join ( package root , 'bin' , * paths ) )", "predictions": ["get a path to a file with the given paths"], "references": ["like os . path . join but acts relative to this packages bin path ."], "bleu": 0.08461586088475063, "rouge_l": 0.15443037974683543}
{"id": 5101, "code": "def ensure path exists ( path , * args ) : if os . path . exists ( path ) : return os . makedirs ( path , * args )", "predictions": ["fmt path a path path instance ."], "references": ["like os . makedirs but keeps quiet if path already exists"], "bleu": 0.1160873020151595, "rouge_l": 0.10683012259194395}
{"id": 5102, "code": "def walk up ( start dir , depth = 20 ) : root = start dir for i in xrange ( depth ) : contents = os . listdir ( root ) subdirs , files = [ ] , [ ] for f in contents : if os . path . isdir ( os . path . join ( root , f ) ) : subdirs . append ( f ) else : files . append ( f ) yield root , subdirs , files parent = os . path . dirname ( root ) if parent and not parent == root : root = parent else : break", "predictions": ["site path to all files in a directory = parent = true = true"], "references": ["walk up a directory tree"], "bleu": 0.11633270842295028, "rouge_l": 0.23018867924528305}
{"id": 5103, "code": "def join seq ( d , k , v ) : if k not in d : d [ k ] = list ( v ) elif isinstance ( d [ k ] , list ) : for item in v : if item not in d [ k ] : d [ k ] . insert ( 0 , item ) elif isinstance ( d [ k ] , string types ) : v . append ( d [ k ] ) d [ k ] = v", "predictions": ["command to command with cmd cmd"], "references": ["add a sequence value to env dict"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5104, "code": "def join dicts ( * dicts ) : out dict = { } for d in dicts : for k , v in d . iteritems ( ) : if not type ( v ) in JOINERS : raise Key Error ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) JOINERS [ type ( v ) ] ( out dict , k , v ) return out dict", "predictions": ["merge a list of dictionaries into a single dict"], "references": ["join a bunch of dicts"], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 5105, "code": "def get store env tmp ( ) : tempdir = tempfile . gettempdir ( ) temp name = 'envstore{0:0>3d}' temp path = unipath ( tempdir , temp name . format ( random . getrandbits ( 9 ) ) ) if not os . path . exists ( temp path ) : return temp path else : return get store env tmp ( )", "predictions": ["add in temporary file to the temporary environment ."], "references": ["returns an unused random filepath ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5106, "code": "def upstream url ( self , uri ) : return self . application . options . upstream + self . request . uri", "predictions": ["set upstream active active uri"], "references": ["returns the url to the upstream data source for the given uri based on configuration"], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 5107, "code": "def make upstream request ( self ) : url = self . upstream url ( self . request . uri ) return tornado . httpclient . HTTP Request ( url , method = self . request . method , headers = self . request . headers , body = self . request . body if self . request . body else None )", "predictions": ["format the objects to be sent to the objects"], "references": ["return request object for calling the upstream"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5108, "code": "def k ion ( self , E ) : return self . n p * np . power ( spc . e , 2 ) / ( 2 * sltr . Ge V2joule ( E ) * spc . epsilon 0 )", "predictions": ["calculates the list of list of the list of points given a = surface"], "references": ["geometric focusing force due to ion column for given plasma density as a function of * e *"], "bleu": 0.07899414209772945, "rouge_l": 0.12224448897795591}
{"id": 5109, "code": "def paginate update ( update ) : from happenings . models import Update time = update . pub time event = update . event try : next = Update . objects . filter ( event = event , pub time gt = time ) . order by ( 'pub time' ) . only ( 'title' ) [ 0 ] except : next = None try : previous = Update . objects . filter ( event = event , pub time lt = time ) . order by ( '-pub time' ) . only ( 'title' ) [ 0 ] except : previous = None return { 'next' : next , 'previous' : previous , 'event' : event }", "predictions": ["wrap update in debug mode"], "references": ["attempts to get next and previous on updates"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 5110, "code": "def settings and attributes ( self ) : attrs = self . setting values ( ) attrs . update ( self . dict ) skip = [ \" instance settings\" , \"aliases\" ] for a in skip : del attrs [ a ] return attrs", "predictions": ["a dictionary of list of list of list of list list of list list environments environments environments environments environments environments environments environments environments environments environments environments environments that may include in"], "references": ["return a combined dictionary of setting values and attribute values ."], "bleu": 0.055177848898164926, "rouge_l": 0.15627668659265584}
{"id": 5111, "code": "def get reference to class ( cls , class or class name ) : if isinstance ( class or class name , type ) : return class or class name elif isinstance ( class or class name , string types ) : if \":\" in class or class name : mod name , class name = class or class name . split ( \":\" ) if not mod name in sys . modules : import ( mod name ) mod = sys . modules [ mod name ] return mod . dict [ class name ] else : return cls . load class from locals ( class or class name ) else : msg = \"Unexpected Type '%s'\" % type ( class or class name ) raise Internal Cashew Exception ( msg )", "predictions": ["get reference class class class class class instance"], "references": ["detect if we get a class or a name convert a name to a class ."], "bleu": 0.07015765577419673, "rouge_l": 0.23582474226804123}
{"id": 5112, "code": "def check docstring ( cls ) : docstring = inspect . getdoc ( cls ) if not docstring : breadcrumbs = \" -> \" . join ( t . name for t in inspect . getmro ( cls ) [ : - 1 ] [ : : - 1 ] ) msg = \"docstring required for plugin '%s' (%s, defined in %s)\" args = ( cls . name , breadcrumbs , cls . module ) raise Internal Cashew Exception ( msg % args ) max line length = cls . class settings . get ( 'max-docstring-length' ) if max line length : for i , line in enumerate ( docstring . splitlines ( ) ) : if len ( line ) > max line length : msg = \"docstring line %s of %s is %s chars too long\" args = ( i , cls . name , len ( line ) - max line length ) raise Exception ( msg % args ) return docstring", "predictions": ["path to all plugin defined defined defined if not valid if not raises exception ."], "references": ["asserts that the class has a docstring returning it if successful ."], "bleu": 0.09103526405546068, "rouge_l": 0.15117719950433703}
{"id": 5113, "code": "def resource Path ( self , relative path ) : from os import path import sys try : base path = sys . MEIPASS except Exception : base path = path . dirname ( path . abspath ( file ) ) return path . join ( base path , relative path )", "predictions": ["get the path path path path path path"], "references": ["get absolute path to resource works for dev and for pyinstaller"], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 5114, "code": "def add Logbook ( self , phys Def = \"LCLS\" , mcc Def = \"MCC\" , initial Instance = False ) : if self . log Menu Count < 5 : self . log Menus . append ( Log Select Menu ( self . logui . multi Log Layout , initial Instance ) ) self . log Menus [ - 1 ] . add Logbooks ( self . log Type List [ 1 ] , self . physics programs , phys Def ) self . log Menus [ - 1 ] . add Logbooks ( self . log Type List [ 0 ] , self . mcc programs , mcc Def ) self . log Menus [ - 1 ] . show ( ) self . log Menu Count += 1 if initial Instance : Q Object . connect ( self . log Menus [ - 1 ] . log Button , SIGNAL ( \"clicked()\" ) , self . add Logbook ) else : from functools import partial Q Object . connect ( self . log Menus [ - 1 ] . log Button , SIGNAL ( \"clicked()\" ) , partial ( self . remove Logbook , self . log Menus [ - 1 ] ) )", "predictions": ["cache a log instance"], "references": ["add new block of logbook selection windows . only 5 allowed ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 5115, "code": "def remove Logbook ( self , menu = None ) : if self . log Menu Count > 1 and menu is not None : menu . remove Menu ( ) self . log Menus . remove ( menu ) self . log Menu Count -= 1", "predictions": ["module was registered with menu"], "references": ["remove logbook menu set ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 5116, "code": "def selected Logs ( self ) : mcclogs = [ ] physlogs = [ ] for i in range ( len ( self . log Menus ) ) : log Type = self . log Menus [ i ] . selected Type ( ) log = self . log Menus [ i ] . selected Program ( ) if log Type == \"MCC\" : if log not in mcclogs : mcclogs . append ( log ) elif log Type == \"Physics\" : if log not in physlogs : physlogs . append ( log ) return mcclogs , physlogs", "predictions": ["returns the active raise a list of active active active not active"], "references": ["return selected log books by type ."], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 5117, "code": "def accepted User ( self , log Type ) : from urllib2 import urlopen , URL Error , HTTP Error import json is Approved = False user Name = str ( self . logui . user Name . text ( ) ) if user Name == \"\" : return False if log Type == \"MCC\" : network Fault = False data = [ ] log url = \"https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev json user list.php/?username=\" + user Name try : data = urlopen ( log url , None , 5 ) . read ( ) data = json . loads ( data ) except URL Error as error : print ( \"URL Error: \" + str ( error . reason ) ) network Fault = True except HTTP Error as error : print ( \"HTTP Error: \" + str ( error . reason ) ) network Fault = True if network Fault : msg Box = Q Message Box ( ) msg Box . set Text ( \"Cannot connect to MCC Log Server!\" ) msg Box . set Informative Text ( \"Use entered User name anyway?\" ) msg Box . set Standard Buttons ( Q Message Box . Ok | Q Message Box . Cancel ) msg Box . set Default Button ( Q Message Box . Ok ) if msg Box . exec ( ) == Q Message Box . Ok : is Approved = True if data != [ ] and ( data is not None ) : is Approved = True else : is Approved = True return is Approved", "predictions": ["tuple of user ."], "references": ["verify enetered user name is on accepted mcc logbook list ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 5118, "code": "def prettify ( self , elem ) : from xml . etree import Element Tree from re import sub raw String = Element Tree . tostring ( elem , 'utf-8' ) parsed String = sub ( r'(?=<[^/].*>)' , '\\n' , raw String ) return parsed String [ 1 : ]", "predictions": ["return is a pretty - printed xml string for the element"], "references": ["parse xml elements for pretty printing"], "bleu": 0.1354599427337814, "rouge_l": 0.2484725050916497}
{"id": 5119, "code": "def prepare Images ( self , file Name , log Type ) : import subprocess if self . image Type == \"png\" : self . image Pixmap . save ( file Name + \".png\" , \"PNG\" , - 1 ) if log Type == \"Physics\" : make Post Script = \"convert \" + file Name + \".png \" + file Name + \".ps\" process = subprocess . Popen ( make Post Script , shell = True ) process . wait ( ) thumbnail Pixmap = self . image Pixmap . scaled ( 500 , 450 , Qt . Keep Aspect Ratio ) thumbnail Pixmap . save ( file Name + \".png\" , \"PNG\" , - 1 ) else : rename Image = \"cp \" + self . image + \" \" + file Name + \".gif\" process = subprocess . Popen ( rename Image , shell = True ) process . wait ( ) if log Type == \"Physics\" : thumbnail Pixmap = self . image Pixmap . scaled ( 500 , 450 , Qt . Keep Aspect Ratio ) thumbnail Pixmap . save ( file Name + \".png\" , \"PNG\" , - 1 )", "predictions": ["function for manipulating except the except for the except version of the except"], "references": ["convert supplied qpixmap object to image file ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 5120, "code": "def submit Entry ( self ) : mcclogs , physlogs = self . selected Logs ( ) success = True if mcclogs != [ ] : if not self . accepted User ( \"MCC\" ) : Q Message Box ( ) . warning ( self , \"Invalid User\" , \"Please enter a valid user name!\" ) return file Name = self . xml Setup ( \"MCC\" , mcclogs ) if file Name is None : return if not self . image Pixmap . is Null ( ) : self . prepare Images ( file Name , \"MCC\" ) success = self . send To Logbook ( file Name , \"MCC\" ) if physlogs != [ ] : for i in range ( len ( physlogs ) ) : file Name = self . xml Setup ( \"Physics\" , physlogs [ i ] ) if file Name is None : return if not self . image Pixmap . is Null ( ) : self . prepare Images ( file Name , \"Physics\" ) success phys = self . send To Logbook ( file Name , \"Physics\" , physlogs [ i ] ) success = success and success phys self . done ( success )", "predictions": ["submit the selected image to the xml file ."], "references": ["process user inputs and subit logbook entry when user clicks submit button"], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 5121, "code": "def send To Logbook ( self , file Name , log Type , location = None ) : import subprocess success = True if log Type == \"MCC\" : file String = \"\" if not self . image Pixmap . is Null ( ) : file String = file Name + \".\" + self . image Type logcmd = \"xml2elog \" + file Name + \".xml \" + file String process = subprocess . Popen ( logcmd , shell = True ) process . wait ( ) if process . returncode != 0 : success = False else : from shutil import copy path = \"/u1/\" + location . lower ( ) + \"/physics/logbook/data/\" try : if not self . image Pixmap . is Null ( ) : copy ( file Name + \".png\" , path ) if self . image Type == \"png\" : copy ( file Name + \".ps\" , path ) else : copy ( file Name + \".\" + self . image Type , path ) copy ( file Name + \".xml\" , path ) except IO Error as error : print ( error ) success = False return success", "predictions": ["send the image to the image ."], "references": ["process log information and push to selected logbooks ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5122, "code": "def show ( self ) : self . parent . add Layout ( self . log Select Layout ) self . menu Count += 1 self . connect Slots ( )", "predictions": ["show the menu for window"], "references": ["display menus and connect even signals ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 5123, "code": "def add Logbooks ( self , type = None , logs = [ ] , default = \"\" ) : if type is not None and len ( logs ) != 0 : if type in self . log List : for logbook in logs : if logbook not in self . log List . get ( type ) [ 0 ] : self . log List . get ( type ) [ 0 ] . append ( logbook ) else : self . log List [ type ] = [ ] self . log List [ type ] . append ( logs ) if len ( self . log List [ type ] ) > 1 and default != \"\" : self . log List . get ( type ) [ 1 ] == default else : self . log List . get ( type ) . append ( default ) self . log Type . clear ( ) self . log Type . add Items ( list ( self . log List . keys ( ) ) ) self . change Log Type ( )", "predictions": ["add a new type to the list of type"], "references": ["add or change list of logbooks ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 5124, "code": "def remove Logbooks ( self , type = None , logs = [ ] ) : if type is not None and type in self . log List : if len ( logs ) == 0 or logs == \"All\" : del self . log List [ type ] else : for logbook in logs : if logbook in self . log List [ type ] : self . log List [ type ] . remove ( logbook ) self . change Log Type ( )", "predictions": ["removes all logbooks from the list"], "references": ["remove unwanted logbooks from list ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 5125, "code": "def change Log Type ( self ) : log Type = self . selected Type ( ) programs = self . log List . get ( log Type ) [ 0 ] default = self . log List . get ( log Type ) [ 1 ] if log Type in self . log List : self . program Name . clear ( ) self . program Name . add Items ( programs ) self . program Name . set Current Index ( programs . index ( default ) )", "predictions": ["change the program log ."], "references": ["populate log program list to correspond with log type selection ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 5126, "code": "def add Menu ( self ) : self . parent . multi Log Layout . add Layout ( self . log Select Layout ) self . get Programs ( log Type , program Name )", "predictions": ["adds a program to the list of dictionaries"], "references": ["add menus to parent gui ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5127, "code": "def remove Layout ( self , layout ) : for cnt in reversed ( range ( layout . count ( ) ) ) : item = layout . take At ( cnt ) widget = item . widget ( ) if widget is not None : widget . delete Later ( ) else : '''If sublayout encountered, iterate recursively.''' self . remove Layout ( item . layout ( ) )", "predictions": ["remove all items that have been deleted ."], "references": ["iteratively remove graphical objects from layout ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 5128, "code": "def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set window title ( windowlabel ) if fig is None : fig = plt . gcf ( ) if fig is not None and axes is None : axes = fig . get axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set title ( toplabel ) if xlabel is not None : axes . set xlabel ( xlabel ) if ylabel is not None : axes . set ylabel ( ylabel ) if zlabel is not None : axes . set zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )", "predictions": ["set the label of the given axis ."], "references": ["adds labels to a plot ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5129, "code": "def linkcode resolve ( domain , info ) : if domain != 'py' : return None modname = info [ 'module' ] fullname = info [ 'fullname' ] submod = sys . modules . get ( modname ) if submod is None : return None obj = submod for part in fullname . split ( '.' ) : try : obj = getattr ( obj , part ) except : return None try : fn = inspect . getsourcefile ( obj ) except : fn = None if not fn : return None try : source , lineno = inspect . getsourcelines ( obj ) except : lineno = None if lineno : linespec = \"#L%d-L%d\" % ( lineno , lineno + len ( source ) - 1 ) else : linespec = \"\" fn = relpath ( fn , start = dirname ( scisalt . file ) ) if 'dev' in scisalt . version : return \"http://github.com/joelfrederico/Sci Salt/blob/master/scisalt/%s%s\" % ( fn , linespec ) else : return \"http://github.com/joelfrederico/Sci Salt/blob/v%s/scisalt/%s%s\" % ( scisalt . version , fn , linespec )", "predictions": ["determine the url corresponding to python object ."], "references": ["determine the url corresponding to python object"], "bleu": 0.8633400213704505, "rouge_l": 0.9446902654867256}
{"id": 5130, "code": "def syncdb ( args ) : cmd = args and 'syncdb %s' % ' ' . join ( options . args ) or 'syncdb --noinput' call manage ( cmd ) for fixture in options . paved . django . syncdb . fixtures : call manage ( \"loaddata %s\" % fixture )", "predictions": ["run syncdb command ."], "references": ["update the database with model schema . shorthand for paver manage syncdb ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 5131, "code": "def schema ( args ) : try : import south cmd = args and 'schemamigration %s' % ' ' . join ( options . args ) or 'schemamigration' call manage ( cmd ) except Import Error : error ( 'Could not import south.' )", "predictions": ["run command in appropriate schema ."], "references": ["run south s schemamigration command ."], "bleu": 0.2626909894424158, "rouge_l": 0.5}
{"id": 5132, "code": "def start again message ( self , message = None ) : logging . debug ( \"Start again message delivered: {}\" . format ( message ) ) the answer = ', ' . join ( [ str ( d ) for d in self . game . answer ] [ : - 1 ] ) + ', and ' + [ str ( d ) for d in self . game . answer ] [ - 1 ] return \"{0}{1} The correct answer was {2}. Please start a new game.\" . format ( message , \".\" if message [ - 1 ] not in [ \".\" , \",\" , \";\" , \":\" , \"!\" ] else \"\" , the answer )", "predictions": ["start a again message ."], "references": ["simple method to form a start again message and give the answer in readable form ."], "bleu": 0.047344749835889495, "rouge_l": 0.34807417974322397}
{"id": 5133, "code": "def showfig ( fig , aspect = \"auto\" ) : ax = fig . gca ( ) alim = list ( ax . axis ( ) ) if alim [ 3 ] < alim [ 2 ] : temp = alim [ 2 ] alim [ 2 ] = alim [ 3 ] alim [ 3 ] = temp ax . axis ( alim ) ax . set aspect ( aspect ) fig . show ( )", "predictions": ["show a aspect plot of the axes ."], "references": ["shows a figure with a typical orientation so that x and y axes are set up as expected ."], "bleu": 0.048218604638712956, "rouge_l": 0.20701357466063344}
{"id": 5134, "code": "def cmd init push to cloud ( args ) : ( lcat , ccat ) = ( args . local catalog , args . cloud catalog ) logging . info ( \"[init-push-to-cloud]: %s => %s\" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( \"[init-push-to-cloud] The local catalog does not exist: %s\" % lcat ) if isfile ( ccat ) : args . error ( \"[init-push-to-cloud] The cloud catalog already exist: %s\" % ccat ) ( lmeta , cmeta ) = ( \"%s.lrcloud\" % lcat , \"%s.lrcloud\" % ccat ) if isfile ( lmeta ) : args . error ( \"[init-push-to-cloud] The local meta-data already exist: %s\" % lmeta ) if isfile ( cmeta ) : args . error ( \"[init-push-to-cloud] The cloud meta-data already exist: %s\" % cmeta ) #Let's \"lock\" the local catalog logging . info ( \"Locking local catalog: %s\" % ( lcat ) ) if not lock file ( lcat ) : raise Runtime Error ( \"The catalog %s is locked!\" % lcat ) #Copy catalog from local to cloud, which becomes the new \"base\" changeset util . copy ( lcat , ccat ) mfile = Meta File ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last push' ] [ 'filename' ] = ccat mfile [ 'last push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last push' ] [ 'modification utc' ] = utcnow mfile . flush ( ) mfile = Meta File ( cmeta ) mfile [ 'changeset' ] [ 'is base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) #Let's copy Smart Previews if not args . no smart previews : copy smart previews ( lcat , ccat , local2cloud = True ) #Finally,let's unlock the catalog files logging . info ( \"Unlocking local catalog: %s\" % ( lcat ) ) unlock file ( lcat ) logging . info ( \"[init-push-to-cloud]: Success!\" )", "predictions": ["switches a catalog catalog to a cloud catalog ."], "references": ["initiate the local catalog and push it the cloud"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5135, "code": "def cmd init pull from cloud ( args ) : ( lcat , ccat ) = ( args . local catalog , args . cloud catalog ) logging . info ( \"[init-pull-from-cloud]: %s => %s\" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( \"[init-pull-from-cloud] The local catalog already exist: %s\" % lcat ) if not isfile ( ccat ) : args . error ( \"[init-pull-from-cloud] The cloud catalog does not exist: %s\" % ccat ) ( lmeta , cmeta ) = ( \"%s.lrcloud\" % lcat , \"%s.lrcloud\" % ccat ) if isfile ( lmeta ) : args . error ( \"[init-pull-from-cloud] The local meta-data already exist: %s\" % lmeta ) if not isfile ( cmeta ) : args . error ( \"[init-pull-from-cloud] The cloud meta-data does not exist: %s\" % cmeta ) #Let's \"lock\" the local catalog logging . info ( \"Locking local catalog: %s\" % ( lcat ) ) if not lock file ( lcat ) : raise Runtime Error ( \"The catalog %s is locked!\" % lcat ) #Copy base from cloud to local util . copy ( ccat , lcat ) #Apply changesets cloud DAG = Changeset DAG ( ccat ) path = cloud DAG . path ( cloud DAG . root . hash , cloud DAG . leafs [ 0 ] . hash ) util . apply changesets ( args , path , lcat ) mfile = Meta File ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last push' ] [ 'filename' ] = cloud DAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last push' ] [ 'hash' ] = cloud DAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last push' ] [ 'modification utc' ] = cloud DAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification utc' ] mfile . flush ( ) #Let's copy Smart Previews if not args . no smart previews : copy smart previews ( lcat , ccat , local2cloud = False ) #Finally, let's unlock the catalog files logging . info ( \"Unlocking local catalog: %s\" % ( lcat ) ) unlock file ( lcat ) logging . info ( \"[init-pull-from-cloud]: Success!\" )", "predictions": ["initializes a catalog from a cloud catalog ."], "references": ["initiate the local catalog by downloading the cloud catalog"], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 5136, "code": "def rindex ( mylist : Sequence [ T ] , x : T ) -> int : return len ( mylist ) - mylist [ : : - 1 ] . index ( x ) - 1", "predictions": ["returns the number of samples of the given mylist ."], "references": ["index of the last occurrence of x in the sequence ."], "bleu": 0.17851905035930718, "rouge_l": 0.37770897832817335}
{"id": 5137, "code": "def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assert True ( response ) self . authed = True", "predictions": ["authenticate with the api ."], "references": ["authenticates the superuser account via the web login ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 5138, "code": "def imgmax ( self ) : if not hasattr ( self , ' imgmax' ) : imgmax = np . max ( self . images [ 0 ] ) for img in self . images : imax = np . max ( img ) if imax > imgmax : imgmax = imax self . imgmax = imgmax return self . imgmax", "predictions": ["identify the imgmax of the image"], "references": ["highest value of input image ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 5139, "code": "def imgmin ( self ) : if not hasattr ( self , ' imgmin' ) : imgmin = np . min ( self . images [ 0 ] ) for img in self . images : imin = np . min ( img ) if imin > imgmin : imgmin = imin self . imgmin = imgmin return np . min ( self . image )", "predictions": ["inverse of the image"], "references": ["lowest value of input image ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 5140, "code": "def usage ( prog name = os . path . basename ( sys . argv [ 0 ] ) ) : spacer = ' ' * len ( 'usage: ' ) usage = prog name + ' -b LIST [-S SEPARATOR] [file ...]\\n' + spacer + prog name + ' -c LIST [-S SEPERATOR] [file ...]\\n' + spacer + prog name + ' -f LIST [-d DELIM] [-e] [-S SEPERATOR] [-s] [file ...]' return \"usage: \" + usage . rstrip ( )", "predictions": ["return usage string for given prog ."], "references": ["returns usage string with no trailing whitespace ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 5141, "code": "def parse args ( args ) : parser = argparse . Argument Parser ( description = \"Remove and/or rearrange \" + \"sections from each line of a file(s).\" , usage = usage ( ) [ len ( 'usage: ' ) : ] ) parser . add argument ( '-b' , \"--bytes\" , action = 'store' , type = lst , default = [ ] , help = \"Bytes to select\" ) parser . add argument ( '-c' , \"--chars\" , action = 'store' , type = lst , default = [ ] , help = \"Character to select\" ) parser . add argument ( '-f' , \"--fields\" , action = 'store' , type = lst , default = [ ] , help = \"Fields to select\" ) parser . add argument ( '-d' , \"--delimiter\" , action = 'store' , default = \"\\t\" , help = \"Sets field delimiter(default is TAB)\" ) parser . add argument ( '-e' , \"--regex\" , action = 'store true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add argument ( '-s' , '--skip' , action = 'store true' , help = \"Skip lines that do not contain input delimiter.\" ) parser . add argument ( '-S' , \"--separator\" , action = 'store' , default = \"\\t\" , help = \"Sets field separator for output.\" ) parser . add argument ( 'file' , nargs = '*' , default = \"-\" , help = \"File(s) to cut\" ) return parser . parse args ( args )", "predictions": ["parse command line arguments"], "references": ["setup argparser to process arguments and generate help"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 5142, "code": "def open s3 ( bucket ) : conn = boto . connect s3 ( options . paved . s3 . access id , options . paved . s3 . secret ) try : bucket = conn . get bucket ( bucket ) except boto . exception . S3Response Error : bucket = conn . create bucket ( bucket ) return bucket", "predictions": ["open s3 bucket to s3 bucket"], "references": ["opens connection to s3 returning bucket and key"], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 5143, "code": "def upload s3 ( file path , bucket name , file key , force = False , acl = 'private' ) : file path = path ( file path ) bucket = open s3 ( bucket name ) if file path . isdir ( ) : paths = file path . listdir ( ) paths keys = list ( zip ( paths , [ '%s/%s' % ( file key , p . name ) for p in paths ] ) ) else : paths keys = [ ( file path , file key ) ] for p , k in paths keys : headers = { } s3 key = bucket . get key ( k ) if not s3 key : from boto . s3 . key import Key s3 key = Key ( bucket , k ) content type = mimetypes . guess type ( p ) [ 0 ] if content type : headers [ 'Content-Type' ] = content type file size = p . stat ( ) . st size file data = p . bytes ( ) file md5 , file md5 64 = s3 key . get md5 from hexdigest ( hashlib . md5 ( file data ) . hexdigest ( ) ) if s3 key . etag : s3 md5 = s3 key . etag . replace ( '\"' , '' ) if s3 md5 == file md5 : info ( 'Hash is the same. Skipping %s' % file path ) continue elif not force : s3 datetime = datetime . datetime ( * time . strptime ( s3 key . last modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local datetime = datetime . datetime . utcfromtimestamp ( p . stat ( ) . st mtime ) if local datetime < s3 datetime : info ( \"File %s hasn't been modified since last \" \"being uploaded\" % ( file key ) ) continue info ( \"Uploading %s...\" % ( file key ) ) try : s3 key . set contents from string ( file data , headers , policy = acl , replace = True , md5 = ( file md5 , file md5 64 ) ) except Exception as e : error ( \"Failed: %s\" % e ) raise", "predictions": ["upload file from s3 bucket to s3"], "references": ["upload a local file to s3 ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 5144, "code": "def download s3 ( bucket name , file key , file path , force = False ) : file path = path ( file path ) bucket = open s3 ( bucket name ) file dir = file path . dirname ( ) file dir . makedirs ( ) s3 key = bucket . get key ( file key ) if file path . exists ( ) : file data = file path . bytes ( ) file md5 , file md5 64 = s3 key . get md5 from hexdigest ( hashlib . md5 ( file data ) . hexdigest ( ) ) try : s3 md5 = s3 key . etag . replace ( '\"' , '' ) except Key Error : pass else : if s3 md5 == file md5 : info ( 'Hash is the same. Skipping %s' % file path ) return elif not force : s3 datetime = datetime . datetime ( * time . strptime ( s3 key . last modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local datetime = datetime . datetime . utcfromtimestamp ( file path . stat ( ) . st mtime ) if s3 datetime < local datetime : info ( \"File at %s is less recent than the local version.\" % ( file key ) ) return info ( \"Downloading %s...\" % ( file key ) ) try : with open ( file path , 'w' ) as fo : s3 key . get contents to file ( fo ) except Exception as e : error ( \"Failed: %s\" % e ) raise", "predictions": ["download the file from s3 bucket"], "references": ["download a remote file from s3 ."], "bleu": 0.36798327352994814, "rouge_l": 0.6069651741293532}
{"id": 5145, "code": "def create ical ( request , slug ) : event = get object or 404 ( Event , slug = slug ) start = event . start date start = datetime . datetime ( start . year , start . month , start . day ) if event . end date : end = event . end date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card me . i Calendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = Http Response ( cal . serialize ( ) , content type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response", "predictions": ["create a ical response"], "references": ["creates an ical . ics file for an event using python - card - me ."], "bleu": 0.017888698387160718, "rouge_l": 0.09023668639053255}
{"id": 5146, "code": "def video list ( request , slug ) : event = get object or 404 ( Event , slug = slug ) return render ( request , 'video/video list.html' , { 'event' : event , 'video list' : event . eventvideo set . all ( ) } )", "predictions": ["show a list of all the video ."], "references": ["displays list of videos for given event ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 5147, "code": "def add event ( request ) : form = Add Event Form ( request . POST or None ) if form . is valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE ID instance . submitted by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return Http Response Redirect ( reverse ( 'events index' ) ) return render ( request , 'happenings/event form.html' , { 'form' : form , 'form title' : 'Add an event' } )", "predictions": ["add an event to a single user ."], "references": ["public form to add an event ."], "bleu": 0.3303164318013807, "rouge_l": 0.5398230088495575}
{"id": 5148, "code": "def add memory ( request , slug ) : event = get object or 404 ( Event , slug = slug ) form = Memory Form ( request . POST or None , request . FILES or None ) if form . is valid ( ) : instance = form . save ( commit = False ) instance . user = request . user instance . event = event instance . save ( ) msg = \"Your thoughts were added. \" if request . FILES : photo list = request . FILES . getlist ( 'photos' ) photo count = len ( photo list ) for upload file in photo list : process upload ( upload file , instance , form , event , request ) if photo count > 1 : msg += \"{} images were added and should appear soon.\" . format ( photo count ) else : msg += \"{} image was added and should appear soon.\" . format ( photo count ) messages . success ( request , msg ) return Http Response Redirect ( '../' ) return render ( request , 'happenings/add memories.html' , { 'form' : form , 'event' : event } )", "predictions": ["add memory images to a photo ."], "references": ["adds a memory to an event ."], "bleu": 0.23356898886410002, "rouge_l": 0.42857142857142855}
{"id": 5149, "code": "def register library ( self , module name : str , attr : str , fallback : str = None ) : try : module = importlib . import module ( module name ) except Import Error : if fallback is not None : module = importlib . import module ( fallback ) self . logger . warn ( module name + \" not available: Replaced with \" + fallback ) else : self . logger . warn ( module name + \" not available: No Replacement Specified\" ) if not attr in dir ( self . sketch ) : setattr ( self . sketch , attr , module ) else : self . logger . warn ( attr + \" could not be imported as it's label is already used in the sketch\" )", "predictions": ["register a library library library"], "references": ["inserts interpreter library of imports into sketch in a very non - consensual way"], "bleu": 0.04994299940831281, "rouge_l": 0.09697933227344992}
{"id": 5150, "code": "def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( \".zip\" ) , dst . endswith ( \".zip\" ) ) logging . info ( \"Copy: %s => %s\" % ( src , dst ) ) if szip and dzip : #If both zipped, we can simply use copy shutil . copy2 ( src , dst ) elif szip : with zipfile . Zip File ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise Runtime Error ( \"The zip file '%s' should only have one \" \"compressed file\" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OS Error : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore errors = True ) elif dzip : with zipfile . Zip File ( dst , mode = 'w' , compression = ZIP DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : #None of them are zipped shutil . copy2 ( src , dst )", "predictions": ["copy src from src to dst"], "references": ["file copy that support compress and decompress of zip files"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5151, "code": "def apply changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp patch = join ( tmpdir , \"tmp.patch\" ) tmp lcat = join ( tmpdir , \"tmp.lcat\" ) for node in changesets : remove ( tmp patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp patch ) logging . info ( \"mv %s %s\" % ( catalog , tmp lcat ) ) shutil . move ( catalog , tmp lcat ) cmd = args . patch cmd . replace ( \"$in1\" , tmp lcat ) . replace ( \"$patch\" , tmp patch ) . replace ( \"$out\" , catalog ) logging . info ( \"Patch: %s\" % cmd ) subprocess . check call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore errors = True )", "predictions": ["apply changesets to a specific catalog"], "references": ["apply to the catalog the changesets in the metafile list changesets"], "bleu": 0.12071482560966854, "rouge_l": 0.33516483516483514}
{"id": 5152, "code": "def clean ( self ) : cleaned = super ( Event Form , self ) . clean ( ) if Event . objects . filter ( name = cleaned [ 'name' ] , start date = cleaned [ 'start date' ] ) . count ( ) : raise forms . Validation Error ( u'This event appears to be in the database already.' ) return cleaned", "predictions": ["check that the return value is valid selected ."], "references": ["validate that an event with this name on this date does not exist ."], "bleu": 0.08961856124931385, "rouge_l": 0.1673525377229081}
{"id": 5153, "code": "def loop ( self ) : while True : try : with uncaught greenlet exception context ( ) : self . loop callback ( ) except gevent . Greenlet Exit : break if self . stop event . wait ( self . interval ) : break self . clear ( )", "predictions": ["main loop of the event loop"], "references": ["main loop - used internally ."], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 5154, "code": "def start ( self ) : assert not self . has started ( ) , \"called start() on an active Gevent Loop\" self . stop event = Event ( ) self . greenlet = gevent . spawn ( self . loop )", "predictions": ["show the greenlet add - on - event loop add a new connection to the event loop add it to the event queue add"], "references": ["starts the loop . calling a running loop is an error ."], "bleu": 0.06370405230161802, "rouge_l": 0.2364341085271318}
{"id": 5155, "code": "def kill ( self ) : assert self . has started ( ) , \"called kill() on a non-active Gevent Loop\" self . stop event . set ( ) self . greenlet . kill ( ) self . clear ( )", "predictions": ["send a signal to all the tasks = wait for it to finish"], "references": ["kills the running loop and waits till it gets killed ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 5156, "code": "def hyphens to dashes ( self ) : problematic hyphens = [ ( r'-([.,!)])' , r'---\\1' ) , ( r'(?<=\\d)-(?=\\d)' , '--' ) , ( r'(?<=\\s)-(?=\\s)' , '---' ) ] for problem case in problematic hyphens : self . regex replacement ( * problem case )", "predictions": ["write a normalized normalized normalized normalized normalized normalized normalized normalized case"], "references": ["transform hyphens to various kinds of dashes"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5157, "code": "def str replacement ( self , target , replacement ) : self . data = self . data . replace ( target , replacement )", "predictions": ["programs a replacement in the given target = value = replacement"], "references": ["replace target with replacement"], "bleu": 0.12605968092174913, "rouge_l": 0.2911694510739857}
{"id": 5158, "code": "def regex replacement ( self , target , replacement ) : match = re . compile ( target ) self . data = match . sub ( replacement , self . data )", "predictions": ["compile a replacement log into the given target log log log log log log log log log log log log log log log log log log log log log log log"], "references": ["regex substitute target with replacement"], "bleu": 0.04317900023606586, "rouge_l": 0.06387434554973821}
{"id": 5159, "code": "def showhtml ( ) : import webbrowser opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise Build Failure ( \"Sphinx documentation root (%s) does not exist.\" % docroot ) builddir = docroot / opts . get ( \"builddir\" , \".build\" ) builddir = builddir / 'html' if not builddir . exists ( ) : raise Build Failure ( \"Sphinx build directory (%s) does not exist.\" % builddir ) webbrowser . open ( builddir / 'index.html' )", "predictions": ["make the documentation documentation"], "references": ["open your web browser and display the generated html documentation ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 5160, "code": "def start again ( self , message = None ) : logging . debug ( \"Start again message delivered: {}\" . format ( message ) ) the answer = self . game . answer str return \"{0} The correct answer was {1}. Please start a new game.\" . format ( message , the answer )", "predictions": ["start a message message message cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb cb and"], "references": ["simple method to form a start again message and give the answer in readable form ."], "bleu": 0.04906081629292276, "rouge_l": 0.13545521835677277}
{"id": 5161, "code": "def minify ( self , css ) : css = css . replace ( \"\\r\\n\" , \"\\n\" ) for rule in REPLACERS [ self . level ] : css = re . compile ( rule [ 0 ] , re . MULTILINE | re . UNICODE | re . DOTALL ) . sub ( rule [ 1 ] , css ) return css", "predictions": ["include css and replace css with css"], "references": ["tries to minimize the length of css code passed as parameter . returns string ."], "bleu": 0.059237077985967744, "rouge_l": 0.08531468531468532}
{"id": 5162, "code": "def get or create index ( self , index ratio , index width ) : if not self . index path . exists ( ) or not self . filepath . stat ( ) . st mtime == self . index path . stat ( ) . st mtime : create index ( self . filepath , self . index path , index ratio = index ratio , index width = index width ) return Index File ( str ( self . index path ) )", "predictions": ["syncdb - args syncdb"], "references": ["return an open file - object to the index file"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 5163, "code": "def create ( self , server ) : for chunk in self . cut to size ( ) : server . post ( 'tasks admin' , chunk . as payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )", "predictions": ["schema a new cmd cmd with the given server and replacements and replacements and replacements and replacements and replacements and replacements and replacements and replacements and replacements and replacements and replacements"], "references": ["create the tasks on the server"], "bleu": 0.04317900023606586, "rouge_l": 0.12310797174571139}
{"id": 5164, "code": "def update ( self , server ) : for chunk in self . cut to size ( ) : server . put ( 'tasks admin' , chunk . as payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )", "predictions": ["start all the self logging logging"], "references": ["update existing tasks on the server"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5165, "code": "def reconcile ( self , server ) : if not self . challenge . exists ( server ) : raise Exception ( 'Challenge does not exist on server' ) existing = Map Roulette Task Collection . from server ( server , self . challenge ) same = [ ] new = [ ] changed = [ ] deleted = [ ] for task in self . tasks : if task . identifier in [ existing task . identifier for existing task in existing . tasks ] : if task == existing . get by identifier ( task . identifier ) : same . append ( task ) else : changed . append ( task ) else : new . append ( task ) for task in existing . tasks : if task . identifier not in [ task . identifier for task in self . tasks ] : deleted . append ( task ) if new : new Collection = Map Roulette Task Collection ( self . challenge , tasks = new ) new Collection . create ( server ) if changed : changed Collection = Map Roulette Task Collection ( self . challenge , tasks = changed ) changed Collection . update ( server ) if deleted : deleted Collection = Map Roulette Task Collection ( self . challenge , tasks = deleted ) for task in deleted Collection . tasks : task . status = 'deleted' deleted Collection . update ( server ) return { 'same' : same , 'new' : new , 'changed' : changed , 'deleted' : deleted }", "predictions": ["reconcile a aspect aspect ax ax ax ax ax ax ax ax ax ax ax ax"], "references": ["reconcile this collection with the server ."], "bleu": 0.07692375026049747, "rouge_l": 0.09355828220858894}
{"id": 5166, "code": "def yn prompt ( msg , default = True ) : ret = custom prompt ( msg , [ \"y\" , \"n\" ] , \"y\" if default else \"n\" ) if ret == \"y\" : return True return False", "predictions": ["prompts user for confirmation a init init init init"], "references": ["prompts the user for yes or no ."], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 5167, "code": "def custom prompt ( msg , options , default ) : formatted options = [ x . upper ( ) if x == default else x . lower ( ) for x in options ] sure = input ( \"{0} [{1}]: \" . format ( msg , \"/\" . join ( formatted options ) ) ) if len ( sure ) == 0 : return default for option in options : if sure . upper ( ) == option . upper ( ) : return option return default", "predictions": ["prompts the user for a cmd ."], "references": ["prompts the user with custom options ."], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 5168, "code": "def read ( args ) : if args . config file is None or not isfile ( args . config file ) : return logging . info ( \"Reading configure file: %s\" % args . config file ) config = cparser . Config Parser ( ) config . read ( args . config file ) if not config . has section ( 'lrcloud' ) : raise Runtime Error ( \"Configure file has no [lrcloud] section!\" ) for ( name , value ) in config . items ( 'lrcloud' ) : if value == \"True\" : value = True elif value == \"False\" : value = False if getattr ( args , name ) is None : setattr ( args , name , value )", "predictions": ["read read for args ."], "references": ["reading the configure file and adds non - existing attributes to args"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 5169, "code": "def write ( args ) : logging . info ( \"Writing configure file: %s\" % args . config file ) if args . config file is None : return #Let's add each attribute of 'args' to the configure file config = cparser . Config Parser ( ) config . add section ( \"lrcloud\" ) for p in [ x for x in dir ( args ) if not x . startswith ( \" \" ) ] : if p in IGNORE ARGS : continue #We ignore some attributes value = getattr ( args , p ) if value is not None : config . set ( 'lrcloud' , p , str ( value ) ) with open ( args . config file , 'w' ) as f : config . write ( f )", "predictions": ["login to a password = 0 - 1 password = 1 = 0 = 0 = 0 = 0 = 1 ..."], "references": ["writing the configure file with the attributes in args"], "bleu": 0.04657469807170698, "rouge_l": 0.0}
{"id": 5170, "code": "def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t", "predictions": ["clone the images . ."], "references": ["returns a copy of this object"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 5171, "code": "def with revision ( self , label , number ) : t = self . clone ( ) t . revision = Revision ( label , number ) return t", "predictions": ["create a new revision instance"], "references": ["returns a tag with a given revision"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 5172, "code": "def parse ( s ) : try : m = regex . match ( s ) t = Tag ( int ( m . group ( 'major' ) ) , int ( m . group ( 'minor' ) ) , int ( m . group ( 'patch' ) ) ) return t if m . group ( 'label' ) is None else t . with revision ( m . group ( 'label' ) , int ( m . group ( 'number' ) ) ) except Attribute Error : return None", "predictions": ["usage string to usage"], "references": ["parses a string into a tag"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 5173, "code": "def tile ( ) : figs = plt . get fignums ( ) x = 0 y = 0 toppad = 21 size = np . array ( [ 0 , 0 ] ) if ( len ( figs ) != 0 ) : fig = plt . figure ( figs [ 0 ] ) screen = fig . canvas . window . get screen ( ) screenx = screen . get monitor geometry ( screen . get primary monitor ( ) ) screenx = screenx [ 2 ] fig = plt . figure ( figs [ 0 ] ) fig . canvas . manager . window . move ( x , y ) maxy = np . array ( fig . canvas . manager . window . get position ( ) ) [ 1 ] size = np . array ( fig . canvas . manager . window . get size ( ) ) y = maxy x += size [ 0 ] + 1 for fig in figs [ 1 : ] : fig = plt . figure ( fig ) size = np . array ( fig . canvas . manager . window . get size ( ) ) if ( x + size [ 0 ] > screenx ) : x = 0 y = maxy maxy = y + size [ 1 ] + toppad else : maxy = max ( maxy , y + size [ 1 ] + toppad ) fig . canvas . manager . window . move ( x , y ) x += size [ 0 ] + 1", "predictions": ["plot a parse parse argparse argparse"], "references": ["tiles open figures ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5174, "code": "def update time ( sender , * * kwargs ) : comment = kwargs [ 'instance' ] if comment . content type . app label == \"happenings\" and comment . content type . name == \"Update\" : from . models import Update item = Update . objects . get ( id = comment . object pk ) item . save ( )", "predictions": ["open the s3 ."], "references": ["when a comment is added updates the update to set last_updated time"], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 5175, "code": "def start again ( self , message = None ) : logging . debug ( \"Start again message delivered: {}\" . format ( message ) ) the answer = self . get text answer ( ) return \"{0} The correct answer was {1}. Please start a new game.\" . format ( message , the answer )", "predictions": ["upload a message message to s3 . ."], "references": ["simple method to form a start again message and give the answer in readable form ."], "bleu": 0.0741826891259906, "rouge_l": 0.23582474226804123}
{"id": 5176, "code": "def create ( self , server ) : return server . post ( 'challenge admin' , self . as payload ( ) , replacements = { 'slug' : self . slug } )", "predictions": ["download a name for a name path"], "references": ["create the challenge on the server"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5177, "code": "def update ( self , server ) : return server . put ( 'challenge admin' , self . as payload ( ) , replacements = { 'slug' : self . slug } )", "predictions": ["create a server ."], "references": ["update existing challenge on the server"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 5178, "code": "def exists ( self , server ) : try : server . get ( 'challenge' , replacements = { 'slug' : self . slug } ) except Exception : return False return True", "predictions": ["check if a server exists"], "references": ["check if a challenge exists on the server"], "bleu": 0.3086194627209984, "rouge_l": 0.5907990314769976}
{"id": 5179, "code": "def axesfontsize ( ax , fontsize ) : items = ( [ ax . title , ax . xaxis . label , ax . yaxis . label ] + ax . get xticklabels ( ) + ax . get yticklabels ( ) ) for item in items : item . set fontsize ( fontsize )", "predictions": ["add a or all the = true to the or a new one"], "references": ["change the font size for the title x and y labels and x and y tick labels for axis * ax * to * fontsize * ."], "bleu": 0.03869473870131793, "rouge_l": 0.09406322282189668}
{"id": 5180, "code": "def less labels ( ax , x fraction = 0.5 , y fraction = 0.5 ) : nbins = np . size ( ax . get xticklabels ( ) ) ax . locator params ( nbins = np . floor ( nbins * x fraction ) , axis = 'x' ) nbins = np . size ( ax . get yticklabels ( ) ) ax . locator params ( nbins = np . floor ( nbins * y fraction ) , axis = 'y' )", "predictions": ["compute add memory memory memory memory ."], "references": ["scale the number of tick labels in x and y by * x_fraction * and * y_fraction * respectively ."], "bleu": 0.0289990174645553, "rouge_l": 0.0681564245810056}
{"id": 5181, "code": "def send zip ( self , exercise , file , params ) : resp = self . post ( exercise . return url , params = params , files = { \"submission[file]\" : ( 'submission.zip' , file ) } , data = { \"commit\" : \"Submit\" } ) return self . to json ( resp )", "predictions": ["register a library to a library . . ."], "references": ["send zipfile to tmc for given exercise"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5182, "code": "def ancestors ( self ) : ancestors = set ( [ ] ) self . depth ascend ( self , ancestors ) try : ancestors . remove ( self ) except Key Error : pass return list ( ancestors )", "predictions": ["list of copy of the ."], "references": ["returns a list of the ancestors of this node ."], "bleu": 0.19643490002396302, "rouge_l": 0.47843137254901963}
{"id": 5183, "code": "def descendents ( self ) : visited = set ( [ ] ) self . depth descend ( self , visited ) try : visited . remove ( self ) except Key Error : pass return list ( visited )", "predictions": ["apply a . . . . . . . apply ."], "references": ["returns a list of descendents of this node ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5184, "code": "def chisq red ( self ) : if self . chisq red is None : self . chisq red = chisquare ( self . y unweighted . transpose ( ) , np . dot ( self . X unweighted , self . beta ) , self . y error , ddof = 3 , verbose = False ) return self . chisq red", "predictions": ["* red * red * red * red *"], "references": ["the reduced chi - square of the linear least squares"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 5185, "code": "def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task admin' , self . as payload ( ) , replacements = { 'slug' : self . challenge . slug , 'identifier' : self . identifier } )", "predictions": ["create a server ."], "references": ["create the task on the server"], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 5186, "code": "def update ( self , server ) : return server . put ( 'task admin' , self . as payload ( ) , replacements = { 'slug' : self . challenge . slug , 'identifier' : self . identifier } )", "predictions": ["update a server ."], "references": ["update existing task on the server"], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 5187, "code": "def from server ( cls , server , slug , identifier ) : task = server . get ( 'task' , replacements = { 'slug' : slug , 'identifier' : identifier } ) return cls ( * * task )", "predictions": ["create a task object from a server ."], "references": ["retrieve a task from the server"], "bleu": 0.239802967618271, "rouge_l": 0.5865384615384615}
{"id": 5188, "code": "def formatter ( color , s ) : if no coloring : return s return \"{begin}{s}{reset}\" . format ( begin = color , s = s , reset = Colors . RESET )", "predictions": ["format a string to be used in a string with a color ."], "references": ["formats a string with color"], "bleu": 0.18798317647335086, "rouge_l": 0.48316831683168315}
{"id": 5189, "code": "def set Virtual Env ( ) : try : activate = options . virtualenv . activate cmd except Attribute Error : activate = None if activate is None : virtualenv = path ( os . environ . get ( 'VIRTUAL ENV' , '' ) ) if not virtualenv : virtualenv = options . paved . cwd else : virtualenv = path ( virtualenv ) activate = virtualenv / 'bin' / 'activate' if activate . exists ( ) : info ( 'Using default virtualenv at %s' % activate ) options . setdotted ( 'virtualenv.activate cmd' , 'source %s' % activate )", "predictions": ["set the virtualenv environment ."], "references": ["attempt to set the virtualenv activate command if it hasn t been specified ."], "bleu": 0.08881260752338878, "rouge_l": 0.38791732909379967}
{"id": 5190, "code": "def pip install ( * args ) : download cache = ( '--download-cache=%s ' % options . paved . pip . download cache ) if options . paved . pip . download cache else '' shv ( 'pip install %s%s' % ( download cache , ' ' . join ( args ) ) )", "predictions": ["install pip requirements file"], "references": ["send the given arguments to pip install ."], "bleu": 0.14628187563941414, "rouge_l": 0.15721649484536082}
{"id": 5191, "code": "def put resource ( self , url , body ) : headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } if self . token : headers [ \"W-Token\" ] = \"%s\" % self . token response = When I Work DAO ( ) . put URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )", "predictions": ["put a resource to the server"], "references": ["when i work put method ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5192, "code": "def post resource ( self , url , body ) : headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } if self . token : headers [ \"W-Token\" ] = \"%s\" % self . token response = When I Work DAO ( ) . post URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )", "predictions": ["post a resource to the api ."], "references": ["when i work post method ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5193, "code": "def delete resource ( self , url ) : headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } if self . token : headers [ \"W-Token\" ] = \"%s\" % self . token response = When I Work DAO ( ) . delete URL ( url , headers ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )", "predictions": ["delete a resource ."], "references": ["when i work delete method ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 5194, "code": "def all comments ( self ) : ctype = Content Type . objects . get ( app label exact = \"happenings\" , model exact = 'event' ) update ctype = Content Type . objects . get ( app label exact = \"happenings\" , model exact = 'update' ) update ids = self . update set . values list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content type = ctype . id , object pk = self . id ) | Q ( content type = update ctype . id , object pk in = update ids ) )", "predictions": ["get all comments for this group ."], "references": ["returns combined list of event and update comments ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5195, "code": "def get all images ( self ) : self imgs = self . image set . all ( ) update ids = self . update set . values list ( 'id' , flat = True ) u images = Update Image . objects . filter ( update id in = update ids ) return list ( chain ( self imgs , u images ) )", "predictions": ["returns all images for this image ."], "references": ["returns chained list of event and update images ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 5196, "code": "def get all images count ( self ) : self imgs = self . image set . count ( ) update ids = self . update set . values list ( 'id' , flat = True ) u images = Update Image . objects . filter ( update id in = update ids ) . count ( ) count = self imgs + u images return count", "predictions": ["get all images count for this image"], "references": ["gets count of all images from both event and updates ."], "bleu": 0.14834636222628117, "rouge_l": 0.2136602451838879}
{"id": 5197, "code": "def repack ( self ) : items = self . grouped filter ( ) . order by ( 'rank' ) . select for update ( ) for count , item in enumerate ( items ) : item . rank = count + 1 item . save ( rerank = False )", "predictions": ["empty all items in the scene ."], "references": ["removes any blank ranks in the order ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 5198, "code": "def selected course ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : course = Course . get selected ( ) return func ( course , * args , * * kwargs ) return inner", "predictions": ["decorator to mark a course course as a method as a course course method"], "references": ["passes the selected course as the first argument to func ."], "bleu": 0.1250076305588977, "rouge_l": 0.16353887399463804}
{"id": 5199, "code": "def selected exercise ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : exercise = Exercise . get selected ( ) return func ( exercise , * args , * * kwargs ) return inner", "predictions": ["decorator to create selected exercise ."], "references": ["passes the selected exercise as the first argument to func ."], "bleu": 0.1435549295013305, "rouge_l": 0.33516483516483514}
{"id": 5200, "code": "def false exit ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : ret = func ( * args , * * kwargs ) if ret is False : if \"TMC TESTING\" in os . environ : raise TMC Exit ( ) else : sys . exit ( - 1 ) return ret return inner", "predictions": ["decorator to check if a method is false ."], "references": ["if func returns false the program exits immediately ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 5201, "code": "def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn prompt ( \"Override old configuration\" , False ) : return False reset db ( ) if not server : while True : server = input ( \"Server url [https://tmc.mooc.fi/mooc/]: \" ) . strip ( ) if len ( server ) == 0 : server = \"https://tmc.mooc.fi/mooc/\" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( \"http://\" ) or server . startswith ( \"https://\" ) ) : ret = custom prompt ( \"Server should start with http:// or https://\\n\" + \"R: Retry, H: Assume http://, S: Assume https://\" , [ \"r\" , \"h\" , \"s\" ] , \"r\" ) if ret == \"r\" : continue if \"://\" in server : server = server . split ( \"://\" ) [ 1 ] if ret == \"h\" : server = \"http://\" + server elif ret == \"s\" : server = \"https://\" + server break print ( \"Using URL: '{0}'\" . format ( server ) ) while True : if not username : username = input ( \"Username: \" ) if not password : password = getpass ( \"Password: \" ) token = b64encode ( bytes ( \"{0}:{1}\" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( \"utf-8\" ) try : api . configure ( url = server , token = token , test = True ) except API Error as e : print ( e ) if auto is False and yn prompt ( \"Retry authentication\" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )", "predictions": ["configure a connection with the templating server ."], "references": ["configure tmc . py to use your account ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5202, "code": "def download ( course , tid = None , dl all = False , force = False , upgradejava = False , update = False ) : def dl ( id ) : download exercise ( Exercise . get ( Exercise . tid == id ) , force = force , update java = upgradejava , update = update ) if dl all : for exercise in list ( course . exercises ) : dl ( exercise . tid ) elif tid is not None : dl ( int ( tid ) ) else : for exercise in list ( course . exercises ) : if not exercise . is completed : dl ( exercise . tid ) else : exercise . update downloaded ( )", "predictions": ["download all course data"], "references": ["download the exercises from the server ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 5203, "code": "def skip ( course , num = 1 ) : sel = None try : sel = Exercise . get selected ( ) if sel . course . tid != course . tid : sel = None except No Exercise Selected : pass if sel is None : sel = course . exercises . first ( ) else : try : sel = Exercise . get ( Exercise . id == sel . id + num ) except peewee . Does Not Exist : print ( \"There are no more exercises in this course.\" ) return False sel . set select ( ) list all ( single = sel )", "predictions": ["skip a course by course"], "references": ["go to the next exercise ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 5204, "code": "def run ( exercise , command ) : Popen ( [ 'nohup' , command , exercise . path ( ) ] , stdout = DEVNULL , stderr = DEVNULL )", "predictions": ["run a shell command ."], "references": ["spawns a process with command path - of - exercise"], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5205, "code": "def select ( course = False , tid = None , auto = False ) : if course : update ( course = True ) course = None try : course = Course . get selected ( ) except No Course Selected : pass ret = { } if not tid : ret = Menu . launch ( \"Select a course\" , Course . select ( ) . execute ( ) , course ) else : ret [ \"item\" ] = Course . get ( Course . tid == tid ) if \"item\" in ret : ret [ \"item\" ] . set select ( ) update ( ) if ret [ \"item\" ] . path == \"\" : select a path ( auto = auto ) skip ( ) return else : print ( \"You can select the course with `tmc select --course`\" ) return else : selected = None try : selected = Exercise . get selected ( ) except No Exercise Selected : pass ret = { } if not tid : ret = Menu . launch ( \"Select an exercise\" , Course . get selected ( ) . exercises , selected ) else : ret [ \"item\" ] = Exercise . byid ( tid ) if \"item\" in ret : ret [ \"item\" ] . set select ( ) print ( \"Selected {}\" . format ( ret [ \"item\" ] ) )", "predictions": ["displays the course data for a certain course"], "references": ["select a course or an exercise ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 5206, "code": "def submit ( course , tid = None , pastebin = False , review = False ) : if tid is not None : return submit exercise ( Exercise . byid ( tid ) , pastebin = pastebin , request review = review ) else : sel = Exercise . get selected ( ) if not sel : raise No Exercise Selected ( ) return submit exercise ( sel , pastebin = pastebin , request review = review )", "predictions": ["submit a course exercise to a scheduled course ."], "references": ["submit the selected exercise to the server ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 5207, "code": "def paste ( tid = None , review = False ) : submit ( pastebin = True , tid = tid , review = False )", "predictions": ["paste a polygon ."], "references": ["sends the selected exercise to the tmc pastebin ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 5208, "code": "def list all ( course , single = None ) : def bs ( val ) : return \"\u25cf\" i  v l e se \" \" def bc ( val ) : return as success ( \"\u2714\")  i  v l e se a error(\" \u2718 \") def format line ( exercise ) : return \"{0} \u2502 {1} \u2502 {2} \u2502 {3} \u2502 {4}\".format( e xercis e .tid, bs ( exercise . is selected ) , bc ( exercise . is downloaded ) , bc ( exercise . is completed ) , exercise . menuname ( ) ) print ( \"ID{0}\u2502 S \u2502 D \u2502 C \u2502 Name\".format( ( len ( str ( course . exercises [ 0 ] . tid ) ) - 1 ) * \" \" ) ) if single : print ( format line ( single ) ) return for exercise in course . exercises : print ( format line ( exercise ) )", "predictions": ["list all available course ."], "references": ["lists all of the exercises in the current course ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 5209, "code": "def update ( course = False ) : if course : with Spinner . context ( msg = \"Updated course metadata.\" , waitmsg = \"Updating course metadata.\" ) : for course in api . get courses ( ) : old = None try : old = Course . get ( Course . tid == course [ \"id\" ] ) except peewee . Does Not Exist : old = None if old : old . details url = course [ \"details url\" ] old . save ( ) continue Course . create ( tid = course [ \"id\" ] , name = course [ \"name\" ] , details url = course [ \"details url\" ] ) else : selected = Course . get selected ( ) print ( \"Updating exercise data.\" ) for exercise in api . get exercises ( selected ) : old = None try : old = Exercise . byid ( exercise [ \"id\" ] ) except peewee . Does Not Exist : old = None if old is not None : old . name = exercise [ \"name\" ] old . course = selected . id old . is attempted = exercise [ \"attempted\" ] old . is completed = exercise [ \"completed\" ] old . deadline = exercise . get ( \"deadline\" ) old . is downloaded = os . path . isdir ( old . path ( ) ) old . return url = exercise [ \"return url\" ] old . zip url = exercise [ \"zip url\" ] old . submissions url = exercise [ \"exercise submissions url\" ] old . save ( ) download exercise ( old , update = True ) else : ex = Exercise . create ( tid = exercise [ \"id\" ] , name = exercise [ \"name\" ] , course = selected . id , is attempted = exercise [ \"attempted\" ] , is completed = exercise [ \"completed\" ] , deadline = exercise . get ( \"deadline\" ) , return url = exercise [ \"return url\" ] , zip url = exercise [ \"zip url\" ] , submissions url = exercise [ ( \"exercise \" \"submissions \" \"url\" ) ] ) ex . is downloaded = os . path . isdir ( ex . path ( ) ) ex . save ( )", "predictions": ["update all selected course"], "references": ["update the data of courses and or exercises from server ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 5210, "code": "def determine type ( x ) : types = ( int , float , str ) type = filter ( lambda a : is type ( a , x ) , types ) [ 0 ] return type ( x )", "predictions": ["determine the type of x from a list of arrays ."], "references": ["determine the type of x"], "bleu": 0.41722614486115056, "rouge_l": 0.6703296703296704}
{"id": 5211, "code": "def dmap ( fn , record ) : values = ( fn ( v ) for k , v in record . items ( ) ) return dict ( itertools . izip ( record , values ) )", "predictions": ["returns a new record with the same values ."], "references": ["map for a directory"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 5212, "code": "def apply types ( use types , guess type , line ) : new line = { } for k , v in line . items ( ) : if use types . has key ( k ) : new line [ k ] = force type ( use types [ k ] , v ) elif guess type : new line [ k ] = determine type ( v ) else : new line [ k ] = v return new line", "predictions": ["apply types to a single line ."], "references": ["apply the types on the elements of the line"], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 5213, "code": "def format to csv ( filename , skiprows = 0 , delimiter = \"\" ) : if not delimiter : delimiter = \"\\t\" input file = open ( filename , \"r\" ) if skiprows : [ input file . readline ( ) for in range ( skiprows ) ] new filename = os . path . splitext ( filename ) [ 0 ] + \".csv\" output file = open ( new filename , \"w\" ) header = input file . readline ( ) . split ( ) reader = csv . Dict Reader ( input file , fieldnames = header , delimiter = delimiter ) writer = csv . Dict Writer ( output file , fieldnames = header , delimiter = \",\" ) writer . writerow ( dict ( ( x , x ) for x in header ) ) for line in reader : if None in line : del line [ None ] writer . writerow ( line ) input file . close ( ) output file . close ( ) print \"Saved %s.\" % new filename", "predictions": ["format a csv file to csv"], "references": ["convert a file to a . csv file"], "bleu": 0.2619317629190374, "rouge_l": 0.5570776255707762}
{"id": 5214, "code": "def sigma prime ( self ) : return np . sqrt ( self . emit / self . beta ( self . E ) )", "predictions": ["the sigma of the segment ."], "references": ["divergence of matched beam"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 5215, "code": "def n p ( self ) : return 2 * sltr . Ge V2joule ( self . E ) * spc . epsilon 0 / ( self . beta * spc . elementary charge ) ** 2", "predictions": ["return the number of p p ."], "references": ["the plasma density in si units ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5216, "code": "def main ( target , label ) : check environment ( target , label ) click . secho ( 'Fetching tags from the upstream ...' ) handler = Tag Handler ( git . list tags ( ) ) print information ( handler , label ) tag = handler . yield tag ( target , label ) confirm ( tag )", "predictions": ["chisq unweighted * target *"], "references": ["semver tag triggered deployment helper"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5217, "code": "def check environment ( target , label ) : if not git . exists ( ) : click . secho ( 'You must have git installed to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not os . path . isdir ( '.git' ) : click . secho ( 'You must cd into a git repository to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not git . is committed ( ) : click . secho ( 'You must commit or stash your work before proceeding.' , fg = 'red' ) sys . exit ( 1 ) if target is None and label is None : click . secho ( 'You must specify either a target or a label.' , fg = 'red' ) sys . exit ( 1 )", "predictions": ["create a geometries environment if it doesn t exist ."], "references": ["performs some environment checks prior to the program s execution"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 5218, "code": "def print information ( handler , label ) : click . echo ( '=> Latest stable: {tag}' . format ( tag = click . style ( str ( handler . latest stable or 'N/A' ) , fg = 'yellow' if handler . latest stable else 'magenta' ) ) ) if label is not None : latest revision = handler . latest revision ( label ) click . echo ( '=> Latest relative revision ({label}): {tag}' . format ( label = click . style ( label , fg = 'blue' ) , tag = click . style ( str ( latest revision or 'N/A' ) , fg = 'yellow' if latest revision else 'magenta' ) ) )", "predictions": ["update the information information information put a collection of label put it put ."], "references": ["prints latest tag s information"], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 5219, "code": "def confirm ( tag ) : click . echo ( ) if click . confirm ( 'Do you want to create the tag {tag}?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True , abort = True ) : git . create tag ( tag ) if click . confirm ( 'Do you want to push the tag {tag} into the upstream?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True ) : git . push tag ( tag ) click . echo ( 'Done!' ) else : git . delete tag ( tag ) click . echo ( 'Aborted!' )", "predictions": ["push a tag tag tag ."], "references": ["prompts user before proceeding"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5220, "code": "def get state ( self ) : return [ os . path . join ( dp , f ) for dp , , fn in os . walk ( self . dir ) for f in fn ]", "predictions": ["return a list of all the state state coloring coloring coloring coloring"], "references": ["get the current directory state"], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 5221, "code": "def tick ( self ) : self . current += 1 if self . current == self . factor : sys . stdout . write ( '+' ) sys . stdout . flush ( ) self . current = 0", "predictions": ["empty the terminal set up the cursor . ."], "references": ["add one tick to progress bar"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5222, "code": "def pickle ( obj , filepath ) : arr = pkl . dumps ( obj , - 1 ) with open ( filepath , 'wb' ) as f : s = 0 while s < len ( arr ) : e = min ( s + blosc . MAX BUFFERSIZE , len ( arr ) ) carr = blosc . compress ( arr [ s : e ] , typesize = 8 ) f . write ( carr ) s = e", "predictions": ["pip - print an arbitrary graph as a single string"], "references": ["pickle and compress ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5223, "code": "def unpickle ( filepath ) : arr = [ ] with open ( filepath , 'rb' ) as f : carr = f . read ( blosc . MAX BUFFERSIZE ) while len ( carr ) > 0 : arr . append ( blosc . decompress ( carr ) ) carr = f . read ( blosc . MAX BUFFERSIZE ) return pkl . loads ( b\"\" . join ( arr ) )", "predictions": ["put filepath into a nice sequence of } if it s a given file if it s not already ."], "references": ["decompress and unpickle ."], "bleu": 0.06108557268562171, "rouge_l": 0.09472049689440994}
{"id": 5224, "code": "def contact ( request ) : form = Contact Form ( request . POST or None ) if form . is valid ( ) : subject = form . cleaned data [ 'subject' ] message = form . cleaned data [ 'message' ] sender = form . cleaned data [ 'sender' ] cc myself = form . cleaned data [ 'cc myself' ] recipients = settings . CONTACTFORM RECIPIENTS if cc myself : recipients . append ( sender ) send mail ( getattr ( settings , \"CONTACTFORM SUBJECT PREFIX\" , '' ) + subject , message , sender , recipients ) return render ( request , 'contactform/thanks.html' ) return render ( request , 'contactform/contact.html' , { 'form' : form } )", "predictions": ["display a post request"], "references": ["displays the contact form and sends the email"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 5225, "code": "def write ( ) : click . echo ( \"Fantastic. Let's get started. \" ) title = click . prompt ( \"What's the title?\" ) url = slugify ( title ) url = click . prompt ( \"What's the URL?\" , default = url ) click . echo ( \"Got it. Creating %s...\" % url ) scaffold piece ( title , url )", "predictions": ["delete scaffold not scaffold url url url"], "references": ["start a new piece"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5226, "code": "def scaffold ( ) : click . echo ( \"A whole new site? Awesome.\" ) title = click . prompt ( \"What's the title?\" ) url = click . prompt ( \"Great. What's url? http://\" ) click . echo ( \"Got it. Creating %s...\" % url )", "predictions": ["generates a all all all all packages for the project . . . . . . . . . . . . . . . . . . . . ."], "references": ["start a new site ."], "bleu": 0.04317900023606586, "rouge_l": 0.12774869109947642}
{"id": 5227, "code": "def get branches ( self ) : return [ self . sanitize ( branch ) for branch in self . git . branch ( color = \"never\" ) . splitlines ( ) ]", "predictions": ["return the current all all all all all all all all all all all all all all all all all all all all all all all all all all all all"], "references": ["returns a list of the branches"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 5228, "code": "def get current branch ( self ) : return next ( ( self . sanitize ( branch ) for branch in self . git . branch ( color = \"never\" ) . splitlines ( ) if branch . startswith ( '*' ) ) , None )", "predictions": ["get the all images in the current images"], "references": ["returns the currently active branch"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5229, "code": "def create patch ( self , from tag , to tag ) : return str ( self . git . diff ( '{}..{}' . format ( from tag , to tag ) , tty out = False ) )", "predictions": ["repack the patch and order the select = 0"], "references": ["create a patch between tags"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 5230, "code": "def get fuel prices ( self ) -> Get Fuel Prices Response : response = requests . get ( '{}/prices' . format ( API URL BASE ) , headers = self . get headers ( ) , timeout = self . timeout , ) if not response . ok : raise Fuel Check Error . create ( response ) return Get Fuel Prices Response . deserialize ( response . json ( ) )", "predictions": ["returns all course prices for the current request ."], "references": ["fetches fuel prices for all stations ."], "bleu": 0.21105340631872635, "rouge_l": 0.38364779874213834}
{"id": 5231, "code": "def get fuel prices for station ( self , station : int ) -> List [ Price ] : response = requests . get ( '{}/prices/station/{}' . format ( API URL BASE , station ) , headers = self . get headers ( ) , timeout = self . timeout , ) if not response . ok : raise Fuel Check Error . create ( response ) data = response . json ( ) return [ Price . deserialize ( data ) for data in data [ 'prices' ] ]", "predictions": ["retrieve all exercise func func for a station ."], "references": ["gets the fuel prices for a specific fuel station ."], "bleu": 0.2090067144241744, "rouge_l": 0.41709401709401706}
{"id": 5232, "code": "def get fuel prices within radius ( self , latitude : float , longitude : float , radius : int , fuel type : str , brands : Optional [ List [ str ] ] = None ) -> List [ Station Price ] : if brands is None : brands = [ ] response = requests . post ( '{}/prices/nearby' . format ( API URL BASE ) , json = { 'fueltype' : fuel type , 'latitude' : latitude , 'longitude' : longitude , 'radius' : radius , 'brand' : brands , } , headers = self . get headers ( ) , timeout = self . timeout , ) if not response . ok : raise Fuel Check Error . create ( response ) data = response . json ( ) stations = { station [ 'code' ] : Station . deserialize ( station ) for station in data [ 'stations' ] } station prices = [ ] for serialized price in data [ 'prices' ] : price = Price . deserialize ( serialized price ) station prices . append ( Station Price ( price = price , station = stations [ price . station code ] ) ) return station prices", "predictions": ["returns all station func func func func func"], "references": ["gets all the fuel prices within the specified radius ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 5233, "code": "def get fuel price trends ( self , latitude : float , longitude : float , fuel types : List [ str ] ) -> Price Trends : response = requests . post ( '{}/prices/trends/' . format ( API URL BASE ) , json = { 'location' : { 'latitude' : latitude , 'longitude' : longitude , } , 'fueltypes' : [ { 'code' : type } for type in fuel types ] , } , headers = self . get headers ( ) , timeout = self . timeout , ) if not response . ok : raise Fuel Check Error . create ( response ) data = response . json ( ) return Price Trends ( variances = [ Variance . deserialize ( variance ) for variance in data [ 'Variances' ] ] , average prices = [ Average Price . deserialize ( avg price ) for avg price in data [ 'Average Prices' ] ] )", "predictions": ["returns a list of fuel = 0 ."], "references": ["gets the fuel price trends for the given location and fuel types ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 5234, "code": "def pre ( self , command , output dir , vars ) : vars [ 'license name' ] = 'Apache' vars [ 'year' ] = time . strftime ( '%Y' , time . localtime ( ) )", "predictions": ["run download command before each step ."], "references": ["called before template is applied ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5235, "code": "def add Route ( self , f , matcher ) : self . routes . append ( ( f . func name , f , matcher ) )", "predictions": ["skip a function to skip a function"], "references": ["add a route handler and matcher to the collection of possible routes ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 5236, "code": "def tempfile ( filename ) : return tempfile . Named Temporary File ( mode = 'w' , dir = os . path . dirname ( filename ) , prefix = os . path . basename ( filename ) , suffix = os . fsencode ( '.tmp' ) , delete = False )", "predictions": ["create a temporary file from a temporary filename"], "references": ["create a namedtemporaryfile instance to be passed to atomic_writer"], "bleu": 0.1862539773562041, "rouge_l": 0.232824427480916}
{"id": 5237, "code": "def atomic write ( filename ) : f = tempfile ( os . fsencode ( filename ) ) try : yield f finally : f . close ( ) os . replace ( f . name , filename )", "predictions": ["select a file to a given = path ."], "references": ["open a namedtemoraryfile handle in a context manager"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5238, "code": "def get item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , \"r\" ) as f : data = json . load ( f ) results = [ i for i in data if i [ \"uuid\" ] == str ( uuid ) ] if results : return results return None", "predictions": ["submit a single item version of a json"], "references": ["read entry from json file"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5239, "code": "def set item ( filename , item ) : with atomic write ( os . fsencode ( str ( filename ) ) ) as temp file : with open ( os . fsencode ( str ( filename ) ) ) as products file : products data = json . load ( products file ) uuid list = [ i for i in filter ( lambda z : z [ \"uuid\" ] == str ( item [ \"uuid\" ] ) , products data ) ] if len ( uuid list ) == 0 : products data . append ( item ) json . dump ( products data , temp file ) return True return None", "predictions": ["paste a json item to file"], "references": ["save entry to json file"], "bleu": 0.2626909894424158, "rouge_l": 0.3696969696969697}
{"id": 5240, "code": "def update item ( filename , item , uuid ) : with atomic write ( os . fsencode ( str ( filename ) ) ) as temp file : with open ( os . fsencode ( str ( filename ) ) ) as products file : products data = json . load ( products file ) if 'products' in products data [ - 1 ] : [ products data [ i ] [ \"products\" ] [ 0 ] . update ( item ) for ( i , j ) in enumerate ( products data ) if j [ \"uuid\" ] == str ( uuid ) ] else : [ products data [ i ] . update ( item ) for ( i , j ) in enumerate ( products data ) if j [ \"uuid\" ] == str ( uuid ) ] json . dump ( products data , temp file ) return True", "predictions": ["list all products in a success file"], "references": ["update entry by uuid in the json file"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5241, "code": "def main ( ) : plugin = Register ( ) if plugin . args . option == 'command' : plugin . command handle ( ) else : plugin . unknown ( \"Unknown actions.\" )", "predictions": ["update cli with command with a cli with a command with the command with the command with the specified ."], "references": ["register your own mode and handle method here ."], "bleu": 0.06108557268562171, "rouge_l": 0.07402912621359223}
{"id": 5242, "code": "def command handle ( self ) : self . results = self . execute ( self . args . command ) self . close ( ) self . logger . debug ( \"results: {}\" . format ( self . results ) ) if not self . results : self . unknown ( \"{} return nothing.\" . format ( self . args . command ) ) if len ( self . results ) != 1 : self . unknown ( \"{} return more than one number.\" . format ( self . args . command ) ) self . result = int ( self . results [ 0 ] ) self . logger . debug ( \"result: {}\" . format ( self . result ) ) if not isinstance ( self . result , ( int , long ) ) : self . unknown ( \"{} didn't return single number.\" . format ( self . args . command ) ) status = self . ok if self . result > self . args . warning : status = self . warning if self . result > self . args . critical : status = self . critical self . shortoutput = \"{0} return {1}.\" . format ( self . args . command , self . result ) [ self . longoutput . append ( line ) for line in self . results if self . results ] self . perfdata . append ( \"{command}={result};{warn};{crit};0;\" . format ( crit = self . args . critical , warn = self . args . warning , result = self . result , command = self . args . command ) ) status ( self . output ( long output limit = None ) ) self . logger . debug ( \"Return status and exit to Nagios.\" )", "predictions": ["executes a determine if the determine whether a determine connection is available ."], "references": ["get the number of the shell command ."], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 5243, "code": "def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get transport ( ) . open session ( ) except paramiko . SSH Exception as e : self . unknown ( \"Create channel error: %s\" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( \"Settimeout for channel error: %s\" % e ) try : self . logger . debug ( \"command: {}\" . format ( command ) ) self . channel . exec command ( command ) except paramiko . SSH Exception as e : self . unknown ( \"Execute command error: %s\" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( \"Get result error: %s\" % e ) try : self . status = self . channel . recv exit status ( ) except paramiko . SSH Exception as e : self . unknown ( \"Get return code error: %s\" % e ) else : if self . status != 0 : self . unknown ( \"Return code: %d , stderr: %s\" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( \"Execute command finish.\" )", "predictions": ["executes a record and returns the output of the record ."], "references": ["execute a shell command ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 5244, "code": "def close ( self ) : try : self . ssh . close ( ) self . logger . debug ( \"close connect succeed.\" ) except paramiko . SSH Exception as e : self . unknown ( \"close connect error: %s\" % e )", "predictions": ["apply line to the line interface type type type type type type type type type type type and line ."], "references": ["close and exit the connection ."], "bleu": 0.07264339766175722, "rouge_l": 0.17039106145251398}
{"id": 5245, "code": "def slinky ( filename , seconds available , bucket name , aws key , aws secret ) : if not os . environ . get ( 'AWS ACCESS KEY ID' ) and os . environ . get ( 'AWS SECRET ACCESS KEY' ) : print 'Need to set environment variables for AWS access and create a slinky bucket.' exit ( ) print create temp s3 link ( filename , seconds available , bucket name )", "predictions": ["header split splitext + fieldnames into split splitext ."], "references": ["simple program that creates an temp s3 link ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 5246, "code": "def main ( ) : plugin = Register ( ) if plugin . args . option == 'filenumber' : plugin . filenumber handle ( ) else : plugin . unknown ( \"Unknown actions.\" )", "predictions": ["sigma - sigma entrypoint"], "references": ["register your own mode and handle method here ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 5247, "code": "def filenumber handle ( self ) : self . results = [ ] self . dirs = [ ] self . files = [ ] self . ftp = self . connect ( ) self . ftp . dir ( self . args . path , self . results . append ) self . logger . debug ( \"dir results: {}\" . format ( self . results ) ) self . quit ( ) status = self . ok for data in self . results : if \"<DIR>\" in data : self . dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . files . append ( str ( data . split ( ) [ 2 ] ) ) self . result = len ( self . files ) self . logger . debug ( \"result: {}\" . format ( self . result ) ) if self . result > self . args . warning : status = self . warning if self . result > self . args . critical : status = self . critical self . shortoutput = \"Found {0} files in {1}.\" . format ( self . result , self . args . path ) [ self . longoutput . append ( line ) for line in self . results if self . results ] self . perfdata . append ( \"{path}={result};{warn};{crit};0;\" . format ( crit = self . args . critical , warn = self . args . warning , result = self . result , path = self . args . path ) ) self . logger . debug ( \"Return status and output.\" ) status ( self . output ( ) )", "predictions": ["executes the n - 2 - 2 - d command"], "references": ["get the number of files in the folder ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 5248, "code": "def register json ( self , data ) : j = json . loads ( data ) self . last data timestamp = datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) try : for v in j : self . data [ v [ self . id key ] ] = { } self . data [ v [ self . id key ] ] [ self . id key ] = v [ self . id key ] self . data [ v [ self . id key ] ] [ self . value key ] = v [ self . value key ] if self . unit key in v : self . data [ v [ self . id key ] ] [ self . unit key ] = v [ self . unit key ] if self . threshold key in v : self . data [ v [ self . id key ] ] [ self . threshold key ] = v [ self . threshold key ] for k in self . other keys : if k in v : self . data [ v [ self . id key ] ] [ k ] = v [ k ] if self . sensor time key in v : self . data [ v [ self . sensor time key ] ] [ self . sensor time key ] = v [ self . sensor time key ] self . data [ v [ self . id key ] ] [ self . time key ] = self . last data timestamp except Key Error as e : print ( \"The main key was not found on the serial input line: \" + str ( e ) ) except Value Error as e : print ( \"No valid JSON string received. Waiting for the next turn.\" ) print ( \"The error was: \" + str ( e ) )", "predictions": ["register a json data object"], "references": ["register the contents as json"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 5249, "code": "def get translated data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j", "predictions": ["get translated data as a dictionary"], "references": ["translate the data with the translation table"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5250, "code": "def get json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get translated data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j", "predictions": ["returns a json string containing the result of the analysis"], "references": ["get the data in json form"], "bleu": 0.13950796967929133, "rouge_l": 0.13090128755364808}
{"id": 5251, "code": "def get json tuples ( self , prettyprint = False , translate = True ) : j = self . get json ( prettyprint , translate ) if len ( j ) > 2 : if prettyprint : j = j [ 1 : - 2 ] + \",\\n\" else : j = j [ 1 : - 1 ] + \",\" else : j = \"\" return j", "predictions": ["get the json representation of a predefined domain"], "references": ["get the data as json tuples"], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 5252, "code": "def generate html diff ( self , expected fn , expected lines , obtained fn , obtained lines ) : import difflib differ = difflib . Html Diff ( ) return differ . make file ( fromlines = expected lines , fromdesc = expected fn , tolines = obtained lines , todesc = obtained fn , )", "predictions": ["generate html diff for the html file ."], "references": ["returns a nice side - by - side diff of the given files as a string ."], "bleu": 0.06191391391332487, "rouge_l": 0.22536945812807882}
{"id": 5253, "code": "def broadcast tx ( self , address , amount , secret , secondsecret = None , vendorfield = '' ) : peer = random . choice ( self . PEERS ) park = Park ( peer , 4001 , constants . ARK NETHASH , '1.1.1' ) return park . transactions ( ) . create ( address , str ( amount ) , vendorfield , secret , secondsecret )", "predictions": ["broadcast a random tx"], "references": ["broadcasts a transaction to the peerslist using ark - js library"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 5254, "code": "def register ( self , service , name = '' ) : try : is model = issubclass ( service , orb . Model ) except Standard Error : is model = False if is model : self . services [ service . schema ( ) . dbname ( ) ] = ( Model Service , service ) else : super ( Orb Api Factory , self ) . register ( service , name = name )", "predictions": ["register a given service with the given service ."], "references": ["exposes a given service to this api ."], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 5255, "code": "def main ( ) : plugin = Register ( ) if plugin . args . option == 'sql' : plugin . sql handle ( ) else : plugin . unknown ( \"Unknown actions.\" )", "predictions": ["main entry point for the application ."], "references": ["register your own mode and handle method here ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5256, "code": "def main ( ) : global DEBUG argd = docopt ( USAGESTR , version = VERSIONSTR , script = SCRIPT ) DEBUG = argd [ '--debug' ] width = parse int ( argd [ '--width' ] or DEFAULT WIDTH ) or 1 indent = parse int ( argd [ '--indent' ] or ( argd [ '--INDENT' ] or 0 ) ) prepend = ' ' * ( indent * 4 ) if prepend and argd [ '--indent' ] : width -= len ( prepend ) userprepend = argd [ '--prepend' ] or ( argd [ '--PREPEND' ] or '' ) prepend = '' . join ( ( prepend , userprepend ) ) if argd [ '--prepend' ] : width -= len ( userprepend ) userappend = argd [ '--append' ] or ( argd [ '--APPEND' ] or '' ) if argd [ '--append' ] : width -= len ( userappend ) if argd [ 'WORDS' ] : argd [ 'WORDS' ] = ( ( try read file ( w ) if len ( w ) < 256 else w ) for w in argd [ 'WORDS' ] ) words = ' ' . join ( ( w for w in argd [ 'WORDS' ] if w ) ) else : words = read stdin ( ) block = Format Block ( words ) . iter format block ( chars = argd [ '--chars' ] , fill = argd [ '--fill' ] , prepend = prepend , strip first = argd [ '--stripfirst' ] , append = userappend , strip last = argd [ '--striplast' ] , width = width , newlines = argd [ '--newlines' ] , lstrip = argd [ '--lstrip' ] , ) for i , line in enumerate ( block ) : if argd [ '--enumerate' ] : print ( '{: >3}: {}' . format ( i + 1 , line ) ) else : print ( line ) return 0", "predictions": ["main entry point for the program ."], "references": ["main entry point expects doctopt arg dict as argd ."], "bleu": 0.23813694985189848, "rouge_l": 0.45607476635514016}
{"id": 5257, "code": "def debug ( * args , * * kwargs ) : if not ( DEBUG and args ) : return None parent = kwargs . get ( 'parent' , None ) with suppress ( Key Error ) : kwargs . pop ( 'parent' ) backlevel = kwargs . get ( 'back' , 1 ) with suppress ( Key Error ) : kwargs . pop ( 'back' ) frame = inspect . currentframe ( ) while backlevel > 0 : frame = frame . f back backlevel -= 1 fname = os . path . split ( frame . f code . co filename ) [ - 1 ] lineno = frame . f lineno if parent : func = '{}.{}' . format ( parent . class . name , frame . f code . co name ) else : func = frame . f code . co name lineinfo = '{}:{} {}: ' . format ( C ( fname , 'yellow' ) , C ( str ( lineno ) . ljust ( 4 ) , 'blue' ) , C ( ) . join ( C ( func , 'magenta' ) , '()' ) . ljust ( 20 ) ) pargs = list ( C ( a , 'green' ) . str ( ) for a in args ) pargs [ 0 ] = '' . join ( ( lineinfo , pargs [ 0 ] ) ) print err ( * pargs , * * kwargs )", "predictions": ["debug debug messages to stdout"], "references": ["print a message only if debug is truthy ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5258, "code": "def close ( self ) : self . tempfile . close ( ) self . process . terminate ( ) if self . process . is alive ( ) : self . process . kill ( )", "predictions": ["close the process ."], "references": ["disconnect and close * vim * ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 5259, "code": "def init app ( self , app ) : app . config . setdefault ( \"TRACY REQUIRE CLIENT\" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before request ( self . before ) app . after request ( self . after )", "predictions": ["flask application initialization ."], "references": ["setup before_request after_request handlers for tracing ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 5260, "code": "def before ( self ) : if request . path in self . excluded routes : request . tracy exclude = True return request . tracy start time = monotonic ( ) client = request . headers . get ( trace header client , None ) require client = current app . config . get ( \"TRACY REQUIRE CLIENT\" , False ) if client is None and require client : abort ( 400 , \"Missing %s header\" % trace header client ) request . tracy client = client request . tracy id = request . headers . get ( trace header id , new id ( ) )", "predictions": ["before this module before it has been run ."], "references": ["records the starting time of this reqeust ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5261, "code": "def expand words ( self , line , width = 60 ) : if not line . strip ( ) : return line wordi = 1 while len ( strip codes ( line ) ) < width : wordendi = self . find word end ( line , wordi ) if wordendi < 0 : wordi = 1 wordendi = self . find word end ( line , wordi ) if wordendi < 0 : line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 if ' ' not in strip codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line", "predictions": ["expand words in a single string"], "references": ["insert spaces between words until it is wide enough for width ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 5262, "code": "def iter add text ( self , lines , prepend = None , append = None ) : if ( prepend is None ) and ( append is None ) : yield from lines else : fmtpcs = [ '{prepend}' ] if prepend else [ ] fmtpcs . append ( '{line}' ) if append : fmtpcs . append ( '{append}' ) fmtstr = '' . join ( fmtpcs ) yield from ( fmtstr . format ( prepend = prepend , line = line , append = append ) for line in lines )", "predictions": ["iterate over text ."], "references": ["prepend or append text to lines . yields each line ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 5263, "code": "def iter char block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 text = ( self . text if text is None else text ) or '' text = ' ' . join ( text . split ( '\\n' ) ) escapecodes = get codes ( text ) if not escapecodes : yield from ( fmtfunc ( text [ i : i + width ] ) for i in range ( 0 , len ( text ) , width ) ) else : blockwidth = 0 block = [ ] for i , s in enumerate ( get indices list ( text ) ) : block . append ( s ) if len ( s ) == 1 : blockwidth += 1 if blockwidth == width : yield '' . join ( block ) block = [ ] blockwidth = 0 if block : yield '' . join ( block )", "predictions": ["iterate through all valid counterpart characters ."], "references": ["format block by splitting on individual characters ."], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 5264, "code": "def iter space block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 curline = '' text = ( self . text if text is None else text ) or '' for word in text . split ( ) : possibleline = ' ' . join ( ( curline , word ) ) if curline else word codelen = sum ( len ( s ) for s in get codes ( possibleline ) ) reallen = len ( possibleline ) - codelen if reallen > width : yield fmtfunc ( curline ) curline = word else : curline = possibleline if curline : yield fmtfunc ( curline )", "predictions": ["iterator over all space space - separated space ."], "references": ["format block by wrapping on spaces ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5265, "code": "def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )", "predictions": ["runs the game loop ."], "references": ["runs the consumer ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 5266, "code": "def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on error : self . on error ( e , batch ) finally : for item in batch : self . queue . task done ( ) return success", "predictions": ["upload the batch of data to the server ."], "references": ["upload the next batch of items return whether successful ."], "bleu": 0.2187537716852318, "rouge_l": 0.5213675213675214}
{"id": 5267, "code": "def next ( self ) : queue = self . queue items = [ ] item = self . next item ( ) if item is None : return items items . append ( item ) while len ( items ) < self . upload size and not queue . empty ( ) : item = self . next item ( ) if item : items . append ( item ) return items", "predictions": ["get the next item in the queue ."], "references": ["return the next batch of items to upload ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 5268, "code": "def next item ( self ) : queue = self . queue try : item = queue . get ( block = True , timeout = 5 ) return item except Exception : return None", "predictions": ["get the next item in the queue ."], "references": ["get a single item from the queue ."], "bleu": 0.3303164318013807, "rouge_l": 0.625}
{"id": 5269, "code": "def request ( self , batch , attempt = 0 ) : try : q = self . api . new queue ( ) for msg in batch : q . add ( msg [ 'event' ] , msg [ 'value' ] , source = msg [ 'source' ] ) q . submit ( ) except : if attempt > self . retries : raise self . request ( batch , attempt + 1 )", "predictions": ["request a batch of data"], "references": ["attempt to upload the batch and retry before raising an error"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5270, "code": "def main ( ) : ep = requests . get ( TRELLO API DOC ) . content root = html . fromstring ( ep ) links = root . xpath ( '//a[contains(@class, \"reference internal\")]/@href' ) pages = [ requests . get ( TRELLO API DOC + u ) for u in links if u . endswith ( 'index.html' ) ] endpoints = [ ] for page in pages : root = html . fromstring ( page . content ) sections = root . xpath ( '//div[@class=\"section\"]/h2/..' ) for sec in sections : ep html = etree . tostring ( sec ) . decode ( 'utf-8' ) ep text = html2text ( ep html ) . splitlines ( ) match = EP DESC REGEX . match ( ep text [ 0 ] ) if not match : continue ep method , ep url = match . groups ( ) ep text [ 0 ] = ' ' . join ( [ ep method , ep url ] ) ep doc = b64encode ( gzip . compress ( '\\n' . join ( ep text ) . encode ( 'utf-8' ) ) ) endpoints . append ( ( ep method , ep url , ep doc ) ) print ( yaml . dump ( create tree ( endpoints ) ) )", "predictions": ["parse argument and dump pages ."], "references": ["prints the complete yaml ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5271, "code": "def quit ( self ) : try : self . ftp . quit ( ) self . logger . debug ( \"quit connect succeed.\" ) except ftplib . Error as e : self . unknown ( \"quit connect error: %s\" % e )", "predictions": ["quit the ftp connection"], "references": ["close and exit the connection ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 5272, "code": "def query ( self , wql ) : try : self . wql = [ 'wmic' , '-U' , self . args . domain + '\\\\' + self . args . user + '%' + self . args . password , '//' + self . args . host , '--namespace' , self . args . namespace , '--delimiter' , self . args . delimiter , wql ] self . logger . debug ( \"wql: {}\" . format ( self . wql ) ) self . output = subprocess . check output ( self . wql ) self . logger . debug ( \"output: {}\" . format ( self . output ) ) self . logger . debug ( \"wmi connect succeed.\" ) self . wmi output = self . output . splitlines ( ) [ 1 : ] self . logger . debug ( \"wmi output: {}\" . format ( self . wmi output ) ) self . csv header = csv . Dict Reader ( self . wmi output , delimiter = '|' ) self . logger . debug ( \"csv header: {}\" . format ( self . csv header ) ) return list ( self . csv header ) except subprocess . Called Process Error as e : self . unknown ( \"Connect by wmi and run wql error: %s\" % e )", "predictions": ["query wmi for wmi"], "references": ["connect by wmi and run wql ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 5273, "code": "def log file ( self , url = None ) : if url is None : url = self . url f = re . sub ( \"file://\" , \"\" , url ) try : with open ( f , \"a\" ) as of : of . write ( str ( self . store . get json tuples ( True ) ) ) except IO Error as e : print ( e ) print ( \"Could not write the content to the file..\" )", "predictions": ["log the content to the store ."], "references": ["write to a local log file"], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 5274, "code": "def log post ( self , url = None , credentials = None , do verify certificate = True ) : if url is None : url = self . url if credentials is None : credentials = self . credentials if do verify certificate is None : do verify certificate = self . do verify certificate if credentials and \"base64\" in credentials : headers = { \"Content-Type\" : \"application/json\" , 'Authorization' : 'Basic %s' % credentials [ \"base64\" ] } else : headers = { \"Content-Type\" : \"application/json\" } try : request = requests . post ( url , headers = headers , data = self . store . get json ( ) , verify = do verify certificate ) except httplib . Incomplete Read as e : request = e . partial", "predictions": ["log a post request"], "references": ["write to a remote host via http post"], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 5275, "code": "def register credentials ( self , credentials = None , user = None , user file = None , password = None , password file = None ) : if credentials is not None : self . credentials = credentials else : self . credentials = { } if user : self . credentials [ \"user\" ] = user elif user file : with open ( user file , \"r\" ) as of : pattern = re . compile ( \"^user: \" ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ \"user\" ] = re . sub ( pattern , \"\" , l ) if self . credentials [ \"user\" ] [ 0 : 1 ] == '\"' and self . credentials [ \"user\" ] [ - 1 : ] == '\"' : self . credentials [ \"user\" ] = self . credentials [ \"user\" ] [ 1 : - 1 ] if password : self . credentials [ \"password\" ] = password elif password file : with open ( password file , \"r\" ) as of : pattern = re . compile ( \"^password: \" ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ \"password\" ] = re . sub ( pattern , \"\" , l ) if self . credentials [ \"password\" ] [ 0 : 1 ] == '\"' and self . credentials [ \"password\" ] [ - 1 : ] == '\"' : self . credentials [ \"password\" ] = self . credentials [ \"password\" ] [ 1 : - 1 ] if \"user\" in self . credentials and \"password\" in self . credentials : c = self . credentials [ \"user\" ] + \":\" + self . credentials [ \"password\" ] self . credentials [ \"base64\" ] = b64encode ( c . encode ( ) ) . decode ( \"ascii\" )", "predictions": ["register credentials with the user and password ."], "references": ["helper method to store username and password"], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 5276, "code": "def set connection ( host = None , database = None , user = None , password = None ) : c . CONNECTION [ 'HOST' ] = host c . CONNECTION [ 'DATABASE' ] = database c . CONNECTION [ 'USER' ] = user c . CONNECTION [ 'PASSWORD' ] = password", "predictions": ["set a connection to the database ."], "references": ["set connection parameters . call set_connection with no arguments to clear ."], "bleu": 0.11434175042957104, "rouge_l": 0.40197693574958815}
{"id": 5277, "code": "def set delegate ( address = None , pubkey = None , secret = None ) : c . DELEGATE [ 'ADDRESS' ] = address c . DELEGATE [ 'PUBKEY' ] = pubkey c . DELEGATE [ 'PASSPHRASE' ] = secret", "predictions": ["set the address of an address ."], "references": ["set delegate parameters . call set_delegate with no arguments to clear ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5278, "code": "def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipient Id == address : balance += i . amount if i . sender Id == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged blocks = Delegate . blocks ( i . pubkey ) for block in forged blocks : balance += ( block . reward + block . total Fee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise Negative Balance Error ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance", "predictions": ["return the balance of the given address ."], "references": ["takes a single address and returns the current balance ."], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 5279, "code": "def balance over time ( address ) : forged blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged blocks = Delegate . blocks ( i . pubkey ) balance over time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged blocks : while forged blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged blocks [ block ] . reward + forged blocks [ block ] . total Fee ) balance over time . append ( Balance ( timestamp = forged blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . sender Id == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance over time . append ( res ) if tx . recipient Id == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance over time . append ( res ) if forged blocks and block <= len ( forged blocks ) - 1 : if forged blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged blocks [ block : ] : balance += ( i . reward + i . total Fee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance over time . append ( res ) return balance over time", "predictions": ["get the balance of a balance"], "references": ["returns a list of named tuples x . timestamp x . amount including block rewards"], "bleu": 0.054546736148076896, "rouge_l": 0.08840579710144927}
{"id": 5280, "code": "def real time thread ( self ) : while self . ws client . connected ( ) : if self . die : break if self . pause : sleep ( 5 ) continue message = self . ws client . receive ( ) if message is None : break message type = message [ 'type' ] if message type == 'error' : continue if message [ 'sequence' ] <= self . sequence : continue if message type == 'open' : self . handle open ( message ) elif message type == 'match' : self . handle match ( message ) elif message type == 'done' : self . handle done ( message ) elif message type == 'change' : self . handle change ( message ) else : continue self . ws client . disconnect ( )", "predictions": ["process messages received from the socket j j j j j j j j j j j j j j j j j j j to the message j j j"], "references": ["handles real - time updates to the order book ."], "bleu": 0.0513487742994337, "rouge_l": 0.10748898678414096}
{"id": 5281, "code": "def keep alive thread ( self ) : while True : with self . lock : if self . connected ( ) : self . ws . ping ( ) else : self . disconnect ( ) self . thread = None return sleep ( 30 )", "predictions": ["get a translated data data data data data data from the event loop } } } } } }"], "references": ["used exclusively as a thread which keeps the websocket alive ."], "bleu": 0.0712695567709093, "rouge_l": 0.14006888633754305}
{"id": 5282, "code": "def connect ( self ) : if not self . connected ( ) : self . ws = create connection ( self . WS URI ) message = { 'type' : self . WS TYPE , 'product id' : self . WS PRODUCT ID } self . ws . send ( dumps ( message ) ) with self . lock : if not self . thread : thread = Thread ( target = self . keep alive thread , args = [ ] ) thread . start ( )", "predictions": ["get websocket connection to server"], "references": ["connects and subscribes to the websocket feed ."], "bleu": 0.1658165975077607, "rouge_l": 0.1476997578692494}
{"id": 5283, "code": "def handle data ( self , data ) : if data . strip ( ) : data = djeffify string ( data ) self . djhtml += data", "predictions": ["handles json messages ."], "references": ["djeffify data between tags"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 5284, "code": "def wrap color ( self , code , text , format = None , style = None ) : color = None if code [ : 3 ] == self . bg . PREFIX : color = self . bg . COLORS . get ( code , None ) if not color : color = self . fg . COLORS . get ( code , None ) if not color : raise Exception ( 'Color code not found' ) if format and format not in self . formats : raise Exception ( 'Color format not found' ) fmt = \"0;\" if format == 'bold' : fmt = \"1;\" elif format == 'underline' : fmt = \"4;\" parts = color . split ( '[' ) color = '{0}[{1}{2}' . format ( parts [ 0 ] , fmt , parts [ 1 ] ) if self . has colors and self . colors enabled : st = '' if style : st = self . st . COLORS . get ( style , '' ) return \"{0}{1}{2}{3}\" . format ( st , color , text , self . st . COLORS [ 'reset all' ] ) else : return text", "predictions": ["generate a html html html string for the given code . . . . . . . . . . . . . . . . . . . . ."], "references": ["colors text with code and given format"], "bleu": 0.04317900023606586, "rouge_l": 0.05939629990262901}
{"id": 5285, "code": "def collect links ( self , env = None ) : for asset in self . assets . values ( ) : if asset . has bundles ( ) : asset . collect files ( ) if env is None : env = self . config . env if env == static bundle . ENV PRODUCTION : self . minify ( emulate = True ) self . add url prefix ( )", "predictions": ["broadcast all tx tx tx ."], "references": ["return links without build files"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5286, "code": "def require ( name , field , data type ) : if not isinstance ( field , data type ) : msg = '{0} must have {1}, got: {2}' . format ( name , data type , field ) raise Assertion Error ( msg )", "predictions": ["raises an warning if the given service is not a valid name ."], "references": ["require that the named field has the right data_type"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 5287, "code": "def flush ( self ) : queue = self . queue size = queue . qsize ( ) queue . join ( ) self . log . debug ( 'successfully flushed %s items.' , size )", "predictions": ["main function to main main main task if one isn t already exist if it will be called"], "references": ["forces a flush from the internal queue to the server"], "bleu": 0.06809398432036522, "rouge_l": 0.07530864197530865}
{"id": 5288, "code": "def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )", "predictions": ["opens a file from a file - like object"], "references": ["use all decompressor possible to make the stream"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5289, "code": "def marv ( ctx , config , loglevel , logfilter , verbosity ) : if config is None : cwd = os . path . abspath ( os . path . curdir ) while cwd != os . path . sep : config = os . path . join ( cwd , 'marv.conf' ) if os . path . exists ( config ) : break cwd = os . path . dirname ( cwd ) else : config = '/etc/marv/marv.conf' if not os . path . exists ( config ) : config = None ctx . obj = config setup logging ( loglevel , verbosity , logfilter )", "predictions": [". - . your config"], "references": ["manage a marv site"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5290, "code": "def main ( global config , * * settings ) : set cache regions from settings ( settings ) config = Configurator ( settings = settings ) config . include ( 'cms' ) config . configure celery ( global config [ ' file ' ] ) return config . make wsgi app ( )", "predictions": ["this function returns a pyramid wsgi wsgi instance ."], "references": ["this function returns a pyramid wsgi application ."], "bleu": 0.6606328636027614, "rouge_l": 0.8323586744639376}
{"id": 5291, "code": "def get function ( function name ) : module , basename = str ( function name ) . rsplit ( '.' , 1 ) try : return getattr ( import ( module , fromlist = [ basename ] ) , basename ) except ( Import Error , Attribute Error ) : raise Function Not Found ( function name )", "predictions": ["init a app from its app self . ."], "references": ["given a python function name return the function it refers to ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 5292, "code": "def handle add fun ( self , function name ) : function name = function name . strip ( ) try : function = get function ( function name ) except Exception , exc : self . wfile . write ( js error ( exc ) + NEWLINE ) return if not getattr ( function , 'view decorated' , None ) : self . functions [ function name ] = ( self . function counter , function ) else : self . functions [ function name ] = ( self . function counter , function ( self . log ) ) self . function counter += 1 return True", "predictions": ["process a request routes"], "references": ["add a function to the function list in order ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 5293, "code": "def handle map doc ( self , document ) : for function in sorted ( self . functions . values ( ) , key = lambda x : x [ 0 ] ) : try : #\u00a0generator function. yield [ list ( function ( document ) ) ] except Exception , exc : yield [ ] self . log ( repr ( exc ) )", "predictions": ["process strip strip strip strip strip strip strip"], "references": ["return the mapping of a document according to the function list ."], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 5294, "code": "def handle reduce ( self , reduce function names , mapped docs ) : reduce functions = [ ] for reduce function name in reduce function names : try : reduce function = get function ( reduce function name ) if getattr ( reduce function , 'view decorated' , None ) : reduce function = reduce function ( self . log ) reduce functions . append ( reduce function ) except Exception , exc : self . log ( repr ( exc ) ) reduce functions . append ( lambda * args , * * kwargs : None ) keys , values = zip ( ( key , value ) for ( ( key , doc id ) , value ) in mapped docs ) results = [ ] for reduce function in reduce functions : try : results . append ( reduce function ( keys , values , rereduce = False ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]", "predictions": ["call the add - functions to the add functions"], "references": ["reduce several mapped documents by several reduction functions ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 5295, "code": "def handle rereduce ( self , reduce function names , values ) : reduce functions = [ ] for reduce function name in reduce function names : try : reduce function = get function ( reduce function name ) if getattr ( reduce function , 'view decorated' , None ) : reduce function = reduce function ( self . log ) reduce functions . append ( reduce function ) except Exception , exc : self . log ( repr ( exc ) ) reduce functions . append ( lambda * args , * * kwargs : None ) results = [ ] for reduce function in reduce functions : try : results . append ( reduce function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]", "predictions": ["process each reduce of the provided reduce"], "references": ["re - reduce a set of values with a list of rereduction functions ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 5296, "code": "def handle validate ( self , function name , new doc , old doc , user ctx ) : try : function = get function ( function name ) except Exception , exc : self . log ( repr ( exc ) ) return False try : return function ( new doc , old doc , user ctx ) except Exception , exc : self . log ( repr ( exc ) ) return repr ( exc )", "predictions": ["iter a function in the doc"], "references": ["validate ... this function is undocumented but still in couchdb ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 5297, "code": "def handle ( self ) : while True : try : line = self . rfile . readline ( ) try : cmd = json . loads ( line ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue else : #\u00a0Automagically get the command handler. handler = getattr ( self , 'handle ' + cmd [ 0 ] , None ) if not handler : self . wfile . write ( repr ( Command Not Found ( cmd [ 0 ] ) ) + NEWLINE ) continue return value = handler ( * cmd [ 1 : ] ) if not return value : continue self . wfile . write ( one lineify ( json . dumps ( return value ) ) + NEWLINE ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue", "predictions": ["handles the actual work of the class . . . . . . . ."], "references": ["the main function called to handle a request ."], "bleu": 0.09103526405546068, "rouge_l": 0.17453505007153075}
{"id": 5298, "code": "def log ( self , string ) : self . wfile . write ( json . dumps ( { 'log' : string } ) + NEWLINE )", "predictions": ["upload a string to the upload"], "references": ["log an event on the couchdb server ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5299, "code": "def revoke token ( self , token , callback ) : yield Task ( self . data store . remove , 'tokens' , token = token ) callback ( )", "predictions": ["next token next token ."], "references": ["revoke_token removes the access token from the data_store"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5300, "code": "def merge ordered ( ordereds : typing . Iterable [ typing . Any ] ) -> typing . Iterable [ typing . Any ] : seen set = set ( ) add seen = seen set . add return reversed ( tuple ( map ( lambda obj : add seen ( obj ) or obj , filterfalse ( seen set . contains , chain . from iterable ( map ( reversed , reversed ( ordereds ) ) ) , ) , ) ) )", "predictions": ["next item into a single order ."], "references": ["merge multiple ordered so that within - ordered order is preserved"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 5301, "code": "def main ( ) : plugin = Register ( ) if plugin . args . option == 'filenumber' : plugin . filenumber handle ( ) elif plugin . args . option == 'fileage' : plugin . fileage handle ( ) elif plugin . args . option == 'sqlserverlocks' : plugin . sqlserverlocks handle ( ) else : plugin . unknown ( \"Unknown actions.\" )", "predictions": ["request the core . . . . . . . . . . . . . . ."], "references": ["register your own mode and handle method here ."], "bleu": 0.06809398432036522, "rouge_l": 0.07881136950904392}
{"id": 5302, "code": "def filenumber handle ( self ) : self . file list = [ ] self . count = 0 status = self . ok if self . args . recursion : self . result , self . file list = self . get folder ( self . args . path ) else : self . result , self . file list = self . get file ( self . args . path ) if self . result > self . args . critical : status = self . critical elif self . result > self . args . warning : status = self . warning else : status = self . ok self . shortoutput = \"Found {0} files in {1}.\" . format ( self . result , self . args . path ) self . logger . debug ( \"file list: {}\" . format ( self . file list ) ) [ self . longoutput . append ( file data . get ( 'Name' ) ) for file data in self . file list ] self . perfdata . append ( \"{path}={result};{warn};{crit};0;\" . format ( crit = self . args . critical , warn = self . args . warning , result = self . result , path = self . args . path ) ) status ( self . output ( long output limit = None ) ) self . logger . debug ( \"Return status and exit to Nagios.\" )", "predictions": ["process a . perfdata file"], "references": ["get the number of file in the folder ."], "bleu": 0.13575914775035755, "rouge_l": 0.1358574610244989}
{"id": 5303, "code": "def get current datetime ( self ) : self . wql time = \"SELECT Local Date Time FROM Win32 Operating System\" self . current time = self . query ( self . wql time ) self . current time string = str ( self . current time [ 0 ] . get ( 'Local Date Time' ) . split ( '.' ) [ 0 ] ) self . current time format = datetime . datetime . strptime ( self . current time string , '%Y%m%d%H%M%S' ) return self . current time format", "predictions": ["quit the current and return"], "references": ["get current datetime for every file ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 5304, "code": "def get version ( relpath ) : from os . path import dirname , join if ' file ' not in globals ( ) : root = '.' else : root = dirname ( file ) for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if ' version ' in line : if '\"' in line : return line . split ( '\"' ) [ 1 ] elif \"'\" in line : return line . split ( \"'\" ) [ 1 ]", "predictions": ["get package version version"], "references": ["read version info from a file without importing it"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 5305, "code": "def Get Top Level Containing Type ( self ) : desc = self while desc . containing type is not None : desc = desc . containing type return desc", "predictions": ["get the description of this key ."], "references": ["returns the root if this is a nested type or itself if its the root ."], "bleu": 0.06106734767839363, "rouge_l": 0.2436750998668442}
{"id": 5306, "code": "def Find Method By Name ( self , name ) : for method in self . methods : if name == method . name : return method return None", "predictions": ["find a specific certificate by name ."], "references": ["searches for the specified method and returns its descriptor ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5307, "code": "def main ( ) : plugin = Register ( ) if plugin . args . option == 'sqlserverlocks' : plugin . sqlserverlocks handle ( ) else : plugin . unknown ( \"Unknown actions.\" )", "predictions": ["register the core file file file file file file file file file file file server file file file ."], "references": ["register your own mode and handle method here ."], "bleu": 0.0712695567709093, "rouge_l": 0.15269086357947434}
{"id": 5308, "code": "def Message To Json Object ( message , including default value fields ) : message descriptor = message . DESCRIPTOR full name = message descriptor . full name if Is Wrapper Message ( message descriptor ) : return Wrapper Message To Json Object ( message ) if full name in WKTJSONMETHODS : return WKTJSONMETHODS [ full name ] [ 0 ] ( message , including default value fields ) js = { } return Regular Message To Json Object ( message , js , including default value fields )", "predictions": ["converts a json message into json specification ."], "references": ["converts message to an object according to proto3 json specification ."], "bleu": 0.2270229421855783, "rouge_l": 0.511744966442953}
{"id": 5309, "code": "def Struct Message To Json Object ( message , unused including default = False ) : fields = message . fields ret = { } for key in fields : ret [ key ] = Value Message To Json Object ( fields [ key ] ) return ret", "predictions": ["converts a json specification of a schema into json specification . . . . . . . . . . . . . . . . . . . . ."], "references": ["converts struct message according to proto3 json specification ."], "bleu": 0.07678432706586173, "rouge_l": 0.22202001819836215}
{"id": 5310, "code": "def Convert Value Message ( value , message ) : if isinstance ( value , dict ) : Convert Struct Message ( value , message . struct value ) elif isinstance ( value , list ) : Convert List Value Message ( value , message . list value ) elif value is None : message . null value = 0 elif isinstance ( value , bool ) : message . bool value = value elif isinstance ( value , six . string types ) : message . string value = value elif isinstance ( value , INT OR FLOAT ) : message . number value = value else : raise Parse Error ( 'Unexpected type for Value message.' )", "predictions": ["convert a json representation of a json representation into a json representation in a json representation in a dictionary in a json representation in a json representation in a json representation"], "references": ["convert a json representation into value message ."], "bleu": 0.14291173574075164, "rouge_l": 0.2869238005644403}
{"id": 5311, "code": "def Convert List Value Message ( value , message ) : if not isinstance ( value , list ) : raise Parse Error ( 'List Value must be in [] which is {0}.' . format ( value ) ) message . Clear Field ( 'values' ) for item in value : Convert Value Message ( item , message . values . add ( ) )", "predictions": ["converts a list of extended extended extended extended extended extended message ."], "references": ["convert a json representation into listvalue message ."], "bleu": 0.14694106251955755, "rouge_l": 0.3112244897959184}
