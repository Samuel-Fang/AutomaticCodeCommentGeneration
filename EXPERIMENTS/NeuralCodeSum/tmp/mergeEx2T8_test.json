{"id": 0, "code": "def get vid from url ( url ) : return match1 ( url , r'youtu\\.be/([^?/]+)' ) or match1 ( url , r'youtube\\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\\.com/watch/([^/?]+)' ) or parse query param ( url , 'v' ) or parse query param ( parse query param ( url , 'u' ) , 'v' )", "predictions": ["extract vid vid information from ( and put internal data into the cache ."], "references": ["extracts video id from url ."], "bleu": 0.09782375748961449, "rouge_l": 0.21554770318021202}
{"id": 1, "code": "def dailymotion download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : html = get content ( rebuilt url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real url = info [ quality ] [ 1 ] [ \"url\" ] if real url : break except Key Error : pass mime , ext , size = url info ( real url ) print info ( site info , title , mime , size ) if not info only : download urls ( [ real url ] , title , ext , size , output dir = output dir , merge = merge )", "predictions": ["downloads the specified html and builds its children ."], "references": ["downloads dailymotion videos by url ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2, "code": "def sina download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina zxt ( url , output dir = output dir , merge = merge , info only = info only , * * kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video page = get content ( url ) vid = hd vid = match1 ( video page , r'hd vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd vid == '0' : vids = match1 ( video page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video page , r'vid:\"?(\\d+)\"?' ) if vid : #title = match1(video page, r'title\\s*:\\s*\\'([^\\']+)\\'') sina download by vid ( vid , output dir = output dir , merge = merge , info only = info only ) else : vkey = match1 ( video page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina download by vid ( vid , output dir = output dir , merge = merge , info only = info only ) return title = match1 ( video page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina download by vkey ( vkey , title = title , output dir = output dir , merge = merge , info only = info only )", "predictions": ["download and download the sina sina directory ."], "references": ["downloads sina videos by url ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 3, "code": "def get vid from url ( self , url ) : hit = re . search ( r'live.qq.com/(\\d+)' , url ) if hit is not None : return hit . group ( 1 ) hit = re . search ( r'live.qq.com/directory/match/(\\d+)' , url ) if hit is not None : return self . get room id from url ( hit . group ( 1 ) ) html = get content ( url ) room id = match1 ( html , r'room id\\\":(\\d+)' ) if room id is None : log . wtf ( 'Unknown page {}' . format ( url ) ) return room id", "predictions": ["creates the vid room from the specified url ."], "references": ["extracts video id from live . qq . com ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 4, "code": "def sprint ( text , * colors ) : return \"\\33[{}m{content}\\33[{}m\" . format ( \";\" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS ANSI TERMINAL and colors else text", "predictions": ["add text to the rate ."], "references": ["format text with color or other effects into ansi escaped string ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 5, "code": "def print log ( text , * colors ) : sys . stderr . write ( sprint ( \"{}: {}\" . format ( script name , text ) , * colors ) + \"\\n\" )", "predictions": ["prints an log message to the output stream ."], "references": ["print a log message to standard error ."], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 6, "code": "def e ( message , exit code = None ) : print log ( message , YELLOW , BOLD ) if exit code is not None : sys . exit ( exit code )", "predictions": ["prints a stress usage ."], "references": ["print an error log message ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 7, "code": "def wtf ( message , exit code = 1 ) : print log ( message , RED , BOLD ) if exit code is not None : sys . exit ( exit code )", "predictions": ["what a terrible . was started ."], "references": ["what a terrible failure!"], "bleu": 0.345720784641941, "rouge_l": 0.5736677115987461}
{"id": 8, "code": "def detect os ( ) : syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os", "predictions": ["detects whether the user is an , ."], "references": ["detect operating system ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 9, "code": "def get vid from url ( url ) : vid = match1 ( url , 'https?://www.mgtv.com/(?:b|l)/\\d+/(\\d+).html' ) if not vid : vid = match1 ( url , 'https?://www.mgtv.com/hz/bdpz/\\d+/(\\d+).html' ) return vid", "predictions": ["gets the vid vid url from the given url ."], "references": ["extracts video id from url ."], "bleu": 0.17827531042796255, "rouge_l": 0.3927038626609442}
{"id": 10, "code": "def legitimize ( text , os = detect os ( ) ) : text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) if os == 'windows' or os == 'cygwin' or os == 'wsl' : text = text . translate ( { ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) else : if os == 'mac' : text = text . translate ( { ord ( ':' ) : '-' , } ) if text . startswith ( \".\" ) : text = text [ 1 : ] text = text [ : 80 ] return text", "predictions": ["decodes the given text to the passed bytebuffer ."], "references": ["converts a string to a valid filename ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 11, "code": "def cbs download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : html = get content ( url ) pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html , r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"' ) theplatform download by pid ( pid , title , output dir = output dir , merge = merge , info only = info only )", "predictions": ["downloads the cbs . will block until the cbs is complete ."], "references": ["downloads cbs videos by url ."], "bleu": 0.1235622127262679, "rouge_l": 0.3546511627906977}
{"id": 12, "code": "def parse host ( host ) : if re . match ( r'^(\\d+)$' , host ) is not None : return ( \"0.0.0.0\" , int ( host ) ) if re . match ( r'^(\\w+)://' , host ) is None : host = \"//\" + host o = parse . urlparse ( host ) hostname = o . hostname or \"0.0.0.0\" port = o . port or 0 return ( hostname , port )", "predictions": ["parse and return the fully qualified name ."], "references": ["parses host name and port number from a string ."], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 13, "code": "def get conn ( self ) : conn config = self . get conn params ( ) conn = snowflake . connector . connect ( * * conn config ) return conn", "predictions": ["for this object . this is a bit done by the connection manager ."], "references": ["returns a snowflake . connection object"], "bleu": 0.11114924776032006, "rouge_l": 0.21554770318021202}
{"id": 14, "code": "def bulk load ( self , table , tmp file ) : self . copy expert ( \"COPY {table} FROM STDIN\" . format ( table = table ) , tmp file )", "predictions": ["loads a table from the given table ."], "references": ["loads a tab - delimited file into a database table"], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 15, "code": "def bulk dump ( self , table , tmp file ) : self . copy expert ( \"COPY {table} TO STDOUT\" . format ( table = table ) , tmp file )", "predictions": ["copies the given table to a file ."], "references": ["dumps a database table into a tab - delimited file"], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 16, "code": "def execute ( self , context ) : hook = Google Cloud Storage Hook ( google cloud storage conn id = self . google cloud storage conn id , delegate to = self . delegate to ) hook . upload ( bucket name = self . bucket , object name = self . dst , mime type = self . mime type , filename = self . src , gzip = self . gzip , )", "predictions": ["executes a machine from the underlying image and store the image in the database ."], "references": ["uploads the file to google cloud storage"], "bleu": 0.08225964699966554, "rouge_l": 0.09728867623604465}
{"id": 17, "code": "def get conn ( self ) : conn = self . get connection ( self . mysql conn id ) conn config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } if not conn . port : conn config [ \"port\" ] = 3306 else : conn config [ \"port\" ] = int ( conn . port ) if conn . extra dejson . get ( 'charset' , False ) : conn config [ \"charset\" ] = conn . extra dejson [ \"charset\" ] if ( conn config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn config [ \"charset\" ] ) . lower ( ) == 'utf-8' : conn config [ \"use unicode\" ] = True if conn . extra dejson . get ( 'cursor' , False ) : if ( conn . extra dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : conn config [ \"cursorclass\" ] = My SQ Ldb . cursors . SS Cursor elif ( conn . extra dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : conn config [ \"cursorclass\" ] = My SQ Ldb . cursors . Dict Cursor elif ( conn . extra dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : conn config [ \"cursorclass\" ] = My SQ Ldb . cursors . SS Dict Cursor local infile = conn . extra dejson . get ( 'local infile' , False ) if conn . extra dejson . get ( 'ssl' , False ) : dejson ssl = conn . extra dejson [ 'ssl' ] if isinstance ( dejson ssl , six . string types ) : dejson ssl = json . loads ( dejson ssl ) conn config [ 'ssl' ] = dejson ssl if conn . extra dejson . get ( 'unix socket' ) : conn config [ 'unix socket' ] = conn . extra dejson [ 'unix socket' ] if local infile : conn config [ \"local infile\" ] = 1 conn = My SQ Ldb . connect ( * * conn config ) return conn", "predictions": ["return a connection to the login ."], "references": ["returns a mysql connection object"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 18, "code": "def bulk load ( self , table , tmp file ) : conn = self . get conn ( ) cur = conn . cursor ( ) cur . execute ( . format ( tmp file = tmp file , table = table ) ) conn . commit ( )", "predictions": ["using the specified table ."], "references": ["loads a tab - delimited file into a database table"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 19, "code": "def get proxy version ( self ) : self . download sql proxy if needed ( ) command to run = [ self . sql proxy path ] command to run . extend ( [ '--version' ] ) command to run . extend ( self . get credential parameters ( ) ) result = subprocess . check output ( command to run ) . decode ( 'utf-8' ) pattern = re . compile ( \"^.*[V|v]ersion ([^;]*);.*$\" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None", "predictions": ["get the tracks of this object , using the specified version ."], "references": ["returns version of the cloud sql proxy ."], "bleu": 0.13065113298388567, "rouge_l": 0.3112244897959184}
{"id": 20, "code": "def cleanup database hook ( self ) : if self . database type == 'postgres' : if hasattr ( self . db hook , 'conn' ) and self . db hook . conn and self . db hook . conn . notices : for output in self . db hook . conn . notices : self . log . info ( output )", "predictions": ["cleans up any ( marker and the ( can be called after the database has been made ."], "references": ["clean up database hook after it was used ."], "bleu": 0.08562365224473284, "rouge_l": 0.2364341085271318}
{"id": 21, "code": "def reserve free tcp port ( self ) : self . reserved tcp socket = socket . socket ( socket . AF INET , socket . SOCK STREAM ) self . reserved tcp socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql proxy tcp port = self . reserved tcp socket . getsockname ( ) [ 1 ]", "predictions": ["sets up the tcp lock for this tcp ."], "references": ["reserve free tcp port to be used by cloud sql proxy"], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 22, "code": "def get error code ( self , e ) : try : matches = self . error code pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except Value Error : return e", "predictions": ["return compiled error code by the specific pattern ."], "references": ["extract error code from ftp exception"], "bleu": 0.18575057999133596, "rouge_l": 0.27664399092970515}
{"id": 23, "code": "def integrate plugins ( ) : import sys from airflow . plugins manager import sensors modules for sensors module in sensors modules : sys . modules [ sensors module . name ] = sensors module globals ( ) [ sensors module . name ] = sensors module", "predictions": ["integrate plugins . this method does not actually integrate the list of plugins ."], "references": ["integrate plugins to the context"], "bleu": 0.1250076305588977, "rouge_l": 0.34528301886792445}
{"id": 24, "code": "def clear dag runs ( ) : session = settings . Session ( ) drs = session . query ( Dag Run ) . filter ( Dag Run . dag id . in ( DAG IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting Dag Run :: {}' . format ( dr ) ) session . delete ( dr )", "predictions": ["clear any existing runs ."], "references": ["remove any existing dag runs for the perf test dags ."], "bleu": 0.12296202211076697, "rouge_l": 0.4683301343570058}
{"id": 25, "code": "def clear dag task instances ( ) : session = settings . Session ( ) TI = Task Instance tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting Task Instance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )", "predictions": ["clear the set of ti instances for some test case ."], "references": ["remove any existing task instances for the perf test dags ."], "bleu": 0.17827531042796255, "rouge_l": 0.36363636363636365}
{"id": 26, "code": "def set dags paused state ( is paused ) : session = settings . Session ( ) dms = session . query ( Dag Model ) . filter ( Dag Model . dag id . in ( DAG IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is paused={}' . format ( dm , is paused ) ) dm . is paused = is paused session . commit ( )", "predictions": ["instantiates the state of the ( ."], "references": ["toggle the pause state of the dags in the test ."], "bleu": 0.21606281467072083, "rouge_l": 0.5341506129597198}
{"id": 27, "code": "def print stats ( self ) : session = settings . Session ( ) TI = Task Instance tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . all ( ) ) successful tis = [ x for x in tis if x . state == State . SUCCESS ] ti perf = [ ( ti . dag id , ti . task id , ti . execution date , ( ti . queued dttm - self . start date ) . total seconds ( ) , ( ti . start date - self . start date ) . total seconds ( ) , ( ti . end date - self . start date ) . total seconds ( ) , ti . duration ) for ti in successful tis ] ti perf df = pd . Data Frame ( ti perf , columns = [ 'dag id' , 'task id' , 'execution date' , 'queue delay' , 'start delay' , 'land time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag id in DAG IDS : print ( 'DAG {}' . format ( dag id ) ) print ( ti perf df [ ti perf df [ 'dag id' ] == dag id ] ) print ( '###################' ) if len ( tis ) > len ( successful tis ) : print ( \"WARNING!! The following task instances haven't completed\" ) print ( pd . Data Frame ( [ ( ti . dag id , ti . task id , ti . execution date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag id' , 'task id' , 'execution date' , 'state' ] ) ) session . commit ( )", "predictions": ["prints the following dag ."], "references": ["print operational metrics for the scheduler test ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 28, "code": "def heartbeat ( self ) : super ( Scheduler Metrics Job , self ) . heartbeat ( ) session = settings . Session ( ) TI = Task Instance successful tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . filter ( TI . state . in ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = Dag Bag ( SUBDIR ) dags = [ dagbag . dags [ dag id ] for dag id in DAG IDS ] num task instances = sum ( [ ( timezone . utcnow ( ) - task . start date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful tis ) == num task instances or ( timezone . utcnow ( ) - self . start date ) . total seconds ( ) > MAX RUNTIME SECS ) : if len ( successful tis ) == num task instances : self . log . info ( \"All tasks processed! Printing stats.\" ) else : self . log . info ( \"Test timeout reached. Printing available stats.\" ) self . print stats ( ) set dags paused state ( True ) sys . exit ( )", "predictions": ["creates a . for the specified dag ."], "references": ["override the scheduler heartbeat to determine when the test is complete"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 29, "code": "def get dag run state ( dag id , execution date ) : dagbag = Dag Bag ( ) if dag id not in dagbag . dags : error message = \"Dag id {} not found\" . format ( dag id ) raise Dag Not Found ( error message ) dag = dagbag . get dag ( dag id ) dagrun = dag . get dagrun ( execution date = execution date ) if not dagrun : error message = ( 'Dag Run for date {} not found in dag {}' . format ( execution date , dag id ) ) raise Dag Run Not Found ( error message ) return { 'state' : dagrun . get state ( ) }", "predictions": ["runs the dag ( formatted ) against this dag ."], "references": ["return the task object identified by the given dag_id and task_id ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 30, "code": "def get conn ( self ) : conn = self . get connection ( self . druid broker conn id ) druid broker conn = connect ( host = conn . host , port = conn . port , path = conn . extra dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid broker conn", "predictions": ["get a conn object with the connection ."], "references": ["establish a connection to druid broker ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 31, "code": "def create session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( )", "predictions": ["create the session object ."], "references": ["contextmanager that will create and teardown a session ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 32, "code": "def resetdb ( ) : from airflow import models from alembic . migration import Migration Context log . info ( \"Dropping tables that exist\" ) models . base . Base . metadata . drop all ( settings . engine ) mc = Migration Context . configure ( settings . engine ) if mc . version . exists ( settings . engine ) : mc . version . drop ( settings . engine ) from flask appbuilder . models . sqla import Base Base . metadata . drop all ( settings . engine ) initdb ( )", "predictions": ["makes the get and version from all the query query classes ."], "references": ["clear out the database"], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 33, "code": "def execute ( self , context ) : hook = Wasb Hook ( wasb conn id = self . wasb conn id ) self . log . info ( 'Uploading %s to wasb://%s ' 'as %s' . format ( self . file path , self . container name , self . blob name ) ) hook . load file ( self . file path , self . container name , self . blob name , * * self . load options )", "predictions": ["dailymotion operation to dailymotion itself on this object ."], "references": ["upload a file to azure blob storage ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 34, "code": "def get conn ( self ) : db = self . get connection ( self . presto conn id ) reqkwargs = None if db . password is not None : reqkwargs = { 'auth' : HTTP Basic Auth ( db . login , db . password ) } return presto . connect ( host = db . host , port = db . port , username = db . login , source = db . extra dejson . get ( 'source' , 'airflow' ) , protocol = db . extra dejson . get ( 'protocol' , 'http' ) , catalog = db . extra dejson . get ( 'catalog' , 'hive' ) , requests kwargs = reqkwargs , schema = db . schema )", "predictions": ["sina to a database ."], "references": ["returns a connection object"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 35, "code": "def get pretty exception message ( e ) : if ( hasattr ( e , 'message' ) and 'error Name' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'error Name' ] , message = e . message [ 'message' ] ) ) else : return str ( e )", "predictions": ["extracts an from from the from the template ."], "references": ["parses some databaseerror to provide a better error message"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 36, "code": "def get records ( self , hql , parameters = None ) : try : return super ( ) . get records ( self . strip sql ( hql ) , parameters ) except Database Error as e : raise Presto Exception ( self . get pretty exception message ( e ) )", "predictions": ["calculate derived ( or non - negative ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["get a set of records from presto"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 37, "code": "def get pandas df ( self , hql , parameters = None ) : import pandas cursor = self . get cursor ( ) try : cursor . execute ( self . strip sql ( hql ) , parameters ) data = cursor . fetchall ( ) except Database Error as e : raise Presto Exception ( self . get pretty exception message ( e ) ) column descriptions = cursor . description if data : df = pandas . Data Frame ( data ) df . columns = [ c [ 0 ] for c in column descriptions ] else : df = pandas . Data Frame ( ) return df", "predictions": ["extracts the ui from the given angle ."], "references": ["get a pandas dataframe from a sql query ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 38, "code": "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . strip sql ( hql ) , parameters )", "predictions": ["run the given operator ."], "references": ["execute the statement against presto . can be used to create views ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 39, "code": "def get conn ( self ) : if self . cosmos client is not None : return self . cosmos client self . cosmos client = cosmos client . Cosmos Client ( self . endpoint uri , { 'master Key' : self . master key } ) return self . cosmos client", "predictions": ["this is a quiet method for use in ( ."], "references": ["return a cosmos db client ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 40, "code": "def does collection exist ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( \"Collection name cannot be None.\" ) existing container = list ( self . get conn ( ) . Query Containers ( get database link ( self . get database name ( database name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection name } ] } ) ) if len ( existing container ) == 0 : return False return True", "predictions": ["detect if the specified os exists ."], "references": ["checks if a collection exists in cosmosdb ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 41, "code": "def create collection ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( \"Collection name cannot be None.\" ) existing container = list ( self . get conn ( ) . Query Containers ( get database link ( self . get database name ( database name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection name } ] } ) ) if len ( existing container ) == 0 : self . get conn ( ) . Create Container ( get database link ( self . get database name ( database name ) ) , { \"id\" : collection name } )", "predictions": ["get a vid container ."], "references": ["creates a new collection in the cosmosdb database ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 42, "code": "def does database exist ( self , database name ) : if database name is None : raise Airflow Bad Request ( \"Database name cannot be None.\" ) existing database = list ( self . get conn ( ) . Query Databases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database name } ] } ) ) if len ( existing database ) == 0 : return False return True", "predictions": ["loop a ( without a ( without a ( without a pool ) ."], "references": ["checks if a database exists in cosmosdb ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 43, "code": "def create database ( self , database name ) : if database name is None : raise Airflow Bad Request ( \"Database name cannot be None.\" ) existing database = list ( self . get conn ( ) . Query Databases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database name } ] } ) ) if len ( existing database ) == 0 : self . get conn ( ) . Create Database ( { \"id\" : database name } )", "predictions": ["cbs method that creates a download or download ."], "references": ["creates a new database in cosmosdb ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 44, "code": "def delete database ( self , database name ) : if database name is None : raise Airflow Bad Request ( \"Database name cannot be None.\" ) self . get conn ( ) . Delete Database ( get database link ( database name ) )", "predictions": ["creates a host to save the database ."], "references": ["deletes an existing database in cosmosdb ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 45, "code": "def delete collection ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( \"Collection name cannot be None.\" ) self . get conn ( ) . Delete Container ( get collection link ( self . get database name ( database name ) , collection name ) )", "predictions": ["creates a conn object ."], "references": ["deletes an existing collection in the cosmosdb database ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 46, "code": "def insert documents ( self , documents , database name = None , collection name = None ) : if documents is None : raise Airflow Bad Request ( \"You cannot insert empty documents\" ) created documents = [ ] for single document in documents : created documents . append ( self . get conn ( ) . Create Item ( get collection link ( self . get database name ( database name ) , self . get collection name ( collection name ) ) , single document ) ) return created documents", "predictions": ["inserts a : 1 ."], "references": ["insert a list of new documents into an existing collection in the cosmosdb database ."], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 47, "code": "def delete document ( self , document id , database name = None , collection name = None ) : if document id is None : raise Airflow Bad Request ( \"Cannot delete a document without an id\" ) self . get conn ( ) . Delete Item ( get document link ( self . get database name ( database name ) , self . get collection name ( collection name ) , document id ) )", "predictions": ["deletes a dump dump ."], "references": ["delete an existing document out of a collection in the cosmosdb database ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 48, "code": "def get document ( self , document id , database name = None , collection name = None ) : if document id is None : raise Airflow Bad Request ( \"Cannot get a document without an id\" ) try : return self . get conn ( ) . Read Item ( get document link ( self . get database name ( database name ) , self . get collection name ( collection name ) , document id ) ) except HTTP Failure : return None", "predictions": ["execute a ( or gzip ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["get a document from an existing collection in the cosmosdb database ."], "bleu": 0.02614431568998955, "rouge_l": 0.038558786346396964}
{"id": 49, "code": "def get documents ( self , sql string , database name = None , collection name = None , partition key = None ) : if sql string is None : raise Airflow Bad Request ( \"SQL query string cannot be None\" ) query = { 'query' : sql string } try : result iterable = self . get conn ( ) . Query Items ( get collection link ( self . get database name ( database name ) , self . get collection name ( collection name ) ) , query , partition key ) return list ( result iterable ) except HTTP Failure : return None", "predictions": ["returns a list for the given = \"user\" , \"user\" or { ( , . , . , . , . , . , . , . , . , . , . , . , ( , . , . , . , . , . , . ,"], "references": ["get a list of documents from an existing collection in the cosmosdb database via sql query ."], "bleu": 0.0359340051359579, "rouge_l": 0.1310418904403867}
{"id": 50, "code": "def get code ( dag id ) : session = settings . Session ( ) DM = models . Dag Model dag = session . query ( DM ) . filter ( DM . dag id == dag id ) . first ( ) session . close ( ) if dag is None : error message = \"Dag id {} not found\" . format ( dag id ) raise Dag Not Found ( error message ) try : with wwwutils . open maybe zipped ( dag . fileloc , 'r' ) as f : code = f . read ( ) return code except IO Error as e : error message = \"Error {} while reading Dag id {} Code\" . format ( str ( e ) , dag id ) raise Airflow Exception ( error message )", "predictions": ["retrieves the load from the self ."], "references": ["return python code of a given dag_id ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 51, "code": "def get conn ( self ) : conn = self . get connection ( self . vertica conn id ) conn config = { \"user\" : conn . login , \"password\" : conn . password or '' , \"database\" : conn . schema , \"host\" : conn . host or 'localhost' } if not conn . port : conn config [ \"port\" ] = 5433 else : conn config [ \"port\" ] = int ( conn . port ) conn = connect ( * * conn config ) return conn", "predictions": ["return a proxy object for the passed download ."], "references": ["returns verticaql connection object"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 52, "code": "def flush ( self ) : if len ( self . buffer ) > 0 : self . logger . log ( self . level , self . buffer ) self . buffer = str ( )", "predictions": ["flushes the output stream for writing to disk ."], "references": ["ensure all logging output has been flushed"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 53, "code": "def start ( self ) : self . process = self . launch process ( self . dag directory , self . file paths , self . max runs , self . processor factory , self . child signal conn , self . stat queue , self . result queue , self . async mode ) self . log . info ( \"Launched Dag File Processor Manager with pid: %s\" , self . process . pid )", "predictions": ["reserve the worker process ."], "references": ["launch dagfileprocessormanager processor and start dag parsing loop in manager ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 54, "code": "def exit gracefully ( self , signum , frame ) : self . log . info ( \"Exiting gracefully upon receiving signal %s\" , signum ) self . terminate ( ) self . end ( ) self . log . debug ( \"Finished terminating DAG processors.\" ) sys . exit ( os . EX OK )", "predictions": ["get a running process ."], "references": ["helper method to clean up dag file processors to avoid leaving orphan processes ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 55, "code": "def start in async ( self ) : while True : loop start time = time . time ( ) if self . signal conn . poll ( ) : agent signal = self . signal conn . recv ( ) if agent signal == Dag Parsing Signal . TERMINATE MANAGER : self . terminate ( ) break elif agent signal == Dag Parsing Signal . END MANAGER : self . end ( ) sys . exit ( os . EX OK ) self . refresh dag dir ( ) simple dags = self . heartbeat ( ) for simple dag in simple dags : self . result queue . put ( simple dag ) self . print stat ( ) all files processed = all ( self . get last finish time ( x ) is not None for x in self . file paths ) max runs reached = self . max runs reached ( ) dag parsing stat = Dag Parsing Stat ( self . file paths , self . get all pids ( ) , max runs reached , all files processed , len ( simple dags ) ) self . stat queue . put ( dag parsing stat ) if max runs reached : self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . max runs ) break loop duration = time . time ( ) - loop start time if loop duration < 1 : sleep length = 1 - loop duration self . log . debug ( \"Sleeping for %.2f seconds to prevent excessive logging\" , sleep length ) time . sleep ( sleep length )", "predictions": ["integrate all the program to integrate them into the parallel sequence ."], "references": ["parse dag files repeatedly in a standalone loop ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 56, "code": "def refresh dag dir ( self ) : elapsed time since refresh = ( timezone . utcnow ( ) - self . last dag dir refresh time ) . total seconds ( ) if elapsed time since refresh > self . dag dir list interval : self . log . info ( \"Searching for files in %s\" , self . dag directory ) self . file paths = list py file paths ( self . dag directory ) self . last dag dir refresh time = timezone . utcnow ( ) self . log . info ( \"There are %s files in %s\" , len ( self . file paths ) , self . dag directory ) self . set file paths ( self . file paths ) try : self . log . debug ( \"Removing old import errors\" ) self . clear nonexistent import errors ( ) except Exception : self . log . exception ( \"Error removing old import errors\" )", "predictions": ["look for stats of stats ."], "references": ["refresh file paths from dag dir if we haven t done it for too long ."], "bleu": 0.046172815301777345, "rouge_l": 0.1680440771349862}
{"id": 57, "code": "def print stat ( self ) : if ( ( timezone . utcnow ( ) - self . last stat print time ) . total seconds ( ) > self . print stats interval ) : if len ( self . file paths ) > 0 : self . log file processing stats ( self . file paths ) self . last stat print time = timezone . utcnow ( )", "predictions": ["prints a garbage collected at the current time ."], "references": ["occasionally print out stats about how fast the files are getting processed"], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 58, "code": "def wait until finished ( self ) : for file path , processor in self . processors . items ( ) : while not processor . done : time . sleep ( 0.1 )", "predictions": ["set up all reports to finish ."], "references": ["sleeps until all the processors are done ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 59, "code": "def open slots ( self , session ) : from airflow . models . taskinstance import Task Instance as TI used slots = session . query ( func . count ( ) ) . filter ( TI . pool == self . pool ) . filter ( TI . state . in ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) return self . slots - used slots", "predictions": ["print out a new log message about this : or stats ."], "references": ["returns the number of slots open at the moment"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 60, "code": "def run command ( command ) : process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close fds = True ) output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] if process . returncode != 0 : raise Airflow Config Exception ( \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\" . format ( command , process . returncode , output , stderr ) ) return output", "predictions": ["runs the command . this ensures that the ( ) can be called first ."], "references": ["runs command and returns stdout"], "bleu": 0.09103526405546068, "rouge_l": 0.21981981981981977}
{"id": 61, "code": "def get task ( dag id , task id ) : dagbag = Dag Bag ( ) if dag id not in dagbag . dags : error message = \"Dag id {} not found\" . format ( dag id ) raise Dag Not Found ( error message ) dag = dagbag . get dag ( dag id ) if not dag . has task ( task id ) : error message = 'Task {} not found in dag {}' . format ( task id , dag id ) raise Task Not Found ( error message ) return dag . get task ( task id )", "predictions": ["get dag in specified state ."], "references": ["return the task object identified by the given dag_id and task_id ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 62, "code": "def dispose orm ( ) : log . debug ( \"Disposing DB connection pool (PID %s)\" , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . dispose ( ) engine = None", "predictions": ["get the . for the ( ."], "references": ["properly close pooled database connections"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 63, "code": "def prepare classpath ( ) : if DAGS FOLDER not in sys . path : sys . path . append ( DAGS FOLDER ) config path = os . path . join ( AIRFLOW HOME , 'config' ) if config path not in sys . path : sys . path . append ( config path ) if PLUGINS FOLDER not in sys . path : sys . path . append ( PLUGINS FOLDER )", "predictions": ["create and get all jars that are named in test configuration ."], "references": ["ensures that certain subfolders of airflow_home are on the classpath"], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 64, "code": "def alchemy to dict ( obj ) : if not obj : return None d = { } for c in obj . table . columns : value = getattr ( obj , c . name ) if type ( value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d", "predictions": ["convert an object into a mary ."], "references": ["transforms a sqlalchemy model instance into a dictionary"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 65, "code": "def chunks ( items , chunk size ) : if chunk size <= 0 : raise Value Error ( 'Chunk size must be a positive integer' ) for i in range ( 0 , len ( items ) , chunk size ) : yield items [ i : i + chunk size ]", "predictions": ["yield successive n-sized chunks from the given chunk of the given items ."], "references": ["yield successive chunks of a given size from a list of items"], "bleu": 0.16064553745630614, "rouge_l": 0.4834874504623514}
{"id": 66, "code": "def get task instance ( dag id , task id , execution date ) : dagbag = Dag Bag ( ) if dag id not in dagbag . dags : error message = \"Dag id {} not found\" . format ( dag id ) raise Dag Not Found ( error message ) dag = dagbag . get dag ( dag id ) if not dag . has task ( task id ) : error message = 'Task {} not found in dag {}' . format ( task id , dag id ) raise Task Not Found ( error message ) dagrun = dag . get dagrun ( execution date = execution date ) if not dagrun : error message = ( 'Dag Run for date {} not found in dag {}' . format ( execution date , dag id ) ) raise Dag Run Not Found ( error message ) task instance = dagrun . get task instance ( task id ) if not task instance : error message = ( 'Task {} instance for date {} not found' . format ( task id , execution date ) ) raise Task Instance Not Found ( error message ) return task instance", "predictions": ["get the task instance for a given dag ."], "references": ["return the task object identified by the given dag_id and task_id ."], "bleu": 0.15122637383061946, "rouge_l": 0.3713850837138508}
{"id": 67, "code": "def integrate plugins ( ) : import sys from airflow . plugins manager import operators modules for operators module in operators modules : sys . modules [ operators module . name ] = operators module globals ( ) [ operators module . name ] = operators module", "predictions": ["integrate plugins ."], "references": ["integrate plugins to the context"], "bleu": 0.36304072644520663, "rouge_l": 0.47843137254901963}
{"id": 68, "code": "def get conn ( self ) : http authorized = self . authorize ( ) return build ( 'dataproc' , self . api version , http = http authorized , cache discovery = False )", "predictions": ["get the http object to be collected ."], "references": ["returns a google cloud dataproc service object ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 69, "code": "def wait ( self , operation ) : submitted = Data Proc Operation ( self . get conn ( ) , operation , self . num retries ) submitted . wait for done ( )", "predictions": ["blocks until the operation is completed ."], "references": ["awaits for google cloud dataproc operation to complete ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 70, "code": "def get conn ( self ) : authed http = self . authorize ( ) return build ( 'ml' , 'v1' , http = authed http , cache discovery = False )", "predictions": ["method to get the http object ."], "references": ["returns a google mlengine service object ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 71, "code": "def set default version ( self , project id , model name , version name ) : full version name = 'projects/{}/models/{}/versions/{}' . format ( project id , model name , version name ) request = self . mlengine . projects ( ) . models ( ) . versions ( ) . set Default ( name = full version name , body = { } ) try : response = request . execute ( ) self . log . info ( 'Successfully set version: %s to default' , response ) return response except Http Error as e : self . log . error ( 'Something went wrong: %s' , e ) raise", "predictions": ["set the default version of the ( ."], "references": ["sets a version to be the default . blocks until finished ."], "bleu": 0.14544785215055717, "rouge_l": 0.28955696202531644}
{"id": 72, "code": "def list versions ( self , project id , model name ) : result = [ ] full parent name = 'projects/{}/models/{}' . format ( project id , model name ) request = self . mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full parent name , page Size = 100 ) response = request . execute ( ) next page token = response . get ( 'next Page Token' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) while next page token is not None : next request = self . mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full parent name , page Token = next page token , page Size = 100 ) response = next request . execute ( ) next page token = response . get ( 'next Page Token' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) time . sleep ( 5 ) return result", "predictions": ["list all versions of the project ."], "references": ["lists all available versions of a model . blocks until finished ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 73, "code": "def delete version ( self , project id , model name , version name ) : full name = 'projects/{}/models/{}/versions/{}' . format ( project id , model name , version name ) delete request = self . mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full name ) response = delete request . execute ( ) get request = self . mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return poll with exponential delay ( request = get request , max n = 9 , is done func = lambda resp : resp . get ( 'done' , False ) , is error func = lambda resp : resp . get ( 'error' , None ) is not None )", "predictions": ["deletes an existing version ."], "references": ["deletes the given version of a model . blocks until finished ."], "bleu": 0.08006212224540951, "rouge_l": 0.3285457809694794}
{"id": 74, "code": "def create model ( self , project id , model ) : if not model [ 'name' ] : raise Value Error ( \"Model name must be provided and \" \"could not be an empty string\" ) project = 'projects/{}' . format ( project id ) request = self . mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )", "predictions": ["creates a class and returns its ( ."], "references": ["create a model . blocks until finished ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 75, "code": "def get model ( self , project id , model name ) : if not model name : raise Value Error ( \"Model name must be provided and \" \"it could not be an empty string\" ) full model name = 'projects/{}/models/{}' . format ( project id , model name ) request = self . mlengine . projects ( ) . models ( ) . get ( name = full model name ) try : return request . execute ( ) except Http Error as e : if e . resp . status == 404 : self . log . error ( 'Model was not found: %s' , e ) return None raise", "predictions": ["get a model by name ."], "references": ["gets a model . blocks until finished ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 76, "code": "def write batch data ( self , items ) : dynamodb conn = self . get conn ( ) try : table = dynamodb conn . Table ( self . table name ) with table . batch writer ( overwrite by pkeys = self . table keys ) as batch : for item in items : batch . put item ( Item = item ) return True except Exception as general error : raise Airflow Exception ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general error ) ) )", "predictions": ["write to the database ."], "references": ["write batch items to dynamodb table with provisioned throughout capacity ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 77, "code": "def integrate plugins ( ) : from airflow . plugins manager import executors modules for executors module in executors modules : sys . modules [ executors module . name ] = executors module globals ( ) [ executors module . name ] = executors module", "predictions": ["integrate plugins to the context ."], "references": ["integrate plugins to the context ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 78, "code": "def get default executor ( ) : global DEFAULT EXECUTOR if DEFAULT EXECUTOR is not None : return DEFAULT EXECUTOR executor name = configuration . conf . get ( 'core' , 'EXECUTOR' ) DEFAULT EXECUTOR = get executor ( executor name ) log = Logging Mixin ( ) . log log . info ( \"Using executor %s\" , executor name ) return DEFAULT EXECUTOR", "predictions": ["returns the default executor ."], "references": ["creates a new instance of the configured executor if none exists and returns it"], "bleu": 0.053667245469253895, "rouge_l": 0.19395866454689983}
{"id": 79, "code": "def on error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment error} with ' 'items: {with items}' . format ( segment error = error , with items = items ) ) raise Airflow Exception ( 'Segment error: {}' . format ( error ) )", "predictions": ["passes the error to receive callbacks on the error ."], "references": ["handles error callbacks when using segment with segment_debug_mode set to true"], "bleu": 0.13564514503163538, "rouge_l": 0.18885448916408668}
{"id": 80, "code": "def get conn ( self ) : conn = self . get connection ( self . mssql conn id ) conn = pymssql . connect ( server = conn . host , user = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn", "predictions": ["returns a connection object ."], "references": ["returns a mssql connection object"], "bleu": 0.45180100180492244, "rouge_l": 0.8}
{"id": 81, "code": "def execute ( self , context ) : self . hook = Spark Submit Hook ( conf = self . conf , conn id = self . conn id , files = self . files , py files = self . py files , archives = self . archives , driver class path = self . driver class path , jars = self . jars , java class = self . java class , packages = self . packages , exclude packages = self . exclude packages , repositories = self . repositories , total executor cores = self . total executor cores , executor cores = self . executor cores , executor memory = self . executor memory , driver memory = self . driver memory , keytab = self . keytab , principal = self . principal , name = self . name , num executors = self . num executors , application args = self . application args , env vars = self . env vars , verbose = self . verbose , spark binary = self . spark binary ) self . hook . submit ( self . application )", "predictions": ["optimized implementation of handling hook ."], "references": ["call the sparksubmithook to run the provided spark job"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 82, "code": "def delete dag ( dag id ) : try : count = delete . delete dag ( dag id ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response return jsonify ( message = \"Removed {} record(s)\" . format ( count ) , count = count )", "predictions": ["deletes the specified dag ."], "references": ["delete all db records related to the specified dag ."], "bleu": 0.27952792741962756, "rouge_l": 0.5030927835051546}
{"id": 83, "code": "def get dag code ( dag id ) : try : return get code ( dag id ) except Airflow Exception as err : log . info ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response", "predictions": ["get dag code from specified dag ."], "references": ["return python code of a given dag_id ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 84, "code": "def task info ( dag id , task id ) : try : info = get task ( dag id , task id ) except Airflow Exception as err : log . info ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( ' ' ) } return jsonify ( fields )", "predictions": ["format the specified dag into a task or group ."], "references": ["returns a json with a task s public instance variables ."], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 85, "code": "def get pools ( ) : try : pools = pool api . get pools ( ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response else : return jsonify ( [ p . to json ( ) for p in pools ] )", "predictions": ["get all pools and its status ."], "references": ["get all pools ."], "bleu": 0.3655552228545123, "rouge_l": 0.7648902821316614}
{"id": 86, "code": "def create pool ( ) : params = request . get json ( force = True ) try : pool = pool api . create pool ( * * params ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response else : return jsonify ( pool . to json ( ) )", "predictions": ["create json from soundcloud ."], "references": ["create a pool ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 87, "code": "def get task instances ( self , state = None , session = None ) : from airflow . models . taskinstance import Task Instance tis = session . query ( Task Instance ) . filter ( Task Instance . dag id == self . dag id , Task Instance . execution date == self . execution date , ) if state : if isinstance ( state , six . string types ) : tis = tis . filter ( Task Instance . state == state ) else : if None in state : tis = tis . filter ( or ( Task Instance . state . in ( state ) , Task Instance . state . is ( None ) ) ) else : tis = tis . filter ( Task Instance . state . in ( state ) ) if self . dag and self . dag . partial : tis = tis . filter ( Task Instance . task id . in ( self . dag . task ids ) ) return tis . all ( )", "predictions": ["get the instance of the appropriate task ."], "references": ["returns the task instances for this dag run"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 88, "code": "def get previous dagrun ( self , session = None ) : return session . query ( Dag Run ) . filter ( Dag Run . dag id == self . dag id , Dag Run . execution date < self . execution date ) . order by ( Dag Run . execution date . desc ( ) ) . first ( )", "predictions": ["method to get the previous dagrun ."], "references": ["the previous dagrun if there is one"], "bleu": 0.345720784641941, "rouge_l": 0.42857142857142855}
{"id": 89, "code": "def get previous scheduled dagrun ( self , session = None ) : dag = self . get dag ( ) return session . query ( Dag Run ) . filter ( Dag Run . dag id == self . dag id , Dag Run . execution date == dag . previous schedule ( self . execution date ) ) . first ( )", "predictions": ["returns the previous scheduled session ."], "references": ["the previous scheduled dagrun if there is one"], "bleu": 0.2945901093386716, "rouge_l": 0.4178082191780822}
{"id": 90, "code": "def conditionally trigger ( context , dag run obj ) : c p = context [ 'params' ] [ 'condition param' ] print ( \"Controller DAG : conditionally trigger = {}\" . format ( c p ) ) if context [ 'params' ] [ 'condition param' ] : dag run obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } pp . pprint ( dag run obj . payload ) return dag run obj", "predictions": ["trigger the object using the specified dag ."], "references": ["this function decides whether or not to trigger the remote dag"], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 91, "code": "def get dag ( self , dag id ) : from airflow . models . dag import Dag Model root dag id = dag id if dag id in self . dags : dag = self . dags [ dag id ] if dag . is subdag : root dag id = dag . parent dag . dag id orm dag = Dag Model . get current ( root dag id ) if orm dag and ( root dag id not in self . dags or ( orm dag . last expired and dag . last loaded < orm dag . last expired ) ) : found dags = self . process file ( filepath = orm dag . fileloc , only if updated = False ) if found dags and dag id in [ found dag . dag id for found dag in found dags ] : return self . dags [ dag id ] elif dag id in self . dags : del self . dags [ dag id ] return self . dags . get ( dag id )", "predictions": ["returns the dag object ."], "references": ["gets the dag out of the dictionary and refreshes it if expired"], "bleu": 0.08860330314183162, "rouge_l": 0.2190305206463196}
{"id": 92, "code": "def dagbag report ( self ) : report = textwrap . dedent ( ) stats = self . dagbag stats return report . format ( dag folder = self . dag folder , duration = sum ( [ o . duration for o in stats ] ) , dag num = sum ( [ o . dag num for o in stats ] ) , task num = sum ( [ o . task num for o in stats ] ) , table = pprinttable ( stats ) , )", "predictions": ["reports reports as a report ."], "references": ["prints a report around dagbag loading stats"], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 93, "code": "def execute ( self , context ) : self . hook = Spark JDBC Hook ( spark app name = self . spark app name , spark conn id = self . spark conn id , spark conf = self . spark conf , spark py files = self . spark py files , spark files = self . spark files , spark jars = self . spark jars , num executors = self . num executors , executor cores = self . executor cores , executor memory = self . executor memory , driver memory = self . driver memory , verbose = self . verbose , keytab = self . keytab , principal = self . principal , cmd type = self . cmd type , jdbc table = self . jdbc table , jdbc conn id = self . jdbc conn id , jdbc driver = self . jdbc driver , metastore table = self . metastore table , jdbc truncate = self . jdbc truncate , save mode = self . save mode , save format = self . save format , batch size = self . batch size , fetch size = self . fetch size , num partitions = self . num partitions , partition column = self . partition column , lower bound = self . lower bound , upper bound = self . upper bound , create table column types = self . create table column types ) self . hook . submit jdbc job ( )", "predictions": ["utility method for ( read from ("], "references": ["call the sparksubmithook to run the provided spark job"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 94, "code": "def integrate plugins ( ) : import sys from airflow . plugins manager import macros modules for macros module in macros modules : sys . modules [ macros module . name ] = macros module globals ( ) [ macros module . name ] = macros module", "predictions": ["integrate plugins ."], "references": ["integrate plugins to the context"], "bleu": 0.36304072644520663, "rouge_l": 0.47843137254901963}
{"id": 95, "code": "def error ( self , session = None ) : self . log . error ( \"Recording the task instance as FAILED\" ) self . state = State . FAILED session . merge ( self ) session . commit ( )", "predictions": ["creates a full error log entry ."], "references": ["forces the task instance s state to failed in the database ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 96, "code": "def clear xcom data ( self , session = None ) : session . query ( X Com ) . filter ( X Com . dag id == self . dag id , X Com . task id == self . task id , X Com . execution date == self . execution date ) . delete ( ) session . commit ( )", "predictions": ["remove all entries in the table ."], "references": ["clears all xcom data from the database for the task instance"], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 97, "code": "def key ( self ) : return self . dag id , self . task id , self . execution date , self . try number", "predictions": ["get the name of the 0 - 1 if there is no more than a chunks ."], "references": ["returns a tuple that identifies the task instance uniquely"], "bleu": 0.07994607499472013, "rouge_l": 0.0814419225634179}
{"id": 98, "code": "def init run context ( self , raw = False ) : self . raw = raw self . set context ( self )", "predictions": ["initialize the full dateline ."], "references": ["sets the log context ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 99, "code": "def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload on close : return local loc = os . path . join ( self . local base , self . log relative path ) remote loc = os . path . join ( self . remote base , self . log relative path ) if os . path . exists ( local loc ) : with open ( local loc , 'r' ) as logfile : log = logfile . read ( ) self . wasb write ( log , remote loc , append = True ) if self . delete local copy : shutil . rmtree ( os . path . dirname ( local loc ) ) self . closed = True", "predictions": ["integrate this name and its contents ."], "references": ["close and upload local log file to remote storage wasb ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 100, "code": "def query cassandra ( self ) : self . hook = Cassandra Hook ( cassandra conn id = self . cassandra conn id ) session = self . hook . get conn ( ) cursor = session . execute ( self . cql ) return cursor", "predictions": ["executed before the entity has been executed ."], "references": ["queries cassandra and returns a cursor to the results ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 101, "code": "def execute ( self , context ) : self . hook = Spark Sql Hook ( sql = self . sql , conf = self . conf , conn id = self . conn id , total executor cores = self . total executor cores , executor cores = self . executor cores , executor memory = self . executor memory , keytab = self . keytab , principal = self . principal , name = self . name , num executors = self . num executors , master = self . master , yarn queue = self . yarn queue ) self . hook . run query ( )", "predictions": ["this method transforms the database into a preferred = . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , ."], "references": ["call the sparksqlhook to run the provided sql query"], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 102, "code": "def get conn ( self ) : conn = self . get connection ( self . conn id ) service options = conn . extra dejson self . account name = service options . get ( 'account name' ) adl Creds = lib . auth ( tenant id = service options . get ( 'tenant' ) , client secret = conn . password , client id = conn . login ) adls File System Client = core . Azure DL File System ( adl Creds , store name = self . account name ) adls File System Client . connect ( ) return adls File System Client", "predictions": ["get a tcp , if available ."], "references": ["return a azuredlfilesystem object ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 103, "code": "def execute ( self , context ) : self . hook = self . get hook ( ) self . hook . get conn ( ) self . query execution context [ 'Database' ] = self . database self . result configuration [ 'Output Location' ] = self . output location self . query execution id = self . hook . run query ( self . query , self . query execution context , self . result configuration , self . client request token ) query status = self . hook . poll query status ( self . query execution id , self . max tries ) if query status in AWS Athena Hook . FAILURE STATES : raise Exception ( 'Final state of Athena job is {}, query execution id is {}.' . format ( query status , self . query execution id ) ) elif not query status or query status in AWS Athena Hook . INTERMEDIATE STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query execution id is {}.' . format ( query status , self . query execution id ) )", "predictions": ["this is a wrapper around the database ."], "references": ["run presto query on athena"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 104, "code": "def on kill ( self ) : if self . query execution id : self . log . info ( '\u26b0\ufe0f\u26b0\ufe0f\u26b0\ufe0f Received a kill Signal. Time to Die') self . log . info ( 'Stopping Query with execution Id - %s' , self . query execution id ) response = self . hook . stop query ( self . query execution id ) http status code = None try : http status code = response [ 'Response Metadata' ] [ 'HTTP Status Code' ] except Exception as ex : self . log . error ( 'Exception while cancelling query' , ex ) finally : if http status code is None or http status code != 200 : self . log . error ( 'Unable to request query cancel on athena. Exiting' ) else : self . log . info ( 'Polling Athena for query with id %s to reach final state' , self . query execution id ) self . hook . poll query status ( self . query execution id )", "predictions": ["method to call to handle api call to . ( , , . , , , , . , self , . , . , . , . , . , . , . , . , . , . , . , . , . , . , ."], "references": ["cancel the submitted athena query"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 105, "code": "def uncompress file ( input file name , file extension , dest dir ) : if file extension . lower ( ) not in ( '.gz' , '.bz2' ) : raise Not Implemented Error ( \"Received {} format. Only gz and bz2 \" \"files can currently be uncompressed.\" . format ( file extension ) ) if file extension . lower ( ) == '.gz' : fmodule = gzip . Gzip File elif file extension . lower ( ) == '.bz2' : fmodule = bz2 . BZ2File with fmodule ( input file name , mode = 'rb' ) as f compressed , Named Temporary File ( dir = dest dir , mode = 'wb' , delete = False ) as f uncompressed : shutil . copyfileobj ( f compressed , f uncompressed ) return f uncompressed . name", "predictions": ["lambda a version of the version of the version file ."], "references": ["uncompress gz and bz2 files"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 106, "code": "def get conn ( self ) : if not self . conn : connection = self . get connection ( self . conn id ) extras = connection . extra dejson self . conn = Salesforce ( username = connection . login , password = connection . password , security token = extras [ 'security token' ] , instance url = connection . host , sandbox = extras . get ( 'sandbox' , False ) ) return self . conn", "predictions": ["this method is called by the database when it wants to connect to ."], "references": ["sign into salesforce only if we are not already signed in ."], "bleu": 0.08839374326825923, "rouge_l": 0.07800511508951406}
{"id": 107, "code": "def put records ( self , records ) : firehose conn = self . get conn ( ) response = firehose conn . put record batch ( Delivery Stream Name = self . delivery stream , Records = records ) return response", "predictions": ["puts the model to the cache ."], "references": ["write batch records to kinesis firehose"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 108, "code": "def send email ( to , subject , html content , files = None , dryrun = False , cc = None , bcc = None , mime subtype = 'mixed' , mime charset = 'utf-8' , * * kwargs ) : path , attr = configuration . conf . get ( 'email' , 'EMAIL BACKEND' ) . rsplit ( '.' , 1 ) module = importlib . import module ( path ) backend = getattr ( module , attr ) to = get email address list ( to ) to = \", \" . join ( to ) return backend ( to , subject , html content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime subtype = mime subtype , mime charset = mime charset , * * kwargs )", "predictions": ["write out an batch ."], "references": ["send email using backend specified in email_backend ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 109, "code": "def get conn ( self ) : if self . conn is None : params = self . get connection ( self . ftp conn id ) pasv = params . extra dejson . get ( \"passive\" , True ) self . conn = ftplib . FTP ( params . host , params . login , params . password ) self . conn . set pasv ( pasv ) return self . conn", "predictions": ["integrate this object to the database ."], "references": ["returns a ftp connection object"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 110, "code": "def execute ( self , context ) : self . hook = Discord Webhook Hook ( self . http conn id , self . webhook endpoint , self . message , self . username , self . avatar url , self . tts , self . proxy ) self . hook . execute ( )", "predictions": ["fixme we get a full proxy from this method ."], "references": ["call the discordwebhookhook to post message"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 111, "code": "def get conn ( self ) : conn = self . get connection ( self . conn id ) service options = conn . extra dejson return File Service ( account name = conn . login , account key = conn . password , * * service options )", "predictions": ["used to gather connection ."], "references": ["return the fileservice object ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 112, "code": "def get conn ( self ) : if not self . conn : self . conn = storage . Client ( credentials = self . get credentials ( ) ) return self . conn", "predictions": ["get a conn object ."], "references": ["returns a google cloud storage service object ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 113, "code": "def describe training job with log ( self , job name , positions , stream names , instance count , state , last description , last describe job call ) : log group = '/aws/sagemaker/Training Jobs' if len ( stream names ) < instance count : logs conn = self . get log conn ( ) try : streams = logs conn . describe log streams ( log Group Name = log group , log Stream Name Prefix = job name + '/' , order By = 'Log Stream Name' , limit = instance count ) stream names = [ s [ 'log Stream Name' ] for s in streams [ 'log Streams' ] ] positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream names if s not in positions ] ) except logs conn . exceptions . Resource Not Found Exception : pass if len ( stream names ) > 0 : for idx , event in self . multi stream iter ( log group , stream names , positions ) : self . log . info ( event [ 'message' ] ) ts , count = positions [ stream names [ idx ] ] if event [ 'timestamp' ] == ts : positions [ stream names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) else : positions [ stream names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) if state == Log State . COMPLETE : return state , last description , last describe job call if state == Log State . JOB COMPLETE : state = Log State . COMPLETE elif time . time ( ) - last describe job call >= 30 : description = self . describe training job ( job name ) last describe job call = time . time ( ) if secondary training status changed ( description , last description ) : self . log . info ( secondary training status message ( description , last description ) ) last description = description status = description [ 'Training Job Status' ] if status not in self . non terminal states : state = Log State . JOB COMPLETE return state , last description , last describe job call", "predictions": ["callback for ( i . e . ( . archives archives archives archives archives archives archives archives archives archives archives archives archives archives . . archives archives . . archives . . . . archives . . . . archives . . . . . . . . . ."], "references": ["return the training job info associated with job_name and print cloudwatch logs"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 114, "code": "def execute ( self , context ) : bucket helper = Google Cloud Bucket Helper ( self . gcp conn id , self . delegate to ) self . py file = bucket helper . google cloud to local ( self . py file ) hook = Data Flow Hook ( gcp conn id = self . gcp conn id , delegate to = self . delegate to , poll sleep = self . poll sleep ) dataflow options = self . dataflow default options . copy ( ) dataflow options . update ( self . options ) camel to snake = lambda name : re . sub ( r'[A-Z]' , lambda x : ' ' + x . group ( 0 ) . lower ( ) , name ) formatted options = { camel to snake ( key ) : dataflow options [ key ] for key in dataflow options } hook . start python dataflow ( self . job name , formatted options , self . py file , self . py options )", "predictions": ["this method executes the parameter to be executed on each object in this object ."], "references": ["execute the python dataflow job ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 115, "code": "def prepare cli cmd ( self ) : conn = self . conn hive bin = 'hive' cmd extra = [ ] if self . use beeline : hive bin = 'beeline' jdbc url = \"jdbc:hive2://{host}:{port}/{schema}\" . format ( host = conn . host , port = conn . port , schema = conn . schema ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : template = conn . extra dejson . get ( 'principal' , \"hive/ HOST@EXAMPLE.COM\" ) if \" HOST\" in template : template = utils . replace hostname pattern ( utils . get components ( template ) ) proxy user = \"\" if conn . extra dejson . get ( 'proxy user' ) == \"login\" and conn . login : proxy user = \"hive.server2.proxy.user={0}\" . format ( conn . login ) elif conn . extra dejson . get ( 'proxy user' ) == \"owner\" and self . run as : proxy user = \"hive.server2.proxy.user={0}\" . format ( self . run as ) jdbc url += \";principal={template};{proxy user}\" . format ( template = template , proxy user = proxy user ) elif self . auth : jdbc url += \";auth=\" + self . auth jdbc url = '\"{}\"' . format ( jdbc url ) cmd extra += [ '-u' , jdbc url ] if conn . login : cmd extra += [ '-n' , conn . login ] if conn . password : cmd extra += [ '-p' , conn . password ] hive params list = self . hive cli params . split ( ) return [ hive bin ] + cmd extra + hive params list", "predictions": ["this sets proxy to the dag ' s dag list . the dag will only be called from the client before running ."], "references": ["this function creates the command list from available information"], "bleu": 0.0665422126355551, "rouge_l": 0.27141268075639596}
{"id": 116, "code": "def get metastore client ( self ) : import hmsclient from thrift . transport import T Socket , T Transport from thrift . protocol import T Binary Protocol ms = self . metastore conn auth mechanism = ms . extra dejson . get ( 'auth Mechanism' , 'NOSASL' ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth mechanism = ms . extra dejson . get ( 'auth Mechanism' , 'GSSAPI' ) kerberos service name = ms . extra dejson . get ( 'kerberos service name' , 'hive' ) socket = T Socket . T Socket ( ms . host , ms . port ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' and auth mechanism == 'GSSAPI' : try : import saslwrapper as sasl except Import Error : import sasl def sasl factory ( ) : sasl client = sasl . Client ( ) sasl client . set Attr ( \"host\" , ms . host ) sasl client . set Attr ( \"service\" , kerberos service name ) sasl client . init ( ) return sasl client from thrift sasl import T Sasl Client Transport transport = T Sasl Client Transport ( sasl factory , \"GSSAPI\" , socket ) else : transport = T Transport . T Buffered Transport ( socket ) protocol = T Binary Protocol . T Binary Protocol ( transport ) return hmsclient . HMS Client ( iprot = protocol )", "predictions": ["for a sasl , : http : / / www . com / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( - http - client - http - http - get - get - get"], "references": ["returns a hive thrift client ."], "bleu": 0.028577262451992175, "rouge_l": 0.08321964529331514}
{"id": 117, "code": "def get tables ( self , db , pattern = '*' ) : with self . metastore as client : tables = client . get tables ( db name = db , pattern = pattern ) return client . get table objects by name ( db , tables )", "predictions": ["extracts all the fields of the given = targets ."], "references": ["get a metastore table object"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 118, "code": "def get conn ( self , schema = None ) : db = self . get connection ( self . hiveserver2 conn id ) auth mechanism = db . extra dejson . get ( 'auth Mechanism' , 'NONE' ) if auth mechanism == 'NONE' and db . login is None : username = 'airflow' kerberos service name = None if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth mechanism = db . extra dejson . get ( 'auth Mechanism' , 'KERBEROS' ) kerberos service name = db . extra dejson . get ( 'kerberos service name' , 'hive' ) if auth mechanism == 'GSSAPI' : self . log . warning ( \"Detected deprecated 'GSSAPI' for auth Mechanism \" \"for %s. Please use 'KERBEROS' instead\" , self . hiveserver2 conn id ) auth mechanism = 'KERBEROS' from pyhive . hive import connect return connect ( host = db . host , port = db . port , auth = auth mechanism , kerberos service name = kerberos service name , username = db . login or username , password = db . password , database = schema or db . schema or 'default' )", "predictions": ["create a pool for the given connection ."], "references": ["returns a hive connection object ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 119, "code": "def get endpoint ( self ) : conn = self . get connection ( self . http conn id ) token = conn . password if not token : raise Airflow Exception ( 'Dingding token is requests but get nothing, ' 'check you conn id configuration.' ) return 'robot/send?access token={}' . format ( token )", "predictions": ["get task task task ."], "references": ["get dingding endpoint for sending message ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 120, "code": "def bind parameters ( operation , parameters ) : string parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string parameters [ name ] = \"'\" + escape ( value ) + \"'\" else : string parameters [ name ] = str ( value ) return operation % string parameters", "predictions": ["binds the previous previous previous ( such as \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" , \" ,"], "references": ["helper method that binds parameters to a sql query ."], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 121, "code": "def escape ( s ) : e = s e = e . replace ( '\\\\' , '\\\\\\\\' ) e = e . replace ( '\\n' , '\\\\n' ) e = e . replace ( '\\r' , '\\\\r' ) e = e . replace ( \"'\" , \"\\\\'\" ) e = e . replace ( '\"' , '\\\\\"' ) return e", "predictions": ["escapes the given string ."], "references": ["helper method that escapes parameters to a sql query ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 122, "code": "def get conn ( self ) : service = self . get service ( ) project = self . get field ( 'project' ) return Big Query Connection ( service = service , project id = project , use legacy sql = self . use legacy sql , location = self . location , num retries = self . num retries )", "predictions": ["use this to conditionally conditionally to get information about this : we have to conditionally conditionally ."], "references": ["returns a bigquery pep 249 connection object ."], "bleu": 0.07223943354597204, "rouge_l": 0.08555399719495091}
{"id": 123, "code": "def get service ( self ) : http authorized = self . authorize ( ) return build ( 'bigquery' , 'v2' , http = http authorized , cache discovery = False )", "predictions": ["method to get the ) dag from the dag object ."], "references": ["returns a bigquery service object ."], "bleu": 0.14991106946711685, "rouge_l": 0.2484725050916497}
{"id": 124, "code": "def cancel query ( self ) : jobs = self . service . jobs ( ) if ( self . running job id and not self . poll job complete ( self . running job id ) ) : self . log . info ( 'Attempting to cancel job : %s, %s' , self . project id , self . running job id ) if self . location : jobs . cancel ( project Id = self . project id , job Id = self . running job id , location = self . location ) . execute ( num retries = self . num retries ) else : jobs . cancel ( project Id = self . project id , job Id = self . running job id ) . execute ( num retries = self . num retries ) else : self . log . info ( 'No running Big Query jobs to cancel.' ) return max polling attempts = 12 polling attempts = 0 job complete = False while polling attempts < max polling attempts and not job complete : polling attempts = polling attempts + 1 job complete = self . poll job complete ( self . running job id ) if job complete : self . log . info ( 'Job successfully canceled: %s, %s' , self . project id , self . running job id ) elif polling attempts == max polling attempts : self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running job id ) else : self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running job id ) time . sleep ( 5 )", "predictions": ["decorator for taking the format of the format \" format \" ."], "references": ["cancel all started queries that have not yet completed"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 125, "code": "def query postgres ( self ) : postgres = Postgres Hook ( postgres conn id = self . postgres conn id ) conn = postgres . get conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor", "predictions": ["handle a parameter_declaration from the latest drill list ."], "references": ["queries postgres and returns a cursor to the results ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 126, "code": "def integrate plugins ( ) : from airflow . plugins manager import hooks modules for hooks module in hooks modules : sys . modules [ hooks module . name ] = hooks module globals ( ) [ hooks module . name ] = hooks module", "predictions": ["integrate all plugins . this method is called from the list of plugins ."], "references": ["integrate plugins to the context"], "bleu": 0.10511846841633776, "rouge_l": 0.34528301886792445}
{"id": 127, "code": "def on finish ( self ) : if self . cfg path and os . path . isfile ( self . cfg path ) : if self . run as user : subprocess . call ( [ 'sudo' , 'rm' , self . cfg path ] , close fds = True ) else : os . remove ( self . cfg path )", "predictions": ["remove a pull request ."], "references": ["a callback that should be called when this is done running ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 128, "code": "def main ( ) : usage = \"usage: nvd3.py [options]\" parser = Option Parser ( usage = usage , version = ( \"python-nvd3 - Charts generator with \" \"nvd3.js and d3.js\" ) ) parser . add option ( \"-q\" , \"--quiet\" , action = \"store false\" , dest = \"verbose\" , default = True , help = \"don't print messages to stdout\" ) ( options , args ) = parser . parse args ( )", "predictions": ["main entry point for the script ."], "references": ["parse options and process commands"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 129, "code": "def buildhtmlheader ( self ) : self . htmlheader = '' global js initialized if ' js initialized' not in globals ( ) or not js initialized : for css in self . header css : self . htmlheader += css for js in self . header js : self . htmlheader += js", "predictions": ["this method is called to provide javascript tokens for each header ."], "references": ["generate html header content"], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 130, "code": "def buildjschart ( self ) : self . jschart = '' if self . tooltip condition string == '' : self . tooltip condition string = 'var y = String(graph.point.y);\\n' self . series js = json . dumps ( self . series )", "predictions": ["creates a series object for the specified condition and series ."], "references": ["generate javascript code for the chart"], "bleu": 0.14991106946711685, "rouge_l": 0.2484725050916497}
{"id": 131, "code": "def create x axis ( self , name , label = None , format = None , date = False , custom format = False ) : axis = { } if custom format and format : axis [ 'tick Format' ] = format elif format : if format == 'AM PM' : axis [ 'tick Format' ] = \"function(d) { return get am pm(parse Int(d)); }\" else : axis [ 'tick Format' ] = \"d3.format(',%s')\" % format if label : axis [ 'axis Label' ] = \"'\" + label + \"'\" if date : self . dateformat = format axis [ 'tick Format' ] = ( \"function(d) { return d3.time.format('%s')\" \"(new Date(parse Int(d))) }\\n\" \"\" % self . dateformat ) if name [ 0 ] == 'x' : self . x axis date = True self . axislist [ name ] = axis if name == \"x Axis\" and self . focus enable : self . axislist [ 'x2Axis' ] = axis", "predictions": ["create a new axis for an axis ."], "references": ["create x - axis"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 132, "code": "def create y axis ( self , name , label = None , format = None , custom format = False ) : axis = { } if custom format and format : axis [ 'tick Format' ] = format elif format : axis [ 'tick Format' ] = \"d3.format(',%s')\" % format if label : axis [ 'axis Label' ] = \"'\" + label + \"'\" self . axislist [ name ] = axis", "predictions": ["create an axis for a certain axis ."], "references": ["create y - axis"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 133, "code": "def get conn ( self ) : conn = self . get connection ( self . sqlite conn id ) conn = sqlite3 . connect ( conn . host ) return conn", "predictions": ["connection to the database ."], "references": ["returns a sqlite connection object"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 134, "code": "def action logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : with create session ( ) as session : if g . user . is anonymous : user = 'anonymous' else : user = g . user . username log = Log ( event = f . name , task instance = None , owner = user , extra = str ( list ( request . args . items ( ) ) ) , task id = request . args . get ( 'task id' ) , dag id = request . args . get ( 'dag id' ) ) if 'execution date' in request . args : log . execution date = pendulum . parse ( request . args . get ( 'execution date' ) ) session . add ( log ) return f ( * args , * * kwargs ) return wrapper", "predictions": ["now we wrap a logging request ."], "references": ["decorator to log user actions"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 135, "code": "def gzipped ( f ) : @ functools . wraps ( f ) def view func ( * args , * * kwargs ) : @ after this request def zipper ( response ) : accept encoding = request . headers . get ( 'Accept-Encoding' , '' ) if 'gzip' not in accept encoding . lower ( ) : return response response . direct passthrough = False if ( response . status code < 200 or response . status code >= 300 or 'Content-Encoding' in response . headers ) : return response gzip buffer = IO ( ) gzip file = gzip . Gzip File ( mode = 'wb' , fileobj = gzip buffer ) gzip file . write ( response . data ) gzip file . close ( ) response . data = gzip buffer . getvalue ( ) response . headers [ 'Content-Encoding' ] = 'gzip' response . headers [ 'Vary' ] = 'Accept-Encoding' response . headers [ 'Content-Length' ] = len ( response . data ) return response return f ( * args , * * kwargs ) return view func", "predictions": ["decorator to help help help call the gzipped method ."], "references": ["decorator to make a view compressed"], "bleu": 0.16590387014219712, "rouge_l": 0.26180257510729615}
{"id": 136, "code": "def json response ( obj ) : return Response ( response = json . dumps ( obj , indent = 4 , cls = Airflow Json Encoder ) , status = 200 , mimetype = \"application/json\" )", "predictions": ["returns a response for the http response ."], "references": ["returns a json response from a json serializable python object"], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 137, "code": "def make cache key ( * args , * * kwargs ) : path = request . path args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) return ( path + args ) . encode ( 'ascii' , 'ignore' )", "predictions": ["create a cache object ."], "references": ["used by cache to get a unique key per url"], "bleu": 0.11115018927487523, "rouge_l": 0.12577319587628866}
{"id": 138, "code": "def get api key ( self ) : conn = self . get connection ( self . http conn id ) api key = conn . password if not api key : raise Airflow Exception ( 'Opsgenie API Key is required for this hook, ' 'please check your conn id configuration.' ) return api key", "predictions": ["fetch the api key ."], "references": ["get opsgenie api_key for creating alert"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 139, "code": "def execute ( self , context ) : self . hook = Opsgenie Alert Hook ( self . opsgenie conn id ) self . hook . execute ( self . build opsgenie payload ( ) )", "predictions": ["this is a wrapper around the ( method of the game ."], "references": ["call the opsgeniealerthook to post message"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 140, "code": "def get conn ( self ) : if self . conn is None : cnopts = pysftp . Cn Opts ( ) if self . no host key check : cnopts . hostkeys = None cnopts . compression = self . compress conn params = { 'host' : self . remote host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn params [ 'password' ] = self . password if self . key file : conn params [ 'private key' ] = self . key file if self . private key pass : conn params [ 'private key pass' ] = self . private key pass self . conn = pysftp . Connection ( * * conn params ) return self . conn", "predictions": ["this method is called by the database when a key is received ."], "references": ["returns an sftp connection object"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 141, "code": "def execute ( self , context ) : s3 conn = S3Hook ( self . s3 conn id ) if self . is pipeline : results = Mongo Hook ( self . mongo conn id ) . aggregate ( mongo collection = self . mongo collection , aggregate query = self . mongo query , mongo db = self . mongo db ) else : results = Mongo Hook ( self . mongo conn id ) . find ( mongo collection = self . mongo collection , query = self . mongo query , mongo db = self . mongo db ) docs str = self . stringify ( self . transform ( results ) ) s3 conn . load string ( string data = docs str , key = self . s3 key , bucket name = self . s3 bucket , replace = self . replace ) return True", "predictions": ["execute a s3 execute method from the s3 at the given s3 location ."], "references": ["executed by task_instance at runtime"], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 142, "code": "def get pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( \"Pool name shouldn't be empty\" ) pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : raise Pool Not Found ( \"Pool '%s' doesn't exist\" % name ) return pool", "predictions": ["get a pool by name ."], "references": ["get pool by a given name ."], "bleu": 0.3365910691208744, "rouge_l": 0.7587064676616916}
{"id": 143, "code": "def create pool ( name , slots , description , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( \"Pool name shouldn't be empty\" ) try : slots = int ( slots ) except Value Error : raise Airflow Bad Request ( \"Bad value for `slots`: %s\" % slots ) session . expire on commit = False pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : pool = Pool ( pool = name , slots = slots , description = description ) session . add ( pool ) else : pool . slots = slots pool . description = description session . commit ( ) return pool", "predictions": ["create a pool with the given description and description . the slots are stored as the result is stored as the result of the pool ."], "references": ["create a pool with a given parameters ."], "bleu": 0.14132052098159442, "rouge_l": 0.39019189765458434}
{"id": 144, "code": "def delete pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( \"Pool name shouldn't be empty\" ) pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : raise Pool Not Found ( \"Pool '%s' doesn't exist\" % name ) session . delete ( pool ) session . commit ( ) return pool", "predictions": ["delete a pool by name ."], "references": ["delete pool by a given name ."], "bleu": 0.3365910691208744, "rouge_l": 0.7587064676616916}
{"id": 145, "code": "def execute ( self ) : proxies = { } if self . proxy : proxies = { 'https' : self . proxy } discord payload = self . build discord payload ( ) self . run ( endpoint = self . webhook endpoint , data = discord payload , headers = { 'Content-type' : 'application/json' } , extra options = { 'proxies' : proxies } )", "predictions": ["from this object ' s children ."], "references": ["execute the discord webhook call"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 146, "code": "def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload on close : return local loc = os . path . join ( self . local base , self . log relative path ) remote loc = os . path . join ( self . remote base , self . log relative path ) if os . path . exists ( local loc ) : with open ( local loc , 'r' ) as logfile : log = logfile . read ( ) self . s3 write ( log , remote loc ) self . closed = True", "predictions": ["close this packet for the local file ."], "references": ["close and upload local log file to remote storage s3 ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 147, "code": "def get init containers ( self ) : if self . kube config . dags volume claim or self . kube config . dags volume host or self . kube config . dags in image : return [ ] init environment = [ { 'name' : 'GIT SYNC REPO' , 'value' : self . kube config . git repo } , { 'name' : 'GIT SYNC BRANCH' , 'value' : self . kube config . git branch } , { 'name' : 'GIT SYNC ROOT' , 'value' : self . kube config . git sync root } , { 'name' : 'GIT SYNC DEST' , 'value' : self . kube config . git sync dest } , { 'name' : 'GIT SYNC DEPTH' , 'value' : '1' } , { 'name' : 'GIT SYNC ONE TIME' , 'value' : 'true' } ] if self . kube config . git user : init environment . append ( { 'name' : 'GIT SYNC USERNAME' , 'value' : self . kube config . git user } ) if self . kube config . git password : init environment . append ( { 'name' : 'GIT SYNC PASSWORD' , 'value' : self . kube config . git password } ) volume mounts = [ { 'mount Path' : self . kube config . git sync root , 'name' : self . dags volume name , 'read Only' : False } ] if self . kube config . git ssh key secret name : volume mounts . append ( { 'name' : self . git sync ssh secret volume name , 'mount Path' : '/etc/git-secret/ssh' , 'sub Path' : 'ssh' } ) init environment . extend ( [ { 'name' : 'GIT SSH KEY FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT SYNC SSH' , 'value' : 'true' } ] ) if self . kube config . git ssh known hosts configmap name : volume mounts . append ( { 'name' : self . git sync ssh known hosts volume name , 'mount Path' : '/etc/git-secret/known hosts' , 'sub Path' : 'known hosts' } ) init environment . extend ( [ { 'name' : 'GIT KNOWN HOSTS' , 'value' : 'true' } , { 'name' : 'GIT SSH KNOWN HOSTS FILE' , 'value' : '/etc/git-secret/known hosts' } ] ) else : init environment . append ( { 'name' : 'GIT KNOWN HOSTS' , 'value' : 'false' } ) return [ { 'name' : self . kube config . git sync init container name , 'image' : self . kube config . git sync container , 'security Context' : { 'run As User' : 65533 } , 'env' : init environment , 'volume Mounts' : volume mounts } ]", "predictions": ["a git method for handling containers that can be called in git mode ."], "references": ["when using git to retrieve the dags use the gitsync init container"], "bleu": 0.08839374326825923, "rouge_l": 0.07800511508951406}
{"id": 148, "code": "def get environment ( self ) : env = { } for env var name , env var val in six . iteritems ( self . kube config . kube env vars ) : env [ env var name ] = env var val env [ \"AIRFLOW CORE EXECUTOR\" ] = \"Local Executor\" if self . kube config . airflow configmap : env [ 'AIRFLOW HOME' ] = self . worker airflow home env [ 'AIRFLOW CORE DAGS FOLDER' ] = self . worker airflow dags if ( not self . kube config . airflow configmap and 'AIRFLOW CORE SQL ALCHEMY CONN' not in self . kube config . kube secrets ) : env [ 'AIRFLOW CORE SQL ALCHEMY CONN' ] = conf . get ( \"core\" , \"SQL ALCHEMY CONN\" ) if self . kube config . git dags folder mount point : dag volume mount path = os . path . join ( self . kube config . git dags folder mount point , self . kube config . git sync dest , self . kube config . git subpath ) env [ 'AIRFLOW CORE DAGS FOLDER' ] = dag volume mount path return env", "predictions": ["method to get mount environment variables from git this ."], "references": ["defines any necessary environment variables for the pod executor"], "bleu": 0.16590387014219712, "rouge_l": 0.21254355400696867}
{"id": 149, "code": "def get secrets ( self ) : worker secrets = [ ] for env var name , obj key pair in six . iteritems ( self . kube config . kube secrets ) : k8s secret obj , k8s secret key = obj key pair . split ( '=' ) worker secrets . append ( Secret ( 'env' , env var name , k8s secret obj , k8s secret key ) ) if self . kube config . env from secret ref : for secret ref in self . kube config . env from secret ref . split ( ',' ) : worker secrets . append ( Secret ( 'env' , None , secret ref ) ) return worker secrets", "predictions": ["extracts all secrets secrets from the given worker and key ."], "references": ["defines any necessary secrets for the pod executor"], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 150, "code": "def get security context ( self ) : security context = { } if self . kube config . worker run as user : security context [ 'run As User' ] = self . kube config . worker run as user if self . kube config . worker fs group : security context [ 'fs Group' ] = self . kube config . worker fs group if self . kube config . git ssh key secret name and security context . get ( 'fs Group' ) is None : security context [ 'fs Group' ] = 65533 return security context", "predictions": ["retrieves the security context for this worker ."], "references": ["defines the security context"], "bleu": 0.2984745896009823, "rouge_l": 0.5319767441860466}
{"id": 151, "code": "def start ( self ) : self . process = Dag File Processor . launch process ( self . result queue , self . file path , self . pickle dags , self . dag id white list , \"Dag File Processor{}\" . format ( self . instance id ) , self . zombies ) self . start time = timezone . utcnow ( )", "predictions": ["starts the timezone of this tool_shed_url ."], "references": ["launch the process and start processing the dag ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 152, "code": "def exit gracefully ( self , signum , frame ) : self . log . info ( \"Exiting gracefully upon receiving signal %s\" , signum ) if self . processor agent : self . processor agent . end ( ) sys . exit ( os . EX OK )", "predictions": ["exit with the process ."], "references": ["helper method to clean up processor_agent to avoid leaving orphan processes ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 153, "code": "def process executor events ( self , simple dag bag , session = None ) : TI = models . Task Instance for key , state in list ( self . executor . get event buffer ( simple dag bag . dag ids ) . items ( ) ) : dag id , task id , execution date , try number = key self . log . info ( \"Executor reports execution of %s.%s execution date=%s \" \"exited with status %s for try number %s\" , dag id , task id , execution date , state , try number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag id == dag id , TI . task id == task id , TI . execution date == execution date ) ti = qry . first ( ) if not ti : self . log . warning ( \"Task Instance %s went missing from the database\" , ti ) continue if ti . try number == try number and ti . state == State . QUEUED : msg = ( \"Executor reports task instance {} finished ({}) \" \"although the task says its {}. Was the task \" \"killed externally?\" . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple dag = simple dag bag . get dag ( dag id ) dagbag = models . Dag Bag ( simple dag . full filepath ) dag = dagbag . get dag ( dag id ) ti . task = dag . get task ( task id ) ti . handle failure ( msg ) except Exception : self . log . error ( \"Cannot load the dag bag to handle failure for %s\" \". Setting task to FAILED without callbacks or \" \"retries. Do you have enough resources?\" , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )", "predictions": ["does the actual work of ( on the dag ."], "references": ["respond to executor events ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 154, "code": "def heartbeat callback ( self , session = None ) : if self . terminating : self . task runner . terminate ( ) return self . task instance . refresh from db ( ) ti = self . task instance fqdn = get hostname ( ) same hostname = fqdn == ti . hostname same process = ti . pid == os . getpid ( ) if ti . state == State . RUNNING : if not same hostname : self . log . warning ( \"The recorded hostname %s \" \"does not match this instance's hostname \" \"%s\" , ti . hostname , fqdn ) raise Airflow Exception ( \"Hostname of job runner does not match\" ) elif not same process : current pid = os . getpid ( ) self . log . warning ( \"Recorded pid %s does not match \" \"the current pid %s\" , ti . pid , current pid ) raise Airflow Exception ( \"PID of job runner does not match\" ) elif ( self . task runner . return code ( ) is None and hasattr ( self . task runner , 'process' ) ) : self . log . warning ( \"State of this instance has been externally set to %s. \" \"Taking the poison pill.\" , ti . state ) self . task runner . terminate ( ) self . terminating = True", "predictions": ["forward for a heartbeat ."], "references": ["self destruct task if state has been moved away from running externally"], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 155, "code": "def get conn ( self ) : if self . session and not self . session . is shutdown : return self . session self . session = self . cluster . connect ( self . keyspace ) return self . session", "predictions": ["get the default session ."], "references": ["returns a cassandra session object"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 156, "code": "def query mysql ( self ) : mysql = My Sql Hook ( mysql conn id = self . mysql conn id ) conn = mysql . get conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor", "predictions": ["issue a mysql cursor to the database ."], "references": ["queries mysql and returns a cursor to the results ."], "bleu": 0.2673580441032691, "rouge_l": 0.5446428571428571}
{"id": 157, "code": "def get col type dict ( self ) : schema = [ ] if isinstance ( self . schema , string types ) : schema = json . loads ( self . schema ) elif isinstance ( self . schema , list ) : schema = self . schema elif self . schema is not None : self . log . warn ( 'Using default schema due to unexpected type.' 'Should be a string or list.' ) col type dict = { } try : col type dict = { col [ 'name' ] : col [ 'type' ] for col in schema } except Key Error : self . log . warn ( 'Using default schema due to missing name or type. Please ' 'refer to: https://cloud.google.com/bigquery/docs/schemas' '#specifying a json schema file' ) return col type dict", "predictions": ["allow due to due to due to a schema for a schema ."], "references": ["return a dict of column name and column type based on self . schema if not none ."], "bleu": 0.07732453710762957, "rouge_l": 0.18807810894141833}
{"id": 158, "code": "def extra dejson ( self ) : obj = { } if self . extra : try : obj = json . loads ( self . extra ) except Exception as e : self . log . exception ( e ) self . log . error ( \"Failed parsing the json for conn id %s\" , self . conn id ) return obj", "predictions": ["method that retrieves api extra extra extra objects ."], "references": ["returns the extra property by deserializing json ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 159, "code": "def scale time units ( time seconds arr , unit ) : if unit == 'minutes' : return list ( map ( lambda x : x * 1.0 / 60 , time seconds arr ) ) elif unit == 'hours' : return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time seconds arr ) ) elif unit == 'days' : return list ( map ( lambda x : x * 1.0 / ( 24 * 60 * 60 ) , time seconds arr ) ) return time seconds arr", "predictions": ["returns an array of units that can be used to scale the time . this returns the time of the time units ."], "references": ["convert an array of time durations in seconds to the specified time unit ."], "bleu": 0.12062940248564934, "rouge_l": 0.3957367933271547}
{"id": 160, "code": "def get all permissions views ( self ) : perms views = set ( ) for role in self . get user roles ( ) : perms views . update ( { ( perm view . permission . name , perm view . view menu . name ) for perm view in role . permissions } ) return perms views", "predictions": ["returns all permissions that have been added to the given - and and and and and its and and path ."], "references": ["returns a set of tuples with the perm name and view menu name"], "bleu": 0.0690889519686715, "rouge_l": 0.18429003021148035}
{"id": 161, "code": "def has role ( self , role name or list ) : if not isinstance ( role name or list , list ) : role name or list = [ role name or list ] return any ( [ r . name in role name or list for r in self . get user roles ( ) ] )", "predictions": ["checks if a given ( possibly null ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["whether the user has this role name"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 162, "code": "def has perm ( self , permission name , view menu name ) : if hasattr ( self , 'perms' ) : if ( permission name , view menu name ) in self . perms : return True self . get and cache perms ( ) return ( permission name , view menu name ) in self . perms", "predictions": ["check if this method has a ."], "references": ["whether the user has this perm"], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 163, "code": "def clean perms ( self ) : self . log . debug ( 'Cleaning faulty perms' ) sesh = self . get session pvms = ( sesh . query ( sqla models . Permission View ) . filter ( or ( sqla models . Permission View . permission == None , sqla models . Permission View . view menu == None , ) ) ) deleted count = pvms . delete ( ) sesh . commit ( ) if deleted count : self . log . info ( 'Deleted %s faulty permissions' , deleted count )", "predictions": ["generate permissions for indexed permission ."], "references": ["fab leaves faulty permissions that need to be cleaned up"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 164, "code": "def create perm vm for all dag ( self ) : for dag vm in self . DAG VMS : for perm in self . DAG PERMS : self . merge perm ( permission name = perm , view menu name = dag vm )", "predictions": ["create a axis to which all the associated machine will be created ."], "references": ["create perm - vm if not exist and insert into fab security model for all - dags ."], "bleu": 0.07732453710762957, "rouge_l": 0.18807810894141833}
{"id": 165, "code": "def poke ( self , context ) : if '.' in self . table name : self . database name , self . table name = self . table name . split ( '.' ) self . log . info ( 'Poking for table %s. %s, expression %s' , self . database name , self . table name , self . expression ) return self . get hook ( ) . check for partition ( self . database name , self . table name , self . expression )", "predictions": ["get a sqlite3 expression object ."], "references": ["checks for existence of the partition in the aws glue catalog table"], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 166, "code": "def get conn ( self ) : effective user = self . proxy user autoconfig = self . autoconfig use sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' try : connections = self . get connections ( self . hdfs conn id ) if not effective user : effective user = connections [ 0 ] . login if not autoconfig : autoconfig = connections [ 0 ] . extra dejson . get ( 'autoconfig' , False ) hdfs namenode principal = connections [ 0 ] . extra dejson . get ( 'hdfs namenode principal' ) except Airflow Exception : if not autoconfig : raise if autoconfig : client = Auto Config Client ( effective user = effective user , use sasl = use sasl ) elif len ( connections ) == 1 : client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective user = effective user , use sasl = use sasl , hdfs namenode principal = hdfs namenode principal ) elif len ( connections ) > 1 : nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] client = HA Client ( nn , effective user = effective user , use sasl = use sasl , hdfs namenode principal = hdfs namenode principal ) else : raise HDFS Hook Exception ( \"conn id doesn't exist in the repository \" \"and autoconfig is not specified\" ) return client", "predictions": ["this is a method for use with ( ( ) ."], "references": ["returns a snakebite hdfsclient object ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 167, "code": "def get conn ( self ) : conn = self . get connection ( self . pinot broker conn id ) pinot broker conn = connect ( host = conn . host , port = conn . port , path = conn . extra dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot broker conn", "predictions": ["gzipped a connection to the broker ."], "references": ["establish a connection to pinot broker through pinot dbqpi ."], "bleu": 0.24924249970265266, "rouge_l": 0.5700934579439253}
{"id": 168, "code": "def convert date to dict ( field date ) : return { DAY : field date . day , MONTH : field date . month , YEAR : field date . year }", "predictions": ["converts a response to a datetime ."], "references": ["convert native python datetime . date object to a format supported by the api"], "bleu": 0.1130839610732672, "rouge_l": 0.17967599410898377}
{"id": 169, "code": "def convert time to dict ( time ) : return { HOURS : time . hour , MINUTES : time . minute , SECONDS : time . second }", "predictions": ["converts a cache cache to a time ."], "references": ["convert native python datetime . time object to a format supported by the api"], "bleu": 0.11327490115090784, "rouge_l": 0.1732954545454545}
{"id": 170, "code": "def get conn ( self ) : conn = self . get connection ( self . redis conn id ) self . host = conn . host self . port = conn . port self . password = None if str ( conn . password ) . lower ( ) in [ 'none' , 'false' , '' ] else conn . password self . db = conn . extra dejson . get ( 'db' , None ) if not self . redis : self . log . debug ( 'Initializing redis object for conn id \"%s\" on %s:%s:%s' , self . redis conn id , self . host , self . port , self . db ) self . redis = Redis ( host = self . host , port = self . port , password = self . password , db = self . db ) return self . redis", "predictions": ["get ."], "references": ["returns a redis connection ."], "bleu": 0.16954225822593183, "rouge_l": 0.26521739130434785}
{"id": 171, "code": "def get conn ( self ) : db = self . get connection ( getattr ( self , self . conn name attr ) ) return self . connector . connect ( host = db . host , port = db . port , username = db . login , schema = db . schema )", "predictions": ["builds and returns a = schema ."], "references": ["returns a connection object"], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 172, "code": "def set autocommit ( self , conn , autocommit ) : if not self . supports autocommit and autocommit : self . log . warn ( ( \"%s connection doesn't support \" \"autocommit but autocommit activated.\" ) , getattr ( self , self . conn name attr ) ) conn . autocommit = autocommit", "predictions": ["simulate an existing host ."], "references": ["sets the autocommit flag on the connection"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 173, "code": "def get query ( self ) : return ( super ( ) . get query ( ) . filter ( or ( models . Dag Model . is active , models . Dag Model . is paused ) ) . filter ( ~ models . Dag Model . is subdag ) )", "predictions": ["gets all the content of this ( context = . . . context = . . . context context = 0 context context = 0 , 1 context = comet context context context context context = count , 2 context = 0 , 2 , 3 , 2 , 3"], "references": ["default filters for model"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 174, "code": "def get count query ( self ) : return ( super ( ) . get count query ( ) . filter ( models . Dag Model . is active ) . filter ( ~ models . Dag Model . is subdag ) )", "predictions": ["returns the pool of this ( the ( running session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session session"], "references": ["default filters for model"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 175, "code": "def execute ( self , context ) : self . hook = Slack Webhook Hook ( self . http conn id , self . webhook token , self . message , self . attachments , self . channel , self . username , self . icon emoji , self . link names , self . proxy ) self . hook . execute ( )", "predictions": ["5 . 2 . this is a full int int for this object ."], "references": ["call the slackwebhookhook to post the provided slack message"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 176, "code": "def get credentials ( self ) : key path = self . get field ( 'key path' , False ) keyfile dict = self . get field ( 'keyfile dict' , False ) scope = self . get field ( 'scope' , None ) if scope : scopes = [ s . strip ( ) for s in scope . split ( ',' ) ] else : scopes = DEFAULT SCOPES if not key path and not keyfile dict : self . log . info ( 'Getting connection using `google.auth.default()` ' 'since no key file is defined for hook.' ) credentials , = google . auth . default ( scopes = scopes ) elif key path : if key path . endswith ( '.json' ) : self . log . debug ( 'Getting connection using JSON key file %s' % key path ) credentials = ( google . oauth2 . service account . Credentials . from service account file ( key path , scopes = scopes ) ) elif key path . endswith ( '.p12' ) : raise Airflow Exception ( 'Legacy P12 key file are not supported, ' 'use a JSON key file.' ) else : raise Airflow Exception ( 'Unrecognised extension for key file.' ) else : try : keyfile dict = json . loads ( keyfile dict ) keyfile dict [ 'private key' ] = keyfile dict [ 'private key' ] . replace ( '\\\\n' , '\\n' ) credentials = ( google . oauth2 . service account . Credentials . from service account info ( keyfile dict , scopes = scopes ) ) except json . decoder . JSON Decode Error : raise Airflow Exception ( 'Invalid key JSON.' ) return credentials . with subject ( self . delegate to ) if self . delegate to else credentials", "predictions": ["builds the pool from the given scope ."], "references": ["returns the credentials object for google api"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 177, "code": "def read image file ( data dir , image ext , n ) : def PIL2array ( img ) : return np . array ( img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64 , 64 ) def find files ( data dir , image ext ) : files = [ ] for file dir in os . listdir ( data dir ) : if file dir . endswith ( image ext ) : files . append ( os . path . join ( data dir , file dir ) ) return sorted ( files ) patches = [ ] list files = find files ( data dir , image ext ) for fpath in list files : img = Image . open ( fpath ) for y in range ( 0 , 1024 , 64 ) : for x in range ( 0 , 1024 , 64 ) : patch = img . crop ( ( x , y , x + 64 , y + 64 ) ) patches . append ( PIL2array ( patch ) ) return torch . Byte Tensor ( np . array ( patches [ : n ] ) )", "predictions": ["execute an ( directory if any if it is a sorted if it is true if it is false if it is not a sorted if it is true , return the result . otherwise , crop is false ."], "references": ["return a tensor containing the patches"], "bleu": 0.035817229106400346, "rouge_l": 0.10032894736842105}
{"id": 178, "code": "def accuracy ( output , target , topk = ( 1 , ) ) : with torch . no grad ( ) : maxk = max ( topk ) batch size = target . size ( 0 ) , pred = output . topk ( maxk , 1 , True , True ) pred = pred . t ( ) correct = pred . eq ( target [ None ] ) res = [ ] for k in topk : correct k = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) res . append ( correct k * ( 100.0 / batch size ) ) return res", "predictions": ["close the close of the given : : : : : : : self = : self = 1 . 2 . 05 . ( super . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ("], "references": ["computes the accuracy over the k top predictions for the specified values of k"], "bleu": 0.028577262451992175, "rouge_l": 0.06955530216647662}
{"id": 179, "code": "def setup for distributed ( is master ) : import builtins as builtin builtin print = builtin . print def print ( * args , * * kwargs ) : force = kwargs . pop ( 'force' , False ) if is master or force : builtin print ( * args , * * kwargs ) builtin . print = print", "predictions": ["create the thread - safe volume ."], "references": ["this function disables printing when not in master process"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 180, "code": "def download ( self ) : import tarfile if self . check integrity ( ) : print ( 'Files already downloaded and verified' ) return download url ( self . url , self . root , self . filename , self . md5 checksum ) with tarfile . open ( os . path . join ( self . root , self . filename ) , 'r:gz' ) as tar : tar . extractall ( path = self . root ) with open ( os . path . join ( self . root , 'dataset' , 'SBU captioned photo dataset urls.txt' ) ) as fh : for line in fh : url = line . rstrip ( ) try : download url ( url , os . path . join ( self . root , 'dataset' ) ) except OS Error : pass", "predictions": ["get a get text for the manager ."], "references": ["download and extract the tarball and download each individual photo ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 181, "code": "def download ( self ) : if self . check exists ( ) : return makedir exist ok ( self . raw folder ) makedir exist ok ( self . processed folder ) for url in self . urls : filename = url . rpartition ( '/' ) [ 2 ] file path = os . path . join ( self . raw folder , filename ) download url ( url , root = self . raw folder , filename = filename , md5 = None ) self . extract gzip ( gzip path = file path , remove finished = True ) print ( 'Processing...' ) training set = ( read image file ( os . path . join ( self . raw folder , 'train-images-idx3-ubyte' ) ) , read label file ( os . path . join ( self . raw folder , 'train-labels-idx1-ubyte' ) ) ) test set = ( read image file ( os . path . join ( self . raw folder , 't10k-images-idx3-ubyte' ) ) , read label file ( os . path . join ( self . raw folder , 't10k-labels-idx1-ubyte' ) ) ) with open ( os . path . join ( self . processed folder , self . training file ) , 'wb' ) as f : torch . save ( training set , f ) with open ( os . path . join ( self . processed folder , self . test file ) , 'wb' ) as f : torch . save ( test set , f ) print ( 'Done!' )", "predictions": ["get a , and get all of its torch ."], "references": ["download the mnist data if it doesn t exist in processed_folder already ."], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 182, "code": "def download ( self ) : import shutil import zipfile if self . check exists ( ) : return makedir exist ok ( self . raw folder ) makedir exist ok ( self . processed folder ) filename = self . url . rpartition ( '/' ) [ 2 ] file path = os . path . join ( self . raw folder , filename ) download url ( self . url , root = self . raw folder , filename = filename , md5 = None ) print ( 'Extracting zip archive' ) with zipfile . Zip File ( file path ) as zip f : zip f . extractall ( self . raw folder ) os . unlink ( file path ) gzip folder = os . path . join ( self . raw folder , 'gzip' ) for gzip file in os . listdir ( gzip folder ) : if gzip file . endswith ( '.gz' ) : self . extract gzip ( gzip path = os . path . join ( gzip folder , gzip file ) ) for split in self . splits : print ( 'Processing ' + split ) training set = ( read image file ( os . path . join ( gzip folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read label file ( os . path . join ( gzip folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) test set = ( read image file ( os . path . join ( gzip folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read label file ( os . path . join ( gzip folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) with open ( os . path . join ( self . processed folder , self . training file ( split ) ) , 'wb' ) as f : torch . save ( training set , f ) with open ( os . path . join ( self . processed folder , self . test file ( split ) ) , 'wb' ) as f : torch . save ( test set , f ) shutil . rmtree ( gzip folder ) print ( 'Done!' )", "predictions": ["get the entire zip object ."], "references": ["download the emnist data if it doesn t exist in processed_folder already ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 183, "code": "def preferences ( ) : if request . method == 'POST' : resp = make response ( redirect ( urljoin ( settings [ 'server' ] [ 'base url' ] , url for ( 'index' ) ) ) ) try : request . preferences . parse form ( request . form ) except Validation Exception : request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) return resp return request . preferences . save ( resp ) image proxy = request . preferences . get value ( 'image proxy' ) lang = request . preferences . get value ( 'language' ) disabled engines = request . preferences . engines . get disabled ( ) allowed plugins = request . preferences . plugins . get enabled ( ) stats = { } for c in categories : for e in categories [ c ] : stats [ e . name ] = { 'time' : None , 'warn timeout' : False , 'warn time' : False } if e . timeout > settings [ 'outgoing' ] [ 'request timeout' ] : stats [ e . name ] [ 'warn timeout' ] = True stats [ e . name ] [ 'supports selected language' ] = is selected language supported ( e , request . preferences ) for engine stat in get engines stats ( ) [ 0 ] [ 1 ] : stats [ engine stat . get ( 'name' ) ] [ 'time' ] = round ( engine stat . get ( 'avg' ) , 3 ) if engine stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request timeout' ] : stats [ engine stat . get ( 'name' ) ] [ 'warn time' ] = True return render ( 'preferences.html' , locales = settings [ 'locales' ] , current locale = get locale ( ) , image proxy = image proxy , engines by category = categories , stats = stats , answerers = [ { 'info' : a . self info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled engines = disabled engines , autocomplete backends = autocomplete backends , shortcuts = { y : x for x , y in engine shortcuts . items ( ) } , themes = themes , plugins = plugins , doi resolvers = settings [ 'doi resolvers' ] , current doi resolver = get doi resolver ( request . args , request . preferences . get value ( 'doi resolver' ) ) , allowed plugins = allowed plugins , theme = get current theme name ( ) , preferences url params = request . preferences . get as url params ( ) , base url = get base url ( ) , preferences = True )", "predictions": ["run all plugins themes in the database ."], "references": ["render preferences page && save user preferences"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 184, "code": "def get themes ( templates path ) : themes = os . listdir ( templates path ) if ' common ' in themes : themes . remove ( ' common ' ) return themes", "predictions": ["returns a list of all gracefully gracefully gracefully ."], "references": ["returns available themes list ."], "bleu": 0.16784459625186196, "rouge_l": 0.4518518518518518}
{"id": 185, "code": "def searx bang ( full query ) : if len ( full query . get Search Query ( ) ) == 0 : return [ ] results = [ ] first char = full query . get Search Query ( ) [ 0 ] if first char == '!' or first char == '?' : if len ( full query . get Search Query ( ) ) == 1 : results . append ( first char + \"images\" ) results . append ( first char + \"wikipedia\" ) results . append ( first char + \"osm\" ) else : engine query = full query . get Search Query ( ) [ 1 : ] for categorie in categories : if categorie . startswith ( engine query ) : results . append ( first char + '{categorie}' . format ( categorie = categorie ) ) for engine in engines : if engine . startswith ( engine query . replace ( ' ' , ' ' ) ) : results . append ( first char + '{engine}' . format ( engine = engine . replace ( ' ' , ' ' ) ) ) for engine shortcut in engine shortcuts : if engine shortcut . startswith ( engine query ) : results . append ( first char + '{engine shortcut}' . format ( engine shortcut = engine shortcut ) ) elif first char == ':' : if len ( full query . get Search Query ( ) ) == 1 : results . append ( \":en\" ) results . append ( \":en us\" ) results . append ( \":english\" ) results . append ( \":united kingdom\" ) else : engine query = full query . get Search Query ( ) [ 1 : ] for lc in language codes : lang id , lang name , country , english name = map ( unicode . lower , lc ) if lang id . startswith ( engine query ) : if len ( engine query ) <= 2 : results . append ( u':{lang id}' . format ( lang id = lang id . split ( '-' ) [ 0 ] ) ) else : results . append ( u':{lang id}' . format ( lang id = lang id ) ) if lang name . startswith ( engine query ) or english name . startswith ( engine query ) : results . append ( u':{lang name}' . format ( lang name = lang name ) ) if country . startswith ( engine query . replace ( ' ' , ' ' ) ) : results . append ( u':{country}' . format ( country = country . replace ( ' ' , ' ' ) ) ) result set = set ( results ) for query part in full query . query parts : if query part in result set : result set . remove ( query part ) return list ( result set )", "predictions": ["does the actual process of process strings ."], "references": ["check if the searchquery contain a bang and create fitting autocompleter results"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 186, "code": "def response ( resp ) : json resp = resp . text [ resp . text . find ( '\\n' ) + 1 : resp . text . rfind ( '\\n' ) - 2 ] results = [ ] try : conversion rate = float ( json . loads ( json resp ) [ 'conversion' ] [ 'converted-amount' ] ) except : return results answer = '{0} {1} = {2} {3}, 1 {1} ({5}) = {4} {3} ({6})' . format ( resp . search params [ 'amount' ] , resp . search params [ 'from' ] , resp . search params [ 'amount' ] * conversion rate , resp . search params [ 'to' ] , conversion rate , resp . search params [ 'from name' ] , resp . search params [ 'to name' ] , ) url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}' . format ( resp . search params [ 'from' ] . upper ( ) , resp . search params [ 'to' ] ) results . append ( { 'answer' : answer , 'url' : url } ) return results", "predictions": ["build the heartbeat of an http response from the given session and return the heartbeat ."], "references": ["remove first and last lines to get only json"], "bleu": 0.07692375026049747, "rouge_l": 0.08425414364640883}
{"id": 187, "code": "def mvn ( * args , * * kwargs ) : return tfd . Independent ( tfd . Normal ( * args , * * kwargs ) , reinterpreted batch ndims = 1 )", "predictions": ["emulate command with the normal parameters ."], "references": ["convenience function to efficiently construct a multivariatenormaldiag ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 188, "code": "def eight schools joint log prob ( treatment effects , treatment stddevs , avg effect , avg stddev , school effects standard ) : rv avg effect = tfd . Normal ( loc = 0. , scale = 10. ) rv avg stddev = tfd . Normal ( loc = 5. , scale = 1. ) rv school effects standard = mvn ( loc = tf . zeros like ( school effects standard ) , scale = tf . ones like ( school effects standard ) ) rv treatment effects = mvn ( loc = ( avg effect + tf . exp ( avg stddev ) * school effects standard ) , scale = treatment stddevs ) return ( rv avg effect . log prob ( avg effect ) + rv avg stddev . log prob ( avg stddev ) + rv school effects standard . log prob ( school effects standard ) + rv treatment effects . log prob ( treatment effects ) )", "predictions": ["calculates the probability that a distance from the given normal distribution ."], "references": ["eight - schools joint log - prob ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 189, "code": "def benchmark eight schools hmc ( num results = int ( 5e3 ) , num burnin steps = int ( 3e3 ) , num leapfrog steps = 3 , step size = 0.4 ) : num schools = 8 treatment effects = tf . constant ( [ 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ] , dtype = np . float32 , name = 'treatment effects' ) treatment stddevs = tf . constant ( [ 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 ] , dtype = np . float32 , name = 'treatment stddevs' ) def unnormalized posterior log prob ( avg effect , avg stddev , school effects standard ) : \"\"\"Eight-schools unnormalized log posterior.\"\"\" return eight schools joint log prob ( treatment effects , treatment stddevs , avg effect , avg stddev , school effects standard ) if tf . executing eagerly ( ) : sample chain = tf . function ( tfp . mcmc . sample chain ) else : sample chain = tfp . mcmc . sample chain def computation ( ) : \"\"\"The benchmark computation.\"\"\" , kernel results = sample chain ( num results = num results , num burnin steps = num burnin steps , current state = ( tf . zeros ( [ ] , name = 'init avg effect' ) , tf . zeros ( [ ] , name = 'init avg stddev' ) , tf . ones ( [ num schools ] , name = 'init school effects standard' ) , ) , kernel = tfp . mcmc . Hamiltonian Monte Carlo ( target log prob fn = unnormalized posterior log prob , step size = step size , num leapfrog steps = num leapfrog steps ) ) return kernel results . is accepted is accepted tensor = computation ( ) if not tf . executing eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( is accepted tensor ) start time = time . time ( ) if tf . executing eagerly ( ) : is accepted = computation ( ) else : is accepted = session . run ( is accepted tensor ) wall time = time . time ( ) - start time num accepted = np . sum ( is accepted ) acceptance rate = np . float32 ( num accepted ) / np . float32 ( num results ) return dict ( iters = ( num results + num burnin steps ) * num leapfrog steps , extras = { 'acceptance rate' : acceptance rate } , wall time = wall time )", "predictions": ["get an . for a ( ."], "references": ["runs hmc on the eight - schools unnormalized posterior ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 190, "code": "def build custom rv ( distribution , sample shape , value , name ) : del name return Random Variable ( distribution = distribution , sample shape = sample shape , value = value )", "predictions": ["builds a variable from the distribution ."], "references": ["randomvariable constructor with a dummy name argument ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 191, "code": "def make random variable ( distribution cls ) : @ interceptable @ functools . wraps ( distribution cls , assigned = ( ' module ' , ' name ' ) ) @ docstring util . expand docstring ( cls = distribution cls . name , doc = inspect . cleandoc ( distribution cls . init . doc or '' ) ) def func ( * args , * * kwargs ) : sample shape = kwargs . pop ( 'sample shape' , ( ) ) value = kwargs . pop ( 'value' , None ) return Random Variable ( distribution = distribution cls ( * args , * * kwargs ) , sample shape = sample shape , value = value ) return func", "predictions": ["builds and returns a time serie ."], "references": ["factory function to make random variable given distribution class ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 192, "code": "def max mask non finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : m = np . max ( x , axis = astuple ( axis ) , keepdims = keepdims ) needs masking = ~ np . isfinite ( m ) if needs masking . ndim > 0 : m [ needs masking ] = mask elif needs masking : m = mask return m", "predictions": ["compute the maximum of all entries that do not actually make a finite period ."], "references": ["returns max or mask if max is not finite ."], "bleu": 0.09782375748961449, "rouge_l": 0.2489795918367347}
{"id": 193, "code": "def eval all one hot ( fn , dist , name = None ) : with tf . compat . v1 . name scope ( name , 'eval all one hot' ) : event size = dist . event shape tensor ( ) [ - 1 ] batch ndims = tf . size ( input = dist . batch shape tensor ( ) ) x = tf . reshape ( tf . eye ( event size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch ndims , tf . int32 ) , paddings = [ [ 1 , 1 ] ] , constant values = event size ) ) perm = tf . pad ( tensor = tf . range ( 1 , batch ndims + 1 ) , paddings = [ [ 0 , 1 ] ] ) return tf . transpose ( a = fn ( dist , x ) , perm = perm )", "predictions": ["evaluate the function for a set of functions ."], "references": ["onehotcategorical helper computing probs cdf etc over its support ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 194, "code": "def get convert to tensor fn ( identifier ) : if identifier is None : return None if isinstance ( identifier , six . string types ) : identifier = str ( identifier ) return deserialize ( identifier ) if isinstance ( identifier , dict ) : return deserialize ( identifier ) if isinstance ( identifier , property ) : identifier = identifier . fget if callable ( identifier ) : return identifier raise Value Error ( 'Could not interpret ' 'convert-to-tensor function identifier:' , identifier )", "predictions": ["convert a string representation of this method into a tensor ."], "references": ["return a convert - to - tensor func given a name config callable etc ."], "bleu": 0.1042097697398488, "rouge_l": 0.22453987730061348}
{"id": 195, "code": "def new ( params , event size , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Multivariate Normal Tri L' , [ params , event size ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) scale tril = tfb . Scale Tri L ( diag shift = np . array ( 1e-5 , params . dtype . as numpy dtype ( ) ) , validate args = validate args ) return tfd . Multivariate Normal Tri L ( loc = params [ ... , : event size ] , scale tril = scale tril ( params [ ... , event size : ] ) , validate args = validate args )", "predictions": ["creates a new photo ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 196, "code": "def params size ( event size , name = None ) : with tf . compat . v1 . name scope ( name , 'Multivariate Normal Tri L params size' , [ event size ] ) : return event size + event size * ( event size + 1 ) // 2", "predictions": ["helper method for comparing transformation transformation ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 197, "code": "def new ( params , event size , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'One Hot Categorical' , [ params , event size ] ) : return tfd . One Hot Categorical ( logits = params , dtype = dtype or params . dtype . base dtype , validate args = validate args )", "predictions": ["creates a new parameters of the given parameters ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 198, "code": "def new ( params , event size , num components , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Categorical Mixture Of One Hot Categorical' , [ params , event size , num components ] ) : dist = Mixture Same Family . new ( params , num components , One Hot Categorical ( event size , validate args = False , name = name ) , validate args = validate args , name = name ) dist . mean = functools . partial ( eval all one hot , tfd . Distribution . prob , dist ) dist . log mean = functools . partial ( eval all one hot , tfd . Distribution . log prob , dist ) return dist", "predictions": ["creates a new dataset of a cluster"], "references": ["create the distribution instance from a params vector ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 199, "code": "def params size ( event size , num components , name = None ) : with tf . compat . v1 . name scope ( name , 'Categorical Mixture Of One Hot Categorical params size' , [ event size , num components ] ) : return Mixture Same Family . params size ( num components , One Hot Categorical . params size ( event size , name = name ) , name = name )", "predictions": ["constructs a & amp = components of the given parameters ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.14323145079400493, "rouge_l": 0.18181818181818182}
{"id": 200, "code": "def new ( params , event shape = ( ) , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Bernoulli' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) new shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) dist = tfd . Independent ( tfd . Bernoulli ( logits = tf . reshape ( params , new shape ) , dtype = dtype or params . dtype . base dtype , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args ) dist . logits = dist . distribution . logits dist . probs = dist . distribution . probs dist . logits = tfd . Bernoulli . logits dist . probs = tfd . Bernoulli . probs return dist", "predictions": ["creates a new reinterpreted ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 201, "code": "def new ( params , event shape = ( ) , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Logistic' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) output shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) loc params , scale params = tf . split ( params , 2 , axis = - 1 ) return tfd . Independent ( tfd . Logistic ( loc = tf . reshape ( loc params , output shape ) , scale = tf . math . softplus ( tf . reshape ( scale params , output shape ) ) , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args )", "predictions": ["creates a new tfd ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 202, "code": "def params size ( event shape = ( ) , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Normal params size' , [ event shape ] ) : event shape = tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) return 2 * event size ( event shape , name = name or 'Independent Normal params size' )", "predictions": ["helper method to get the hint of a normal transformation ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.14991106946711685, "rouge_l": 0.36363636363636365}
{"id": 203, "code": "def new ( params , event shape = ( ) , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Poisson' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) output shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) return tfd . Independent ( tfd . Poisson ( log rate = tf . reshape ( params , output shape ) , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args )", "predictions": ["creates a new tfd ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 204, "code": "def new ( params , num components , component layer , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Mixture Same Family' , [ params , num components , component layer ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) num components = tf . convert to tensor ( value = num components , name = 'num components' , dtype hint = tf . int32 ) components dist = component layer ( tf . reshape ( params [ ... , num components : ] , tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , [ num components , - 1 ] ] , axis = 0 ) ) ) mixture dist = tfd . Categorical ( logits = params [ ... , : num components ] ) return tfd . Mixture Same Family ( mixture dist , components dist , validate args = False )", "predictions": ["creates a new ( of the specified components ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 205, "code": "def params size ( num components , event shape = ( ) , name = None ) : return Mixture Same Family . params size ( num components , Independent Normal . params size ( event shape , name = name ) , name = name )", "predictions": ["builds the optimal distributed random variable with the given components ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 206, "code": "def new ( params , num components , event shape = ( ) , validate args = False , name = None ) : return Mixture Same Family . new ( params , num components , Independent Logistic ( event shape , validate args = validate args , name = name ) , validate args = validate args , name = name )", "predictions": ["create a new empty cluster with the given components ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.15851165692617156, "rouge_l": 0.31881533101045295}
{"id": 207, "code": "def params size ( num components , event shape = ( ) , name = None ) : return Mixture Same Family . params size ( num components , Independent Logistic . params size ( event shape , name = name ) , name = name )", "predictions": ["calculates the percentage of components in this population ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 208, "code": "def maybe check valid map values ( map values , validate args ) : assertions = [ ] message = 'Rank of map values must be 1.' if tensorshape util . rank ( map values . shape ) is not None : if tensorshape util . rank ( map values . shape ) != 1 : raise Value Error ( message ) elif validate args : assertions . append ( assert util . assert rank ( map values , 1 , message = message ) ) message = 'Size of map values must be greater than 0.' if tensorshape util . num elements ( map values . shape ) is not None : if tensorshape util . num elements ( map values . shape ) == 0 : raise Value Error ( message ) elif validate args : assertions . append ( assert util . assert greater ( tf . size ( input = map values ) , 0 , message = message ) ) if validate args : assertions . append ( assert util . assert equal ( tf . math . is strictly increasing ( map values ) , True , message = 'map values is not strictly increasing.' ) ) return assertions", "predictions": ["helper for ( . map values to map values ."], "references": ["validate map_values if validate_args == true ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 209, "code": "def as tensor ( x , name , dtype ) : return None if x is None else tf . convert to tensor ( value = x , name = name , dtype = dtype )", "predictions": ["instantiates a tensor variable ."], "references": ["convenience to convert to tensor or leave as none ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 210, "code": "def get default reinterpreted batch ndims ( self , distribution ) : ndims = prefer static . rank from shape ( distribution . batch shape tensor , distribution . batch shape ) return prefer static . maximum ( 0 , ndims - 1 )", "predictions": ["gets the default reinterpreted for this batch ."], "references": ["computes the default value for reinterpreted_batch_ndim __init__ arg ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 211, "code": "def cat probs ( self , log probs ) : which softmax = tf . nn . log softmax if log probs else tf . nn . softmax cat probs = which softmax ( self . cat . logits ) cat probs = tf . unstack ( cat probs , num = self . num components , axis = - 1 ) return cat probs", "predictions": ["create a new circuit to indicate the given log ."], "references": ["get a list of num_components batchwise probabilities ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 212, "code": "def maybe validate args ( outcomes , logits , probs , validate args ) : assertions = [ ] def validate equal last dim ( tensor a , tensor b , message ) : if tensor a . shape . is fully defined ( ) and tensor b . shape . is fully defined ( ) : if tensor a . shape [ - 1 ] != tensor b . shape [ - 1 ] : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . shape ( input = tensor a ) [ - 1 ] , tf . shape ( input = tensor b ) [ - 1 ] , message = message ) ) if logits is not None : validate equal last dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) if probs is not None : validate equal last dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) message = 'Rank of outcomes must be 1.' if outcomes . shape . ndims is not None : if outcomes . shape . ndims != 1 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank ( outcomes , 1 , message = message ) ) message = 'Size of outcomes must be greater than 0.' if outcomes . shape . num elements ( ) is not None : if outcomes . shape . num elements ( ) == 0 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) if validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . math . is strictly increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) return assertions", "predictions": ["helper to validate shape and validate all inputs . this must be called before the shape is set to the shape of the shape ."], "references": ["validate outcomes logits and probs s shapes ."], "bleu": 0.057783239927083445, "rouge_l": 0.20043811610076673}
{"id": 213, "code": "def logistic regression ( features ) : coeffs = ed . Multivariate Normal Diag ( loc = tf . zeros ( features . shape [ 1 ] ) , name = \"coeffs\" ) labels = ed . Bernoulli ( logits = tf . tensordot ( features , coeffs , [ [ 1 ] , [ 0 ] ] ) , name = \"labels\" ) return labels", "predictions": ["creates a copy of this tree ."], "references": ["bayesian logistic regression which returns labels given features ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 214, "code": "def covertype ( ) : import sklearn . datasets data = sklearn . datasets . covtype . fetch covtype ( ) features = data . data labels = data . target features -= features . mean ( 0 ) features /= features . std ( 0 ) features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) features = tf . cast ( features , dtype = tf . float32 ) , counts = np . unique ( labels , return counts = True ) specific category = np . argmax ( counts ) labels = ( labels == specific category ) labels = tf . cast ( labels , dtype = tf . int32 ) return features , labels", "predictions": ["construct an import of the given data ."], "references": ["builds the covertype data set ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 215, "code": "def maybe assert valid concentration ( self , concentration , validate args ) : if not validate args : return concentration return distribution util . with dependencies ( [ assert util . assert positive ( concentration , message = \"Concentration parameter must be positive.\" ) , assert util . assert rank at least ( concentration , 1 , message = \"Concentration parameter must have >=1 dimensions.\" ) , assert util . assert less ( 1 , tf . shape ( input = concentration ) [ - 1 ] , message = \"Concentration parameter must have event size >= 2.\" ) , ] , concentration )", "predictions": ["run all ( routines ."], "references": ["checks the validity of the concentration parameter ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 216, "code": "def maybe assert valid sample ( self , x ) : if not self . validate args : return x return distribution util . with dependencies ( [ assert util . assert positive ( x , message = \"samples must be positive\" ) , assert util . assert near ( tf . ones ( [ ] , dtype = self . dtype ) , tf . reduce sum ( input tensor = x , axis = - 1 ) , message = \"sample last-dimension must sum to `1`\" ) , ] , x )", "predictions": ["assert that this object is a valid sample ."], "references": ["checks the validity of a sample ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 217, "code": "def make positive axis ( axis , ndims ) : axis = make list or 1d tensor ( axis ) ndims = tf . convert to tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) ndims = tf . get static value ( ndims ) if is list like ( axis ) and ndims is not None : positive axis = [ ] for a in axis : if a < 0 : a = ndims + a positive axis . append ( a ) else : axis = tf . convert to tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) positive axis = tf . where ( axis >= 0 , axis , axis + ndims ) return positive axis", "predictions": ["creates the positive axis from the given axis . the axis of the given axis is a new positive axis ."], "references": ["rectify possibly negatively axis . prefer return python list ."], "bleu": 0.0821610732492254, "rouge_l": 0.20677966101694914}
{"id": 218, "code": "def squeeze ( x , axis ) : x = tf . convert to tensor ( value = x , name = 'x' ) if axis is None : return tf . squeeze ( x , axis = None ) axis = tf . convert to tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) axis += tf . zeros ( [ 1 ] , dtype = axis . dtype ) keep axis , = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , axis ) return tf . reshape ( x , tf . gather ( tf . shape ( input = x ) , keep axis ) )", "predictions": ["squeeze the tensor . < p > modifies the squeeze of the axis ."], "references": ["a version of squeeze that works with dynamic axis ."], "bleu": 0.13217947626377288, "rouge_l": 0.2577464788732394}
{"id": 219, "code": "def z ( self , x ) : with tf . name scope ( \"standardize\" ) : return ( x - self . loc ) / self . scale", "predictions": ["get the z representation of this image ."], "references": ["standardize input x to a unit normal ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 220, "code": "def inv z ( self , z ) : with tf . name scope ( \"reconstruct\" ) : return z * self . scale + self . loc", "predictions": ["compute the default scope ."], "references": ["reconstruct input x from a its normalized version ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 221, "code": "def semilocal linear trend transition matrix ( autoregressive coef ) : fixed entries = tf . constant ( [ [ 1. , 1. ] , [ 0. , 0. ] ] , dtype = autoregressive coef . dtype ) autoregressive coef mask = tf . constant ( [ [ 0. , 0. ] , [ 0. , 1. ] ] , dtype = autoregressive coef . dtype ) bottom right entry = ( autoregressive coef [ ... , tf . newaxis , tf . newaxis ] * autoregressive coef mask ) return tf . linalg . Linear Operator Full Matrix ( fixed entries + bottom right entry )", "predictions": ["generate a semilocal . this is done by ( ."], "references": ["build the transition matrix for a semi - local linear trend model ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 222, "code": "def semilocal linear trend transition noise ( level scale , slope mean , slope scale , autoregressive coef ) : broadcast batch shape = dist util . get broadcast shape ( level scale , slope mean , slope scale , autoregressive coef ) broadcast ones = tf . ones ( broadcast batch shape , dtype = level scale . dtype ) scale diag = tf . stack ( [ level scale * broadcast ones , slope scale * broadcast ones ] , axis = - 1 ) bias = tf . stack ( [ tf . zeros like ( broadcast ones ) , slope mean * ( 1 - autoregressive coef ) * broadcast ones ] , axis = - 1 ) return tfd . Multivariate Normal Diag ( loc = bias , scale diag = scale diag )", "predictions": ["semilocal ( linear trend ) with parameters of linear probability probability ."], "references": ["build the transition noise model for a semi - local linear trend model ."], "bleu": 0.12438292405881463, "rouge_l": 0.22761194029850743}
{"id": 223, "code": "def machine eps ( dtype ) : if isinstance ( dtype , tf . D Type ) : dtype = dtype . as numpy dtype ( ) return np . finfo ( dtype ) . eps", "predictions": ["convert the given machine to the new np ."], "references": ["returns the machine epsilon for the supplied dtype ."], "bleu": 0.17747405280050263, "rouge_l": 0.4444444444444444}
{"id": 224, "code": "def fix step size ( value and gradients function , val c input , active , step size shrink param ) : iter max = np . ceil ( - np . log2 ( machine eps ( val c input . x . dtype ) ) ) def cond ( i , val c , to fix ) : del val c return ( i < iter max ) & tf . reduce any ( input tensor = to fix ) def body ( i , val c , to fix ) : next c = tf . where ( to fix , val c . x * step size shrink param , val c . x ) next val c = value and gradients function ( next c ) still to fix = to fix & ~ hzl . is finite ( next val c ) return ( i + 1 , next val c , still to fix ) to fix = active & ~ hzl . is finite ( val c input ) return tf . while loop ( cond = cond , body = body , loop vars = ( 0 , val c input , to fix ) )", "predictions": ["' max ' input ' for ' input ' and ' input '"], "references": ["shrinks the input step size until the value and grad become finite ."], "bleu": 0.10571070857151538, "rouge_l": 0.15384615384615383}
{"id": 225, "code": "def line search inner bisection ( value and gradients function , search interval , active , f lim ) : midpoint = ( search interval . left . x + search interval . right . x ) / 2 val mid = value and gradients function ( midpoint ) is valid mid = hzl . is finite ( val mid ) still active = active & is valid mid new failed = active & ~ is valid mid next inteval = search interval . replace ( failed = search interval . failed | new failed , func evals = search interval . func evals + 1 ) def apply update ( ) : update result = hzl . update ( value and gradients function , next inteval . left , next inteval . right , val mid , f lim , active = still active ) return Hager Zhang Line Search Result ( converged = next inteval . converged , failed = next inteval . failed | update result . failed , iterations = next inteval . iterations + update result . iteration , func evals = next inteval . func evals + update result . num evals , left = update result . left , right = update result . right ) return prefer static . cond ( tf . reduce any ( input tensor = still active ) , apply update , lambda : next inteval )", "predictions": ["performs a all of the given all the ) all the given all the ) ."], "references": ["performs bisection and updates the interval ."], "bleu": 0.09147827112247602, "rouge_l": 0.28067484662576686}
{"id": 226, "code": "def print ( pass through tensor , values ) : flat values = [ ] for value in values : if hasattr ( value , ' fields' ) : for field in value . fields : flat values . extend ( [ field , to str ( getattr ( value , field ) ) ] ) continue if isinstance ( value , ( list , tuple ) ) : for v in value : flat values . append ( to str ( v ) ) continue flat values . append ( to str ( value ) ) return tf . compat . v1 . Print ( pass through tensor , flat values )", "predictions": ["prints all the specified identifier ."], "references": ["wrapper for tf . print which supports lists and namedtuples for printing ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 227, "code": "def maybe check quadrature param ( param , name , validate args ) : with tf . name scope ( \"check \" + name ) : assertions = [ ] if tensorshape util . rank ( param . shape ) is not None : if tensorshape util . rank ( param . shape ) == 0 : raise Value Error ( \"Mixing params must be a (batch of) vector; \" \"{}.rank={} is not at least one.\" . format ( name , tensorshape util . rank ( param . shape ) ) ) elif validate args : assertions . append ( assert util . assert rank at least ( param , 1 , message = ( \"Mixing params must be a (batch of) vector; \" \"{}.rank is not at least one.\" . format ( name ) ) ) ) if tensorshape util . with rank at least ( param . shape , 1 ) [ - 1 ] is not None : if tf . compat . dimension value ( param . shape [ - 1 ] ) != 1 : raise Not Implemented Error ( \"Currently only bimixtures are supported; \" \"{}.shape[-1]={} is not 1.\" . format ( name , tf . compat . dimension value ( param . shape [ - 1 ] ) ) ) elif validate args : assertions . append ( assert util . assert equal ( tf . shape ( input = param ) [ - 1 ] , 1 , message = ( \"Currently only bimixtures are supported; \" \"{}.shape[-1] is not 1.\" . format ( name ) ) ) ) if assertions : return distribution util . with dependencies ( assertions , param ) return param", "predictions": [". - ( , , , , , , , , , , , , , . , , int , int , , int , int , , int ) ) ) ) ) ) ) ) ) ) ) , etc . ) , . , etc ."], "references": ["helper which checks validity of loc and scale init args ."], "bleu": 0.02403051755364481, "rouge_l": 0.037059538274605106}
{"id": 228, "code": "def determine batch event shapes ( grid , endpoint affine ) : with tf . name scope ( \"determine batch event shapes\" ) : batch shape = grid . shape [ : - 2 ] batch shape tensor = tf . shape ( input = grid ) [ : - 2 ] event shape = None event shape tensor = None def set event shape ( shape , shape tensor ) : if event shape is None : return shape , shape tensor return ( tf . broadcast static shape ( event shape , shape ) , tf . broadcast dynamic shape ( event shape tensor , shape tensor ) ) for aff in endpoint affine : if aff . shift is not None : batch shape = tf . broadcast static shape ( batch shape , aff . shift . shape [ : - 1 ] ) batch shape tensor = tf . broadcast dynamic shape ( batch shape tensor , tf . shape ( input = aff . shift ) [ : - 1 ] ) event shape , event shape tensor = set event shape ( aff . shift . shape [ - 1 : ] , tf . shape ( input = aff . shift ) [ - 1 : ] ) if aff . scale is not None : batch shape = tf . broadcast static shape ( batch shape , aff . scale . batch shape ) batch shape tensor = tf . broadcast dynamic shape ( batch shape tensor , aff . scale . batch shape tensor ( ) ) event shape , event shape tensor = set event shape ( tf . Tensor Shape ( [ aff . scale . range dimension ] ) , aff . scale . range dimension tensor ( ) [ tf . newaxis ] ) return batch shape , batch shape tensor , event shape , event shape tensor", "predictions": ["determines the size of this size ."], "references": ["helper to infer batch_shape and event_shape ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 229, "code": "def interpolate loc ( grid , loc ) : if len ( loc ) != 2 : raise Not Implemented Error ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( loc ) ) ) deg = tf . compat . dimension value ( tensorshape util . with rank at least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise Value Error ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) with tf . name scope ( \"interpolate loc\" ) : if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : return [ None ] * deg w = grid [ ... , tf . newaxis , : , : ] loc = [ x [ ... , tf . newaxis ] if x is not None else None for x in loc ] if loc [ 0 ] is None : x = w [ ... , 1 , : ] * loc [ 1 ] elif loc [ 1 ] is None : x = w [ ... , 0 , : ] * loc [ 0 ] else : delta = loc [ 0 ] - loc [ 1 ] x = w [ ... , 0 , : ] * delta + loc [ 1 ] return [ x [ ... , k ] for k in range ( deg ) ]", "predictions": ["new graph params from one , which is a copy of the other edge . this is an identity operation ."], "references": ["helper which interpolates between two locs ."], "bleu": 0.06429451441231726, "rouge_l": 0.157014157014157}
{"id": 230, "code": "def interpolate scale ( grid , scale ) : if len ( scale ) != 2 : raise Not Implemented Error ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( scale ) ) ) deg = tf . compat . dimension value ( tensorshape util . with rank at least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise Value Error ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) with tf . name scope ( \"interpolate scale\" ) : return [ linop add lib . add operators ( [ linop scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ 0 ] for q in range ( deg ) ]", "predictions": ["( c size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size size"], "references": ["helper which interpolates between two scales ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 231, "code": "def linop scale ( w , op ) : with tf . name scope ( \"linop scale\" ) : def scaled identity ( w ) : return tf . linalg . Linear Operator Scaled Identity ( num rows = op . range dimension tensor ( ) , multiplier = w , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) if isinstance ( op , tf . linalg . Linear Operator Identity ) : return scaled identity ( w ) if isinstance ( op , tf . linalg . Linear Operator Scaled Identity ) : return scaled identity ( w * op . multiplier ) if isinstance ( op , tf . linalg . Linear Operator Diag ) : return tf . linalg . Linear Operator Diag ( diag = w [ ... , tf . newaxis ] * op . diag part ( ) , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) if isinstance ( op , tf . linalg . Linear Operator Lower Triangular ) : return tf . linalg . Linear Operator Lower Triangular ( tril = w [ ... , tf . newaxis , tf . newaxis ] * op . to dense ( ) , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) raise Not Implemented Error ( \"Unsupported Linop type ({})\" . format ( type ( op ) . name ) )", "predictions": ["size of ( . this is a ( to . ."], "references": ["creates weighted linop from existing linop ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 232, "code": "def concat vectors ( * args ) : args = [ tf . get static value ( x ) for x in args ] if any ( vec is None for vec in args ) : return tf . concat ( args , axis = 0 ) return [ val for vec in args for val in vec ]", "predictions": ["like , but returns a list of params ."], "references": ["concatenates input vectors statically if possible ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 233, "code": "def log vector matrix ( vs , ms ) : return tf . reduce logsumexp ( input tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2 )", "predictions": ["new region new region"], "references": ["multiply tensor of vectors by matrices assuming values stored are logs ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 234, "code": "def log matrix vector ( ms , vs ) : return tf . reduce logsumexp ( input tensor = ms + vs [ ... , tf . newaxis , : ] , axis = - 1 )", "predictions": ["params is the original size of the matrix ."], "references": ["multiply tensor of matrices by vectors assuming values stored are logs ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 235, "code": "def vector matrix ( vs , ms ) : return tf . reduce sum ( input tensor = vs [ ... , tf . newaxis ] * ms , axis = - 2 )", "predictions": ["applies the new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new vectors ."], "references": ["multiply tensor of vectors by matrices ."], "bleu": 0.026594139297659906, "rouge_l": 0.08122503328894808}
{"id": 236, "code": "def extract log probs ( num states , dist ) : states = tf . reshape ( tf . range ( num states ) , tf . concat ( [ [ num states ] , tf . ones like ( dist . batch shape tensor ( ) ) ] , axis = 0 ) ) return distribution util . move dimension ( dist . log prob ( states ) , 0 , - 1 )", "predictions": ["extracts the values of a given components as a list of components ."], "references": ["tabulate log probabilities from a batch of distributions ."], "bleu": 0.1135935489027116, "rouge_l": 0.2819722650231125}
{"id": 237, "code": "def marginal hidden probs ( self ) : initial log probs = tf . broadcast to ( self . log init , tf . concat ( [ self . batch shape tensor ( ) , [ self . num states ] ] , axis = 0 ) ) if self . num steps > 1 : transition log probs = self . log trans def forward step ( log probs , ) : return log vector matrix ( log probs , transition log probs ) dummy index = tf . zeros ( self . num steps - 1 , dtype = tf . float32 ) forward log probs = tf . scan ( forward step , dummy index , initializer = initial log probs , name = \"forward log probs\" ) forward log probs = tf . concat ( [ [ initial log probs ] , forward log probs ] , axis = 0 ) else : forward log probs = initial log probs [ tf . newaxis , ... ] return tf . exp ( forward log probs )", "predictions": ["creates a new estimate for the given shape and . ."], "references": ["compute marginal pdf for each individual observable ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 238, "code": "def choose random direction ( current state parts , batch rank , seed = None ) : seed gen = distributions . Seed Stream ( seed , salt = ' choose random direction' ) rnd direction parts = [ tf . random . normal ( tf . shape ( input = current state part ) , dtype = tf . float32 , seed = seed gen ( ) ) for current state part in current state parts ] sum squares = sum ( tf . reduce sum ( input tensor = rnd direction ** 2. , axis = tf . range ( batch rank , tf . rank ( rnd direction ) ) , keepdims = True ) for rnd direction in rnd direction parts ) rnd direction parts = [ rnd direction / tf . sqrt ( sum squares ) for rnd direction in rnd direction parts ] return rnd direction parts", "predictions": ["new an internal new params ."], "references": ["chooses a random direction in the event space ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 239, "code": "def maybe call fn ( fn , fn arg list , fn result = None , description = 'target log prob' ) : fn arg list = ( list ( fn arg list ) if mcmc util . is list like ( fn arg list ) else [ fn arg list ] ) if fn result is None : fn result = fn ( * fn arg list ) if not fn result . dtype . is floating : raise Type Error ( '`{}` must be a `Tensor` with `float` `dtype`.' . format ( description ) ) return fn result", "predictions": ["size of the function call ."], "references": ["helper which computes fn_result if needed ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 240, "code": "def prepare args ( target log prob fn , state , step size , target log prob = None , maybe expand = False , description = 'target log prob' ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( value = s , name = 'current state' ) for s in state parts ] target log prob = maybe call fn ( target log prob fn , state parts , target log prob , description ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) def maybe flatten ( x ) : return x if maybe expand or mcmc util . is list like ( state ) else x [ 0 ] return [ maybe flatten ( state parts ) , maybe flatten ( step sizes ) , target log prob ]", "predictions": ["maybe maybe a set of ( to be used as part of the internal storage ."], "references": ["processes input args to meet list - like assumptions ."], "bleu": 0.08513012360883544, "rouge_l": 0.16052631578947368}
{"id": 241, "code": "def build trainable posterior ( param , initial loc fn ) : loc = tf . compat . v1 . get variable ( param . name + ' loc' , initializer = lambda : initial loc fn ( param ) , dtype = param . prior . dtype , use resource = True ) scale = tf . nn . softplus ( tf . compat . v1 . get variable ( param . name + ' scale' , initializer = lambda : - 4 * tf . ones like ( initial loc fn ( param ) ) , dtype = param . prior . dtype , use resource = True ) ) q = tfd . Normal ( loc = loc , scale = scale ) if ( param . prior . event shape . ndims is None or param . prior . event shape . ndims > 0 ) : q = tfd . Independent ( q , reinterpreted batch ndims = param . prior . event shape . ndims ) return tfd . Transformed Distribution ( q , param . bijector )", "predictions": ["as the ( of the ( ."], "references": ["built a transformed - normal variational dist over a parameter s support ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 242, "code": "def minimize in graph ( build loss fn , num steps = 200 , optimizer = None ) : optimizer = tf . compat . v1 . train . Adam Optimizer ( 0.1 ) if optimizer is None else optimizer def train loop body ( step ) : train op = optimizer . minimize ( build loss fn if tf . executing eagerly ( ) else build loss fn ( ) ) return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control inputs = [ train op ] ) minimize op = tf . compat . v1 . while loop ( cond = lambda step : step < num steps , body = train loop body , loop vars = [ tf . constant ( 0 ) ] , return same structure = True ) [ 0 ] return minimize op", "predictions": ["get get normal normal cost cost cost for training training values ."], "references": ["run an optimizer within the graph to minimize a loss function ."], "bleu": 0.10390302174233558, "rouge_l": 0.08333333333333333}
{"id": 243, "code": "def broadcast batch shape ( distributions ) : batch shape = distributions [ 0 ] . batch shape for distribution in distributions : batch shape = tf . broadcast static shape ( batch shape , distribution . batch shape ) if batch shape . is fully defined ( ) : return batch shape . as list ( ) batch shape = distributions [ 0 ] . batch shape tensor ( ) for distribution in distributions : batch shape = tf . broadcast dynamic shape ( batch shape , distribution . batch shape tensor ( ) ) return tf . convert to tensor ( value = batch shape )", "predictions": ["cat given batch by ( ( ( ( ( ( ( ( ( ( ( ( ( log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log"], "references": ["get broadcast batch shape from distributions statically if possible ."], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 244, "code": "def range ( self , name = \"range\" ) : with self . name scope ( name ) : return self . high - self . low", "predictions": ["a maybe maybe use this method to get the maybe maybe be called before the maybe block ."], "references": ["high - low ."], "bleu": 0.06809398432036522, "rouge_l": 0.1026936026936027}
{"id": 245, "code": "def make summary statistic ( attr ) : def fn ( self ) : if any ( self . dist fn args ) : raise Value Error ( 'Can only compute ' + attr + ' when all distributions are ' 'independent; {}' . format ( self . model ) ) return self . unflatten ( getattr ( d ( ) , attr ) ( ) for d in self . dist fn wrapped ) return fn", "predictions": ["logistic a method for making a ( with the same settings as the method ."], "references": ["factory for making summary statistics eg mean mode stddev ."], "bleu": 0.11633270842295028, "rouge_l": 0.2489795918367347}
{"id": 246, "code": "def resolve distribution names ( dist fn args , dist names , leaf name ) : if dist names is None : dist names = [ ] else : dist names = dist names . copy ( ) n = len ( dist fn args ) dist names . extend ( [ None ] * ( n - len ( dist names ) ) ) for i , args in enumerate ( reversed ( dist fn args ) ) : if not args : continue i = n - i - 1 for j , arg name in enumerate ( args ) : dist names [ i - j - 1 ] = arg name j = 0 for i in range ( len ( dist names ) ) : i = n - i - 1 if dist names [ i ] is None : dist names [ i ] = leaf name if j == 0 else leaf name + str ( j ) j += 1 return tuple ( dist names )", "predictions": ["resolves an object from a path ."], "references": ["uses arg names to resolve distribution names ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 247, "code": "def get required args ( fn ) : argspec = tf inspect . getfullargspec ( fn ) args = argspec . args if tf inspect . isclass ( fn ) : args = args [ 1 : ] if argspec . defaults : args = args [ : - len ( argspec . defaults ) ] return tuple ( args )", "predictions": ["returns an argument list of the arguments for the given command ."], "references": ["returns the distribution s required args ."], "bleu": 0.1235622127262679, "rouge_l": 0.3315217391304348}
{"id": 248, "code": "def build ( self , model ) : if not isinstance ( model , collections . Sequence ) : raise Type Error ( '`model` must be `list`-like (saw: {}).' . format ( type ( model ) . name ) ) self . dist fn = model self . dist fn wrapped , self . dist fn args = zip ( * [ unify call signature ( i , dist fn ) for i , dist fn in enumerate ( model ) ] )", "predictions": ["builds a method that builds a ( ( as an array of ( self self self self self self self self self self self self self self - dimension self self self self self self self self self self self self self - dimension self self self self self self"], "references": ["creates dist_fn dist_fn_wrapped dist_fn_args ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 249, "code": "def entropy ( self ) : if any ( self . dist fn args ) : raise Value Error ( 'Can only compute entropy when all distributions are independent.' ) return sum ( joint distribution lib . maybe check wont broadcast ( ( d ( ) . entropy ( ) for d in self . dist fn wrapped ) , self . validate args ) )", "predictions": ["make a make a make has the same effect as the make ."], "references": ["shannon entropy in nats ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 250, "code": "def prepare args ( log likelihood fn , state , log likelihood = None , description = 'log likelihood' ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( s , name = 'current state' ) for s in state parts ] log likelihood = maybe call fn ( log likelihood fn , state parts , log likelihood , description ) return [ state parts , log likelihood ]", "predictions": ["squeeze will be passed to the shared variables ."], "references": ["processes input args to meet list - like assumptions ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 251, "code": "def vector size to square matrix size ( d , validate args , name = None ) : if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : n = ( - 1 + np . sqrt ( 1 + 8 * d ) ) / 2. if float ( int ( n ) ) != n : raise Value Error ( \"Vector length is not a triangular number.\" ) return int ( n ) else : with tf . name scope ( name or \"vector size to square matrix size\" ) as name : n = ( - 1. + tf . sqrt ( 1 + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. if validate args : with tf . control dependencies ( [ assert util . assert equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = \"Vector length is not a triangular number\" ) ] ) : n = tf . identity ( n ) return tf . cast ( n , d . dtype )", "predictions": ["convert a z or 3d z to a , , , where the z and the corresponding \"vector is between 0 and 1 ."], "references": ["convert a vector size to a matrix size ."], "bleu": 0.0877491027594595, "rouge_l": 0.33008658008658004}
{"id": 252, "code": "def argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise Value Error ( 'Unrecognized direction: {}.' . format ( direction ) ) return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' )", "predictions": ["get all non - null values of ( or null * tf * tf * tf * tf * tf * tf * tf * tf * tf * tf * tf * tf * tf * tf * with tf * tf ."], "references": ["numpy implementation of tf . argsort ."], "bleu": 0.03867468300268994, "rouge_l": 0.13535502958579881}
{"id": 253, "code": "def sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise Value Error ( 'Unrecognized direction: {}.' . format ( direction ) ) result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) if direction == 'DESCENDING' : return np . negative ( result ) return result", "predictions": ["sort : return trend ."], "references": ["numpy implementation of tf . sort ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 254, "code": "def ndtr ( x ) : half sqrt 2 = tf . constant ( 0.5 * np . sqrt ( 2. ) , dtype = x . dtype , name = \"half sqrt 2\" ) w = x * half sqrt 2 z = tf . abs ( w ) y = tf . where ( tf . less ( z , half sqrt 2 ) , 1. + tf . math . erf ( w ) , tf . where ( tf . greater ( w , 0. ) , 2. - tf . math . erfc ( z ) , tf . math . erfc ( z ) ) ) return 0.5 * y", "predictions": ["calculates the ndtr of the given vector2 ."], "references": ["implements ndtr core logic ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 255, "code": "def log ndtr asymptotic series ( x , series order ) : npdt = dtype util . as numpy dtype ( x . dtype ) if series order <= 0 : return npdt ( 1 ) x 2 = tf . square ( x ) even sum = tf . zeros like ( x ) odd sum = tf . zeros like ( x ) x 2n = x 2 for n in range ( 1 , series order + 1 ) : y = npdt ( double factorial ( 2 * n - 1 ) ) / x 2n if n % 2 : odd sum += y else : even sum += y x 2n *= x 2 return 1. + even sum - odd sum", "predictions": ["calculates the natural machine of the dtype ."], "references": ["calculates the asymptotic series used in log_ndtr ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 256, "code": "def text messages joint log prob ( count data , lambda 1 , lambda 2 , tau ) : alpha = ( 1. / tf . reduce mean ( input tensor = count data ) ) rv lambda = tfd . Exponential ( rate = alpha ) rv tau = tfd . Uniform ( ) lambda = tf . gather ( [ lambda 1 , lambda 2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( input = count data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) rv observation = tfd . Poisson ( rate = lambda ) return ( rv lambda . log prob ( lambda 1 ) + rv lambda . log prob ( lambda 2 ) + rv tau . log prob ( tau ) + tf . reduce sum ( input tensor = rv observation . log prob ( count data ) ) )", "predictions": ["run an input variable ."], "references": ["joint log probability function ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 257, "code": "def outer squared difference ( x , y ) : z = x - y return z [ ... , tf . newaxis , : ] * z [ ... , tf . newaxis ]", "predictions": ["outer squared difference between two built nodes ."], "references": ["convenience function analogous to tf . squared_difference ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 258, "code": "def split covariance into marginals ( covariance , block sizes ) : start dim = 0 marginals = [ ] for size in block sizes : end dim = start dim + size marginals . append ( covariance [ ... , start dim : end dim , start dim : end dim ] ) start dim = end dim return marginals", "predictions": ["split points at a specific covariance matrix ."], "references": ["split a covariance matrix into block - diagonal marginals of given sizes ."], "bleu": 0.13434323860909256, "rouge_l": 0.45658682634730546}
{"id": 259, "code": "def numpy text ( tensor , is repr = False ) : if tensor . dtype . is numpy compatible : text = repr ( tensor . numpy ( ) ) if is repr else str ( tensor . numpy ( ) ) else : text = \"<unprintable>\" if \"\\n\" in text : text = \"\\n\" + text return text", "predictions": ["converts a text from a tensor to a numpy or a numpy array ."], "references": ["human - readable representation of a tensor s numpy value ."], "bleu": 0.13217947626377288, "rouge_l": 0.3270777479892761}
{"id": 260, "code": "def sample shape ( self ) : if isinstance ( self . sample shape , tf . Tensor ) : return tf . Tensor Shape ( tf . get static value ( self . sample shape ) ) return tf . Tensor Shape ( self . sample shape )", "predictions": ["samples the shape of this shape and returns a sample ."], "references": ["sample shape of random variable as a tensorshape ."], "bleu": 0.17827531042796255, "rouge_l": 0.4073455759599332}
{"id": 261, "code": "def value ( self ) : if self . value is None : try : self . value = self . distribution . sample ( self . sample shape tensor ( ) ) except Not Implemented Error : raise Not Implemented Error ( \"sample is not implemented for {0}. You must either pass in the \" \"value argument or implement sample for {0}.\" . format ( self . distribution . class . name ) ) return self . value", "predictions": ["this is a value of the value of this sample ."], "references": ["get tensor that the random variable corresponds to ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 262, "code": "def numpy ( self ) : if not isinstance ( self . value , ops . Eager Tensor ) : raise Not Implemented Error ( \"value argument must be a Eager Tensor.\" ) return self . value . numpy ( )", "predictions": ["override this method to get the numpy array of the variable ."], "references": ["value as numpy array only available for tf eager ."], "bleu": 0.14694106251955755, "rouge_l": 0.2772727272727273}
{"id": 263, "code": "def uniform unit norm ( dimension , shape , dtype , seed ) : raw = normal . Normal ( loc = dtype util . as numpy dtype ( dtype ) ( 0 ) , scale = dtype util . as numpy dtype ( dtype ) ( 1 ) ) . sample ( tf . concat ( [ shape , [ dimension ] ] , axis = 0 ) , seed = seed ( ) ) unit norm = raw / tf . norm ( tensor = raw , ord = 2 , axis = - 1 ) [ ... , tf . newaxis ] return unit norm", "predictions": ["instantiates a uniform tensor of the specified dimension ."], "references": ["returns a batch of points chosen uniformly from the unit hypersphere ."], "bleu": 0.12716571564598603, "rouge_l": 0.3713850837138508}
{"id": 264, "code": "def common dtype ( args list , preferred dtype = None ) : dtype = None preferred dtype = ( None if preferred dtype is None else tf . as dtype ( preferred dtype ) ) for a in tf . nest . flatten ( args list ) : if hasattr ( a , 'dtype' ) : dt = tf . as dtype ( a . dtype ) else : continue if dtype is None : dtype = dt elif dtype != dt : raise Type Error ( 'Found incompatible dtypes, {} and {}.' . format ( dtype , dt ) ) if dtype is None and preferred dtype is None : return None return ( preferred dtype if dtype is None else dtype ) . as numpy dtype", "predictions": ["converts the specified variable to a common string ."], "references": ["returns explict dtype from args_list if exists else preferred_dtype ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 265, "code": "def make summary statistic ( attr ) : def fn ( self , * * kwargs ) : \"\"\"Implements summary statistic, eg, mean, stddev, mode.\"\"\" x = getattr ( self . distribution , attr ) ( * * kwargs ) shape = prefer static . concat ( [ self . distribution . batch shape tensor ( ) , prefer static . ones ( prefer static . rank from shape ( self . sample shape ) , dtype = self . sample shape . dtype ) , self . distribution . event shape tensor ( ) , ] , axis = 0 ) x = tf . reshape ( x , shape = shape ) shape = prefer static . concat ( [ self . distribution . batch shape tensor ( ) , self . sample shape , self . distribution . event shape tensor ( ) , ] , axis = 0 ) return tf . broadcast to ( x , shape ) return fn", "predictions": ["creates a summary of the given summary and returns it ."], "references": ["factory for implementing summary statistics eg mean stddev mode ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 266, "code": "def broadcast to ( tensor to broadcast , target tensors ) : output = tensor to broadcast for tensor in target tensors : output += tf . zeros like ( tensor ) return output", "predictions": ["broadcast all tensors from one tensor to another ."], "references": ["helper to broadcast a tensor using a list of target tensors ."], "bleu": 0.13309610652103343, "rouge_l": 0.2785388127853881}
{"id": 267, "code": "def pdf at peak ( self ) : return ( self . peak - self . low ) / ( self . high - self . low )", "predictions": ["returns a pdf for this image ."], "references": ["pdf evaluated at the peak ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 268, "code": "def effective sample size single state ( states , filter beyond lag , filter threshold ) : with tf . compat . v1 . name scope ( 'effective sample size single state' , values = [ states , filter beyond lag , filter threshold ] ) : states = tf . convert to tensor ( value = states , name = 'states' ) dt = states . dtype auto corr = stats . auto correlation ( states , axis = 0 , max lags = filter beyond lag ) if filter threshold is not None : filter threshold = tf . convert to tensor ( value = filter threshold , dtype = dt , name = 'filter threshold' ) mask = auto corr < filter threshold mask = tf . cast ( mask , dtype = dt ) mask = tf . cumsum ( mask , axis = 0 ) mask = tf . maximum ( 1. - mask , 0. ) auto corr *= mask n = axis size ( states , axis = 0 ) k = tf . range ( 0. , axis size ( auto corr , axis = 0 ) ) nk factor = ( n - k ) / n if auto corr . shape . ndims is not None : new shape = [ - 1 ] + [ 1 ] * ( auto corr . shape . ndims - 1 ) else : new shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) nk factor = tf . reshape ( nk factor , new shape ) return n / ( - 1 + 2 * tf . reduce sum ( input tensor = nk factor * auto corr , axis = 0 ) )", "predictions": ["compute effective sample positions ."], "references": ["ess computation for one single tensor argument ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 269, "code": "def potential scale reduction single state ( state , independent chain ndims ) : with tf . compat . v1 . name scope ( 'potential scale reduction single state' , values = [ state , independent chain ndims ] ) : state = tf . convert to tensor ( value = state , name = 'state' ) sample ndims = 1 sample axis = tf . range ( 0 , sample ndims ) chain axis = tf . range ( sample ndims , sample ndims + independent chain ndims ) sample and chain axis = tf . range ( 0 , sample ndims + independent chain ndims ) n = axis size ( state , sample axis ) m = axis size ( state , chain axis ) b div n = reduce variance ( tf . reduce mean ( input tensor = state , axis = sample axis , keepdims = True ) , sample and chain axis , biased = False ) w = tf . reduce mean ( input tensor = reduce variance ( state , sample axis , keepdims = True , biased = True ) , axis = sample and chain axis ) sigma 2 plus = w + b div n return ( ( m + 1. ) / m ) * sigma 2 plus / w - ( n - 1. ) / ( m * n )", "predictions": ["potential mean of reduction to the mean of the reduction . < p > the potential mean of the mean of the mean of the mean of the mean of the mean of the mean . the mean of the mean is the mean of the mean of the mean"], "references": ["potential_scale_reduction for one single state tensor ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 270, "code": "def axis size ( x , axis = None ) : if axis is None : return tf . cast ( tf . size ( input = x ) , x . dtype ) return tf . cast ( tf . reduce prod ( input tensor = tf . gather ( tf . shape ( input = x ) , axis ) ) , x . dtype )", "predictions": ["computes the axis of the specified axis ."], "references": ["get number of elements of x in axis as type x . dtype ."], "bleu": 0.09008421318929809, "rouge_l": 0.25994318181818177}
{"id": 271, "code": "def broadcast maybelist arg ( states , secondary arg , name ) : if is list like ( secondary arg ) : if len ( secondary arg ) != len ( states ) : raise Value Error ( 'Argument `%s` was a list of different length ({}) than ' '`states` ({})' . format ( name , len ( states ) ) ) else : secondary arg = [ secondary arg ] * len ( states ) return secondary arg", "predictions": ["broadcast the given argument to the given arguments ."], "references": ["broadcast a listable secondary_arg to that of states ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 272, "code": "def remove ( self , field ) : return Mapping ( x = None if field == \"x\" else self . x , y = None if field == \"y\" else self . y , ildj = self . ildj , kwargs = self . kwargs )", "predictions": ["removes the mapping at the specified index ."], "references": ["to support weak referencing removes cache key from the cache value ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 273, "code": "def merge ( self , old , new , use equals = False ) : if old is None : return new if new is None : return old if ( old == new ) if use equals else ( old is new ) : return old raise Value Error ( \"Incompatible values: %s != %s\" % ( old , new ) )", "predictions": ["merges a number of purges . this is called on each object of another one ."], "references": ["helper to merge which handles merging one value ."], "bleu": 0.08513012360883544, "rouge_l": 0.16850828729281767}
{"id": 274, "code": "def deep tuple ( self , x ) : if isinstance ( x , dict ) : return self . deep tuple ( tuple ( sorted ( x . items ( ) ) ) ) elif isinstance ( x , ( list , tuple ) ) : return tuple ( map ( self . deep tuple , x ) ) return x", "predictions": ["transforms this message by the specified tuple of this tuple . each tuple may be then returned ."], "references": ["converts nested tuple list or dict to nested tuple ."], "bleu": 0.09629943614188137, "rouge_l": 0.2259259259259259}
{"id": 275, "code": "def vggconv block ( x , filters , kernel , stride , kernel posterior fn ) : out = tfp . layers . Convolution2D Flipout ( filters , kernel , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) out = tf . keras . layers . Batch Normalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tfp . layers . Convolution2D Flipout ( filters , kernel , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( out ) out = tf . keras . layers . Batch Normalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tf . keras . layers . Max Pooling2D ( pool size = ( 2 , 2 ) , strides = stride ) ( out ) return out", "predictions": ["the vggconv . . ( x , y ) returns an array of parameters ."], "references": ["network block for vgg ."], "bleu": 0.08225964699966554, "rouge_l": 0.10990990990990988}
{"id": 276, "code": "def embed no none gradient check ( value and gradients fn ) : @ functools . wraps ( value and gradients fn ) def func wrapped ( * args , * * kwargs ) : \"\"\"Wrapped function which checks for None gradients.\"\"\" value , grads = value and gradients fn ( * args , * * kwargs ) if any ( grad is None for grad in grads ) : raise Value Error ( \"Gradient is None for a state.\" ) return value , grads return func wrapped", "predictions": ["embed the gradient of a gradient with respect to a none ."], "references": ["wraps value and gradients function to assist with none gradients ."], "bleu": 0.13065113298388567, "rouge_l": 0.2629310344827586}
{"id": 277, "code": "def has no u turn ( state one , state two , momentum ) : dot product = sum ( [ tf . reduce sum ( input tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state one , state two , momentum ) ] ) return dot product > 0", "predictions": ["checks if there are one dot product of the dot product ."], "references": ["if two given states and momentum do not exhibit a u - turn pattern ."], "bleu": 0.08955242946910898, "rouge_l": 0.14523809523809522}
{"id": 278, "code": "def leapfrog ( value and gradients fn , current state , current grads target log prob , current momentum , step size ) : mid momentum = [ m + 0.5 * step * g for m , step , g in zip ( current momentum , step size , current grads target log prob ) ] next state = [ s + step * m for s , step , m in zip ( current state , step size , mid momentum ) ] next target log prob , next grads target log prob = value and gradients fn ( * next state ) next momentum = [ m + 0.5 * step * g for m , step , g in zip ( mid momentum , step size , next grads target log prob ) ] return [ next state , next target log prob , next grads target log prob , next momentum , ]", "predictions": ["compute the log of all tasks based on the value of the distribution ."], "references": ["runs one step of leapfrog integration ."], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 279, "code": "def log joint ( current target log prob , current momentum ) : momentum log prob = - sum ( [ tf . reduce sum ( input tensor = 0.5 * ( m ** 2. ) ) for m in current momentum ] ) return current target log prob + momentum log prob", "predictions": ["log the current probability distribution ."], "references": ["log - joint probability given a state s log - probability and momentum ."], "bleu": 0.06924459302580939, "rouge_l": 0.2798165137614679}
{"id": 280, "code": "def random bernoulli ( shape , probs , dtype = tf . int32 , seed = None , name = None ) : with tf . compat . v1 . name scope ( name , \"random bernoulli\" , [ shape , probs ] ) : probs = tf . convert to tensor ( value = probs ) random uniform = tf . random . uniform ( shape , dtype = probs . dtype , seed = seed ) return tf . cast ( tf . less ( random uniform , probs ) , dtype )", "predictions": ["create a random . ."], "references": ["returns samples from a bernoulli distribution ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 281, "code": "def expand as args ( args ) : return ( isinstance ( args , collections . Sequence ) and not is namedtuple ( args ) and not force leaf ( args ) )", "predictions": ["expand or updates a namedtuple as a namedtuple of the arguments ."], "references": ["returns true if args should be expanded as * args ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 282, "code": "def nested convert to tensor ( struct , dtype = None , name = None ) : if dtype is not None or not tf . nest . is nested ( struct ) : return tf . convert to tensor ( struct , dtype = dtype ) if maybe convertible to tensor ( struct ) : try : return tf . convert to tensor ( value = struct , name = name ) except ( Value Error , Type Error ) : pass shallow struct = get shallow structure ( struct ) return nest . map structure up to ( shallow struct , lambda s : nested convert to tensor ( s , name = name ) , struct )", "predictions": ["convert a nested ( into a tensor and shallow tensor ."], "references": ["eagerly converts struct to tensor recursing upon failure ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 283, "code": "def get tensor like attributes ( ) : attrs = dict ( ) attrs . update ( ( attr , wrap method ( tf . Tensor , attr ) ) for attr in tf . Tensor . OVERLOADABLE OPERATORS . union ( { ' iter ' } ) ) attrs . update ( ( attr , getattr ( tf . Tensor , attr ) ) for attr in { ' nonzero ' , ' bool ' , ' array priority ' } ) return attrs", "predictions": ["creates the union of all ( ."], "references": ["returns tensor attributes related to shape and python builtins ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 284, "code": "def pack images ( images , rows , cols ) : shape = tf . shape ( input = images ) width = shape [ - 3 ] height = shape [ - 2 ] depth = shape [ - 1 ] images = tf . reshape ( images , ( - 1 , width , height , depth ) ) batch = tf . shape ( input = images ) [ 0 ] rows = tf . minimum ( rows , batch ) cols = tf . minimum ( batch // rows , cols ) images = images [ : rows * cols ] images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) images = tf . transpose ( a = images , perm = [ 0 , 2 , 1 , 3 , 4 ] ) images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) return images", "predictions": ["pack images into a single batch of images ."], "references": ["helper utility to make a field of images ."], "bleu": 0.2777619034011791, "rouge_l": 0.4444444444444444}
{"id": 285, "code": "def download ( directory , filename ) : filepath = os . path . join ( directory , filename ) if tf . io . gfile . exists ( filepath ) : return filepath if not tf . io . gfile . exists ( directory ) : tf . io . gfile . makedirs ( directory ) url = os . path . join ( ROOT PATH , filename ) print ( \"Downloading %s to %s\" % ( url , filepath ) ) urllib . request . urlretrieve ( url , filepath ) return filepath", "predictions": ["download a directory recursively ."], "references": ["downloads a file ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 286, "code": "def build fake input fns ( batch size ) : random sample = np . random . rand ( batch size , * IMAGE SHAPE ) . astype ( \"float32\" ) def train input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch size ) . repeat ( ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) def eval input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) return train input fn , eval input fn", "predictions": ["build a fake batch on the input ."], "references": ["builds fake mnist - style data for unit testing ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 287, "code": "def build input fns ( data dir , batch size ) : def train input fn ( ) : dataset = static mnist dataset ( data dir , \"train\" ) dataset = dataset . shuffle ( 50000 ) . repeat ( ) . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) def eval input fn ( ) : eval dataset = static mnist dataset ( data dir , \"valid\" ) eval dataset = eval dataset . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( eval dataset ) . get next ( ) return train input fn , eval input fn", "predictions": ["build a train for a batch of tests ."], "references": ["builds an iterator switching between train and heldout data ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 288, "code": "def validate block sizes ( block sizes , bijectors , validate args ) : block sizes shape = block sizes . shape if tensorshape util . is fully defined ( block sizes shape ) : if ( tensorshape util . rank ( block sizes shape ) != 1 or ( tensorshape util . num elements ( block sizes shape ) != len ( bijectors ) ) ) : raise Value Error ( '`block sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block sizes shape , len ( bijectors ) ) ) return block sizes elif validate args : message = ( '`block sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) with tf . control dependencies ( [ assert util . assert equal ( tf . size ( input = block sizes ) , len ( bijectors ) , message = message ) , assert util . assert equal ( tf . rank ( block sizes ) , 1 ) ] ) : return tf . identity ( block sizes ) else : return block sizes", "predictions": ["text must be a valid messages . this function performs the ( = true if the messages are empty and the other is set ."], "references": ["helper to validate block sizes ."], "bleu": 0.048589719316429775, "rouge_l": 0.07253269916765755}
{"id": 289, "code": "def maybe check wont broadcast ( flat xs , validate args ) : flat xs = tuple ( flat xs ) if not validate args : return flat xs msg = 'Broadcasting probably indicates an error in model specification.' s = tuple ( x . shape for x in flat xs ) if all ( tensorshape util . is fully defined ( s ) for s in s ) : if not all ( a == b for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ) : raise Value Error ( msg ) return flat xs assertions = [ assert util . assert equal ( a , b , message = msg ) for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ] with tf . control dependencies ( assertions ) : return tuple ( tf . identity ( x ) for x in flat xs )", "predictions": ["verifies that all the dependencies of the same inputs have been matched ."], "references": ["verifies that parts don t broadcast ."], "bleu": 0.1350862565735141, "rouge_l": 0.31715771230502604}
{"id": 290, "code": "def maybe call volatility fn and grads ( volatility fn , state , volatility fn results = None , grads volatility fn = None , sample shape = None , parallel iterations = 10 ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] needs volatility fn gradients = grads volatility fn is None if volatility fn results is None : volatility fn results = volatility fn ( * state parts ) volatility fn results = ( list ( volatility fn results ) if mcmc util . is list like ( volatility fn results ) else [ volatility fn results ] ) if len ( volatility fn results ) == 1 : volatility fn results *= len ( state parts ) if len ( state parts ) != len ( volatility fn results ) : raise Value Error ( '`volatility fn` should return a tensor or a list ' 'of the same length as `current state`.' ) volatility fn results = maybe broadcast volatility ( volatility fn results , state parts ) if grads volatility fn is None : [ , grads volatility fn , ] = diag jacobian ( xs = state parts , ys = volatility fn results , sample shape = sample shape , parallel iterations = parallel iterations , fn = volatility fn ) if needs volatility fn gradients : grads volatility fn = [ 2. * g * volatility if g is not None else tf . zeros like ( fn arg , dtype = fn arg . dtype . base dtype ) for g , volatility , fn arg in zip ( grads volatility fn , volatility fn results , state parts ) ] return volatility fn results , grads volatility fn", "predictions": ["covariance matrix using ( ."], "references": ["helper which computes volatility_fn results and grads if needed ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 291, "code": "def maybe broadcast volatility ( volatility parts , state parts ) : return [ v + tf . zeros like ( sp , dtype = sp . dtype . base dtype ) for v , sp in zip ( volatility parts , state parts ) ]", "predictions": ["broadcasts an array of , given a given repr of , i . e . , the resulting repr should be a long ."], "references": ["helper to broadcast volatility_parts to the shape of state_parts ."], "bleu": 0.06024757292375468, "rouge_l": 0.12708333333333333}
{"id": 292, "code": "def prepare args ( target log prob fn , volatility fn , state , step size , target log prob = None , grads target log prob = None , volatility = None , grads volatility fn = None , diffusion drift = None , parallel iterations = 10 ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] [ target log prob , grads target log prob , ] = mcmc util . maybe call fn and grads ( target log prob fn , state parts , target log prob , grads target log prob ) [ volatility parts , grads volatility , ] = maybe call volatility fn and grads ( volatility fn , state parts , volatility , grads volatility fn , distribution util . prefer static shape ( target log prob ) , parallel iterations ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) if diffusion drift is None : diffusion drift parts = get drift ( step sizes , volatility parts , grads volatility , grads target log prob ) else : diffusion drift parts = ( list ( diffusion drift ) if mcmc util . is list like ( diffusion drift ) else [ diffusion drift ] ) if len ( state parts ) != len ( diffusion drift ) : raise Value Error ( 'There should be exactly one `diffusion drift` or it ' 'should have same length as list-like `current state`.' ) return [ state parts , step sizes , target log prob , grads target log prob , volatility parts , grads volatility , diffusion drift parts , ]", "predictions": ["prepares the ) ) for the specified ( using the specified ( and ( ."], "references": ["helper which processes input args to meet list - like assumptions ."], "bleu": 0.08225964699966554, "rouge_l": 0.07558859975216851}
{"id": 293, "code": "def validate init args statically ( distribution , batch shape ) : if tensorshape util . rank ( batch shape . shape ) is not None : if tensorshape util . rank ( batch shape . shape ) != 1 : raise Value Error ( \"`batch shape` must be a vector \" \"(saw rank: {}).\" . format ( tensorshape util . rank ( batch shape . shape ) ) ) batch shape static = tensorshape util . constant value as shape ( batch shape ) batch size static = tensorshape util . num elements ( batch shape static ) dist batch size static = tensorshape util . num elements ( distribution . batch shape ) if batch size static is not None and dist batch size static is not None : if batch size static != dist batch size static : raise Value Error ( \"`batch shape` size ({}) must match \" \"`distribution.batch shape` size ({}).\" . format ( batch size static , dist batch size static ) ) if tensorshape util . dims ( batch shape static ) is not None : if any ( tf . compat . dimension value ( dim ) is not None and tf . compat . dimension value ( dim ) < 1 for dim in batch shape static ) : raise Value Error ( \"`batch shape` elements must be >=-1.\" )", "predictions": ["performs a is valid . this helper method must be called after ( : ( . ( : ( . ( : ( . ( . ( . ( : ( . ( . ( . ( . ( . ( : ( . ( . ( : ( ."], "references": ["helper to __init__ which makes or raises assertions ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 294, "code": "def sample shape ( self , x ) : x ndims = ( tf . rank ( x ) if tensorshape util . rank ( x . shape ) is None else tensorshape util . rank ( x . shape ) ) event ndims = ( tf . size ( input = self . event shape tensor ( ) ) if tensorshape util . rank ( self . event shape ) is None else tensorshape util . rank ( self . event shape ) ) batch ndims = ( tf . size ( input = self . batch shape unexpanded ) if tensorshape util . rank ( self . batch shape ) is None else tensorshape util . rank ( self . batch shape ) ) sample ndims = x ndims - batch ndims - event ndims if isinstance ( sample ndims , int ) : static sample shape = x . shape [ : sample ndims ] else : static sample shape = tf . Tensor Shape ( None ) if tensorshape util . is fully defined ( static sample shape ) : sample shape = np . int32 ( static sample shape ) else : sample shape = tf . shape ( input = x ) [ : sample ndims ] return sample shape , static sample shape", "predictions": ["numpy < p > this is an efficient numpy array , i . e . numpy . numpy < if this numpy is not a numpy , this is a bit with the same shape and the new one ."], "references": ["computes graph and static sample_shape ."], "bleu": 0.03333168744598931, "rouge_l": 0.10032894736842105}
{"id": 295, "code": "def call reshape input output ( self , fn , x , extra kwargs = None ) : with tf . control dependencies ( self . runtime assertions + self . validate sample arg ( x ) ) : sample shape , static sample shape = self . sample shape ( x ) old shape = tf . concat ( [ sample shape , self . distribution . batch shape tensor ( ) , self . event shape tensor ( ) , ] , axis = 0 ) x reshape = tf . reshape ( x , old shape ) result = fn ( x reshape , * * extra kwargs ) if extra kwargs else fn ( x reshape ) new shape = tf . concat ( [ sample shape , self . batch shape unexpanded , ] , axis = 0 ) result = tf . reshape ( result , new shape ) if ( tensorshape util . rank ( static sample shape ) is not None and tensorshape util . rank ( self . batch shape ) is not None ) : new shape = tensorshape util . concatenate ( static sample shape , self . batch shape ) tensorshape util . set shape ( result , new shape ) return result", "predictions": ["finish the specified ( norm normal normal command ."], "references": ["calls fn appropriately reshaping its input x and output ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 296, "code": "def call and reshape output ( self , fn , event shape list = None , static event shape list = None , extra kwargs = None ) : with tf . control dependencies ( self . runtime assertions ) : if event shape list is None : event shape list = [ self . event shape tensor ( ) ] if static event shape list is None : static event shape list = [ self . event shape ] new shape = tf . concat ( [ self . batch shape unexpanded ] + event shape list , axis = 0 ) result = tf . reshape ( fn ( * * extra kwargs ) if extra kwargs else fn ( ) , new shape ) if ( tensorshape util . rank ( self . batch shape ) is not None and tensorshape util . rank ( self . event shape ) is not None ) : event shape = tf . Tensor Shape ( [ ] ) for rss in static event shape list : event shape = tensorshape util . concatenate ( event shape , rss ) static shape = tensorshape util . concatenate ( self . batch shape , event shape ) tensorshape util . set shape ( result , static shape ) return result", "predictions": ["activated args ."], "references": ["calls fn and appropriately reshapes its output ."], "bleu": 0.10148002183214462, "rouge_l": 0.1680440771349862}
{"id": 297, "code": "def validate sample arg ( self , x ) : with tf . name scope ( \"validate sample arg\" ) : x ndims = ( tf . rank ( x ) if tensorshape util . rank ( x . shape ) is None else tensorshape util . rank ( x . shape ) ) event ndims = ( tf . size ( input = self . event shape tensor ( ) ) if tensorshape util . rank ( self . event shape ) is None else tensorshape util . rank ( self . event shape ) ) batch ndims = ( tf . size ( input = self . batch shape unexpanded ) if tensorshape util . rank ( self . batch shape ) is None else tensorshape util . rank ( self . batch shape ) ) expected batch event ndims = batch ndims + event ndims if ( isinstance ( x ndims , int ) and isinstance ( expected batch event ndims , int ) ) : if x ndims < expected batch event ndims : raise Not Implemented Error ( \"Broadcasting is not supported; too few batch and event dims \" \"(expected at least {}, saw {}).\" . format ( expected batch event ndims , x ndims ) ) ndims assertion = [ ] elif self . validate args : ndims assertion = [ assert util . assert greater equal ( x ndims , expected batch event ndims , message = ( \"Broadcasting is not supported; too few \" \"batch and event dims.\" ) , name = \"assert batch and event ndims large enough\" ) , ] if ( tensorshape util . is fully defined ( self . batch shape ) and tensorshape util . is fully defined ( self . event shape ) ) : expected batch event shape = np . int32 ( tensorshape util . concatenate ( self . batch shape , self . event shape ) ) else : expected batch event shape = tf . concat ( [ self . batch shape tensor ( ) , self . event shape tensor ( ) , ] , axis = 0 ) sample ndims = x ndims - expected batch event ndims if isinstance ( sample ndims , int ) : sample ndims = max ( sample ndims , 0 ) if ( isinstance ( sample ndims , int ) and tensorshape util . is fully defined ( x . shape [ sample ndims : ] ) ) : actual batch event shape = np . int32 ( x . shape [ sample ndims : ] ) else : sample ndims = tf . maximum ( sample ndims , 0 ) actual batch event shape = tf . shape ( input = x ) [ sample ndims : ] if ( isinstance ( expected batch event shape , np . ndarray ) and isinstance ( actual batch event shape , np . ndarray ) ) : if any ( expected batch event shape != actual batch event shape ) : raise Not Implemented Error ( \"Broadcasting is not supported; \" \"unexpected batch and event shape \" \"(expected {}, saw {}).\" . format ( expected batch event shape , actual batch event shape ) ) runtime assertions = ndims assertion elif self . validate args : with tf . control dependencies ( ndims assertion ) : shape assertion = assert util . assert equal ( expected batch event shape , actual batch event shape , message = ( \"Broadcasting is not supported; \" \"unexpected batch and event shape.\" ) , name = \"assert batch and event shape same\" ) runtime assertions = [ shape assertion ] else : runtime assertions = [ ] return runtime assertions", "predictions": ["validates if this summary is valid . this is a call to ensure that the summary is assigned to the summary ."], "references": ["helper which validates sample arg e . g . input to log_prob ."], "bleu": 0.06964541799727335, "rouge_l": 0.23968565815324167}
{"id": 298, "code": "def maybe assert valid sample ( self , counts ) : if not self . validate args : return counts counts = distribution util . embed check nonnegative integer form ( counts ) return distribution util . with dependencies ( [ assert util . assert less equal ( counts , self . total count , message = \"counts are not less than or equal to n.\" ) , ] , counts )", "predictions": ["check that two objects can be compared ."], "references": ["check counts for proper shape values then return tensor version ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 299, "code": "def flat sample distributions ( self , sample shape = ( ) , seed = None , value = None ) : ds = [ ] values out = [ ] seed = seed stream . Seed Stream ( 'Joint Distribution Coroutine' , seed ) gen = self . model ( ) index = 0 d = next ( gen ) try : while True : actual distribution = d . distribution if isinstance ( d , self . Root ) else d ds . append ( actual distribution ) if ( value is not None and len ( value ) > index and value [ index ] is not None ) : seed ( ) next value = value [ index ] else : next value = actual distribution . sample ( sample shape = sample shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) values out . append ( next value ) index += 1 d = gen . send ( next value ) except Stop Iteration : pass return ds , values out", "predictions": ["pdf - pdf de - modularity at the given at the given at the given at the given at the given at the given at the given at the given - level . pdf ( - : [ + - 1 , 3 , 10 . - - 180 -"], "references": ["executes model creating both samples and distributions ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 300, "code": "def newsgroups dataset ( directory , split name , num words , shuffle and repeat ) : data = np . load ( download ( directory , FILE TEMPLATE . format ( split = split name ) ) ) data = data [ : - 1 ] num documents = data . shape [ 0 ] indices = np . array ( [ ( row idx , column idx ) for row idx , row in enumerate ( data ) for column idx in row ] ) sparse matrix = scipy . sparse . coo matrix ( ( np . ones ( indices . shape [ 0 ] ) , ( indices [ : , 0 ] , indices [ : , 1 ] ) ) , shape = ( num documents , num words ) , dtype = np . float32 ) sparse matrix = sparse matrix . tocsr ( ) dataset = tf . data . Dataset . range ( num documents ) if shuffle and repeat : dataset = dataset . shuffle ( num documents ) . repeat ( ) def get row py func ( idx ) : def get row python ( idx py ) : return np . squeeze ( np . array ( sparse matrix [ idx py ] . todense ( ) ) , axis = 0 ) py func = tf . compat . v1 . py func ( get row python , [ idx ] , tf . float32 , stateful = False ) py func . set shape ( ( num words , ) ) return py func dataset = dataset . map ( get row py func ) return dataset", "predictions": ["v1 v1 . . . . . ."], "references": ["20 newsgroups as a tf . data . dataset ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 301, "code": "def build fake input fns ( batch size ) : num words = 1000 vocabulary = [ str ( i ) for i in range ( num words ) ] random sample = np . random . randint ( 10 , size = ( batch size , num words ) ) . astype ( np . float32 ) def train input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) dataset = dataset . batch ( batch size ) . repeat ( ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) def eval input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) dataset = dataset . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) return train input fn , eval input fn , vocabulary", "predictions": ["potential function for the ( process ."], "references": ["builds fake data for unit testing ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 302, "code": "def load bernoulli mnist dataset ( directory , split name ) : amat file = download ( directory , FILE TEMPLATE . format ( split = split name ) ) dataset = tf . data . Text Line Dataset ( amat file ) str to arr = lambda string : np . array ( [ c == b\"1\" for c in string . split ( ) ] ) def parser ( s ) : booltensor = tf . compat . v1 . py func ( str to arr , [ s ] , tf . bool ) reshaped = tf . reshape ( booltensor , [ 28 , 28 , 1 ] ) return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( 0 , tf . int32 ) return dataset . map ( parser )", "predictions": ["loads the ( and their corresponding ( as active if they are as a size if the x is found if the x is not affected if the x and y are ignored if the x is ignored if the x is thrown ."], "references": ["returns hugo larochelle s binary static mnist tf . data . dataset ."], "bleu": 0.027347130611442165, "rouge_l": 0.0389030612244898}
{"id": 303, "code": "def build input pipeline ( data dir , batch size , heldout size , mnist type ) : if mnist type in [ Mnist Type . FAKE DATA , Mnist Type . THRESHOLD ] : if mnist type == Mnist Type . FAKE DATA : mnist data = build fake data ( ) else : mnist data = mnist . read data sets ( data dir ) training dataset = tf . data . Dataset . from tensor slices ( ( mnist data . train . images , np . int32 ( mnist data . train . labels ) ) ) heldout dataset = tf . data . Dataset . from tensor slices ( ( mnist data . validation . images , np . int32 ( mnist data . validation . labels ) ) ) elif mnist type == Mnist Type . BERNOULLI : training dataset = load bernoulli mnist dataset ( data dir , \"train\" ) heldout dataset = load bernoulli mnist dataset ( data dir , \"valid\" ) else : raise Value Error ( \"Unknown MNIST type.\" ) training batches = training dataset . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) heldout frozen = ( heldout dataset . take ( heldout size ) . repeat ( ) . batch ( heldout size ) ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout frozen ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) images = tf . reshape ( images , shape = [ - 1 ] + IMAGE SHAPE ) if mnist type in [ Mnist Type . FAKE DATA , Mnist Type . THRESHOLD ] : images = tf . cast ( images > 0.5 , dtype = tf . int32 ) return images , labels , handle , training iterator , heldout iterator", "predictions": ["broadcast the batches for all datasets ."], "references": ["builds an iterator switching between train and heldout data ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 304, "code": "def as numpy dtype ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'as numpy dtype' ) : return dtype . as numpy dtype return dtype", "predictions": ["convert a ( possibly null field field field field / self field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field field"], "references": ["returns a np . dtype based on this dtype ."], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 305, "code": "def base dtype ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'base dtype' ) : return dtype . base dtype return dtype", "predictions": ["instantiates a merge and returns it ."], "references": ["returns a non - reference dtype based on this dtype ."], "bleu": 0.1247439242120089, "rouge_l": 0.2136602451838879}
{"id": 306, "code": "def is bool ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'is bool' ) : return dtype . is bool return np . dtype ( dtype ) . kind == 'b'", "predictions": ["return true if the provided self and is a boolean array ."], "references": ["returns whether this is a boolean data type ."], "bleu": 0.2044800736021839, "rouge_l": 0.39102564102564097}
{"id": 307, "code": "def is complex ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'is complex' ) : return dtype . is complex return np . issubdtype ( np . dtype ( dtype ) , np . complex )", "predictions": ["test if a x is block ."], "references": ["returns whether this is a complex floating point type ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 308, "code": "def max ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'max' ) : return dtype . max use finfo = is floating ( dtype ) or is complex ( dtype ) return np . finfo ( dtype ) . max if use finfo else np . iinfo ( dtype ) . max", "predictions": ["determine if the specified none is equivalent to the baseline of the specified none ."], "references": ["returns the maximum representable value in this data type ."], "bleu": 0.09103526405546068, "rouge_l": 0.1659863945578231}
{"id": 309, "code": "def name ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'name' ) : return dtype . name if hasattr ( dtype , ' name ' ) : return dtype . name return str ( dtype )", "predictions": ["build an empty has the same type and all its ones ."], "references": ["returns the string name for this dtype ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 310, "code": "def size ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'size' ) : return dtype . size return np . dtype ( dtype ) . itemsize", "predictions": ["instantiates the leapfrog data ."], "references": ["returns the number of bytes to represent this dtype ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 311, "code": "def nelder mead one step ( current simplex , current objective values , objective function = None , dim = None , func tolerance = None , position tolerance = None , batch evaluate objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : with tf . compat . v1 . name scope ( name , 'nelder mead one step' ) : domain dtype = current simplex . dtype . base dtype order = tf . argsort ( current objective values , direction = 'ASCENDING' , stable = True ) ( best index , worst index , second worst index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] worst vertex = current simplex [ worst index ] ( best objective value , worst objective value , second worst objective value ) = ( current objective values [ best index ] , current objective values [ worst index ] , current objective values [ second worst index ] ) face centroid = tf . reduce sum ( input tensor = current simplex , axis = 0 ) - worst vertex face centroid /= tf . cast ( dim , domain dtype ) reflected = face centroid + reflection * ( face centroid - worst vertex ) objective at reflected = objective function ( reflected ) num evaluations = 1 has converged = check convergence ( current simplex , current simplex [ best index ] , best objective value , worst objective value , func tolerance , position tolerance ) def converged fn ( ) : return ( True , current simplex , current objective values , 0 ) case0 = has converged , converged fn accept reflected = ( ( objective at reflected < second worst objective value ) & ( objective at reflected >= best objective value ) ) accept reflected fn = accept reflected fn ( current simplex , current objective values , worst index , reflected , objective at reflected ) case1 = accept reflected , accept reflected fn do expansion = objective at reflected < best objective value expansion fn = expansion fn ( objective function , current simplex , current objective values , worst index , reflected , objective at reflected , face centroid , expansion ) case2 = do expansion , expansion fn do outside contraction = ( ( objective at reflected < worst objective value ) & ( objective at reflected >= second worst objective value ) ) outside contraction fn = outside contraction fn ( objective function , current simplex , current objective values , face centroid , best index , worst index , reflected , objective at reflected , contraction , shrinkage , batch evaluate objective ) case3 = do outside contraction , outside contraction fn default fn = inside contraction fn ( objective function , current simplex , current objective values , face centroid , best index , worst index , worst objective value , contraction , shrinkage , batch evaluate objective ) ( converged , next simplex , next objective at simplex , case evals ) = prefer static . case ( [ case0 , case1 , case2 , case3 ] , default = default fn , exclusive = False ) next simplex . set shape ( current simplex . shape ) next objective at simplex . set shape ( current objective values . shape ) return ( converged , next simplex , next objective at simplex , num evaluations + case evals )", "predictions": ["log a new current current current current current current current current current current current current current sync position ."], "references": ["a single iteration of the nelder mead algorithm ."], "bleu": 0.0712695567709093, "rouge_l": 0.15269086357947434}
{"id": 312, "code": "def accept reflected fn ( simplex , objective values , worst index , reflected , objective at reflected ) : def replace worst with reflected ( ) : next simplex = replace at index ( simplex , worst index , reflected ) next objective values = replace at index ( objective values , worst index , objective at reflected ) return False , next simplex , next objective values , 0 return replace worst with reflected", "predictions": ["random ( ( onto the provided = = = = = dtype , shape ."], "references": ["creates the condition function pair for a reflection to be accepted ."], "bleu": 0.09103526405546068, "rouge_l": 0.15117719950433703}
{"id": 313, "code": "def expansion fn ( objective function , simplex , objective values , worst index , reflected , objective at reflected , face centroid , expansion ) : def expand and maybe replace ( ) : \"\"\"Performs the expansion step.\"\"\" expanded = face centroid + expansion * ( reflected - face centroid ) expanded objective value = objective function ( expanded ) expanded is better = ( expanded objective value < objective at reflected ) accept expanded fn = lambda : ( expanded , expanded objective value ) accept reflected fn = lambda : ( reflected , objective at reflected ) next pt , next objective value = prefer static . cond ( expanded is better , accept expanded fn , accept reflected fn ) next simplex = replace at index ( simplex , worst index , next pt ) next objective at simplex = replace at index ( objective values , worst index , next objective value ) return False , next simplex , next objective at simplex , 1 return expand and maybe replace", "predictions": ["the inverse of ( that has more ( but does not fill ( with the arguments of the ) . this is more efficient than the arguments of the ( ."], "references": ["creates the condition function pair for an expansion ."], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 314, "code": "def outside contraction fn ( objective function , simplex , objective values , face centroid , best index , worst index , reflected , objective at reflected , contraction , shrinkage , batch evaluate objective ) : def contraction ( ) : \"\"\"Performs a contraction.\"\"\" contracted = face centroid + contraction * ( reflected - face centroid ) objective at contracted = objective function ( contracted ) is contracted acceptable = objective at contracted <= objective at reflected def accept contraction ( ) : next simplex = replace at index ( simplex , worst index , contracted ) objective at next simplex = replace at index ( objective values , worst index , objective at contracted ) return ( False , next simplex , objective at next simplex , 1 ) def reject contraction ( ) : return shrink towards best ( objective function , simplex , best index , shrinkage , batch evaluate objective ) return prefer static . cond ( is contracted acceptable , accept contraction , reject contraction ) return contraction", "predictions": ["performs a ) convert the given ( to a ) ."], "references": ["creates the condition function pair for an outside contraction ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 315, "code": "def shrink towards best ( objective function , simplex , best index , shrinkage , batch evaluate objective ) : best vertex = simplex [ best index ] shrunk simplex = best vertex + shrinkage * ( simplex - best vertex ) objective at shrunk simplex , evals = evaluate objective multiple ( objective function , shrunk simplex , batch evaluate objective ) return ( False , shrunk simplex , objective at shrunk simplex , evals )", "predictions": ["get a like ( with the given attrs ."], "references": ["shrinks the simplex around the best vertex ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 316, "code": "def replace at index ( x , index , replacement ) : x new = tf . concat ( [ x [ : index ] , tf . expand dims ( replacement , axis = 0 ) , x [ ( index + 1 ) : ] ] , axis = 0 ) return x new", "predictions": ["perform a pack operation on the given parameterization and returns the result ."], "references": ["replaces an element at supplied index ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 317, "code": "def prepare args with initial simplex ( objective function , initial simplex , objective at initial simplex , batch evaluate objective ) : initial simplex = tf . convert to tensor ( value = initial simplex ) num vertices = tf . shape ( input = initial simplex ) [ 0 ] dim = num vertices - 1 num evaluations = 0 if objective at initial simplex is None : objective at initial simplex , n evals = evaluate objective multiple ( objective function , initial simplex , batch evaluate objective ) num evaluations += n evals objective at initial simplex = tf . convert to tensor ( value = objective at initial simplex ) return ( dim , num vertices , initial simplex , objective at initial simplex , num evaluations )", "predictions": ["prepares the arguments for use with the arguments . all issues with the arguments ."], "references": ["evaluates the objective function at the specified initial simplex ."], "bleu": 0.09782375748961449, "rouge_l": 0.2489795918367347}
{"id": 318, "code": "def prepare args with initial vertex ( objective function , initial vertex , step sizes , objective at initial vertex , batch evaluate objective ) : dim = tf . size ( input = initial vertex ) num vertices = dim + 1 unit vectors along axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial vertex . dtype . base dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial vertex ) ] , axis = 0 ) ) simplex face = initial vertex + step sizes * unit vectors along axes simplex = tf . concat ( [ tf . expand dims ( initial vertex , axis = 0 ) , simplex face ] , axis = 0 ) num evaluations = 0 if objective at initial vertex is None : objective at initial vertex = objective function ( initial vertex ) num evaluations += 1 objective at simplex face , num evals = evaluate objective multiple ( objective function , simplex face , batch evaluate objective ) num evaluations += num evals objective at simplex = tf . concat ( [ tf . expand dims ( objective at initial vertex , axis = 0 ) , objective at simplex face ] , axis = 0 ) return ( dim , num vertices , simplex , objective at simplex , num evaluations )", "predictions": [". all possible shot elements . < p > note that we have a simple iterator , as well as any shot points , so we have to map the shot ( rand rand rand rand rand rand rand rand rand rand rand rand rand rand rand rand rand rand"], "references": ["constructs a standard axes aligned simplex ."], "bleu": 0.026594139297659906, "rouge_l": 0.04061251664447404}
{"id": 319, "code": "def build input pipeline ( mnist data , batch size , heldout size ) : training dataset = tf . data . Dataset . from tensor slices ( ( mnist data . train . images , np . int32 ( mnist data . train . labels ) ) ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) heldout dataset = tf . data . Dataset . from tensor slices ( ( mnist data . validation . images , np . int32 ( mnist data . validation . labels ) ) ) heldout frozen = ( heldout dataset . take ( heldout size ) . repeat ( ) . batch ( heldout size ) ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout frozen ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) return images , labels , handle , training iterator , heldout iterator", "predictions": ["build the data for the data set ."], "references": ["build an iterator switching between train and heldout data ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 320, "code": "def build fake data ( num examples = 10 ) : class Dummy ( object ) : pass num examples = 10 mnist data = Dummy ( ) mnist data . train = Dummy ( ) mnist data . train . images = np . float32 ( np . random . randn ( num examples , * IMAGE SHAPE ) ) mnist data . train . labels = np . int32 ( np . random . permutation ( np . arange ( num examples ) ) ) mnist data . train . num examples = num examples mnist data . validation = Dummy ( ) mnist data . validation . images = np . float32 ( np . random . randn ( num examples , * IMAGE SHAPE ) ) mnist data . validation . labels = np . int32 ( np . random . permutation ( np . arange ( num examples ) ) ) mnist data . validation . num examples = num examples return mnist data", "predictions": ["generate mnist data for the trains set ."], "references": ["build fake mnist - style data for unit testing ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 321, "code": "def get config ( self ) : return { 'initializers' : [ tf . compat . v2 . initializers . serialize ( tf . keras . initializers . get ( init ) ) for init in self . initializers ] , 'sizes' : self . sizes , 'validate args' : self . validate args , }", "predictions": ["makes the represenation method call ."], "references": ["returns initializer configuration as a json - serializable dict ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 322, "code": "def from config ( cls , config ) : return cls ( * * { 'initializers' : [ tf . compat . v2 . initializers . deserialize ( init ) for init in config . get ( 'initializers' , [ ] ) ] , 'sizes' : config . get ( 'sizes' , [ ] ) , 'validate args' : config . get ( 'validate args' , False ) , } )", "predictions": ["read all configuration from ( and return the result ."], "references": ["instantiates an initializer from a configuration dictionary ."], "bleu": 0.14991106946711685, "rouge_l": 0.22676579925650556}
{"id": 323, "code": "def matmul ( a , b , transpose a = False , transpose b = False , adjoint a = False , adjoint b = False , a is sparse = False , b is sparse = False , name = None ) : if a is sparse or b is sparse : raise Not Implemented Error ( 'Numpy backend does not support sparse matmul.' ) if transpose a or adjoint a : a = matrix transpose ( a , conjugate = adjoint a ) if transpose b or adjoint b : b = matrix transpose ( b , conjugate = adjoint b ) return np . matmul ( a , b )", "predictions": ["returns the matmul or the quotient of this a or sparse matrix ."], "references": ["numpy matmul wrapper ."], "bleu": 0.10571070857151538, "rouge_l": 0.2601279317697228}
{"id": 324, "code": "def std var helper ( self , statistic , statistic name , statistic ndims , df factor fn ) : df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic ndims ] , dtype = tf . int32 ) ] , - 1 ) ) df = broadcast to shape ( df , tf . shape ( input = statistic ) ) denom = tf . where ( df > 2. , df - 2. , tf . ones like ( df ) ) statistic = statistic * df factor fn ( df / denom ) inf = dtype util . as numpy dtype ( self . dtype ) ( np . inf ) result where defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = \"inf\" ) ) if self . allow nan stats : nan = dtype util . as numpy dtype ( self . dtype ) ( np . nan ) return tf . where ( df > 1. , result where defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = \"nan\" ) ) else : with tf . control dependencies ( [ assert util . assert less ( tf . cast ( 1. , self . dtype ) , df , message = statistic name + \" not defined for components of df <= 1\" ) , ] ) : return tf . identity ( result where defined )", "predictions": ["helper method to helper to helper to helper method called to helper method to helper to helper to helper instances ."], "references": ["helper to compute stddev covariance and variance ."], "bleu": 0.0821610732492254, "rouge_l": 0.2250922509225092}
{"id": 325, "code": "def pick scalar condition ( pred , cond true , cond false ) : pred = tf . get static value ( tf . convert to tensor ( value = pred ) ) if pred is None : return tf . where ( pred , cond true , cond false ) return cond true if pred else cond false", "predictions": ["pick a addes ( lat / lon ) node ."], "references": ["convenience function which chooses the condition based on the predicate ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 326, "code": "def finish log prob for one fiber ( self , y , x , ildj , event ndims , * * distribution kwargs ) : x = self . maybe rotate dims ( x , rotate right = True ) log prob = self . distribution . log prob ( x , * * distribution kwargs ) if self . is maybe event override : log prob = tf . reduce sum ( input tensor = log prob , axis = self . reduce event indices ) log prob += tf . cast ( ildj , log prob . dtype ) if self . is maybe event override and isinstance ( event ndims , int ) : tensorshape util . set shape ( log prob , tf . broadcast static shape ( tensorshape util . with rank at least ( y . shape , 1 ) [ : - event ndims ] , self . batch shape ) ) return log prob", "predictions": ["first step of all the log elements ."], "references": ["finish computation of log_prob on one element of the inverse image ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 327, "code": "def finish prob for one fiber ( self , y , x , ildj , event ndims , * * distribution kwargs ) : x = self . maybe rotate dims ( x , rotate right = True ) prob = self . distribution . prob ( x , * * distribution kwargs ) if self . is maybe event override : prob = tf . reduce prod ( input tensor = prob , axis = self . reduce event indices ) prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) if self . is maybe event override and isinstance ( event ndims , int ) : tensorshape util . set shape ( prob , tf . broadcast static shape ( tensorshape util . with rank at least ( y . shape , 1 ) [ : - event ndims ] , self . batch shape ) ) return prob", "predictions": ["first step . this is used to finish evaluation ."], "references": ["finish computation of prob on one element of the inverse image ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 328, "code": "def maybe rotate dims ( self , x , rotate right = False ) : needs rotation const = tf . get static value ( self . needs rotation ) if needs rotation const is not None and not needs rotation const : return x ndims = prefer static . rank ( x ) n = ( ndims - self . rotate ndims ) if rotate right else self . rotate ndims perm = prefer static . concat ( [ prefer static . range ( n , ndims ) , prefer static . range ( 0 , n ) ] , axis = 0 ) return tf . transpose ( a = x , perm = perm )", "predictions": ["rotates the character sequence by the given rotation ."], "references": ["helper which rolls left event_dims left or right event_dims right ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 329, "code": "def apply single step ( dist , params event ndims , slices , params overrides ) : if len ( slices ) == 1 and slices [ 0 ] == Ellipsis : override dict = { } else : override dict = slice params to dict ( dist , params event ndims , slices ) override dict . update ( params overrides ) parameters = dict ( dist . parameters , * * override dict ) new dist = type ( dist ) ( * * parameters ) return new dist", "predictions": ["applies the specified neurons to the group using the specified parameters ."], "references": ["applies a single slicing step to dist returning a new instance ."], "bleu": 0.1235622127262679, "rouge_l": 0.25}
{"id": 330, "code": "def apply slice sequence ( dist , params event ndims , slice overrides seq ) : for slices , overrides in slice overrides seq : dist = apply single step ( dist , params event ndims , slices , overrides ) return dist", "predictions": ["applies the slice of this sequence into the given sequence ."], "references": ["applies a sequence of slice or copy - with - overrides operations to dist ."], "bleu": 0.1042097697398488, "rouge_l": 0.22453987730061348}
{"id": 331, "code": "def num cols ( x ) : if tf . compat . dimension value ( x . shape [ - 1 ] ) is not None : return tf . compat . dimension value ( x . shape [ - 1 ] ) return tf . shape ( input = x ) [ - 1 ]", "predictions": ["compute the average size of the input variable ."], "references": ["returns number of cols in a given tensor ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 332, "code": "def prefer static ( original fn , static fn ) : original spec = tf inspect . getfullargspec ( original fn ) static spec = tf inspect . getfullargspec ( static fn ) if original spec != static spec : raise Value Error ( 'Arg specs do not match: original={}, static={}, fn={}' . format ( original spec , static spec , original fn ) ) @ decorator . decorator def wrap ( wrapped fn , * args , * * kwargs ) : del wrapped fn [ args , kwargs ] , all static = maybe get static args ( [ args , kwargs ] ) if all static : return static fn ( * args , * * kwargs ) return original fn ( * args , * * kwargs ) return wrap ( original fn )", "predictions": ["returns a copy of the given argument ."], "references": ["wraps original_fn preferring to call static_fn when inputs are static ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 333, "code": "def copy docstring ( original fn , new fn ) : original spec = tf inspect . getfullargspec ( original fn ) new spec = tf inspect . getfullargspec ( new fn ) if original spec != new spec : raise Value Error ( 'Arg specs do not match: original={}, new={}, fn={}' . format ( original spec , new spec , original fn ) ) @ decorator . decorator def wrap ( wrapped fn , * args , * * kwargs ) : del wrapped fn return new fn ( * args , * * kwargs ) return wrap ( original fn )", "predictions": ["copies the docstring from one or more ( ."], "references": ["wraps new_fn with the doc of original_fn ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 334, "code": "def get static predicate ( pred ) : if pred in { 0 , 1 } : pred value = bool ( pred ) elif isinstance ( pred , bool ) : pred value = pred elif isinstance ( pred , tf . Tensor ) : pred value = tf . get static value ( pred ) if pred value is None : pred value = c api . TF Try Evaluate Constant wrapper ( pred . graph . c graph , pred . as tf output ( ) ) else : raise Type Error ( '`pred` must be a Tensor, or a Python bool, or 1 or 0. ' 'Found instead: {}' . format ( pred ) ) return pred value", "predictions": ["gets the . of a given type and index ."], "references": ["helper function for statically evaluating predicates in cond ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 335, "code": "def rank from shape ( shape tensor fn , tensorshape = None ) : if tensorshape is None : shape tensor = ( shape tensor fn ( ) if callable ( shape tensor fn ) else shape tensor fn ) if ( hasattr ( shape tensor , 'shape' ) and hasattr ( shape tensor . shape , 'num elements' ) ) : ndims = tensorshape util . num elements ( shape tensor . shape ) else : ndims = len ( shape tensor ) ndims fn = lambda : tf . size ( input = shape tensor ) else : ndims = tensorshape util . rank ( tensorshape ) ndims fn = lambda : tf . size ( input = shape tensor fn ( ) if callable ( shape tensor fn ) else shape tensor fn ) return ndims fn ( ) if ndims is None else ndims", "predictions": ["turns a tensor using a tensor and tensor ."], "references": ["computes rank given a tensor s shape ."], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 336, "code": "def name scope ( self , name = None , default name = None , values = None ) : with tf . compat . v1 . name scope ( self . name ) : with tf . compat . v1 . name scope ( name , default name , values = values or [ ] ) as scope : yield scope", "predictions": ["create a new name ."], "references": ["helper function to standardize op scope ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 337, "code": "def embed check nonnegative integer form ( x , name = \"embed check nonnegative integer form\" ) : with tf . name scope ( name ) : x = tf . convert to tensor ( value = x , name = \"x\" ) assertions = [ assert util . assert non negative ( x , message = \"'{}' must be non-negative.\" . format ( x ) ) , ] if not dtype util . is integer ( x . dtype ) : assertions += [ assert integer form ( x , message = \"'{}' cannot contain fractional components.\" . format ( x ) ) , ] return with dependencies ( assertions , x )", "predictions": ["checks for all required dependencies ."], "references": ["assert x is a non - negative tensor and optionally of integers ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 338, "code": "def is known unsigned by dtype ( dt ) : return { tf . bool : True , tf . uint8 : True , tf . uint16 : True , } . get ( dt . base dtype , False )", "predictions": ["returns true if the given unsigned unsigned unsigned unsigned unsigned unsigned unsigned ."], "references": ["helper returning true if dtype is known to be unsigned ."], "bleu": 0.15807437922444714, "rouge_l": 0.33841886269070737}
{"id": 339, "code": "def is known signed by dtype ( dt ) : return { tf . float16 : True , tf . float32 : True , tf . float64 : True , tf . int8 : True , tf . int16 : True , tf . int32 : True , tf . int64 : True , } . get ( dt . base dtype , False )", "predictions": ["test whether a signed dtype is known to be . ."], "references": ["helper returning true if dtype is known to be signed ."], "bleu": 0.44833867003844585, "rouge_l": 0.5454545454545454}
{"id": 340, "code": "def largest integer by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) if dt . is floating : return int ( 2 ** ( np . finfo ( dt . as numpy dtype ) . nmant + 1 ) ) if dt . is integer : return np . iinfo ( dt . as numpy dtype ) . max if dt . base dtype == tf . bool : return int ( 1 ) raise Type Error ( \"Unrecognized dtype: {}\" . format ( dt . name ) )", "predictions": ["return an integer integer or largest dtype"], "references": ["helper returning the largest integer exactly representable by dtype ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 341, "code": "def smallest integer by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) if is known unsigned by dtype ( dt ) : return 0 return - 1 * largest integer by dtype ( dt )", "predictions": ["return the smallest integer by the smallest dtype ."], "references": ["helper returning the smallest integer exactly representable by dtype ."], "bleu": 0.29052561567528307, "rouge_l": 0.6256410256410255}
{"id": 342, "code": "def is integer like by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) return dt . is integer or dt . base dtype == tf . bool", "predictions": ["test if an integer like a dtype or integer ."], "references": ["helper returning true if dtype . is_integer or is bool ."], "bleu": 0.1434272783816789, "rouge_l": 0.37770897832817335}
{"id": 343, "code": "def gen new seed ( seed , salt ) : if seed is None : return None string = ( str ( seed ) + salt ) . encode ( \"utf-8\" ) return int ( hashlib . md5 ( string ) . hexdigest ( ) [ : 8 ] , 16 ) & 0x7FFFFFFF", "predictions": ["generate a new fingerprint key ."], "references": ["generate a new seed from the given seed and salt ."], "bleu": 0.18892891214792823, "rouge_l": 0.4468864468864468}
{"id": 344, "code": "def dimension size ( x , axis ) : s = tf . compat . dimension value ( tensorshape util . with rank at least ( x . shape , np . abs ( axis ) ) [ axis ] ) if s is not None : return s return tf . shape ( input = x ) [ axis ]", "predictions": ["get the dimension of the given axis ."], "references": ["returns the size of a specific dimension ."], "bleu": 0.20164945583740668, "rouge_l": 0.375}
{"id": 345, "code": "def maybe validate rightmost transposed ndims ( rightmost transposed ndims , validate args , name = None ) : with tf . name scope ( name or 'maybe validate rightmost transposed ndims' ) : assertions = [ ] if not dtype util . is integer ( rightmost transposed ndims . dtype ) : raise Type Error ( '`rightmost transposed ndims` must be integer type.' ) if tensorshape util . rank ( rightmost transposed ndims . shape ) is not None : if tensorshape util . rank ( rightmost transposed ndims . shape ) != 0 : raise Value Error ( '`rightmost transposed ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape util . rank ( rightmost transposed ndims . shape ) ) ) elif validate args : assertions += [ assert util . assert rank ( rightmost transposed ndims , 0 ) ] rightmost transposed ndims = tf . get static value ( rightmost transposed ndims ) msg = '`rightmost transposed ndims` must be non-negative.' if rightmost transposed ndims is not None : if rightmost transposed ndims < 0 : raise Value Error ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost transposed ndims ) ) elif validate args : assertions += [ assert util . assert non negative ( rightmost transposed ndims , message = msg ) ] return assertions", "predictions": ["validate if the shape is valid or if the shape parameter is a valid shape ."], "references": ["checks that rightmost_transposed_ndims is valid ."], "bleu": 0.10878661088699644, "rouge_l": 0.2970779220779221}
{"id": 346, "code": "def maybe validate perm ( perm , validate args , name = None ) : with tf . name scope ( name or 'maybe validate perm' ) : assertions = [ ] if not dtype util . is integer ( perm . dtype ) : raise Type Error ( '`perm` must be integer type' ) msg = '`perm` must be a vector.' if tensorshape util . rank ( perm . shape ) is not None : if tensorshape util . rank ( perm . shape ) != 1 : raise Value Error ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape util . rank ( perm . shape ) ) ) elif validate args : assertions += [ assert util . assert rank ( perm , 1 , message = msg ) ] perm = tf . get static value ( perm ) msg = '`perm` must be a valid permutation vector.' if perm is not None : if not np . all ( np . arange ( np . size ( perm ) ) == np . sort ( perm ) ) : raise Value Error ( msg [ : - 1 ] + ', saw: {}.' . format ( perm ) ) elif validate args : assertions += [ assert util . assert equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] return assertions", "predictions": ["validates the shape of a certain scope . this function can be used to ensure that the validation of a certain scope is a valid shape ."], "references": ["checks that perm is valid ."], "bleu": 0.05647857271976959, "rouge_l": 0.2738496071829405}
{"id": 347, "code": "def event shape ( self , shape , static perm to shape ) : rightmost = tf . get static value ( self . rightmost transposed ndims ) if tensorshape util . rank ( shape ) is None or rightmost is None : return tf . Tensor Shape ( None ) if tensorshape util . rank ( shape ) < rightmost : raise Value Error ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost , shape ) ) perm = tf . get static value ( self . perm , partial = True ) if perm is None : return shape [ : tensorshape util . rank ( shape ) - rightmost ] . concatenate ( [ None ] * int ( rightmost ) ) if sum ( p is None for p in perm ) == 1 : present = np . argsort ( [ - 1 if p is None else p for p in perm ] ) for i , p in enumerate ( present [ 1 : ] ) : if i != p : perm = [ i if p is None else p for p in perm ] break return shape [ : tensorshape util . rank ( shape ) - rightmost ] . concatenate ( static perm to shape ( shape [ tensorshape util . rank ( shape ) - rightmost : ] , perm ) )", "predictions": ["convert event shape to a ( . the result is a ( with the shape of the shape ."], "references": ["helper for _forward and _inverse_event_shape ."], "bleu": 0.06439931429457924, "rouge_l": 0.0882778581765557}
{"id": 348, "code": "def check equal shape ( name , static shape , dynamic shape , static target shape , dynamic target shape = None ) : static target shape = tf . Tensor Shape ( static target shape ) if tensorshape util . is fully defined ( static shape ) and tensorshape util . is fully defined ( static target shape ) : if static shape != static target shape : raise Value Error ( \"{}: required shape {} but found {}\" . format ( name , static target shape , static shape ) ) return None else : if dynamic target shape is None : if tensorshape util . is fully defined ( static target shape ) : dynamic target shape = tensorshape util . as list ( static target shape ) else : raise Value Error ( \"{}: cannot infer target shape: no dynamic shape \" \"specified and static shape {} is not fully defined\" . format ( name , static target shape ) ) return assert util . assert equal ( dynamic shape , dynamic target shape , message = ( \"{}: required shape {}\" . format ( name , static target shape ) ) )", "predictions": ["checks that the shape is in a effect of another shape ."], "references": ["check that source and target shape match statically if possible ."], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 349, "code": "def kalman transition ( filtered mean , filtered cov , transition matrix , transition noise ) : predicted mean = propagate mean ( filtered mean , transition matrix , transition noise ) predicted cov = propagate cov ( filtered cov , transition matrix , transition noise ) return predicted mean , predicted cov", "predictions": ["calculates the kalman on the given transition and then sends the specified kalman ."], "references": ["propagate a filtered distribution through a transition model ."], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 350, "code": "def propagate mean ( mean , linop , dist ) : return linop . matmul ( mean ) + dist . mean ( ) [ ... , tf . newaxis ]", "predictions": ["propagate all mean of the mean"], "references": ["propagate a mean through linear gaussian transformation ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 351, "code": "def propagate cov ( cov , linop , dist ) : return linop . matmul ( linop . matmul ( cov ) , adjoint arg = True ) + dist . covariance ( )", "predictions": ["calculates the command from the given cov and returns it as a ( ."], "references": ["propagate covariance through linear gaussian transformation ."], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 352, "code": "def joint sample n ( self , n , seed = None ) : with tf . name scope ( \"sample n joint\" ) : stream = seed stream . Seed Stream ( seed , salt = \"Linear Gaussian State Space Model sample n joint\" ) sample and batch shape = distribution util . prefer static value ( tf . concat ( [ [ n ] , self . batch shape tensor ( ) ] , axis = 0 ) ) with tf . control dependencies ( self . runtime assertions ) : initial latent = self . initial state prior . sample ( sample shape = augment sample shape ( self . initial state prior , sample and batch shape , self . validate args ) , seed = stream ( ) ) initial latent = initial latent [ ... , tf . newaxis ] initial observation matrix = ( self . get observation matrix for timestep ( self . initial step ) ) initial observation noise = ( self . get observation noise for timestep ( self . initial step ) ) initial observation pred = initial observation matrix . matmul ( initial latent ) initial observation = ( initial observation pred + initial observation noise . sample ( sample shape = augment sample shape ( initial observation noise , sample and batch shape , self . validate args ) , seed = stream ( ) ) [ ... , tf . newaxis ] ) sample step = build kalman sample step ( self . get transition matrix for timestep , self . get transition noise for timestep , self . get observation matrix for timestep , self . get observation noise for timestep , full sample and batch shape = sample and batch shape , stream = stream , validate args = self . validate args ) ( latents , observations ) = tf . scan ( sample step , elems = tf . range ( self . initial step + 1 , self . final step ) , initializer = ( initial latent , initial observation ) ) latents = tf . concat ( [ initial latent [ tf . newaxis , ... ] , latents ] , axis = 0 ) observations = tf . concat ( [ initial observation [ tf . newaxis , ... ] , observations ] , axis = 0 ) latents = tf . squeeze ( latents , - 1 ) latents = distribution util . move dimension ( latents , 0 , - 2 ) observations = tf . squeeze ( observations , - 1 ) observations = distribution util . move dimension ( observations , 0 , - 2 ) return latents , observations", "predictions": ["create a new build group with the given = data . this is a simple build method that caches the build ."], "references": ["draw a joint sample from the prior over latents and observations ."], "bleu": 0.06586656967644004, "rouge_l": 0.18635437881873726}
{"id": 353, "code": "def log normalization ( self ) : event dim = tf . compat . dimension value ( self . event shape [ 0 ] ) if event dim is None : raise Value Error ( 'v MF  log normalizer currently only supports ' 'statically known event shape' ) safe conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones like ( self . concentration ) ) safe lognorm = ( ( event dim / 2 - 1 ) * tf . math . log ( safe conc ) - ( event dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( bessel ive ( event dim / 2 - 1 , safe conc ) ) - tf . abs ( safe conc ) ) log nsphere surface area = ( np . log ( 2. ) + ( event dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event dim / 2 , self . dtype ) ) ) return tf . where ( self . concentration > 0 , - safe lognorm , log nsphere surface area * tf . ones like ( safe lognorm ) )", "predictions": ["get config of an image ."], "references": ["computes the log - normalizer of the distribution ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 354, "code": "def maybe assert valid sample ( self , samples ) : if not self . validate args : return samples with tf . control dependencies ( [ assert util . assert near ( 1. , tf . linalg . norm ( tensor = samples , axis = - 1 ) , message = 'samples must be unit length' ) , assert util . assert equal ( tf . shape ( input = samples ) [ - 1 : ] , self . event shape tensor ( ) , message = ( 'samples must have innermost dimension matching that of ' '`self.mean direction`' ) ) , ] ) : return tf . identity ( samples )", "predictions": ["now this operation is a little helper to ensure two innermost are equal ."], "references": ["check counts for proper shape values then return tensor version ."], "bleu": 0.08839374326825923, "rouge_l": 0.08176943699731902}
{"id": 355, "code": "def mode ( self ) : return ( self . mean direction + tf . zeros like ( self . concentration ) [ ... , tf . newaxis ] )", "predictions": ["returns the mean matmul ."], "references": ["the mode of the von mises - fisher distribution is the mean direction ."], "bleu": 0.06382147015463427, "rouge_l": 0.2909379968203497}
{"id": 356, "code": "def rotate ( self , samples ) : event dim = ( tf . compat . dimension value ( self . event shape [ 0 ] ) or self . event shape tensor ( ) [ 0 ] ) basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event dim - 1 ] , dtype = self . dtype ) ] , axis = 0 ) , u = tf . nn . l2 normalize ( basis - self . mean direction , axis = - 1 ) return samples - 2 * tf . reduce sum ( input tensor = samples * u , axis = - 1 , keepdims = True ) * u", "predictions": ["rotates the input input ."], "references": ["applies a householder rotation to samples ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 357, "code": "def sample 3d ( self , n , seed = None ) : seed = seed stream . Seed Stream ( seed , salt = 'von mises fisher 3d' ) u shape = tf . concat ( [ [ n ] , self . batch shape tensor ( ) ] , axis = 0 ) z = tf . random . uniform ( u shape , seed = seed ( ) , dtype = self . dtype ) safe conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones like ( self . concentration ) ) safe z = tf . where ( z > 0 , z , tf . ones like ( z ) ) safe u = 1 + tf . reduce logsumexp ( input tensor = [ tf . math . log ( safe z ) , tf . math . log1p ( - safe z ) - 2 * safe conc ] , axis = 0 ) / safe conc u = tf . where ( self . concentration > tf . zeros like ( safe u ) , safe u , 2 * z - 1 ) u = tf . where ( tf . equal ( z , 0 ) , - tf . ones like ( u ) , u ) if not self . allow nan stats : u = tf . debugging . check numerics ( u , 'u in  sample 3d' ) return u [ ... , tf . newaxis ]", "predictions": ["pick the scalar tf ."], "references": ["specialized inversion sampler for 3d ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 358, "code": "def remove dict keys with value ( dict , val ) : return { k : v for k , v in dict . items ( ) if v is not val }", "predictions": ["finish the value with a value and a value from a map ."], "references": ["removes dict keys which have have self as value ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 359, "code": "def recursively replace dict for pretty dict ( x ) : if isinstance ( x , dict ) : return Pretty Dict ( { k : recursively replace dict for pretty dict ( v ) for k , v in x . items ( ) } ) if ( isinstance ( x , collections . Sequence ) and not isinstance ( x , six . string types ) ) : args = ( recursively replace dict for pretty dict ( x ) for x in x ) is named tuple = ( isinstance ( x , tuple ) and hasattr ( x , \" asdict\" ) and hasattr ( x , \" fields\" ) ) return type ( x ) ( * args ) if is named tuple else type ( x ) ( args ) if isinstance ( x , collections . Mapping ) : return type ( x ) ( * * { k : recursively replace dict for pretty dict ( v ) for k , v in x . items ( ) } ) return x", "predictions": ["replaces all variable references in the specified collection with their value ."], "references": ["recursively replace dict s with _prettydict ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 360, "code": "def get samples ( dist , z , n , seed ) : with tf . compat . v1 . name scope ( 'get samples' , values = [ z , n ] ) : if ( n is None ) == ( z is None ) : raise Value Error ( 'Must specify exactly one of arguments \"n\" and \"z\".  Found: ' 'n = %s, z = %s' % ( n , z ) ) if n is not None : return dist . sample ( n , seed = seed ) else : return tf . convert to tensor ( value = z , name = 'z' )", "predictions": ["maybe maybe ( dims ( ."], "references": ["check args and return samples ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 361, "code": "def is namedtuple like ( x ) : try : for fn in x . fields : = getattr ( x , fn ) return True except Attribute Error : return False", "predictions": ["test if this is a single single single single value of another ."], "references": ["helper which returns true if input is collections . namedtuple - like ."], "bleu": 0.1135935489027116, "rouge_l": 0.23076923076923084}
{"id": 362, "code": "def make name ( super name , default super name , sub name ) : name = super name if super name is not None else default super name if sub name is not None : name += ' ' + sub name return name", "predictions": ["create a new slice ."], "references": ["helper which makes a str name ; useful for tf . compat . v1 . name_scope ."], "bleu": 0.027409299216719144, "rouge_l": 0.1655359565807327}
{"id": 363, "code": "def choose base case ( is accepted , accepted , rejected , name = None ) : def expand is accepted like ( x ) : \"\"\"Helper to expand `is accepted` like the shape of some input arg.\"\"\" with tf . compat . v1 . name scope ( 'expand is accepted like' ) : expand shape = tf . concat ( [ tf . shape ( input = is accepted ) , tf . ones ( [ tf . rank ( x ) - tf . rank ( is accepted ) ] , dtype = tf . int32 ) , ] , axis = 0 ) multiples = tf . concat ( [ tf . ones ( [ tf . rank ( is accepted ) ] , dtype = tf . int32 ) , tf . shape ( input = x ) [ tf . rank ( is accepted ) : ] , ] , axis = 0 ) m = tf . tile ( tf . reshape ( is accepted , expand shape ) , multiples ) m . set shape ( m . shape . merge with ( x . shape ) ) return m def where ( accepted , rejected ) : if accepted is rejected : return accepted accepted = tf . convert to tensor ( value = accepted , name = 'accepted' ) rejected = tf . convert to tensor ( value = rejected , name = 'rejected' ) r = tf . where ( expand is accepted like ( accepted ) , accepted , rejected ) r . set shape ( r . shape . merge with ( accepted . shape . merge with ( rejected . shape ) ) ) return r with tf . compat . v1 . name scope ( name , 'choose' , values = [ is accepted , accepted , rejected ] ) : if not is list like ( accepted ) : return where ( accepted , rejected ) return [ ( choose ( is accepted , a , r , name = name ) if is namedtuple like ( a ) else where ( a , r ) ) for a , r in zip ( accepted , rejected ) ]", "predictions": ["num cols . retrieve an input from all input vectors ."], "references": ["helper to choose which expand_dims is_accepted and applies tf . where ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 364, "code": "def choose ( is accepted , accepted , rejected , name = None ) : if not is namedtuple like ( accepted ) : return choose base case ( is accepted , accepted , rejected , name = name ) if not isinstance ( accepted , type ( rejected ) ) : raise Type Error ( 'Type of `accepted` ({}) must be identical to ' 'type of `rejected` ({})' . format ( type ( accepted ) . name , type ( rejected ) . name ) ) return type ( accepted ) ( * * dict ( [ ( fn , choose ( is accepted , getattr ( accepted , fn ) , getattr ( rejected , fn ) , name = name ) ) for fn in accepted . fields ] ) )", "predictions": ["prefer the result of ( to be processed . the method is called after the type is complete ."], "references": ["helper which expand_dims is_accepted then applies tf . where ."], "bleu": 0.0712695567709093, "rouge_l": 0.14610778443113773}
{"id": 365, "code": "def value and gradients ( fn , fn arg list , result = None , grads = None , name = None ) : with tf . compat . v1 . name scope ( name , 'value and gradients' , [ fn arg list , result , grads ] ) : def convert to tensor ( x , name ) : ctt = lambda x : x if x is None else tf . convert to tensor ( value = x , name = name ) return [ ctt ( x ) for x in x ] if is list like ( x ) else ctt ( x ) fn arg list = ( list ( fn arg list ) if is list like ( fn arg list ) else [ fn arg list ] ) fn arg list = convert to tensor ( fn arg list , 'fn arg' ) if result is None : result = fn ( * fn arg list ) if grads is None and tf . executing eagerly ( ) : fn arg list = [ 0 + x for x in fn arg list ] result = convert to tensor ( result , 'fn result' ) if grads is not None : grads = convert to tensor ( grads , 'fn grad' ) return result , grads if is list like ( result ) and len ( result ) == len ( fn arg list ) : def fn slice ( i ) : \"\"\"Needed to prevent `cell-var-from-loop` pylint warning.\"\"\" return lambda x : fn ( * ( fn arg list [ : i ] + [ x ] + fn arg list [ i + 1 : ] ) ) grads = [ tfp math value and gradients ( fn slice ( i ) , fn arg list [ i ] ) [ 1 ] for i in range ( len ( result ) ) ] else : , grads = tfp math value and gradients ( fn , fn arg list ) return result , grads", "predictions": ["copy the given function using the specified scope ."], "references": ["helper to maybe_call_fn_and_grads ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 366, "code": "def maybe call fn and grads ( fn , fn arg list , result = None , grads = None , check non none grads = True , name = None ) : with tf . compat . v1 . name scope ( name , 'maybe call fn and grads' , [ fn arg list , result , grads ] ) : fn arg list = ( list ( fn arg list ) if is list like ( fn arg list ) else [ fn arg list ] ) result , grads = value and gradients ( fn , fn arg list , result , grads ) if not all ( r . dtype . is floating for r in ( result if is list like ( result ) else [ result ] ) ) : raise Type Error ( 'Function result must be a `Tensor` with `float` ' '`dtype`.' ) if len ( fn arg list ) != len ( grads ) : raise Value Error ( 'Function args must be in one-to-one correspondence ' 'with grads.' ) if check non none grads and any ( g is None for g in grads ) : raise Value Error ( 'Encountered `None` gradient.\\n' '  fn arg list: {}\\n' '  grads: {}' . format ( fn arg list , grads ) ) return result , grads", "predictions": ["calls the function ) and returns the in a shared expression ."], "references": ["calls fn and computes the gradient of the result wrt args_list ."], "bleu": 0.1367440667823257, "rouge_l": 0.3333333333333333}
{"id": 367, "code": "def maybe check valid shape ( shape , validate args ) : if not dtype util . is integer ( shape . dtype ) : raise Type Error ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype util . name ( shape . dtype ) ) ) assertions = [ ] message = '`{}` rank should be <= 1.' if tensorshape util . rank ( shape . shape ) is not None : if tensorshape util . rank ( shape . shape ) > 1 : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) shape = tf . get static value ( shape ) message = '`{}` elements must have at most one `-1`.' if shape is not None : if sum ( shape == - 1 ) > 1 : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert less ( tf . reduce sum ( input tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) message = '`{}` elements must be either positive integers or `-1`.' if shape is not None : if np . any ( shape < - 1 ) : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert greater ( shape , - 2 , message = message . format ( shape ) ) ) return assertions", "predictions": ["checks that this ( ( i . e . , raise = ( = ( = ( callable = true = false = ( callable = true = false = ( tensor if ( if this is a valid ( if ( if ( if this is a valid ("], "references": ["check that a shape tensor is int - type and otherwise sane ."], "bleu": 0.03162593967015063, "rouge_l": 0.10651920838183936}
{"id": 368, "code": "def maybe assert valid sample ( self , x ) : if not self . validate args : return x return distribution util . with dependencies ( [ assert util . assert positive ( x , message = \"sample must be positive\" ) , assert util . assert less ( x , 1. , message = \"sample must be less than `1`.\" ) , ] , x )", "predictions": ["scope until all operation matching the self - ( default values are as expected values values values are as expected values values values ."], "references": ["checks the validity of a sample ."], "bleu": 0.05606668411195419, "rouge_l": 0.14319248826291078}
{"id": 369, "code": "def converged any ( converged , failed ) : return ( tf . reduce any ( input tensor = converged ) | tf . reduce all ( input tensor = failed ) )", "predictions": ["get a anchor based on the input"], "references": ["condition to stop when any batch member converges or all have failed ."], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 370, "code": "def update position ( state , position delta , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) : failed = state . failed | ~ tf . math . is finite ( next objective ) | ~ tf . reduce all ( input tensor = tf . math . is finite ( next gradient ) , axis = - 1 ) next position = state . position + position delta converged = ~ failed & check convergence ( state . position , next position , state . objective value , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) return update fields ( state , converged = state . converged | converged , failed = failed , position = next position , objective value = next objective , objective gradient = next gradient )", "predictions": ["is the : is fast , reduce or uint8 to 0 dtype ."], "references": ["updates the state advancing its position by a given position_delta ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 371, "code": "def check convergence ( current position , next position , current objective , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) : grad converged = norm ( next gradient , dims = 1 ) <= grad tolerance x converged = norm ( next position - current position , dims = 1 ) <= x tolerance f converged = ( norm ( next objective - current objective , dims = 0 ) <= f relative tolerance * current objective ) return grad converged | x converged | f converged", "predictions": ["checks the { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { {"], "references": ["checks if the algorithm satisfies the convergence criteria ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 372, "code": "def get field ( kernel results , field name ) : if hasattr ( kernel results , field name ) : return getattr ( kernel results , field name ) if hasattr ( kernel results , 'accepted results' ) : return getattr ( kernel results . accepted results , field name ) raise Type Error ( 'Cannot extract %s from %s' % ( field name , kernel results ) )", "predictions": ["get the integer value of this integer ."], "references": ["field_name from kernel_results or kernel_results . accepted_results ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 373, "code": "def variance scale term ( self ) : c0 = self . total concentration [ ... , tf . newaxis ] return tf . sqrt ( ( 1. + c0 / self . total count [ ... , tf . newaxis ] ) / ( 1. + c0 ) )", "predictions": ["translation method to smallest item ."], "references": ["helper to _covariance and _variance which computes a shared scale ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 374, "code": "def maybe assert valid concentration ( self , concentration , validate args ) : if not validate args : return concentration concentration = distribution util . embed check categorical event shape ( concentration ) return distribution util . with dependencies ( [ assert util . assert positive ( concentration , message = \"Concentration parameter must be positive.\" ) , ] , concentration )", "predictions": ["integer integer checking whether or not this method always returns true ."], "references": ["checks the validity of the concentration parameter ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 375, "code": "def maybe assert valid sample ( self , counts ) : if not self . validate args : return counts counts = distribution util . embed check nonnegative integer form ( counts ) return distribution util . with dependencies ( [ assert util . assert equal ( self . total count , tf . reduce sum ( input tensor = counts , axis = - 1 ) , message = \"counts last-dimension must sum to `self.total count`\" ) , ] , counts )", "predictions": ["new bigdecimal : 1 . 5 . 2 . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . encode . ("], "references": ["check counts for proper shape values then return tensor version ."], "bleu": 0.02403051755364481, "rouge_l": 0.037059538274605106}
{"id": 376, "code": "def forward log det jacobian fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( transformed state parts , event ndims ) : return sum ( [ b . forward log det jacobian ( sp , event ndims = e ) for b , e , sp in zip ( bijector , event ndims , transformed state parts ) ] ) return fn", "predictions": ["dimension the size of this ( and return the result of the given events ."], "references": ["makes a function which applies a list of bijectors log_det_jacobian s ."], "bleu": 0.09103526405546068, "rouge_l": 0.15117719950433703}
{"id": 377, "code": "def forward transform fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( transformed state parts ) : return [ b . forward ( sp ) for b , sp in zip ( bijector , transformed state parts ) ] return fn", "predictions": ["maybe the ( operation on the ( ."], "references": ["makes a function which applies a list of bijectors forward s ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 378, "code": "def inverse transform fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( state parts ) : return [ b . inverse ( sp ) for b , sp in zip ( bijector , state parts ) ] return fn", "predictions": ["validate all streams from the assertions ."], "references": ["makes a function which applies a list of bijectors inverse s ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 379, "code": "def val where ( cond , tval , fval ) : if isinstance ( tval , tf . Tensor ) : return tf . where ( cond , tval , fval ) elif isinstance ( tval , tuple ) : cls = type ( tval ) return cls ( * ( val where ( cond , t , f ) for t , f in zip ( tval , fval ) ) ) else : raise Exception ( Type Error )", "predictions": ["evaluate an evaluation or a given value ."], "references": ["like tf . where but works on namedtuples ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 380, "code": "def secant2 inner ( value and gradients function , initial args , val 0 , val c , f lim , sufficient decrease param , curvature param ) : update result = update ( value and gradients function , initial args . left , initial args . right , val c , f lim , active = initial args . active ) active = initial args . active & ~ update result . failed failed = initial args . failed | update result . failed val left = val where ( active , update result . left , initial args . left ) val right = val where ( active , update result . right , initial args . right ) updated left = active & tf . equal ( val left . x , val c . x ) updated right = active & tf . equal ( val right . x , val c . x ) is new = updated left | updated right next c = tf . where ( updated left , secant ( initial args . left , val left ) , val c . x ) next c = tf . where ( updated right , secant ( initial args . right , val right ) , next c ) in range = ( val left . x <= next c ) & ( next c <= val right . x ) needs extra eval = tf . reduce any ( input tensor = in range & is new ) num evals = initial args . num evals + update result . num evals num evals = num evals + tf . cast ( needs extra eval , num evals . dtype ) next args = Secant2Result ( active = active & in range , converged = initial args . converged , failed = failed , num evals = num evals , left = val left , right = val right ) def apply inner update ( ) : next val c = prefer static . cond ( needs extra eval , ( lambda : value and gradients function ( next c ) ) , ( lambda : val c ) ) return secant2 inner update ( value and gradients function , next args , val 0 , next val c , f lim , sufficient decrease param , curvature param ) return prefer static . cond ( tf . reduce any ( input tensor = next args . active ) , apply inner update , lambda : next args )", "predictions": ["performs a check check with the arguments passed ."], "references": ["helper function for secant square ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 381, "code": "def secant2 inner update ( value and gradients function , initial args , val 0 , val c , f lim , sufficient decrease param , curvature param ) : new failed = initial args . active & ~ is finite ( val c ) active = initial args . active & ~ new failed failed = initial args . failed | new failed found wolfe = active & satisfies wolfe ( val 0 , val c , f lim , sufficient decrease param , curvature param ) val left = val where ( found wolfe , val c , initial args . left ) val right = val where ( found wolfe , val c , initial args . right ) converged = initial args . converged | found wolfe active = active & ~ found wolfe def apply update ( ) : update result = update ( value and gradients function , val left , val right , val c , f lim , active = active ) return Secant2Result ( active = tf . zeros like ( active ) , converged = converged , failed = failed | update result . failed , num evals = initial args . num evals + update result . num evals , left = update result . left , right = update result . right ) def default ( ) : return Secant2Result ( active = active , converged = converged , failed = failed , num evals = initial args . num evals , left = val left , right = val right ) return prefer static . cond ( tf . reduce any ( input tensor = active ) , apply update , default )", "predictions": ["performs a secant2 ( secant2 ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["helper function for secant - square step ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 382, "code": "def bisect ( value and gradients function , initial args , f lim ) : def loop cond ( curr ) : return ~ tf . reduce all ( input tensor = curr . stopped ) def loop body ( curr ) : \"\"\"Narrow down interval to satisfy opposite slope conditions.\"\"\" mid = value and gradients function ( ( curr . left . x + curr . right . x ) / 2 ) failed = ( curr . failed | ~ is finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) to update = ~ ( curr . stopped | failed ) update left = ( mid . df < 0 ) & ( mid . f <= f lim ) left = val where ( to update & update left , mid , curr . left ) right = val where ( to update & ~ update left , mid , curr . right ) stopped = curr . stopped | failed | ( right . df >= 0 ) return [ Intermediate Result ( iteration = curr . iteration , stopped = stopped , failed = failed , num evals = curr . num evals + 1 , left = left , right = right ) ] return tf . while loop ( cond = loop cond , body = loop body , loop vars = [ initial args ] ) [ 0 ]", "predictions": ["performs propagate . propagate mean ."], "references": ["actual implementation of bisect given initial_args in a _bracketresult ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 383, "code": "def prepare args ( target log prob fn , state , step size , target log prob = None , grads target log prob = None , maybe expand = False , state gradients are stopped = False ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( value = s , name = 'current state' ) for s in state parts ] if state gradients are stopped : state parts = [ tf . stop gradient ( x ) for x in state parts ] target log prob , grads target log prob = mcmc util . maybe call fn and grads ( target log prob fn , state parts , target log prob , grads target log prob ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) def maybe flatten ( x ) : return x if maybe expand or mcmc util . is list like ( state ) else x [ 0 ] return [ maybe flatten ( state parts ) , maybe flatten ( step sizes ) , target log prob , grads target log prob , ]", "predictions": ["propagate a set of . ("], "references": ["helper which processes input args to meet list - like assumptions ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 384, "code": "def bootstrap results ( self , init state ) : kernel results = self . impl . bootstrap results ( init state ) if self . step size update fn is not None : step size assign = self . step size update fn ( self . step size , None ) kernel results = kernel results . replace ( extra = Hamiltonian Monte Carlo Extra Kernel Results ( step size assign = step size assign ) ) return kernel results", "predictions": [". a bootstrap kernel function ."], "references": ["creates initial previous_kernel_results using a supplied state ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 385, "code": "def resnet block ( x , filters , kernel , stride , kernel posterior fn ) : x = tf . keras . layers . Batch Normalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) if stride != 1 or filters != x . shape [ 1 ] : shortcut = projection shortcut ( x , filters , stride , kernel posterior fn ) else : shortcut = x x = tfp . layers . Convolution2D Flipout ( filters , kernel , strides = stride , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) x = tf . keras . layers . Batch Normalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tfp . layers . Convolution2D Flipout ( filters , kernel , strides = 1 , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) x = tf . keras . layers . add ( [ x , shortcut ] ) return x", "predictions": ["print an resnet block ."], "references": ["network block for resnet ."], "bleu": 0.32466791547509893, "rouge_l": 0.4}
{"id": 386, "code": "def deep exponential family ( data size , feature size , units , shape ) : w2 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 2 ] , units [ 1 ] ] , name = \"w2\" ) w1 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 1 ] , units [ 0 ] ] , name = \"w1\" ) w0 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 0 ] , feature size ] , name = \"w0\" ) z2 = ed . Gamma ( 0.1 , 0.1 , sample shape = [ data size , units [ 2 ] ] , name = \"z2\" ) z1 = ed . Gamma ( shape , shape / tf . matmul ( z2 , w2 ) , name = \"z1\" ) z0 = ed . Gamma ( shape , shape / tf . matmul ( z1 , w1 ) , name = \"z0\" ) x = ed . Poisson ( tf . matmul ( z0 , w0 ) , name = \"x\" ) return x", "predictions": ["create a deep exponential of the specified data ."], "references": ["a multi - layered topic model over a documents - by - terms matrix ."], "bleu": 0.08019421212222273, "rouge_l": 0.15947712418300655}
{"id": 387, "code": "def trainable positive deterministic ( shape , min loc = 1e-3 , name = None ) : with tf . compat . v1 . variable scope ( None , default name = \"trainable positive deterministic\" ) : unconstrained loc = tf . compat . v1 . get variable ( \"unconstrained loc\" , shape ) loc = tf . maximum ( tf . nn . softplus ( unconstrained loc ) , min loc ) rv = ed . Deterministic ( loc = loc , name = name ) return rv", "predictions": ["create a positive positive variable ."], "references": ["learnable deterministic distribution over positive reals ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 388, "code": "def trainable gamma ( shape , min concentration = 1e-3 , min scale = 1e-5 , name = None ) : with tf . compat . v1 . variable scope ( None , default name = \"trainable gamma\" ) : unconstrained concentration = tf . compat . v1 . get variable ( \"unconstrained concentration\" , shape , initializer = tf . compat . v1 . initializers . random normal ( mean = 0.5 , stddev = 0.1 ) ) unconstrained scale = tf . compat . v1 . get variable ( \"unconstrained scale\" , shape , initializer = tf . compat . v1 . initializers . random normal ( stddev = 0.1 ) ) concentration = tf . maximum ( tf . nn . softplus ( unconstrained concentration ) , min concentration ) rate = tf . maximum ( 1. / tf . nn . softplus ( unconstrained scale ) , 1. / min scale ) rv = ed . Gamma ( concentration = concentration , rate = rate , name = name ) return rv", "predictions": ["trainable gamma gamma to a unconstrained expression ."], "references": ["learnable gamma via concentration and scale parameterization ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 389, "code": "def registered kl ( type a , type b ) : hierarchy a = tf inspect . getmro ( type a ) hierarchy b = tf inspect . getmro ( type b ) dist to children = None kl fn = None for mro to a , parent a in enumerate ( hierarchy a ) : for mro to b , parent b in enumerate ( hierarchy b ) : candidate dist = mro to a + mro to b candidate kl fn = DIVERGENCES . get ( ( parent a , parent b ) , None ) if not kl fn or ( candidate kl fn and candidate dist < dist to children ) : dist to children = candidate dist kl fn = candidate kl fn return kl fn", "predictions": ["return the argument of this endpoint"], "references": ["get the kl function registered for classes a and b ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 390, "code": "def read image ( filepath ) : im bytes = tf . io . read file ( filepath ) im = tf . image . decode image ( im bytes , channels = CHANNELS ) im = tf . image . convert image dtype ( im , tf . float32 ) return im", "predictions": ["read an image from a file ."], "references": ["returns an image tensor ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 391, "code": "def download sprites ( ) : filepath = os . path . join ( FLAGS . data dir , DATA SPRITES DIR ) if not tf . io . gfile . exists ( filepath ) : if not tf . io . gfile . exists ( FLAGS . data dir ) : tf . io . gfile . makedirs ( FLAGS . data dir ) zip name = \"{}.zip\" . format ( filepath ) urllib . request . urlretrieve ( DATA SPRITES URL , zip name ) with zipfile . Zip File ( zip name , \"r\" ) as zip file : zip file . extractall ( FLAGS . data dir ) tf . io . gfile . remove ( zip name ) return filepath", "predictions": ["download the wrong file from this project ."], "references": ["downloads the sprites data and returns the saved filepath ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 392, "code": "def create character ( skin , hair , top , pants ) : dtype = skin . dtype hair mask = tf . cast ( hair [ ... , - 1 : ] <= 0 , dtype ) top mask = tf . cast ( top [ ... , - 1 : ] <= 0 , dtype ) pants mask = tf . cast ( pants [ ... , - 1 : ] <= 0 , dtype ) char = ( skin * hair mask ) + hair char = ( char * top mask ) + top char = ( char * pants mask ) + pants return char", "predictions": ["creates a character to create a pants character ."], "references": ["creates a character sprite from a set of attribute sprites ."], "bleu": 0.23278666914796883, "rouge_l": 0.4911433172302737}
{"id": 393, "code": "def create random seq ( character , action metadata , direction , length = 8 ) : start = tf . random . uniform ( [ ] , maxval = action metadata [ 1 ] , dtype = tf . int32 ) return create seq ( character , action metadata , direction , length , start )", "predictions": ["creates a random element of this . ."], "references": ["creates a random sequence ."], "bleu": 0.3155984539112945, "rouge_l": 0.6421052631578947}
{"id": 394, "code": "def maybe validate distributions ( distributions , dtype override , validate args ) : assertions = [ ] if not is iterable ( distributions ) or not distributions : raise Value Error ( '`distributions` must be a list of one or more ' 'distributions.' ) if dtype override is None : dts = [ dtype util . base dtype ( d . dtype ) for d in distributions if d . dtype is not None ] if dts [ 1 : ] != dts [ : - 1 ] : raise Type Error ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype util . name ( dt ) for dt in dts ) ) ) for d in distributions : if tensorshape util . rank ( d . event shape ) is not None : if tensorshape util . rank ( d . event shape ) != 1 : raise Value Error ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape util . rank ( d . event shape ) ) ) elif validate args : assertions . append ( assert util . assert equal ( 1 , tf . size ( input = d . event shape tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) batch shapes = [ d . batch shape for d in distributions ] if all ( tensorshape util . is fully defined ( b ) for b in batch shapes ) : if batch shapes [ 1 : ] != batch shapes [ : - 1 ] : raise Value Error ( 'Distributions must have the same `batch shape`; ' 'found: {}.' . format ( batch shapes ) ) elif validate args : batch shapes = [ tensorshape util . as list ( d . batch shape ) if tensorshape util . is fully defined ( d . batch shape ) else d . batch shape tensor ( ) for d in distributions ] assertions . extend ( assert util . assert equal ( b1 , b2 , message = 'Distribution `batch shape`s must be identical.' ) for b1 , b2 in zip ( batch shapes [ 1 : ] , batch shapes [ : - 1 ] ) ) return assertions", "predictions": ["validate an iterable of shapes . this helper function provides a list of shapes of the given dtype ."], "references": ["checks that distributions satisfies all assumptions ."], "bleu": 0.06439931429457924, "rouge_l": 0.08390646492434663}
{"id": 395, "code": "def build input pipeline ( x train , x test , y train , y test , batch size , valid size ) : x train = x train . astype ( \"float32\" ) x test = x test . astype ( \"float32\" ) x train /= 255 x test /= 255 y train = y train . flatten ( ) y test = y test . flatten ( ) if FLAGS . subtract pixel mean : x train mean = np . mean ( x train , axis = 0 ) x train -= x train mean x test -= x train mean print ( \"x train shape:\" + str ( x train . shape ) ) print ( str ( x train . shape [ 0 ] ) + \" train samples\" ) print ( str ( x test . shape [ 0 ] ) + \" test samples\" ) training dataset = tf . data . Dataset . from tensor slices ( ( x train , np . int32 ( y train ) ) ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) heldout dataset = tf . data . Dataset . from tensor slices ( ( x test , np . int32 ( y test ) ) ) heldout batches = heldout dataset . repeat ( ) . batch ( valid size ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout batches ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) return images , labels , handle , training iterator , heldout iterator", "predictions": ["build an input tree ."], "references": ["build an iterator switching between train and heldout data ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 396, "code": "def build fake data ( ) : num examples = 10 x train = np . random . rand ( num examples , * IMAGE SHAPE ) . astype ( np . float32 ) y train = np . random . permutation ( np . arange ( num examples ) ) . astype ( np . int32 ) x test = np . random . rand ( num examples , * IMAGE SHAPE ) . astype ( np . float32 ) y test = np . random . permutation ( np . arange ( num examples ) ) . astype ( np . int32 ) return ( x train , y train ) , ( x test , y test )", "predictions": ["build a data set"], "references": ["build fake cifar10 - style data for unit testing ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 397, "code": "def sort tensor ( tensor ) : sorted , = tf . nn . top k ( tensor , k = tf . shape ( input = tensor ) [ - 1 ] ) sorted . set shape ( tensor . shape ) return sorted", "predictions": ["sorts the specified tensor ."], "references": ["use top_k to sort a tensor along the last dimension ."], "bleu": 0.09778809693469985, "rouge_l": 0.2341650671785029}
{"id": 398, "code": "def assert ndims statically ( x , expect ndims = None , expect ndims at least = None , expect static = False ) : ndims = x . shape . ndims if ndims is None : if expect static : raise Value Error ( 'Expected static ndims. Found: {}' . format ( x ) ) return if expect ndims is not None and ndims != expect ndims : raise Value Error ( 'ndims must be {}.  Found: {}' . format ( expect ndims , ndims ) ) if expect ndims at least is not None and ndims < expect ndims at least : raise Value Error ( 'ndims must be at least {}. Found {}' . format ( expect ndims at least , ndims ) )", "predictions": ["helper to validate two different shuffles ."], "references": ["assert that tensor x has expected number of dimensions ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 399, "code": "def batch gather with broadcast ( params , indices , axis ) : leading bcast shape = tf . broadcast dynamic shape ( tf . shape ( input = params ) [ : axis ] , tf . shape ( input = indices ) [ : - 1 ] ) params += tf . zeros ( tf . concat ( ( leading bcast shape , tf . shape ( input = params ) [ axis : ] ) , axis = 0 ) , dtype = params . dtype ) indices += tf . zeros ( tf . concat ( ( leading bcast shape , tf . shape ( input = indices ) [ - 1 : ] ) , axis = 0 ) , dtype = indices . dtype ) return tf . compat . v1 . batch gather ( params , indices )", "predictions": ["broadcast an axis to all edges ."], "references": ["like batch_gather but broadcasts to the left of axis ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 400, "code": "def broadcast cat event and params ( event , params , base dtype ) : if dtype util . is integer ( event . dtype ) : pass elif dtype util . is floating ( event . dtype ) : event = tf . cast ( event , dtype = tf . int32 ) else : raise Type Error ( \"`value` should have integer `dtype` or \" \"`self.dtype` ({})\" . format ( base dtype ) ) shape known statically = ( tensorshape util . rank ( params . shape ) is not None and tensorshape util . is fully defined ( params . shape [ : - 1 ] ) and tensorshape util . is fully defined ( event . shape ) ) if not shape known statically or params . shape [ : - 1 ] != event . shape : params *= tf . ones like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) params shape = tf . shape ( input = params ) [ : - 1 ] event *= tf . ones ( params shape , dtype = event . dtype ) if tensorshape util . rank ( params . shape ) is not None : tensorshape util . set shape ( event , params . shape [ : - 1 ] ) return event , params", "predictions": ["broadcasts an event to the specified event ."], "references": ["broadcasts the event or distribution parameters ."], "bleu": 0.20164945583740668, "rouge_l": 0.5398230088495575}
{"id": 401, "code": "def broadcast event and samples ( event , samples , event ndims ) : samples shape = tf . concat ( [ tf . shape ( input = samples ) [ : - event ndims - 1 ] , tf . shape ( input = samples ) [ tf . rank ( samples ) - event ndims : ] ] , axis = 0 ) event *= tf . ones ( samples shape , dtype = event . dtype ) event = tf . expand dims ( event , axis = - event ndims - 1 ) samples *= tf . ones like ( event , dtype = samples . dtype ) return event , samples", "predictions": ["broadcasts an event to the given event ."], "references": ["broadcasts the event or samples ."], "bleu": 0.20164945583740668, "rouge_l": 0.5865384615384615}
{"id": 402, "code": "def update inv hessian ( prev state , next state ) : should update = ~ next state . converged & ~ next state . failed gradient delta = next state . objective gradient - prev state . objective gradient position delta = next state . position - prev state . position normalization factor = tf . reduce sum ( input tensor = gradient delta * position delta , axis = - 1 ) should update = should update & ~ tf . equal ( normalization factor , 0 ) def do update inv hessian ( ) : next inv hessian = bfgs inv hessian update ( gradient delta , position delta , normalization factor , prev state . inverse hessian estimate ) return bfgs utils . update fields ( next state , inverse hessian estimate = tf . where ( should update , next inv hessian , prev state . inverse hessian estimate ) ) return prefer static . cond ( tf . reduce any ( input tensor = should update ) , do update inv hessian , lambda : next state )", "predictions": ["updates an inverse distribution with a new axis ."], "references": ["update the bgfs state by computing the next inverse hessian estimate ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 403, "code": "def get initial state ( value and gradients function , initial position , num correction pairs , tolerance ) : init args = bfgs utils . get initial state args ( value and gradients function , initial position , tolerance ) empty queue = make empty queue for ( num correction pairs , initial position ) init args . update ( position deltas = empty queue , gradient deltas = empty queue ) return L Bfgs Optimizer Results ( * * init args )", "predictions": ["this will get the initial state with the given position ."], "references": ["create lbfgsoptimizerresults with initial state of search procedure ."], "bleu": 0.17033186037639278, "rouge_l": 0.3055091819699499}
{"id": 404, "code": "def von mises cdf series ( x , concentration , num terms , dtype ) : num terms = tf . cast ( num terms , dtype = dtype ) def loop body ( n , rn , drn dconcentration , vn , dvn dconcentration ) : \"\"\"One iteration of the series loop.\"\"\" denominator = 2. * n / concentration + rn ddenominator dk = - 2. * n / concentration ** 2 + drn dconcentration rn = 1. / denominator drn dconcentration = - ddenominator dk / denominator ** 2 multiplier = tf . sin ( n * x ) / n + vn vn = rn * multiplier dvn dconcentration = ( drn dconcentration * multiplier + rn * dvn dconcentration ) n -= 1. return n , rn , drn dconcentration , vn , dvn dconcentration ( , , , vn , dvn dconcentration ) = tf . while loop ( cond = lambda n , * : n > 0. , body = loop body , loop vars = ( num terms , tf . zeros like ( x , name = \"rn\" ) , tf . zeros like ( x , name = \"drn dconcentration\" ) , tf . zeros like ( x , name = \"vn\" ) , tf . zeros like ( x , name = \"dvn dconcentration\" ) , ) , ) cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi dcdf dconcentration = dvn dconcentration / np . pi cdf clipped = tf . clip by value ( cdf , 0. , 1. ) dcdf dconcentration *= tf . cast ( ( cdf >= 0. ) & ( cdf <= 1. ) , dtype ) return cdf clipped , dcdf dconcentration", "predictions": ["calculates an von network using a given series of shape ."], "references": ["computes the von mises cdf and its derivative via series expansion ."], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 405, "code": "def von mises cdf normal ( x , concentration , dtype ) : def cdf func ( concentration ) : \"\"\"A helper function that is passed to value and gradient.\"\"\" z = ( ( np . sqrt ( 2. / np . pi ) / tf . math . bessel i0e ( concentration ) ) * tf . sin ( .5 * x ) ) z2 = z ** 2 z3 = z2 * z z4 = z2 ** 2 c = 24. * concentration c1 = 56. xi = z - z3 / ( ( c - 2. * z2 - 16. ) / 3. - ( z4 + ( 7. / 4. ) * z2 + 167. / 2. ) / ( c - c1 - z2 + 3. ) ) ** 2 distrib = normal . Normal ( tf . cast ( 0. , dtype ) , tf . cast ( 1. , dtype ) ) return distrib . cdf ( xi ) return value and gradient ( cdf func , concentration )", "predictions": ["implements a von . ( x ) for the given ( ."], "references": ["computes the von mises cdf and its derivative via normal approximation ."], "bleu": 0.1235622127262679, "rouge_l": 0.16666666666666666}
{"id": 406, "code": "def get initial args ( objective function , initial population , initial position , population size , population stddev , max iterations , func tolerance , position tolerance , differential weight , crossover prob , seed ) : was iterable = False if initial position is not None : initial position , was iterable = ensure list ( initial position ) if initial population is not None : initial population , was iterable = ensure list ( initial population ) population = get starting population ( initial population , initial position , population size , population stddev , seed = seed ) differential weight = tf . convert to tensor ( value = differential weight , dtype = population [ 0 ] . dtype . base dtype ) crossover prob = tf . convert to tensor ( value = crossover prob ) population values = objective function ( * population ) if max iterations is not None : max iterations = tf . convert to tensor ( value = max iterations ) func tolerance = tf . convert to tensor ( value = func tolerance , dtype = population values . dtype . base dtype ) position tolerance = tf . convert to tensor ( value = position tolerance , dtype = population [ 0 ] . dtype . base dtype ) return ( was iterable , population , population values , max iterations , func tolerance , position tolerance , differential weight , crossover prob )", "predictions": ["returns the initial objective function for the given function ."], "references": ["processes initial args ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 407, "code": "def find best in population ( population , values ) : best value = tf . math . reduce min ( input tensor = values ) best index = tf . where ( tf . math . equal ( values , best value ) ) [ 0 , 0 ] return ( [ population part [ best index ] for population part in population ] , best value )", "predictions": ["finds the best match for the given population ."], "references": ["finds the population member with the lowest value ."], "bleu": 0.2208959113415788, "rouge_l": 0.4444444444444444}
{"id": 408, "code": "def check convergence ( population , population values , func tolerance , position tolerance ) : value range = tf . math . abs ( tf . math . reduce max ( input tensor = population values ) - tf . math . reduce min ( input tensor = population values ) ) value converged = value range <= func tolerance half tol = position tolerance / 2 def part converged ( part ) : return tf . math . reduce max ( input tensor = tf . math . abs ( part - part [ 0 ] ) ) <= half tol x converged = tf . math . reduce all ( input tensor = [ part converged ( part ) for part in population ] ) return value converged | x converged", "predictions": ["check the convergence of a convergence ."], "references": ["checks whether the convergence criteria have been met ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 409, "code": "def get tol ( tol , dtype , validate args ) : if tol is None : return tf . convert to tensor ( value = 0 , dtype = dtype ) tol = tf . convert to tensor ( value = tol , dtype = dtype ) if validate args : tol = distribution util . with dependencies ( [ assert util . assert non negative ( tol , message = \"Argument 'tol' must be non-negative\" ) ] , tol ) return tol", "predictions": ["define a tol or distribution ."], "references": ["gets a tensor of type dtype 0 if tol is none validation optional ."], "bleu": 0.06924459302580939, "rouge_l": 0.2798165137614679}
{"id": 410, "code": "def build input pipeline ( train images , batch size ) : training dataset = tf . data . Dataset . from tensor slices ( train images ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) images = training iterator . get next ( ) return images", "predictions": ["build the compat for the ( ."], "references": ["build an iterator over training batches ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 411, "code": "def hat integral inverse ( self , x ) : x = tf . cast ( x , self . power . dtype ) t = self . power - 1. return tf . math . expm1 ( - ( tf . math . log ( t ) + tf . math . log ( x ) ) / t )", "predictions": ["compute the integral of this ( ."], "references": ["inverse function of _hat_integral ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 412, "code": "def lu reconstruct assertions ( lower upper , perm , validate args ) : assertions = [ ] message = 'Input `lower upper` must have at least 2 dimensions.' if lower upper . shape . ndims is not None : if lower upper . shape . ndims < 2 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( lower upper , rank = 2 , message = message ) ) message = '`rank(lower upper)` must equal `rank(perm) + 1`' if lower upper . shape . ndims is not None and perm . shape . ndims is not None : if lower upper . shape . ndims != perm . shape . ndims + 1 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank ( lower upper , rank = tf . rank ( perm ) + 1 , message = message ) ) message = '`lower upper` must be square.' if lower upper . shape [ : - 2 ] . is fully defined ( ) : if lower upper . shape [ - 2 ] != lower upper . shape [ - 1 ] : raise Value Error ( message ) elif validate args : m , n = tf . split ( tf . shape ( input = lower upper ) [ - 2 : ] , num or size splits = 2 ) assertions . append ( tf . compat . v1 . assert equal ( m , n , message = message ) ) return assertions", "predictions": ["reconstruct assertions from inputs ."], "references": ["returns list of assertions related to lu_reconstruct assumptions ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 413, "code": "def lu solve assertions ( lower upper , perm , rhs , validate args ) : assertions = lu reconstruct assertions ( lower upper , perm , validate args ) message = 'Input `rhs` must have at least 2 dimensions.' if rhs . shape . ndims is not None : if rhs . shape . ndims < 2 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( rhs , rank = 2 , message = message ) ) message = '`lower upper.shape[-1]` must equal `rhs.shape[-1]`.' if ( tf . compat . dimension value ( lower upper . shape [ - 1 ] ) is not None and tf . compat . dimension value ( rhs . shape [ - 2 ] ) is not None ) : if lower upper . shape [ - 1 ] != rhs . shape [ - 2 ] : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . shape ( input = lower upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) return assertions", "predictions": ["solve the assertions given a lower or examples: ."], "references": ["returns list of assertions related to lu_solve assumptions ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 414, "code": "def maybe validate matrix ( a , validate args ) : assertions = [ ] if not a . dtype . is floating : raise Type Error ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) if a . shape . ndims is not None : if a . shape . ndims < 2 : raise Value Error ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) return assertions", "predictions": ["validate that the given numpy array is compatible with the given args and returns the rank which can be returned ."], "references": ["checks that input is a float matrix ."], "bleu": 0.0690889519686715, "rouge_l": 0.2250922509225092}
{"id": 415, "code": "def gen slices ( num blocks , n in , n out , mask type = MASK EXCLUSIVE ) : slices = [ ] col = 0 d in = n in // num blocks d out = n out // num blocks row = d out if mask type == MASK EXCLUSIVE else 0 for in range ( num blocks ) : row slice = slice ( row , None ) col slice = slice ( col , col + d in ) slices . append ( [ row slice , col slice ] ) col += d in row += d out return slices", "predictions": ["encode an integer into slices ."], "references": ["generate the slices for building an autoregressive mask ."], "bleu": 0.1593301391270729, "rouge_l": 0.2573839662447257}
{"id": 416, "code": "def gen mask ( num blocks , n in , n out , mask type = MASK EXCLUSIVE , dtype = tf . float32 ) : mask = np . zeros ( [ n out , n in ] , dtype = dtype . as numpy dtype ( ) ) slices = gen slices ( num blocks , n in , n out , mask type = mask type ) for [ row slice , col slice ] in slices : mask [ row slice , col slice ] = 1 return mask", "predictions": ["generate results for results of length state ."], "references": ["generate the mask for building an autoregressive dense layer ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 417, "code": "def create input order ( input size , input order = \"left-to-right\" ) : if isinstance ( input order , six . string types ) : if input order == \"left-to-right\" : return np . arange ( start = 1 , stop = input size + 1 ) elif input order == \"right-to-left\" : return np . arange ( start = input size , stop = 0 , step = - 1 ) elif input order == \"random\" : ret = np . arange ( start = 1 , stop = input size + 1 ) np . random . shuffle ( ret ) return ret elif np . all ( np . sort ( input order ) == np . arange ( 1 , input size + 1 ) ) : return np . array ( input order ) raise Value Error ( \"Invalid input order: '{}'.\" . format ( input order ) )", "predictions": ["resnet a gpu with a given block ."], "references": ["returns a degree vectors for the input ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 418, "code": "def create masks ( degrees ) : return [ inp [ : , np . newaxis ] <= out for inp , out in zip ( degrees [ : - 1 ] , degrees [ 1 : ] ) ] + [ degrees [ - 1 ] [ : , np . newaxis ] < degrees [ 0 ] ]", "predictions": ["deep copy of all colour arrays . useful for deep outer classes ."], "references": ["returns a list of binary mask matrices enforcing autoregressivity ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 419, "code": "def make masked initializer ( mask , initializer ) : initializer = tf . keras . initializers . get ( initializer ) def masked initializer ( shape , dtype = None , partition info = None ) : if partition info is None : x = initializer ( shape , dtype ) else : x = initializer ( shape , dtype , partition info ) return tf . cast ( mask , x . dtype ) * x return masked initializer", "predictions": ["helper method to create the new positive tensor ."], "references": ["returns a masked version of the given initializer ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 420, "code": "def build ( self , input shape ) : if self . event shape is None : self . event shape = [ tf . compat . dimension value ( input shape [ - 1 ] ) ] self . event size = self . event shape [ - 1 ] self . event ndims = len ( self . event shape ) if input shape [ - 1 ] != self . event shape [ - 1 ] : raise Value Error ( \"Invalid final dimension of `input shape`. \" \"Expected `{!r}`, but got `{!r}`\" . format ( self . event shape [ - 1 ] , input shape [ - 1 ] ) ) self . input order = create input order ( self . event size , self . input order param ) self . masks = create masks ( create degrees ( input size = self . event size , hidden units = self . hidden units , input order = self . input order , hidden degrees = self . hidden degrees ) ) # self . masks [ - 1 ] = np . reshape ( np . tile ( self . masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . params ] ) , [ self . masks [ - 1 ] . shape [ 0 ] , self . event size * self . params ] ) self . network = tf . keras . Sequential ( [ tf . keras . layers . Input Layer ( ( self . event size , ) , dtype = self . dtype ) ] ) layer output sizes = self . hidden units + [ self . event size * self . params ] for k in range ( len ( self . masks ) ) : self . network . add ( tf . keras . layers . Dense ( layer output sizes [ k ] , kernel initializer = make masked initializer ( self . masks [ k ] , self . kernel initializer ) , kernel constraint = make masked constraint ( self . masks [ k ] ) , activation = self . activation if k + 1 < len ( self . masks ) else None , use bias = self . use bias , * * self . kwargs ) ) super ( Autoregressive Layer , self ) . build ( input shape )", "predictions": ["creates a . this is taken from the , , both of the , and min and min ."], "references": ["see tfkl . layer . build ."], "bleu": 0.0712695567709093, "rouge_l": 0.16781292984869325}
{"id": 421, "code": "def call ( self , x ) : with tf . compat . v2 . name scope ( self . name or \"Autoregressive Layer call\" ) : x = tf . convert to tensor ( value = x , dtype = self . dtype , name = \"x\" ) input shape = tf . shape ( input = x ) if tensorshape util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] return tf . reshape ( self . network ( x ) , tf . concat ( [ input shape , [ self . params ] ] , axis = 0 ) )", "predictions": ["define a registered not found by the specified ."], "references": ["see tfkl . layer . call ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 422, "code": "def zero dimensional mvndiag ( dtype ) : dummy mvndiag = tfd . Multivariate Normal Diag ( scale diag = tf . ones ( [ 0 ] , dtype = dtype ) ) dummy mvndiag . covariance = lambda : dummy mvndiag . variance ( ) [ ... , tf . newaxis ] return dummy mvndiag", "predictions": ["transform the read of the read symbols to a read - only variable ."], "references": ["build a zero - dimensional mvndiag object ."], "bleu": 0.10511846841633776, "rouge_l": 0.28683385579937304}
{"id": 423, "code": "def observe timeseries fn ( timeseries ) : def observation noise fn ( t ) : current slice = timeseries [ ... , t , : ] return tfd . Multivariate Normal Diag ( loc = current slice , scale diag = tf . zeros like ( current slice ) ) return observation noise fn", "predictions": ["given a sprites and a sprites of all other functions ."], "references": ["build an observation_noise_fn that observes a tensor timeseries ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 424, "code": "def params to weights ( self , global scale variance , global scale noncentered , local scale variances , local scales noncentered , weights noncentered ) : global scale = ( global scale noncentered * tf . sqrt ( global scale variance ) * self . weights prior scale ) local scales = local scales noncentered * tf . sqrt ( local scale variances ) return weights noncentered * local scales * global scale [ ... , tf . newaxis ]", "predictions": ["convert the request parameters to the ( ."], "references": ["build regression weights from model parameters ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 425, "code": "def depth ( g ) : def explore ( v ) : if v . depth < 0 : v . depth = ( ( 1 + max ( [ - 1 ] + [ explore ( annotated graph [ u ] ) for u in v . parents ] ) ) if v . parents else 0 ) return v . depth annotated graph = { k : Node ( k , v ) for k , v in g . items ( ) } for v in annotated graph . values ( ) : explore ( v ) return annotated graph", "predictions": ["calculates information about the create ] of all nodes of g . this method is just a full projection ."], "references": ["computes the number of edges on longest path from node to root ."], "bleu": 0.07264339766175722, "rouge_l": 0.18904958677685949}
{"id": 426, "code": "def best order ( g ) : def explore ( u ) : \"\"\"Recursive function to ascend up through unvisited dependencies.\"\"\" if u . depth < 0 : return if not u . parents : result . append ( ( u . name , u . parents ) ) u . depth = - 1 return b = ( u . name , [ ] ) result . append ( b ) u . depth = - 1 d = 0 for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : n0 = len ( result ) explore ( v ) n1 = len ( result ) b [ 1 ] . extend ( [ ' ' ] * d + [ v . name ] ) d = n1 - n0 - 1 g = depth ( g ) result = [ ] for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : explore ( u ) return tuple ( reversed ( result ) )", "predictions": ["return the maybe validate the maybe edges of the two nodes ."], "references": ["creates tuple of str tuple - str pairs representing resolved & sorted dag ."], "bleu": 0.09733489823443878, "rouge_l": 0.1517412935323383}
{"id": 427, "code": "def prob chain rule flatten ( named makers ) : def make ( dist fn , args ) : if args is None : return lambda * : dist fn if not args : return lambda * : dist fn ( ) def fn ( * xs ) : kwargs = dict ( zip ( args , reversed ( xs [ - len ( args ) : ] ) ) ) kwargs . pop ( ' ' , None ) return dist fn ( * * kwargs ) return fn named makers = convert to dict ( named makers ) g = { k : ( None if distribution util . is distribution instance ( v ) else joint distribution sequential . get required args ( v ) ) for k , v in named makers . items ( ) } g = best order ( g ) dist fn name , dist fn args = zip ( * g ) dist fn args = tuple ( None if a is None else tuple ( a ) for a in dist fn args ) dist fn wrapped = tuple ( make ( named makers [ name ] , parents ) for ( name , parents ) in g ) dist fn = tuple ( named makers . get ( n ) for n in dist fn name ) return dist fn , dist fn wrapped , dist fn args , dist fn name", "predictions": ["( input test test test test test test test test test test test test test test test test test for ( ."], "references": ["creates lists of callables suitable for jdseq ."], "bleu": 0.0612957497932821, "rouge_l": 0.14558472553699284}
{"id": 428, "code": "def build ( self , model ) : if not is dict like ( model ) : raise Type Error ( '`model` must be convertible to `dict` (saw: {}).' . format ( type ( model ) . name ) ) [ self . dist fn , self . dist fn wrapped , self . dist fn args , self . dist fn name , ] = prob chain rule flatten ( model )", "predictions": ["builds a rule rule from a ) and builds a rule ."], "references": ["creates dist_fn dist_fn_wrapped dist_fn_args dist_fn_name ."], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 429, "code": "def build is last day of season ( num steps per season ) : num steps per cycle = np . sum ( num steps per season ) changepoints = np . cumsum ( np . ravel ( num steps per season ) ) - 1 def is last day of season ( t ) : t = dist util . maybe get static value ( t ) if t is not None : step in cycle = t % num steps per cycle return any ( step in cycle == changepoints ) else : step in cycle = tf . math . floormod ( t , num steps per cycle ) return tf . reduce any ( input tensor = tf . equal ( step in cycle , changepoints ) ) return is last day of season", "predictions": ["builds an k - th ) ) ) number of tf ."], "references": ["build utility method to compute whether the season is changing ."], "bleu": 0.10390302174233558, "rouge_l": 0.08764367816091953}
{"id": 430, "code": "def build seasonal transition matrix ( num seasons , is last day of season , dtype , basis change matrix = None , basis change matrix inv = None ) : with tf . compat . v1 . name scope ( 'build seasonal transition matrix' ) : seasonal permutation = np . concatenate ( [ np . arange ( 1 , num seasons ) , [ 0 ] ] , axis = 0 ) seasonal permutation matrix = tf . constant ( np . eye ( num seasons ) [ seasonal permutation ] , dtype = dtype ) if basis change matrix is not None : seasonal permutation matrix = tf . matmul ( basis change matrix , tf . matmul ( seasonal permutation matrix , basis change matrix inv ) ) identity matrix = tf . eye ( tf . shape ( input = seasonal permutation matrix ) [ - 1 ] , dtype = dtype ) def seasonal transition matrix ( t ) : return tf . linalg . Linear Operator Full Matrix ( matrix = dist util . pick scalar condition ( is last day of season ( t ) , seasonal permutation matrix , identity matrix ) ) return seasonal transition matrix", "predictions": ["assert the given statically ( given rotation raise . raise . if the given x = < p > this method computes the seasonal flag ."], "references": ["build a function computing transitions for a seasonal effect model ."], "bleu": 0.051660454541342535, "rouge_l": 0.11663479923518164}
{"id": 431, "code": "def build seasonal transition noise ( drift scale , num seasons , is last day of season ) : drift scale diag = tf . stack ( [ tf . zeros like ( drift scale ) ] * ( num seasons - 1 ) + [ drift scale ] , axis = - 1 ) def seasonal transition noise ( t ) : noise scale diag = dist util . pick scalar condition ( is last day of season ( t ) , drift scale diag , tf . zeros like ( drift scale diag ) ) return tfd . Multivariate Normal Diag ( loc = tf . zeros ( num seasons , dtype = drift scale . dtype ) , scale diag = noise scale diag ) return seasonal transition noise", "predictions": ["batch gather of a platform using a gather elimination ."], "references": ["build the transition noise model for a seasonalstatespacemodel ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 432, "code": "def build constrained seasonal transition noise ( drift scale , num seasons , is last day of season ) : # # # drift scale tril nonzeros = tf . concat ( [ tf . ones ( [ num seasons - 1 , 1 ] , dtype = drift scale . dtype ) , tf . zeros ( [ num seasons - 1 , num seasons - 2 ] , dtype = drift scale . dtype ) ] , axis = - 1 ) drift scale tril = ( drift scale tril nonzeros * drift scale [ ... , tf . newaxis , tf . newaxis ] / num seasons ) def seasonal transition noise ( t ) : noise scale tril = dist util . pick scalar condition ( is last day of season ( t ) , drift scale tril , tf . zeros like ( drift scale tril ) ) return tfd . Multivariate Normal Tri L ( loc = tf . zeros ( num seasons - 1 , dtype = drift scale . dtype ) , scale tril = noise scale tril ) return seasonal transition noise", "predictions": ["event to broadcast an cat of the given and integer points ."], "references": ["build transition noise distribution for a constrainedseasonalssm ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 433, "code": "def optimize ( self ) : jmodel = call Java Func ( self . value . optimize ) from bigdl . nn . layer import Layer return Layer . of ( jmodel )", "predictions": ["broadcast a input method call ."], "references": ["do an optimization ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 434, "code": "def get end trigger ( options ) : if options . end Trigger Type . lower ( ) == \"epoch\" : return Max Epoch ( options . end Trigger Num ) else : return Max Iteration ( options . end Trigger Num )", "predictions": ["attempts to update the appropriate hessian list ."], "references": ["when to end the optimization based on input option ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 435, "code": "def validate optimizer ( optimizer , test data , options ) : optimizer . set validation ( batch size = options . batch Size , val rdd = test data , trigger = Every Epoch ( ) , val method = [ Top1Accuracy ( ) ] ) optimizer . set checkpoint ( Every Epoch ( ) , options . checkpoint Path )", "predictions": ["validates the initial state of an object using the ( function ."], "references": ["set validation and checkpoint for distributed optimizer ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 436, "code": "def value ( self ) : if not hasattr ( self , \" value\" ) and self . path is not None : self . value = self . load ( self . path ) return self . value", "predictions": ["invokes this object for the given file ."], "references": ["return the broadcasted value"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 437, "code": "def call Big Dl Func ( bigdl type , name , * args ) : gateway = get gateway ( ) error = Exception ( \"Cannot find function: %s\" % name ) for jinvoker in Java Creator . instance ( bigdl type , gateway ) . value : try : api = getattr ( jinvoker , name ) result = call Java Func ( api , * args ) except Exception as e : error = e if \"does not exist\" not in str ( e ) : raise e else : return result raise error", "predictions": ["calls the method \" x \" command with arguments \" ."], "references": ["call api in pythonbigdl"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 438, "code": "def py2java ( gateway , obj ) : if isinstance ( obj , RDD ) : obj = to java object rdd ( obj ) elif isinstance ( obj , Data Frame ) : obj = obj . jdf elif isinstance ( obj , Spark Context ) : obj = obj . jsc elif isinstance ( obj , ( list , tuple ) ) : obj = List Converter ( ) . convert ( [ py2java ( gateway , x ) for x in obj ] , gateway . gateway client ) elif isinstance ( obj , dict ) : result = { } for ( key , value ) in obj . items ( ) : result [ key ] = py2java ( gateway , value ) obj = Map Converter ( ) . convert ( result , gateway . gateway client ) elif isinstance ( obj , Java Value ) : obj = obj . value elif isinstance ( obj , Java Object ) : pass elif isinstance ( obj , ( int , long , float , bool , bytes , unicode ) ) : pass else : data = bytearray ( Pickle Serializer ( ) . dumps ( obj ) ) obj = gateway . jvm . org . apache . spark . bigdl . api . python . Big DL Ser De . loads ( data ) return obj", "predictions": ["creates the data from the given object ."], "references": ["convert python object into java"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 439, "code": "def get label ( self ) : label = call Big Dl Func ( self . bigdl type , \"image Feature To Label Tensor\" , self . value ) return label . to ndarray ( )", "predictions": ["this method is called to get the best best best best operation ."], "references": ["get label as ndarray from imagefeature"], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 440, "code": "def read parquet ( cls , path , sc , bigdl type = \"float\" ) : return Distributed Image Frame ( jvalue = call Big Dl Func ( bigdl type , \"read Parquet\" , path , sc ) )", "predictions": ["reads an image from a raster ."], "references": ["read parquet file as distributedimageframe"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 441, "code": "def write parquet ( cls , path , output , sc , partition num = 1 , bigdl type = \"float\" ) : return call Big Dl Func ( bigdl type , \"write Parquet\" , path , output , sc , partition num )", "predictions": ["get a tol . save the original data to the server ."], "references": ["write imageframe as parquet file"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 442, "code": "def get image ( self , float key = \"floats\" , to chw = True ) : return self . image frame . get image ( float key , to chw )", "predictions": ["get the ( for this input stream ."], "references": ["get image from imageframe"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 443, "code": "def get image ( self , float key = \"floats\" , to chw = True ) : tensors = call Big Dl Func ( self . bigdl type , \"local Image Frame To Image Tensor\" , self . value , float key , to chw ) return map ( lambda tensor : tensor . to ndarray ( ) , tensors )", "predictions": ["get a ( for this dimenisions ."], "references": ["get image list from imageframe"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 444, "code": "def get label ( self ) : tensor rdd = call Big Dl Func ( self . bigdl type , \"distributed Image Frame To Label Tensor Rdd\" , self . value ) return tensor rdd . map ( lambda tensor : tensor . to ndarray ( ) )", "predictions": ["borrowed from this frame ."], "references": ["get label rdd from imageframe"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 445, "code": "def get predict ( self , key = \"predict\" ) : predicts = call Big Dl Func ( self . bigdl type , \"distributed Image Frame To Predict\" , self . value , key ) return predicts . map ( lambda predict : ( predict [ 0 ] , predict [ 1 ] . to ndarray ( ) ) if predict [ 1 ] else ( predict [ 0 ] , None ) )", "predictions": ["a method that transforms this affinetransform to a given upper and upper bounds ."], "references": ["get prediction rdd from imageframe"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 446, "code": "def save keras definition ( keras model , path ) : model json = keras model . to json ( ) with open ( path , \"w\" ) as json file : json file . write ( model json )", "predictions": ["saves the validate matrix to the given a assertions ."], "references": ["save a keras model definition to json with given path"], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 447, "code": "def build keras model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation , Flatten from keras . layers import Convolution2D , Max Pooling2D keras model = Sequential ( ) keras model . add ( Convolution2D ( 32 , 3 , 3 , border mode = 'valid' , input shape = input shape ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Convolution2D ( 32 , 3 , 3 ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Max Pooling2D ( pool size = ( 2 , 2 ) ) ) keras model . add ( Dropout ( 0.25 ) ) keras model . add ( Flatten ( ) ) keras model . add ( Dense ( 128 ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Dropout ( 0.5 ) ) keras model . add ( Dense ( 10 ) ) keras model . add ( Activation ( 'softmax' ) ) return keras model", "predictions": ["gen ( num num is a wrapper of ( . this is a wrapper for ( that can be the ( ."], "references": ["define a convnet model in keras 1 . 2 . 2"], "bleu": 0.06586656967644004, "rouge_l": 0.19344608879492597}
{"id": 448, "code": "def training ( self , is training = True ) : if is training : call Java Func ( self . value . training ) else : call Java Func ( self . value . evaluate ) return self", "predictions": ["shutdown the training event ."], "references": ["set this layer in the training mode or in predition mode if is_training = false"], "bleu": 0.04862652376060361, "rouge_l": 0.18345864661654135}
{"id": 449, "code": "def build keras model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation from keras . layers import Embedding from keras . layers import LSTM from keras . layers import Convolution1D , Max Pooling1D keras model = Sequential ( ) keras model . add ( Embedding ( 20000 , 128 , input length = 100 ) ) keras model . add ( Dropout ( 0.25 ) ) keras model . add ( Convolution1D ( nb filter = 64 , filter length = 5 , border mode = 'valid' , activation = 'relu' , subsample length = 1 ) ) keras model . add ( Max Pooling1D ( pool length = 4 ) ) keras model . add ( LSTM ( 70 ) ) keras model . add ( Dense ( 1 ) ) keras model . add ( Activation ( 'sigmoid' ) ) return keras model", "predictions": ["build a model that is a keras keras model"], "references": ["define a recurrent convolutional model in keras 1 . 2 . 2"], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 450, "code": "def get bigdl classpath ( ) : if os . getenv ( \"BIGDL CLASSPATH\" ) : return os . environ [ \"BIGDL CLASSPATH\" ] jar dir = os . path . abspath ( file + \"/../../\" ) jar paths = glob . glob ( os . path . join ( jar dir , \"share/lib/*.jar\" ) ) if jar paths : assert len ( jar paths ) == 1 , \"Expecting one jar: %s\" % len ( jar paths ) return jar paths [ 0 ] return \"\"", "predictions": ["gets the classpath of the project ."], "references": ["get and return the jar path for bigdl if exists ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 451, "code": "def is spark below 2 2 ( ) : import pyspark if ( hasattr ( pyspark , \"version\" ) ) : full version = pyspark . version . version parts = full version . split ( \".\" ) spark version = parts [ 0 ] + \".\" + parts [ 1 ] if ( compare version ( spark version , \"2.2\" ) >= 0 ) : return False return True", "predictions": ["' ( ' is ("], "references": ["check if spark version is below 2 . 2"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 452, "code": "def attention ( inputs , state , att size , mask , scope = \"attention\" ) : with tf . variable scope ( scope ) : u = tf . concat ( [ tf . tile ( tf . expand dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2 ) logits = tf . layers . dense ( tf . layers . dense ( u , att size , activation = tf . nn . tanh ) , 1 , use bias = False ) logits = softmax mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) att weights = tf . expand dims ( tf . nn . softmax ( logits ) , axis = 2 ) res = tf . reduce sum ( att weights * inputs , axis = 1 ) return res , logits", "predictions": ["attention the entity join ."], "references": ["computes weighted sum of inputs conditioned on state"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 453, "code": "def summary gradient updates ( grads , opt , lr ) : vars grads = { } for v in tf . trainable variables ( ) : vars grads [ v . name ] = [ v , None , None ] for g , v in grads : vars grads [ v . name ] [ 1 ] = g vars grads [ v . name ] [ 2 ] = opt . get slot ( v , 'accumulator' ) ret = [ ] for vname , ( v , g , a ) in vars grads . items ( ) : if g is None : continue if isinstance ( g , tf . Indexed Slices ) : updates = lr * g . values if a is not None : updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) else : updates = lr * g if a is not None : updates /= tf . sqrt ( a ) values norm = tf . sqrt ( tf . reduce sum ( v * v ) ) + 1.0e-7 updates norm = tf . sqrt ( tf . reduce sum ( updates * updates ) ) ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( \":\" , \" \" ) , updates norm / values norm ) ) return ret", "predictions": ["decodes all the updates of the given set of updates to the specified updates ."], "references": ["get summary ops for the magnitude of gradient updates"], "bleu": 0.09782375748961449, "rouge_l": 0.26180257510729615}
{"id": 454, "code": "def dump weights ( tf save dir , outfile , options ) : def get outname ( tf name ) : outname = re . sub ( ':0$' , '' , tf name ) outname = outname . lstrip ( 'lm/' ) outname = re . sub ( '/rnn/' , '/RNN/' , outname ) outname = re . sub ( '/multi rnn cell/' , '/Multi RNN Cell/' , outname ) outname = re . sub ( '/cell ' , '/Cell' , outname ) outname = re . sub ( '/lstm cell/' , '/LSTM Cell/' , outname ) if '/RNN/' in outname : if 'projection' in outname : outname = re . sub ( 'projection/kernel' , 'W P 0' , outname ) else : outname = re . sub ( '/kernel' , '/W 0' , outname ) outname = re . sub ( '/bias' , '/B' , outname ) return outname ckpt file = tf . train . latest checkpoint ( tf save dir ) config = tf . Config Proto ( allow soft placement = True ) with tf . Graph ( ) . as default ( ) : with tf . Session ( config = config ) as sess : with tf . variable scope ( 'lm' ) : Language Model ( options , False ) loader = tf . train . Saver ( ) loader . restore ( sess , ckpt file ) with h5py . File ( outfile , 'w' ) as fout : for v in tf . trainable variables ( ) : if v . name . find ( 'softmax' ) >= 0 : continue outname = get outname ( v . name ) shape = v . get shape ( ) . as list ( ) dset = fout . create dataset ( outname , shape , dtype = 'float32' ) values = sess . run ( [ v ] ) [ 0 ] dset [ ... ] = values", "predictions": ["dump a weights in ( ."], "references": ["dump the trained weights from a model to a hdf5 file ."], "bleu": 0.10218289380194193, "rouge_l": 0.31443298969072164}
{"id": 455, "code": "def read data by config ( config : dict ) : dataset config = config . get ( 'dataset' , None ) if dataset config : config . pop ( 'dataset' ) ds type = dataset config [ 'type' ] if ds type == 'classification' : reader = { 'class name' : 'basic classification reader' } iterator = { 'class name' : 'basic classification iterator' } config [ 'dataset reader' ] = { * * dataset config , * * reader } config [ 'dataset iterator' ] = { * * dataset config , * * iterator } else : raise Exception ( \"Unsupported dataset type: {}\" . format ( ds type ) ) try : reader config = dict ( config [ 'dataset reader' ] ) except Key Error : raise Config Error ( \"No dataset reader is provided in the JSON config.\" ) reader = get model ( reader config . pop ( 'class name' ) ) ( ) data path = reader config . pop ( 'data path' , '' ) if isinstance ( data path , list ) : data path = [ expand path ( x ) for x in data path ] else : data path = expand path ( data path ) return reader . read ( data path , * * reader config )", "predictions": ["reads data structure from given configuration using given configuration ."], "references": ["read data by dataset_reader from specified config ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 456, "code": "def train evaluate model from config ( config : Union [ str , Path , dict ] , iterator : Union [ Data Learning Iterator , Data Fitting Iterator ] = None , * , to train : bool = True , evaluation targets : Optional [ Iterable [ str ] ] = None , to validate : Optional [ bool ] = None , download : bool = False , start epoch num : Optional [ int ] = None , recursive : bool = False ) -> Dict [ str , Dict [ str , float ] ] : config = parse config ( config ) if download : deep download ( config ) if to train and recursive : for subconfig in get all elems from json ( config [ 'chainer' ] , 'config path' ) : log . info ( f'Training \"{subconfig}\"' ) train evaluate model from config ( subconfig , download = False , recursive = True ) import packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) if iterator is None : try : data = read data by config ( config ) except Config Error as e : to train = False log . warning ( f'Skipping training. {e.message}' ) else : iterator = get iterator from config ( config , data ) if 'train' not in config : log . warning ( 'Train config is missing. Populating with default values' ) train config = config . get ( 'train' ) if start epoch num is not None : train config [ 'start epoch num' ] = start epoch num if 'evaluation targets' not in train config and ( 'validate best' in train config or 'test best' in train config ) : log . warning ( '\"validate best\" and \"test best\" parameters are deprecated.' ' Please, use \"evaluation targets\" list instead' ) train config [ 'evaluation targets' ] = [ ] if train config . pop ( 'validate best' , True ) : train config [ 'evaluation targets' ] . append ( 'valid' ) if train config . pop ( 'test best' , True ) : train config [ 'evaluation targets' ] . append ( 'test' ) trainer class = get model ( train config . pop ( 'class name' , 'nn trainer' ) ) trainer = trainer class ( config [ 'chainer' ] , * * train config ) if to train : trainer . train ( iterator ) res = { } if iterator is not None : if to validate is not None : if evaluation targets is None : log . warning ( '\"to validate\" parameter is deprecated and will be removed in future versions.' ' Please, use \"evaluation targets\" list instead' ) evaluation targets = [ 'test' ] if to validate : evaluation targets . append ( 'valid' ) else : log . warn ( 'Both \"evaluation targets\" and \"to validate\" parameters are specified.' ' \"to validate\" is deprecated and will be ignored' ) res = trainer . evaluate ( iterator , evaluation targets , print reports = True ) trainer . get chainer ( ) . destroy ( ) res = { k : v [ 'metrics' ] for k , v in res . items ( ) } return res", "predictions": ["does a train of the configuration window . the full trainer can be used to process the data ."], "references": ["make training and evaluation of the model described in corresponding configuration file ."], "bleu": 0.09629943614188137, "rouge_l": 0.2587486744432662}
{"id": 457, "code": "def load ( self ) -> None : if self . load path . exists ( ) : path = str ( self . load path . resolve ( ) ) log . info ( '[loading model from {}]' . format ( path ) ) self . net . load ( path )", "predictions": ["load a path from the underlying resource ."], "references": ["checks existence of the model file loads the model if the file exists"], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 458, "code": "def build ( self ) : word inputs = kl . Input ( shape = ( None , MAX WORD LENGTH + 2 ) , dtype = \"int32\" ) inputs = [ word inputs ] word outputs = self . build word cnn ( word inputs ) if len ( self . word vectorizers ) > 0 : additional word inputs = [ kl . Input ( shape = ( None , input dim ) , dtype = \"float32\" ) for input dim , dense dim in self . word vectorizers ] inputs . extend ( additional word inputs ) additional word embeddings = [ kl . Dense ( dense dim ) ( additional word inputs [ i ] ) for i , ( , dense dim ) in enumerate ( self . word vectorizers ) ] word outputs = kl . Concatenate ( ) ( [ word outputs ] + additional word embeddings ) outputs , lstm outputs = self . build basic network ( word outputs ) compile args = { \"optimizer\" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , \"loss\" : \"categorical crossentropy\" , \"metrics\" : [ \"accuracy\" ] } self . model = Model ( inputs , outputs ) self . model . compile ( * * compile args ) if self . verbose > 0 : self . model . summary ( print fn = log . info ) return self", "predictions": ["create a word from a decoder ."], "references": ["builds the network using keras ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 459, "code": "def build word cnn ( self , inputs ) : inputs = kl . Lambda ( kb . one hot , arguments = { \"num classes\" : self . symbols number } , output shape = lambda x : tuple ( x ) + ( self . symbols number , ) ) ( inputs ) char embeddings = kl . Dense ( self . char embeddings size , use bias = False ) ( inputs ) conv outputs = [ ] self . char output dim = 0 for window size , filters number in zip ( self . char window size , self . char filters ) : curr output = char embeddings curr filters number = ( min ( self . char filter multiple * window size , 200 ) if filters number is None else filters number ) for in range ( self . char conv layers - 1 ) : curr output = kl . Conv2D ( curr filters number , ( 1 , window size ) , padding = \"same\" , activation = \"relu\" , data format = \"channels last\" ) ( curr output ) if self . conv dropout > 0.0 : curr output = kl . Dropout ( self . conv dropout ) ( curr output ) curr output = kl . Conv2D ( curr filters number , ( 1 , window size ) , padding = \"same\" , activation = \"relu\" , data format = \"channels last\" ) ( curr output ) conv outputs . append ( curr output ) self . char output dim += curr filters number if len ( conv outputs ) > 1 : conv output = kl . Concatenate ( axis = - 1 ) ( conv outputs ) else : conv output = conv outputs [ 0 ] highway input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv output ) if self . intermediate dropout > 0.0 : highway input = kl . Dropout ( self . intermediate dropout ) ( highway input ) for i in range ( self . char highway layers - 1 ) : highway input = Highway ( activation = \"relu\" ) ( highway input ) if self . highway dropout > 0.0 : highway input = kl . Dropout ( self . highway dropout ) ( highway input ) highway output = Highway ( activation = \"relu\" ) ( highway input ) return highway output", "predictions": ["build a word from a word ."], "references": ["builds word - level network"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 460, "code": "def main ( ) : args = parser . parse args ( ) path = get settings path ( ) if args . default : if populate settings dir ( force = True ) : print ( f'Populated {path} with default settings files' ) else : print ( f'{path} is already a default settings directory' ) else : print ( f'Current Deep Pavlov settings path: {path}' )", "predictions": ["the main method of this class ."], "references": ["deeppavlov console configuration utility ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 461, "code": "def graph wrap ( func , graph ) : @ wraps ( func ) def wrapped ( * args , * * kwargs ) : with graph . as default ( ) : return func ( * args , * * kwargs ) return wrapped", "predictions": ["evaluates a graph graph ."], "references": ["constructs function encapsulated in the graph ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 462, "code": "def keras wrap ( func , graph , session ) : import keras . backend as K @ wraps ( func ) def wrapped ( * args , * * kwargs ) : with graph . as default ( ) : K . set session ( session ) return func ( * args , * * kwargs ) return wrapped", "predictions": ["wrap a function to be executed on a graph ."], "references": ["constructs function encapsulated in the graph and the session ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 463, "code": "def prettify metrics ( metrics : List [ Tuple [ str , float ] ] , precision : int = 4 ) -> Ordered Dict : prettified metrics = Ordered Dict ( ) for key , value in metrics : value = round ( value , precision ) prettified metrics [ key ] = value return prettified metrics", "predictions": ["transforms a list of metrics into a given sorted array with a given precision ."], "references": ["prettifies the dictionary of metrics ."], "bleu": 0.11633270842295028, "rouge_l": 0.3096446700507614}
{"id": 464, "code": "def load ( self , exclude scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise Runtime Error ( 'Your Tensor Flow model {} must' ' have sess attribute!' . format ( self . class . name ) ) path = str ( self . load path . resolve ( ) ) if tf . train . checkpoint exists ( path ) : log . info ( '[loading model from {}]' . format ( path ) ) var list = self . get saveable variables ( exclude scopes ) saver = tf . train . Saver ( var list ) saver . restore ( self . sess , path )", "predictions": ["loads a tuple from a tuple of trees"], "references": ["load model parameters from self . load_path"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 465, "code": "def save ( self , exclude scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise Runtime Error ( 'Your Tensor Flow model {} must' ' have sess attribute!' . format ( self . class . name ) ) path = str ( self . save path . resolve ( ) ) log . info ( '[saving model to {}]' . format ( path ) ) var list = self . get saveable variables ( exclude scopes ) saver = tf . train . Saver ( var list ) saver . save ( self . sess , path )", "predictions": ["saves a ( model to a file ."], "references": ["save model parameters to self . save_path"], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 466, "code": "def print number of parameters ( ) : log . info ( 'Number of parameters: ' ) variables = tf . trainable variables ( ) blocks = defaultdict ( int ) for var in variables : block name = var . name . split ( '/' ) [ 0 ] number of parameters = np . prod ( var . get shape ( ) . as list ( ) ) blocks [ block name ] += number of parameters for block name , cnt in blocks . items ( ) : log . info ( \"{} - {}.\" . format ( block name , cnt ) ) total num parameters = np . sum ( list ( blocks . values ( ) ) ) log . info ( 'Total number of parameters equal {}' . format ( total num parameters ) )", "predictions": ["prints the number of variables ."], "references": ["print number of * trainable * parameters in the network"], "bleu": 0.16038842424444547, "rouge_l": 0.23921568627450981}
{"id": 467, "code": "def search ( self , word , d , allow spaces = True , return cost = True ) : if not all ( ( c in self . alphabet or ( c == \" \" and self . allow spaces ) ) for c in word ) : return [ ] return self . trie search ( word , d , allow spaces = allow spaces , return cost = return cost )", "predictions": ["searches for a cost of the trie ."], "references": ["finds all dictionary words in d - window from word"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 468, "code": "def make default operation costs ( self , allow spaces = False ) : self . operation costs = dict ( ) self . operation costs [ \"\" ] = { c : 1.0 for c in list ( self . alphabet ) + [ ' ' ] } for a in self . alphabet : current costs = { c : 1.0 for c in self . alphabet } current costs [ a ] = 0.0 current costs [ \"\" ] = 1.0 if allow spaces : current costs [ \" \" ] = 1.0 self . operation costs [ a ] = current costs for a , b in itertools . permutations ( self . alphabet , 2 ) : self . operation costs [ a + b ] = { b + a : 1.0 } if allow spaces : self . operation costs [ \" \" ] = { c : 1.0 for c in self . alphabet } self . operation costs [ \" \" ] [ \"\" ] = 1.0", "predictions": ["creates the default operation for this object ."], "references": ["sets 1 . 0 cost for every replacement insertion deletion and transposition"], "bleu": 0.10764345432696364, "rouge_l": 0.09651898734177215}
{"id": 469, "code": "def start timer ( self ) -> None : self . timer = Timer ( self . config [ 'conversation lifetime' ] , self . self destruct callback ) self . timer . start ( )", "predictions": ["create the timer object ."], "references": ["initiates self - destruct timer ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 470, "code": "def build model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , load trained : bool = False , download : bool = False , serialized : Optional [ bytes ] = None ) -> Chainer : config = parse config ( config ) if serialized : serialized : list = pickle . loads ( serialized ) if download : deep download ( config ) import packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) model config = config [ 'chainer' ] model = Chainer ( model config [ 'in' ] , model config [ 'out' ] , model config . get ( 'in y' ) ) for component config in model config [ 'pipe' ] : if load trained and ( 'fit on' in component config or 'in y' in component config ) : try : component config [ 'load path' ] = component config [ 'save path' ] except Key Error : log . warning ( 'No \"save path\" parameter for the {} component, so \"load path\" will not be renewed' . format ( component config . get ( 'class name' , component config . get ( 'ref' , 'UNKNOWN' ) ) ) ) if serialized and 'in' in component config : component serialized = serialized . pop ( 0 ) else : component serialized = None component = from params ( component config , mode = mode , serialized = component serialized ) if 'in' in component config : c in = component config [ 'in' ] c out = component config [ 'out' ] in y = component config . get ( 'in y' , None ) main = component config . get ( 'main' , False ) model . append ( component , c in , c out , in y , main ) return model", "predictions": ["create all configuration instances in the binary transaction ."], "references": ["build and return the model described in corresponding configuration file ."], "bleu": 0.14211011212459496, "rouge_l": 0.19645732689210954}
{"id": 471, "code": "def interact model ( config : Union [ str , Path , dict ] ) -> None : model = build model ( config ) while True : args = [ ] for in x in model . in x : args . append ( ( input ( '{}::' . format ( in x ) ) , ) ) if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : return pred = model ( * args ) if len ( model . out params ) > 1 : pred = zip ( * pred ) print ( '>>' , * pred )", "predictions": ["interact cluster information about the interact control"], "references": ["start interaction with the model described in corresponding configuration file ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 472, "code": "def predict on stream ( config : Union [ str , Path , dict ] , batch size : int = 1 , file path : Optional [ str ] = None ) -> None : if file path is None or file path == '-' : if sys . stdin . isatty ( ) : raise Runtime Error ( 'To process data from terminal please use interact mode' ) f = sys . stdin else : f = open ( file path , encoding = 'utf8' ) model : Chainer = build model ( config ) args count = len ( model . in x ) while True : batch = list ( ( l . strip ( ) for l in islice ( f , batch size * args count ) ) ) if not batch : break args = [ ] for i in range ( args count ) : args . append ( batch [ i : : args count ] ) res = model ( * args ) if len ( model . out params ) == 1 : res = [ res ] for res in zip ( * res ) : res = json . dumps ( res , ensure ascii = False ) print ( res , flush = True ) if f is not sys . stdin : f . close ( )", "predictions": ["this function performs the predict predict analysis for the given stream ."], "references": ["make a prediction with the component described in corresponding configuration file ."], "bleu": 0.11498759556447223, "rouge_l": 0.16666666666666666}
{"id": 473, "code": "def fn from str ( name : str ) -> Callable [ ... , Any ] : try : module name , fn name = name . split ( ':' ) except Value Error : raise Config Error ( 'Expected function description in a `module.submodules:function name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import module ( module name ) , fn name )", "predictions": ["import a module into a module ."], "references": ["returns a function object with the name given in string ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 474, "code": "def register metric ( metric name : str ) -> Callable [ ... , Any ] : def decorate ( fn ) : fn name = fn . module + ':' + fn . name if metric name in REGISTRY and REGISTRY [ metric name ] != fn name : log . warning ( '\"{}\" is already registered as a metric name, the old function will be ignored' . format ( metric name ) ) REGISTRY [ metric name ] = fn name return fn return decorate", "predictions": ["register a new metric for the runtime ."], "references": ["decorator for metric registration ."], "bleu": 0.19070828081828378, "rouge_l": 0.32105263157894737}
{"id": 475, "code": "def get metric by name ( name : str ) -> Callable [ ... , Any ] : if name not in REGISTRY : raise Config Error ( f'\"{name}\" is not registered as a metric' ) return fn from str ( REGISTRY [ name ] )", "predictions": ["retrieves a metric by name ."], "references": ["returns a metric callable with a corresponding name ."], "bleu": 0.22172045047934616, "rouge_l": 0.5147679324894514}
{"id": 476, "code": "def read requirements ( ) : reqs path = os . path . join ( location , 'requirements.txt' ) with open ( reqs path , encoding = 'utf8' ) as f : reqs = [ line . strip ( ) for line in f if not line . strip ( ) . startswith ( '#' ) ] names = [ ] links = [ ] for req in reqs : if '://' in req : links . append ( req ) else : names . append ( req ) return { 'install requires' : names , 'dependency links' : links }", "predictions": ["reads the requirements from the command line ."], "references": ["parses requirements from requirements . txt"], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 477, "code": "def export2hub ( weight file , hub dir , options ) : spec = make module spec ( options , str ( weight file ) ) try : with tf . Graph ( ) . as default ( ) : module = hub . Module ( spec ) with tf . Session ( ) as sess : sess . run ( tf . global variables initializer ( ) ) if hub dir . exists ( ) : shutil . rmtree ( hub dir ) module . export ( str ( hub dir ) , sess ) finally : pass", "predictions": ["instantiates an undo . return true if all the necessary files are . ."], "references": ["exports a tf - hub module"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 478, "code": "def main ( ) : args = parser . parse args ( ) run ms bot framework server ( agent generator = make agent , app id = args . ms id , app secret = args . ms secret , stateful = True )", "predictions": ["command line execution entry point ."], "references": ["parse parameters and run ms bot framework"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 479, "code": "def check gpu existence ( ) : global gpu available if gpu available is None : sess config = tf . Config Proto ( ) sess config . gpu options . allow growth = True try : with tf . Session ( config = sess config ) : device list = device lib . list local devices ( ) gpu available = any ( device . device type == 'GPU' for device in device list ) except Attribute Error as e : log . warning ( f'Got an Attribute Error `{e}`, assuming documentation building' ) gpu available = False return gpu available", "predictions": ["checks if there are gpu options available on configuration ."], "references": ["r return true if at least one gpu is available"], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 480, "code": "def parse config property ( item : T , variables : Dict [ str , Union [ str , Path , float , bool , None ] ] ) -> T : if isinstance ( item , str ) : return item . format ( * * variables ) elif isinstance ( item , list ) : return [ parse config property ( item , variables ) for item in item ] elif isinstance ( item , dict ) : return { k : parse config property ( v , variables ) for k , v in item . items ( ) } else : return item", "predictions": ["training the configuration parameters ."], "references": ["recursively apply config s variables values to its property"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 481, "code": "def parse config ( config : Union [ str , Path , dict ] ) -> dict : if isinstance ( config , ( str , Path ) ) : config = read json ( find config ( config ) ) variables = { 'DEEPPAVLOV PATH' : os . getenv ( f'DP DEEPPAVLOV PATH' , Path ( file ) . parent . parent . parent ) } for name , value in config . get ( 'metadata' , { } ) . get ( 'variables' , { } ) . items ( ) : env name = f'DP {name}' if env name in os . environ : value = os . getenv ( env name ) variables [ name ] = value . format ( * * variables ) return parse config property ( config , variables )", "predictions": ["parses the configuration from a . ."], "references": ["read config s variables and apply their values to all its properties"], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 482, "code": "def expand path ( path : Union [ str , Path ] ) -> Path : return Path ( path ) . expanduser ( ) . resolve ( )", "predictions": ["get a relative path from a path ."], "references": ["convert relative paths to absolute with resolving user directory ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 483, "code": "def from params ( params : Dict , mode : str = 'infer' , serialized : Any = None , * * kwargs ) -> Component : config params = { k : resolve ( v ) for k , v in params . items ( ) } if 'ref' in config params : try : component = refs [ config params [ 'ref' ] ] if serialized is not None : component . deserialize ( serialized ) return component except Key Error : e = Config Error ( 'Component with id \"{id}\" was referenced but not initialized' . format ( id = config params [ 'ref' ] ) ) log . exception ( e ) raise e elif 'config path' in config params : from deeppavlov . core . commands . infer import build model refs = refs . copy ( ) refs . clear ( ) config = parse config ( expand path ( config params [ 'config path' ] ) ) model = build model ( config , serialized = serialized ) refs . clear ( ) refs . update ( refs ) try : refs [ config params [ 'id' ] ] = model except Key Error : pass return model cls name = config params . pop ( 'class name' , None ) if not cls name : e = Config Error ( 'Component config has no `class name` nor `ref` fields' ) log . exception ( e ) raise e cls = get model ( cls name ) config params = { k : init param ( v , mode ) for k , v in config params . items ( ) } try : spec = inspect . getfullargspec ( cls ) if 'mode' in spec . args + spec . kwonlyargs or spec . varkw is not None : kwargs [ 'mode' ] = mode component = cls ( * * dict ( config params , * * kwargs ) ) try : refs [ config params [ 'id' ] ] = component except Key Error : pass except Exception : log . exception ( \"Exception in {}\" . format ( cls ) ) raise if serialized is not None : component . deserialize ( serialized ) return component", "predictions": ["returns a dictionary of configuration parameters that can be used to process evaluation evaluation ."], "references": ["builds and returns the component from corresponding dictionary of parameters ."], "bleu": 0.12874330508144843, "rouge_l": 0.39559014267185466}
{"id": 484, "code": "def run ( self ) -> None : while True : request = self . input queue . get ( ) response = self . handle request ( request ) self . output queue . put ( response )", "predictions": ["the main thread to attention to the client ."], "references": ["thread run method implementation ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 485, "code": "def refresh valid certs ( self ) -> None : self . timer = Timer ( REFRESH VALID CERTS PERIOD SECS , self . refresh valid certs ) self . timer . start ( ) expired certificates = [ ] for valid cert url , valid cert in self . valid certificates . items ( ) : valid cert : Validated Cert = valid cert cert expiration time : datetime = valid cert . expiration timestamp if datetime . utcnow ( ) > cert expiration time : expired certificates . append ( valid cert url ) for expired cert url in expired certificates : del self . valid certificates [ expired cert url ] log . info ( f'Validation period of {expired cert url} certificate expired' )", "predictions": ["this method removes all [ 0 , 6 ] from all [ 0 , 6 ] ."], "references": ["conducts cleanup of periodical certificates with expired validation ."], "bleu": 0.07223943354597204, "rouge_l": 0.0814419225634179}
{"id": 486, "code": "def cls from str ( name : str ) -> type : try : module name , cls name = name . split ( ':' ) except Value Error : raise Config Error ( 'Expected class description in a `module.submodules:Class Name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import module ( module name ) , cls name )", "predictions": ["in this class , find it in the name of the full class ."], "references": ["returns a class object with the name given as a string ."], "bleu": 0.13217947626377288, "rouge_l": 0.31202046035805625}
{"id": 487, "code": "def get model ( name : str ) -> type : if name not in REGISTRY : if ':' not in name : raise Config Error ( \"Model {} is not registered.\" . format ( name ) ) return cls from str ( name ) return cls from str ( REGISTRY [ name ] )", "predictions": ["read a data structure for the class ."], "references": ["returns a registered class object with the name given in the string ."], "bleu": 0.10793517579160734, "rouge_l": 0.2739520958083832}
{"id": 488, "code": "def list jobs ( self ) : res = h2o . api ( \"GET /3/Jobs\" ) table = [ [ \"type\" ] , [ \"dest\" ] , [ \"description\" ] , [ \"status\" ] ] for job in res [ \"jobs\" ] : job dest = job [ \"dest\" ] table [ 0 ] . append ( self . translate job type ( job dest [ \"type\" ] ) ) table [ 1 ] . append ( job dest [ \"name\" ] ) table [ 2 ] . append ( job [ \"description\" ] ) table [ 3 ] . append ( job [ \"status\" ] ) return table", "predictions": ["train a train for this to all evaluate evaluate the to a train ."], "references": ["list all jobs performed by the cluster ."], "bleu": 0.10511846841633776, "rouge_l": 0.28683385579937304}
{"id": 489, "code": "def list timezones ( self ) : from h2o . expr import Expr Node return h2o . H2O Frame . expr ( expr = Expr Node ( \"list Time Zones\" ) ) . frame ( )", "predictions": ["a load operation for this node ."], "references": ["return the list of all known timezones ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 490, "code": "def summary ( self , key , column = \"C1\" , timeout Secs = 10 , * * kwargs ) : params dict = { } h2o methods . check params update kwargs ( params dict , kwargs , 'summary' , True ) result = self . do json request ( '3/Frames.json/%s/columns/%s/summary' % ( key , column ) , timeout = timeout Secs , params = params dict ) h2o sandbox . check sandbox for errors ( ) return result", "predictions": ["add a build method to the client ."], "references": ["return the summary for a single column for a single frame in the h2o cluster ."], "bleu": 0.07015765577419673, "rouge_l": 0.23582474226804123}
{"id": 491, "code": "def delete frame ( self , key , ignore Missing Key = True , timeout Secs = 60 , * * kwargs ) : assert key is not None , '\"key\" parameter is null' result = self . do json request ( '/3/Frames.json/' + key , cmd = 'delete' , timeout = timeout Secs ) if not ignore Missing Key and 'f00b4r' in result : raise Value Error ( 'Frame key not found: ' + key ) return result", "predictions": ["deletes a word from the underlying storage ."], "references": ["delete a frame on the h2o cluster given its key ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 492, "code": "def compute model metrics ( self , model , frame , timeout Secs = 60 , * * kwargs ) : assert model is not None , '\"model\" parameter is null' assert frame is not None , '\"frame\" parameter is null' models = self . models ( key = model , timeout Secs = timeout Secs ) assert models is not None , \"/Models REST call failed\" assert models [ 'models' ] [ 0 ] [ 'model id' ] [ 'name' ] == model , \"/Models/{0} returned Model {1} rather than Model {2}\" . format ( model , models [ 'models' ] [ 0 ] [ 'key' ] [ 'name' ] , model ) frames = self . frames ( key = frame ) assert frames is not None , \"/Frames/{0} REST call failed\" . format ( frame ) print \"frames:\" , dump json ( frames ) result = self . do json request ( '/3/Model Metrics.json/models/' + model + '/frames/' + frame , cmd = 'post' , timeout = timeout Secs ) mm = result [ 'model metrics' ] [ 0 ] verboseprint ( \"model metrics: \" + repr ( mm ) ) h2o sandbox . check sandbox for errors ( ) return mm", "predictions": ["extend the ) on the ) transaction ."], "references": ["score a model on the h2o cluster on the given frame and return only the model metrics ."], "bleu": 0.06870470052394348, "rouge_l": 0.28773584905660377}
{"id": 493, "code": "def delete model ( self , key , ignore Missing Key = True , timeout Secs = 60 , * * kwargs ) : assert key is not None , '\"key\" parameter is null' result = self . do json request ( '/3/Models.json/' + key , cmd = 'delete' , timeout = timeout Secs ) if not ignore Missing Key and 'f00b4r' in result : raise Value Error ( 'Model key not found: ' + key ) verboseprint ( \"delete model result:\" , dump json ( result ) ) return result", "predictions": ["deletes a classnode from the underlying storage ."], "references": ["delete a model on the h2o cluster given its key ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 494, "code": "def tabulate ( self , tablefmt = \"simple\" , rollups = False , rows = 10 ) : if not self . is valid ( ) : self . fill ( rows = rows ) d = collections . Ordered Dict ( ) if rollups : col = next ( iter ( viewvalues ( self . data ) ) ) lrows = len ( col [ 'data' ] ) d [ \"\" ] = [ \"type\" , \"mins\" , \"mean\" , \"maxs\" , \"sigma\" , \"zeros\" , \"missing\" ] + list ( map ( str , range ( lrows ) ) ) for k , v in viewitems ( self . data ) : x = v [ 'data' ] t = v [ \"type\" ] if t == \"enum\" : domain = v [ 'domain' ] x = [ \"\" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] elif t == \"time\" : x = [ \"\" if math . isnan ( z ) else time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( z / 1000 ) ) for z in x ] if rollups : mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ \"type\" ] != \"enum\" else None maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ \"type\" ] != \"enum\" else None #Cross check type with mean and sigma. Set to None if of type enum. if v [ 'type' ] == \"enum\" : v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero count' ] = None x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero count' ] , v [ 'missing count' ] ] + x d [ k ] = x return tabulate . tabulate ( d , headers = \"keys\" , tablefmt = tablefmt )", "predictions": [". : this is a set of ( mins wrap the algorithm in the case of mins ."], "references": ["pretty tabulated string of all the cached data and column names"], "bleu": 0.07535838128770536, "rouge_l": 0.14420803782505912}
{"id": 495, "code": "def run instances ( count , ec2 config , region , wait For SSH = True , tags = None ) : ec2params = inheritparams ( ec2 config , EC2 API RUN INSTANCE ) ec2params . setdefault ( 'min count' , count ) ec2params . setdefault ( 'max count' , count ) reservation = None conn = ec2 connect ( region ) try : reservation = conn . run instances ( * * ec2params ) log ( 'Reservation: {0}' . format ( reservation . id ) ) log ( 'Waiting for {0} EC2 instances {1} to come up, this can take 1-2 minutes.' . format ( len ( reservation . instances ) , reservation . instances ) ) start = time . time ( ) time . sleep ( 1 ) for instance in reservation . instances : while instance . update ( ) == 'pending' : time . sleep ( 1 ) h2o cmd . dot ( ) if not instance . state == 'running' : raise Exception ( '\\033[91m[ec2] Error waiting for running state. Instance is in state {0}.\\033[0m' . format ( instance . state ) ) log ( 'Instances started in {0} seconds' . format ( time . time ( ) - start ) ) log ( 'Instances: ' ) for inst in reservation . instances : log ( \"   {0} ({1}) : public ip: {2}, private ip: {3}\" . format ( inst . public dns name , inst . id , inst . ip address , inst . private ip address ) ) if wait For SSH : wait for ssh ( [ i . private ip address for i in reservation . instances ] ) try : if tags : conn . create tags ( [ i . id for i in reservation . instances ] , tags ) except : warn ( 'Something wrong during tagging instances. Exceptions IGNORED!' ) print sys . exc info ( ) pass return reservation except : print \"\\033[91m Unexpected error\\033[0m :\" , sys . exc info ( ) if reservation : terminate reservation ( reservation , region ) raise", "predictions": ["poll metrics and wait for all metrics ."], "references": ["create a new reservation for count instances"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 496, "code": "def terminate instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( \"Terminating instances {0}.\" . format ( instances ) ) conn . terminate instances ( instances ) log ( \"Done\" )", "predictions": ["load and return a connection ."], "references": ["terminate all the instances given by its ids"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 497, "code": "def stop instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( \"Stopping instances {0}.\" . format ( instances ) ) conn . stop instances ( instances ) log ( \"Done\" )", "predictions": ["save connection for all ( and associated ( if any scopes have been started scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes scopes ."], "references": ["stop all the instances given by its ids"], "bleu": 0.03011857955989304, "rouge_l": 0.04736024844720497}
{"id": 498, "code": "def start instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( \"Starting instances {0}.\" . format ( instances ) ) conn . start instances ( instances ) log ( \"Done\" )", "predictions": ["print the connection to the list of logged number ."], "references": ["start all the instances given by its ids"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 499, "code": "def reboot instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( \"Rebooting instances {0}.\" . format ( instances ) ) conn . reboot instances ( instances ) log ( \"Done\" )", "predictions": ["search for ( re - blocking d d d d , destroy d , ( d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d"], "references": ["reboot all the instances given by its ids"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 500, "code": "def wait for ssh ( ips , port = 22 , skip Alive = True , requiredsuccess = 3 ) : log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) for ip in ips : if not skip Alive or not ssh live ( ip , port ) : log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) count = 0 while count < requiredsuccess : if ssh live ( ip , port ) : count += 1 else : count = 0 time . sleep ( 1 ) h2o cmd . dot ( )", "predictions": ["waits for all ( spaces dict dict dict dict dict of ( dict , , , , , , , , , , , , , , , , , , , , then make a reference to the + 1 dict . dict , then we get the last"], "references": ["wait for ssh service to appear on given hosts"], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 501, "code": "def join ( self ) : self . future = False self . job . poll ( ) model key = self . job . dest key self . job = None model json = h2o . api ( \"GET /%d/Models/%s\" % ( self . rest version , model key ) ) [ \"models\" ] [ 0 ] self . resolve model ( model key , model json )", "predictions": ["creates a new ( ."], "references": ["wait until job s completion ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 502, "code": "def signal handler ( signum , stackframe ) : global g runner global g handling signal if g handling signal : return g handling signal = True print ( \"\" ) print ( \"----------------------------------------------------------------------\" ) print ( \"\" ) print ( \"SIGNAL CAUGHT (\" + str ( signum ) + \").  TEARING DOWN CLOUDS.\" ) print ( \"\" ) print ( \"----------------------------------------------------------------------\" ) g runner . terminate ( )", "predictions": ["get a build model with the given stackframe and . ."], "references": ["helper function to handle caught signals ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 503, "code": "def wipe output dir ( ) : print ( \"Wiping output directory.\" ) try : if os . path . exists ( g output dir ) : shutil . rmtree ( str ( g output dir ) ) except OS Error as e : print ( \"ERROR: Removing output directory %s failed: \" % g output dir ) print ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) print ( \"\" ) sys . exit ( 1 )", "predictions": ["interact this directory , then create it from the given directory ."], "references": ["clear the output directory ."], "bleu": 0.14694106251955755, "rouge_l": 0.38125000000000003}
{"id": 504, "code": "def get ip ( self ) : if len ( self . client nodes ) > 0 : node = self . client nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get ip ( )", "predictions": ["this method returns on this node ."], "references": ["return an ip to use to talk to this cluster ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 505, "code": "def get port ( self ) : if len ( self . client nodes ) > 0 : node = self . client nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get port ( )", "predictions": ["fn this session object ."], "references": ["return a port to use to talk to this cluster ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 506, "code": "def determine vec size ( self ) : first column = self . pre trained . types [ self . pre trained . columns [ 0 ] ] if first column != 'string' : raise H2O Value Error ( \"First column of given pre trained model %s is required to be a String\" , self . pre trained . frame id ) if list ( self . pre trained . types . values ( ) ) . count ( 'string' ) > 1 : raise H2O Value Error ( \"There are multiple columns in given pre trained model %s with a String type.\" , self . pre trained . frame id ) self . vec size = self . pre trained . dim [ 1 ] - 1", "predictions": ["for each column , the new model will always be , ."], "references": ["determines vec_size for a pre - trained model after basic model verification ."], "bleu": 0.11368272367804307, "rouge_l": 0.23828125000000006}
{"id": 507, "code": "def get lambda source code ( lambda fn , src ) : def gen lambdas ( ) : def gen ( ) : yield src + \"\\n\" g = gen ( ) step = 0 tokens = [ ] for tok in tokenize . generate tokens ( getattr ( g , \"next\" , getattr ( g , \" next \" , None ) ) ) : if step == 0 : if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == \"lambda\" : step = 1 tokens = [ tok ] level = 0 elif step == 1 : if tok [ 0 ] == tokenize . NAME : tokens . append ( tok ) step = 2 else : step = 0 elif step == 2 : if tok [ 0 ] == tokenize . OP and tok [ 1 ] == \":\" : tokens . append ( tok ) step = 3 else : step = 0 elif step == 3 : if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in \",)\" or tok [ 0 ] == tokenize . ENDMARKER ) : yield tokenize . untokenize ( tokens ) . strip ( ) step = 0 else : tokens . append ( tok ) if tok [ 0 ] == tokenize . OP : if tok [ 1 ] in \"[({\" : level += 1 if tok [ 1 ] in \"])}\" : level -= 1 assert not tokens actual code = lambda fn . code . co code for lambda src in gen lambdas ( ) : try : fn = eval ( lambda src , globals ( ) , locals ( ) ) if fn . code . co code == actual code : return lambda src . split ( \":\" , 1 ) [ 1 ] . strip ( ) except Exception : pass return \"<lambda>\"", "predictions": [", , return an iterator of all the same breaks as the given by the given by the given by the given by the given by the given iterator ."], "references": ["attempt to find the source code of the lambda_fn within the string src ."], "bleu": 0.053091875602826286, "rouge_l": 0.1945773524720893}
{"id": 508, "code": "def name ( self , src = None ) : res = [ get type name ( tt , src ) for tt in self . types ] if len ( res ) == 2 and \"None\" in res : res . remove ( \"None\" ) return \"?\" + res [ 0 ] else : return \" | \" . join ( res )", "predictions": ["removes an event from this map ."], "references": ["return string representing the name of this type ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 509, "code": "def check ( self , var ) : return all ( check type ( var , tt ) for tt in self . types )", "predictions": ["wraps the given variable in this sequence ."], "references": ["return true if the variable matches this type and false otherwise ."], "bleu": 0.1223065774797558, "rouge_l": 0.3860759493670886}
{"id": 510, "code": "def name ( self , src = None ) : return \" & \" . join ( get type name ( tt , src ) for tt in self . types )", "predictions": ["alias for this element in the multi - memory representation ."], "references": ["return string representing the name of this type ."], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 511, "code": "def check ( self , var ) : return not any ( check type ( var , tt ) for tt in self . types )", "predictions": ["conditionally check that this is not a legal variable in the gradient ."], "references": ["return true if the variable does not match any of the types and false otherwise ."], "bleu": 0.09535849051927603, "rouge_l": 0.2031076581576027}
{"id": 512, "code": "def name ( self , src = None ) : if len ( self . types ) > 1 : return \"!(%s)\" % str ( \"|\" . join ( get type name ( tt , src ) for tt in self . types ) ) else : return \"!\" + get type name ( self . types [ 0 ] , src )", "predictions": ["maps this event to the specified class ."], "references": ["return string representing the name of this type ."], "bleu": 0.16829946711936866, "rouge_l": 0.232824427480916}
{"id": 513, "code": "def check ( self , var ) : return isinstance ( var , tuple ) and all ( check type ( t , self . element type ) for t in var )", "predictions": ["converts a tuple to a tuple of strings ."], "references": ["return true if the variable matches this type and false otherwise ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 514, "code": "def check ( self , var ) : if not isinstance ( var , dict ) : return False if any ( key not in self . types for key in var ) : return False for key , ktype in viewitems ( self . types ) : val = var . get ( key , None ) if not check type ( val , ktype ) : return False return True", "predictions": ["check whether this variable is any of the given variable ."], "references": ["return true if the variable matches this type and false otherwise ."], "bleu": 0.1307847403141535, "rouge_l": 0.25884016973125884}
{"id": 515, "code": "def name ( self , src = None ) : return \"{%s}\" % \", \" . join ( \"%s: %s\" % ( key , get type name ( ktype , src ) ) for key , ktype in viewitems ( self . types ) )", "predictions": ["print out the full name of this element in the map ."], "references": ["return string representing the name of this type ."], "bleu": 0.21401603033752975, "rouge_l": 0.48878205128205127}
{"id": 516, "code": "def check ( self , var ) : return ( isinstance ( var , int type ) and ( self . lower bound is None or var >= self . lower bound ) and ( self . upper bound is None or var <= self . upper bound ) )", "predictions": ["method to check whether this field is in the specified sub - way ."], "references": ["return true if the variable matches the specified type ."], "bleu": 0.1250076305588977, "rouge_l": 0.2577464788732394}
{"id": 517, "code": "def name ( self , src = None ) : if self . upper bound is None and self . lower bound is None : return \"int\" if self . upper bound is None : if self . lower bound == 1 : return \"int>0\" return \"int\u2265%d\" % s lf. l ower bound if self . lower bound is None : return \"int\u2264%d\" % s lf. u pper bound return \"int[%d\u2026%d]\" % ( e lf. l ower bound,  s lf. u pper bound)", "predictions": ["return an item for this region ."], "references": ["return string representing the name of this type ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 518, "code": "def check ( self , var ) : return ( isinstance ( var , num type ) and ( self . lower bound is None or var >= self . lower bound ) and ( self . upper bound is None or var <= self . upper bound ) )", "predictions": ["method to check whether this field is in the specified sub - way ."], "references": ["return true if the variable matches the specified type ."], "bleu": 0.1250076305588977, "rouge_l": 0.2577464788732394}
{"id": 519, "code": "def check ( self , var ) : if self . class is None : self . init ( ) return self . class and self . checker ( var , self . class )", "predictions": ["factory for . , verifies that the ` ( ` . ` var ` var ` ."], "references": ["return true if the variable matches this type and false otherwise ."], "bleu": 0.07994607499472013, "rouge_l": 0.1423570595099183}
{"id": 520, "code": "def check ( self , var ) : if not isinstance ( var , str type ) : return False return enum mangle ( var ) in self . consts", "predictions": ["check that the given variable is in the set of variables ."], "references": ["check whether the provided value is a valid enum constant ."], "bleu": 0.13065113298388567, "rouge_l": 0.3505747126436781}
{"id": 521, "code": "def get config ( ) : self = H2O Config Reader . get instance ( ) if not self . config loaded : self . read config ( ) return self . config", "predictions": ["retrieves the configuration for this config object ."], "references": ["retrieve the config as a dictionary of key - value pairs ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 522, "code": "def read config ( self ) : self . config loaded = True conf = [ ] for f in self . candidate log files ( ) : if os . path . isfile ( f ) : self . logger . info ( \"Reading config file %s\" % f ) section rx = re . compile ( r\"^\\[(\\w+)\\]$\" ) keyvalue rx = re . compile ( r\"^(\\w+:)?([\\w.]+)\\s*=(.*)$\" ) with io . open ( f , \"rt\" , encoding = \"utf-8\" ) as config file : section name = None for lineno , line in enumerate ( config file ) : line = line . strip ( ) if line == \"\" or line . startswith ( \"#\" ) : continue m1 = section rx . match ( line ) if m1 : section name = m1 . group ( 1 ) continue m2 = keyvalue rx . match ( line ) if m2 : lng = m2 . group ( 1 ) key = m2 . group ( 2 ) val = m2 . group ( 3 ) . strip ( ) if lng and lng . lower ( ) != \"py:\" : continue if section name : key = section name + \".\" + key if key in H2O Config Reader . allowed config keys : conf . append ( ( key , val ) ) else : self . logger . error ( \"Key %s is not a valid config key\" % key ) continue self . logger . error ( \"Syntax error in config file line %d: %s\" % ( lineno , line ) ) self . config = dict ( conf ) return", "predictions": ["reads in configuration file of the given fo using the given ( ."], "references": ["find and parse config file storing all variables in self . _config ."], "bleu": 0.1135935489027116, "rouge_l": 0.15384615384615383}
{"id": 523, "code": "def candidate log files ( ) : relpath = \".h2oconfig\" prevpath = None while True : abspath = os . path . abspath ( relpath ) if abspath == prevpath : break prevpath = abspath relpath = \"../\" + relpath yield abspath yield os . path . expanduser ( \"~/.h2oconfig\" )", "predictions": ["yields an iterator of all \"~/.h2oconfig\" in the given files ."], "references": ["return possible locations for the . h2oconfig file one at a time ."], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 524, "code": "def recalculate model parameters ( self , now ) : time until end = self . estimate progress completion time ( now ) - now assert time until end >= 0 , \"Estimated progress completion cannot be in the past.\" x real = self . get real progress ( ) if x real == 1 : t0 , x0 , v0 , ve = now , 1 , 0 , 0 else : x0 , v0 = self . compute progress at time ( now ) t0 = now if x0 >= 1 : t0 , x0 , v0 = self . t0 , self . x0 , self . v0 time until end += now - t0 z = self . BETA * time until end max speed = ( 1 - x real ** 2 ) / self . FINISH DELAY ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) if ve < 0 : v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) ve = 0 if ve > max speed : ve = max speed self . t0 , self . x0 , self . v0 , self . ve = t0 , x0 , v0 , ve", "predictions": ["determine the number of methods ."], "references": ["compute t0 x0 v0 ve ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 525, "code": "def draw ( self , txt , final = False ) : if not self . file mode : sys . stdout . write ( \"\\r\" ) sys . stdout . write ( txt ) if final and not isinstance ( self . widget , Hidden Widget ) : sys . stdout . write ( \"\\n\" ) else : if not self . file mode : sys . stdout . write ( \"\\r\" ) sys . stdout . flush ( )", "predictions": ["draw the widget on the current figure ."], "references": ["print the rendered string to the stdout ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 526, "code": "def render ( self , progress , width = None , status = None ) : results = [ widget . render ( progress , width = self . widget lengths [ i ] , status = status ) for i , widget in enumerate ( self . widgets ) ] if self . file mode : res = \"\" for i , result in enumerate ( results ) : res += result . rendered if result . length < self . widget lengths [ i ] and progress < 1 : break res += \" \" if i < len ( results ) - 1 else \"\" rendered str = res [ len ( self . rendered ) : ] self . rendered = res else : rendered str = \" \" . join ( r . rendered for r in results ) if self . to render : rendered str = self . to render + rendered str self . to render = None next progress = min ( r . next progress for r in results ) next time = min ( r . next time for r in results ) return Render Result ( rendered str , next progress = next progress , next time = next time )", "predictions": ["render the precalculated ast ."], "references": ["render the widget ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 527, "code": "def compute widget sizes ( self ) : wl = [ 0 ] * len ( self . widgets ) flex count = 0 for i , widget in enumerate ( self . widgets ) : if isinstance ( widget , Progress Bar Flexible Widget ) : flex count += 1 else : wl [ i ] = widget . render ( 1 ) . length remaining width = self . width - sum ( wl ) remaining width -= len ( self . widgets ) - 1 if remaining width < 10 * flex count : if self . file mode : remaining width = 10 * flex count else : widget0 = self . widgets [ 0 ] if isinstance ( widget0 , PBW String ) and remaining width + widget0 . render ( 0 ) . length >= 10 * flex count : remaining width += widget0 . render ( 0 ) . length + 1 self . to render = widget0 . render ( 0 ) . rendered + \"\\n\" self . widgets = self . widgets [ 1 : ] if remaining width < 10 * flex count : self . file mode = True remaining width = 10 * flex count remaining width = max ( remaining width , 10 * flex count ) for i , widget in enumerate ( self . widgets ) : if isinstance ( widget , Progress Bar Flexible Widget ) : target length = int ( remaining width / flex count ) result = widget . render ( 1 , target length ) wl [ i ] = result . length remaining width -= result . length flex count -= 1 return wl", "predictions": ["for each widget . we want to generate the rendered widget ."], "references": ["initial rendering stage done in order to compute widths of all widgets ."], "bleu": 0.10579369505074822, "rouge_l": 0.15885416666666669}
{"id": 528, "code": "def get terminal size ( ) : if not sys . stdout . isatty ( ) : return 80 try : import subprocess ret = subprocess . check output ( [ \"stty\" , \"size\" ] ) . strip ( ) . split ( \" \" ) if len ( ret ) == 2 : return int ( ret [ 1 ] ) except : pass try : from termios import TIOCGWINSZ from fcntl import ioctl from struct import unpack res = unpack ( \"hh\" , ioctl ( sys . stdout , TIOCGWINSZ , b\"1234\" ) ) return int ( res [ 1 ] ) except : pass return int ( os . environ . get ( \"COLUMNS\" , 80 ) )", "predictions": ["get system terminal size ."], "references": ["find current stdout s width in characters ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 529, "code": "def render ( self , progress , width = None , status = None ) : if width <= 3 : return Render Result ( ) bar width = width - 2 n chars = int ( progress * bar width + 0.001 ) endf , endl = self . bar ends if self . file mode : out = endf out += self . bar symbols [ - 1 ] * n chars out += endl if progress == 1 else \"\" if status : out += \" (%s)\" % status next progress = ( n chars + 1 ) / bar width rendered len = len ( out ) else : frac chars = int ( ( progress * bar width - n chars ) * len ( self . bar symbols ) ) out = endf out += self . bar symbols [ - 1 ] * n chars out += self . bar symbols [ frac chars - 1 ] if frac chars > 0 else \"\" rendered len = len ( out ) if status : out += colorama . Fore . RED + \" (\" + status + \")\" + colorama . Style . RESET ALL rendered len += 3 + len ( status ) out += \" \" * ( width - 1 - rendered len ) out += endl next progress = ( n chars + ( frac chars + 1 ) / len ( self . bar symbols ) ) / bar width rendered len += max ( 0 , width - 1 - rendered len ) + 1 return Render Result ( rendered = out , length = rendered len , next progress = next progress )", "predictions": ["renders the progress object to the ( ."], "references": ["render the widget ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 530, "code": "def set encoding ( self , encoding ) : self . bar ends = \"[]\" self . bar symbols = \"#\" if not encoding : return s1 = \"\\u258F\\u258E\\u258D\\u258C\\u258B\\u258A\\u2589\\u2588\" s2 = \"\\u258C\\u2588\" s3 = \"\\u2588\" if self . file mode : s1 = s2 = None assert len ( s3 ) == 1 for s in ( s1 , s2 , s3 ) : if s is None : continue try : s . encode ( encoding ) self . bar ends = \"||\" self . bar symbols = s return except Unicode Encode Error : pass except Lookup Error : print ( \"Warning: unknown encoding %s\" % encoding )", "predictions": ["set the encoding of the read operation ."], "references": ["inform the widget about the encoding of the underlying character stream ."], "bleu": 0.2833335058083106, "rouge_l": 0.4825949367088607}
{"id": 531, "code": "def render ( self , progress , width = None , status = None ) : current pct = int ( progress * 100 + 0.1 ) return Render Result ( rendered = \"%3d%%\" % current pct , next progress = ( current pct + 1 ) / 100 )", "predictions": ["renders the progress message to a file using the result of calling the currently running ."], "references": ["render the widget ."], "bleu": 0.08513012360883544, "rouge_l": 0.22426470588235295}
{"id": 532, "code": "def refresh ( self ) : self . ex . cache . flush ( ) self . frame ( fill cache = True )", "predictions": ["refresh the cache and get appropriate cache ."], "references": ["reload frame information from the backend h2o server ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 533, "code": "def structure ( self ) : df = self . as data frame ( use pandas = False ) cn = df . pop ( 0 ) nr = self . nrow nc = self . ncol width = max ( [ len ( c ) for c in cn ] ) isfactor = self . isfactor ( ) numlevels = self . nlevels ( ) lvls = self . levels ( ) print ( \"H2O Frame: '{}' \\n Dimensions: {} obs. of {} variables\" . format ( self . frame id , nr , nc ) ) for i in range ( nc ) : print ( \"$ {} {}: \" . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) if isfactor [ i ] : nl = numlevels [ i ] print ( \"Factor w/ {} level(s) {} \" . format ( nl , '\"' + '\",\"' . join ( lvls [ i ] ) + '\"' ) , end = '\\n' ) else : print ( \"num {}\" . format ( \" \" . join ( it [ 0 ] if it else \"nan\" for it in h2o . as list ( self [ : 10 , i ] , False ) [ 1 : ] ) ) )", "predictions": ["creates a d3d structure for a 3d set of players ."], "references": ["compactly display the internal structure of an h2oframe ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 534, "code": "def parse text ( text ) : assert isinstance ( text , str type ) , \"`text` parameter should be a string, got %r\" % type ( text ) gen = iter ( text . splitlines ( True ) ) readline = gen . next if hasattr ( gen , \"next\" ) else gen . next return Code ( tokenize ( readline ) )", "predictions": ["parse text of the first code of this function ."], "references": ["parse code from a string of text ."], "bleu": 0.16590387014219712, "rouge_l": 0.4535315985130111}
{"id": 535, "code": "def parse file ( filename ) : assert isinstance ( filename , str type ) , \"`filename` parameter should be a string, got %r\" % type ( filename ) with open ( filename , \"rt\" , encoding = \"utf-8\" ) as f : return Code ( tokenize ( f . readline ) )", "predictions": ["parse the file . this is done by the name of the file ."], "references": ["parse the provided file and return code object ."], "bleu": 0.13217947626377288, "rouge_l": 0.3620178041543027}
{"id": 536, "code": "def move ( self , drow , dcol = 0 ) : self . start row += drow self . start col += dcol self . end row += drow self . end col += dcol", "predictions": ["example : this is meant to provide ( to move to a default ."], "references": ["move the token by drow rows and dcol columns ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 537, "code": "def unparse ( self ) : ut = Untokenizer ( start row = self . tokens [ 0 ] . start row ) self . unparse ( ut ) return ut . result ( )", "predictions": ["get the jcombobox for this row ."], "references": ["convert the parsed representation back into the source code ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 538, "code": "def centers ( self ) : o = self . model json [ \"output\" ] cvals = o [ \"centers\" ] . cell values centers = [ list ( cval [ 1 : ] ) for cval in cvals ] return centers", "predictions": ["do a centers of this object ."], "references": ["the centers for the kmeans model ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 539, "code": "def centers std ( self ) : o = self . model json [ \"output\" ] cvals = o [ \"centers std\" ] . cell values centers std = [ list ( cval [ 1 : ] ) for cval in cvals ] centers std = [ list ( x ) for x in zip ( * centers std ) ] return centers std", "predictions": ["this is a convenience function for ( ."], "references": ["the standardized centers for the kmeans model ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 540, "code": "def version check ( ) : from . init import version as ver pkg ci = h2oconn . cluster if not ci : raise H2O Connection Error ( \"Connection not initialized. Did you run h2o.connect()?\" ) ver h2o = ci . version if ver pkg == \"SUBST PROJECT VERSION\" : ver pkg = \"UNKNOWN\" if str ( ver h2o ) != str ( ver pkg ) : branch name h2o = ci . branch name build number h2o = ci . build number if build number h2o is None or build number h2o == \"unknown\" : raise H2O Connection Error ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Upgrade H2O and h2o-Python to latest stable version - \" \"http://h2o-release.s3.amazonaws.com/h2o/latest stable.html\" \"\" . format ( ver h2o , ver pkg ) ) elif build number h2o == \"99999\" : raise H2O Connection Error ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"This is a developer build, please contact your developer.\" \"\" . format ( ver h2o , ver pkg ) ) else : raise H2O Connection Error ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Install the matching h2o-Python version from - \" \"http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html.\" \"\" . format ( ver h2o , ver pkg , branch name h2o , build number h2o ) ) if ci . build too old : print ( \"Warning: Your H2O cluster version is too old ({})! Please download and install the latest \" \"version from http://h2o.ai/download/\" . format ( ci . build age ) )", "predictions": ["check the latest version of the given ( ."], "references": ["used to verify that h2o - python module and the h2o server are compatible with each other ."], "bleu": 0.05746166391236872, "rouge_l": 0.13974799541809851}
{"id": 541, "code": "def load dataset ( relative path ) : assert is type ( relative path , str ) h2o dir = os . path . split ( file ) [ 0 ] for possible file in [ os . path . join ( h2o dir , relative path ) , os . path . join ( h2o dir , \"h2o data\" , relative path ) , os . path . join ( h2o dir , \"h2o data\" , relative path + \".csv\" ) ] : if os . path . exists ( possible file ) : return upload file ( possible file ) raise H2O Value Error ( \"Data file %s cannot be found\" % relative path )", "predictions": ["load a dataset from the provided path ."], "references": ["imports a data file within the h2o_data folder ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 542, "code": "def check frame id ( frame id ) : if frame id is None : return if frame id . strip ( ) == \"\" : raise H2O Value Error ( \"Frame id cannot be an empty string: %r\" % frame id ) for i , ch in enumerate ( frame id ) : if ch == \"$\" and i == 0 : continue if ch not in id allowed characters : raise H2O Value Error ( \"Character '%s' is illegal in frame id: %s\" % ( ch , frame id ) ) if re . match ( r\"-?[0-9]\" , frame id ) : raise H2O Value Error ( \"Frame id cannot start with a number: %s\" % frame id )", "predictions": ["check a frame or a single frame id ."], "references": ["check that the provided frame id is valid in rapids language ."], "bleu": 0.15122637383061946, "rouge_l": 0.3713850837138508}
{"id": 543, "code": "def slice is normalized ( s ) : return ( s . start is not None and s . stop is not None and s . step is not None and s . start <= s . stop )", "predictions": ["check if a given string is a slice of another ."], "references": ["return true if slice s in normalized form ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 544, "code": "def deprecated ( message ) : from traceback import extract stack assert message , \"`message` argument in @deprecated is required.\" def deprecated decorator ( fun ) : def decorator invisible ( * args , * * kwargs ) : stack = extract stack ( ) assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == \"decorator invisible\" , \"Got confusing stack... %r\" % stack print ( \"[WARNING] in %s line %d:\" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) print ( \"    >>> %s\" % ( stack [ - 2 ] [ 3 ] or \"????\" ) ) print ( \"        ^^^^ %s\" % message ) return fun ( * args , * * kwargs ) decorator invisible . doc = message decorator invisible . name = fun . name decorator invisible . module = fun . module decorator invisible . deprecated = True return decorator invisible return deprecated decorator", "predictions": ["str str decorator for receipt command . this is a ( operation ."], "references": ["the decorator to mark deprecated functions ."], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 545, "code": "def join ( self ) : self . future = False self . job . poll ( ) self . job = None", "predictions": ["create a new processed tuple ."], "references": ["wait until grid finishes computing ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 546, "code": "def summary ( self , header = True ) : table = [ ] for model in self . models : model summary = model . model json [ \"output\" ] [ \"model summary\" ] r values = list ( model summary . cell values [ 0 ] ) r values [ 0 ] = model . model id table . append ( r values ) print ( ) if header : print ( 'Grid Summary:' ) print ( ) H2O Display ( table , [ 'Model Id' ] + model summary . col header [ 1 : ] , numalign = \"left\" , stralign = \"left\" )", "predictions": ["for the given isinstance ."], "references": ["print a detailed summary of the explored models ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 547, "code": "def show ( self ) : hyper combos = itertools . product ( * list ( self . hyper params . values ( ) ) ) if not self . models : c values = [ [ idx + 1 , list ( val ) ] for idx , val in enumerate ( hyper combos ) ] print ( H2O Two Dim Table ( col header = [ 'Model' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper params . keys ( ) ) ) + ']' ] , table header = 'Grid Search of Model ' + self . model . class . name , cell values = c values ) ) else : print ( self . sorted metric table ( ) )", "predictions": ["name of the search criterion ."], "references": ["print models sorted by metric ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 548, "code": "def parse ( self ) : f = open ( self . parse log path , \"r\" ) self . parse2 ( f ) f . close ( )", "predictions": ["check for the given object and add to the list ."], "references": ["parse file specified by constructor ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 549, "code": "def log start transaction ( self , endpoint , data , json , files , params ) : self . requests counter += 1 if not self . is logging : return msg = \"\\n---- %d --------------------------------------------------------\\n\" % self . requests counter msg += \"[%s] %s\\n\" % ( time . strftime ( \"%H:%M:%S\" ) , endpoint ) if params is not None : msg += \"     params: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( params ) ) if data is not None : msg += \"     body: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( data ) ) if json is not None : import json as j msg += \"     json: %s\\n\" % j . dumps ( json ) if files is not None : msg += \"     file: %s\\n\" % \", \" . join ( f . name for f in viewvalues ( files ) ) self . log message ( msg + \"\\n\" )", "predictions": ["for each self - processing self ."], "references": ["log the beginning of an api request ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 550, "code": "def log end transaction ( self , start time , response ) : if not self . is logging : return elapsed time = int ( ( time . time ( ) - start time ) * 1000 ) msg = \"<<< HTTP %d %s   (%d ms)\\n\" % ( response . status code , response . reason , elapsed time ) if \"Content-Type\" in response . headers : msg += \"    Content-Type: %s\\n\" % response . headers [ \"Content-Type\" ] msg += response . text self . log message ( msg + \"\\n\\n\" )", "predictions": ["mapping self - catching self to the self - native ( ."], "references": ["log response from an api request ."], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 551, "code": "def print ( self , msg , flush = False , end = \"\\n\" ) : if self . verbose : print2 ( msg , end = end , flush = flush )", "predictions": ["prints the specified message and flushes to the output stream ."], "references": ["helper function to print connection status messages when in verbose mode ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 552, "code": "def normalize enum constant ( s ) : if s . islower ( ) : return s if s . isupper ( ) : return s . lower ( ) return \"\" . join ( ch if ch . islower ( ) else \" \" + ch . lower ( ) for ch in s ) . strip ( \" \" )", "predictions": ["normalizes a self - style self - style self - style syntax ."], "references": ["return enum constant s converted to a canonical snake - case ."], "bleu": 0.1135935489027116, "rouge_l": 0.2417437252311757}
{"id": 553, "code": "def default params ( self ) : params = { } for p in self . parms : params [ p ] = self . parms [ p ] [ \"default value\" ] return params", "predictions": ["invoked when a client parameters are set ."], "references": ["dictionary of the default parameters of the model ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 554, "code": "def actual params ( self ) : params to select = { \"model id\" : \"name\" , \"response column\" : \"column name\" , \"training frame\" : \"name\" , \"validation frame\" : \"name\" } params = { } for p in self . parms : if p in params to select . keys ( ) : params [ p ] = self . parms [ p ] [ \"actual value\" ] . get ( params to select [ p ] , None ) else : params [ p ] = self . parms [ p ] [ \"actual value\" ] return params", "predictions": ["contract of ( . this is only called when we have a unit square ."], "references": ["dictionary of actual parameters of the model ."], "bleu": 0.09103526405546068, "rouge_l": 0.18401206636500753}
{"id": 555, "code": "def show ( self ) : if self . future : self . job . poll once ( ) return if self . model json is None : print ( \"No model trained yet\" ) return if self . model id is None : print ( \"This H2O Estimator has been removed.\" ) return model = self . model json [ \"output\" ] print ( \"Model Details\" ) print ( \"=============\" ) print ( self . class . name , \": \" , self . model json [ \"algo full name\" ] ) print ( \"Model Key: \" , self . id ) self . summary ( ) print ( ) tm = model [ \"training metrics\" ] if tm : tm . show ( ) vm = model [ \"validation metrics\" ] if vm : vm . show ( ) xm = model [ \"cross validation metrics\" ] if xm : xm . show ( ) xms = model [ \"cross validation metrics summary\" ] if xms : xms . show ( ) if \"scoring history\" in model and model [ \"scoring history\" ] : model [ \"scoring history\" ] . show ( ) if \"variable importances\" in model and model [ \"variable importances\" ] : model [ \"variable importances\" ] . show ( )", "predictions": ["candidate for handling of the model"], "references": ["print innards of model without regards to type ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 556, "code": "def gbm ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load dataset ( \"prostate\" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) from h2o . estimators import H2O Gradient Boosting Estimator prostate gbm = H2O Gradient Boosting Estimator ( distribution = \"bernoulli\" , ntrees = 10 , max depth = 8 , min rows = 10 , learn rate = 0.2 ) prostate gbm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training frame = train ) go ( ) prostate gbm . show ( ) go ( ) predictions = prostate gbm . predict ( test ) predictions . show ( ) go ( ) from h2o . tree import H2O Tree , H2O Node tree = H2O Tree ( prostate gbm , 0 , \"0\" ) len ( tree ) tree . left children tree . right children tree . root node . show ( ) go ( ) performance = prostate gbm . model performance ( test ) performance . show ( ) run demo ( demo body , interactive , echo , testing )", "predictions": ["creates the player created by ' . ' and ' ."], "references": ["gbm model demo ."], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 557, "code": "def deeplearning ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load dataset ( \"prostate\" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) from h2o . estimators import H2O Deep Learning Estimator prostate dl = H2O Deep Learning Estimator ( activation = \"Tanh\" , hidden = [ 10 , 10 , 10 ] , epochs = 10000 ) prostate dl . train ( x = list ( set ( prostate . col names ) - { \"ID\" , \"CAPSULE\" } ) , y = \"CAPSULE\" , training frame = train ) go ( ) prostate dl . show ( ) go ( ) predictions = prostate dl . predict ( test ) predictions . show ( ) go ( ) performance = prostate dl . model performance ( test ) performance . show ( ) run demo ( demo body , interactive , echo , testing )", "predictions": ["creates new instance of ."], "references": ["deep learning model demo ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 558, "code": "def glm ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load dataset ( \"prostate\" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) from h2o . estimators import H2O Generalized Linear Estimator prostate glm = H2O Generalized Linear Estimator ( family = \"binomial\" , alpha = [ 0.5 ] ) prostate glm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training frame = train ) go ( ) prostate glm . show ( ) go ( ) predictions = prostate glm . predict ( test ) predictions . show ( ) go ( ) performance = prostate glm . model performance ( test ) performance . show ( ) run demo ( demo body , interactive , echo , testing )", "predictions": ["creates new instance of ("], "references": ["glm model demo ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 559, "code": "def as data frame ( self ) : if can use pandas ( ) : import pandas pandas . options . display . max colwidth = 70 return pandas . Data Frame ( self . cell values , columns = self . col header ) return self", "predictions": ["call this method to ] the widget ."], "references": ["convert to a python data frame ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 560, "code": "def show ( self , header = True ) : if header and self . table header : print ( self . table header + \":\" , end = ' ' ) if self . table description : print ( self . table description ) print ( ) table = copy . deepcopy ( self . cell values ) nr = 0 if is list of lists ( table ) : nr = len ( table ) if nr > 20 : trunc table = [ ] trunc table += [ v for v in table [ : 5 ] ] trunc table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) trunc table += [ v for v in table [ ( nr - 5 ) : ] ] table = trunc table H2O Display ( table , self . col header , numalign = \"left\" , stralign = \"left\" ) if nr > 20 and can use pandas ( ) : print ( '\\n See the whole table with table.as data frame()' )", "predictions": ["get a check to show for each column in ( ."], "references": ["print the contents of this table ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 561, "code": "def jar paths ( ) : own jar = os . getenv ( \"H2O JAR PATH\" , \"\" ) if own jar != \"\" : if not os . path . isfile ( own jar ) : raise H2O Startup Error ( \"Environment variable H2O JAR PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file.\" % own jar ) yield own jar cwd chunks = os . path . abspath ( \".\" ) . split ( os . path . sep ) for i in range ( len ( cwd chunks ) , 0 , - 1 ) : if cwd chunks [ i - 1 ] == \"h2o-3\" : yield os . path . sep . join ( cwd chunks [ : i ] + [ \"build\" , \"h2o.jar\" ] ) backend dir = os . path . split ( os . path . realpath ( file ) ) [ 0 ] yield os . path . join ( backend dir , \"bin\" , \"h2o.jar\" ) prefix1 = prefix2 = sys . prefix if prefix1 . startswith ( os . path . sep + \"Library\" ) : prefix2 = os . path . join ( \"\" , \"System\" , prefix1 ) elif prefix1 . startswith ( os . path . sep + \"System\" ) : prefix2 = prefix1 [ len ( os . path . join ( \"\" , \"System\" ) ) : ] yield os . path . join ( prefix1 , \"h2o jar\" , \"h2o.jar\" ) yield os . path . join ( os . path . abspath ( os . sep ) , \"usr\" , \"local\" , \"h2o jar\" , \"h2o.jar\" ) yield os . path . join ( prefix1 , \"local\" , \"h2o jar\" , \"h2o.jar\" ) yield os . path . join ( get config var ( \"userbase\" ) , \"h2o jar\" , \"h2o.jar\" ) yield os . path . join ( prefix2 , \"h2o jar\" , \"h2o.jar\" )", "predictions": ["% width and % status files ."], "references": ["produce potential paths for an h2o . jar executable ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 562, "code": "def csv dict writer ( f , fieldnames , * * kwargs ) : import csv if \"delimiter\" in kwargs : kwargs [ \"delimiter\" ] = str ( kwargs [ \"delimiter\" ] ) return csv . Dict Writer ( f , fieldnames , * * kwargs )", "predictions": ["instantiate data for set of set of bar data structures ."], "references": ["equivalent of csv . dictwriter but allows delimiter to be a unicode string on py2 ."], "bleu": 0.08001467044102561, "rouge_l": 0.14336075205640422}
{"id": 563, "code": "def path2uri ( self , dirpath ) : relpath = dirpath . replace ( self . root path , self . package name ) if relpath . startswith ( os . path . sep ) : relpath = relpath [ 1 : ] return relpath . replace ( os . path . sep , '.' )", "predictions": ["get a render to the imported directory ."], "references": ["convert directory path to uri"], "bleu": 0.17747405280050269, "rouge_l": 0.16052631578947368}
{"id": 564, "code": "def parse module ( self , uri ) : filename = self . uri2path ( uri ) if filename is None : return ( [ ] , [ ] ) f = open ( filename , 'rt' ) functions , classes = self . parse lines ( f ) f . close ( ) return functions , classes", "predictions": ["refresh this ( . . . . . ex . . ex ex , java . tools . file ex . . ex . . ex : ( . . . . . . ex ex ex ex ex ex ex . ex . ex . ex . self ex"], "references": ["parse module defined in * uri *"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 565, "code": "def parse lines ( self , linesource ) : functions = [ ] classes = [ ] for line in linesource : if line . startswith ( 'def ' ) and line . count ( '(' ) : name = self . get object name ( line ) if not name . startswith ( ' ' ) : functions . append ( name ) elif line . startswith ( 'class ' ) : name = self . get object name ( line ) if not name . startswith ( ' ' ) : classes . append ( name ) else : pass functions . sort ( ) classes . sort ( ) return functions , classes", "predictions": ["returns a list of ( that has just been iterated but is a single for each for each for each for each for each for the trailing as a file ."], "references": ["parse lines of text for functions and classes"], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 566, "code": "def to list ( self ) : return [ [ int ( self . table . cell values [ 0 ] [ 1 ] ) , int ( self . table . cell values [ 0 ] [ 2 ] ) ] , [ int ( self . table . cell values [ 1 ] [ 1 ] ) , int ( self . table . cell values [ 1 ] [ 2 ] ) ] ]", "predictions": ["returns a list of tuples from this object ."], "references": ["convert this confusion matrix into a 2x2 plain list of values ."], "bleu": 0.158278836853973, "rouge_l": 0.3713850837138508}
{"id": 567, "code": "def locate files ( root dir ) : all files = [ ] root dir = os . path . abspath ( root dir ) for dir name , subdirs , files in os . walk ( root dir ) : for f in files : if f . endswith ( \".py\" ) : all files . append ( os . path . join ( dir name , f ) ) return all files", "predictions": ["locates the directory where we can find the given filenames ."], "references": ["find all python files in the given directory and all subfolders ."], "bleu": 0.16278331364721524, "rouge_l": 0.34512022630834516}
{"id": 568, "code": "def main ( ) : for filename in locate files ( ROOT DIR ) : print ( \"Processing %s\" % filename ) with open ( filename , \"rt\" ) as f : tokens = list ( tokenize . generate tokens ( f . readline ) ) text1 = tokenize . untokenize ( tokens ) ntokens = normalize tokens ( tokens ) text2 = tokenize . untokenize ( ntokens ) assert text1 == text2", "predictions": ["move to the application ."], "references": ["executed when script is run as - is ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 569, "code": "def generate schema ( class name , schema ) : has map = False for field in schema [ \"fields\" ] : if field [ \"type\" ] . startswith ( \"Map\" ) : has map = True superclass = schema [ \"superclass\" ] if superclass == \"Iced\" : superclass = \"Object\" yield \"/**\" yield \" * This file is auto-generated by h2o-3/h2o-bindings/bin/gen csharp.py\" yield \" * Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details)\" yield \" */\" yield \"namespace ai.h2o\" yield \"{\" yield \"  using System;\" yield \"  using System.Collections.Generic;\" if has map else None yield \"\" yield \"  public class {name}: {super} {{\" . format ( name = class name , super = superclass ) for field in schema [ \"fields\" ] : if field [ \"name\" ] == \" meta\" : continue csharp type = translate type ( field [ \"type\" ] , field [ \"schema name\" ] ) yield \"    /// <summary>\" yield bi . wrap ( field [ \"help\" ] , \"    ///   \" ) yield \"    /// </summary>\" yield \"    public {type} {name} {{ get; set; }}\" . format ( type = csharp type , name = field [ \"name\" ] ) yield \"\" yield \"  }\" yield \"}\"", "predictions": ["generate a ( which should be called after the ( = 1 = 3 = 4 . 4 = 3 = 3 = 8 = 8 = 3 = 3 = 3 = 3 = 8 = 3 = 3 = 8 = 8 = 8 = 8 = 8"], "references": ["generate c# declaration file for a schema ."], "bleu": 0.028577262451992175, "rouge_l": 0.11898569570871263}
{"id": 570, "code": "def available ( ) : builder json = h2o . api ( \"GET /3/Model Builders\" , data = { \"algo\" : \"deepwater\" } ) visibility = builder json [ \"model builders\" ] [ \"deepwater\" ] [ \"visibility\" ] if visibility == \"Experimental\" : print ( \"Cannot build a Deep Water model - no backend found.\" ) return False else : return True", "predictions": ["retrieve the centers of the ."], "references": ["returns true if a deep water model can be built or false otherwise ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 571, "code": "def endpoint groups ( ) : groups = defaultdict ( list ) for e in endpoints ( ) : groups [ e [ \"class name\" ] ] . append ( e ) return groups", "predictions": ["returns all the std of this cluster ."], "references": ["return endpoints grouped by the class which handles them ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 572, "code": "def update site forward ( apps , schema editor ) : Site = apps . get model ( \"sites\" , \"Site\" ) Site . objects . update or create ( id = settings . SITE ID , defaults = { \"domain\" : \"{{cookiecutter.domain name}}\" , \"name\" : \"{{cookiecutter.project name}}\" , } , )", "predictions": ["version of ( . save the settings if necessary ."], "references": ["set site domain and name ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 573, "code": "def json data ( self , data = None ) : if data is None : data = { } data . update ( self . default data ) return json . dumps ( data )", "predictions": ["get load or split dataset as a load balancer output ."], "references": ["adds the default_data to data and dumps it to a json ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 574, "code": "def comment user ( self , user id , amount = None ) : if not self . check user ( user id , filter closed acc = True ) : return False self . logger . info ( \"Going to comment user %s's feed:\" % user id ) user id = self . convert to user id ( user id ) medias = self . get user medias ( user id , is comment = True ) if not medias : self . logger . info ( \"None medias received: account is closed or medias have been filtered.\" ) return False return self . comment medias ( medias [ : amount ] )", "predictions": ["creates a check to see if the frame is authentication or closed ."], "references": ["comments last user_id s medias"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 575, "code": "def get credentials ( username = None ) : while not check secret ( ) : pass while True : try : with open ( SECRET FILE , \"r\" ) as f : lines = [ line . strip ( ) . split ( \":\" , 2 ) for line in f . readlines ( ) ] except Value Error : msg = 'Problem with opening `{}`, will remove the file.' raise Exception ( msg . format ( SECRET FILE ) ) if username is not None : for login , password in lines : if login == username . strip ( ) : return login , password print ( \"Which account do you want to use? (Type number)\" ) for ind , ( login , password ) in enumerate ( lines ) : print ( \"%d: %s\" % ( ind + 1 , login ) ) print ( \"%d: %s\" % ( 0 , \"add another account.\" ) ) print ( \"%d: %s\" % ( - 1 , \"delete all accounts.\" ) ) try : ind = int ( sys . stdin . readline ( ) ) if ind == 0 : add credentials ( ) continue elif ind == - 1 : delete credentials ( ) check secret ( ) continue elif 0 <= ind - 1 < len ( lines ) : return lines [ ind - 1 ] except Exception : print ( \"Wrong input, enter the number of the account to use.\" )", "predictions": ["removes is the is login"], "references": ["returns login and password stored in secret . txt ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 576, "code": "def like user ( self , user id , amount = None , filtration = True ) : if filtration : if not self . check user ( user id ) : return False self . logger . info ( \"Liking user %s's feed:\" % user id ) user id = self . convert to user id ( user id ) medias = self . get user medias ( user id , filtration = filtration ) if not medias : self . logger . info ( \"None medias received: account is closed or medias have been filtered.\" ) return False return self . like medias ( medias [ : amount ] )", "predictions": ["creates a user with the specified user ."], "references": ["likes last user_id s medias"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 577, "code": "def like hashtag ( self , hashtag , amount = None ) : self . logger . info ( \"Going to like media with hashtag #%s.\" % hashtag ) medias = self . get total hashtag medias ( hashtag , amount ) return self . like medias ( medias )", "predictions": ["get a hashtag object ."], "references": ["likes last medias from hashtag"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 578, "code": "def check not bot ( self , user id ) : self . small delay ( ) user id = self . convert to user id ( user id ) if not user id : return False if user id in self . whitelist : return True if user id in self . blacklist : return False user info = self . get user info ( user id ) if not user info : return True skipped = self . skipped file if \"following count\" in user info and user info [ \"following count\" ] > self . max following to block : msg = 'following count > bot.max following to block, skipping!' self . console print ( msg , 'red' ) skipped . append ( user id ) return False if search stop words in user ( self , user info ) : msg = '`bot.search stop words in user` found in user, skipping!' skipped . append ( user id ) return False return True", "predictions": ["generate the unique user for this user ."], "references": ["filter bot from real users ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 579, "code": "def get uri ( self , request ) : protocol = request . protocol override if request . protocol override else self . protocol protocol = protocol . lower ( ) port = HTTP PORT if protocol == 'http' else HTTPS PORT return protocol + '://' + request . host + ':' + str ( port ) + request . path", "predictions": ["get a protocol for the session ."], "references": ["return the target uri for the request ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 580, "code": "def get connection ( self , request ) : protocol = request . protocol override if request . protocol override else self . protocol protocol = protocol . lower ( ) target host = request . host connection = Requests Connection ( target host , protocol , self . request session , self . timeout ) proxy host = self . proxy host proxy port = self . proxy port if self . proxy host : headers = None if self . proxy user and self . proxy password : auth = base64 . b64encode ( \"{0}:{1}\" . format ( self . proxy user , self . proxy password ) . encode ( ) ) headers = { 'Proxy-Authorization' : 'Basic {0}' . format ( auth . decode ( ) ) } connection . set tunnel ( proxy host , int ( proxy port ) , headers ) return connection", "predictions": ["an http connection to the session ."], "references": ["create connection for the request ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 581, "code": "def perform request ( self , request ) : connection = self . get connection ( request ) try : connection . putrequest ( request . method , request . path ) self . send request headers ( connection , request . headers ) self . send request body ( connection , request . body ) if DEBUG REQUESTS and request . body : print ( 'request:' ) try : print ( request . body ) except : pass resp = connection . getresponse ( ) status = int ( resp . status ) message = resp . reason respheaders = resp . getheaders ( ) for i , value in enumerate ( respheaders ) : respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) respbody = None if resp . length is None : respbody = resp . read ( ) elif resp . length > 0 : respbody = resp . read ( resp . length ) if DEBUG RESPONSES and respbody : print ( 'response:' ) try : print ( respbody ) except : pass response = HTTP Response ( status , resp . reason , respheaders , respbody ) if status == 307 : new url = urlparse ( dict ( respheaders ) [ 'location' ] ) request . host = new url . hostname request . path = new url . path request . path , request . query = self . update request uri query ( request ) return self . perform request ( request ) if status >= 300 : raise HTTP Error ( status , message , respheaders , respbody ) return response finally : connection . close ( )", "predictions": ["executes an http request and closes the resulting connection ."], "references": ["sends request to cloud service server and return the response ."], "bleu": 0.1434272783816789, "rouge_l": 0.37770897832817335}
{"id": 582, "code": "def get authorization server ( self ) : value = '' for key in [ 'authorization uri' , 'authorization' ] : value = self . get value ( key ) or '' if value : break return value", "predictions": ["this method returns the value of the authorization server ."], "references": ["returns the uri for the authorization server if present otherwise empty string ."], "bleu": 0.2148213130469897, "rouge_l": 0.5097493036211699}
{"id": 583, "code": "def validate request uri ( self , uri ) : if not uri : raise Value Error ( 'request uri cannot be empty' ) uri = parse . urlparse ( uri ) if not uri . netloc : raise Value Error ( 'request uri must be an absolute URI' ) if uri . scheme . lower ( ) not in [ 'http' , 'https' ] : raise Value Error ( 'request uri must be HTTP or HTTPS' ) return uri . netloc", "predictions": ["validate that this request is encoded . if this cannot be called if a uri cannot be generated , will validate that the given uri is already encoded ."], "references": ["extracts the host authority from the given uri ."], "bleu": 0.08218074077265651, "rouge_l": 0.23260247855100094}
{"id": 584, "code": "def set timeout ( self , timeout in seconds ) : timeout in ms = int ( timeout in seconds * 1000 ) Win Http Request . Set Timeouts ( self , 0 , timeout in ms , timeout in ms , timeout in ms )", "predictions": ["set the timeout for the timeout ."], "references": ["sets up the timeout for the request ."], "bleu": 0.46905226098954195, "rouge_l": 0.6587473002159828}
{"id": 585, "code": "def set request header ( self , name , value ) : name = BSTR ( name ) value = BSTR ( value ) Win Http Request . Set Request Header ( self , name , value )", "predictions": ["set a header on a request ."], "references": ["sets the request header ."], "bleu": 0.22089591134157885, "rouge_l": 0.34366197183098596}
{"id": 586, "code": "def get all response headers ( self ) : bstr headers = c void p ( ) Win Http Request . Get All Response Headers ( self , byref ( bstr headers ) ) bstr headers = ctypes . cast ( bstr headers , c wchar p ) headers = bstr headers . value Sys Free String ( bstr headers ) return headers", "predictions": ["set all headers of the request ."], "references": ["gets back all response headers ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 587, "code": "def send ( self , request = None ) : if request is None : var empty = VARIANT . create empty ( ) Win Http Request . Send ( self , var empty ) else : request = VARIANT . create safearray from str ( request ) Win Http Request . Send ( self , request )", "predictions": ["send an empty request to the server ."], "references": ["sends the request body ."], "bleu": 0.19070828081828378, "rouge_l": 0.32105263157894737}
{"id": 588, "code": "def status ( self ) : status = c long ( ) Win Http Request . Status ( self , byref ( status ) ) return int ( status . value )", "predictions": ["set status of the connection object as status ."], "references": ["gets status of response ."], "bleu": 0.19960198807747329, "rouge_l": 0.4518518518518518}
{"id": 589, "code": "def status text ( self ) : bstr status text = c void p ( ) Win Http Request . Status Text ( self , byref ( bstr status text ) ) bstr status text = ctypes . cast ( bstr status text , c wchar p ) status text = bstr status text . value Sys Free String ( bstr status text ) return status text", "predictions": ["create a status object for a text ."], "references": ["gets status text of response ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 590, "code": "def response body ( self ) : var respbody = VARIANT ( ) Win Http Request . Response Body ( self , byref ( var respbody ) ) if var respbody . is safearray of bytes ( ) : respbody = var respbody . str from safearray ( ) return respbody else : return ''", "predictions": ["set response body on the given body and returns it ."], "references": ["gets response body as a safearray and converts the safearray to str ."], "bleu": 0.1486375602900724, "rouge_l": 0.3283983849259758}
{"id": 591, "code": "def set client certificate ( self , certificate ) : certificate = BSTR ( certificate ) Win Http Request . Set Client Certificate ( self , certificate )", "predictions": ["sets certificate certificate to zero ."], "references": ["sets client certificate for the request ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 592, "code": "def set tunnel ( self , host , port ) : url = host if port : url = url + u':' + port var host = VARIANT . create bstr from str ( url ) var empty = VARIANT . create empty ( ) Win Http Request . Set Proxy ( self , HTTPREQUEST PROXYSETTING PROXY , var host , var empty )", "predictions": ["create a new ( url ) ."], "references": ["sets up the host and the port for the http connect tunnelling ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 593, "code": "def set tunnel ( self , host , port = None , headers = None ) : self . httprequest . set tunnel ( unicode ( host ) , unicode ( str ( port ) ) )", "predictions": ["sets the request on the specified host and port ."], "references": ["sets up the host and the port for the http connect tunnelling ."], "bleu": 0.15705810592958255, "rouge_l": 0.5097493036211699}
{"id": 594, "code": "def putrequest ( self , method , uri ) : protocol = unicode ( self . protocol + '://' ) url = protocol + self . host + unicode ( uri ) self . httprequest . set timeout ( self . timeout ) self . httprequest . open ( unicode ( method ) , url ) if self . cert file is not None : self . httprequest . set client certificate ( unicode ( self . cert file ) )", "predictions": ["creates a new instance of this class ."], "references": ["connects to host and sends the request ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 595, "code": "def putheader ( self , name , value ) : if sys . version info < ( 3 , ) : name = str ( name ) . decode ( 'utf-8' ) value = str ( value ) . decode ( 'utf-8' ) self . httprequest . set request header ( name , value )", "predictions": ["setup a putheader unicode string ."], "references": ["sends the headers of request ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 596, "code": "def send ( self , request body ) : if not request body : self . httprequest . send ( ) else : self . httprequest . send ( request body )", "predictions": ["send an request to the client ."], "references": ["sends request body ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 597, "code": "def getresponse ( self ) : status = self . httprequest . status ( ) status text = self . httprequest . status text ( ) resp headers = self . httprequest . get all response headers ( ) fixed headers = [ ] for resp header in resp headers . split ( '\\n' ) : if ( resp header . startswith ( '\\t' ) or resp header . startswith ( ' ' ) ) and fixed headers : fixed headers [ - 1 ] += resp header else : fixed headers . append ( resp header ) headers = [ ] for resp header in fixed headers : if ':' in resp header : pos = resp header . find ( ':' ) headers . append ( ( resp header [ : pos ] . lower ( ) , resp header [ pos + 1 : ] . strip ( ) ) ) body = self . httprequest . response body ( ) length = len ( body ) return Response ( status , status text , length , headers , body )", "predictions": ["makes a result of getresponse based on http post ."], "references": ["gets the response and generates the _response object"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 598, "code": "def get readable id ( id name , id prefix to skip ) : pos = id name . find ( '//' ) if pos != - 1 : pos += 2 if id prefix to skip : pos = id name . find ( id prefix to skip , pos ) if pos != - 1 : pos += len ( id prefix to skip ) pos = id name . find ( '/' , pos ) if pos != - 1 : return id name [ pos + 1 : ] return id name", "predictions": ["utility to fix an id by inserting its prefix ."], "references": ["simplified an id to be more friendly for us people"], "bleu": 0.17827531042796255, "rouge_l": 0.2}
{"id": 599, "code": "def get serialization name ( element name ) : known = KNOWN SERIALIZATION XFORMS . get ( element name ) if known is not None : return known if element name . startswith ( 'x ms ' ) : return element name . replace ( ' ' , '-' ) if element name . endswith ( ' id' ) : element name = element name . replace ( ' id' , 'ID' ) for name in [ 'content ' , 'last modified' , 'if ' , 'cache control' ] : if element name . startswith ( name ) : element name = element name . replace ( ' ' , '- ' ) return '' . join ( name . capitalize ( ) for name in element name . split ( ' ' ) )", "predictions": ["converts a name into a variable ."], "references": ["converts a python name into a serializable name"], "bleu": 0.3564026463354183, "rouge_l": 0.6587473002159828}
{"id": 600, "code": "def get entry properties from node ( entry , include id , id prefix to skip = None , use title as id = False ) : properties = { } etag = entry . get Attribute NS ( METADATA NS , 'etag' ) if etag : properties [ 'etag' ] = etag for updated in Minidom Xml To Object . get child nodes ( entry , 'updated' ) : properties [ 'updated' ] = updated . first Child . node Value for name in Minidom Xml To Object . get children from path ( entry , 'author' , 'name' ) : if name . first Child is not None : properties [ 'author' ] = name . first Child . node Value if include id : if use title as id : for title in Minidom Xml To Object . get child nodes ( entry , 'title' ) : properties [ 'name' ] = title . first Child . node Value else : for id in Minidom Xml To Object . get child nodes ( entry , 'id' ) : properties [ 'name' ] = get readable id ( id . first Child . node Value , id prefix to skip ) return properties", "predictions": ["read all the nodes from a node and return them as a dict ."], "references": ["get properties from entry xml"], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 601, "code": "def parse response body from xml node ( node , return type ) : return obj = return type ( ) Minidom Xml To Object . fill data to return object ( node , return obj ) return return obj", "predictions": ["this is used to convert the xml to an xml element . this can only be overridden by other applications ."], "references": ["parse the xml and fill all the data into a class of return_type"], "bleu": 0.07645949399477267, "rouge_l": 0.1228600201409869}
{"id": 602, "code": "def fill instance child ( xmldoc , element name , return type ) : xmlelements = Minidom Xml To Object . get child nodes ( xmldoc , get serialization name ( element name ) ) if not xmlelements : return None return obj = return type ( ) Minidom Xml To Object . fill data to return object ( xmlelements [ 0 ] , return obj ) return return obj", "predictions": ["writes the fields from the given element ."], "references": ["converts a child of the current dom element to the specified type ."], "bleu": 0.10793517579160734, "rouge_l": 0.2739520958083832}
{"id": 603, "code": "def build package from pr number ( gh token , sdk id , pr number , output folder , * , with comment = False ) : con = Github ( gh token ) repo = con . get repo ( sdk id ) sdk pr = repo . get pull ( pr number ) package names = { f . filename . split ( '/' ) [ 0 ] for f in sdk pr . get files ( ) if f . filename . startswith ( \"azure\" ) } absolute output folder = Path ( output folder ) . resolve ( ) with tempfile . Temporary Directory ( ) as temp dir , manage git folder ( gh token , Path ( temp dir ) / Path ( \"sdk\" ) , sdk id , pr number = pr number ) as sdk folder : for package name in package names : LOGGER . debug ( \"Build {}\" . format ( package name ) ) execute simple command ( [ \"python\" , \"./build package.py\" , \"--dest\" , str ( absolute output folder ) , package name ] , cwd = sdk folder ) LOGGER . debug ( \"Build finished: {}\" . format ( package name ) ) if with comment : files = [ f . name for f in absolute output folder . iterdir ( ) ] comment message = None dashboard = Dashboard Commentable Object ( sdk pr , \"(message created by the CI based on PR content)\" ) try : installation message = build installation message ( sdk pr ) download message = build download message ( sdk pr , files ) comment message = installation message + \"\\n\\n\" + download message dashboard . create comment ( comment message ) except Exception : LOGGER . critical ( \"Unable to do PR comment:\\n%s\" , comment message )", "predictions": ["build a package of all of the specified package ."], "references": ["will clone the given pr branch and vuild the package with the given name ."], "bleu": 0.0909256598621168, "rouge_l": 0.23164556962025318}
{"id": 604, "code": "def extract api version from code ( function ) : try : srccode = inspect . getsource ( function ) try : ast tree = ast . parse ( srccode ) except Indentation Error : ast tree = ast . parse ( 'with 0:\\n' + srccode ) api version visitor = Api Version Extractor ( ) api version visitor . visit ( ast tree ) return api version visitor . api version except Exception : raise", "predictions": ["extract all the api version information from the given function ."], "references": ["will extract from __code__ the api version . should be use if you use this is an operation group with no constant api_version ."], "bleu": 0.07479130265136551, "rouge_l": 0.2677787532923617}
{"id": 605, "code": "def build receiver ( self ) : self . handler . message handler = self . handler . receiver type ( self . handler . session , self . handler . remote address , self . handler . name , on message received = self . handler . message received , name = 'receiver-link-{}' . format ( uuid . uuid4 ( ) ) , debug = self . handler . debug trace , prefetch = self . handler . prefetch , max message size = self . handler . max message size , properties = self . handler . link properties , error policy = self . handler . error policy , encoding = self . handler . encoding ) if self . mode != Receive Settle Mode . Peek Lock : self . handler . message handler . send settle mode = constants . Sender Settle Mode . Settled self . handler . message handler . receive settle mode = constants . Receiver Settle Mode . Receive And Delete self . handler . message handler . settle mode = constants . Receiver Settle Mode . Receive And Delete self . handler . message handler . open ( )", "predictions": ["creates a receiver from the receiver ."], "references": ["this is a temporary patch pending a fix in uamqp ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 606, "code": "def parse response for async op ( response ) : if response is None : return None result = Asynchronous Operation Result ( ) if response . headers : for name , value in response . headers : if name . lower ( ) == 'x-ms-request-id' : result . request id = value return result", "predictions": ["convert the response into a python object ."], "references": ["extracts request id from response header ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 607, "code": "def update management header ( self , request , x ms version ) : if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) request . headers . append ( ( 'x-ms-version' , x ms version or self . x ms version ) ) if not request . method in [ 'GET' , 'HEAD' ] : for name , in request . headers : if 'content-type' == name . lower ( ) : break else : request . headers . append ( ( 'Content-Type' , self . content type ) ) return request . headers", "predictions": ["update request ."], "references": ["add additional headers for management ."], "bleu": 0.19765609300943976, "rouge_l": 0.20962199312714777}
{"id": 608, "code": "def get regions ( self ) : response = self . perform get ( self . get path ( 'services/service Bus/Regions/' , None ) , None ) return Minidom Xml To Object . convert response to feeds ( response , Service Bus Management Xml Serializer . xml to region )", "predictions": ["like check . commons . inverts ( , java . awt . object , java . security . list , java . tools . list . object [ ] , int , int , int , int , int , int , int , int , int , self ,"], "references": ["get list of available service bus regions ."], "bleu": 0.026594139297659906, "rouge_l": 0.07932379713914176}
{"id": 609, "code": "def list namespaces ( self ) : response = self . perform get ( self . get path ( 'services/service Bus/Namespaces/' , None ) , None ) return Minidom Xml To Object . convert response to feeds ( response , Service Bus Management Xml Serializer . xml to namespace )", "predictions": ["returns a like object that contains the specified like xml ."], "references": ["list the service bus namespaces defined on the account ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 610, "code": "def create ( env dir , system site packages = False , clear = False , symlinks = False , with pip = False , prompt = None ) : builder = Extended Env Builder ( system site packages = system site packages , clear = clear , symlinks = symlinks , with pip = with pip , prompt = prompt ) builder . create ( env dir ) return builder . context", "predictions": ["creates the directory in its stored id ."], "references": ["create a virtual environment in a directory ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 611, "code": "def list databases ( self , name ) : response = self . perform get ( self . get list databases path ( name ) , None ) return Minidom Xml To Object . parse service resources response ( response , Database )", "predictions": ["show the list of uri in the database ."], "references": ["list the sql databases defined on the specified server name"], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 612, "code": "def validate challenge ( self , challenge ) : bearer string = 'Bearer ' if not challenge : raise Value Error ( 'Challenge cannot be empty' ) challenge = challenge . strip ( ) if not challenge . startswith ( bearer string ) : raise Value Error ( 'Challenge is not Bearer' ) return challenge [ len ( bearer string ) : ]", "predictions": ["validates authentication connection for the given connection ."], "references": ["verifies that the challenge is a bearer challenge and returns the key = value pairs ."], "bleu": 0.06528905536667998, "rouge_l": 0.15721649484536082}
{"id": 613, "code": "def list queues ( self ) : request = HTTP Request ( ) request . method = 'GET' request . host = self . get host ( ) request . path = '/$Resources/Queues' request . path , request . query = self . httpclient . update request uri query ( request ) request . headers = self . update service bus header ( request ) response = self . perform request ( request ) return E Tree Xml To Object . convert response to feeds ( response , convert etree element to queue )", "predictions": ["get a perform : perform a perform : : : : : : : : / / www . com / questions / questions / . - . - . - . - . - . - ."], "references": ["enumerates the queues in the service namespace ."], "bleu": 0.03172629746109536, "rouge_l": 0.049273021001615507}
{"id": 614, "code": "def send event ( self , hub name , message , device id = None , broker properties = None ) : validate not none ( 'hub name' , hub name ) request = HTTP Request ( ) request . method = 'POST' request . host = self . get host ( ) if device id : request . path = '/{0}/publishers/{1}/messages?api-version=2014-01' . format ( hub name , device id ) else : request . path = '/{0}/messages?api-version=2014-01' . format ( hub name ) if broker properties : request . headers . append ( ( 'Broker Properties' , str ( broker properties ) ) ) request . body = get request body ( message ) request . path , request . query = self . httpclient . update request uri query ( request ) request . headers = self . update service bus header ( request ) self . perform request ( request )", "predictions": ["get authorization if found ."], "references": ["sends a new message event to an event hub ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 615, "code": "def update service bus header ( self , request ) : if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) if not request . method in [ 'GET' , 'HEAD' ] : for name , in request . headers : if name . lower ( ) == 'content-type' : break else : request . headers . append ( ( 'Content-Type' , 'application/atom+xml;type=entry;charset=utf-8' ) ) self . authentication . sign request ( request , self . httpclient ) return request . headers", "predictions": ["validate an existing request ."], "references": ["add additional headers for service bus ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 616, "code": "def get authorization ( self , request , httpclient ) : return 'WRAP access token=\"' + self . get token ( request . host , request . path , httpclient ) + '\"'", "predictions": ["get the timeout for the in - memory language ."], "references": ["return the signed string with token ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 617, "code": "def token is expired ( self , token ) : time pos begin = token . find ( 'Expires On=' ) + len ( 'Expires On=' ) time pos end = token . find ( '&' , time pos begin ) token expire time = int ( token [ time pos begin : time pos end ] ) time now = time . mktime ( time . localtime ( ) ) return ( token expire time - time now ) < 30", "predictions": ["returns true if the set of ) is header and at least one of the set ."], "references": ["check if token expires or not ."], "bleu": 0.07994607499472013, "rouge_l": 0.18020679468242246}
{"id": 618, "code": "def add headers ( self , request ) : if self . custom properties : for name , value in self . custom properties . items ( ) : request . headers . append ( ( name , self . serialize escaped properties value ( value ) ) ) request . headers . append ( ( 'Content-Type' , self . type ) ) if self . broker properties : if hasattr ( self . broker properties , 'items' ) : broker properties = { name : self . serialize basic properties value ( value ) for name , value in self . broker properties . items ( ) } broker properties = json . dumps ( broker properties ) else : broker properties = self . broker properties request . headers . append ( ( 'Broker Properties' , str ( broker properties ) ) ) return request . headers", "predictions": ["call this method to get the parameter . we need to customize the session on the parameter ."], "references": ["add addtional headers to request for message request ."], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 619, "code": "def as batch body ( self ) : if sys . version info >= ( 3 , ) and isinstance ( self . body , bytes ) : body = self . body . decode ( 'utf-8' ) else : body = self . body result = { 'Body' : body } if self . custom properties : result [ 'User Properties' ] = { name : self . serialize basic properties value ( value ) for name , value in self . custom properties . items ( ) } if self . broker properties : result [ 'Broker Properties' ] = { name : self . serialize basic properties value ( value ) for name , value in self . broker properties . items ( ) } return result", "predictions": ["for each ( currently only a ( i . e . , the second clip = 0 , 2 = 0 , 2 = 0 , 2 , 2 , 2 = 8 , 2 = 8 , 2 = 8 , 2 = 8 , 2 = 8 ,"], "references": ["return the current message as expected by batch body format"], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 620, "code": "def general error handler ( http error ) : message = str ( http error ) if http error . respbody is not None : message += '\\n' + http error . respbody . decode ( 'utf-8-sig' ) raise Azure Http Error ( message , http error . status )", "predictions": ["raise : the ( if ( = true = false = true = false = false = true = false = true = false = true = false = 0 . . . . = true = if the long = true = 0 . 1"], "references": ["simple error handler for azure ."], "bleu": 0.02614431568998955, "rouge_l": 0.044655929721815514}
{"id": 621, "code": "def handle redirect ( self , r , * * kwargs ) : if r . is redirect : self . thread local . auth attempted = False", "predictions": ["fallback for offline methods ."], "references": ["reset auth_attempted on redirects ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 622, "code": "def use ( self , profile ) : if not isinstance ( profile , ( Known Profiles , Profile Definition ) ) : raise Value Error ( \"Can only set as default a Profile Definition or a Known Profiles\" ) type ( self ) . profile = profile", "predictions": ["create a new profile instance ."], "references": ["define a new default profile ."], "bleu": 0.3303164318013807, "rouge_l": 0.6666666666666666}
{"id": 623, "code": "def build config ( config : Dict [ str , Any ] ) -> Dict [ str , str ] : result = config . copy ( ) is stable = result . pop ( \"is stable\" , False ) if is stable : result [ \"classifier\" ] = \"Development Status :: 5 - Production/Stable\" else : result [ \"classifier\" ] = \"Development Status :: 4 - Beta\" package name = result [ \"package name\" ] result [ \"package nspkg\" ] = result . pop ( \"package nspkg\" , package name [ : package name . rindex ( '-' ) ] + \"-nspkg\" ) result [ 'is arm' ] = result . pop ( \"is arm\" , True ) result [ 'need msrestazure' ] = result . pop ( \"need msrestazure\" , True ) package parts = result [ \"package nspkg\" ] [ : - len ( '-nspkg' ) ] . split ( '-' ) result [ 'nspkg names' ] = [ \".\" . join ( package parts [ : i + 1 ] ) for i in range ( len ( package parts ) ) ] result [ 'init names' ] = [ \"/\" . join ( package parts [ : i + 1 ] ) + \"/ init .py\" for i in range ( len ( package parts ) ) ] return result", "predictions": ["set up a configuration constructed from the network api ."], "references": ["will build the actual config for jinja2 based on sdk config ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 624, "code": "def get entry properties from element ( element , include id , id prefix to skip = None , use title as id = False ) : properties = { } etag = element . attrib . get ( make etree ns attr name ( etree entity feed namespaces [ 'm' ] , 'etag' ) , None ) if etag is not None : properties [ 'etag' ] = etag updated = element . findtext ( './atom:updated' , '' , etree entity feed namespaces ) if updated : properties [ 'updated' ] = updated author name = element . findtext ( './atom:author/atom:name' , '' , etree entity feed namespaces ) if author name : properties [ 'author' ] = author name if include id : if use title as id : title = element . findtext ( './atom:title' , '' , etree entity feed namespaces ) if title : properties [ 'name' ] = title else : element id = element . findtext ( './atom:id' , '' , etree entity feed namespaces ) if element id : properties [ 'name' ] = get readable id ( element id , id prefix to skip ) return properties", "predictions": ["returns a feed object based on the passed , , and attributes ."], "references": ["get properties from element tree element"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 625, "code": "def parse response body from xml node ( node , return type ) : return obj = return type ( ) E Tree Xml To Object . fill data to return object ( node , return obj ) return return obj", "predictions": ["set the tunnel ( either an http tunnel ) or a tunnel ."], "references": ["parse the xml and fill all the data into a class of return_type"], "bleu": 0.10571070857151538, "rouge_l": 0.15384615384615383}
{"id": 626, "code": "def fill instance child ( xmldoc , element name , return type ) : element = xmldoc . find ( get serialization name ( element name ) ) if element is None : return None return obj = return type ( ) E Tree Xml To Object . fill data to return object ( element , return obj ) return return obj", "predictions": ["writes the fields from the given ) to the given ) ."], "references": ["converts a child of the current dom element to the specified type ."], "bleu": 0.14294845713017917, "rouge_l": 0.31770833333333337}
{"id": 627, "code": "def terminal width ( value ) : if isinstance ( value , bytes ) : value = value . decode ( \"utf8\" , \"ignore\" ) return sum ( map ( get width , map ( ord , value ) ) )", "predictions": ["maps a terminal or ( coordinates to a decode ( sum of bytes name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["returns the width of the string it would be when displayed ."], "bleu": 0.02403051755364481, "rouge_l": 0.03626634958382877}
{"id": 628, "code": "def get cut prefix ( value , max len ) : should convert = isinstance ( value , bytes ) if should convert : value = value . decode ( \"utf8\" , \"ignore\" ) for i in range ( len ( value ) ) : if terminal width ( value [ i : ] ) <= max len : break return value [ i : ] . encode ( \"utf8\" , \"ignore\" ) if should convert else value [ i : ]", "predictions": ["send bytes from queue , possibly null if it ' s empty ."], "references": ["drops characters by unicode not by bytes ."], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 629, "code": "def print inplace ( msg ) : term width = get terminal size ( ) . columns spacing = term width - terminal width ( msg ) if is win32 : spacing -= 1 sys . stderr . write ( \"\\r{0}\" . format ( msg ) ) sys . stderr . write ( \" \" * max ( 0 , spacing ) ) sys . stderr . flush ( )", "predictions": ["clears an error message ."], "references": ["clears out the previous line and prints a new one ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 630, "code": "def format filesize ( size ) : for suffix in ( \"bytes\" , \"KB\" , \"MB\" , \"GB\" , \"TB\" ) : if size < 1024.0 : if suffix in ( \"GB\" , \"TB\" ) : return \"{0:3.2f} {1}\" . format ( size , suffix ) else : return \"{0:3.1f} {1}\" . format ( size , suffix ) size /= 1024.0", "predictions": ["formats the entity ' s ( into a human - readable list ."], "references": ["formats the file size into a human readable format ."], "bleu": 0.22718709780542312, "rouge_l": 0.6233576642335765}
{"id": 631, "code": "def format time ( elapsed ) : hours = int ( elapsed / ( 60 * 60 ) ) minutes = int ( ( elapsed % ( 60 * 60 ) ) / 60 ) seconds = int ( elapsed % 60 ) rval = \"\" if hours : rval += \"{0}h\" . format ( hours ) if elapsed > 60 : rval += \"{0}m\" . format ( minutes ) rval += \"{0}s\" . format ( seconds ) return rval", "predictions": ["returns the serialization difference between two dates ."], "references": ["formats elapsed seconds into a human readable format ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 632, "code": "def create status line ( * * params ) : max size = get terminal size ( ) . columns - 1 for fmt in PROGRESS FORMATS : status = fmt . format ( * * params ) if len ( status ) <= max size : break return status", "predictions": ["get a entry with the optional entry ."], "references": ["creates a status line with appropriate size ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 633, "code": "def close ( self ) : if not self . closed : log . debug ( \"Closing worker thread\" ) self . closed = True if self . wait : self . wait . set ( )", "predictions": ["parse and parse the obj ."], "references": ["shuts down the thread ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 634, "code": "def close ( self ) : if not self . closed : log . debug ( \"Closing writer thread\" ) self . closed = True self . reader . buffer . close ( ) self . executor . shutdown ( wait = False ) if concurrent . futures . thread . threads queues : concurrent . futures . thread . threads queues . clear ( )", "predictions": ["closes the current process and releases all associated resources ."], "references": ["shuts down the thread ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 635, "code": "def put ( self , segment ) : if self . closed : return if segment is not None : future = self . executor . submit ( self . fetch , segment , retries = self . retries ) else : future = None self . queue ( self . futures , ( segment , future ) )", "predictions": ["support the specific number of bytes for this object ."], "references": ["adds a segment to the download pool and write queue ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 636, "code": "def queue ( self , queue , value ) : while not self . closed : try : queue . put ( value , block = True , timeout = 1 ) return except queue . Full : continue", "predictions": ["add or remove an item in this extract extract extract code ."], "references": ["puts a value into a queue but aborts if this thread is closed ."], "bleu": 0.09733489823443878, "rouge_l": 0.1517412935323383}
{"id": 637, "code": "def pkcs7 decode ( padded Data , key Size = 16 ) : val = ord ( padded Data [ - 1 : ] ) if val > key Size : raise Stream Error ( \"Input is not padded or padding is corrupt, got padding size of {0}\" . format ( val ) ) return padded Data [ : - val ]", "predictions": ["decodes data from base64 notation to see if data is already verified ."], "references": ["remove the pkcs#7 padding"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 638, "code": "def prepend www ( url ) : parsed = urlparse ( url ) if parsed . netloc . split ( \".\" ) [ 0 ] != \"www\" : return parsed . scheme + \"://www.\" + parsed . netloc + parsed . path else : return url", "predictions": ["prepends the async async async async async async async async ."], "references": ["changes google . com to www . google . com"], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 639, "code": "def json ( cls , res , * args , * * kwargs ) : if res . encoding is None : res . encoding = cls . determine json encoding ( res . content [ : 4 ] ) return parse json ( res . text , * args , * * kwargs )", "predictions": ["[ jackson - update update operation ."], "references": ["parses json from a response ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 640, "code": "def xml ( cls , res , * args , * * kwargs ) : return parse xml ( res . text , * args , * * kwargs )", "predictions": ["parse the given xml tree ."], "references": ["parses xml from a response ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 641, "code": "def get streams ( self ) : token = self . login ( self . get option ( \"username\" ) , self . get option ( \"password\" ) ) m = self . url re . match ( self . url ) scode = m and m . group ( \"scode\" ) or self . get option ( \"station code\" ) res = self . session . http . get ( self . guide url , params = dict ( token = token ) ) channels = Ordered Dict ( ) for t in itertags ( res . text , \"a\" ) : if t . attributes . get ( 'cs' ) : channels [ t . attributes . get ( 'cs' ) . lower ( ) ] = t . attributes . get ( 'title' ) . replace ( \"Watch \" , \"\" ) . strip ( ) if not scode : log . error ( \"Station code not provided, use --ustvnow-station-code.\" ) log . info ( \"Available stations are: \\n{0} \" . format ( '\\n' . join ( '    {0} ({1})' . format ( c , n ) for c , n in channels . items ( ) ) ) ) return if scode in channels : log . debug ( \"Finding streams for: {0}\" , channels . get ( scode ) ) r = self . session . http . get ( self . stream url , params = { \"scode\" : scode , \"token\" : token , \"br n\" : \"Firefox\" , \"br v\" : \"52\" , \"br d\" : \"desktop\" } , headers = { \"User-Agent\" : useragents . FIREFOX } ) data = self . session . http . json ( r ) return HLS Stream . parse variant playlist ( self . session , data [ \"stream\" ] ) else : log . error ( \"Invalid station-code: {0}\" , scode )", "predictions": ["this method is called by the ( when it is made from the client ."], "references": ["finds the streams from tvcatchup . com ."], "bleu": 0.09782375748961449, "rouge_l": 0.2760180995475113}
{"id": 642, "code": "def login ( self ) : email = self . get option ( \"email\" ) password = self . get option ( \"password\" ) if email and password : res = self . session . http . get ( self . login url ) csrf match = self . csrf re . search ( res . text ) token = csrf match and csrf match . group ( 1 ) self . logger . debug ( \"Attempting login as {0} (token={1})\" , email , token ) res = self . session . http . post ( self . login url , data = dict ( login = email , password = password , csrfmiddlewaretoken = token ) , allow redirects = False , raise for status = False , headers = { \"Referer\" : self . login url } ) if res . status code != 302 : self . logger . error ( \"Failed to login to Live Edu account: {0}\" , email )", "predictions": ["login to the user ."], "references": ["attempt a login to liveedu . tv"], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 643, "code": "def output stream http ( plugin , initial streams , external = False , port = 0 ) : global output if not external : if not args . player : console . exit ( \"The default player (VLC) does not seem to be \" \"installed. You must specify the path to a player \" \"executable with --player.\" ) title = create title ( plugin ) server = create http server ( ) player = output = Player Output ( args . player , args = args . player args , filename = server . url , quiet = not args . verbose player , title = title ) try : log . info ( \"Starting player: {0}\" , args . player ) if player : player . open ( ) except OS Error as err : console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) else : server = create http server ( host = None , port = port ) player = None log . info ( \"Starting server, access with one of:\" ) for url in server . urls : log . info ( \" \" + url ) for req in iter http requests ( server , player ) : user agent = req . headers . get ( \"User-Agent\" ) or \"unknown player\" log . info ( \"Got HTTP request from {0}\" . format ( user agent ) ) stream fd = prebuffer = None while not stream fd and ( not player or player . running ) : try : streams = initial streams or fetch streams ( plugin ) initial streams = None for stream name in ( resolve stream name ( streams , s ) for s in args . stream ) : if stream name in streams : stream = streams [ stream name ] break else : log . info ( \"Stream not available, will re-fetch \" \"streams in 10 sec\" ) sleep ( 10 ) continue except Plugin Error as err : log . error ( u\"Unable to fetch new streams: {0}\" , err ) continue try : log . info ( \"Opening stream: {0} ({1})\" , stream name , type ( stream ) . shortname ( ) ) stream fd , prebuffer = open stream ( stream ) except Stream Error as err : log . error ( \"{0}\" , err ) if stream fd and prebuffer : log . debug ( \"Writing stream to player\" ) read stream ( stream fd , server , prebuffer ) server . close ( True ) player . close ( ) server . close ( )", "predictions": ["creates a stream of players from a console ."], "references": ["continuously output the stream over http ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 644, "code": "def output stream passthrough ( plugin , stream ) : global output title = create title ( plugin ) filename = '\"{0}\"' . format ( stream to url ( stream ) ) output = Player Output ( args . player , args = args . player args , filename = filename , call = True , quiet = not args . verbose player , title = title ) try : log . info ( \"Starting player: {0}\" , args . player ) output . open ( ) except OS Error as err : console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) return False return True", "predictions": ["send a stream to a stream ."], "references": ["prepares a filename to be passed to the player ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 645, "code": "def output stream ( plugin , stream ) : global output success open = False for i in range ( args . retry open ) : try : stream fd , prebuffer = open stream ( stream ) success open = True break except Stream Error as err : log . error ( \"Try {0}/{1}: Could not open stream {2} ({3})\" , i + 1 , args . retry open , stream , err ) if not success open : console . exit ( \"Could not open stream {0}, tried {1} times, exiting\" , stream , args . retry open ) output = create output ( plugin ) try : output . open ( ) except ( IO Error , OS Error ) as err : if isinstance ( output , Player Output ) : console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) else : console . exit ( \"Failed to open output: {0} ({1})\" , args . output , err ) with closing ( output ) : log . debug ( \"Writing stream to output\" ) read stream ( stream fd , output , prebuffer ) return True", "predictions": ["open the stream and open the console ."], "references": ["open stream create output and finally write the stream to output ."], "bleu": 0.15223083300988077, "rouge_l": 0.4825949367088607}
{"id": 646, "code": "def read stream ( stream , output , prebuffer , chunk size = 8192 ) : is player = isinstance ( output , Player Output ) is http = isinstance ( output , HTTP Server ) is fifo = is player and output . namedpipe show progress = isinstance ( output , File Output ) and output . fd is not stdout and sys . stdout . isatty ( ) show record progress = hasattr ( output , \"record\" ) and isinstance ( output . record , File Output ) and output . record . fd is not stdout and sys . stdout . isatty ( ) stream iterator = chain ( [ prebuffer ] , iter ( partial ( stream . read , chunk size ) , b\"\" ) ) if show progress : stream iterator = progress ( stream iterator , prefix = os . path . basename ( args . output ) ) elif show record progress : stream iterator = progress ( stream iterator , prefix = os . path . basename ( args . record ) ) try : for data in stream iterator : if is win32 and is fifo : output . player . poll ( ) if output . player . returncode is not None : log . info ( \"Player closed\" ) break try : output . write ( data ) except IO Error as err : if is player and err . errno in ACCEPTABLE ERRNO : log . info ( \"Player closed\" ) elif is http and err . errno in ACCEPTABLE ERRNO : log . info ( \"HTTP connection closed\" ) else : console . exit ( \"Error when writing to output: {0}, exiting\" , err ) break except IO Error as err : console . exit ( \"Error when reading from stream: {0}, exiting\" , err ) finally : stream . close ( ) log . info ( \"Stream ended\" )", "predictions": ["reads data from the stream ."], "references": ["reads data from stream and then writes it to the output ."], "bleu": 0.17395944730633453, "rouge_l": 0.5240549828178694}
{"id": 647, "code": "def fetch streams ( plugin ) : return plugin . streams ( stream types = args . stream types , sorting excludes = args . stream sorting excludes )", "predictions": ["fetches all streams from the plugin ."], "references": ["fetches streams using correct parameters ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 648, "code": "def resolve stream name ( streams , stream name ) : if stream name in STREAM SYNONYMS and stream name in streams : for name , stream in streams . items ( ) : if stream is streams [ stream name ] and name not in STREAM SYNONYMS : return name return stream name", "predictions": ["resolve the given stream name by name ."], "references": ["returns the real stream name of a synonym ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 649, "code": "def print plugins ( ) : pluginlist = list ( streamlink . get plugins ( ) . keys ( ) ) pluginlist formatted = \", \" . join ( sorted ( pluginlist ) ) if console . json : console . msg json ( pluginlist ) else : console . msg ( \"Loaded plugins: {0}\" , pluginlist formatted )", "predictions": ["prints all plugins . this is a string representing how to process the output of the ( message was written ."], "references": ["outputs a list of all plugins streamlink has loaded ."], "bleu": 0.09092617426809149, "rouge_l": 0.20677966101694914}
{"id": 650, "code": "def load plugins ( dirs ) : dirs = [ os . path . expanduser ( d ) for d in dirs ] for directory in dirs : if os . path . isdir ( directory ) : streamlink . load plugins ( directory ) else : log . warning ( \"Plugin path {0} does not exist or is not \" \"a directory!\" , directory )", "predictions": ["load all plugins from the given directory ."], "references": ["attempts to load plugins from a list of directories ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 651, "code": "def setup http session ( ) : if args . http proxy : streamlink . set option ( \"http-proxy\" , args . http proxy ) if args . https proxy : streamlink . set option ( \"https-proxy\" , args . https proxy ) if args . http cookie : streamlink . set option ( \"http-cookies\" , dict ( args . http cookie ) ) if args . http header : streamlink . set option ( \"http-headers\" , dict ( args . http header ) ) if args . http query param : streamlink . set option ( \"http-query-params\" , dict ( args . http query param ) ) if args . http ignore env : streamlink . set option ( \"http-trust-env\" , False ) if args . http no ssl verify : streamlink . set option ( \"http-ssl-verify\" , False ) if args . http disable dh : streamlink . set option ( \"http-disable-dh\" , True ) if args . http ssl cert : streamlink . set option ( \"http-ssl-cert\" , args . http ssl cert ) if args . http ssl cert crt key : streamlink . set option ( \"http-ssl-cert\" , tuple ( args . http ssl cert crt key ) ) if args . http timeout : streamlink . set option ( \"http-timeout\" , args . http timeout ) if args . http cookies : streamlink . set option ( \"http-cookies\" , args . http cookies ) if args . http headers : streamlink . set option ( \"http-headers\" , args . http headers ) if args . http query params : streamlink . set option ( \"http-query-params\" , args . http query params )", "predictions": ["setup the ( , but only if the proxy has been set ."], "references": ["sets the global http settings such as proxy and headers ."], "bleu": 0.1135935489027116, "rouge_l": 0.2538141470180305}
{"id": 652, "code": "def setup plugins ( extra plugin dir = None ) : if os . path . isdir ( PLUGINS DIR ) : load plugins ( [ PLUGINS DIR ] ) if extra plugin dir : load plugins ( extra plugin dir )", "predictions": ["adds all plugins to the command ."], "references": ["loads any additional plugins ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 653, "code": "def setup plugin args ( session , parser ) : plugin args = parser . add argument group ( \"Plugin options\" ) for pname , plugin in session . plugins . items ( ) : defaults = { } for parg in plugin . arguments : plugin args . add argument ( parg . argument name ( pname ) , * * parg . options ) defaults [ parg . dest ] = parg . default plugin . options = Plugin Options ( defaults )", "predictions": ["adds all the necessary ( to the command line ."], "references": ["sets streamlink plugin options ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 654, "code": "def setup plugin options ( session , plugin ) : pname = plugin . module required = Ordered Dict ( { } ) for parg in plugin . arguments : if parg . options . get ( \"help\" ) != argparse . SUPPRESS : if parg . required : required [ parg . name ] = parg value = getattr ( args , parg . namespace dest ( pname ) ) session . set plugin option ( pname , parg . dest , value ) if parg . required or value : try : for rparg in plugin . arguments . requires ( parg . name ) : required [ rparg . name ] = rparg except Runtime Error : console . logger . error ( \"{0} plugin has a configuration error and the arguments \" \"cannot be parsed\" . format ( pname ) ) break if required : for req in required . values ( ) : if not session . get plugin option ( pname , req . dest ) : prompt = req . prompt or \"Enter {0} {1}\" . format ( pname , req . name ) session . set plugin option ( pname , req . dest , console . askpass ( prompt + \": \" ) if req . sensitive else console . ask ( prompt + \": \" ) )", "predictions": ["for the given tuple , prompt and prompt the prompt ."], "references": ["sets streamlink plugin options ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 655, "code": "def log current versions ( ) : if logger . root . is Enabled For ( logging . DEBUG ) : if sys . platform == \"darwin\" : os version = \"mac OS {0}\" . format ( platform . mac ver ( ) [ 0 ] ) elif sys . platform . startswith ( \"win\" ) : os version = \"{0} {1}\" . format ( platform . system ( ) , platform . release ( ) ) else : os version = platform . platform ( ) log . debug ( \"OS:         {0}\" . format ( os version ) ) log . debug ( \"Python:     {0}\" . format ( platform . python version ( ) ) ) log . debug ( \"Streamlink: {0}\" . format ( streamlink version ) ) log . debug ( \"Requests({0}), Socks({1}), Websocket({2})\" . format ( requests . version , socks version , websocket version ) )", "predictions": ["log a ( and websocket ."], "references": ["show current installed versions"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 656, "code": "def get stream id ( self , text ) : m = self . image re . search ( text ) if m : return m . group ( \"stream id\" )", "predictions": ["get the stream id of the stream ."], "references": ["try to find a stream_id"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 657, "code": "def get iframe ( self , text ) : m = self . iframe re . search ( text ) if m : return self . session . streams ( m . group ( \"url\" ) )", "predictions": ["get the description of this instance ."], "references": ["fallback if no stream_id was found before"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 658, "code": "def startswith ( string ) : def starts with ( value ) : validate ( text , value ) if not value . startswith ( string ) : raise Value Error ( \"'{0}' does not start with '{1}'\" . format ( value , string ) ) return True return starts with", "predictions": ["checks if the given string starts with the given string ."], "references": ["checks if the string value starts with another string ."], "bleu": 0.29502343631964045, "rouge_l": 0.768503937007874}
{"id": 659, "code": "def endswith ( string ) : def ends with ( value ) : validate ( text , value ) if not value . endswith ( string ) : raise Value Error ( \"'{0}' does not end with '{1}'\" . format ( value , string ) ) return True return ends with", "predictions": ["performs a string endswith on the given string ."], "references": ["checks if the string value ends with another string ."], "bleu": 0.18885888592159467, "rouge_l": 0.31282051282051276}
{"id": 660, "code": "def contains ( string ) : def contains str ( value ) : validate ( text , value ) if string not in value : raise Value Error ( \"'{0}' does not contain '{1}'\" . format ( value , string ) ) return True return contains str", "predictions": ["checks if the given string contains the given value ."], "references": ["checks if the string value contains another string ."], "bleu": 0.27901593935858265, "rouge_l": 0.6376306620209059}
{"id": 661, "code": "def url ( * * attributes ) : def check url ( value ) : validate ( text , value ) parsed = urlparse ( value ) if not parsed . netloc : raise Value Error ( \"'{0}' is not a valid URL\" . format ( value ) ) for name , schema in attributes . items ( ) : if not hasattr ( parsed , name ) : raise Value Error ( \"Invalid URL attribute '{0}'\" . format ( name ) ) try : validate ( schema , getattr ( parsed , name ) ) except Value Error as err : raise Value Error ( \"Unable to validate URL attribute '{0}': {1}\" . format ( name , err ) ) return True if attributes . get ( \"scheme\" ) == \"http\" : attributes [ \"scheme\" ] = any ( \"http\" , \"https\" ) return check url", "predictions": ["check if the value is correct ."], "references": ["parses an url and validates its attributes ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 662, "code": "def xml find ( xpath ) : def xpath find ( value ) : validate ( ET . iselement , value ) value = value . find ( xpath ) if value is None : raise Value Error ( \"X Path '{0}' did not return an element\" . format ( xpath ) ) return validate ( ET . iselement , value ) return transform ( xpath find )", "predictions": ["find the xml element ."], "references": ["find a xml element via xpath ."], "bleu": 0.2736570128577077, "rouge_l": 0.6472148541114059}
{"id": 663, "code": "def xml findall ( xpath ) : def xpath findall ( value ) : validate ( ET . iselement , value ) return value . findall ( xpath ) return transform ( xpath findall )", "predictions": ["find the xml element in the given xpath ."], "references": ["find a list of xml elements via xpath ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 664, "code": "def dologin ( self , email , password , emailauth = \"\" , emailsteamid = \"\" , captchagid = \"-1\" , captcha text = \"\" , twofactorcode = \"\" ) : epassword , rsatimestamp = self . encrypt password ( email , password ) login data = { 'username' : email , \"password\" : epassword , \"emailauth\" : emailauth , \"loginfriendlyname\" : \"Streamlink\" , \"captchagid\" : captchagid , \"captcha text\" : captcha text , \"emailsteamid\" : emailsteamid , \"rsatimestamp\" : rsatimestamp , \"remember login\" : True , \"donotcache\" : self . donotcache , \"twofactorcode\" : twofactorcode } res = self . session . http . post ( self . dologin url , data = login data ) resp = self . session . http . json ( res , schema = self . dologin schema ) if not resp [ u\"success\" ] : if resp . get ( u\"captcha needed\" ) : captchagid = resp [ u\"captcha gid\" ] log . error ( \"Captcha result required, open this URL to see the captcha: {}\" . format ( self . captcha url . format ( captchagid ) ) ) try : captcha text = self . input ask ( \"Captcha text\" ) except Fatal Plugin Error : captcha text = None if not captcha text : return False else : if resp . get ( u\"emailauth needed\" ) : if not emailauth : try : emailauth = self . input ask ( \"Email auth code required\" ) except Fatal Plugin Error : emailauth = None if not emailauth : return False else : raise Steam Login Failed ( \"Email auth key error\" ) if resp . get ( u\"requires twofactor\" ) : try : twofactorcode = self . input ask ( \"Two factor auth code required\" ) except Fatal Plugin Error : twofactorcode = None if not twofactorcode : return False if resp . get ( u\"message\" ) : raise Steam Login Failed ( resp [ u\"message\" ] ) return self . dologin ( email , password , emailauth = emailauth , emailsteamid = resp . get ( u\"emailsteamid\" , u\"\" ) , captcha text = captcha text , captchagid = captchagid , twofactorcode = twofactorcode ) elif resp . get ( \"login complete\" ) : return True else : log . error ( \"Something when wrong when logging in to Steam\" ) return False", "predictions": ["encrypt the dologin so that it can be automatically visible ."], "references": ["logs in to steam"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 665, "code": "def get stream id ( self , html ) : stream id = stream id pattern . search ( html ) if not stream id : self . logger . error ( \"Failed to extract stream id.\" ) return stream id . group ( \"stream id\" )", "predictions": ["get a stream id from the stream ."], "references": ["returns the stream_id contained in the html ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 666, "code": "def login ( self , username , password ) : self . logger . debug ( 'login ...' ) res = self . session . http . get ( self . login url ) input list = self . input re . findall ( res . text ) if not input list : raise Plugin Error ( 'Missing input data on login website.' ) data = { } for input data in input list : try : input name = self . name re . search ( input data ) . group ( 1 ) except Attribute Error : continue try : input value = self . value re . search ( input data ) . group ( 1 ) except Attribute Error : input value = '' data [ input name ] = input value login data = { 'ctl00$Login1$User Name' : username , 'ctl00$Login1$Password' : password , 'ctl00$Login1$Login Button.x' : '0' , 'ctl00$Login1$Login Button.y' : '0' } data . update ( login data ) res = self . session . http . post ( self . login url , data = data ) for cookie in self . session . http . cookies : self . session attributes . set ( cookie . name , cookie . value , expires = 3600 * 24 ) if self . session attributes . get ( 'ASP.NET Session Id' ) and self . session attributes . get ( '.abportail1' ) : self . logger . debug ( 'New session data' ) self . set expires time cache ( ) return True else : self . logger . error ( 'Failed to login, check your username/password' ) return False", "predictions": ["this method is called when the user selects the link ."], "references": ["login and update cached cookies"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 667, "code": "def output Char Formatter ( c ) : #TODO 2: allow hex only output if 32 < c < 127 : return chr ( c ) elif c == 10 : return '\\\\n' elif c == 13 : return '\\\\r' elif c == 32 : return '\" \"' else : return '\\\\x{:02x}' . format ( c )", "predictions": ["returns the character identified by the specified output ."], "references": ["show character in readable format"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 668, "code": "def output Formatter ( s ) : result = '' def format Sub String ( s ) : for c in s : if c == 32 : yield ' ' else : yield output Char Formatter ( c ) if len ( result ) < 200 : return '' . join ( format Sub String ( s ) ) else : return '' . join ( format Sub String ( s [ : 100 ] ) ) + '...' + '' . join ( format Sub String ( s [ - 100 : ] ) )", "predictions": ["generate human readable description of string ."], "references": ["show string or char ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 669, "code": "def read Bytes ( self , n ) : if self . pos & 7 : raise Value Error ( 'read Bytes: need byte boundary' ) result = self . data [ self . pos >> 3 : ( self . pos >> 3 ) + n ] self . pos += 8 * n return result", "predictions": ["read the byte at the current position . the position is set to zero ."], "references": ["read n bytes from the stream on a byte boundary ."], "bleu": 0.10343603005129705, "rouge_l": 0.3164721141374838}
{"id": 670, "code": "def show Code ( self , width = 80 ) : #make table of all symbols with binary strings symbol Strings = [ ( self . bit Pattern ( s . index ) , self . mnemonic ( s . index ) ) for s in self ] #determine column widths the way Lisp programmers do it left Col Width , right Col Width = map ( max , map ( map , repeat ( len ) , zip ( * symbol Strings ) ) ) colwidth = left Col Width + right Col Width columns = 81 // ( colwidth + 2 ) rows = - ( - len ( symbol Strings ) // columns ) def justify ( bs ) : b , s = bs return b . rjust ( left Col Width ) + ':' + s . ljust ( right Col Width ) for i in range ( rows ) : print ( ' ' . join ( map ( justify , symbol Strings [ i : : rows ] ) ) . rstrip ( ) )", "predictions": ["copies this symbol ' s contained in two wildcard code points ."], "references": ["show all words of the code in a nice format ."], "bleu": 0.1235622127262679, "rouge_l": 0.17528735632183906}
{"id": 671, "code": "def read Tuple ( self , stream ) : length , symbol = self . decode Peek ( stream . peek ( self . max Length ) ) stream . pos += length return length , symbol", "predictions": ["read the tuple from the stream ."], "references": ["read symbol from stream . returns symbol length ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 672, "code": "def value ( self , index , extra ) : lower , upper = self . span ( index ) value = lower + ( extra or 0 ) if value > upper : raise Value Error ( 'value: extra out of range' ) return value", "predictions": ["later later be replaced by the index ."], "references": ["override if you don t define value0 and extratable"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 673, "code": "def value ( self , index , extra ) : index = index if index == 0 : return 1 , 0 if index <= self . RLEMAX : return ( 1 << index ) + extra , 0 return 1 , index - self . RLEMAX", "predictions": ["get the amount of bytes for this location ."], "references": ["give count and value ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 674, "code": "def mnemonic ( self , index ) : i , c , d0 = self . split Symbol ( index ) i Lower , = i . code . span ( i . index ) i Extra = i . extra Bits ( ) c Lower , = c . code . span ( c . index ) c Extra = c . extra Bits ( ) return 'I{}{}{}C{}{}{}{}' . format ( i Lower , '+' if i Extra else '' , 'x' * i Extra if i Extra < 6 else '[{}*x]' . format ( i Extra ) , c Lower , '+' if c Extra else '' , 'x' * c Extra if c Extra < 6 else '[{}*x]' . format ( c Extra ) , '&D=0' if d0 else '' )", "predictions": ["adds a login at the specified : this method is used to generate a login rule ."], "references": ["make a nice mnemonic"], "bleu": 0.07223943354597204, "rouge_l": 0.10720562390158171}
{"id": 675, "code": "def compile Actions ( self ) : import re self . action List = actions = [ None ] * 121 #Action 73, which is too long, looks like this when expanded: actions [ 73 ] = \"b' the '+w+b' of the '\" #find out what the columns are action Lines = self . action Table . splitlines ( ) colon Positions = [ m . start ( ) for m in re . finditer ( ':' , action Lines [ 1 ] ) ] + [ 100 ] columns = [ ( colon Positions [ i ] - 3 , colon Positions [ i + 1 ] - 3 ) for i in range ( len ( colon Positions ) - 1 ) ] for line in self . action Table . splitlines ( keepends = False ) : for start , end in columns : action = line [ start : end ] #skip empty actions if not action or action . isspace ( ) : continue #chop it up, and check if the colon is properly placed index , colon , action = action [ : 3 ] , action [ 3 ] , action [ 4 : ] assert colon == ':' #remove filler spaces at right action = action . rstrip ( ) #replace space symbols action = action . replace ( ' ' , ' ' ) w Pos = action . index ( 'w' ) #add quotes around left string when present #translation: any pattern from beginning, up to #(but not including) a + following by a w later on action = re . sub ( r\"^(.*)(?=\\+[U(]*w)\" , r\"b'\\1'\" , action ) #add quotes around right string when present #translation: anything with a w in it, followed by a + #and a pattern up to the end #(there is no variable lookbehind assertion, #so we have to copy the pattern) action = re . sub ( r\"(w[[:\\-1\\]).U]*)\\+(.*)$\" , r\"\\1+b'\\2'\" , action ) #expand shortcut for uppercase All action = action . replace ( \".U\" , \".upper()\" ) #store action actions [ int ( index ) ] = action", "predictions": ["output the initial initial = = http : / / www . org / tr / ( / ( / ( . html"], "references": ["build the action table from the text above"], "bleu": 0.05291907393644996, "rouge_l": 0.07068366164542293}
{"id": 676, "code": "def do Action ( self , w , action ) : #set environment for the Upper Case First U = self . upper Case1 return eval ( self . action List [ action ] , locals ( ) )", "predictions": ["allows you to call this to perform some queries ."], "references": ["perform the proper action"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 677, "code": "def process Stream ( self ) : print ( 'addr  hex{:{}s}binary context explanation' . format ( '' , self . width - 10 ) ) print ( 'Stream header' . center ( 60 , '-' ) ) self . window Size = self . verbose Read ( Window Size Alphabet ( ) ) print ( 'Metablock header' . center ( 60 , '=' ) ) self . ISLAST = False self . output = bytearray ( ) while not self . ISLAST : self . ISLAST = self . verbose Read ( Bool Code ( 'LAST' , description = \"Last block\" ) ) if self . ISLAST : if self . verbose Read ( Bool Code ( 'EMPTY' , description = \"Empty block\" ) ) : break if self . metablock Length ( ) : continue if not self . ISLAST and self . uncompressed ( ) : continue print ( 'Block type descriptors' . center ( 60 , '-' ) ) self . number Of Block Types = { } self . current Block Counts = { } self . block Type Codes = { } self . block Count Codes = { } for block Type in ( L , I , D ) : self . block Type ( block Type ) print ( 'Distance code parameters' . center ( 60 , '-' ) ) self . NPOSTFIX , self . NDIRECT = self . verbose Read ( Distance Param Alphabet ( ) ) self . read Literal Context Modes ( ) print ( 'Context maps' . center ( 60 , '-' ) ) self . cmaps = { } #keep the number of each kind of prefix tree for the last loop number Of Trees = { I : self . number Of Block Types [ I ] } for block Type in ( L , D ) : number Of Trees [ block Type ] = self . context Map ( block Type ) print ( 'Prefix code lists' . center ( 60 , '-' ) ) self . prefix Codes = { } for block Type in ( L , I , D ) : self . read Prefix Array ( block Type , number Of Trees [ block Type ] ) self . metablock ( )", "predictions": ["generate a stream of ( that is in ( and collects the builtins of this class ."], "references": ["process a brotli stream ."], "bleu": 0.0859076483566362, "rouge_l": 0.3024793388429752}
{"id": 678, "code": "def uncompressed ( self ) : ISUNCOMPRESSED = self . verbose Read ( Bool Code ( 'UNCMPR' , description = 'Is uncompressed?' ) ) if ISUNCOMPRESSED : self . verbose Read ( Filler Alphabet ( stream Pos = self . stream . pos ) ) print ( 'Uncompressed data:' ) self . output += self . stream . read Bytes ( self . MLEN ) print ( output Formatter ( self . output [ - self . MLEN : ] ) ) return ISUNCOMPRESSED", "predictions": ["read and return the read position ."], "references": ["if true handle uncompressed data"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 679, "code": "def block Type ( self , kind ) : NBLTYPES = self . verbose Read ( Type Count Alphabet ( 'BT#' + kind [ 0 ] . upper ( ) , description = '{} block types' . format ( kind ) , ) ) self . number Of Block Types [ kind ] = NBLTYPES if NBLTYPES >= 2 : self . block Type Codes [ kind ] = self . read Prefix Code ( Block Type Alphabet ( 'BT' + kind [ 0 ] . upper ( ) , NBLTYPES ) ) self . block Count Codes [ kind ] = self . read Prefix Code ( Block Count Alphabet ( 'BC' + kind [ 0 ] . upper ( ) ) ) block Count = self . verbose Read ( self . block Count Codes [ kind ] ) else : block Count = 1 << 24 self . current Block Counts [ kind ] = block Count", "predictions": ["blocks and returns the raw ( of this type ."], "references": ["read block type switch descriptor for given kind of blocktype ."], "bleu": 0.13564514503163538, "rouge_l": 0.18885448916408668}
{"id": 680, "code": "def IMTF ( v ) : #mtf is initialized virtually with range(infinity) mtf = [ ] for i , vi in enumerate ( v ) : #get old value from mtf. If never seen, take virtual value try : value = mtf . pop ( vi ) except Index Error : value = vi #put value at front mtf . insert ( 0 , value ) #replace transformed value v [ i ] = value", "predictions": ["tries to resolve the value into the map ."], "references": ["in place inverse move to front transform ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 681, "code": "def read Prefix Array ( self , kind , number Of Trees ) : prefixes = [ ] for i in range ( number Of Trees ) : if kind == L : alphabet = Literal Alphabet ( i ) elif kind == I : alphabet = Insert And Copy Alphabet ( i ) elif kind == D : alphabet = Distance Alphabet ( i , NPOSTFIX = self . NPOSTFIX , NDIRECT = self . NDIRECT ) self . read Prefix Code ( alphabet ) prefixes . append ( alphabet ) self . prefix Codes [ kind ] = prefixes", "predictions": ["print a fragment from a reader ."], "references": ["read prefix code array"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 682, "code": "def arrow table from vaex df ( ds , column names = None , selection = None , strings = True , virtual = False ) : names = [ ] arrays = [ ] for name , array in ds . to items ( column names = column names , selection = selection , strings = strings , virtual = virtual ) : names . append ( name ) arrays . append ( arrow array from numpy array ( array ) ) return pyarrow . Table . from arrays ( arrays , names )", "predictions": ["construct a list of load analytics from a given location"], "references": ["implementation of dataset . to_arrow_table"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 683, "code": "def patch ( f ) : name = f . name Dataset . hidden [ name ] = f return f", "predictions": ["setup the type of session ."], "references": ["adds method f to the dataset class"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 684, "code": "def graphviz ( self , dot = None ) : from graphviz import Graph , Digraph node = self . graph ( ) dot = dot or Digraph ( comment = self . expression ) def walk ( node ) : if isinstance ( node , six . string types ) : dot . node ( node , node ) return node , node else : node repr , fname , fobj , deps = node node id = node repr dot . node ( node id , node repr ) for dep in deps : dep id , dep = walk ( dep ) dot . edge ( node id , dep id ) return node id , node walk ( node ) return dot", "predictions": ["we can extend this to create a setup from the given plugin ."], "references": ["return a graphviz . digraph object with a graph of the expression"], "bleu": 0.1135935489027116, "rouge_l": 0.16116248348745044}
{"id": 685, "code": "def from astropy table ( table ) : import vaex . file . other return vaex . file . other . Dataset Astropy Table ( table = table )", "predictions": ["return the value of this args object ."], "references": ["create a vaex dataframe from an astropy table ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 686, "code": "def zeldovich ( dim = 2 , N = 256 , n = - 2.5 , t = None , scale = 1 , seed = None ) : import vaex . file return vaex . file . other . Zeldovich ( dim = dim , N = N , n = n , t = t , scale = scale )", "predictions": ["parties - squared standardized transformation for the shape ."], "references": ["creates a zeldovich dataframe ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 687, "code": "def vrange ( start , stop , step = 1 , dtype = 'f8' ) : from . column import Column Virtual Range return Column Virtual Range ( start , stop , step , dtype )", "predictions": ["create a log between versions of the specified versions ."], "references": ["creates a virtual column which is the equivalent of numpy . arange but uses 0 memory"], "bleu": 0.08699304177762722, "rouge_l": 0.22154963680387407}
{"id": 688, "code": "def open ( self , path ) : logger . debug ( \"open dataset: %r\" , path ) if path . startswith ( \"http\" ) or path . startswith ( \"ws\" ) : dataset = vaex . open ( path , thread mover = self . call in main thread ) else : dataset = vaex . open ( path ) self . add recently opened ( path ) self . dataset selector . add ( dataset ) return dataset", "predictions": ["forces a self to the module ."], "references": ["add a dataset and add it to the ui"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 689, "code": "def evaluate ( self , expression , i1 = None , i2 = None , out = None , selection = None , delay = False ) : expression = ensure strings from expressions ( expression ) result = self . server . call dataset ( \"evaluate\" , self , expression = expression , i1 = i1 , i2 = i2 , selection = selection , delay = delay ) return result", "predictions": ["evaluates a result of this object ."], "references": ["basic support for evaluate at server at least to run some unittest do not expect this to work from strings"], "bleu": 0.0289990174645553, "rouge_l": 0.0681564245810056}
{"id": 690, "code": "def depending columns ( self , ds ) : depending = set ( ) for expression in self . expressions : expression = ds . expr ( expression ) depending |= expression . variables ( ) if self . previous selection : depending |= self . previous selection . depending columns ( ds ) return depending", "predictions": ["start a 3d if the if the if it is startswith a particular if the if it has a particular if the if it exists ."], "references": ["find all columns that this selection depends on for df ds"], "bleu": 0.03925345689749394, "rouge_l": 0.0}
{"id": 691, "code": "def task ( self , task , progressbar = False ) : if self . delay : return self . executor . schedule ( task ) else : import vaex . utils callback = None try : if progressbar == True : def update ( fraction ) : bar . update ( fraction ) return True bar = vaex . utils . progressbar ( task . name ) callback = self . executor . signal progress . connect ( update ) elif progressbar : callback = self . executor . signal progress . connect ( progressbar ) result = self . executor . run ( task ) if progressbar == True : bar . finish ( ) sys . stdout . write ( '\\n' ) return result finally : if callback : self . executor . signal progress . disconnect ( callback )", "predictions": ["decorator for a endswith ."], "references": ["helper function for returning tasks results result when immediate is true otherwise the task itself which is a promise"], "bleu": 0.018373002712755784, "rouge_l": 0.1508034610630408}
{"id": 692, "code": "def sort ( self , Ncol , order ) : self . emit ( Qt Core . SIGNAL ( \"layout About To Be Changed()\" ) ) if Ncol == 0 : print ( \"by name\" ) sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) print ( sortlist ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) print ( sortlist ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) print ( ( self . indices ) ) if Ncol == 1 : if None not in self . ranking : sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) else : self . indices = list ( range ( len ( self . pairs ) ) ) print ( ( self . indices ) ) if order == Qt Core . Qt . Descending Order : self . indices . reverse ( ) print ( ( self . indices ) ) self . emit ( Qt Core . SIGNAL ( \"layout Changed()\" ) )", "predictions": ["sorts the list of players ."], "references": ["sort table by given column number ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 693, "code": "def wait ( self ) : logger . debug ( \"will wait for last plot to finish\" ) self . plot event = threading . Event ( ) self . queue update . wait ( ) self . queue replot . wait ( ) self . queue redraw . wait ( ) qt app = Qt Core . Q Core Application . instance ( ) sleep = 10 while not self . plot event . is set ( ) : logger . debug ( \"waiting for last plot to finish\" ) qt app . process Events ( ) Qt Test . Q Test . q Sleep ( sleep ) logger . debug ( \"waiting for plot finished\" )", "predictions": ["url until all the raise exceptions have been executed ."], "references": ["used for unittesting to make sure the plots are all done"], "bleu": 0.12623203108004888, "rouge_l": 0.09442724458204334}
{"id": 694, "code": "def os open ( document ) : osname = platform . system ( ) . lower ( ) if osname == \"darwin\" : os . system ( \"open \\\"\" + document + \"\\\"\" ) if osname == \"linux\" : cmd = \"xdg-open \\\"\" + document + \"\\\"&\" os . system ( cmd ) if osname == \"windows\" : os . system ( \"start \\\"\" + document + \"\\\"\" )", "predictions": ["opens the ( command for the given xpath and return the xpath ."], "references": ["open document by the default handler of the os could be a url opened by a browser a text file by an editor etc"], "bleu": 0.04535644814080821, "rouge_l": 0.10260723296888138}
{"id": 695, "code": "def write to ( f , mode ) : if hasattr ( f , 'write' ) : yield f else : f = open ( f , mode ) yield f f . close ( )", "predictions": ["xml file xpath by using an iterator ."], "references": ["flexible writing where f can be a filename or f object if filename closed after writing"], "bleu": 0.04960895415008605, "rouge_l": 0.0}
{"id": 696, "code": "def split and combine mask ( arrays ) : masks = [ np . ma . getmaskarray ( block ) for block in arrays if np . ma . is Masked Array ( block ) ] arrays = [ block . data if np . ma . is Masked Array ( block ) else block for block in arrays ] mask = None if masks : mask = masks [ 0 ] . copy ( ) for other in masks [ 1 : ] : mask |= other return arrays , mask", "predictions": ["split way to split email ( x , y ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["combines all masks from a list of arrays and logically ors them into a single mask"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 697, "code": "def nop ( self , expression , progress = False , delay = False ) : expression = ensure string from expression ( expression ) def map ( ar ) : pass def reduce ( a , b ) : pass return self . map reduce ( map , reduce , [ expression ] , delay = delay , progress = progress , name = 'nop' , to numpy = False )", "predictions": ["monkeypatches operation . this is similar to ( ."], "references": ["evaluates expression and drop the result usefull for benchmarking since vaex is usually lazy"], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 698, "code": "def plot3d ( self , x , y , z , vx = None , vy = None , vz = None , vwhat = None , limits = None , grid = None , what = \"count(*)\" , shape = 128 , selection = [ None , True ] , f = None , vcount limits = None , smooth pre = None , smooth post = None , grid limits = None , normalize = \"normalize\" , colormap = \"afmhot\" , figure key = None , fig = None , lighting = True , level = [ 0.1 , 0.5 , 0.9 ] , opacity = [ 0.01 , 0.05 , 0.1 ] , level width = 0.1 , show = True , * * kwargs ) : import vaex . ext . ipyvolume cls = vaex . ext . ipyvolume . Plot Default plot3d = cls ( df = self , x = x , y = y , z = z , vx = vx , vy = vy , vz = vz , grid = grid , shape = shape , limits = limits , what = what , f = f , figure key = figure key , fig = fig , selection = selection , smooth pre = smooth pre , smooth post = smooth post , grid limits = grid limits , vcount limits = vcount limits , normalize = normalize , colormap = colormap , * * kwargs ) if show : plot3d . show ( ) return plot3d", "predictions": ["create and return a login ."], "references": ["use at own risk requires ipyvolume"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 699, "code": "def dtype ( self , expression , internal = False ) : expression = ensure string from expression ( expression ) if expression in self . variables : return np . float64 ( 1 ) . dtype elif expression in self . columns . keys ( ) : column = self . columns [ expression ] data = column [ 0 : 1 ] dtype = data . dtype else : data = self . evaluate ( expression , 0 , 1 , filtered = False ) dtype = data . dtype if not internal : if dtype != str type : if dtype . kind in 'US' : return str type if dtype . kind == 'O' : if isinstance ( data [ 0 ] , six . string types ) : return str type return dtype", "predictions": ["see description of . for underlying c ."], "references": ["return the numpy dtype for the given expression if not a column the first row will be evaluated to get the dtype ."], "bleu": 0.027216527483056172, "rouge_l": 0.11867704280155641}
{"id": 700, "code": "def remove virtual meta ( self ) : dir = self . get private dir ( create = True ) path = os . path . join ( dir , \"virtual meta.yaml\" ) try : if os . path . exists ( path ) : os . remove ( path ) if not os . listdir ( dir ) : os . rmdir ( dir ) except : logger . exception ( \"error while trying to remove %s or %s\" , path , dir )", "predictions": ["removes the directory named by the ( ."], "references": ["removes the file with the virtual column etc it does not change the current virtual columns etc ."], "bleu": 0.06870470052394348, "rouge_l": 0.28773584905660377}
{"id": 701, "code": "def evaluate variable ( self , name ) : if isinstance ( self . variables [ name ] , six . string types ) : value = eval ( self . variables [ name ] , expression namespace , self . variables ) return value else : return self . variables [ name ]", "predictions": ["evaluates the given variable on this entity ."], "references": ["evaluates the variable given by name ."], "bleu": 0.25098621243978964, "rouge_l": 0.5398230088495575}
{"id": 702, "code": "def evaluate selection mask ( self , name = \"default\" , i1 = None , i2 = None , selection = None , cache = False ) : i1 = i1 or 0 i2 = i2 or len ( self ) scope = scopes . Block Scope Selection ( self , i1 , i2 , selection , cache = cache ) return scope . evaluate ( name )", "predictions": ["evaluates a selection ( python object symbols symbols symbols symbols symbols symbols symbols symbols symbols symbols symbols on this selection ."], "references": ["internal use ignores the filter"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 703, "code": "def add column ( self , name , f or array ) : if isinstance ( f or array , ( np . ndarray , Column ) ) : data = ar = f or array if self . length original is None : self . length unfiltered = len ( data ) self . length original = len ( data ) self . index end = self . length unfiltered if len ( ar ) != self . length original ( ) : if self . filtered : if len ( self ) == len ( ar ) : raise Value Error ( \"Array is of length %s, while the length of the Data Frame is %s due to the filtering, the (unfiltered) length is %s.\" % ( len ( ar ) , len ( self ) , self . length unfiltered ( ) ) ) raise Value Error ( \"array is of length %s, while the length of the Data Frame is %s\" % ( len ( ar ) , self . length original ( ) ) ) self . columns [ name ] = f or array if name not in self . column names : self . column names . append ( name ) else : raise Value Error ( \"functions not yet implemented\" ) self . save assign expression ( name , Expression ( self , name ) )", "predictions": ["this creates and adds appropriate columns to the array ."], "references": ["add an in memory array as a column ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 704, "code": "def rename column ( self , name , new name , unique = False , store in state = True ) : new name = vaex . utils . find valid name ( new name , used = [ ] if not unique else list ( self ) ) data = self . columns . get ( name ) if data is not None : del self . columns [ name ] self . column names [ self . column names . index ( name ) ] = new name self . columns [ new name ] = data else : expression = self . virtual columns [ name ] del self . virtual columns [ name ] self . virtual columns [ new name ] = expression if store in state : self . renamed columns . append ( ( name , new name ) ) for d in [ self . ucds , self . units , self . descriptions ] : if name in d : d [ new name ] = d [ name ] del d [ name ] return new name", "predictions": ["renames an array of column numbers ."], "references": ["renames a column not this is only the in memory name this will not be reflected on disk"], "bleu": 0.04270613179243733, "rouge_l": 0.14823815309842042}
{"id": 705, "code": "def delete virtual column ( self , name ) : del self . virtual columns [ name ] self . signal column changed . emit ( self , name , \"delete\" )", "predictions": ["deletes a virtual column ."], "references": ["deletes a virtual column from a dataframe ."], "bleu": 0.43645382979233377, "rouge_l": 0.7384987893462469}
{"id": 706, "code": "def delete variable ( self , name ) : del self . variables [ name ] self . signal variable changed . emit ( self , name , \"delete\" )", "predictions": ["deletes a launchd variable ."], "references": ["deletes a variable from a dataframe ."], "bleu": 0.2736570128577077, "rouge_l": 0.6472148541114059}
{"id": 707, "code": "def tail ( self , n = 10 ) : N = len ( self ) return self [ max ( 0 , N - n ) : min ( len ( self ) , N ) ]", "predictions": ["returns a tail of this list ."], "references": ["return a shallow copy a dataframe with the last n rows ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 708, "code": "def head and tail print ( self , n = 5 ) : from I Python import display display . display ( display . HTML ( self . head and tail table ( n ) ) )", "predictions": ["prints full head of this method ."], "references": ["display the first and last n elements of a dataframe ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 709, "code": "def set current row ( self , value ) : if ( value is not None ) and ( ( value < 0 ) or ( value >= len ( self ) ) ) : raise Index Error ( \"index %d out of range [0,%d]\" % ( value , len ( self ) ) ) self . current row = value self . signal pick . emit ( self , value )", "predictions": ["set the current row to a row . the value is incremented to reflect the parent of the underlying stream ."], "references": ["set the current row and emit the signal signal_pick ."], "bleu": 0.1758818104423743, "rouge_l": 0.4135593220338983}
{"id": 710, "code": "def selection undo ( self , name = \"default\" , executor = None ) : logger . debug ( \"undo\" ) executor = executor or self . executor assert self . selection can undo ( name = name ) selection history = self . selection histories [ name ] index = self . selection history indices [ name ] self . selection history indices [ name ] -= 1 self . signal selection changed . emit ( self ) logger . debug ( \"undo: selection history is %r, index is %r\" , selection history , self . selection history indices [ name ] )", "predictions": ["create a new selection event ."], "references": ["undo selection for the name ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 711, "code": "def selection redo ( self , name = \"default\" , executor = None ) : logger . debug ( \"redo\" ) executor = executor or self . executor assert self . selection can redo ( name = name ) selection history = self . selection histories [ name ] index = self . selection history indices [ name ] next = selection history [ index + 1 ] self . selection history indices [ name ] += 1 self . signal selection changed . emit ( self ) logger . debug ( \"redo: selection history is %r, index is %r\" , selection history , index )", "predictions": ["creates a selection over the top of the managed history ."], "references": ["redo selection for the name ."], "bleu": 0.1354599427337814, "rouge_l": 0.3727087576374745}
{"id": 712, "code": "def selection can redo ( self , name = \"default\" ) : return ( self . selection history indices [ name ] + 1 ) < len ( self . selection histories [ name ] )", "predictions": ["determines if this similarly can be pointed to the selection event ."], "references": ["can selection name be redone?"], "bleu": 0.1235622127262679, "rouge_l": 0.25416666666666665}
{"id": 713, "code": "def selection ( self , create selection , name , executor = None , execute fully = False ) : selection history = self . selection histories [ name ] previous index = self . selection history indices [ name ] current = selection history [ previous index ] if selection history else None selection = create selection ( current ) executor = executor or self . executor selection history . append ( selection ) self . selection history indices [ name ] += 1 del selection history [ self . selection history indices [ name ] : - 1 ] if 0 : if self . is local ( ) : if selection : result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) else : result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) else : self . signal selection changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) logger . debug ( \"select selection history is %r, index is %r\" , selection history , self . selection history indices [ name ] ) return result", "predictions": ["create a selection with the default signal ."], "references": ["select_lasso and select almost share the same code"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 714, "code": "def find valid name ( self , initial name ) : return vaex . utils . find valid name ( initial name , used = self . get column names ( hidden = True ) )", "predictions": ["find a name in this initial component ."], "references": ["finds a non - colliding name by optional postfixing"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 715, "code": "def root nodes ( self ) : root nodes = [ ] leafes = [ ] def walk ( node ) : if isinstance ( node , six . string types ) : leafes . append ( node ) if node in root nodes : root nodes . remove ( node ) else : node repr , fname , fobj , deps = node if node repr in self . virtual columns : leafes . append ( node repr ) if node repr in root nodes : root nodes . remove ( node repr ) for dep in deps : walk ( dep ) for column in self . virtual columns . keys ( ) : if column not in leafes : root nodes . append ( column ) node = self [ column ] . graph ( ) node repr , fname , fobj , deps = node for dep in deps : walk ( dep ) return root nodes", "predictions": ["read all current nodes from the given expression"], "references": ["returns a list of string which are the virtual columns that are not used in any other virtual column ."], "bleu": 0.03578247111355201, "rouge_l": 0.06630434782608696}
{"id": 716, "code": "def graphviz ( self , dot = None ) : from graphviz import Digraph dot = dot or Digraph ( comment = 'whole dataframe' ) root nodes = self . root nodes ( ) for column in root nodes : self [ column ] . graphviz ( dot = dot ) return dot", "predictions": ["create a graphviz for this ( ."], "references": ["return a graphviz . digraph object with a graph of all virtual columns"], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 717, "code": "def categorize ( self , column , labels = None , check = True ) : column = ensure string from expression ( column ) if check : vmin , vmax = self . minmax ( column ) if labels is None : N = int ( vmax + 1 ) labels = list ( map ( str , range ( N ) ) ) if ( vmax - vmin ) >= len ( labels ) : raise Value Error ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) self . categories [ column ] = dict ( labels = labels , N = len ( labels ) )", "predictions": ["transforms the strings to a multi - line array of labels ."], "references": ["mark column as categorical with given labels assuming zero indexing"], "bleu": 0.10390302174233558, "rouge_l": 0.09242424242424242}
{"id": 718, "code": "def hstack ( self , other , prefix = None ) : assert len ( self ) == len ( other ) , \"does not make sense to horizontally stack Data Frames with different lengths\" for name in other . get column names ( ) : if prefix : new name = prefix + name else : new name = name self . add column ( new name , other . columns [ name ] )", "predictions": ["hstack ( ) hstack ( ) , hstack , ( , ( ."], "references": ["join the columns of the other dataframe to this one assuming the ordering is the same"], "bleu": 0.06377006603195191, "rouge_l": 0.0}
{"id": 719, "code": "def patch ( f ) : name = f . name setattr ( Data Frame , name , f ) return f", "predictions": ["patch the name of the ( ."], "references": ["adds method f to the dataframe class"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 720, "code": "def as recarray ( self ) : dtype = [ ( k , v . dtype ) for k , v in self . dict . iteritems ( ) ] R = numpy . recarray ( len ( self . dict [ k ] ) , dtype = dtype ) for key in self . dict : R [ key ] = self . dict [ key ] return R", "predictions": ["transform this list into a key ."], "references": ["convert into numpy recordarray"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 721, "code": "def show versions ( ) : core deps = [ 'audioread' , 'numpy' , 'scipy' , 'sklearn' , 'joblib' , 'decorator' , 'six' , 'soundfile' , 'resampy' , 'numba' ] extra deps = [ 'numpydoc' , 'sphinx' , 'sphinx rtd theme' , 'sphinxcontrib.versioning' , 'sphinx-gallery' , 'pytest' , 'pytest-mpl' , 'pytest-cov' , 'matplotlib' ] print ( 'INSTALLED VERSIONS' ) print ( '------------------' ) print ( 'python: {}\\n' . format ( sys . version ) ) print ( 'librosa: {}\\n' . format ( version ) ) for dep in core deps : print ( '{}: {}' . format ( dep , get mod version ( dep ) ) ) print ( '' ) for dep in extra deps : print ( '{}: {}' . format ( dep , get mod version ( dep ) ) ) pass", "predictions": ["show the ( and outputs its dependencies ."], "references": ["return the version information for all librosa dependencies ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 722, "code": "def adjust tuning ( input file , output file ) : print ( 'Loading ' , input file ) y , sr = librosa . load ( input file ) print ( 'Separating harmonic component ... ' ) y harm = librosa . effects . harmonic ( y ) print ( 'Estimating tuning ... ' ) tuning = librosa . estimate tuning ( y = y harm , sr = sr ) print ( '{:+0.2f} cents' . format ( 100 * tuning ) ) print ( 'Applying pitch-correction of {:+0.2f} cents' . format ( - 100 * tuning ) ) y tuned = librosa . effects . pitch shift ( y , sr , - tuning ) print ( 'Saving tuned audio to: ' , output file ) librosa . output . write wav ( output file , y tuned , sr )", "predictions": ["adjust the librosa object to show all contents ."], "references": ["load audio estimate tuning apply pitch correction and save ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 723, "code": "def cqt filter fft ( sr , fmin , n bins , bins per octave , tuning , filter scale , norm , sparsity , hop length = None , window = 'hann' ) : basis , lengths = filters . constant q ( sr , fmin = fmin , n bins = n bins , bins per octave = bins per octave , tuning = tuning , filter scale = filter scale , norm = norm , pad fft = True , window = window ) n fft = basis . shape [ 1 ] if ( hop length is not None and n fft < 2.0 ** ( 1 + np . ceil ( np . log2 ( hop length ) ) ) ) : n fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop length ) ) ) ) basis *= lengths [ : , np . newaxis ] / float ( n fft ) fft = get fftlib ( ) fft basis = fft . fft ( basis , n = n fft , axis = 1 ) [ : , : ( n fft // 2 ) + 1 ] fft basis = util . sparsify rows ( fft basis , quantile = sparsity ) return fft basis , n fft , lengths", "predictions": ["cqt filter the fft according to the rules of the fft ."], "references": ["generate the frequency domain constant - q filter basis ."], "bleu": 0.1235622127262679, "rouge_l": 0.18484848484848485}
{"id": 724, "code": "def trim stack ( cqt resp , n bins ) : max col = min ( x . shape [ 1 ] for x in cqt resp ) cqt resp = np . vstack ( [ x [ : , : max col ] for x in cqt resp ] [ : : - 1 ] ) return np . ascontiguousarray ( cqt resp [ - n bins : ] . T ) . T", "predictions": ["trims the ( at the given shape ."], "references": ["helper function to trim and stack a collection of cqt responses"], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 725, "code": "def cqt response ( y , n fft , hop length , fft basis , mode ) : D = stft ( y , n fft = n fft , hop length = hop length , window = 'ones' , pad mode = mode ) return fft basis . dot ( D )", "predictions": ["create a cqt instance from response to an cqt ."], "references": ["compute the filter response with a target stft hop ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 726, "code": "def early downsample count ( nyquist , filter cutoff , hop length , n octaves ) : downsample count1 = max ( 0 , int ( np . ceil ( np . log2 ( audio . BW FASTEST * nyquist / filter cutoff ) ) - 1 ) - 1 ) num twos = num two factors ( hop length ) downsample count2 = max ( 0 , num twos - n octaves + 1 ) return min ( downsample count1 , downsample count2 )", "predictions": ["count the count of the count between the given list of ( ."], "references": ["compute the number of early downsampling operations"], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 727, "code": "def early downsample ( y , sr , hop length , res type , n octaves , nyquist , filter cutoff , scale ) : downsample count = early downsample count ( nyquist , filter cutoff , hop length , n octaves ) if downsample count > 0 and res type == 'kaiser fast' : downsample factor = 2 ** ( downsample count ) hop length //= downsample factor if len ( y ) < downsample factor : raise Parameter Error ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n octaves ) ) new sr = sr / float ( downsample factor ) y = audio . resample ( y , sr , new sr , res type = res type , scale = True ) if not scale : y *= np . sqrt ( downsample factor ) sr = new sr return y , sr , hop length", "predictions": ["return the factor of cluster factor ."], "references": ["perform early downsampling on an audio signal if it applies ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 728, "code": "def check axes ( axes ) : if axes is None : import matplotlib . pyplot as plt axes = plt . gca ( ) elif not isinstance ( axes , Axes ) : raise Value Error ( \"`axes` must be an instance of matplotlib.axes.Axes. \" \"Found type(axes)={}\" . format ( type ( axes ) ) ) return axes", "predictions": ["check if this ledger has at least one active active axes ."], "references": ["check if axes is an instance of an axis object . if not use gca ."], "bleu": 0.1113283703518327, "rouge_l": 0.2785388127853881}
{"id": 729, "code": "def scale axes ( axes , ax type , which ) : kwargs = dict ( ) if which == 'x' : thresh = 'linthreshx' base = 'basex' scale = 'linscalex' scaler = axes . set xscale limit = axes . set xlim else : thresh = 'linthreshy' base = 'basey' scale = 'linscaley' scaler = axes . set yscale limit = axes . set ylim if ax type == 'mel' : mode = 'symlog' kwargs [ thresh ] = 1000.0 kwargs [ base ] = 2 elif ax type == 'log' : mode = 'symlog' kwargs [ base ] = 2 kwargs [ thresh ] = core . note to hz ( 'C2' ) kwargs [ scale ] = 0.5 elif ax type in [ 'cqt' , 'cqt hz' , 'cqt note' ] : mode = 'log' kwargs [ base ] = 2 elif ax type == 'tempo' : mode = 'log' kwargs [ base ] = 2 limit ( 16 , 480 ) else : return scaler ( mode , * * kwargs )", "predictions": ["scale the current axes of the input axes ."], "references": ["set the axis scaling"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 730, "code": "def coord fft hz ( n , sr = 22050 , * * kwargs ) : n fft = 2 * ( n - 1 ) basis = core . fft frequencies ( sr = sr , n fft = n fft ) fmax = basis [ - 1 ] basis -= 0.5 * ( basis [ 1 ] - basis [ 0 ] ) basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) return basis", "predictions": ["compute the fft basis for faces in the basis . the basis is computed in the case of the basis ."], "references": ["get the frequencies for fft bins"], "bleu": 0.0690889519686715, "rouge_l": 0.16464237516869096}
{"id": 731, "code": "def coord mel hz ( n , fmin = 0 , fmax = 11025.0 , * * kwargs ) : if fmin is None : fmin = 0 if fmax is None : fmax = 11025.0 basis = core . mel frequencies ( n , fmin = fmin , fmax = fmax ) basis [ 1 : ] -= 0.5 * np . diff ( basis ) basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) return basis", "predictions": ["mel basis function for o instances . this method is a simple basis for the case where the basis is the first basis of the basis ."], "references": ["get the frequencies for mel bins"], "bleu": 0.053414136238197775, "rouge_l": 0.06846240179573512}
{"id": 732, "code": "def coord cqt hz ( n , fmin = None , bins per octave = 12 , * * kwargs ) : if fmin is None : fmin = core . note to hz ( 'C1' ) return core . cqt frequencies ( n + 1 , fmin = fmin / 2.0 ** ( 0.5 / bins per octave ) , bins per octave = bins per octave )", "predictions": ["return the cqt estimate of the given coordinate ."], "references": ["get cqt bin frequencies"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 733, "code": "def coord chroma ( n , bins per octave = 12 , * * kwargs ) : return np . linspace ( 0 , ( 12.0 * n ) / bins per octave , num = n + 1 , endpoint = True )", "predictions": ["compute the chroma for a given set of points ."], "references": ["get chroma bin numbers"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 734, "code": "def coord time ( n , sr = 22050 , hop length = 512 , * * kwargs ) : return core . frames to time ( np . arange ( n + 1 ) , sr = sr , hop length = hop length )", "predictions": ["compute the core system time stamp for a given period ."], "references": ["get time coordinates from frames"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 735, "code": "def window ss fill ( x , win sq , n frames , hop length ) : n = len ( x ) n fft = len ( win sq ) for i in range ( n frames ) : sample = i * hop length x [ sample : min ( n , sample + n fft ) ] += win sq [ : max ( 0 , min ( n fft , n - sample ) ) ]", "predictions": ["fills the specified range with specified values ."], "references": ["helper function for window sum - square calculation ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 736, "code": "def match interval overlaps ( query , intervals to , candidates ) : best score = - 1 best idx = - 1 for idx in candidates : score = jaccard ( query , intervals to [ idx ] ) if score > best score : best score , best idx = score , idx return best idx", "predictions": ["get the in the in the , given a , and a in the , sum of the in the , as a in the , from the , section 3 ."], "references": ["find the best jaccard match from query to candidates"], "bleu": 0.04180647946097227, "rouge_l": 0.10854092526690391}
{"id": 737, "code": "def match intervals ( intervals from , intervals to , strict = True ) : start index = np . argsort ( intervals to [ : , 0 ] ) end index = np . argsort ( intervals to [ : , 1 ] ) start sorted = intervals to [ start index , 0 ] end sorted = intervals to [ end index , 1 ] search ends = np . searchsorted ( start sorted , intervals from [ : , 1 ] , side = 'right' ) search starts = np . searchsorted ( end sorted , intervals from [ : , 0 ] , side = 'left' ) output = np . empty ( len ( intervals from ) , dtype = numba . uint32 ) for i in range ( len ( intervals from ) ) : query = intervals from [ i ] after query = search ends [ i ] before query = search starts [ i ] candidates = set ( start index [ : after query ] ) & set ( end index [ before query : ] ) if len ( candidates ) > 0 : output [ i ] = match interval overlaps ( query , intervals to , candidates ) elif strict : raise Parameter Error else : dist before = np . inf dist after = np . inf if search starts [ i ] > 0 : dist before = query [ 0 ] - end sorted [ search starts [ i ] - 1 ] if search ends [ i ] + 1 < len ( intervals to ) : dist after = start sorted [ search ends [ i ] + 1 ] - query [ 1 ] if dist before < dist after : output [ i ] = end index [ search starts [ i ] - 1 ] else : output [ i ] = start index [ search ends [ i ] + 1 ] return output", "predictions": ["delete all valid virtual arrays and finds the output in the input array ."], "references": ["numba - accelerated interval matching algorithm ."], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 738, "code": "def get files ( dir name , extensions ) : dir name = os . path . abspath ( os . path . expanduser ( dir name ) ) myfiles = set ( ) for sub ext in extensions : globstr = os . path . join ( dir name , '*' + os . path . extsep + sub ext ) myfiles |= set ( glob . glob ( globstr ) ) return myfiles", "predictions": ["returns a list of directory paths to monitor ."], "references": ["helper function to get files in a single directory"], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 739, "code": "def process arguments ( args ) : parser = argparse . Argument Parser ( description = 'Time stretching example' ) parser . add argument ( 'input file' , action = 'store' , help = 'path to the input file (wav, mp3, etc)' ) parser . add argument ( 'output file' , action = 'store' , help = 'path to the stretched output (wav)' ) parser . add argument ( '-s' , '--speed' , action = 'store' , type = float , default = 2.0 , required = False , help = 'speed' ) return vars ( parser . parse args ( args ) )", "predictions": ["loop through input input and convert them to a single entity ."], "references": ["argparse function to get the program parameters"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 740, "code": "def beat local score ( onset envelope , period ) : window = np . exp ( - 0.5 * ( np . arange ( - period , period + 1 ) * 32.0 / period ) ** 2 ) return scipy . signal . convolve ( normalize onsets ( onset envelope ) , window , 'same' )", "predictions": ["process the and n color with ( ( ."], "references": ["construct the local score for an onset envlope and given period"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 741, "code": "def beat track dp ( localscore , period , tightness ) : backlink = np . zeros like ( localscore , dtype = int ) cumscore = np . zeros like ( localscore ) window = np . arange ( - 2 * period , - np . round ( period / 2 ) + 1 , dtype = int ) if tightness <= 0 : raise Parameter Error ( 'tightness must be strictly positive' ) txwt = - tightness * ( np . log ( - window / period ) ** 2 ) first beat = True for i , score i in enumerate ( localscore ) : z pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) candidates = txwt . copy ( ) candidates [ z pad : ] = candidates [ z pad : ] + cumscore [ window [ z pad : ] ] beat location = np . argmax ( candidates ) cumscore [ i ] = score i + candidates [ beat location ] if first beat and score i < 0.01 * localscore . max ( ) : backlink [ i ] = - 1 else : backlink [ i ] = window [ beat location ] first beat = False window = window + 1 return backlink , cumscore", "predictions": ["generate a current current current current current current current current current raise an ( if not a set of is at least the minimum level ."], "references": ["core dynamic program for beat tracking"], "bleu": 0.03925345689749394, "rouge_l": 0.0}
{"id": 742, "code": "def last beat ( cumscore ) : maxes = util . localmax ( cumscore ) med score = np . median ( cumscore [ np . argwhere ( maxes ) ] ) return np . argwhere ( ( cumscore * maxes * 2 > med score ) ) . max ( )", "predictions": ["compute the selection of the selection"], "references": ["get the last beat from the cumulative score array"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 743, "code": "def conv3x3 ( in planes , out planes , dilation = 1 ) : return nn . Conv2d ( in planes , out planes , kernel size = 3 , padding = dilation , dilation = dilation )", "predictions": ["finds the selection for all self - simplest components ."], "references": ["3x3 convolution with padding"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 744, "code": "def average ( self , n = 0 ) : assert n >= 0 for key in self . val history : values = np . array ( self . val history [ key ] [ - n : ] ) nums = np . array ( self . n history [ key ] [ - n : ] ) avg = np . sum ( values * nums ) / np . sum ( nums ) self . output [ key ] = avg self . ready = True", "predictions": ["generate a selection for the given ] ."], "references": ["average latest n values or all values"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 745, "code": "def scatter ( input , devices , streams = None ) : if streams is None : streams = [ None ] * len ( devices ) if isinstance ( input , list ) : chunk size = ( len ( input ) - 1 ) // len ( devices ) + 1 outputs = [ scatter ( input [ i ] , [ devices [ i // chunk size ] ] , [ streams [ i // chunk size ] ] ) for i in range ( len ( input ) ) ] return outputs elif isinstance ( input , torch . Tensor ) : output = input . contiguous ( ) stream = streams [ 0 ] if output . numel ( ) > 0 else None with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : output = output . cuda ( devices [ 0 ] , non blocking = True ) return output else : raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) )", "predictions": ["copies the self - array of name or name to the given create ."], "references": ["scatters tensor across multiple gpus ."], "bleu": 0.08839374326825923, "rouge_l": 0.10777385159010601}
{"id": 746, "code": "def start ( self ) : if not self . is running : self . t start = time ( ) self . is running = True self . t last = time ( )", "predictions": ["starts the example , but once the container is started ."], "references": ["start the timer ."], "bleu": 0.12605968092174913, "rouge_l": 0.2911694510739857}
{"id": 747, "code": "def scatter kwargs ( inputs , kwargs , target gpus , dim = 0 ) : inputs = scatter ( inputs , target gpus , dim ) if inputs else [ ] kwargs = scatter ( kwargs , target gpus , dim ) if kwargs else [ ] if len ( inputs ) < len ( kwargs ) : inputs . extend ( [ ( ) for in range ( len ( kwargs ) - len ( inputs ) ) ] ) elif len ( kwargs ) < len ( inputs ) : kwargs . extend ( [ { } for in range ( len ( inputs ) - len ( kwargs ) ) ] ) inputs = tuple ( inputs ) kwargs = tuple ( kwargs ) return inputs , kwargs", "predictions": ["create a new root operation ."], "references": ["scatter with support for kwargs dictionary"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 748, "code": "async def json ( self , * , encoding : str = None , loads : JSON Decoder = DEFAULT JSON DECODER , content type : Optional [ str ] = 'application/json' ) -> Any : return await self . aws json ( encoding = encoding , loads = loads , content type = content type )", "predictions": ["use this to aws the ( [ ] , ] [ ] , int ] [ ] , int , root ] [ ] , int , int , int , int , root ] [ ] , int , int , int , int , int , int ,"], "references": ["read and decodes json response ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 749, "code": "async def text ( self , * , encoding : Optional [ str ] = None , errors : str = 'strict' ) -> str : return await self . aws text ( encoding = encoding , errors = errors )", "predictions": [". method for performing ( if specified if specified if specified if specified if the default is exceeded ."], "references": ["read response payload and decode ."], "bleu": 0.06439931429457924, "rouge_l": 0.0882778581765557}
{"id": 750, "code": "async def handle callback ( self , aws callback : typing . Coroutine , response ) : callback result = None try : callback result = await aws callback except Nothing Matched Error as e : self . logger . error ( f'<Item: {str(e).lower()}>' ) except Exception as e : self . logger . error ( f'<Callback[{aws callback. name }]: {e}' ) return callback result , response", "predictions": ["call a self - processing self for processing . < p > note : you can get the output from the normal api ."], "references": ["process coroutine callback function"], "bleu": 0.042601467364417965, "rouge_l": 0.0}
{"id": 751, "code": "async def multiple request ( self , urls , is gather = False , * * kwargs ) : if is gather : resp results = await asyncio . gather ( * [ self . handle request ( self . request ( url = url , * * kwargs ) ) for url in urls ] , return exceptions = True ) for index , task result in enumerate ( resp results ) : if not isinstance ( task result , Runtime Error ) and task result : , response = task result response . index = index yield response else : for index , url in enumerate ( urls ) : , response = await self . handle request ( self . request ( url = url , * * kwargs ) ) response . index = index yield response", "predictions": ["( possibly re ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) = . . . )"], "references": ["for crawling multiple urls"], "bleu": 0.030787460505623344, "rouge_l": 0.0}
{"id": 752, "code": "def request ( self , url : str , method : str = 'GET' , * , callback = None , encoding : typing . Optional [ str ] = None , headers : dict = None , metadata : dict = None , request config : dict = None , request session = None , * * kwargs ) : headers = headers or { } metadata = metadata or { } request config = request config or { } request session = request session or self . request session headers . update ( self . headers . copy ( ) ) request config . update ( self . request config . copy ( ) ) kwargs . update ( self . kwargs . copy ( ) ) return Request ( url = url , method = method , callback = callback , encoding = encoding , headers = headers , metadata = metadata , request config = request config , request session = request session , * * kwargs )", "predictions": ["makes a as json ."], "references": ["init a request class for crawling html"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 753, "code": "async def start master ( self ) : for url in self . start urls : request ins = self . request ( url = url , callback = self . parse , metadata = self . metadata ) self . request queue . put nowait ( self . handle request ( request ins ) ) workers = [ asyncio . ensure future ( self . start worker ( ) ) for i in range ( self . worker numbers ) ] for worker in workers : self . logger . info ( f\"Worker started: {id(worker)}\" ) await self . request queue . join ( ) if not self . is async start : await self . stop ( SIGINT ) else : await self . cancel tasks ( )", "predictions": ["versions of the method ."], "references": ["actually start crawling ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 754, "code": "def normalize task v2 ( task ) : result = dict ( ) mod arg parser = Module Args Parser ( task ) try : action , arguments , result [ 'delegate to' ] = mod arg parser . parse ( ) except Ansible Parser Error as e : try : task info = \"%s:%s\" % ( task [ FILENAME KEY ] , task [ LINE NUMBER KEY ] ) del task [ FILENAME KEY ] del task [ LINE NUMBER KEY ] except Key Error : task info = \"Unknown\" try : import pprint pp = pprint . Pretty Printer ( indent = 2 ) task pprint = pp . pformat ( task ) except Import Error : task pprint = task raise System Exit ( \"Couldn't parse task at %s (%s)\\n%s\" % ( task info , e . message , task pprint ) ) if ' uses shell' in arguments : action = 'shell' del ( arguments [ ' uses shell' ] ) for ( k , v ) in list ( task . items ( ) ) : if k in ( 'action' , 'local action' , 'args' , 'delegate to' ) or k == action : continue else : result [ k ] = v result [ 'action' ] = dict ( ansible module = action ) if ' raw params' in arguments : result [ 'action' ] [ ' ansible arguments ' ] = arguments [ ' raw params' ] . split ( ' ' ) del ( arguments [ ' raw params' ] ) else : result [ 'action' ] [ ' ansible arguments ' ] = list ( ) if 'argv' in arguments and not result [ 'action' ] [ ' ansible arguments ' ] : result [ 'action' ] [ ' ansible arguments ' ] = arguments [ 'argv' ] del ( arguments [ 'argv' ] ) result [ 'action' ] . update ( arguments ) return result", "predictions": ["convert the arguments of a tuning object to a string ."], "references": ["ensures tasks have an action key and strings are converted to python objects"], "bleu": 0.09497094417933137, "rouge_l": 0.08209959623149395}
{"id": 755, "code": "def wheel dist name ( self ) : return '-' . join ( ( safer name ( self . distribution . get name ( ) ) , safer version ( self . distribution . get version ( ) ) ) )", "predictions": ["get the wheel for this wheel ."], "references": ["return distribution full name with - replaced with _"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 756, "code": "def get archive basename ( self ) : impl tag , abi tag , plat tag = self . get tag ( ) archive basename = \"%s-%s-%s-%s\" % ( self . wheel dist name , impl tag , abi tag , plat tag ) return archive basename", "predictions": ["this method is called to read an stack of the underlying image ."], "references": ["return archive name without extension"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 757, "code": "def add requirements ( self , metadata path ) : additional = list ( self . setupcfg requirements ( ) ) if not additional : return pkg info = read pkg info ( metadata path ) if 'Provides-Extra' in pkg info or 'Requires-Dist' in pkg info : warnings . warn ( 'setup.cfg requirements overwrite values from setup.py' ) del pkg info [ 'Provides-Extra' ] del pkg info [ 'Requires-Dist' ] for k , v in additional : pkg info [ k ] = v write pkg info ( metadata path , pkg info )", "predictions": ["we need to add to add this as a response ."], "references": ["add additional requirements from setup . cfg to file metadata_path"], "bleu": 0.1354599427337814, "rouge_l": 0.1921259842519685}
{"id": 758, "code": "def telemetry client ( self , value : Bot Telemetry Client ) -> None : if value is None : self . telemetry client = Null Telemetry Client ( ) else : self . telemetry client = value", "predictions": ["creates a new instance ."], "references": ["sets the telemetry client for logging events ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 759, "code": "def create db and container ( self ) : db id = self . config . database container name = self . config . container self . db = self . get or create database ( self . client , db id ) self . container = self . get or create container ( self . client , container name )", "predictions": ["early the downsample database"], "references": ["call the get or create methods ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 760, "code": "def get step name ( self , index : int ) -> str : step name = self . steps [ index ] . qualname if not step name or \">\" in step name : step name = f\"Step{index + 1}of{len(self. steps)}\" return step name", "predictions": ["check method for axes ."], "references": ["give the waterfall step a unique name"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 761, "code": "def c if ( self , classical , val ) : if not isinstance ( classical , Classical Register ) : raise Qiskit Error ( \"c if must be used with a classical register\" ) if val < 0 : raise Qiskit Error ( \"control value should be non-negative\" ) self . control = ( classical , val ) return self", "predictions": [". control = ( or ( ."], "references": ["add classical control on register classical and value val ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 762, "code": "def qasmif ( self , string ) : if self . control is None : return string return \"if(%s==%d) \" % ( self . control [ 0 ] . name , self . control [ 1 ] ) + string", "predictions": ["transforms the n - bit n to a string in order to build a * * object ."], "references": ["print an if statement if needed ."], "bleu": 0.06809398432036522, "rouge_l": 0.08689458689458689}
{"id": 763, "code": "def u base ( self , theta , phi , lam , q ) : return self . append ( U Base ( theta , phi , lam ) , [ q ] , [ ] )", "predictions": ["return a new instance of this class ."], "references": ["apply u to q ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 764, "code": "def to matrix ( self ) : theta , phi , lam = self . params return numpy . array ( [ [ numpy . cos ( theta / 2 ) , - numpy . exp ( 1j * lam ) * numpy . sin ( theta / 2 ) ] , [ numpy . exp ( 1j * phi ) * numpy . sin ( theta / 2 ) , numpy . exp ( 1j * ( phi + lam ) ) * numpy . cos ( theta / 2 ) ] ] , dtype = complex )", "predictions": ["return a triangle representing this quaternion with an identity matrix ."], "references": ["return a numpy . array for the u3 gate ."], "bleu": 0.16108992769687397, "rouge_l": 0.28818897637795277}
{"id": 765, "code": "def exp fit fun ( x , a , tau , c ) : return a * np . exp ( - x / tau ) + c", "predictions": ["the goodness of the function ."], "references": ["function used to fit the exponential decay ."], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 766, "code": "def osc fit fun ( x , a , tau , f , phi , c ) : return a * np . exp ( - x / tau ) * np . cos ( 2 * np . pi * f * x + phi ) + c", "predictions": ["time all other ( vectors ."], "references": ["function used to fit the decay cosine ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 767, "code": "def trim ( image ) : background = PIL . Image . new ( image . mode , image . size , image . getpixel ( ( 0 , 0 ) ) ) diff = PIL . Image Chops . difference ( image , background ) diff = PIL . Image Chops . add ( diff , diff , 2.0 , - 100 ) bbox = diff . getbbox ( ) if bbox : image = image . crop ( bbox ) return image", "predictions": ["window to ] , ensures that the final fill is not equal to the final image ."], "references": ["trim a pil image and remove white space ."], "bleu": 0.07994607499472013, "rouge_l": 0.1628838451268358}
{"id": 768, "code": "def get gate span ( qregs , instruction ) : min index = len ( qregs ) max index = 0 for qreg in instruction . qargs : index = qregs . index ( qreg ) if index < min index : min index = index if index > max index : max index = index if instruction . cargs : return qregs [ min index : ] return qregs [ min index : max index + 1 ]", "predictions": ["get the gate of the current instruction with the given instruction ."], "references": ["get the list of qubits drawing this gate would cover"], "bleu": 0.15537125692760353, "rouge_l": 0.2772727272727273}
{"id": 769, "code": "def is cptp ( self , atol = None , rtol = None ) : if self . data [ 1 ] is not None : return False if atol is None : atol = self . atol if rtol is None : rtol = self . rtol accum = 0j for op in self . data [ 0 ] : accum += np . dot ( np . transpose ( np . conj ( op ) ) , op ) return is identity matrix ( accum , rtol = rtol , atol = atol )", "predictions": ["check if this vector is just a cptp ."], "references": ["return true if completely - positive trace - preserving ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 770, "code": "def conjugate ( self ) : kraus l , kraus r = self . data kraus l = [ k . conj ( ) for k in kraus l ] if kraus r is not None : kraus r = [ k . conj ( ) for k in kraus r ] return Kraus ( ( kraus l , kraus r ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["compute a conjugate of this instance ."], "references": ["return the conjugate of the quantumchannel ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 771, "code": "def transpose ( self ) : kraus l , kraus r = self . data kraus l = [ k . T for k in kraus l ] if kraus r is not None : kraus r = [ k . T for k in kraus r ] return Kraus ( ( kraus l , kraus r ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )", "predictions": ["transpose content of this future ."], "references": ["return the transpose of the quantumchannel ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 772, "code": "def real ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) lhs = self . children [ 1 ] . real ( nested scope ) rhs = self . children [ 2 ] . real ( nested scope ) return operation ( lhs , rhs )", "predictions": ["invokes the nested operation on this nested operation ."], "references": ["return the correspond floating point number ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 773, "code": "def sym ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) lhs = self . children [ 1 ] . sym ( nested scope ) rhs = self . children [ 2 ] . sym ( nested scope ) return operation ( lhs , rhs )", "predictions": ["generate a sym to the nested operation ."], "references": ["return the correspond symbolic number ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 774, "code": "def process custom unitary ( self , node ) : name = node . name if node . arguments is not None : args = self . process node ( node . arguments ) else : args = [ ] bits = [ self . process bit id ( node element ) for node element in node . bitlist . children ] if name in self . gates : gargs = self . gates [ name ] [ \"args\" ] gbits = self . gates [ name ] [ \"bits\" ] maxidx = max ( map ( len , bits ) ) for idx in range ( maxidx ) : self . arg stack . append ( { gargs [ j ] : args [ j ] for j in range ( len ( gargs ) ) } ) element = [ idx * x for x in [ len ( bits [ j ] ) > 1 for j in range ( len ( bits ) ) ] ] self . bit stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] for j in range ( len ( gbits ) ) } ) self . create dag op ( name , [ self . arg stack [ - 1 ] [ s ] . sym ( ) for s in gargs ] , [ self . bit stack [ - 1 ] [ s ] for s in gbits ] ) self . arg stack . pop ( ) self . bit stack . pop ( ) else : raise Qiskit Error ( \"internal error undefined gate:\" , \"line=%s\" % node . line , \"file=%s\" % node . file )", "predictions": ["process error probabilities on the tree ."], "references": ["process a custom unitary node ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 775, "code": "def process cnot ( self , node ) : id0 = self . process bit id ( node . children [ 0 ] ) id1 = self . process bit id ( node . children [ 1 ] ) if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == 1 or len ( id1 ) == 1 ) : raise Qiskit Error ( \"internal error: qreg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) for idx in range ( maxidx ) : if len ( id0 ) > 1 and len ( id1 ) > 1 : self . dag . apply operation back ( CX Base ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) elif len ( id0 ) > 1 : self . dag . apply operation back ( CX Base ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) else : self . dag . apply operation back ( CX Base ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition )", "predictions": ["this is called by the ( class ."], "references": ["process a cnot gate node ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 776, "code": "def process measure ( self , node ) : id0 = self . process bit id ( node . children [ 0 ] ) id1 = self . process bit id ( node . children [ 1 ] ) if len ( id0 ) != len ( id1 ) : raise Qiskit Error ( \"internal error: reg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) for idx , idy in zip ( id0 , id1 ) : self . dag . apply operation back ( Measure ( ) , [ idx ] , [ idy ] , self . condition )", "predictions": ["generate nodes for ( ."], "references": ["process a measurement node ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 777, "code": "def process if ( self , node ) : creg name = node . children [ 0 ] . name creg = self . dag . cregs [ creg name ] cval = node . children [ 1 ] . value self . condition = ( creg , cval ) self . process node ( node . children [ 2 ] ) self . condition = None", "predictions": ["evaluates all ( reachable nodes on the node ."], "references": ["process an if node ."], "bleu": 0.18575057999133596, "rouge_l": 0.3012345679012346}
{"id": 778, "code": "def qasm ( self , prec = 15 ) : return \"measure \" + self . children [ 0 ] . qasm ( prec ) + \" -> \" + self . children [ 1 ] . qasm ( prec ) + \";\"", "predictions": ["get the qasm at the top of this sensors ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 779, "code": "def to string ( self , indent ) : ind = indent * ' ' print ( ind , 'indexed id' , self . name , self . index )", "predictions": ["override this method to convert a representation of this class to a string ."], "references": ["print with indent ."], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 780, "code": "def validate ( instance ) : try : = instance . schema . validate ( instance . to dict ( ) ) except Validation Error as ex : raise Model Validation Error ( ex . messages , ex . field names , ex . fields , ex . data , * * ex . kwargs )", "predictions": ["validates the received configuration of the model . this method performs the format of the model ."], "references": ["validate the internal representation of the instance ."], "bleu": 0.10802314890908067, "rouge_l": 0.34221598877980364}
{"id": 781, "code": "def validate after init ( init method ) : @ wraps ( init method ) def decorated ( self , * * kwargs ) : try : = self . shallow schema . validate ( kwargs ) except Validation Error as ex : raise Model Validation Error ( ex . messages , ex . field names , ex . fields , ex . data , * * ex . kwargs ) from None init method ( self , * * kwargs ) return decorated", "predictions": ["validates api method required for this method ."], "references": ["add validation after instantiation ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 782, "code": "def qft ( circ , q , n ) : for j in range ( n ) : for k in range ( j ) : circ . cu1 ( math . pi / float ( 2 ** ( j - k ) ) , q [ j ] , q [ k ] ) circ . h ( q [ j ] )", "predictions": ["create an approximation of the elements of the queue with values lower than q ."], "references": ["n - qubit qft on q in circ ."], "bleu": 0.09103526405546068, "rouge_l": 0.17453505007153075}
{"id": 783, "code": "def random unitary matrix ( dim , seed = None ) : warnings . warn ( 'The random unitary matrix() function in qiskit.tools.qi has been ' 'deprecated and will be removed in the future. Instead use ' 'the function in qiskit.quantum info.random' , Deprecation Warning ) return random . random unitary ( dim , seed ) . data", "predictions": ["gets a random deterministic ."], "references": ["deprecated in 0 . 8 +"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 784, "code": "def random density matrix ( length , rank = None , method = 'Hilbert-Schmidt' , seed = None ) : warnings . warn ( 'The random density matrix() function in qiskit.tools.qi has been ' 'deprecated and will be removed in the future. Instead use ' 'the function in qiskit.quantum info.random' , Deprecation Warning ) return random . random density matrix ( length , rank , method , seed )", "predictions": ["creates a random density of the given length ."], "references": ["deprecated in 0 . 8 +"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 785, "code": "def u3 ( self , theta , phi , lam , q ) : return self . append ( U3Gate ( theta , phi , lam ) , [ q ] , [ ] )", "predictions": ["a method that returns a new instance of this class . the first occurrence of the loop will be filled by this class ."], "references": ["apply u3 to q ."], "bleu": 0.050661968099322066, "rouge_l": 0.07820512820512819}
{"id": 786, "code": "def check type ( self , value , attr , data ) : root value = super ( Instruction Parameter , self ) . check type ( value , attr , data ) if is collection ( value ) : = [ super ( Instruction Parameter , self ) . check type ( item , attr , data ) for item in value ] return root value", "predictions": ["checks permissions for . ."], "references": ["customize check_type for handling containers ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 787, "code": "def check range ( self , j ) : if isinstance ( j , int ) : if j < 0 or j >= self . size : raise Qiskit Index Error ( \"register index out of range\" ) elif isinstance ( j , slice ) : if j . start < 0 or j . stop >= self . size or ( j . step is not None and j . step <= 0 ) : raise Qiskit Index Error ( \"register index slice out of range\" )", "predictions": ["check that we can stop our range of the range ."], "references": ["check that j is a valid index into self ."], "bleu": 0.16108992769687397, "rouge_l": 0.28818897637795277}
{"id": 788, "code": "def to string ( self , indent ) : ind = indent * ' ' if self . root : print ( ind , self . type , '---' , self . root ) else : print ( ind , self . type ) indent = indent + 3 ind = indent * ' ' for children in self . children : if children is None : print ( \"OOPS! type of parent is\" , type ( self ) ) print ( self . children ) if isinstance ( children , str ) : print ( ind , children ) elif isinstance ( children , int ) : print ( ind , str ( children ) ) elif isinstance ( children , float ) : print ( ind , str ( children ) ) else : children . to string ( indent )", "predictions": ["converts this string representation of this class ."], "references": ["print with indent ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 789, "code": "def is square matrix ( mat ) : mat = np . array ( mat ) if mat . ndim != 2 : return False shape = mat . shape return shape [ 0 ] == shape [ 1 ]", "predictions": ["return if the matrix is square of the square ."], "references": ["test if an array is a square matrix ."], "bleu": 0.16590387014219712, "rouge_l": 0.42508710801393734}
{"id": 790, "code": "def is diagonal matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol )", "predictions": ["determines if two nonce are equal ."], "references": ["test if an array is a diagonal matrix"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 791, "code": "def is symmetric matrix ( op , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( op ) if mat . ndim != 2 : return False return np . allclose ( mat , mat . T , rtol = rtol , atol = atol )", "predictions": ["check if two expression: states are equal ."], "references": ["test if an array is a symmetrix matrix"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 792, "code": "def is hermitian matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol )", "predictions": ["if 7 is different , return false otherwise ."], "references": ["test if an array is a hermitian matrix"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 793, "code": "def is positive semidefinite matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT if not is hermitian matrix ( mat , rtol = rtol , atol = atol ) : return False vals = np . linalg . eigvalsh ( mat ) for v in vals : if v < - atol : return False return True", "predictions": ["check if values contain positive values ."], "references": ["test if a matrix is positive semidefinite"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 794, "code": "def is identity matrix ( mat , ignore phase = False , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False if ignore phase : theta = np . angle ( mat [ 0 , 0 ] ) mat = np . exp ( - 1j * theta ) * mat iden = np . eye ( len ( mat ) ) return np . allclose ( mat , iden , rtol = rtol , atol = atol )", "predictions": ["check if two expression: vectors are equal ."], "references": ["test if an array is an identity matrix ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 795, "code": "def is unitary matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) mat = np . conj ( mat . T ) . dot ( mat ) return is identity matrix ( mat , ignore phase = False , rtol = rtol , atol = atol )", "predictions": ["determine if an input is a unitary matrix . if the values are not a good ignore the values of the values , the values are compared to the other matrix ."], "references": ["test if an array is a unitary matrix ."], "bleu": 0.15562125171333166, "rouge_l": 0.3798932384341636}
{"id": 796, "code": "def run ( self , dag ) : swaps = dag . op nodes ( Swap Gate ) for swap in swaps : final successor = [ ] for successor in dag . successors ( swap ) : final successor . append ( successor . type == 'out' or ( successor . type == 'op' and successor . op . name == 'measure' ) ) if all ( final successor ) : swap qargs = swap . qargs measure layer = DAG Circuit ( ) for qreg in dag . qregs . values ( ) : measure layer . add qreg ( qreg ) for creg in dag . cregs . values ( ) : measure layer . add creg ( creg ) for successor in dag . successors ( swap ) : if successor . type == 'op' and successor . op . name == 'measure' : dag . remove op node ( successor ) old measure qarg = successor . qargs [ 0 ] new measure qarg = swap qargs [ swap qargs . index ( old measure qarg ) - 1 ] measure layer . apply operation back ( Measure ( ) , [ new measure qarg ] , [ successor . cargs [ 0 ] ] ) dag . extend back ( measure layer ) dag . remove op node ( swap ) return dag", "predictions": ["runs the iterates between two consecutive ( ."], "references": ["return a new circuit that has been optimized ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 797, "code": "def to choi ( rep , data , input dim , output dim ) : if rep == 'Choi' : return data if rep == 'Operator' : return from operator ( 'Choi' , data , input dim , output dim ) if rep == 'Super Op' : return superop to choi ( data , input dim , output dim ) if rep == 'Kraus' : return kraus to choi ( data , input dim , output dim ) if rep == 'Chi' : return chi to choi ( data , input dim , output dim ) if rep == 'PTM' : data = ptm to superop ( data , input dim , output dim ) return superop to choi ( data , input dim , output dim ) if rep == 'Stinespring' : return stinespring to choi ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )", "predictions": ["convert an input to a signed user ."], "references": ["transform a quantumchannel to the choi representation ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 798, "code": "def to superop ( rep , data , input dim , output dim ) : if rep == 'Super Op' : return data if rep == 'Operator' : return from operator ( 'Super Op' , data , input dim , output dim ) if rep == 'Choi' : return choi to superop ( data , input dim , output dim ) if rep == 'Kraus' : return kraus to superop ( data , input dim , output dim ) if rep == 'Chi' : data = chi to choi ( data , input dim , output dim ) return choi to superop ( data , input dim , output dim ) if rep == 'PTM' : return ptm to superop ( data , input dim , output dim ) if rep == 'Stinespring' : return stinespring to superop ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )", "predictions": ["convert an input to a more user ."], "references": ["transform a quantumchannel to the superop representation ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 799, "code": "def to kraus ( rep , data , input dim , output dim ) : if rep == 'Kraus' : return data if rep == 'Stinespring' : return stinespring to kraus ( data , input dim , output dim ) if rep == 'Operator' : return from operator ( 'Kraus' , data , input dim , output dim ) if rep != 'Choi' : data = to choi ( rep , data , input dim , output dim ) return choi to kraus ( data , input dim , output dim )", "predictions": ["convert an input to a more closely representation ."], "references": ["transform a quantumchannel to the kraus representation ."], "bleu": 0.21105340631872635, "rouge_l": 0.35672514619883033}
{"id": 800, "code": "def to chi ( rep , data , input dim , output dim ) : if rep == 'Chi' : return data check nqubit dim ( input dim , output dim ) if rep == 'Operator' : return from operator ( 'Chi' , data , input dim , output dim ) if rep != 'Choi' : data = to choi ( rep , data , input dim , output dim ) return choi to chi ( data , input dim , output dim )", "predictions": ["transform an instruction to the 3d gate representation ."], "references": ["transform a quantumchannel to the chi representation ."], "bleu": 0.24446151121745052, "rouge_l": 0.594541910331384}
{"id": 801, "code": "def to ptm ( rep , data , input dim , output dim ) : if rep == 'PTM' : return data check nqubit dim ( input dim , output dim ) if rep == 'Operator' : return from operator ( 'PTM' , data , input dim , output dim ) if rep != 'Super Op' : data = to superop ( rep , data , input dim , output dim ) return superop to ptm ( data , input dim , output dim )", "predictions": ["convert an = = insert part of this user ."], "references": ["transform a quantumchannel to the ptm representation ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 802, "code": "def to stinespring ( rep , data , input dim , output dim ) : if rep == 'Stinespring' : return data if rep == 'Operator' : return from operator ( 'Stinespring' , data , input dim , output dim ) if rep != 'Kraus' : data = to kraus ( rep , data , input dim , output dim ) return kraus to stinespring ( data , input dim , output dim )", "predictions": ["convert an l representation of this plist matrix to the specified r ."], "references": ["transform a quantumchannel to the stinespring representation ."], "bleu": 0.14283632578659286, "rouge_l": 0.2985318107667211}
{"id": 803, "code": "def to operator ( rep , data , input dim , output dim ) : if rep == 'Operator' : return data if rep == 'Stinespring' : return stinespring to operator ( data , input dim , output dim ) if rep != 'Kraus' : data = to kraus ( rep , data , input dim , output dim ) return kraus to operator ( data , input dim , output dim )", "predictions": ["convert an ( l = data = data = data = ( = data = ( = ( = ( = ( . . . . . . . = 1 = ( . ( self . ( self . ( self = 1 = ( . ( self ="], "references": ["transform a quantumchannel to the operator representation ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 804, "code": "def from operator ( rep , data , input dim , output dim ) : if rep == 'Operator' : return data if rep == 'Super Op' : return np . kron ( np . conj ( data ) , data ) if rep == 'Choi' : vec = np . ravel ( data , order = 'F' ) return np . outer ( vec , np . conj ( vec ) ) if rep == 'Kraus' : return ( [ data ] , None ) if rep == 'Stinespring' : return ( data , None ) if rep == 'Chi' : check nqubit dim ( input dim , output dim ) data = from operator ( 'Choi' , data , input dim , output dim ) return choi to chi ( data , input dim , output dim ) if rep == 'PTM' : check nqubit dim ( input dim , output dim ) data = from operator ( 'Super Op' , data , input dim , output dim ) return superop to ptm ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )", "predictions": ["convert the specified ( 3d : 1 . 0 . 05 : 1 . 0 . 05 . 1 . 0 : 1 . 0 : 1 : 1 . 0 . 05 . 25 . 0 : 1 : 1 . 1 . 1 . 1 . 05 ."], "references": ["transform operator representation to other representation ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 805, "code": "def stinespring to operator ( data , input dim , output dim ) : trace dim = data [ 0 ] . shape [ 0 ] // output dim if data [ 1 ] is not None or trace dim != 1 : raise Qiskit Error ( 'Channel cannot be converted to Operator representation' ) return data [ 0 ]", "predictions": ["convert a . coerce nested matrix to a more closely less than the dimension of this matrix ."], "references": ["transform stinespring representation to operator representation ."], "bleu": 0.07535838128770536, "rouge_l": 0.17378917378917377}
{"id": 806, "code": "def superop to choi ( data , input dim , output dim ) : shape = ( output dim , output dim , input dim , input dim ) return reshuffle ( data , shape )", "predictions": ["converts a process image to a more closely representation ."], "references": ["transform superop representation to choi representation ."], "bleu": 0.17827531042796255, "rouge_l": 0.36454183266932266}
{"id": 807, "code": "def choi to superop ( data , input dim , output dim ) : shape = ( input dim , output dim , input dim , output dim ) return reshuffle ( data , shape )", "predictions": ["converts a process image to a more second representation ."], "references": ["transform choi to superop representation ."], "bleu": 0.17827531042796255, "rouge_l": 0.3927038626609442}
{"id": 808, "code": "def kraus to choi ( data , input dim , output dim ) : choi = 0 kraus l , kraus r = data if kraus r is None : for i in kraus l : vec = i . ravel ( order = 'F' ) choi += np . outer ( vec , vec . conj ( ) ) else : for i , j in zip ( kraus l , kraus r ) : choi += np . outer ( i . ravel ( order = 'F' ) , j . ravel ( order = 'F' ) . conj ( ) ) return choi", "predictions": ["convert a given , positive ( rounded to an angle = ln = 1 , 2 , 4 , 3 = 5 , t = 5 , t = 5 , t = 5 , t = 5 , t = 1 , t = 5 , t = 5"], "references": ["transform kraus representation to choi representation ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 809, "code": "def choi to kraus ( data , input dim , output dim , atol = ATOL DEFAULT ) : if is hermitian matrix ( data , atol = atol ) : w , v = la . eigh ( data ) if len ( w [ w < - atol ] ) == 0 : kraus = [ ] for val , vec in zip ( w , v . T ) : if abs ( val ) > atol : k = np . sqrt ( val ) * vec . reshape ( ( output dim , input dim ) , order = 'F' ) kraus . append ( k ) if not kraus : kraus . append ( np . zeros ( ( output dim , input dim ) , dtype = complex ) ) return ( kraus , None ) mat u , svals , mat vh = la . svd ( data ) kraus l = [ ] kraus r = [ ] for val , vec l , vec r in zip ( svals , mat u . T , mat vh . conj ( ) ) : kraus l . append ( np . sqrt ( val ) * vec l . reshape ( ( output dim , input dim ) , order = 'F' ) ) kraus r . append ( np . sqrt ( val ) * vec r . reshape ( ( output dim , input dim ) , order = 'F' ) ) return ( kraus l , kraus r )", "predictions": ["convert a given , ( possibly null [ 0 ] [ j ] [ j ] [ j ] [ ( : 1 ] [ j ] [ j ] [ ( 2 , t ] [ j ] [ ( 1 [ np ] [ np ] [ np"], "references": ["transform choi representation to kraus representation ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 810, "code": "def stinespring to kraus ( data , input dim , output dim ) : kraus pair = [ ] for stine in data : if stine is None : kraus pair . append ( None ) else : trace dim = stine . shape [ 0 ] // output dim iden = np . eye ( output dim ) kraus = [ ] for j in range ( trace dim ) : vec = np . zeros ( trace dim ) vec [ j ] = 1 kraus . append ( np . kron ( iden , vec [ None , : ] ) . dot ( stine ) ) kraus pair . append ( kraus ) return tuple ( kraus pair )", "predictions": ["transform a stinespring into a self - bit tuple ."], "references": ["transform stinespring representation to kraus representation ."], "bleu": 0.14991106946711685, "rouge_l": 0.36454183266932266}
{"id": 811, "code": "def stinespring to choi ( data , input dim , output dim ) : trace dim = data [ 0 ] . shape [ 0 ] // output dim stine l = np . reshape ( data [ 0 ] , ( output dim , trace dim , input dim ) ) if data [ 1 ] is None : stine r = stine l else : stine r = np . reshape ( data [ 1 ] , ( output dim , trace dim , input dim ) ) return np . reshape ( np . einsum ( 'i Aj,k Al->jilk' , stine l , stine r . conj ( ) ) , 2 * [ input dim * output dim ] )", "predictions": ["converts an ) to a ( possibly null dimensions ."], "references": ["transform stinespring representation to choi representation ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 812, "code": "def kraus to stinespring ( data , input dim , output dim ) : stine pair = [ None , None ] for i , kraus in enumerate ( data ) : if kraus is not None : num kraus = len ( kraus ) stine = np . zeros ( ( output dim * num kraus , input dim ) , dtype = complex ) for j , mat in enumerate ( kraus ) : vec = np . zeros ( num kraus ) vec [ j ] = 1 stine += np . kron ( mat , vec [ : , None ] ) stine pair [ i ] = stine return tuple ( stine pair )", "predictions": ["transform a set of except for this set of except dimension ."], "references": ["transform kraus representation to stinespring representation ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 813, "code": "def kraus to superop ( data , input dim , output dim ) : kraus l , kraus r = data superop = 0 if kraus r is None : for i in kraus l : superop += np . kron ( np . conj ( i ) , i ) else : for i , j in zip ( kraus l , kraus r ) : superop += np . kron ( np . conj ( j ) , i ) return superop", "predictions": ["converts an : . to a init or rounded phase ."], "references": ["transform kraus representation to superop representation ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 814, "code": "def chi to choi ( data , input dim , output dim ) : num qubits = int ( np . log2 ( input dim ) ) return transform from pauli ( data , num qubits )", "predictions": ["transform from q to for decoding ."], "references": ["transform chi representation to a choi representation ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 815, "code": "def choi to chi ( data , input dim , output dim ) : num qubits = int ( np . log2 ( input dim ) ) return transform to pauli ( data , num qubits )", "predictions": ["transform from dim dim to random matrix ."], "references": ["transform choi representation to the chi representation ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 816, "code": "def reravel ( mat1 , mat2 , shape1 , shape2 ) : left dims = shape1 [ : 2 ] + shape2 [ : 2 ] right dims = shape1 [ 2 : ] + shape2 [ 2 : ] tensor shape = left dims + right dims final shape = ( np . product ( left dims ) , np . product ( right dims ) ) data = np . kron ( mat1 , mat2 ) data = np . reshape ( np . transpose ( np . reshape ( data , tensor shape ) , ( 0 , 2 , 1 , 3 , 4 , 6 , 5 , 7 ) ) , final shape ) return data", "predictions": ["loop through the random data from the length of this set ."], "references": ["reravel two bipartite matrices ."], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 817, "code": "def transform from pauli ( data , num qubits ) : basis mat = np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 1 , 1j , 0 ] , [ 0 , 1 , - 1j , 0 ] , [ 1 , 0j , 0 , - 1 ] ] , dtype = complex ) cob = basis mat for in range ( num qubits - 1 ) : dim = int ( np . sqrt ( len ( cob ) ) ) cob = np . reshape ( np . transpose ( np . reshape ( np . kron ( basis mat , cob ) , ( 2 , 2 , dim , dim , 4 , dim * dim ) ) , ( 0 , 2 , 1 , 3 , 4 , 5 ) ) , ( 4 * dim * dim , 4 * dim * dim ) ) return np . dot ( np . dot ( cob , data ) , cob . conj ( ) . T ) / 2 ** num qubits", "predictions": ["make a self - 3d transformation from the given theta ."], "references": ["change of basis of bipartite matrix represenation ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 818, "code": "def check nqubit dim ( input dim , output dim ) : if input dim != output dim : raise Qiskit Error ( 'Not an n-qubit channel: input dim' + ' ({}) != output dim ({})' . format ( input dim , output dim ) ) num qubits = int ( np . log2 ( input dim ) ) if 2 ** num qubits != input dim : raise Qiskit Error ( 'Not an n-qubit channel: input dim != 2 ** n' )", "predictions": ["helper to validate dimension dimension column for dimension dimension ."], "references": ["return true if dims correspond to an n - qubit channel ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 819, "code": "def hide tick lines and labels ( axis ) : for item in axis . get ticklines ( ) + axis . get ticklabels ( ) : item . set visible ( False )", "predictions": ["check if the operation should be shown on the operation ."], "references": ["set visible property of ticklines and ticklabels of an axis to false"], "bleu": 0.08746102712394917, "rouge_l": 0.0}
{"id": 820, "code": "def clear ( self ) : self . points = [ ] self . vectors = [ ] self . point style = [ ] self . annotations = [ ]", "predictions": ["clears the object as a series ."], "references": ["resets bloch sphere data sets to empty ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 821, "code": "def render ( self , title = '' ) : if self . rendered : self . axes . clear ( ) self . rendered = True if not self . ext fig : self . fig = plt . figure ( figsize = self . figsize ) if not self . ext axes : self . axes = Axes3D ( self . fig , azim = self . view [ 0 ] , elev = self . view [ 1 ] ) if self . background : self . axes . clear ( ) self . axes . set xlim3d ( - 1.3 , 1.3 ) self . axes . set ylim3d ( - 1.3 , 1.3 ) self . axes . set zlim3d ( - 1.3 , 1.3 ) else : self . plot axes ( ) self . axes . set axis off ( ) self . axes . set xlim3d ( - 0.7 , 0.7 ) self . axes . set ylim3d ( - 0.7 , 0.7 ) self . axes . set zlim3d ( - 0.7 , 0.7 ) self . axes . grid ( False ) self . plot back ( ) self . plot points ( ) self . plot vectors ( ) self . plot front ( ) self . plot axes labels ( ) self . plot annotations ( ) self . axes . set title ( title , fontsize = self . font size , y = 1.08 )", "predictions": ["makes a if you want to call this method to is called when you want to call this method ."], "references": ["render the bloch sphere and its data sets in on given figure and axes ."], "bleu": 0.06108557268562171, "rouge_l": 0.058653846153846154}
{"id": 822, "code": "def plot front ( self ) : u angle = np . linspace ( - np . pi , 0 , 25 ) v angle = np . linspace ( 0 , np . pi , 25 ) x dir = np . outer ( np . cos ( u angle ) , np . sin ( v angle ) ) y dir = np . outer ( np . sin ( u angle ) , np . sin ( v angle ) ) z dir = np . outer ( np . ones ( u angle . shape [ 0 ] ) , np . cos ( v angle ) ) self . axes . plot surface ( x dir , y dir , z dir , rstride = 2 , cstride = 2 , color = self . sphere color , linewidth = 0 , alpha = self . sphere alpha ) self . axes . plot wireframe ( x dir , y dir , z dir , rstride = 5 , cstride = 5 , color = self . frame color , alpha = self . frame alpha ) self . axes . plot ( 1.0 * np . cos ( u angle ) , 1.0 * np . sin ( u angle ) , zs = 0 , zdir = 'z' , lw = self . frame width , color = self . frame color ) self . axes . plot ( 1.0 * np . cos ( u angle ) , 1.0 * np . sin ( u angle ) , zs = 0 , zdir = 'x' , lw = self . frame width , color = self . frame color )", "predictions": ["draw and return the original diagonal ."], "references": ["front half of sphere"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 823, "code": "def show ( self , title = '' ) : self . render ( title = title ) if self . fig : plt . show ( self . fig )", "predictions": ["displays the validity of the request ."], "references": ["display bloch sphere and corresponding data sets ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 824, "code": "def two qubit kak ( unitary matrix , verify gate sequence = False ) : warnings . warn ( \"two qubit kak function is now accessible under \" \"qiskit.quantum info.synthesis\" , Deprecation Warning ) return synthesis . two qubit kak ( unitary matrix )", "predictions": ["is this a is a is a is a is a is a is resolved with a ."], "references": ["deprecated after 0 . 8"], "bleu": 0.06809398432036522, "rouge_l": 0.09682539682539681}
{"id": 825, "code": "def top ( self ) : ret = self . top format % self . top connect . center ( self . width , self . top pad ) if self . right fill : ret = ret . ljust ( self . right fill , self . top pad ) if self . left fill : ret = ret . rjust ( self . left fill , self . top pad ) ret = ret . center ( self . layer width , self . top bck ) return ret", "predictions": ["returns a is stored in a raster or other objects ."], "references": ["constructs the top line of the element"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 826, "code": "def mid ( self ) : ret = self . mid format % self . mid content . center ( self . width , self . mid padding ) if self . right fill : ret = ret . ljust ( self . right fill , self . mid padding ) if self . left fill : ret = ret . rjust ( self . left fill , self . mid padding ) ret = ret . center ( self . layer width , self . mid bck ) return ret", "predictions": ["creates and returns a array with its contents ."], "references": ["constructs the middle line of the element"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 827, "code": "def bot ( self ) : ret = self . bot format % self . bot connect . center ( self . width , self . bot pad ) if self . right fill : ret = ret . ljust ( self . right fill , self . bot pad ) if self . left fill : ret = ret . rjust ( self . left fill , self . bot pad ) ret = ret . center ( self . layer width , self . bot bck ) return ret", "predictions": ["this will be called from the start of the background thread ."], "references": ["constructs the bottom line of the element"], "bleu": 0.14694106251955755, "rouge_l": 0.3315217391304348}
{"id": 828, "code": "def length ( self ) : return max ( len ( self . top ) , len ( self . mid ) , len ( self . bot ) )", "predictions": ["get the run of this cipher ."], "references": ["returns the length of the element including the box around ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 829, "code": "def label for box ( instruction ) : label = instruction . name . capitalize ( ) params = Text Drawing . params for label ( instruction ) if params : label += \"(%s)\" % ',' . join ( params ) return label", "predictions": ["get the text of the view"], "references": ["creates the label for a box ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 830, "code": "def latex ( self , prec = 15 , nested scope = None ) : if not nested scope : return \"\\textrm{\" + self . name + \"}\" else : if self . name not in nested scope [ - 1 ] : raise Node Exception ( \"Expected local parameter name: \" , \"name=%s, \" % self . name , \"line=%s, \" % self . line , \"file=%s\" % self . file ) else : return nested scope [ - 1 ] [ self . name ] . latex ( prec , nested scope [ 0 : - 1 ] )", "predictions": ["calculate the to be a dim . the to generate the to the to be a dim operation ."], "references": ["return the correspond math mode latex string ."], "bleu": 0.0712695567709093, "rouge_l": 0.1598951507208388}
{"id": 831, "code": "def sym ( self , nested scope = None ) : if not nested scope or self . name not in nested scope [ - 1 ] : raise Node Exception ( \"Expected local parameter name: \" , \"name=%s, line=%s, file=%s\" % ( self . name , self . line , self . file ) ) else : return nested scope [ - 1 ] [ self . name ] . sym ( nested scope [ 0 : - 1 ] )", "predictions": ["returns a , with a , , , , , , with each data removed from the , , using a , and , and the , i . e . a , a , a , and a , which is a , so that the , the ,"], "references": ["return the correspond symbolic number ."], "bleu": 0.026594139297659906, "rouge_l": 0.08321964529331514}
{"id": 832, "code": "def real ( self , nested scope = None ) : if not nested scope or self . name not in nested scope [ - 1 ] : raise Node Exception ( \"Expected local parameter name: \" , \"name=%s, line=%s, file=%s\" % ( self . name , self . line , self . file ) ) else : return nested scope [ - 1 ] [ self . name ] . real ( nested scope [ 0 : - 1 ] )", "predictions": ["call this method to get the real scope of this nested init ."], "references": ["return the correspond floating point number ."], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 833, "code": "def attach ( self , instruction , qargs , cargs ) : self . append ( instruction , qargs , cargs )", "predictions": ["attaches the specified instruction to this object ."], "references": ["deprecated after 0 . 8"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 834, "code": "def check dups ( self , qubits ) : squbits = set ( qubits ) if len ( squbits ) != len ( qubits ) : raise Qiskit Error ( \"duplicate qubit arguments\" )", "predictions": ["perform a check on the , ."], "references": ["raise exception if list of qubits contains duplicates ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 835, "code": "def check qargs ( self , qargs ) : if not all ( isinstance ( i , tuple ) and isinstance ( i [ 0 ] , Quantum Register ) and isinstance ( i [ 1 ] , int ) for i in qargs ) : raise Qiskit Error ( \"qarg not (Quantum Register, int) tuple\" ) if not all ( self . has register ( i [ 0 ] ) for i in qargs ) : raise Qiskit Error ( \"register not in this circuit\" ) for qubit in qargs : qubit [ 0 ] . check range ( qubit [ 1 ] )", "predictions": ["checks that the arguments of a register are ( ."], "references": ["raise exception if a qarg is not in this circuit or bad format ."], "bleu": 0.09351498865776114, "rouge_l": 0.16180371352785147}
{"id": 836, "code": "def check cargs ( self , cargs ) : if not all ( isinstance ( i , tuple ) and isinstance ( i [ 0 ] , Classical Register ) and isinstance ( i [ 1 ] , int ) for i in cargs ) : raise Qiskit Error ( \"carg not (Classical Register, int) tuple\" ) if not all ( self . has register ( i [ 0 ] ) for i in cargs ) : raise Qiskit Error ( \"register not in this circuit\" ) for clbit in cargs : clbit [ 0 ] . check range ( clbit [ 1 ] )", "predictions": ["checks for values that are ( ."], "references": ["raise exception if clbit is not in this circuit or bad format ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 837, "code": "def check compatible regs ( self , rhs ) : list1 = self . qregs + self . cregs list2 = rhs . qregs + rhs . cregs for element1 in list1 : for element2 in list2 : if element2 . name == element1 . name : if element1 != element2 : raise Qiskit Error ( \"circuits are not compatible\" )", "predictions": ["running the fully qualified tools ."], "references": ["raise exception if the circuits are defined on incompatible registers"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 838, "code": "def qasm ( self ) : string temp = self . header + \"\\n\" string temp += self . extension lib + \"\\n\" for register in self . qregs : string temp += register . qasm ( ) + \"\\n\" for register in self . cregs : string temp += register . qasm ( ) + \"\\n\" for instruction , qargs , cargs in self . data : if instruction . name == 'measure' : qubit = qargs [ 0 ] clbit = cargs [ 0 ] string temp += \"%s %s[%d] -> %s[%d];\\n\" % ( instruction . qasm ( ) , qubit [ 0 ] . name , qubit [ 1 ] , clbit [ 0 ] . name , clbit [ 1 ] ) else : string temp += \"%s %s;\\n\" % ( instruction . qasm ( ) , \",\" . join ( [ \"%s[%d]\" % ( j [ 0 ] . name , j [ 1 ] ) for j in qargs + cargs ] ) ) return string temp", "predictions": ["this method creates a string representation of the qasm ."], "references": ["return openqasm string ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 839, "code": "def bind parameter ( self , parameter , value ) : for ( instr , param index ) in self . parameter table [ parameter ] : instr . params [ param index ] = value", "predictions": ["bind a specified parameter to the first / second ."], "references": ["assigns a parameter value to matching instructions in - place ."], "bleu": 0.1434272783816789, "rouge_l": 0.37770897832817335}
{"id": 840, "code": "def score step ( step ) : return len ( [ g for g in step [ 'gates mapped' ] if len ( g . qargs ) == 2 ] ) - 3 * step [ 'swaps added' ]", "predictions": ["returns the score of the given step ."], "references": ["count the mapped two - qubit gates less the number of added swaps ."], "bleu": 0.09525245831601728, "rouge_l": 0.25994318181818177}
{"id": 841, "code": "def transform gate for layout ( gate , layout ) : mapped op node = deepcopy ( [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ 0 ] ) device qreg = Quantum Register ( len ( layout . get physical bits ( ) ) , 'q' ) mapped qargs = [ ( device qreg , layout [ a ] ) for a in mapped op node . qargs ] mapped op node . qargs = mapped op node . op . qargs = mapped qargs mapped op node . pop ( 'name' ) return mapped op node", "predictions": ["transforms the gate array and fill all other nodes with their required values ."], "references": ["return op implementing a virtual gate on given layout ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 842, "code": "def swap ops from edge ( edge , layout ) : device qreg = Quantum Register ( len ( layout . get physical bits ( ) ) , 'q' ) qreg edge = [ ( device qreg , i ) for i in edge ] return [ DAG Node ( { 'op' : Swap Gate ( ) , 'qargs' : qreg edge , 'cargs' : [ ] , 'type' : 'op' } ) ]", "predictions": ["fill the supplied edge with the specified edge ."], "references": ["generate list of ops to implement a swap gate along a coupling edge ."], "bleu": 0.10657503067399117, "rouge_l": 0.1673525377229081}
{"id": 843, "code": "def physical qubits ( self ) : if self . qubit list is None : self . qubit list = sorted ( [ pqubit for pqubit in self . graph . nodes ] ) return self . qubit list", "predictions": ["returns a physical physical process ."], "references": ["returns a sorted list of physical_qubits"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 844, "code": "def cu1 ( self , theta , ctl , tgt ) : return self . append ( Cu1Gate ( theta ) , [ ctl , tgt ] , [ ] )", "predictions": ["returns a cu1 at the end of this class ."], "references": ["apply cu1 from ctl to tgt with angle theta ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 845, "code": "def inverse ( self ) : for index , instruction in enumerate ( self . instructions ) : self . instructions [ index ] = instruction . inverse ( ) return self", "predictions": ["inverse of this method ."], "references": ["invert all instructions ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 846, "code": "def q if ( self , * qregs ) : for gate in self . instructions : gate . q if ( * qregs ) return self", "predictions": ["a method to get a gate ."], "references": ["add controls to all instructions ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 847, "code": "def c if ( self , classical , val ) : for gate in self . instructions : gate . c if ( classical , val ) return self", "predictions": ["get a gate by the specified element ."], "references": ["add classical control register to all instructions ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 848, "code": "def initialize ( self , params , qubits ) : if isinstance ( qubits , Quantum Register ) : qubits = qubits [ : ] else : qubits = convert to bits ( [ qubits ] , [ qbit for qreg in self . qregs for qbit in qreg ] ) [ 0 ] return self . append ( Initialize ( params ) , qubits )", "predictions": ["initializes and prepares the register parameters ."], "references": ["apply initialize to circuit ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 849, "code": "def is virtual ( value ) : return value is None or isinstance ( value , tuple ) and len ( value ) == 2 and isinstance ( value [ 0 ] , Register ) and isinstance ( value [ 1 ] , int )", "predictions": ["check if the provided value is a , ."], "references": ["checks if value has the format of a virtual qubit"], "bleu": 0.15881076016027915, "rouge_l": 0.31282051282051276}
{"id": 850, "code": "def copy ( self ) : layout copy = type ( self ) ( ) layout copy . p2v = self . p2v . copy ( ) layout copy . v2p = self . v2p . copy ( ) return layout copy", "predictions": ["copies this object to a copy of this object ."], "references": ["returns a copy of a layout instance ."], "bleu": 0.24808415001701817, "rouge_l": 0.4535315985130111}
{"id": 851, "code": "def ccx ( self , ctl1 , ctl2 , tgt ) : return self . append ( Toffoli Gate ( ) , [ ctl1 , ctl2 , tgt ] , [ ] )", "predictions": ["a ccx method for creating an object in a j_spaces way ."], "references": ["apply toffoli to from ctl1 and ctl2 to tgt ."], "bleu": 0.10390302174233558, "rouge_l": 0.09242424242424242}
{"id": 852, "code": "def u2 ( self , phi , lam , q ) : return self . append ( U2Gate ( phi , lam ) , [ q ] , [ ] )", "predictions": ["a method to prevent effect of this class ."], "references": ["apply u2 to q ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 853, "code": "def to matrix ( self ) : isqrt2 = 1 / numpy . sqrt ( 2 ) phi , lam = self . params phi , lam = float ( phi ) , float ( lam ) return numpy . array ( [ [ isqrt2 , - numpy . exp ( 1j * lam ) * isqrt2 ] , [ numpy . exp ( 1j * phi ) * isqrt2 , numpy . exp ( 1j * ( phi + lam ) ) * isqrt2 ] ] , dtype = complex )", "predictions": ["convert this quaternion to a matrix"], "references": ["return a numpy . array for the u3 gate ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 854, "code": "def is cptp ( self , atol = None , rtol = None ) : if atol is None : atol = self . atol if rtol is None : rtol = self . rtol if self . data [ 1 ] is not None : return False check = np . dot ( np . transpose ( np . conj ( self . data [ 0 ] ) ) , self . data [ 0 ] ) return is identity matrix ( check , rtol = self . rtol , atol = self . atol )", "predictions": ["compute the cptp of this octagon and another ."], "references": ["return true if completely - positive trace - preserving ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 855, "code": "def conjugate ( self ) : stine l = np . conjugate ( self . data [ 0 ] ) stine r = None if self . data [ 1 ] is not None : stine r = np . conjugate ( self . data [ 1 ] ) return Stinespring ( ( stine l , stine r ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["compute the conjugate of this object ."], "references": ["return the conjugate of the quantumchannel ."], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 856, "code": "def transpose ( self ) : din , dout = self . dim dtr = self . data [ 0 ] . shape [ 0 ] // dout stine = [ None , None ] for i , mat in enumerate ( self . data ) : if mat is not None : stine [ i ] = np . reshape ( np . transpose ( np . reshape ( mat , ( dout , dtr , din ) ) , ( 2 , 1 , 0 ) ) , ( din * dtr , dout ) ) return Stinespring ( tuple ( stine ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )", "predictions": [". for each observation in this closeable ."], "references": ["return the transpose of the quantumchannel ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 857, "code": "def to operator ( self ) : from qiskit . quantum info . operators . operator import Operator return Operator ( self . to matrix ( ) )", "predictions": ["returns a operator for this var2 ."], "references": ["convert to operator object ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 858, "code": "def to instruction ( self ) : from qiskit . circuit import Quantum Circuit , Quantum Register from qiskit . extensions . standard import Id Gate , X Gate , Y Gate , Z Gate gates = { 'I' : Id Gate ( ) , 'X' : X Gate ( ) , 'Y' : Y Gate ( ) , 'Z' : Z Gate ( ) } label = self . to label ( ) n qubits = self . numberofqubits qreg = Quantum Register ( n qubits ) circuit = Quantum Circuit ( qreg , name = 'Pauli:{}' . format ( label ) ) for i , pauli in enumerate ( reversed ( label ) ) : circuit . append ( gates [ pauli ] , [ qreg [ i ] ] ) return circuit . to instruction ( )", "predictions": ["convert an instruction at this location and its bounds into a label ."], "references": ["convert to pauli circuit instruction ."], "bleu": 0.1135935489027116, "rouge_l": 0.3382624768946396}
{"id": 859, "code": "def validate initial statevector ( self ) : if self . initial statevector is None : return length = len ( self . initial statevector ) required dim = 2 ** self . number of qubits if length != required dim : raise Basic Aer Error ( 'initial statevector is incorrect length: ' + '{} != {}' . format ( length , required dim ) )", "predictions": ["this is used to ensure the initial color of the initial color is a valid initial number of digits . this is used to ensure that the initial color of the initial color of the underlying color is always a y bit ."], "references": ["validate an initial statevector"], "bleu": 0.027991033184405684, "rouge_l": 0.05004101722723544}
{"id": 860, "code": "def set options ( self , qobj config = None , backend options = None ) : self . initial statevector = self . DEFAULT OPTIONS [ \"initial statevector\" ] self . chop threshold = self . DEFAULT OPTIONS [ \"chop threshold\" ] if backend options is None : backend options = { } if 'initial statevector' in backend options : self . initial statevector = np . array ( backend options [ 'initial statevector' ] , dtype = complex ) elif hasattr ( qobj config , 'initial statevector' ) : self . initial statevector = np . array ( qobj config . initial statevector , dtype = complex ) if self . initial statevector is not None : norm = np . linalg . norm ( self . initial statevector ) if round ( norm , 12 ) != 1 : raise Basic Aer Error ( 'initial statevector is not normalized: ' + 'norm {} != 1' . format ( norm ) ) if 'chop threshold' in backend options : self . chop threshold = backend options [ 'chop threshold' ] elif hasattr ( qobj config , 'chop threshold' ) : self . chop threshold = qobj config . chop threshold", "predictions": ["sets the options for the given configuration ."], "references": ["set the backend options for all experiments in a qobj"], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 861, "code": "def initialize statevector ( self ) : if self . initial statevector is None : self . statevector = np . zeros ( 2 ** self . number of qubits , dtype = complex ) self . statevector [ 0 ] = 1 else : self . statevector = self . initial statevector . copy ( ) self . statevector = np . reshape ( self . statevector , self . number of qubits * [ 2 ] )", "predictions": ["initializes and returns a statevector for this plot ."], "references": ["set the initial statevector for simulation"], "bleu": 0.18575057999133596, "rouge_l": 0.27664399092970515}
{"id": 862, "code": "def get statevector ( self ) : vec = np . reshape ( self . statevector , 2 ** self . number of qubits ) vec = np . stack ( [ vec . real , vec . imag ] , axis = 1 ) vec [ abs ( vec ) < self . chop threshold ] = 0.0 return vec", "predictions": ["get here for this document ."], "references": ["return the current statevector in json result spec format"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 863, "code": "def validate ( self , qobj ) : n qubits = qobj . config . n qubits max qubits = self . configuration ( ) . n qubits if n qubits > max qubits : raise Basic Aer Error ( 'Number of qubits {} ' . format ( n qubits ) + 'is greater than maximum ({}) ' . format ( max qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) for experiment in qobj . experiments : name = experiment . header . name if experiment . config . memory slots == 0 : logger . warning ( 'No classical registers in circuit \"%s\", ' 'counts will be empty.' , name ) elif 'measure' not in [ op . name for op in experiment . instructions ] : logger . warning ( 'No measurements in circuit \"%s\", ' 'classical register will remain all zeros.' , name )", "predictions": ["validates all experiment in the experiment ."], "references": ["semantic validations of the qobj which cannot be done via schemas ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 864, "code": "def validate initial unitary ( self ) : if self . initial unitary is None : return shape = np . shape ( self . initial unitary ) required shape = ( 2 ** self . number of qubits , 2 ** self . number of qubits ) if shape != required shape : raise Basic Aer Error ( 'initial unitary is incorrect shape: ' + '{} != 2 ** {}' . format ( shape , required shape ) )", "predictions": ["validates this permission scaled to ensure that it can be a valid ( i . e . , the or a more complicated than the ( scope scope is a valid ( scope scope scope scope scope scope scope scope scope scope scope ."], "references": ["validate an initial unitary matrix"], "bleu": 0.022996104098636838, "rouge_l": 0.0}
{"id": 865, "code": "def set options ( self , qobj config = None , backend options = None ) : self . initial unitary = self . DEFAULT OPTIONS [ \"initial unitary\" ] self . chop threshold = self . DEFAULT OPTIONS [ \"chop threshold\" ] if backend options is None : backend options = { } if 'initial unitary' in backend options : self . initial unitary = np . array ( backend options [ 'initial unitary' ] , dtype = complex ) elif hasattr ( qobj config , 'initial unitary' ) : self . initial unitary = np . array ( qobj config . initial unitary , dtype = complex ) if self . initial unitary is not None : shape = np . shape ( self . initial unitary ) if len ( shape ) != 2 or shape [ 0 ] != shape [ 1 ] : raise Basic Aer Error ( \"initial unitary is not a square matrix\" ) iden = np . eye ( len ( self . initial unitary ) ) u dagger u = np . dot ( self . initial unitary . T . conj ( ) , self . initial unitary ) norm = np . linalg . norm ( u dagger u - iden ) if round ( norm , 10 ) != 0 : raise Basic Aer Error ( \"initial unitary is not unitary\" ) if 'chop threshold' in backend options : self . chop threshold = backend options [ 'chop threshold' ] elif hasattr ( qobj config , 'chop threshold' ) : self . chop threshold = qobj config . chop threshold", "predictions": ["configure the ( . this must be called on the server thread ."], "references": ["set the backend options for all experiments in a qobj"], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 866, "code": "def initialize unitary ( self ) : self . validate initial unitary ( ) if self . initial unitary is None : self . unitary = np . eye ( 2 ** self . number of qubits , dtype = complex ) else : self . unitary = self . initial unitary . copy ( ) self . unitary = np . reshape ( self . unitary , self . number of qubits * [ 2 , 2 ] )", "predictions": ["initializes the base font ."], "references": ["set the initial unitary for simulation"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 867, "code": "def get unitary ( self ) : unitary = np . reshape ( self . unitary , 2 * [ 2 ** self . number of qubits ] ) unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - 1 ) unitary [ abs ( unitary ) < self . chop threshold ] = 0.0 return unitary", "predictions": ["get the last point of this release"], "references": ["return the current unitary in json result spec format"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 868, "code": "def is bit ( obj ) : if isinstance ( obj , tuple ) and len ( obj ) == 2 : if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and obj [ 1 ] < len ( obj [ 0 ] ) : return True return False", "predictions": ["determines if the passed value is a , ."], "references": ["determine if obj is a bit"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 869, "code": "def to matrix ( self ) : lam = self . params [ 0 ] lam = float ( lam ) return numpy . array ( [ [ 1 , 0 ] , [ 0 , numpy . exp ( 1j * lam ) ] ] , dtype = complex )", "predictions": ["returns a compatible array of edges ."], "references": ["return a numpy . array for the u3 gate ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 870, "code": "def real ( self , nested scope = None ) : op = self . children [ 0 ] . name expr = self . children [ 1 ] dispatch = { 'sin' : sympy . sin , 'cos' : sympy . cos , 'tan' : sympy . tan , 'asin' : sympy . asin , 'acos' : sympy . acos , 'atan' : sympy . atan , 'exp' : sympy . exp , 'ln' : sympy . log , 'sqrt' : sympy . sqrt } if op in dispatch : arg = expr . real ( nested scope ) return dispatch [ op ] ( arg ) else : raise Node Exception ( \"internal error: undefined external\" )", "predictions": ["do a real or : dispatch - dispatch - dispatch - : dispatch - 2 . . . . . . . . . ."], "references": ["return the correspond floating point number ."], "bleu": 0.048589719316429775, "rouge_l": 0.06955530216647662}
{"id": 871, "code": "def rzz ( self , theta , qubit1 , qubit2 ) : return self . append ( RZZ Gate ( theta ) , [ qubit1 , qubit2 ] , [ ] )", "predictions": ["returns a bind whose content is the first occurrence of this object ."], "references": ["apply rzz to circuit ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 872, "code": "def cswap ( self , ctl , tgt1 , tgt2 ) : return self . append ( Fredkin Gate ( ) , [ ctl , tgt1 , tgt2 ] , [ ] )", "predictions": ["creates a new ( of this layout ."], "references": ["apply fredkin to circuit ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 873, "code": "def select best remaining cx ( self ) : candidates = [ ] for gate in self . gate list : chk1 = gate [ 0 ] in self . available hw qubits chk2 = gate [ 1 ] in self . available hw qubits if chk1 and chk2 : candidates . append ( gate ) best reliab = 0 best item = None for item in candidates : if self . gate cost [ item ] > best reliab : best reliab = self . gate cost [ item ] best item = item return best item", "predictions": ["selects the gate of this [ 0 , 1 ] ."], "references": ["select best remaining cnot in the hardware for the next program edge ."], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 874, "code": "def select best remaining qubit ( self , prog qubit ) : reliab store = { } for hw qubit in self . available hw qubits : reliab = 1 for n in self . prog graph . neighbors ( prog qubit ) : if n in self . prog2hw : reliab *= self . swap costs [ self . prog2hw [ n ] ] [ hw qubit ] reliab *= self . readout errors [ hw qubit ] reliab store [ hw qubit ] = reliab max reliab = 0 best hw qubit = None for hw qubit in reliab store : if reliab store [ hw qubit ] > max reliab : max reliab = reliab store [ hw qubit ] best hw qubit = hw qubit return best hw qubit", "predictions": ["for the ops . this method provides a list of ops to be recorded ."], "references": ["select the best remaining hardware qubit for the next program qubit ."], "bleu": 0.11633270842295028, "rouge_l": 0.22676579925650556}
{"id": 875, "code": "def run ( self , dag ) : self . initialize backend prop ( ) num qubits = self . create program graph ( dag ) if num qubits > len ( self . swap graph ) : raise Transpiler Error ( 'Number of qubits greater than device.' ) for end1 , end2 , in sorted ( self . prog graph . edges ( data = True ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : self . pending program edges . append ( ( end1 , end2 ) ) while self . pending program edges : edge = self . select next edge ( ) q1 mapped = edge [ 0 ] in self . prog2hw q2 mapped = edge [ 1 ] in self . prog2hw if ( not q1 mapped ) and ( not q2 mapped ) : best hw edge = self . select best remaining cx ( ) self . prog2hw [ edge [ 0 ] ] = best hw edge [ 0 ] self . prog2hw [ edge [ 1 ] ] = best hw edge [ 1 ] self . available hw qubits . remove ( best hw edge [ 0 ] ) self . available hw qubits . remove ( best hw edge [ 1 ] ) elif not q1 mapped : best hw qubit = self . select best remaining qubit ( edge [ 0 ] ) self . prog2hw [ edge [ 0 ] ] = best hw qubit self . available hw qubits . remove ( best hw qubit ) else : best hw qubit = self . select best remaining qubit ( edge [ 1 ] ) self . prog2hw [ edge [ 1 ] ] = best hw qubit self . available hw qubits . remove ( best hw qubit ) new edges = [ x for x in self . pending program edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] self . pending program edges = new edges for qid in self . qarg to id . values ( ) : if qid not in self . prog2hw : self . prog2hw [ qid ] = self . available hw qubits [ 0 ] self . available hw qubits . remove ( self . prog2hw [ qid ] ) layout = Layout ( ) for q in dag . qubits ( ) : pid = self . qarg to id ( q ) hwid = self . prog2hw [ pid ] layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid self . property set [ 'layout' ] = layout", "predictions": ["this is a simple . that runs the loading process ."], "references": ["main run method for the noise adaptive layout ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 876, "code": "def inverse ( self ) : self . data = [ gate . inverse ( ) for gate in reversed ( self . data ) ] self . inverse flag = not self . inverse flag return self", "predictions": ["get the inverse stats ."], "references": ["invert this gate ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 877, "code": "def q if ( self , * qregs ) : self . data = [ gate . q if ( qregs ) for gate in self . data ] return self", "predictions": ["a method to get a representation of this cipher ."], "references": ["add controls to this gate ."], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 878, "code": "def c if ( self , classical , val ) : self . data = [ gate . c if ( classical , val ) for gate in self . data ] return self", "predictions": ["override for instructions that are returned from this method to another element ."], "references": ["add classical control register ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 879, "code": "def is unitary ( self , atol = None , rtol = None ) : if atol is None : atol = self . atol if rtol is None : rtol = self . rtol return is unitary matrix ( self . data , rtol = rtol , atol = atol )", "predictions": ["compute the root node ."], "references": ["return true if operator is a unitary matrix ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 880, "code": "def conjugate ( self ) : return Operator ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["compute the initialize of this operator ."], "references": ["return the conjugate of the operator ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 881, "code": "def transpose ( self ) : return Operator ( np . transpose ( self . data ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["create a new ( ."], "references": ["return the transpose of the operator ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 882, "code": "def shape ( self ) : return tuple ( reversed ( self . output dims ( ) ) ) + tuple ( reversed ( self . input dims ( ) ) )", "predictions": [". the copy of this latlng into a type that is incremented by the copy of the type ."], "references": ["return the tensor shape of the matrix operator"], "bleu": 0.09107438368292149, "rouge_l": 0.2398427260812582}
{"id": 883, "code": "def format state ( self , state ) : state = np . array ( state ) shape = state . shape ndim = state . ndim if ndim > 2 : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if ndim == 2 : if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if shape [ 1 ] == 1 : state = np . reshape ( state , shape [ 0 ] ) return state", "predictions": ["formats the given ( between two arrays ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ."], "references": ["format input state so it is statevector or density matrix"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 884, "code": "def instruction to operator ( cls , instruction ) : if isinstance ( instruction , Quantum Circuit ) : instruction = instruction . to instruction ( ) op = Operator ( np . eye ( 2 ** instruction . num qubits ) ) op . append instruction ( instruction ) return op", "predictions": ["convert a u2 u2 u2 u2 u2 to a more sophisticated u2 ."], "references": ["convert a quantumcircuit or instruction to an operator ."], "bleu": 0.14283632578659286, "rouge_l": 0.37596302003081655}
{"id": 885, "code": "def append instruction ( self , obj , qargs = None ) : if isinstance ( obj , Instruction ) : mat = None if hasattr ( obj , 'to matrix' ) : try : mat = obj . to matrix ( ) except Qiskit Error : pass if mat is not None : op = self . compose ( mat , qargs = qargs ) self . data = op . data else : if obj . definition is None : raise Qiskit Error ( 'Cannot apply Instruction: {}' . format ( obj . name ) ) for instr , qregs , cregs in obj . definition : if cregs : raise Qiskit Error ( 'Cannot apply instruction with classical registers: {}' . format ( instr . name ) ) new qargs = [ tup [ 1 ] for tup in qregs ] self . append instruction ( instr , qargs = new qargs ) else : raise Qiskit Error ( 'Input is not an instruction.' )", "predictions": ["appends a new list to the existing *obj* ."], "references": ["update the current operator by apply an instruction ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 886, "code": "def real ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) expr = self . children [ 1 ] . real ( nested scope ) return operation ( expr )", "predictions": ["implementation of the , but for simple , i . e . , the , the , and the , this method is meant to be called to is a simple , since it is no longer in the , we can never need to be called ."], "references": ["return the correspond floating point number ."], "bleu": 0.02771450089816765, "rouge_l": 0.0840220385674931}
{"id": 887, "code": "def sym ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) expr = self . children [ 1 ] . sym ( nested scope ) return operation ( expr )", "predictions": ["evaluates the : this method does not include the : : : : : : : : element - - : : : : : : : : / / www . w3 . org / questions / . / . / . / . / . / . /"], "references": ["return the correspond symbolic number ."], "bleu": 0.026594139297659906, "rouge_l": 0.08321964529331514}
{"id": 888, "code": "def separate bitstring ( bitstring , creg sizes ) : substrings = [ ] running index = 0 for , size in reversed ( creg sizes ) : substrings . append ( bitstring [ running index : running index + size ] ) running index += size return ' ' . join ( substrings )", "predictions": ["fill the give state with the given din ."], "references": ["separate a bitstring according to the registers defined in the result header ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 889, "code": "def bit string index ( text ) : n = len ( text ) k = text . count ( \"1\" ) if text . count ( \"0\" ) != n - k : raise Visualization Error ( \"s must be a string of 0 and 1\" ) ones = [ pos for pos , char in enumerate ( text ) if char == \"1\" ] return lex index ( n , k , ones )", "predictions": ["normalize the operator . this method is called after all characters have been added to the end of the first operator ."], "references": ["return the index of a string of 0s and 1s ."], "bleu": 0.06586656967644004, "rouge_l": 0.19344608879492597}
{"id": 890, "code": "def bit string index ( s ) : n = len ( s ) k = s . count ( \"1\" ) if s . count ( \"0\" ) != n - k : raise Visualization Error ( \"s must be a string of 0 and 1\" ) ones = [ pos for pos , char in enumerate ( s ) if char == \"1\" ] return lex index ( n , k , ones )", "predictions": ["returns the position of the given character ."], "references": ["return the index of a string of 0s and 1s ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 891, "code": "def op ( self ) : if 'type' not in self . data dict or self . data dict [ 'type' ] != 'op' : raise Qiskit Error ( \"The node %s is not an op node\" % ( str ( self ) ) ) return self . data dict . get ( 'op' )", "predictions": ["repr() for default inverse operation ."], "references": ["returns the instruction object corresponding to the op for the node else none"], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 892, "code": "def run ( self , dag ) : diagonal 1q gates = ( RZ Gate , Z Gate , T Gate , S Gate , Tdg Gate , Sdg Gate , U1Gate ) diagonal 2q gates = ( Cz Gate , Crz Gate , Cu1Gate , RZZ Gate ) nodes to remove = set ( ) for measure in dag . op nodes ( Measure ) : predecessor = dag . quantum predecessors ( measure ) [ 0 ] if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal 1q gates ) : nodes to remove . add ( predecessor ) if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal 2q gates ) : successors = dag . quantum successors ( predecessor ) if all ( [ s . type == 'op' and isinstance ( s . op , Measure ) for s in successors ] ) : nodes to remove . add ( predecessor ) for node to remove in nodes to remove : dag . remove op node ( node to remove ) return dag", "predictions": ["runs this loop . this method returns the optparse output ."], "references": ["return a new circuit that has been optimized ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 893, "code": "def to string ( self , indent ) : ind = indent * ' ' print ( ind , 'qreg' ) self . children [ 0 ] . to string ( indent + 3 )", "predictions": ["copies this string representation of the current physically ."], "references": ["print the node data with indent ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 894, "code": "def remove all ops named ( self , opname ) : for n in self . named nodes ( opname ) : self . remove op node ( n )", "predictions": ["get all the default values of this class ."], "references": ["remove all operation nodes with the given name ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 895, "code": "def add qreg ( self , qreg ) : if not isinstance ( qreg , Quantum Register ) : raise DAG Circuit Error ( \"not a Quantum Register instance.\" ) if qreg . name in self . qregs : raise DAG Circuit Error ( \"duplicate register %s\" % qreg . name ) self . qregs [ qreg . name ] = qreg for j in range ( qreg . size ) : self . add wire ( ( qreg , j ) )", "predictions": ["adds a register to the map ."], "references": ["add all wires in a quantum register ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 896, "code": "def add creg ( self , creg ) : if not isinstance ( creg , Classical Register ) : raise DAG Circuit Error ( \"not a Classical Register instance.\" ) if creg . name in self . cregs : raise DAG Circuit Error ( \"duplicate register %s\" % creg . name ) self . cregs [ creg . name ] = creg for j in range ( creg . size ) : self . add wire ( ( creg , j ) )", "predictions": ["adds a register or a base class to this register ."], "references": ["add all wires in a classical register ."], "bleu": 0.16108992769687397, "rouge_l": 0.32504440497335696}
{"id": 897, "code": "def extend back ( self , dag , edge map = None ) : edge map = edge map or { } for qreg in dag . qregs . values ( ) : if qreg . name not in self . qregs : self . add qreg ( Quantum Register ( qreg . size , qreg . name ) ) edge map . update ( [ ( qbit , qbit ) for qbit in qreg if qbit not in edge map ] ) for creg in dag . cregs . values ( ) : if creg . name not in self . cregs : self . add creg ( Classical Register ( creg . size , creg . name ) ) edge map . update ( [ ( cbit , cbit ) for cbit in creg if cbit not in edge map ] ) self . compose back ( dag , edge map )", "predictions": ["back a back - level back at the specified dag ."], "references": ["add dag at the end of self using edge_map ."], "bleu": 0.17033186037639278, "rouge_l": 0.28818897637795277}
{"id": 898, "code": "def named nodes ( self , * names ) : named nodes = [ ] for node in self . multi graph . nodes ( ) : if node . type == 'op' and node . op . name in names : named nodes . append ( node ) return named nodes", "predictions": ["getter method for named graph nodes ."], "references": ["get the set of op nodes with the given name ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 899, "code": "def two Q gates ( self ) : two q gates = [ ] for node in self . gate nodes ( ) : if len ( node . qargs ) == 2 : two q gates . append ( node ) return two q gates", "predictions": ["returns the two tensors ."], "references": ["get list of 2 - qubit gates . ignore snapshot barriers and the like ."], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 900, "code": "def predecessors ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling predecessors() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] return self . multi graph . predecessors ( node )", "predictions": ["returns a graph that has been used to implement the given node ."], "references": ["returns list of the predecessors of a node as dagnodes ."], "bleu": 0.12571192676522522, "rouge_l": 0.33841886269070737}
{"id": 901, "code": "def ancestors ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling ancestors() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] return nx . ancestors ( self . multi graph , node )", "predictions": ["returns true if the given node is a 'calling or . ."], "references": ["returns set of the ancestors of a node as dagnodes ."], "bleu": 0.1367440667823257, "rouge_l": 0.3505747126436781}
{"id": 902, "code": "def remove ancestors of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove ancestors of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] anc = nx . ancestors ( self . multi graph , node ) for anc node in anc : if anc node . type == \"op\" : self . remove op node ( anc node )", "predictions": ["removes all edges whose content is a 'calling ."], "references": ["remove all of the ancestor operation nodes of node ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 903, "code": "def remove descendants of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove descendants of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] desc = nx . descendants ( self . multi graph , node ) for desc node in desc : if desc node . type == \"op\" : self . remove op node ( desc node )", "predictions": ["removes all edges from the given node and do a 'calling ."], "references": ["remove all of the descendant operation nodes of node ."], "bleu": 0.13065113298388567, "rouge_l": 0.3696969696969697}
{"id": 904, "code": "def remove nonancestors of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove nonancestors of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] anc = nx . ancestors ( self . multi graph , node ) comp = list ( set ( self . multi graph . nodes ( ) ) - set ( anc ) ) for n in comp : if n . type == \"op\" : self . remove op node ( n )", "predictions": ["removes all edges from this node and removes them from the given node"], "references": ["remove all of the non - ancestors operation nodes of node ."], "bleu": 0.1135935489027116, "rouge_l": 0.2417437252311757}
{"id": 905, "code": "def remove nondescendants of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove nondescendants of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] dec = nx . descendants ( self . multi graph , node ) comp = list ( set ( self . multi graph . nodes ( ) ) - set ( dec ) ) for n in comp : if n . type == \"op\" : self . remove op node ( n )", "predictions": ["removes all edges from this node and removes them from the given node"], "references": ["remove all of the non - descendants operation nodes of node ."], "bleu": 0.1135935489027116, "rouge_l": 0.2417437252311757}
{"id": 906, "code": "def multigraph layers ( self ) : predecessor count = dict ( ) cur layer = [ node for node in self . input map . values ( ) ] yield cur layer next layer = [ ] while cur layer : for node in cur layer : for successor in self . multi graph . successors ( node ) : multiplicity = self . multi graph . number of edges ( node , successor ) if successor in predecessor count : predecessor count [ successor ] -= multiplicity else : predecessor count [ successor ] = self . multi graph . in degree ( successor ) - multiplicity if predecessor count [ successor ] == 0 : next layer . append ( successor ) del predecessor count [ successor ] yield next layer cur layer = next layer next layer = [ ]", "predictions": ["to construct the layers of the graph ."], "references": ["yield layers of the multigraph ."], "bleu": 0.3155984539112945, "rouge_l": 0.5865384615384615}
{"id": 907, "code": "def properties ( self ) : summary = { \"size\" : self . size ( ) , \"depth\" : self . depth ( ) , \"width\" : self . width ( ) , \"bits\" : self . num cbits ( ) , \"factors\" : self . num tensor factors ( ) , \"operations\" : self . count ops ( ) } return summary", "predictions": ["get the properties of this map ."], "references": ["return a dictionary of circuit properties ."], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 908, "code": "def pauli prep gates ( circuit , qreg , op ) : bas , proj = op if bas not in [ 'X' , 'Y' , 'Z' ] : raise Qiskit Error ( \"There's no X, Y or Z basis for this Pauli \" \"preparation\" ) if bas == \"X\" : if proj == 1 : circuit . u2 ( np . pi , np . pi , qreg ) else : circuit . u2 ( 0. , np . pi , qreg ) elif bas == \"Y\" : if proj == 1 : circuit . u2 ( - 0.5 * np . pi , np . pi , qreg ) else : circuit . u2 ( 0.5 * np . pi , np . pi , qreg ) elif bas == \"Z\" and proj == 1 : circuit . u3 ( np . pi , 0. , np . pi , qreg )", "predictions": ["generate a map from one basis to another ."], "references": ["add state preparation gates to a circuit ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 909, "code": "def pauli meas gates ( circuit , qreg , op ) : if op not in [ 'X' , 'Y' , 'Z' ] : raise Qiskit Error ( \"There's no X, Y or Z basis for this Pauli \" \"measurement\" ) if op == \"X\" : circuit . u2 ( 0. , np . pi , qreg ) elif op == \"Y\" : circuit . u2 ( 0. , 0.5 * np . pi , qreg )", "predictions": ["instantiates a new scrape object ."], "references": ["add state measurement gates to a circuit ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 910, "code": "def sic prep gates ( circuit , qreg , op ) : bas , proj = op if bas != 'S' : raise Qiskit Error ( 'Not in SIC basis!' ) theta = - 2 * np . arctan ( np . sqrt ( 2 ) ) if proj == 1 : circuit . u3 ( theta , np . pi , 0.0 , qreg ) elif proj == 2 : circuit . u3 ( theta , np . pi / 3 , 0.0 , qreg ) elif proj == 3 : circuit . u3 ( theta , - np . pi / 3 , 0.0 , qreg )", "predictions": ["linearly interpolates from individuals ( to be applied to the entity ."], "references": ["add state preparation gates to a circuit ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 911, "code": "def projector ( op list , basis ) : ret = 1 for op in op list : label , eigenstate = op ret = np . kron ( basis [ label ] [ eigenstate ] , ret ) return ret", "predictions": ["utility method to return the value of the ( ."], "references": ["returns a projectors ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 912, "code": "def run ( self , dag ) : resets = dag . op nodes ( Reset ) for reset in resets : predecessor = next ( dag . predecessors ( reset ) ) if predecessor . type == 'in' : dag . remove op node ( reset ) return dag", "predictions": ["runs the given dag ."], "references": ["return a new circuit that has been optimized ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 913, "code": "def cu3 ( self , theta , phi , lam , ctl , tgt ) : return self . append ( Cu3Gate ( theta , phi , lam ) , [ ctl , tgt ] , [ ] )", "predictions": ["the cu3 method is called when ( is required ."], "references": ["apply cu3 from ctl to tgt with angle theta phi lam ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 914, "code": "def build bell circuit ( ) : q = Quantum Register ( 2 ) c = Classical Register ( 2 ) qc = Quantum Circuit ( q , c ) qc . h ( q [ 0 ] ) qc . cx ( q [ 0 ] , q [ 1 ] ) qc . measure ( q , c ) return qc", "predictions": ["build a register register register range of the specified register ."], "references": ["returns a circuit putting 2 qubits in the bell state ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 915, "code": "def drive ( self ) -> Drive Channel : if self . drives : return self . drives [ 0 ] else : raise Pulse Error ( \"No drive channels in q[%d]\" % self . index )", "predictions": ["inserts the user on the first two words ."], "references": ["return the primary drive channel of this qubit ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 916, "code": "def control ( self ) -> Control Channel : if self . controls : return self . controls [ 0 ] else : raise Pulse Error ( \"No control channels in q[%d]\" % self . index )", "predictions": ["get the default control control control for this process ."], "references": ["return the primary control channel of this qubit ."], "bleu": 0.15851165692617156, "rouge_l": 0.42508710801393734}
{"id": 917, "code": "def measure ( self ) -> Measure Channel : if self . measures : return self . measures [ 0 ] else : raise Pulse Error ( \"No measurement channels in q[%d]\" % self . index )", "predictions": ["a convenience method for making this class around the implementation ."], "references": ["return the primary measure channel of this qubit ."], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 918, "code": "def acquire ( self ) -> Acquire Channel : if self . acquires : return self . acquires [ 0 ] else : raise Pulse Error ( \"No acquire channels in q[%d]\" % self . index )", "predictions": ["increments the pseudorandom instance for this cipher ."], "references": ["return the primary acquire channel of this qubit ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 919, "code": "def input state ( circ , q , n ) : for j in range ( n ) : circ . h ( q [ j ] ) circ . u1 ( math . pi / float ( 2 ** ( j ) ) , q [ j ] ) . inverse ( )", "predictions": ["computes the input value from an area of the queue ."], "references": ["n - qubit input state for qft that produces output 1 ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 920, "code": "def unset qiskit logger ( ) : qiskit logger = logging . get Logger ( 'qiskit' ) for handler in qiskit logger . handlers : qiskit logger . remove Handler ( handler )", "predictions": ["removes all ( and ( from the list of ("], "references": ["remove the handlers for the qiskit logger ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 921, "code": "def input ( self , data ) : self . data = data self . lexer . input ( data )", "predictions": ["workspace input method for creating a new instance ."], "references": ["set the input text data ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 922, "code": "def pop ( self ) : self . lexer = self . stack . pop ( ) self . filename = self . lexer . qasm file self . lineno = self . lexer . qasm line", "predictions": ["a method to pop the graph to be called on top of the stack ."], "references": ["pop a ply lexer off the stack ."], "bleu": 0.16943571815930883, "rouge_l": 0.36802413273001505}
{"id": 923, "code": "def push ( self , filename ) : self . lexer . qasm file = self . filename self . lexer . qasm line = self . lineno self . stack . append ( self . lexer ) self . mklexer ( filename )", "predictions": ["creates a push object for the stack ."], "references": ["push a ply lexer on the stack to parse filename ."], "bleu": 0.17250013293422076, "rouge_l": 0.4093959731543625}
{"id": 924, "code": "def get bound method ( self , instruction ) : try : return self . bound instructions [ type ( instruction ) ] except Key Error : raise Pulse Error ( 'Qobj conversion method for %s is not found.' % instruction )", "predictions": ["for the given instruction ."], "references": ["get conversion method for instruction ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 925, "code": "def verify declared bit ( self , obj ) : if obj . name not in self . current symtab : raise Qasm Error ( \"Cannot find symbol '\" + obj . name + \"' in argument list for gate, line\" , str ( obj . line ) , 'file' , obj . file ) sym = self . current symtab [ obj . name ] if not ( sym . type == 'id' and sym . is bit ) : raise Qasm Error ( \"Bit\" , obj . name , 'is not declared as a bit in the gate.' )", "predictions": ["verifies object definition per the contents of the object ."], "references": ["verify a qubit id against the gate prototype ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 926, "code": "def verify exp list ( self , obj ) : # if obj . children is not None : for children in obj . children : if isinstance ( children , node . Id ) : if children . name in self . external functions : continue if children . name not in self . current symtab : raise Qasm Error ( \"Argument '\" + children . name + \"' in expression cannot be \" + \"found, line\" , str ( children . line ) , \"file\" , children . file ) else : if hasattr ( children , \"children\" ) : self . verify exp list ( children )", "predictions": ["verifies all fields in an expression ."], "references": ["verify each expression in a list ."], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 927, "code": "def verify as gate ( self , obj , bitlist , arglist = None ) : if obj . name not in self . global symtab : raise Qasm Error ( \"Cannot find gate definition for '\" + obj . name + \"', line\" , str ( obj . line ) , 'file' , obj . file ) g sym = self . global symtab [ obj . name ] if not ( g sym . type == 'gate' or g sym . type == 'opaque' ) : raise Qasm Error ( \"'\" + obj . name + \"' is used as a gate \" + \"or opaque call but the symbol is neither;\" + \" it is a '\" + g sym . type + \"' line\" , str ( obj . line ) , 'file' , obj . file ) if g sym . n bits ( ) != bitlist . size ( ) : raise Qasm Error ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( bitlist . size ( ) ) , \"qubits but is declared for\" , str ( g sym . n bits ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) if arglist : if g sym . n args ( ) != arglist . size ( ) : raise Qasm Error ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( arglist . size ( ) ) , \"qubits but is declared for\" , str ( g sym . n args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) else : if g sym . n args ( ) > 0 : raise Qasm Error ( \"Gate or opaque call to '\" + obj . name + \"' has no arguments but is declared for\" , str ( g sym . n args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file )", "predictions": ["verify and verifies object methods ."], "references": ["verify a user defined gate call ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 928, "code": "def verify reg ( self , obj , object type ) : if obj . name not in self . global symtab : raise Qasm Error ( 'Cannot find definition for' , object type , \"'\" + obj . name + \"'\" , 'at line' , str ( obj . line ) , 'file' , obj . file ) g sym = self . global symtab [ obj . name ] if g sym . type != object type : raise Qasm Error ( \"Type for '\" + g sym . name + \"' should be '\" + object type + \"' but was found to be '\" + g sym . type + \"'\" , \"line\" , str ( obj . line ) , \"file\" , obj . file ) if obj . type == 'indexed id' : bound = g sym . index ndx = obj . index if ndx < 0 or ndx >= bound : raise Qasm Error ( \"Register index for '\" + g sym . name + \"' out of bounds. Index is\" , str ( ndx ) , \"bound is 0 <= index <\" , str ( bound ) , \"at line\" , str ( obj . line ) , \"file\" , obj . file )", "predictions": ["add object to all fields ."], "references": ["verify a register ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 929, "code": "def verify reg list ( self , obj , object type ) : for children in obj . children : self . verify reg ( children , object type )", "predictions": ["verifies all objects in this ( as well ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["verify a list of registers ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 930, "code": "def get tokens ( self ) : try : while True : token = self . lexer . token ( ) if not token : break yield token except Qasm Error as e : print ( 'Exception tokenizing qasm file:' , e . msg )", "predictions": ["named method to named chunks of this object ."], "references": ["returns a generator of the tokens ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 931, "code": "def parse debug ( self , val ) : if val is True : self . parse deb = True elif val is False : self . parse deb = False else : raise Qasm Error ( \"Illegal debug value '\" + str ( val ) + \"' must be True or False.\" )", "predictions": ["convenience for parsing the token from the given object ."], "references": ["set the parse_deb field ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 932, "code": "def parse ( self , data ) : self . parser . parse ( data , lexer = self . lexer , debug = self . parse deb ) if self . qasm is None : raise Qasm Error ( \"Uncaught exception in parser; \" + \"see previous messages for details.\" ) return self . qasm", "predictions": ["predecessors out the rhs object ."], "references": ["parse some data ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 933, "code": "def get tokens ( self ) : if self . filename : with open ( self . filename ) as ifile : self . data = ifile . read ( ) with Qasm Parser ( self . filename ) as qasm p : return qasm p . get tokens ( )", "predictions": ["returns the full path for this object ."], "references": ["returns a generator of the tokens ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 934, "code": "def parse ( self ) : if self . filename : with open ( self . filename ) as ifile : self . data = ifile . read ( ) with Qasm Parser ( self . filename ) as qasm p : qasm p . parse debug ( False ) return qasm p . parse ( self . data )", "predictions": ["i am a yaml file for this file ."], "references": ["parse the data ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 935, "code": "def crz ( self , theta , ctl , tgt ) : return self . append ( Crz Gate ( theta ) , [ ctl , tgt ] , [ ] )", "predictions": ["creates a new ( at the specified location ."], "references": ["apply crz from ctl to tgt with angle theta ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 936, "code": "def qasm ( self , prec = 15 ) : string = \"gate \" + self . name if self . arguments is not None : string += \"(\" + self . arguments . qasm ( prec ) + \")\" string += \" \" + self . bitlist . qasm ( prec ) + \"\\n\" string += \"{\\n\" + self . body . qasm ( prec ) + \"}\" return string", "predictions": ["get a fig instance for the remove style ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 937, "code": "def backend widget ( backend ) : config = backend . configuration ( ) . to dict ( ) props = backend . properties ( ) . to dict ( ) name = widgets . HTML ( value = \"<h4>{name}</h4>\" . format ( name = backend . name ( ) ) , layout = widgets . Layout ( ) ) n qubits = config [ 'n qubits' ] qubit count = widgets . HTML ( value = \"<h5><b>{qubits}</b></h5>\" . format ( qubits = n qubits ) , layout = widgets . Layout ( justify content = 'center' ) ) cmap = widgets . Output ( layout = widgets . Layout ( min width = '250px' , max width = '250px' , max height = '250px' , min height = '250px' , justify content = 'center' , align items = 'center' , margin = '0px 0px 0px 0px' ) ) with cmap : cmap fig = plot gate map ( backend , plot directed = False , label qubits = False ) if cmap fig is not None : display ( cmap fig ) plt . close ( cmap fig ) pending = generate jobs pending widget ( ) is oper = widgets . HTML ( value = \"<h5></h5>\" , layout = widgets . Layout ( justify content = 'center' ) ) least busy = widgets . HTML ( value = \"<h5></h5>\" , layout = widgets . Layout ( justify content = 'center' ) ) t1 units = props [ 'qubits' ] [ 0 ] [ 0 ] [ 'unit' ] avg t1 = round ( sum ( [ q [ 0 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n qubits , 1 ) t1 widget = widgets . HTML ( value = \"<h5>{t1} {units}</h5>\" . format ( t1 = avg t1 , units = t1 units ) , layout = widgets . Layout ( ) ) t2 units = props [ 'qubits' ] [ 0 ] [ 1 ] [ 'unit' ] avg t2 = round ( sum ( [ q [ 1 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n qubits , 1 ) t2 widget = widgets . HTML ( value = \"<h5>{t2} {units}</h5>\" . format ( t2 = avg t2 , units = t2 units ) , layout = widgets . Layout ( ) ) out = widgets . V Box ( [ name , cmap , qubit count , pending , least busy , is oper , t1 widget , t2 widget ] , layout = widgets . Layout ( display = 'inline-flex' , flex flow = 'column' , align items = 'center' ) ) out . is alive = True return out", "predictions": ["remove warn that aren ' t a fully qualified warn ."], "references": ["creates a backend widget ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 938, "code": "def generate jobs pending widget ( ) : pbar = widgets . Int Progress ( value = 0 , min = 0 , max = 50 , description = '' , orientation = 'horizontal' , layout = widgets . Layout ( max width = '180px' ) ) pbar . style . bar color = '#71cddd' pbar current = widgets . Label ( value = str ( pbar . value ) , layout = widgets . Layout ( min width = 'auto' ) ) pbar max = widgets . Label ( value = str ( pbar . max ) , layout = widgets . Layout ( min width = 'auto' ) ) def on max change ( change ) : pbar max . value = str ( change [ 'new' ] ) def on val change ( change ) : pbar current . value = str ( change [ 'new' ] ) pbar . observe ( on max change , names = 'max' ) pbar . observe ( on val change , names = 'value' ) jobs widget = widgets . H Box ( [ pbar current , pbar , pbar max ] , layout = widgets . Layout ( max width = '250px' , min width = '250px' , justify content = 'center' ) ) return jobs widget", "predictions": ["generates layers for a self - running self ."], "references": ["generates a jobs_pending progress bar widget ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 939, "code": "def bipartite shape ( self ) : return ( self . input dim , self . output dim , self . input dim , self . output dim )", "predictions": ["create a properties object from this reader and the specified properties ."], "references": ["return the shape for bipartite matrix"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 940, "code": "def conjugate ( self ) : return Choi ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["compute the pauli division of this diagnostic ."], "references": ["return the conjugate of the quantumchannel ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 941, "code": "def transpose ( self ) : d in , d out = self . dim data = np . reshape ( self . data , ( d in , d out , d in , d out ) ) data = np . transpose ( data , ( 1 , 0 , 3 , 2 ) ) data = np . reshape ( data , ( d in * d out , d in * d out ) ) return Choi ( data , input dims = self . output dims ( ) , output dims = self . input dims ( ) )", "predictions": ["run and return a non - negative independent result ."], "references": ["return the transpose of the quantumchannel ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 942, "code": "def load schemas and validators ( ) : schema base path = os . path . join ( os . path . dirname ( file ) , '../..' ) for name , path in DEFAULT SCHEMA PATHS . items ( ) : load schema ( os . path . join ( schema base path , path ) , name ) get validator ( name )", "predictions": ["loads the provided op from the given op ."], "references": ["load all default schemas into _schemas ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 943, "code": "def qasm ( self , prec = 15 ) : return \",\" . join ( [ self . children [ j ] . qasm ( prec ) for j in range ( self . size ( ) ) ] )", "predictions": ["a child method for making all the byteenumfield ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 944, "code": "def qasm ( self , prec = 15 ) : string = \"\" for children in self . children : string += \"  \" + children . qasm ( prec ) + \"\\n\" return string", "predictions": ["get all run tensors ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 945, "code": "def calls ( self ) : lst = [ ] for children in self . children : if children . type == \"custom unitary\" : lst . append ( children . name ) return lst", "predictions": ["/ method for affect the list of child classes ."], "references": ["return a list of custom gate names in this gate body ."], "bleu": 0.14595947916189678, "rouge_l": 0.2683284457478006}
{"id": 946, "code": "def qasm ( self , prec = 15 ) : if self . value == pi : return \"pi\" return ccode ( self . value , precision = prec )", "predictions": ["a gstring build method for similarity ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 947, "code": "def conjugate ( self ) : return Super Op ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["compute the drive repeatedly ."], "references": ["return the conjugate of the quantumchannel ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 948, "code": "def transpose ( self ) : return Super Op ( np . transpose ( self . data ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )", "predictions": ["use this to control the [ 0 , 1 ]"], "references": ["return the transpose of the quantumchannel ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 949, "code": "def compose subsystem ( self , other , qargs , front = False ) : input dims = list ( self . input dims ( ) ) output dims = list ( self . output dims ( ) ) if front : num indices = len ( self . input dims ( ) ) shift = 2 * len ( self . output dims ( ) ) right mul = True for pos , qubit in enumerate ( qargs ) : input dims [ qubit ] = other . input dims [ pos ] else : num indices = len ( self . output dims ( ) ) shift = 0 right mul = False for pos , qubit in enumerate ( qargs ) : output dims [ qubit ] = other . output dims [ pos ] tensor = np . reshape ( self . data , self . shape ) mat = np . reshape ( other . data , other . shape ) indices = [ 2 * num indices - 1 - qubit for qubit in qargs ] + [ num indices - 1 - qubit for qubit in qargs ] final shape = [ np . product ( output dims ) ** 2 , np . product ( input dims ) ** 2 ] data = np . reshape ( self . einsum matmul ( tensor , mat , indices , shift , right mul ) , final shape ) return Super Op ( data , input dims , output dims )", "predictions": ["estimate the product . this implementation is useful for ( ."], "references": ["return the composition channel ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 950, "code": "def instruction to superop ( cls , instruction ) : if isinstance ( instruction , Quantum Circuit ) : instruction = instruction . to instruction ( ) op = Super Op ( np . eye ( 4 ** instruction . num qubits ) ) op . append instruction ( instruction ) return op", "predictions": ["convert the given acquire acquire acquire acquire to a acquire acquire ."], "references": ["convert a quantumcircuit or instruction to a superop ."], "bleu": 0.15537125692760353, "rouge_l": 0.39102564102564097}
{"id": 951, "code": "def append instruction ( self , obj , qargs = None ) : if isinstance ( obj , Instruction ) : chan = None if obj . name == 'reset' : chan = Super Op ( np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] ] ) ) if obj . name == 'kraus' : kraus = obj . params dim = len ( kraus [ 0 ] ) chan = Super Op ( to superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) elif hasattr ( obj , 'to matrix' ) : try : kraus = [ obj . to matrix ( ) ] dim = len ( kraus [ 0 ] ) chan = Super Op ( to superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) except Qiskit Error : pass if chan is not None : op = self . compose ( chan , qargs = qargs ) self . data = op . data else : if obj . definition is None : raise Qiskit Error ( 'Cannot apply Instruction: {}' . format ( obj . name ) ) for instr , qregs , cregs in obj . definition : if cregs : raise Qiskit Error ( 'Cannot apply instruction with classical registers: {}' . format ( instr . name ) ) new qargs = [ tup [ 1 ] for tup in qregs ] self . append instruction ( instr , qargs = new qargs ) else : raise Qiskit Error ( 'Input is not an instruction.' )", "predictions": ["input an operation to a list ."], "references": ["update the current operator by apply an instruction ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 952, "code": "def run ( self , dag ) : final op types = [ 'measure' , 'barrier' ] final ops = [ ] for candidate node in dag . named nodes ( * final op types ) : is final op = True for , child successors in dag . bfs successors ( candidate node ) : if any ( suc . type == 'op' and suc . name not in final op types for suc in child successors ) : is final op = False break if is final op : final ops . append ( candidate node ) if not final ops : return dag barrier layer = DAG Circuit ( ) for qreg in dag . qregs . values ( ) : barrier layer . add qreg ( qreg ) for creg in dag . cregs . values ( ) : barrier layer . add creg ( creg ) final qubits = set ( final op . qargs [ 0 ] for final op in final ops ) barrier layer . apply operation back ( Barrier ( len ( final qubits ) ) , list ( final qubits ) , [ ] ) ordered final nodes = [ node for node in dag . topological op nodes ( ) if node in set ( final ops ) ] for final node in ordered final nodes : barrier layer . apply operation back ( final node . op , final node . qargs , final node . cargs ) for final op in final ops : dag . remove op node ( final op ) dag . extend back ( barrier layer ) adjacent pass = Merge Adjacent Barriers ( ) return adjacent pass . run ( dag )", "predictions": ["take care of nodes ."], "references": ["return a circuit with a barrier before last measurements ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 953, "code": "def unitary ( self , obj , qubits , label = None ) : if isinstance ( qubits , Quantum Register ) : qubits = qubits [ : ] return self . append ( Unitary Gate ( obj , label = label ) , qubits , [ ] )", "predictions": [", for simple plotting operation ."], "references": ["apply u2 to q ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 954, "code": "def define ( self ) : if self . num qubits == 1 : q = Quantum Register ( 1 , \"q\" ) angles = euler angles 1q ( self . to matrix ( ) ) self . definition = [ ( U3Gate ( * angles ) , [ q [ 0 ] ] , [ ] ) ] if self . num qubits == 2 : self . definition = two qubit kak ( self . to matrix ( ) )", "predictions": ["defines a matrix for the given range ."], "references": ["calculate a subcircuit that implements this unitary ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 955, "code": "def atol ( self , atol ) : max tol = self . class . MAX TOL if atol < 0 : raise Qiskit Error ( \"Invalid atol: must be non-negative.\" ) if atol > max tol : raise Qiskit Error ( \"Invalid atol: must be less than {}.\" . format ( max tol ) ) self . class . ATOL = atol", "predictions": ["attach to the log for all reasons ."], "references": ["set the absolute tolerence parameter for float comparisons ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 956, "code": "def rtol ( self , rtol ) : max tol = self . class . MAX TOL if rtol < 0 : raise Qiskit Error ( \"Invalid rtol: must be non-negative.\" ) if rtol > max tol : raise Qiskit Error ( \"Invalid rtol: must be less than {}.\" . format ( max tol ) ) self . class . RTOL = rtol", "predictions": ["a simple method to get all others for this class ."], "references": ["set the relative tolerence parameter for float comparisons ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 957, "code": "def input dims ( self , qargs = None ) : if qargs is None : return self . input dims return tuple ( self . input dims [ i ] for i in qargs )", "predictions": ["creates a current current current current current current element in a current current period ."], "references": ["return tuple of input dimension for specified subsystems ."], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 958, "code": "def output dims ( self , qargs = None ) : if qargs is None : return self . output dims return tuple ( self . output dims [ i ] for i in qargs )", "predictions": ["creates a not not altered docstring ."], "references": ["return tuple of output dimension for specified subsystems ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 959, "code": "def copy ( self ) : return self . class ( self . data , self . input dims ( ) , self . output dims ( ) )", "predictions": ["copies this object into a new contact ."], "references": ["make a copy of current operator ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 960, "code": "def automatic dims ( cls , dims , size ) : if dims is None : dims = size elif np . product ( dims ) != size : raise Qiskit Error ( \"dimensions do not match size.\" ) if isinstance ( dims , ( int , np . integer ) ) : num qubits = int ( np . log2 ( dims ) ) if 2 ** num qubits == size : return num qubits * ( 2 , ) return ( dims , ) return tuple ( dims )", "predictions": ["perform a automatic automatic automatic automatic promotion ."], "references": ["check if input dimension corresponds to qubit subsystems ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 961, "code": "def deserialize ( self , value , attr , data ) : try : return super ( ) . deserialize ( value , attr , data ) except Validation Error as ex : if 'deserialization schema selector' in ex . messages [ 0 ] : ex . messages [ 0 ] = 'Cannot find a valid schema among the choices' raise", "predictions": ["deserialize deserialize data from database"], "references": ["override _deserialize for customizing the exception raised ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 962, "code": "def serialize ( self , value , key , obj ) : try : return super ( ) . serialize ( value , key , obj ) except Type Error as ex : if 'serialization schema selector' in str ( ex ) : raise Validation Error ( 'Data from an invalid schema' ) raise", "predictions": ["method which serializes the serialization to a string ."], "references": ["override _serialize for customizing the exception raised ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 963, "code": "def inverse ( self ) : return Snapshot ( self . num qubits , self . num clbits , self . params [ 0 ] , self . params [ 1 ] )", "predictions": ["the put method . this method is called to provide additional entries for each category ."], "references": ["special case . return self ."], "bleu": 0.08513012360883544, "rouge_l": 0.19805194805194803}
{"id": 964, "code": "def is unitary ( self , atol = None , rtol = None ) : try : op = self . to operator ( ) return op . is unitary ( atol = atol , rtol = rtol ) except Qiskit Error : return False", "predictions": ["check if the operation is cached ."], "references": ["return true if quantumchannel is a unitary channel ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 965, "code": "def to operator ( self ) : mat = to operator ( self . rep , self . data , * self . dim ) return Operator ( mat , self . input dims ( ) , self . output dims ( ) )", "predictions": ["convert this latlng to a sphere ."], "references": ["try to convert channel to a unitary representation operator ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 966, "code": "def format state ( self , state , density matrix = False ) : state = np . array ( state ) shape = state . shape ndim = state . ndim if ndim > 2 : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if ndim == 2 : if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if shape [ 1 ] == 1 : state = np . reshape ( state , shape [ 0 ] ) if density matrix and ndim == 1 : state = np . outer ( state , np . transpose ( np . conj ( state ) ) ) return state", "predictions": ["formats the given state of this array ."], "references": ["format input state so it is statevector or density matrix"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 967, "code": "def init transformer ( cls , data ) : if isinstance ( data , Quantum Channel ) : return data if hasattr ( data , 'to quantumchannel' ) : return data . to channel ( ) if hasattr ( data , 'to channel' ) : return data . to channel ( ) return Operator ( data )", "predictions": ["initialize the transformer class ."], "references": ["convert input into a quantumchannel subclass object or operator object"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 968, "code": "def parse time ( self , date string , settings ) : date string = PATTERN . sub ( '' , date string ) date string = re . sub ( r'\\b(?:ago|in)\\b' , '' , date string ) try : return time parser ( date string ) except : pass", "predictions": ["parses the time from the given string ."], "references": ["attemps to parse time part of date strings like 1 day ago 2 pm"], "bleu": 0.07575149194183216, "rouge_l": 0.08664772727272725}
{"id": 969, "code": "def read config ( self ) : self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu count ( ) / 2 ) + 1 ) self . phantom modules path = self . cfg [ \"phantom modules path\" ] self . additional libs = ' ' . join ( self . cfg [ \"additional libs\" ] ) self . answ log level = self . cfg [ \"writelog\" ] if self . answ log level . lower ( ) in [ '0' , 'false' ] : self . answ log level = 'none' elif self . answ log level . lower ( ) in [ '1' , 'true' ] : self . answ log level = 'all' self . timeout = parse duration ( self . cfg [ \"timeout\" ] ) if self . timeout > 120000 : logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) self . answ log = self . core . mkstemp ( \".log\" , \"answ \" ) self . core . add artifact file ( self . answ log ) self . core . add artifact file ( self . phout file ) self . core . add artifact file ( self . stat log ) self . phantom log = self . core . mkstemp ( \".log\" , \"phantom \" ) self . core . add artifact file ( self . phantom log ) main stream = Stream Config ( self . core , len ( self . streams ) , self . phout file , self . answ log , self . answ log level , self . timeout , self . cfg , True ) self . streams . append ( main stream ) for section in self . multi ( ) : self . streams . append ( Stream Config ( self . core , len ( self . streams ) , self . phout file , self . answ log , self . answ log level , self . timeout , section ) ) for stream in self . streams : stream . read config ( ) if any ( stream . ssl for stream in self . streams ) : self . additional libs += ' ssl io benchmark method stream transport ssl'", "predictions": ["read all pointers of the given config ."], "references": ["read phantom tool specific options"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 970, "code": "def compose config ( self ) : streams config = '' stat benchmarks = '' for stream in self . streams : streams config += stream . compose config ( ) if not stream . is main : stat benchmarks += \" \" + \"benchmark io%s\" % stream . sequence no kwargs = { } kwargs [ 'threads' ] = self . threads kwargs [ 'phantom log' ] = self . phantom log kwargs [ 'stat log' ] = self . stat log kwargs [ 'benchmarks block' ] = streams config kwargs [ 'stat benchmarks' ] = stat benchmarks kwargs [ 'additional libs' ] = self . additional libs kwargs [ 'phantom modules path' ] = self . phantom modules path filename = self . core . mkstemp ( \".conf\" , \"phantom \" ) self . core . add artifact file ( filename ) logger . debug ( \"Generating phantom config: %s\" , filename ) template str = resource string ( name , \"config/phantom.conf.tpl\" ) tpl = string . Template ( template str ) config = tpl . substitute ( kwargs ) with open ( filename , 'w' ) as conffile : conffile . write ( config ) return filename", "predictions": ["creates a previously created ( object for the stack ."], "references": ["generate phantom tool run config"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 971, "code": "def get info ( self ) : result = copy . copy ( self . streams [ 0 ] ) result . stat log = self . stat log result . steps = [ ] result . ammo file = '' result . rps schedule = None result . ammo count = 0 result . duration = 0 result . instances = 0 result . loadscheme = [ ] result . loop count = 0 for stream in self . streams : sec no = 0 logger . debug ( \"Steps: %s\" , stream . stepper wrapper . steps ) for item in stream . stepper wrapper . steps : for x in range ( 0 , item [ 1 ] ) : if len ( result . steps ) > sec no : result . steps [ sec no ] [ 0 ] += item [ 0 ] else : result . steps . append ( [ item [ 0 ] , 1 ] ) sec no += 1 if result . rps schedule : result . rps schedule = [ ] else : result . rps schedule = stream . stepper wrapper . loadscheme if result . loadscheme : result . loadscheme = '' else : result . loadscheme = '' if result . loop count : result . loop count = u'0' else : result . loop count = stream . stepper wrapper . loop count result . ammo file += '{} ' . format ( stream . stepper wrapper . ammo file ) result . ammo count += stream . stepper wrapper . ammo count result . duration = max ( result . duration , stream . stepper wrapper . duration ) result . instances += stream . instances if not result . ammo count : raise Value Error ( \"Total ammo count cannot be zero\" ) return result", "predictions": ["returns all info with properties that are kept in the correct order ."], "references": ["get merged info about phantom conf"], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 972, "code": "def expand time ( str time , default unit = 's' , multiplier = 1 ) : parser = re . compile ( r'(\\d+)([a-z A-Z]*)' ) parts = parser . findall ( str time ) result = 0.0 for value , unit in parts : value = int ( value ) unit = unit . lower ( ) if unit == '' : unit = default unit if unit == 'ms' : result += value * 0.001 continue elif unit == 's' : result += value continue elif unit == 'm' : result += value * 60 continue elif unit == 'h' : result += value * 60 * 60 continue elif unit == 'd' : result += value * 60 * 60 * 24 continue elif unit == 'w' : result += value * 60 * 60 * 24 * 7 continue else : raise Value Error ( \"String contains unsupported unit %s: %s\" % ( unit , str time ) ) return int ( result * multiplier )", "predictions": ["expand time value to a start of \" ( \" . if no unit is found , the amount of \" . \" is returned . if the amount is enabled , then the amount is returned . otherwise , the amount is returned ."], "references": ["helper for above functions"], "bleu": 0.022479007417262973, "rouge_l": 0.0}
{"id": 973, "code": "def pid exists ( pid ) : if pid < 0 : return False try : os . kill ( pid , 0 ) except OS Error as exc : logging . debug ( \"No process[%s]: %s\" , exc . errno , exc ) return exc . errno == errno . EPERM else : p = psutil . Process ( pid ) return p . status != psutil . STATUS ZOMBIE", "predictions": ["check if the given pid exists ."], "references": ["check whether pid exists in the current process table ."], "bleu": 0.18938334565508194, "rouge_l": 0.45607476635514016}
{"id": 974, "code": "def read config ( self ) : self . log . info ( \"Configuring Stepper Wrapper...\" ) self . ammo file = self . get option ( self . OPTION AMMOFILE ) self . ammo type = self . get option ( 'ammo type' ) if self . ammo file : self . ammo file = os . path . expanduser ( self . ammo file ) self . loop limit = self . get option ( self . OPTION LOOP ) self . ammo limit = self . get option ( \"ammo limit\" ) self . load profile = Load Profile ( * * self . get option ( 'load profile' ) ) self . instances = int ( self . get option ( self . OPTION INSTANCES LIMIT , '1000' ) ) self . uris = self . get option ( \"uris\" , [ ] ) while '' in self . uris : self . uris . remove ( '' ) self . headers = self . get option ( \"headers\" ) self . http ver = self . get option ( \"header http\" ) self . autocases = self . get option ( \"autocases\" ) self . enum ammo = self . get option ( \"enum ammo\" ) self . use caching = self . get option ( \"use caching\" ) self . file cache = self . get option ( 'file cache' ) cache dir = self . get option ( \"cache dir\" ) or self . core . artifacts base dir self . cache dir = os . path . expanduser ( cache dir ) self . force stepping = self . get option ( \"force stepping\" ) if self . get option ( self . OPTION LOAD ) [ self . OPTION LOAD TYPE ] == 'stpd file' : self . stpd = self . get option ( self . OPTION LOAD ) [ self . OPTION SCHEDULE ] self . chosen cases = self . get option ( \"chosen cases\" ) . split ( ) if self . chosen cases : self . log . info ( \"chosen cases LIMITS: %s\" , self . chosen cases )", "predictions": ["reads and writes the tree at the end of the ( ."], "references": ["stepper part of reading options"], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 975, "code": "def prepare stepper ( self ) : def publish info ( stepper info ) : info . status . publish ( 'loadscheme' , stepper info . loadscheme ) info . status . publish ( 'loop count' , stepper info . loop count ) info . status . publish ( 'steps' , stepper info . steps ) info . status . publish ( 'duration' , stepper info . duration ) info . status . ammo count = stepper info . ammo count info . status . publish ( 'instances' , stepper info . instances ) self . core . publish ( 'stepper' , 'loadscheme' , stepper info . loadscheme ) self . core . publish ( 'stepper' , 'loop count' , stepper info . loop count ) self . core . publish ( 'stepper' , 'steps' , stepper info . steps ) self . core . publish ( 'stepper' , 'duration' , stepper info . duration ) self . core . publish ( 'stepper' , 'ammo count' , stepper info . ammo count ) self . core . publish ( 'stepper' , 'instances' , stepper info . instances ) return stepper info if not self . stpd : self . stpd = self . get stpd filename ( ) if self . use caching and not self . force stepping and os . path . exists ( self . stpd ) and os . path . exists ( self . si filename ( ) ) : self . log . info ( \"Using cached stpd-file: %s\" , self . stpd ) stepper info = self . read cached options ( ) if self . instances and self . load profile . is rps ( ) : self . log . info ( \"rps schedule is set. Overriding cached instances param from config: %s\" , self . instances ) stepper info = stepper info . replace ( instances = self . instances ) publish info ( stepper info ) else : if ( self . force stepping and os . path . exists ( self . si filename ( ) ) ) : os . remove ( self . si filename ( ) ) self . make stpd file ( ) stepper info = info . status . get info ( ) self . write cached options ( stepper info ) else : self . log . info ( \"Using specified stpd-file: %s\" , self . stpd ) stepper info = publish info ( self . read cached options ( ) ) self . ammo count = stepper info . ammo count self . duration = stepper info . duration self . loop count = stepper info . loop count self . loadscheme = stepper info . loadscheme self . steps = stepper info . steps if stepper info . instances : self . instances = stepper info . instances", "predictions": ["setup and writes an stepper ."], "references": ["generate test data if necessary"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 976, "code": "def get stpd filename ( self ) : if self . use caching : sep = \"|\" hasher = hashlib . md5 ( ) hashed str = \"cache version 6\" + sep + ';' . join ( self . load profile . schedule ) + sep + str ( self . loop limit ) hashed str += sep + str ( self . ammo limit ) + sep + ';' . join ( self . load profile . schedule ) + sep + str ( self . autocases ) hashed str += sep + \";\" . join ( self . uris ) + sep + \";\" . join ( self . headers ) + sep + self . http ver + sep + \";\" . join ( self . chosen cases ) hashed str += sep + str ( self . enum ammo ) + sep + str ( self . ammo type ) if self . load profile . is instances ( ) : hashed str += sep + str ( self . instances ) if self . ammo file : opener = resource . get opener ( self . ammo file ) hashed str += sep + opener . hash else : if not self . uris : raise Runtime Error ( \"Neither ammofile nor uris specified\" ) hashed str += sep + ';' . join ( self . uris ) + sep + ';' . join ( self . headers ) self . log . debug ( \"stpd-hash source: %s\" , hashed str ) hasher . update ( hashed str . encode ( 'utf8' ) ) if not os . path . exists ( self . cache dir ) : os . makedirs ( self . cache dir ) stpd = self . cache dir + '/' + os . path . basename ( self . ammo file ) + \" \" + hasher . hexdigest ( ) + \".stpd\" else : stpd = os . path . realpath ( \"ammo.stpd\" ) self . log . debug ( \"Generated cache file name: %s\" , stpd ) return stpd", "predictions": ["load and load a filename that can be used to read the state of the schedule ."], "references": ["choose the name for stepped data file"], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 977, "code": "def read cached options ( self ) : self . log . debug ( \"Reading cached stepper info: %s\" , self . si filename ( ) ) with open ( self . si filename ( ) , 'r' ) as si file : si = info . Stepper Info ( * * json . load ( si file ) ) return si", "predictions": ["read a cached options from the file ."], "references": ["read stepper info from json"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 978, "code": "def write cached options ( self , si ) : self . log . debug ( \"Saving stepper info: %s\" , self . si filename ( ) ) with open ( self . si filename ( ) , 'w' ) as si file : json . dump ( si . asdict ( ) , si file , indent = 4 )", "predictions": ["writes the options to the file ."], "references": ["write stepper info to json"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 979, "code": "def make stpd file ( self ) : self . log . info ( \"Making stpd-file: %s\" , self . stpd ) stepper = Stepper ( self . core , rps schedule = self . load profile . schedule if self . load profile . is rps ( ) else None , http ver = self . http ver , ammo file = self . ammo file , instances schedule = self . load profile . schedule if self . load profile . is instances ( ) else None , instances = self . instances , loop limit = self . loop limit , ammo limit = self . ammo limit , uris = self . uris , headers = [ header . strip ( '[]' ) for header in self . headers ] , autocases = self . autocases , enum ammo = self . enum ammo , ammo type = self . ammo type , chosen cases = self . chosen cases , use cache = self . use caching ) with open ( self . stpd , 'w' , self . file cache ) as os : stepper . write ( os )", "predictions": ["makes the ( object ."], "references": ["stpd generation using stepper class"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 980, "code": "def create ( rps schedule ) : if len ( rps schedule ) > 1 : lp = Composite ( [ Step Factory . produce ( step config ) for step config in rps schedule ] ) else : lp = Step Factory . produce ( rps schedule [ 0 ] ) info . status . publish ( 'duration' , lp . get duration ( ) / 1000 ) info . status . publish ( 'steps' , lp . get rps list ( ) ) info . status . lp len = len ( lp ) return lp", "predictions": ["create the n-d cluster"], "references": ["create load plan as defined in schedule . publish info about its duration ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 981, "code": "def rps at ( self , t ) : if 0 <= t <= self . duration : return self . minrps + float ( self . maxrps - self . minrps ) * t / self . duration else : return 0", "predictions": ["get the rps for this visualization ."], "references": ["return rps for second t"], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 982, "code": "def execute ( self , cmd ) : self . log . info ( \"Executing: %s\" , cmd ) retcode = execute ( cmd , shell = True , poll period = 0.1 , catch out = self . catch out ) [ 0 ] if retcode : raise Runtime Error ( \"Subprocess returned %s\" % retcode ) return retcode", "predictions": ["changed changed since the last used is not below the shell ."], "references": ["execute and check exit code"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 983, "code": "def publish ( self , key , value ) : self . log . debug ( \"Publishing status: %s/%s: %s\" , self . class . name , key , value ) self . core . publish ( self . class . name , key , value )", "predictions": ["publish just a key to a class ."], "references": ["publish value to status"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 984, "code": "def count matched codes ( codes regex , codes dict ) : total = 0 for code , count in codes dict . items ( ) : if codes regex . match ( str ( code ) ) : total += count return total", "predictions": ["returns a list of loaded loaded codes ."], "references": ["helper to aggregate codes by mask"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 985, "code": "def stop ( self ) : self . quit . set ( ) while sorted ( [ self . pool [ i ] . is alive ( ) for i in xrange ( len ( self . pool ) ) ] ) [ - 1 ] : time . sleep ( 1 ) try : while not self . task queue . empty ( ) : self . task queue . get ( timeout = 0.1 ) self . task queue . close ( ) self . feeder . join ( ) except Exception as ex : logger . info ( ex )", "predictions": ["closes all registered filters ."], "references": ["say the workers to finish their jobs and quit ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 986, "code": "def feed ( self ) : self . plan = Stpd Reader ( self . stpd filename ) if self . cached stpd : self . plan = list ( self . plan ) for task in self . plan : if self . quit . is set ( ) : logger . info ( \"Stop feeding: gonna quit\" ) return while True : try : self . task queue . put ( task , timeout = 1 ) break except Full : if self . quit . is set ( ) or self . workers finished : return else : continue workers count = self . instances logger . info ( \"Feeded all data. Publishing %d killer tasks\" % ( workers count ) ) retry delay = 1 for in range ( 5 ) : try : [ self . task queue . put ( None , timeout = 1 ) for in xrange ( 0 , workers count ) ] break except Full : logger . debug ( \"Couldn't post killer tasks\" \" because queue is full. Retrying in %ss\" , retry delay ) time . sleep ( retry delay ) retry delay *= 2 try : logger . info ( \"Waiting for workers\" ) map ( lambda x : x . join ( ) , self . pool ) logger . info ( \"All workers exited.\" ) self . workers finished = True except ( Keyboard Interrupt , System Exit ) : self . task queue . close ( ) self . results . close ( ) self . quit . set ( ) logger . info ( \"Going to quit. Waiting for workers\" ) map ( lambda x : x . join ( ) , self . pool ) self . workers finished = True", "predictions": ["closes the previously saved feed of all workers ."], "references": ["a feeder that runs in distinct thread in main process ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 987, "code": "def worker ( self ) : logger . debug ( \"Init shooter process\" ) try : self . gun . setup ( ) except Exception : logger . exception ( \"Couldn't initialize gun. Exit shooter process\" ) return while not self . quit . is set ( ) : try : task = self . task queue . get ( timeout = 1 ) if not task : logger . debug ( \"Got killer task.\" ) break timestamp , missile , marker = task planned time = self . start time + ( timestamp / 1000.0 ) delay = planned time - time . time ( ) if delay > 0 : time . sleep ( delay ) try : with self . instance counter . get lock ( ) : self . instance counter . value += 1 self . gun . shoot ( missile , marker ) finally : with self . instance counter . get lock ( ) : self . instance counter . value -= 1 except ( Keyboard Interrupt , System Exit ) : break except Empty : if self . quit . is set ( ) : logger . debug ( \"Empty queue. Exiting process\" ) return except Full : logger . warning ( \"Couldn't put to result queue because it's full\" ) except Exception : logger . exception ( \"Bfg shoot exception\" ) try : self . gun . teardown ( ) except Exception : logger . exception ( \"Couldn't finalize gun. Exit shooter process\" ) return logger . debug ( \"Exit shooter process\" )", "predictions": [". the future that enqueues the previously submitted ."], "references": ["a worker that does actual jobs"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 988, "code": "def green worker ( self ) : while not self . quit . is set ( ) : try : task = self . green queue . get ( timeout = 1 ) timestamp , missile , marker = task planned time = self . start time + ( timestamp / 1000.0 ) delay = planned time - time . time ( ) if delay > 0 : time . sleep ( delay ) try : with self . instance counter . get lock ( ) : self . instance counter . value += 1 self . gun . shoot ( missile , marker ) finally : with self . instance counter . get lock ( ) : self . instance counter . value -= 1 self . free threads count += 1 except ( Keyboard Interrupt , System Exit ) : break except Empty : continue except Full : logger . warning ( \"Couldn't put to result queue because it's full\" ) except Exception : logger . exception ( \"Bfg shoot exception\" )", "predictions": [". a number of ( ."], "references": ["a worker that does actual jobs"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 989, "code": "def add user options ( self ) : if self . options . get ( 'user options' , None ) : self . core . apply shorthand options ( self . options [ 'user options' ] )", "predictions": ["add a default user to the request output ."], "references": ["override config options with user specified options"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 990, "code": "def configure ( self , options ) : self . options = options if self . options . get ( 'lock dir' , None ) : self . core . set option ( self . core . SECTION , \"lock dir\" , self . options [ 'lock dir' ] ) if self . options . get ( 'ignore lock' , None ) : self . core . set option ( self . core . SECTION , 'ignore lock' , self . options [ 'ignore lock' ] ) while True : try : self . core . get lock ( ) break except Exception as exc : if self . options . get ( 'lock fail' , None ) : raise Runtime Error ( \"Lock file present, cannot continue\" ) self . log . info ( \"Couldn't get lock. Will retry in 5 seconds... (%s)\" , str ( exc ) ) time . sleep ( 5 ) configs = self . get default configs ( ) if self . options . get ( 'config' , None ) : configs . append ( self . options [ 'config' ] ) self . core . load configs ( configs ) self . add user options ( ) self . core . load plugins ( ) if self . options . get ( 'ignore lock' , None ) : self . core . set option ( self . core . SECTION , self . IGNORE LOCKS , \"1\" )", "predictions": ["setup object configs to be run before the permission method is called ."], "references": ["make preparations before running tank"], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 991, "code": "def collect data ( self , end = False ) : data = get nowait from queue ( self . results ) stats = get nowait from queue ( self . stats results ) logger . debug ( \"Data timestamps: %s\" % [ d . get ( 'ts' ) for d in data ] ) logger . debug ( \"Stats timestamps: %s\" % [ d . get ( 'ts' ) for d in stats ] ) for item in data : ts = item [ 'ts' ] if ts in self . stat cache : data item = item stat item = self . stat cache . pop ( ts ) self . notify listeners ( data item , stat item ) else : self . data cache [ ts ] = item for item in stats : ts = item [ 'ts' ] if ts in self . data cache : data item = self . data cache . pop ( ts ) stat item = item self . notify listeners ( data item , stat item ) else : self . stat cache [ ts ] = item if end and len ( self . data cache ) > 0 : logger . info ( 'Timestamps without stats:' ) for ts , data item in sorted ( self . data cache . items ( ) , key = lambda i : i [ 0 ] ) : logger . info ( ts ) self . notify listeners ( data item , Stats Reader . stats item ( ts , 0 , 0 ) )", "predictions": ["collects statistics about the current data ."], "references": ["collect data cache it and send to listeners"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 992, "code": "def notify listeners ( self , data , stats ) : for listener in self . listeners : listener . on aggregated data ( data , stats )", "predictions": ["automatic the ( method ."], "references": ["notify all listeners about aggregate data and stats"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 993, "code": "def clean markup ( self , orig str ) : for val in [ self . YELLOW , self . RED , self . RESET , self . CYAN , self . BG MAGENTA , self . WHITE , self . BG GREEN , self . GREEN , self . BG BROWN , self . RED DARK , self . MAGENTA , self . BG CYAN ] : orig str = orig str . replace ( val , '' ) return orig str", "predictions": ["this method is called by the native code operator to deserialize each call to the controller . this method is called by the native call to deserialize the ( operation ."], "references": ["clean markup from string"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 994, "code": "def uninstall ( self ) : if self . session : logger . info ( 'Waiting monitoring data...' ) self . session . terminate ( ) self . session . wait ( ) self . session = None log filename = \"agent {host}.log\" . format ( host = \"localhost\" ) data filename = \"agent {host}.rawdata\" . format ( host = \"localhost\" ) try : logger . info ( 'Saving monitoring artefacts from localhost' ) copyfile ( self . workdir + \"/ agent.log\" , log filename ) copyfile ( self . workdir + \"/monitoring.rawdata\" , data filename ) logger . info ( 'Deleting temp directory: %s' , self . workdir ) rmtree ( self . workdir ) except Exception : logger . error ( \"Exception while uninstalling agent\" , exc info = True ) logger . info ( \"Removing agent from: localhost\" ) return log filename , data filename", "predictions": ["removes and flushes the ex part of this object ."], "references": ["remove agent s files from remote host"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 995, "code": "def uninstall ( self ) : log filename = \"agent {host}.log\" . format ( host = self . host ) data filename = \"agent {host}.rawdata\" . format ( host = self . host ) try : if self . session : self . session . send ( \"stop\\n\" ) self . session . close ( ) self . session = None except Base Exception : logger . warning ( 'Unable to correctly stop monitoring agent - session is broken. Pay attention to agent log (%s).' , log filename , exc info = True ) else : try : self . ssh . get file ( os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , \" agent.log\" ) , log filename ) self . ssh . get file ( os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , \"monitoring.rawdata\" ) , data filename ) self . ssh . rm r ( self . path [ 'AGENT REMOTE FOLDER' ] ) except Exception : logger . error ( \"Unable to get agent artefacts\" , exc info = True ) self . kill agent ( ) return log filename , data filename", "predictions": ["removes an agent from the existing return ."], "references": ["remove agent s files from remote host"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 996, "code": "def add jmeter components ( self , jmx , jtl , variables ) : logger . debug ( \"Original JMX: %s\" , os . path . realpath ( jmx ) ) with open ( jmx , 'r' ) as src jmx : source lines = src jmx . readlines ( ) try : closing = source lines . pop ( - 1 ) if \"Work Bench Gui\" in source lines [ - 5 ] : logger . info ( \"Work Bench checkbox enabled...bypassing\" ) last string count = 6 else : last string count = 2 while last string count > 0 : closing = source lines . pop ( - 1 ) + closing last string count -= 1 logger . debug ( \"Closing statement: %s\" , closing ) except Exception as exc : raise Runtime Error ( \"Failed to find the end of JMX XML: %s\" % exc ) udv tpl = resource string ( name , 'config/jmeter var template.xml' ) udv set = [ ] for var name , var value in variables . iteritems ( ) : udv set . append ( udv tpl % ( var name , var name , var value ) ) udv = \"\\n\" . join ( udv set ) if self . jmeter ver >= 2.13 : save connect = '<connect Time>true</connect Time>' else : save connect = '' if self . ext log in [ 'errors' , 'all' ] : level map = { 'errors' : 'true' , 'all' : 'false' } tpl resource = 'jmeter writer ext.xml' tpl args = { 'jtl' : self . jtl file , 'udv' : udv , 'ext log' : self . ext log file , 'ext level' : level map [ self . ext log ] , 'save connect' : save connect } else : tpl resource = 'jmeter writer.xml' tpl args = { 'jtl' : self . jtl file , 'udv' : udv , 'save connect' : save connect } tpl = resource string ( name , 'config/' + tpl resource ) try : new jmx = self . core . mkstemp ( '.jmx' , 'modified ' , os . path . dirname ( os . path . realpath ( jmx ) ) ) except OS Error as exc : logger . debug ( \"Can't create modified jmx near original: %s\" , exc ) new jmx = self . core . mkstemp ( '.jmx' , 'modified ' ) logger . debug ( \"Modified JMX: %s\" , new jmx ) with open ( new jmx , \"wb\" ) as fh : fh . write ( '' . join ( source lines ) ) fh . write ( tpl % tpl args ) fh . write ( closing ) return new jmx", "predictions": ["from the = 1 . 5 ."], "references": ["genius idea by alexey lavrenyuk"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 997, "code": "def terminate ( self ) : if self . stderr file : self . stderr file . close ( ) if not self . process : return waitfor = time . time ( ) + PROCESS KILL TIMEOUT while time . time ( ) < waitfor : try : self . process . terminate ( ) except Environment Error as e : if e . errno != errno . ESRCH : LOGGER . warning ( \"Failed to terminate process '{}': {}\" . format ( self . cmd , e ) ) return time . sleep ( 0.1 ) try : self . process . kill ( ) except Environment Error as e : if e . errno != errno . ESRCH : LOGGER . warning ( \"Failed to kill process '{}': {}\" . format ( self . cmd , e ) ) return", "predictions": ["this method blocks until the operation is executed ."], "references": ["gracefull termination of running process"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 998, "code": "def read data ( self , lines ) : results = [ ] for line in lines : timestamp , rps , instances = line . split ( \"\\t\" ) curr ts = int ( float ( timestamp ) ) if self . last ts < curr ts : self . last ts = curr ts results . append ( self . stats item ( self . last ts , float ( rps ) , float ( instances ) ) ) return results", "predictions": ["reads the state of this button by the given amount of bytes ."], "references": ["parse lines and return stats"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 999, "code": "def create criterion ( self , criterion str ) : parsed = criterion str . split ( \"(\" ) type str = parsed [ 0 ] . strip ( ) . lower ( ) parsed [ 1 ] = parsed [ 1 ] . split ( \")\" ) [ 0 ] . strip ( ) for criterion class in self . custom criterions : if criterion class . get type string ( ) == type str : return criterion class ( self , parsed [ 1 ] ) raise Value Error ( \"Unsupported autostop criterion type: %s\" % criterion str )", "predictions": ["this is a convenience for testing for testing ."], "references": ["instantiate criterion from config string"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1000, "code": "def getconfig ( self , filename , target hint ) : try : tree = self . parse xml ( filename ) except IO Error as exc : logger . error ( \"Error loading config: %s\" , exc ) raise Runtime Error ( \"Can't read monitoring config %s\" % filename ) hosts = tree . findall ( 'Host' ) config = [ ] for host in hosts : host config = self . get host config ( host , target hint ) config . append ( host config ) return config", "predictions": ["parse all hosts template links and save them to the date ."], "references": ["prepare config data ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 1001, "code": "def check disk ( self ) : cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \" cmd += self . core . artifacts base dir cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\" res = execute ( cmd , True , 0.1 , True ) logging . debug ( \"Result: %s\" , res ) if not len ( res [ 1 ] ) : self . log . debug ( \"No disk usage info: %s\" , res [ 2 ] ) return disk free = res [ 1 ] self . log . debug ( \"Disk free space: %s/%s\" , disk free . strip ( ) , self . disk limit ) if int ( disk free . strip ( ) ) < self . disk limit : raise Runtime Error ( \"Not enough local resources: disk space less than %s MB in %s: %s MB\" % ( self . disk limit , self . core . artifacts base dir , int ( disk free . strip ( ) ) ) )", "predictions": ["method for forward config entries that are not always shared"], "references": ["raise exception on disk space exceeded"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1002, "code": "def check mem ( self ) : mem free = psutil . virtual memory ( ) . available / 2 ** 20 self . log . debug ( \"Memory free: %s/%s\" , mem free , self . mem limit ) if mem free < self . mem limit : raise Runtime Error ( \"Not enough resources: free memory less \" \"than %s MB: %s MB\" % ( self . mem limit , mem free ) )", "predictions": ["checks the correct memory for uploaded memory ."], "references": ["raise exception on ram exceeded"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1003, "code": "def get terminal size ( ) : default size = ( 30 , 120 ) env = os . environ def ioctl gwinsz ( file d ) : try : sizes = struct . unpack ( 'hh' , fcntl . ioctl ( file d , termios . TIOCGWINSZ , '1234' ) ) except Exception : sizes = default size return sizes sizes = ioctl gwinsz ( 0 ) or ioctl gwinsz ( 1 ) or ioctl gwinsz ( 2 ) if not sizes : try : file d = os . open ( os . ctermid ( ) , os . O RDONLY ) sizes = ioctl gwinsz ( file d ) os . close ( file d . fileno ( ) ) except Exception : pass if not sizes : try : sizes = ( env [ 'LINES' ] , env [ 'COLUMNS' ] ) except Exception : sizes = default size return int ( sizes [ 1 ] ) , int ( sizes [ 0 ] )", "predictions": ["get a info from the current info ."], "references": ["gets width and height of terminal viewport"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1004, "code": "def get right line ( self , widget output ) : right line = '' if widget output : right line = widget output . pop ( 0 ) if len ( right line ) > self . right panel width : right line plain = self . markup . clean markup ( right line ) if len ( right line plain ) > self . right panel width : right line = right line [ : self . right panel width ] + self . markup . RESET return right line", "predictions": ["returns a time to read the current ( i . e . , if it has no effect ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ."], "references": ["gets next line for right panel"], "bleu": 0.02410513685473638, "rouge_l": 0.0}
{"id": 1005, "code": "def truncate ( self , line arr , max width ) : def is space ( chunk ) : return all ( [ True if i == ' ' else False for i in chunk ] ) def is empty ( chunks , markups ) : result = [ ] for chunk in chunks : if chunk in markups : result . append ( True ) elif is space ( chunk ) : result . append ( True ) else : result . append ( False ) return all ( result ) left = max width result = '' markups = self . markup . get markup vars ( ) for num , chunk in enumerate ( line arr ) : if chunk in markups : result += chunk else : if left > 0 : if len ( chunk ) <= left : result += chunk left -= len ( chunk ) else : leftover = ( chunk [ left : ] , ) + line arr [ num + 1 : ] was cut = not is empty ( leftover , markups ) if was cut : result += chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' else : result += chunk [ : left ] left = 0 return result", "predictions": ["print out stats of this object and removes the content of this : pid ."], "references": ["cut tuple of line chunks according to it s wisible lenght"], "bleu": 0.08225964699966554, "rouge_l": 0.07911802853437094}
{"id": 1006, "code": "def render screen ( self ) : self . term width , self . term height = get terminal size ( ) self . log . debug ( \"Terminal size: %sx%s\" , self . term width , self . term height ) self . right panel width = int ( ( self . term width - len ( self . RIGHT PANEL SEPARATOR ) ) * ( float ( self . info panel percent ) / 100 ) ) - 1 if self . right panel width > 0 : self . left panel width = self . term width - self . right panel width - len ( self . RIGHT PANEL SEPARATOR ) - 2 else : self . right panel width = 0 self . left panel width = self . term width - 1 self . log . debug ( \"Left/right panels width: %s/%s\" , self . left panel width , self . right panel width ) widget output = [ ] if self . right panel width : widget output = [ ] self . log . debug ( \"There are %d info widgets\" % len ( self . info widgets ) ) for index , widget in sorted ( self . info widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get index ( ) , item [ 0 ] ) ) : self . log . debug ( \"Rendering info widget #%s: %s\" , index , widget ) widget out = widget . render ( self ) . strip ( ) if widget out : widget output += widget out . split ( \"\\n\" ) widget output += [ \"\" ] left lines = self . render left panel ( ) self . log . debug ( \"Composing final screen output\" ) output = [ ] for line no in range ( 1 , self . term height ) : line = \" \" if line no > 1 and left lines : left line = left lines . pop ( 0 ) left line plain = self . markup . clean markup ( left line ) left line += ( ' ' * ( self . left panel width - len ( left line plain ) ) ) line += left line else : line += ' ' * self . left panel width if self . right panel width : line += self . markup . RESET line += self . markup . WHITE line += self . RIGHT PANEL SEPARATOR line += self . markup . RESET right line = self . get right line ( widget output ) line += right line output . append ( line ) return self . markup . new line . join ( output ) + self . markup . new line", "predictions": ["read a config chunk ."], "references": ["main method to render screen view"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 1007, "code": "def add info widget ( self , widget ) : index = widget . get index ( ) while index in self . info widgets . keys ( ) : index += 1 self . info widgets [ widget . get index ( ) ] = widget", "predictions": ["prepare a ( what happens when this ( i . e . , the ( publish , . publish publish publish publish to the ( publish publish ."], "references": ["add widget string to right panel of the screen"], "bleu": 0.0478968583748614, "rouge_l": 0.11914062499999999}
{"id": 1008, "code": "def fill rectangle ( self , prepared ) : result = [ ] width = max ( [ self . clean len ( line ) for line in prepared ] ) for line in prepared : spacer = ' ' * ( width - self . clean len ( line ) ) result . append ( line + ( self . screen . markup . RESET , spacer ) ) return ( width , result )", "predictions": ["override for derived fields in this model ."], "references": ["right - pad lines of block to equal width"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 1009, "code": "def clean len ( self , line ) : if isinstance ( line , basestring ) : return len ( self . screen . markup . clean markup ( line ) ) elif isinstance ( line , tuple ) or isinstance ( line , list ) : markups = self . screen . markup . get markup vars ( ) length = 0 for i in line : if i not in markups : length += len ( i ) return length", "predictions": ["handles sequences not used by removes sequences from this text ."], "references": ["calculate wisible length of string"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1010, "code": "def add info widget ( self , widget ) : if not self . screen : self . log . debug ( \"No screen instance to add widget\" ) else : self . screen . add info widget ( widget )", "predictions": ["adds a options to the ( options ."], "references": ["add right panel widget"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1011, "code": "def clean markup ( self , orig str ) : for val in self . get markup vars ( ) : orig str = orig str . replace ( val , '' ) return orig str", "predictions": ["cleans strings for each character in the passed string ."], "references": ["clean markup from string"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 1012, "code": "def make writer request ( self , params = None , json = None , http method = \"POST\" , trace = False ) : request = requests . Request ( http method , self . writer url , params = params , json = json , headers = { 'User-Agent' : self . user agent } ) ids = id gen ( str ( uuid . uuid4 ( ) ) ) network timeouts = self . network timeouts ( ) maintenance timeouts = self . maintenance timeouts ( ) while True : try : response = self . send single request ( request , ids . next ( ) , trace = trace ) return response except ( Timeout , Connection Error , Protocol Error ) : logger . warn ( traceback . format exc ( ) ) try : timeout = next ( network timeouts ) logger . warn ( \"Network error, will retry in %ss...\" % timeout ) time . sleep ( timeout ) continue except Stop Iteration : raise self . Network Error ( ) except self . Under Maintenance as e : try : timeout = next ( maintenance timeouts ) logger . warn ( \"Writer is under maintenance, will retry in %ss...\" % timeout ) time . sleep ( timeout ) continue except Stop Iteration : raise e", "predictions": ["create a ( ( ( retry ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["send request to writer service ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1013, "code": "def load plugins ( self ) : logger . info ( \"Loading plugins...\" ) for ( plugin name , plugin path , plugin cfg ) in self . config . plugins : logger . debug ( \"Loading plugin %s from %s\" , plugin name , plugin path ) if plugin path == \"yandextank.plugins.Overload\" : logger . warning ( \"Deprecated plugin name: 'yandextank.plugins.Overload'\\n\" \"There is a new generic plugin now.\\n\" \"Correcting to 'yandextank.plugins.Data Uploader overload'\" ) plugin path = \"yandextank.plugins.Data Uploader overload\" try : plugin = il . import module ( plugin path ) except Import Error : logger . warning ( 'Plugin name %s path %s import error' , plugin name , plugin path ) logger . debug ( 'Plugin name %s path %s import error' , plugin name , plugin path , exc info = True ) raise try : instance = getattr ( plugin , 'Plugin' ) ( self , cfg = plugin cfg , name = plugin name ) except Attribute Error : logger . warning ( 'Plugin %s classname should be `Plugin`' , plugin name ) raise else : self . register plugin ( self . PLUGIN PREFIX + plugin name , instance ) logger . debug ( \"Plugin instances: %s\" , self . plugins )", "predictions": ["loads the given duration and returns them as a duration ."], "references": ["tells core to take plugin options and instantiate plugin classes"], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 1014, "code": "def get plugin of type ( self , plugin class ) : logger . debug ( \"Searching for plugin: %s\" , plugin class ) matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin class ) ] if matches : if len ( matches ) > 1 : logger . debug ( \"More then one plugin of type %s found. Using first one.\" , plugin class ) return matches [ - 1 ] else : raise Key Error ( \"Requested plugin type not found: %s\" % plugin class )", "predictions": ["find the list of [ lastcommittime ] matching some , where we have been previously generated ( but not yet instantiate log log log log log log cmd log log cmd log once per second ."], "references": ["retrieve a plugin of desired class keyerror raised otherwise"], "bleu": 0.03351542279475122, "rouge_l": 0.049836601307189546}
{"id": 1015, "code": "def get plugins of type ( self , plugin class ) : logger . debug ( \"Searching for plugins: %s\" , plugin class ) matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin class ) ] if matches : return matches else : raise Key Error ( \"Requested plugin type not found: %s\" % plugin class )", "predictions": ["find all ( possibly registered log log log ( possibly also log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log log warnings log log log"], "references": ["retrieve a list of plugins of desired class keyerror raised otherwise"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1016, "code": "def collect file ( self , filename , keep original = False ) : dest = self . artifacts dir + '/' + os . path . basename ( filename ) logger . debug ( \"Collecting file: %s to %s\" , filename , dest ) if not filename or not os . path . exists ( filename ) : logger . warning ( \"File not found to collect: %s\" , filename ) return if os . path . exists ( dest ) : logger . warning ( \"File already exists: %s\" , dest ) return if keep original : shutil . copy ( filename , self . artifacts dir ) else : shutil . move ( filename , self . artifacts dir ) os . chmod ( dest , 0o644 )", "predictions": ["collects all accumulated files that have been previously opened ."], "references": ["move or copy single file to artifacts dir"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1017, "code": "def add artifact file ( self , filename , keep original = False ) : if filename : logger . debug ( \"Adding artifact file to collect (keep=%s): %s\" , keep original , filename ) self . artifact files [ filename ] = keep original", "predictions": ["adds a self - period to the database ."], "references": ["add file to be stored as result artifact on post - process phase"], "bleu": 0.10015045110931886, "rouge_l": 0.08802308802308802}
{"id": 1018, "code": "def load files ( self , configs ) : logger . debug ( \"Reading configs: %s\" , configs ) config filenames = [ resource . resource filename ( config ) for config in configs ] try : self . config . read ( config filenames ) except Exception as ex : logger . error ( \"Can't load configs: %s\" , ex ) raise ex", "predictions": ["feed all ( with name / value pairs plan plan of this object ."], "references": ["read configs set into storage"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 1019, "code": "def flush ( self , filename = None ) : if not filename : filename = self . file if filename : with open ( filename , 'w' ) as handle : self . config . write ( handle )", "predictions": ["flushes the file for this file ."], "references": ["flush current stat to file"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1020, "code": "def get options ( self , section , prefix = '' ) : res = [ ] try : for option in self . config . options ( section ) : if not prefix or option . find ( prefix ) == 0 : res += [ ( option [ len ( prefix ) : ] , self . config . get ( section , option ) ) ] except Config Parser . No Section Error as ex : logger . warning ( \"No section: %s\" , ex ) logger . debug ( \"Section: [%s] prefix: '%s' options:\\n%s\" , section , prefix , res ) return res", "predictions": ["green = none . implemented by this call does not include the configuration and : if no more than one instance , then no exception is thrown ."], "references": ["get options list with requested prefix"], "bleu": 0.03639374222382004, "rouge_l": 0.0}
{"id": 1021, "code": "def find sections ( self , prefix ) : res = [ ] for section in self . config . sections ( ) : if section . startswith ( prefix ) : res . append ( section ) return res", "predictions": ["add user user data to this object ."], "references": ["return sections with specified prefix"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1022, "code": "def decode stat data ( self , chunk ) : for date str , statistics in chunk . iteritems ( ) : date obj = datetime . datetime . strptime ( date str . split ( \".\" ) [ 0 ] , '%Y-%m-%d %H:%M:%S' ) chunk date = int ( time . mktime ( date obj . timetuple ( ) ) ) instances = 0 for benchmark name , benchmark in statistics . iteritems ( ) : if not benchmark name . startswith ( \"benchmark io\" ) : continue for method , meth obj in benchmark . iteritems ( ) : if \"mmtasks\" in meth obj : instances += meth obj [ \"mmtasks\" ] [ 2 ] offset = chunk date - 1 - self . start time reqps = 0 if 0 <= offset < len ( self . phantom info . steps ) : reqps = self . phantom info . steps [ offset ] [ 0 ] yield self . stats item ( chunk date - 1 , instances , reqps )", "predictions": ["takes the statistics and returns a generator of all statistics values ."], "references": ["return all items found in this chunk"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 1023, "code": "def prepare ( self ) : agent configs = [ ] if self . config : agent configs = self . config manager . getconfig ( self . config , self . default target ) for config in agent configs : if config [ 'host' ] in [ 'localhost' , '127.0.0.1' , '::1' ] : client = self . clients [ 'localhost' ] ( config , self . old style configs , kill old = self . kill old ) else : client = self . clients [ 'ssh' ] ( config , self . old style configs , timeout = 5 , kill old = self . kill old ) logger . debug ( 'Installing monitoring agent. Host: %s' , client . host ) agent config , startup config , customs script = client . install ( ) if agent config : self . agents . append ( client ) self . artifact files . append ( agent config ) if startup config : self . artifact files . append ( startup config ) if customs script : self . artifact files . append ( customs script )", "predictions": ["we override this to get all items of the sorted set ."], "references": ["prepare for monitoring - install agents etc"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 1024, "code": "def poll ( self ) : start time = time . time ( ) for agent in self . agents : for collect in agent . reader : if not collect : return 0 for chunk in collect : ts , prepared results = chunk if self . load start time and int ( ts ) >= self . load start time : ready to send = { \"timestamp\" : int ( ts ) , \"data\" : { self . hash hostname ( agent . host ) : { \"comment\" : agent . config . comment , \"metrics\" : prepared results } } } self . collected data . append ( ready to send ) logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start time ) * 1000 ) collected data length = len ( self . collected data ) if not self . first data received and self . collected data : self . first data received = True logger . info ( \"Monitoring received first data.\" ) else : self . send collected data ( ) return collected data length", "predictions": ["poll stats for this session ."], "references": ["poll agents for data"], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 1025, "code": "def send collected data ( self ) : data = self . collected data self . collected data = [ ] for listener in self . listeners : listener . monitoring data ( copy . deepcopy ( data ) )", "predictions": ["send a batch of output ."], "references": ["sends pending data set to listeners"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1026, "code": "def decode agents data ( self , block ) : collect = [ ] if block : for chunk in block . split ( '\\n' ) : try : if chunk : prepared results = { } jsn = json . loads ( chunk ) for ts , values in jsn . iteritems ( ) : for key , value in values . iteritems ( ) : try : key group , key name = key . split ( ' ' ) [ 0 ] . split ( '-' ) [ 0 ] , ' ' . join ( key . split ( ' ' ) [ 1 : ] ) except : key group , key name = key . split ( ' ' ) [ 0 ] , ' ' . join ( key . split ( ' ' ) [ 1 : ] ) if key group in decoder . diff metrics . keys ( ) : if key name in decoder . diff metrics [ key group ] : decoded key = decoder . find common names ( key ) if self . prev check : try : value = jsn [ ts ] [ key ] - self . prev check [ key ] except Key Error : logger . debug ( 'There is no diff value for metric %s.\\n' 'Timestamp: %s. Is it initial data?' , key , ts , exc info = True ) value = 0 prepared results [ decoded key ] = value else : decoded key = decoder . find common names ( key ) prepared results [ decoded key ] = value else : decoded key = decoder . find common names ( key ) prepared results [ decoded key ] = value self . prev check = jsn [ ts ] collect . append ( ( ts , prepared results ) ) except Value Error : logger . error ( 'Telegraf agent send trash to output: %s' , chunk ) logger . debug ( 'Telegraf agent data block w/ trash: %s' , exc info = True ) return [ ] except Base Exception : logger . error ( 'Exception trying to parse agent data: %s' , chunk , exc info = True ) return [ ] if collect : return collect", "predictions": ["decode a single json ."], "references": ["decode agents jsons count diffs"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1027, "code": "async def close ( self ) : if self . ws is not None : await self . ws . close ( ) if self . polygon is not None : await self . polygon . close ( )", "predictions": ["close this packet and release the underlying output ."], "references": ["close any of open connections"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 1028, "code": "def submit order ( self , symbol , qty , side , type , time in force , limit price = None , stop price = None , client order id = None ) : params = { 'symbol' : symbol , 'qty' : qty , 'side' : side , 'type' : type , 'time in force' : time in force , } if limit price is not None : params [ 'limit price' ] = limit price if stop price is not None : params [ 'stop price' ] = stop price if client order id is not None : params [ 'client order id' ] = client order id resp = self . post ( '/orders' , params ) return Order ( resp )", "predictions": ["submits a order to the server ."], "references": ["request a new order"], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 1029, "code": "def get position ( self , symbol ) : resp = self . get ( '/positions/{}' . format ( symbol ) ) return Position ( resp )", "predictions": ["get the position of this object ."], "references": ["get an open position"], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 1030, "code": "def list assets ( self , status = None , asset class = None ) : params = { 'status' : status , 'assert class' : asset class , } resp = self . get ( '/assets' , params ) return [ Asset ( o ) for o in resp ]", "predictions": ["list assets with asset content ."], "references": ["get a list of assets"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 1031, "code": "def construct event logger ( event record callback ) : check . callable param ( event record callback , 'event record callback' ) return construct single handler logger ( 'event-logger' , DEBUG , Structured Logger Handler ( lambda logger message : event record callback ( construct event record ( logger message ) ) ) , )", "predictions": ["constructs an event to be executed on the server ."], "references": ["callback receives a stream of event_records"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1032, "code": "def construct json event logger ( json path ) : check . str param ( json path , 'json path' ) return construct single handler logger ( \"json-event-record-logger\" , DEBUG , Json Event Logger Handler ( json path , lambda record : construct event record ( Structured Logger Message ( name = record . name , message = record . msg , level = record . levelno , meta = record . dagster meta , record = record , ) ) , ) , )", "predictions": ["constructs an instance of the appropriate json object from the given configuration ."], "references": ["record a stream of event records to json"], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 1033, "code": "def format config for graphql ( config ) : def format config subdict ( config , current indent = 0 ) : check . dict param ( config , 'config' , key type = str ) printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) printer . line ( '{' ) n elements = len ( config ) for i , key in enumerate ( sorted ( config , key = lambda x : x [ 0 ] ) ) : value = config [ key ] with printer . with indent ( ) : formatted value = ( format config item ( value , current indent = printer . current indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) printer . line ( '{key}: {formatted value}{comma}' . format ( key = key , formatted value = formatted value , comma = ',' if i != n elements - 1 else '' , ) ) printer . line ( '}' ) return printer . read ( ) def format config sublist ( config , current indent = 0 ) : printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) printer . line ( '[' ) n elements = len ( config ) for i , value in enumerate ( config ) : with printer . with indent ( ) : formatted value = ( format config item ( value , current indent = printer . current indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) printer . line ( '{formatted value}{comma}' . format ( formatted value = formatted value , comma = ',' if i != n elements - 1 else '' ) ) printer . line ( ']' ) return printer . read ( ) def format config item ( config , current indent = 0 ) : printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) if isinstance ( config , dict ) : return format config subdict ( config , printer . current indent ) elif isinstance ( config , list ) : return format config sublist ( config , printer . current indent ) elif isinstance ( config , bool ) : return repr ( config ) . lower ( ) else : return repr ( config ) . replace ( '\\'' , '\"' ) check . dict param ( config , 'config' , key type = str ) if not isinstance ( config , dict ) : check . failed ( 'Expected a dict to format as config, got: {item}' . format ( item = repr ( config ) ) ) return format config subdict ( config )", "predictions": ["format the config according to this function ."], "references": ["this recursive descent thing formats a config dict for graphql ."], "bleu": 0.13107175678306446, "rouge_l": 0.20469798657718125}
{"id": 1034, "code": "def execute pipeline through queue ( repository info , pipeline name , solid subset , environment dict , run id , message queue , reexecution config , step keys to execute , ) : message queue . put ( Process Started Sentinel ( os . getpid ( ) ) ) run config = Run Config ( run id , event callback = message queue . put , executor config = In Process Executor Config ( raise on error = False ) , reexecution config = reexecution config , step keys to execute = step keys to execute , ) repository container = Repository Container ( repository info ) if repository container . repo error : message queue . put ( Multiprocessing Error ( serializable error info from exc info ( repository container . repo error ) ) ) return try : result = execute pipeline ( repository container . repository . get pipeline ( pipeline name ) . build sub pipeline ( solid subset ) , environment dict , run config = run config , ) return result except : error info = serializable error info from exc info ( sys . exc info ( ) ) message queue . put ( Multiprocessing Error ( error info ) ) finally : message queue . put ( Multiprocessing Done ( ) ) message queue . close ( )", "predictions": ["executes the reexecution on the repository ."], "references": ["execute pipeline using message queue as a transport"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 1035, "code": "def join ( self ) : while True : with self . processes lock : if not self . processes and self . processing semaphore . locked ( ) : return True gevent . sleep ( 0.1 )", "predictions": ["requests a full join of this clip to the join ."], "references": ["waits until all there are no processes enqueued ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 1036, "code": "def build ( self , pipeline def , artifacts persisted ) : deps = { step . key : set ( ) for step in self . steps } for step in self . steps : for step input in step . step inputs : deps [ step . key ] . add ( step input . prev output handle . step key ) step dict = { step . key : step for step in self . steps } return Execution Plan ( pipeline def , step dict , deps , artifacts persisted )", "predictions": ["takes a pipeline and returns a pipeline of it as an array of ( ."], "references": ["builds the execution plan ."], "bleu": 0.08225964699966554, "rouge_l": 0.10990990990990988}
{"id": 1037, "code": "def construct publish comands ( additional steps = None , nightly = False ) : publish commands = ( [ 'rm -rf dist' ] + ( additional steps if additional steps else [ ] ) + [ 'python setup.py sdist bdist wheel{nightly}' . format ( nightly = ' --nightly' if nightly else '' ) , 'twine upload dist/*' , ] ) return publish commands", "predictions": ["publish this package from the set of ( commands ."], "references": ["get the shell commands we ll use to actually build and publish a package to pypi ."], "bleu": 0.08238542375473351, "rouge_l": 0.21229698375870074}
{"id": 1038, "code": "def block ( self , text , prefix = '' ) : wrapper = Text Wrapper ( width = self . line length - len ( self . current indent str ) , initial indent = prefix , subsequent indent = prefix , break long words = False , break on hyphens = False , ) for line in wrapper . wrap ( text ) : self . line ( line )", "predictions": ["create a multiple text of the text ."], "references": ["automagically wrap a block of text ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 1039, "code": "def define shared fields ( ) : clustering fields = Field ( List ( String ) , description = , is optional = True , ) create disposition = Field ( BQ Create Disposition , description = , is optional = True , ) destination encryption configuration = Field ( String , description = , is optional = True , ) schema update options = Field ( List ( BQ Schema Update Option ) , description = , is optional = True , ) time partitioning = Field ( Dict ( fields = { 'expiration ms' : Field ( Int , description = , is optional = True , ) , 'field' : Field ( String , description = , is optional = True , ) , 'require partition filter' : Field ( Bool , description = , is optional = True , ) , } ) , description = 'Specifies time-based partitioning for the destination table.' , is optional = True , ) write disposition = Field ( BQ Write Disposition , description = , is optional = True , ) return { 'clustering fields' : clustering fields , 'create disposition' : create disposition , 'destination encryption configuration' : destination encryption configuration , 'schema update options' : schema update options , 'time partitioning' : time partitioning , 'write disposition' : write disposition , }", "predictions": ["this is called when the shared clustering is available ."], "references": ["the following fields are shared between both queryjobconfig and loadjobconfig ."], "bleu": 0.13564514503163538, "rouge_l": 0.28328173374613}
{"id": 1040, "code": "def mkdir p ( newdir , mode = 0o777 ) : try : os . makedirs ( newdir , mode ) except OS Error as err : if err . errno != errno . EEXIST or not os . path . isdir ( newdir ) : raise", "predictions": ["creates a directory if it does not exist ."], "references": ["the missing mkdir - p functionality in os ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 1041, "code": "def success ( self ) : any success = False for step event in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) : if step event . event type == Dagster Event Type . STEP FAILURE : return False if step event . event type == Dagster Event Type . STEP SUCCESS : any success = True return any success", "predictions": ["begins each object in the log ."], "references": ["whether the solid execution was successful"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1042, "code": "def skipped ( self ) : return all ( [ step event . event type == Dagster Event Type . STEP SKIPPED for step event in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) ] )", "predictions": ["skipped the event to all child nodes ."], "references": ["whether the solid execution was skipped"], "bleu": 0.17747405280050269, "rouge_l": 0.14663461538461536}
{"id": 1043, "code": "def failure data ( self ) : for result in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) : if result . event type == Dagster Event Type . STEP FAILURE : return result . step failure data", "predictions": ["get the failure to the failure of this cipher ."], "references": ["returns the failing step s data that happened during this solid s execution if any"], "bleu": 0.08461586088475063, "rouge_l": 0.15443037974683543}
{"id": 1044, "code": "def is valid dataset ( config value ) : return re . match ( r'^' + RE PROJECT + r'\\.' + RE DS TABLE + r'$|^' + RE DS TABLE + r'$' , config value , )", "predictions": ["check if dataset is valid ."], "references": ["datasets must be of form project . dataset or dataset"], "bleu": 0.1255107248036171, "rouge_l": 0.11960784313725491}
{"id": 1045, "code": "def is valid table ( config value ) : return re . match ( r'^' + RE PROJECT + r'\\.' + RE DS TABLE + r'\\.' + RE DS TABLE + r'$|^' + RE DS TABLE + r'\\.' + RE DS TABLE + r'$' , config value , )", "predictions": ["validate if this table name is valid for this table"], "references": ["tables must be of form project . dataset . table or dataset . table"], "bleu": 0.09351498865776114, "rouge_l": 0.16180371352785147}
{"id": 1046, "code": "def coalesce execution steps ( execution plan ) : solid order = coalesce solid order ( execution plan ) steps = defaultdict ( list ) for solid name , solid steps in itertools . groupby ( execution plan . topological steps ( ) , lambda x : x . solid name ) : steps [ solid name ] += list ( solid steps ) return Ordered Dict ( [ ( solid name , steps [ solid name ] ) for solid name in solid order ] )", "predictions": ["coalesce an individual plan by plan ."], "references": ["groups execution steps by solid in topological order of the solids ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 1047, "code": "def create cursor ( self , name = None ) : return Cursor ( self . client connection , self . connection , self . djongo connection )", "predictions": ["create the connection to the database ."], "references": ["returns an active connection cursor to the database ."], "bleu": 0.40661103887968814, "rouge_l": 0.6112224448897796}
{"id": 1048, "code": "def close ( self ) : if self . connection : with self . wrap database errors : self . connection . client . close ( )", "predictions": ["closes the request and closes the database ."], "references": ["closes the client connection to the database ."], "bleu": 0.3549481056010052, "rouge_l": 0.625}
{"id": 1049, "code": "def make mdl ( model , model dict ) : for field name in model dict : field = model . meta . get field ( field name ) model dict [ field name ] = field . to python ( model dict [ field name ] ) return model ( * * model dict )", "predictions": ["creates a model with the given parameters ."], "references": ["builds an instance of model from the model_dict ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 1050, "code": "def formfield ( self , * * kwargs ) : defaults = { 'form class' : Array Form Field , 'model container' : self . model container , 'model form class' : self . model form class , 'name' : self . attname , 'mdl form kw l' : self . model form kwargs l } defaults . update ( kwargs ) return super ( ) . formfield ( * * defaults )", "predictions": [". for a full formfield ."], "references": ["returns the formfield for the array ."], "bleu": 0.22236312185643822, "rouge_l": 0.3034825870646766}
{"id": 1051, "code": "def apply rel filters ( self , queryset ) : queryset . add hints ( instance = self . instance ) if self . db : queryset = queryset . using ( self . db ) queryset = queryset . filter ( * * self . core filters ) return queryset", "predictions": ["apply the given filters to the database ."], "references": ["filter the queryset for the instance this manager is bound to ."], "bleu": 0.1223065774797558, "rouge_l": 0.28955696202531644}
{"id": 1052, "code": "def calc c ( self , a1 , a2 , r1 , r2 ) : if r1 == 0.0 and r2 == 0.0 : return a1 , a2 div = 1 / ( r1 + r2 ) c1 = ( a1 * r2 + a2 * r1 ) * div c2 = ( a1 * r1 + a2 * r2 ) * div return c1 , c2", "predictions": ["calculates the positions of each pair of points based on the positions of the chart ."], "references": ["compute the functions c1 and c2"], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 1053, "code": "def clear ( self ) : self . reg = np . zeros ( ( self . m , ) , dtype = np . int8 )", "predictions": ["clears the list of . ."], "references": ["reset the current hyperloglog to empty ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1054, "code": "def index ( self ) : for i , hashtable in enumerate ( self . hashtables ) : self . sorted hashtables [ i ] = [ H for H in hashtable . keys ( ) ] self . sorted hashtables [ i ] . sort ( )", "predictions": ["creates a new instance ."], "references": ["index all the keys added so far and make them searchable ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 1055, "code": "async def close ( self ) : async with self . lock : for t in self . hashtables : await t . close ( ) if self . keys is not None : await self . keys . close ( ) self . initialized = False", "predictions": ["closes the output of this cipher ."], "references": ["cleanup client resources and disconnect from asyncminhashlsh storage ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1056, "code": "def parse scoped selector ( scoped selector ) : if scoped selector [ 0 ] == '%' : if scoped selector . endswith ( '.value' ) : err str = '{} is invalid cannot use % and end with .value' raise Value Error ( err str . format ( scoped selector ) ) scoped selector = scoped selector [ 1 : ] + '/macro.value' scope selector list = scoped selector . rsplit ( '/' , 1 ) scope = '' . join ( scope selector list [ : - 1 ] ) selector = scope selector list [ - 1 ] return scope , selector", "predictions": ["poll a self self self self ."], "references": ["parse scoped selector ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1057, "code": "def advance one line ( self ) : current line = self . current token . line number while current line == self . current token . line number : self . current token = Config Parser . Token ( * next ( self . token generator ) )", "predictions": ["a method to send the current data for each data ."], "references": ["advances to next line ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 1058, "code": "def augment exception message and reraise ( exception , message ) : class Exception Proxy ( type ( exception ) ) : \"\"\"Acts as a proxy for an exception with an augmented message.\"\"\" module = type ( exception ) . module def init ( self ) : pass def getattr ( self , attr name ) : return getattr ( exception , attr name ) def str ( self ) : return str ( exception ) + message Exception Proxy . name = type ( exception ) . name proxy = Exception Proxy ( ) if six . PY3 : Exception Proxy . qualname = type ( exception ) . qualname six . raise from ( proxy . with traceback ( exception . traceback ) , None ) else : six . reraise ( proxy , None , sys . exc info ( ) [ 2 ] )", "predictions": ["initializes the given agents by trying to get the given data . the method overrides the method to get the in the proxy object ."], "references": ["reraises exception appending message to its string representation ."], "bleu": 0.05377336385080629, "rouge_l": 0.1285563751317176}
{"id": 1059, "code": "def markdownify operative config str ( self , string ) : def process ( line ) : \"\"\"Convert a single line to markdown format.\"\"\" if not line . startswith ( '#' ) : return '    ' + line line = line [ 2 : ] if line . startswith ( '====' ) : return '' if line . startswith ( 'None' ) : return if line . endswith ( ':' ) : return + line return line output lines = [ ] for line in string . splitlines ( ) : procd line = process ( line ) if procd line is not None : output lines . append ( procd line ) return '\\n' . join ( output lines )", "predictions": ["read def info from ( configuration file ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ws ( ws ws"], "references": ["convert an operative config string to markdown format ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1060, "code": "def after create session ( self , session = None , coord = None ) : config str = config . operative config str ( ) if not tf . gfile . Is Directory ( self . output dir ) : tf . gfile . Make Dirs ( self . output dir ) global step val = 0 if session is not None : global step = tf . train . get global step ( ) if global step is not None : global step val = session . run ( global step ) filename = '%s-%s.gin' % ( self . base name , global step val ) config path = os . path . join ( self . output dir , filename ) with tf . gfile . G File ( config path , 'w' ) as f : f . write ( config str ) if self . summarize config : md config str = self . markdownify operative config str ( config str ) summary metadata = summary pb2 . Summary Metadata ( ) summary metadata . plugin data . plugin name = 'text' summary metadata . plugin data . content = b'{}' text tensor = tf . make tensor proto ( md config str ) summary = summary pb2 . Summary ( ) summary . value . add ( tag = 'gin/' + self . base name , tensor = text tensor , metadata = summary metadata ) if not self . summary writer : self . summary writer = tf . summary . File Writer Cache . get ( self . output dir ) self . summary writer . add summary ( summary , global step val ) self . summary writer . flush ( )", "predictions": ["creates a summary . this is used for cleaning everything ."], "references": ["writes out gin s operative config and maybe adds a summary of it ."], "bleu": 0.12263782424860158, "rouge_l": 0.2349165596919127}
{"id": 1061, "code": "def find class construction fn ( cls ) : for base in type . mro ( cls ) : if ' init ' in base . dict : return base . init if ' new ' in base . dict : return base . new", "predictions": ["get the full path to the class ."], "references": ["find the first __init__ or __new__ method in the given class s mro ."], "bleu": 0.09525245831601728, "rouge_l": 0.346590909090909}
{"id": 1062, "code": "def ensure wrappability ( fn ) : if isinstance ( fn , ( type ( object . init ) , type ( object . call ) ) ) : wrappable fn = lambda * args , * * kwargs : fn ( * args , * * kwargs ) wrappable fn . name = fn . name wrappable fn . doc = fn . doc wrappable fn . module = '' wrappable fn . wrapped = fn return wrappable fn return fn", "predictions": ["ensures that the function is properly used in the arguments of the given function ."], "references": ["make sure fn can be wrapped cleanly by functools . wraps ."], "bleu": 0.08225964699966554, "rouge_l": 0.07558859975216851}
{"id": 1063, "code": "def get cached arg spec ( fn ) : arg spec = ARG SPEC CACHE . get ( fn ) if arg spec is None : arg spec fn = inspect . getfullargspec if six . PY3 else inspect . getargspec try : arg spec = arg spec fn ( fn ) except Type Error : arg spec = arg spec fn ( fn . call ) ARG SPEC CACHE [ fn ] = arg spec return arg spec", "predictions": ["gets the function ( s ) of the given argument ."], "references": ["gets cached argspec for fn ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 1064, "code": "def get supplied positional parameter names ( fn , args ) : arg spec = get cached arg spec ( fn ) return arg spec . args [ : len ( args ) ]", "predictions": ["gets the cache of this project ."], "references": ["returns the names of the supplied arguments to the given function ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 1065, "code": "def get all positional parameter names ( fn ) : arg spec = get cached arg spec ( fn ) args = arg spec . args if arg spec . defaults : args = args [ : - len ( arg spec . defaults ) ] return args", "predictions": ["gets the arguments of this node ."], "references": ["returns the names of all positional arguments to the given function ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 1066, "code": "def parse value ( value ) : if not isinstance ( value , six . string types ) : raise Value Error ( 'value ({}) should be a string type.' . format ( value ) ) return config parser . Config Parser ( value , Parser Delegate ( ) ) . parse value ( )", "predictions": ["execute a value from command line ."], "references": ["parse and return a single gin value ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 1067, "code": "def iterate flattened values ( value ) : if isinstance ( value , six . string types ) : yield value return if isinstance ( value , collections . Mapping ) : value = collections . Values View ( value ) if isinstance ( value , collections . Iterable ) : for nested value in value : for nested nested value in iterate flattened values ( nested value ) : yield nested nested value yield value", "predictions": ["join a ( or a nested while maintaining the : nested while consuming the : : nested . , while a nested . nested )"], "references": ["provides an iterator over all values in a nested structure ."], "bleu": 0.06871624004919695, "rouge_l": 0.17923604309500493}
{"id": 1068, "code": "def get all matches ( self , partial selector ) : matching selectors = self . matching selectors ( partial selector ) return [ self . selector map [ selector ] for selector in matching selectors ]", "predictions": ["build a list of ) prefix to the ) ."], "references": ["returns all values matching partial_selector as a list ."], "bleu": 0.17827531042796255, "rouge_l": 0.31881533101045295}
{"id": 1069, "code": "def sp search query ( query ) : result = [ ] for ( field , values ) in query . items ( ) : field = SEARCH FIELD MAP . get ( field , field ) if field is None : continue for value in values : if field == 'year' : value = transform year ( value ) if value is not None : result . append ( '%s:%d' % ( field , value ) ) elif field == 'any' : result . append ( '\"%s\"' % value ) else : result . append ( '%s:\"%s\"' % ( field , value ) ) return ' ' . join ( result )", "predictions": ["publish publish publish from a query query ."], "references": ["translate a mopidy search query to a spotify search query"], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 1070, "code": "def parse retry after ( self , response ) : value = response . headers . get ( 'Retry-After' ) if not value : seconds = 0 elif re . match ( r'^\\s*[0-9]+\\s*$' , value ) : seconds = int ( value ) else : date tuple = email . utils . parsedate ( value ) if date tuple is None : seconds = 0 else : seconds = time . mktime ( date tuple ) - time . time ( ) return max ( 0 , seconds )", "predictions": ["attempts to block the http = . where the http header can be filled . if the request has already been written , it will always be automatically applied to the caller ."], "references": ["parse retry - after header from response if it is set ."], "bleu": 0.04603799154398036, "rouge_l": 0.19411296738265713}
{"id": 1071, "code": "def set default headers ( self , * args , * * kwargs ) : self . set header ( 'Access-Control-Allow-Origin' , '*' ) self . set header ( 'Access-Control-Allow-Headers' , 'Origin, X-Requested-With, Content-Type, Accept' ) self . set header ( 'Access-Control-Allow-Methods' , 'GET, HEAD, PUT, POST, DELETE' )", "predictions": ["sets the request parameters ."], "references": ["set the default headers for all requests ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1072, "code": "def prepare ( self ) : host = self . request . headers . get ( 'Host' , None ) if host is not None and host in self . hosts : return raise tornado . web . HTTP Error ( 403 )", "predictions": ["prepares the ) for use in creating the tornado ."], "references": ["validate host header ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 1073, "code": "def start ( self ) : self . service info = Service Info ( ' webthing. tcp.local.' , '{}. webthing. tcp.local.' . format ( self . name ) , address = socket . inet aton ( get ip ( ) ) , port = self . port , properties = { 'path' : '/' , } , server = '{}.local.' . format ( socket . gethostname ( ) ) ) self . zeroconf = Zeroconf ( ) self . zeroconf . register service ( self . service info ) self . server . listen ( self . port ) tornado . ioloop . IO Loop . current ( ) . start ( )", "predictions": ["creates a new instance of the managed = ."], "references": ["start listening for incoming connections ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1074, "code": "def start ( self ) : self . status = 'pending' self . thing . action notify ( self ) self . perform action ( ) self . finish ( )", "predictions": ["skipped the user to finish and notify the underlying progress bar ."], "references": ["start performing the action ."], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 1075, "code": "def finish ( self ) : self . status = 'completed' self . time completed = timestamp ( ) self . thing . action notify ( self )", "predictions": ["call this method to failure to failure to commit the underlying contact ."], "references": ["finish performing the action ."], "bleu": 0.10571070857151538, "rouge_l": 0.24158415841584158}
{"id": 1076, "code": "def update ( self , * * fields ) : self . for write = True if django . VERSION >= ( 2 , 0 ) : query = self . query . chain ( Update Query ) else : query = self . query . clone ( Update Query ) query . annotations = None query . add update values ( fields ) connection = django . db . connections [ self . db ] compiler = Postgres Returning Update Compiler ( query , connection , self . db ) with transaction . atomic ( using = self . db , savepoint = False ) : rows = compiler . execute sql ( CURSOR ) self . result cache = None for row in rows : signals . update . send ( self . model , pk = row [ 0 ] ) return len ( rows )", "predictions": ["is the ( rules that should have different values ."], "references": ["updates all rows that match the filter ."], "bleu": 0.14991106946711685, "rouge_l": 0.22676579925650556}
{"id": 1077, "code": "def on model save ( sender , * * kwargs ) : created , instance = kwargs [ 'created' ] , kwargs [ 'instance' ] if created : signals . create . send ( sender , pk = instance . pk ) else : signals . update . send ( sender , pk = instance . pk )", "predictions": [", avoid using the ui thread . this is called when the valid event is saved ."], "references": ["when a model gets created or updated ."], "bleu": 0.07994607499472013, "rouge_l": 0.17110799438990182}
{"id": 1078, "code": "def on model delete ( sender , * * kwargs ) : instance = kwargs [ 'instance' ] signals . delete . send ( sender , pk = instance . pk )", "predictions": ["this is called to remove the execution time of a metadata ."], "references": ["when a model gets deleted ."], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 1079, "code": "def resolve expression ( self , * args , * * kwargs ) : result = dict ( ) for key , value in self . value . items ( ) : if hasattr ( value , 'resolve expression' ) : result [ key ] = value . resolve expression ( * args , * * kwargs ) else : result [ key ] = value return H Store Value ( result )", "predictions": ["resolves all cursor from cursor ."], "references": ["resolves expressions inside the dictionary ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1080, "code": "def as sql ( self , compiler , connection ) : qn = compiler . quote name unless alias return \"%s.%s->'%s'\" % ( qn ( self . alias ) , qn ( self . target . column ) , self . hstore key ) , [ ]", "predictions": ["create a if this if an existing ."], "references": ["compiles this expression into sql ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1081, "code": "def relabeled clone ( self , relabels ) : return self . class ( relabels . get ( self . alias , self . alias ) , self . target , self . hstore key , self . output field )", "predictions": ["returns a mdl to the resource ."], "references": ["gets a re - labeled clone of this expression ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1082, "code": "def as sql ( self , compiler , connection ) : sql , params = super ( ) . as sql ( compiler , connection ) return 'EXTRACT(epoch FROM {})' . format ( sql ) , params", "predictions": ["create a new ( ."], "references": ["compiles this expression into sql ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1083, "code": "def create model ( self , model ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue self . add field ( model , field )", "predictions": ["creates a base rel of the given rel ."], "references": ["ran when a new model is created ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 1084, "code": "def delete model ( self , model ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue self . remove field ( model , field )", "predictions": ["deletes a model from the specified c ."], "references": ["ran when a model is being deleted ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 1085, "code": "def alter db table ( self , model , old db table , new db table ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue for key in self . iterate required keys ( field ) : self . rename hstore required ( old db table , new db table , field , field , key )", "predictions": ["clear a self - based self for the given self ."], "references": ["ran when the name of a model is changed ."], "bleu": 0.1354599427337814, "rouge_l": 0.1921259842519685}
{"id": 1086, "code": "def add field ( self , model , field ) : for key in self . iterate required keys ( field ) : self . create hstore required ( model . meta . db table , field , key )", "predictions": ["hashtables is not serializable yet ."], "references": ["ran when a field is added to a model ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 1087, "code": "def remove field ( self , model , field ) : for key in self . iterate required keys ( field ) : self . drop hstore required ( model . meta . db table , field , key )", "predictions": ["def def close to this close model"], "references": ["ran when a field is removed from a model ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 1088, "code": "def alter field ( self , model , old field , new field , strict = False ) : is old field hstore = isinstance ( old field , H Store Field ) is new field hstore = isinstance ( new field , H Store Field ) if not is old field hstore and not is new field hstore : return old required = getattr ( old field , 'required' , [ ] ) or [ ] new required = getattr ( new field , 'required' , [ ] ) or [ ] if str ( old field . column ) != str ( new field . column ) : for key in self . iterate required keys ( old field ) : self . rename hstore required ( model . meta . db table , model . meta . db table , old field , new field , key ) for key in old required : if key not in new required : self . drop hstore required ( model . meta . db table , old field , key ) for key in new required : if key not in old required : self . create hstore required ( model . meta . db table , new field , key )", "predictions": ["this function can be called to alter a field . it will only call ( ( ) to get the field that has been called ."], "references": ["ran when the configuration on a field changed ."], "bleu": 0.06980361417366379, "rouge_l": 0.1878850102669405}
{"id": 1089, "code": "def create hstore required ( self , table name , field , key ) : name = self . required constraint name ( table name , field , key ) sql = self . sql hstore required create . format ( name = self . quote name ( name ) , table = self . quote name ( table name ) , field = self . quote name ( field . column ) , key = key ) self . execute ( sql )", "predictions": ["creates a table in the table ."], "references": ["creates a required constraint for the specified hstore key ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 1090, "code": "def drop hstore required ( self , table name , field , key ) : name = self . required constraint name ( table name , field , key ) sql = self . sql hstore required drop . format ( table = self . quote name ( table name ) , name = self . quote name ( name ) ) self . execute ( sql )", "predictions": ["drop database from db"], "references": ["drops a required constraint for the specified hstore key ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1091, "code": "def create sql ( self , model , schema editor , using = '' ) : if django . VERSION >= ( 2 , 0 ) : statement = super ( ) . create sql ( model , schema editor , using ) statement . template = self . sql create index statement . parts [ 'condition' ] = self . condition return statement else : sql create index = self . sql create index sql parameters = { * * Index . get sql create template values ( self , model , schema editor , using ) , 'condition' : self . condition } return sql create index % sql parameters", "predictions": ["create a statement for a model and return the result ."], "references": ["creates the actual sql used when applying the migration ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 1092, "code": "def create command ( text , commands ) : class Custom Command ( Base Command ) : description = text def run ( self ) : for cmd in commands : subprocess . check call ( cmd ) return Custom Command", "predictions": ["creates a command . this method builds the command to be called for the command line ."], "references": ["creates a custom setup . py command ."], "bleu": 0.12512236921161915, "rouge_l": 0.4277699859747546}
{"id": 1093, "code": "def create model ( self , model ) : super ( ) . create model ( model ) for mixin in self . post processing mixins : mixin . create model ( model )", "predictions": ["creates a new model of this class ."], "references": ["ran when a new model is created ."], "bleu": 0.3155984539112945, "rouge_l": 0.5}
{"id": 1094, "code": "def delete model ( self , model ) : for mixin in self . post processing mixins : mixin . delete model ( model ) super ( ) . delete model ( model )", "predictions": ["deletes a dynamic model"], "references": ["ran when a model is being deleted ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 1095, "code": "def alter db table ( self , model , old db table , new db table ) : super ( Schema Editor , self ) . alter db table ( model , old db table , new db table ) for mixin in self . post processing mixins : mixin . alter db table ( model , old db table , new db table )", "predictions": ["alter the ( in a db ."], "references": ["ran when the name of a model is changed ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 1096, "code": "def add field ( self , model , field ) : super ( Schema Editor , self ) . add field ( model , field ) for mixin in self . post processing mixins : mixin . add field ( model , field )", "predictions": ["this is a helper method for creating a single entry ."], "references": ["ran when a field is added to a model ."], "bleu": 0.14323145079400493, "rouge_l": 0.28818897637795277}
{"id": 1097, "code": "def remove field ( self , model , field ) : for mixin in self . post processing mixins : mixin . remove field ( model , field ) super ( Schema Editor , self ) . remove field ( model , field )", "predictions": ["removes the mapping from this model ."], "references": ["ran when a field is removed from a model ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 1098, "code": "def alter field ( self , model , old field , new field , strict = False ) : super ( Schema Editor , self ) . alter field ( model , old field , new field , strict ) for mixin in self . post processing mixins : mixin . alter field ( model , old field , new field , strict )", "predictions": ["makes a field for a base field ."], "references": ["ran when the configuration on a field changed ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 1099, "code": "def form returning ( self ) : qn = self . connection . ops . quote name return ' RETURNING %s' % qn ( self . query . model . meta . pk . attname )", "predictions": ["the returning a form of the game ."], "references": ["builds the returning part of the query ."], "bleu": 0.2777619034011791, "rouge_l": 0.625}
{"id": 1100, "code": "def as sql ( self , return id = False ) : queries = [ self . rewrite insert ( sql , params , return id ) for sql , params in super ( ) . as sql ( ) ] return queries", "predictions": ["returns a list of queries that can be passed to this permission ."], "references": ["builds the sql insert statement ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 1101, "code": "def alter db table ( self , model , old db table , new db table ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue for keys in self . iterate uniqueness keys ( field ) : self . rename hstore unique ( old db table , new db table , field , field , keys )", "predictions": ["alter a table in a model ."], "references": ["ran when the name of a model is changed ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 1102, "code": "def add field ( self , model , field ) : for keys in self . iterate uniqueness keys ( field ) : self . create hstore unique ( model , field , keys )", "predictions": ["adds a single field to this container ."], "references": ["ran when a field is added to a model ."], "bleu": 0.157044754112095, "rouge_l": 0.43571428571428567}
{"id": 1103, "code": "def remove field ( self , model , field ) : for keys in self . iterate uniqueness keys ( field ) : self . drop hstore unique ( model , field , keys )", "predictions": ["remove all field referenced by a model"], "references": ["ran when a field is removed from a model ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 1104, "code": "def alter field ( self , model , old field , new field , strict = False ) : is old field hstore = isinstance ( old field , H Store Field ) is new field hstore = isinstance ( new field , H Store Field ) if not is old field hstore and not is new field hstore : return old uniqueness = getattr ( old field , 'uniqueness' , [ ] ) or [ ] new uniqueness = getattr ( new field , 'uniqueness' , [ ] ) or [ ] if str ( old field . column ) != str ( new field . column ) : for keys in self . iterate uniqueness keys ( old field ) : self . rename hstore unique ( model . meta . db table , model . meta . db table , old field , new field , keys ) for keys in old uniqueness : if keys not in new uniqueness : self . drop hstore unique ( model , old field , self . compose keys ( keys ) ) for keys in new uniqueness : if keys not in old uniqueness : self . create hstore unique ( model , new field , self . compose keys ( keys ) )", "predictions": ["we need to pass in a field that has already been unique ."], "references": ["ran when the configuration on a field changed ."], "bleu": 0.1350862565735141, "rouge_l": 0.2819722650231125}
{"id": 1105, "code": "def create hstore unique ( self , model , field , keys ) : name = self . unique constraint name ( model . meta . db table , field , keys ) columns = [ '(%s->\\'%s\\')' % ( field . column , key ) for key in keys ] sql = self . sql hstore unique create . format ( name = self . quote name ( name ) , table = self . quote name ( model . meta . db table ) , columns = ',' . join ( columns ) ) self . execute ( sql )", "predictions": ["create a unique table of keys ."], "references": ["creates a unique constraint for the specified hstore keys ."], "bleu": 0.20024850746991507, "rouge_l": 0.45607476635514016}
{"id": 1106, "code": "def drop hstore unique ( self , model , field , keys ) : name = self . unique constraint name ( model . meta . db table , field , keys ) sql = self . sql hstore unique drop . format ( name = self . quote name ( name ) ) self . execute ( sql )", "predictions": ["drop the table from the database ."], "references": ["drops a unique constraint for the specified hstore keys ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1107, "code": "def as sql ( self , compiler , connection ) -> Tuple [ str , List [ Any ] ] : sql , params = super ( ) . as sql ( compiler , connection ) qn = compiler . quote name unless alias extra conditions = ' AND ' . join ( [ '{}.{} = %s' . format ( qn ( self . table name ) , qn ( field . column ) ) for field , value in self . extra conditions ] ) for , value in self . extra conditions : params . append ( value ) rewritten sql = sql . replace ( ')' , ' AND {})' . format ( extra conditions ) ) return rewritten sql , params", "predictions": ["transforms a list of columns into a compiler ."], "references": ["compiles this join into a sql string ."], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 1108, "code": "def select ( self , board ) : if self . unexplored : i = random . randrange ( len ( self . unexplored ) ) pos = self . unexplored [ i ] self . unexplored [ i ] = self . unexplored [ len ( self . unexplored ) - 1 ] self . unexplored . pop ( ) return pos elif self . bestchild : return self . bestchild . pos else : return PASS", "predictions": ["returns a list of points for this board ."], "references": ["select move ; unexplored children first then according to uct value"], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 1109, "code": "def random playout ( self , board ) : for x in range ( MAXMOVES ) : if board . finished : break board . move ( board . random move ( ) )", "predictions": ["randomly move the board ."], "references": ["random play until both players pass"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 1110, "code": "def Get Domain ( self ) : return ( self . knots [ self . degree - 1 ] , self . knots [ len ( self . knots ) - self . degree ] )", "predictions": ["returns a longer - . object for the given list of ( ."], "references": ["returns the domain of the b - spline"], "bleu": 0.12011055432195765, "rouge_l": 0.2985318107667211}
{"id": 1111, "code": "def parse posts ( self , raw posts ) : parsed posts = self . parse json ( raw posts ) for post id in parsed posts [ 'order' ] : yield parsed posts [ 'posts' ] [ post id ]", "predictions": ["iterates through the list of posts and iterates over each of its posts ."], "references": ["parse posts and returns in order ."], "bleu": 0.1250076305588977, "rouge_l": 0.30398671096345514}
{"id": 1112, "code": "def posts ( self , channel , page = None ) : entrypoint = self . RCHANNELS + '/' + channel + '/' + self . RPOSTS params = { self . PPER PAGE : self . max items } if page is not None : params [ self . PPAGE ] = page response = self . fetch ( entrypoint , params ) return response", "predictions": ["print out the posts for this page ."], "references": ["fetch the history of a channel ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1113, "code": "def user ( self , user ) : entrypoint = self . RUSERS + '/' + user response = self . fetch ( entrypoint , None ) return response", "predictions": ["get the user back to the response ."], "references": ["fetch user data ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 1114, "code": "def pre init ( self ) : if not self . parsed args . mboxes path : base path = os . path . expanduser ( '~/.perceval/mailinglists/' ) dirpath = os . path . join ( base path , self . parsed args . url ) else : dirpath = self . parsed args . mboxes path setattr ( self . parsed args , 'dirpath' , dirpath )", "predictions": ["initialize , . must be called before any existing event is set ."], "references": ["initialize mailing lists directory path"], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 1115, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , archive = True ) parser . parser . add argument ( 'url' , help = \"URL of the RSS feed\" ) return parser", "predictions": ["adds command line argument to the command line ."], "references": ["returns the rss argument parser ."], "bleu": 0.16784459625186196, "rouge_l": 0.27664399092970515}
{"id": 1116, "code": "def fetch merge requests ( self , from date ) : merges groups = self . client . merges ( from date = from date ) for raw merges in merges groups : merges = json . loads ( raw merges ) for merge in merges : merge id = merge [ 'iid' ] if self . blacklist ids and merge id in self . blacklist ids : logger . warning ( \"Skipping blacklisted merge request %s\" , merge id ) continue merge full raw = self . client . merge ( merge id ) merge full = json . loads ( merge full raw ) self . init merge extra fields ( merge full ) merge full [ 'notes data' ] = self . get merge notes ( merge id ) merge full [ 'award emoji data' ] = self . get award emoji ( Git Lab Client . MERGES , merge id ) merge full [ 'versions data' ] = self . get merge versions ( merge id ) yield merge full", "predictions": ["creates and registers all requests that have been processed ."], "references": ["fetch the merge requests"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 1117, "code": "def issues ( self , from date = None ) : payload = { 'state' : 'all' , 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } if from date : payload [ 'updated after' ] = from date . isoformat ( ) return self . fetch items ( Git Lab Client . ISSUES , payload )", "predictions": ["fetch issues from the specified date ."], "references": ["get the issues from pagination"], "bleu": 0.2626909894424158, "rouge_l": 0.34366197183098596}
{"id": 1118, "code": "def merges ( self , from date = None ) : payload = { 'state' : 'all' , 'order by' : 'updated at' , 'sort' : 'asc' , 'view' : 'simple' , 'per page' : PER PAGE } if from date : payload [ 'updated after' ] = from date . isoformat ( ) return self . fetch items ( Git Lab Client . MERGES , payload )", "predictions": ["create a lab from this date ."], "references": ["get the merge requests from pagination"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1119, "code": "def merge ( self , merge id ) : path = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , Git Lab Client . MERGES , merge id ) response = self . fetch ( path ) return response . text", "predictions": ["merge the given path into this object ."], "references": ["get the merge full data"], "bleu": 0.17747405280050269, "rouge_l": 0.16052631578947368}
{"id": 1120, "code": "def merge versions ( self , merge id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( Git Lab Client . MERGES , str ( merge id ) , Git Lab Client . VERSIONS ) return self . fetch items ( path , payload )", "predictions": ["alter the field with this one ."], "references": ["get the merge versions from pagination"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1121, "code": "def merge version ( self , merge id , version id ) : path = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , Git Lab Client . MERGES , merge id , Git Lab Client . VERSIONS , version id ) response = self . fetch ( path ) return response . text", "predictions": ["create a hstore from this hstore ."], "references": ["get merge version detail"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1122, "code": "def notes ( self , item type , item id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . NOTES ) return self . fetch items ( path , payload )", "predictions": ["roll back to the server ."], "references": ["get the notes from pagination"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1123, "code": "def emojis ( self , item type , item id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . EMOJI ) return self . fetch items ( path , payload )", "predictions": ["create a create to the specified model ."], "references": ["get emojis from pagination"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1124, "code": "def note emojis ( self , item type , item id , note id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . NOTES , str ( note id ) , Git Lab Client . EMOJI ) return self . fetch items ( path , payload )", "predictions": ["calculates a create to the specified ) ."], "references": ["get emojis of a note"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1125, "code": "def fetch items ( self , path , payload ) : page = 0 last page = None url next = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , path ) logger . debug ( \"Get Git Lab paginated items from \" + url next ) response = self . fetch ( url next , payload = payload ) items = response . text page += 1 if 'last' in response . links : last url = response . links [ 'last' ] [ 'url' ] last page = last url . split ( '&page=' ) [ 1 ] . split ( '&' ) [ 0 ] last page = int ( last page ) logger . debug ( \"Page: %i/%i\" % ( page , last page ) ) while items : yield items items = None if 'next' in response . links : url next = response . links [ 'next' ] [ 'url' ] response = self . fetch ( url next , payload = payload ) page += 1 items = response . text logger . debug ( \"Page: %i/%i\" % ( page , last page ) )", "predictions": ["fetches stats from the provided ) and return as a for each resource ."], "references": ["return the items from gitlab api using links pagination"], "bleu": 0.10511846841633776, "rouge_l": 0.09050445103857567}
{"id": 1126, "code": "def init rate limit ( self ) : url = urijoin ( self . base url , 'projects' , self . owner + '%2F' + self . repository ) try : response = super ( ) . fetch ( url ) self . update rate limit ( response ) except requests . exceptions . HTTP Error as error : if error . response . status code == 401 : raise error else : logger . warning ( \"Rate limit not initialized: %s\" , error )", "predictions": ["initialize the model output ."], "references": ["initialize rate limit information"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1127, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) group = parser . parser . add argument group ( 'Git Lab arguments' ) group . add argument ( '--enterprise-url' , dest = 'base url' , help = \"Base URL for Git Lab Enterprise instance\" ) group . add argument ( '--sleep-for-rate' , dest = 'sleep for rate' , action = 'store true' , help = \"sleep for getting more rate\" ) group . add argument ( '--min-rate-to-sleep' , dest = 'min rate to sleep' , default = MIN RATE LIMIT , type = int , help = ) group . add argument ( '--blacklist-ids' , dest = 'blacklist ids' , nargs = '*' , type = int , help = \"Ids of items that must not be retrieved.\" ) group . add argument ( '--max-retries' , dest = 'max retries' , default = MAX RETRIES , type = int , help = \"number of API call retries\" ) group . add argument ( '--sleep-time' , dest = 'sleep time' , default = DEFAULT SLEEP TIME , type = int , help = \"sleeping time between API call retries\" ) parser . parser . add argument ( 'owner' , help = \"Git Lab owner\" ) parser . parser . add argument ( 'repository' , help = \"Git Lab repository\" ) return parser", "predictions": ["creates and initializes the table for evaluation ."], "references": ["returns the gitlab argument parser ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1128, "code": "def channel info ( self , channel ) : resource = self . RCHANNEL INFO params = { self . PCHANNEL : channel , } response = self . fetch ( resource , params ) return response", "predictions": ["get the field of this add to the add ."], "references": ["fetch information about a channel ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 1129, "code": "def history ( self , channel , oldest = None , latest = None ) : resource = self . RCHANNEL HISTORY params = { self . PCHANNEL : channel , self . PCOUNT : self . max items } if oldest is not None : params [ self . POLDEST ] = oldest if latest is not None : params [ self . PLATEST ] = latest response = self . fetch ( resource , params ) return response", "predictions": ["set the for this object ."], "references": ["fetch the history of a channel ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1130, "code": "def user ( self , user id ) : resource = self . RUSER INFO params = { self . PUSER : user id } response = self . fetch ( resource , params ) return response", "predictions": ["get the alter alter lock on this alter ."], "references": ["fetch user info ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 1131, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) action = parser . parser . option string actions [ '--api-token' ] action . required = True group = parser . parser . add argument group ( 'Slack arguments' ) group . add argument ( '--max-items' , dest = 'max items' , type = int , default = MAX ITEMS , help = \"Maximum number of items requested on the same query\" ) parser . parser . add argument ( 'channel' , help = \"Slack channel identifier\" ) return parser", "predictions": ["creates an ( ( or adds : - - > ( : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"], "references": ["returns the slack argument parser ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1132, "code": "def logout ( self ) : params = { self . PLOGOUT : '1' } self . call ( self . CGI LOGIN , params ) self . close http session ( ) logger . debug ( \"Bugzilla user logged out from %s\" , self . base url )", "predictions": ["logs out the user if he is logged in ."], "references": ["logout from the server ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 1133, "code": "def metadata ( self ) : params = { self . PCTYPE : self . CTYPE XML } response = self . call ( self . CGI BUG , params ) return response", "predictions": ["attach http alter objects to the alter service ."], "references": ["get metadata information in xml format ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 1134, "code": "def events ( self , group , from date = DEFAULT DATETIME ) : date = datetime to utc ( from date ) date = date . strftime ( \"since:%Y-%m-%d T%H:%M:%S.000Z\" ) resource = urijoin ( group , self . REVENTS ) fixed params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT FIELDS ) fixed params += '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) resource += fixed params params = { self . PORDER : self . VUPDATED , self . PSCROLL : date , self . PPAGE : self . max items } try : for page in self . fetch ( resource , params ) : yield page except requests . exceptions . HTTP Error as error : if error . response . status code == 410 : msg = \"Group is no longer accessible: {}\" . format ( error ) raise Repository Error ( cause = msg ) else : raise error", "predictions": ["raise stats ."], "references": ["fetch the events pages of a given group ."], "bleu": 0.07271361304044333, "rouge_l": 0.15288220551378442}
{"id": 1135, "code": "def comments ( self , group , event id ) : resource = urijoin ( group , self . REVENTS , event id , self . RCOMMENTS ) params = { self . PPAGE : self . max items } for page in self . fetch ( resource , params ) : yield page", "predictions": ["a remove method call to get notifications from the remove"], "references": ["fetch the comments of a given event ."], "bleu": 0.13950796967929133, "rouge_l": 0.11338289962825278}
{"id": 1136, "code": "def rsvps ( self , group , event id ) : resource = urijoin ( group , self . REVENTS , event id , self . RRSVPS ) fixed params = '?' + self . PFIELDS + '=' + ',' . join ( self . VRSVP FIELDS ) fixed params += '&' + self . PRESPONSE + '=' + ',' . join ( self . VRESPONSE ) resource += fixed params params = { self . PPAGE : self . max items } for page in self . fetch ( resource , params ) : yield page", "predictions": ["loop through this method ."], "references": ["fetch the rsvps of a given event ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1137, "code": "def parse reviews ( raw data ) : items raw = \"[\" + raw data . replace ( \"\\n\" , \",\" ) + \"]\" items raw = items raw . replace ( \",]\" , \"]\" ) items = json . loads ( items raw ) reviews = [ ] for item in items : if 'project' in item . keys ( ) : reviews . append ( item ) return reviews", "predictions": ["replace all hstore hstore with the values from the first token ."], "references": ["parse a gerrit reviews list ."], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 1138, "code": "def version ( self ) : if self . version : return self . version cmd = self . gerrit cmd + \" %s \" % ( Gerrit Client . CMD VERSION ) logger . debug ( \"Getting version: %s\" % ( cmd ) ) raw data = self . execute ( cmd ) raw data = str ( raw data , \"UTF-8\" ) logger . debug ( \"Gerrit version: %s\" % ( raw data ) ) m = re . match ( Gerrit Client . VERSION REGEX , raw data ) if not m : cause = \"Invalid gerrit version %s\" % raw data raise Backend Error ( cause = cause ) try : mayor = int ( m . group ( 1 ) ) minor = int ( m . group ( 2 ) ) except Exception : cause = \"Gerrit client could not determine the server version.\" raise Backend Error ( cause = cause ) self . version = [ mayor , minor ] return self . version", "predictions": ["get commands from the old web server ."], "references": ["return the gerrit server version ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 1139, "code": "def reviews ( self , last item , filter = None ) : cmd = self . get gerrit cmd ( last item , filter ) logger . debug ( \"Getting reviews with command: %s\" , cmd ) raw data = self . execute ( cmd ) raw data = str ( raw data , \"UTF-8\" ) return raw data", "predictions": ["( method for ( ."], "references": ["get the reviews starting from last_item ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1140, "code": "def next retrieve group item ( self , last item = None , entry = None ) : next item = None gerrit version = self . version if gerrit version [ 0 ] == 2 and gerrit version [ 1 ] > 9 : if last item is None : next item = 0 else : next item = last item elif gerrit version [ 0 ] == 2 and gerrit version [ 1 ] == 9 : cause = \"Gerrit 2.9.0 does not support pagination\" raise Backend Error ( cause = cause ) else : if entry is not None : next item = entry [ 'sort Key' ] return next item", "predictions": ["if no ( is found , will return it ."], "references": ["return the item to start from in next reviews group ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 1141, "code": "def execute from archive ( self , cmd ) : cmd = self . sanitize for archive ( cmd ) response = self . archive . retrieve ( cmd , None , None ) if isinstance ( response , Runtime Error ) : raise response return response", "predictions": ["executes a method call ."], "references": ["execute gerrit command against the archive"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 1142, "code": "def execute from remote ( self , cmd ) : result = None retries = 0 while retries < self . MAX RETRIES : try : result = subprocess . check output ( cmd , shell = True ) break except subprocess . Called Process Error as ex : logger . error ( \"gerrit cmd %s failed: %s\" , cmd , ex ) time . sleep ( self . RETRY WAIT * retries ) retries += 1 if result is None : result = Runtime Error ( cmd + \" failed \" + str ( self . MAX RETRIES ) + \" times. Giving up!\" ) if self . archive : cmd = self . sanitize for archive ( cmd ) self . archive . store ( cmd , None , None , result ) if isinstance ( result , Runtime Error ) : raise result return result", "predictions": ["this method does the actual work of this class ."], "references": ["execute gerrit command with retry if it fails"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1143, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , archive = True ) group = parser . parser . add argument group ( 'Gerrit arguments' ) group . add argument ( '--user' , dest = 'user' , help = \"Gerrit ssh user\" ) group . add argument ( '--max-reviews' , dest = 'max reviews' , type = int , default = MAX REVIEWS , help = \"Max number of reviews per ssh query.\" ) group . add argument ( '--blacklist-reviews' , dest = 'blacklist reviews' , nargs = '*' , help = \"Wrong reviews that must not be retrieved.\" ) group . add argument ( '--disable-host-key-check' , dest = 'disable host key check' , action = 'store true' , help = \"Don't check remote host identity\" ) group . add argument ( '--ssh-port' , dest = 'port' , default = PORT , type = int , help = \"Set SSH port of the Gerrit server\" ) parser . parser . add argument ( 'hostname' , help = \"Hostname of the Gerrit server\" ) return parser", "predictions": ["creates and initializes ( for command use"], "references": ["returns the gerrit argument parser ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1144, "code": "def fetch issue data ( self , issue id ) : raw issue = self . client . issue ( issue id ) issue = json . loads ( raw issue ) return issue", "predictions": ["add an ( i . e . , the ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["get data associated to an issue"], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 1145, "code": "def fetch issue attachments ( self , issue id ) : for attachments raw in self . client . issue collection ( issue id , \"attachments\" ) : attachments = json . loads ( attachments raw ) for attachment in attachments [ 'entries' ] : yield attachment", "predictions": ["fetches data from ( as much = ( = 1 = 3 = 3 = 0 = 1 = 1 = 3 = 1 = 0 = 8 = 8 = 3 = 0 = 8 = 1 = 0 = 1 = 8 = 8 = 8 = 0"], "references": ["get attachments of an issue"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1146, "code": "def fetch issue messages ( self , issue id ) : for messages raw in self . client . issue collection ( issue id , \"messages\" ) : messages = json . loads ( messages raw ) for msg in messages [ 'entries' ] : msg [ 'owner data' ] = self . fetch user data ( '{OWNER}' , msg [ 'owner link' ] ) yield msg", "predictions": ["fetches ( as a generator of all ( possibly required not necessarily not necessarily not necessarily not part not part of this init ."], "references": ["get messages of an issue"], "bleu": 0.050661968099322066, "rouge_l": 0.07820512820512819}
{"id": 1147, "code": "def fetch issue activities ( self , issue id ) : for activities raw in self . client . issue collection ( issue id , \"activity\" ) : activities = json . loads ( activities raw ) for act in activities [ 'entries' ] : act [ 'person data' ] = self . fetch user data ( '{PERSON}' , act [ 'person link' ] ) yield act", "predictions": ["setup all parser parser for this cmd ."], "references": ["get activities on an issue"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1148, "code": "def fetch user data ( self , tag type , user link ) : user name = self . client . user name ( user link ) user = { } if not user name : return user user raw = self . client . user ( user name ) user = json . loads ( user raw ) return user", "predictions": ["increments and returns the merge object for the specified merge ."], "references": ["get data associated to an user"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1149, "code": "def issues ( self , start = None ) : payload = self . build payload ( size = self . items per page , operation = True , startdate = start ) path = self . get url project ( ) return self . fetch items ( path = path , payload = payload )", "predictions": ["builds a page for the entity ."], "references": ["get the issues from pagination"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1150, "code": "def user ( self , user name ) : user = None if user name in self . users : return self . users [ user name ] url user = self . get url ( \"~\" + user name ) logger . info ( \"Getting info for %s\" % ( url user ) ) try : raw user = self . send request ( url user ) user = raw user except requests . exceptions . HTTP Error as e : if e . response . status code in [ 404 , 410 ] : logger . warning ( \"Data is not available - %s\" , url user ) user = '{}' else : raise e self . users [ user name ] = user return user", "predictions": ["call go back with this ."], "references": ["get the user data by url"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1151, "code": "def issue ( self , issue id ) : path = urijoin ( \"bugs\" , str ( issue id ) ) url issue = self . get url ( path ) raw text = self . send request ( url issue ) return raw text", "predictions": ["owner and owner a merge ."], "references": ["get the issue data by its id"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1152, "code": "def issue collection ( self , issue id , collection name ) : path = urijoin ( \"bugs\" , str ( issue id ) , collection name ) url collection = self . get url ( path ) payload = { 'ws.size' : self . items per page , 'ws.start' : 0 , 'order by' : 'date last updated' } raw items = self . fetch items ( path = url collection , payload = payload ) return raw items", "predictions": ["send an issue to the issue ."], "references": ["get a collection list of a given issue"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1153, "code": "def fetch items ( self , path , payload ) : page = 0 url next = path fetch data = True while fetch data : logger . debug ( \"Fetching page: %i\" , page ) try : raw content = self . send request ( url next , payload ) content = json . loads ( raw content ) except requests . exceptions . HTTP Error as e : if e . response . status code in [ 410 ] : logger . warning ( \"Data is not available - %s\" , url next ) raw content = '{\"total size\": 0, \"start\": 0, \"entries\": []}' content = json . loads ( raw content ) else : raise e if 'next collection link' in content : url next = content [ 'next collection link' ] payload = None else : fetch data = False yield raw content page += 1", "predictions": ["fetches stats from given path and fetches data ."], "references": ["return the items from launchpad api using pagination"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 1154, "code": "def find group id ( self ) : group subscriptions = self . subscriptions ( self . auth ) for subscriptions in group subscriptions : for sub in subscriptions : if sub [ 'group name' ] == self . group name : return sub [ 'group id' ] msg = \"Group id not found for group name %s\" % self . group name raise Backend Error ( cause = msg )", "predictions": ["find the best view with this group container ."], "references": ["find the id of a group given its name by iterating on the list of subscriptions"], "bleu": 0.09170230785170237, "rouge_l": 0.22846441947565538}
{"id": 1155, "code": "def fetch ( self , url , payload ) : r = requests . get ( url , params = payload , auth = self . auth , verify = self . verify ) try : r . raise for status ( ) except requests . exceptions . HTTP Error as e : raise e return r", "predictions": ["fetches data from this http server ."], "references": ["fetch requests from groupsio api"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1156, "code": "def pre init ( self ) : if not self . parsed args . mboxes path : base path = os . path . expanduser ( '~/.perceval/mailinglists/' ) dirpath = os . path . join ( base path , GROUPSIO URL , 'g' , self . parsed args . group name ) else : dirpath = self . parsed args . mboxes path setattr ( self . parsed args , 'dirpath' , dirpath )", "predictions": ["creates new full , ."], "references": ["initialize mailing lists directory path"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1157, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True ) action = parser . parser . option string actions [ '--api-token' ] action . required = True group = parser . parser . add argument group ( 'Groupsio arguments' ) group . add argument ( '--mboxes-path' , dest = 'mboxes path' , help = \"Path where mbox files will be stored\" ) group . add argument ( '--no-verify' , dest = 'verify' , action = 'store false' , help = \"Value 'True' enable SSL verification\" ) parser . parser . add argument ( 'group name' , help = \"Name of the group on Groups.io\" ) return parser", "predictions": ["creates command line option for command ( ."], "references": ["returns the groupsio argument parser ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1158, "code": "def set auth arguments ( self , basic auth = True , token auth = False ) : group = self . parser . add argument group ( 'authentication arguments' ) if basic auth : group . add argument ( '-u' , '--backend-user' , dest = 'user' , help = \"backend user\" ) group . add argument ( '-p' , '--backend-password' , dest = 'password' , help = \"backend password\" ) if token auth : group . add argument ( '-t' , '--api-token' , dest = 'api token' , help = \"backend authentication token / API key\" )", "predictions": ["set the authentication class ."], "references": ["activate authentication arguments parsing"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1159, "code": "def set archive arguments ( self ) : group = self . parser . add argument group ( 'archive arguments' ) group . add argument ( '--archive-path' , dest = 'archive path' , default = None , help = \"directory path to the archives\" ) group . add argument ( '--no-archive' , dest = 'no archive' , action = 'store true' , help = \"do not archive data\" ) group . add argument ( '--fetch-archive' , dest = 'fetch archive' , action = 'store true' , help = \"fetch data from the archives\" ) group . add argument ( '--archived-since' , dest = 'archived since' , default = '1970-01-01' , help = \"retrieve items archived since the given date\" )", "predictions": ["for a ( archive ."], "references": ["activate archive arguments parsing"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1160, "code": "def set output arguments ( self ) : group = self . parser . add argument group ( 'output arguments' ) group . add argument ( '-o' , '--output' , type = argparse . File Type ( 'w' ) , dest = 'outfile' , default = sys . stdout , help = \"output file\" ) group . add argument ( '--json-line' , dest = 'json line' , action = 'store true' , help = \"produce a JSON line for each output item\" )", "predictions": ["for example , you can override this method to set the default output of ( ."], "references": ["activate output arguments parsing"], "bleu": 0.07692375026049747, "rouge_l": 0.11213235294117647}
{"id": 1161, "code": "def initialize archive ( self ) : if 'archive path' not in self . parsed args : manager = None elif self . parsed args . no archive : manager = None else : if not self . parsed args . archive path : archive path = os . path . expanduser ( ARCHIVES DEFAULT PATH ) else : archive path = self . parsed args . archive path manager = Archive Manager ( archive path ) self . archive manager = manager", "predictions": ["initializes this . object ."], "references": ["initialize archive based on the parsed parameters"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1162, "code": "def fetch and parse messages ( self , mailing list , from date ) : from date = datetime to utc ( from date ) nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) for mbox in mailing list . mboxes : tmp path = None try : tmp path = self . copy mbox ( mbox ) for message in self . parse mbox ( tmp path ) : tmsgs += 1 if not self . validate message ( message ) : imsgs += 1 continue dt = str to datetime ( message [ M Box . DATE FIELD ] ) if dt < from date : logger . debug ( \"Message %s sent before %s; skipped\" , message [ 'unixfrom' ] , str ( from date ) ) tmsgs -= 1 continue message = self . casedict to dict ( message ) nmsgs += 1 logger . debug ( \"Message %s parsed\" , message [ 'unixfrom' ] ) yield message except ( OS Error , EOF Error ) as e : logger . warning ( \"Ignoring %s mbox due to: %s\" , mbox . filepath , str ( e ) ) except Exception as e : if tmp path and os . path . exists ( tmp path ) : os . remove ( tmp path ) raise e finally : if tmp path and os . path . exists ( tmp path ) : os . remove ( tmp path ) logger . info ( \"Done. %s/%s messages fetched; %s ignored\" , nmsgs , tmsgs , imsgs )", "predictions": ["fetches all the data structures from given date and save it to the messages that are also tracked ."], "references": ["fetch and parse the messages from a mailing list"], "bleu": 0.09629943614188137, "rouge_l": 0.2290362953692115}
{"id": 1163, "code": "def copy mbox ( self , mbox ) : tmp path = tempfile . mktemp ( prefix = 'perceval ' ) with mbox . container as f in : with open ( tmp path , mode = 'wb' ) as f out : for l in f in : f out . write ( l ) return tmp path", "predictions": ["copy files of this object into a new folder ."], "references": ["copy the contents of a mbox to a temporary file"], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 1164, "code": "def validate message ( self , message ) : if self . MESSAGE ID FIELD not in message : logger . warning ( \"Field 'Message-ID' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if not message [ self . MESSAGE ID FIELD ] : logger . warning ( \"Field 'Message-ID' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if self . DATE FIELD not in message : logger . warning ( \"Field 'Date' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if not message [ self . DATE FIELD ] : logger . warning ( \"Field 'Date' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) return False try : str to datetime ( message [ self . DATE FIELD ] ) except Invalid Date Error : logger . warning ( \"Invalid date %s in message %s; ignoring\" , message [ self . DATE FIELD ] , message [ 'unixfrom' ] ) return False return True", "predictions": ["validates the message passed to the \"invalid ."], "references": ["check if the given message has the mandatory fields"], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 1165, "code": "def get message ( self , key ) : start , stop = self . lookup ( key ) self . file . seek ( start ) from line = self . file . readline ( ) . replace ( mailbox . linesep , b'' ) string = self . file . read ( stop - self . file . tell ( ) ) msg = self . message factory ( string . replace ( mailbox . linesep , b'\\n' ) ) try : msg . set from ( from line [ 5 : ] . decode ( 'ascii' ) ) return msg except Unicode Decode Error : pass try : msg . set from ( from line [ 5 : ] . decode ( 'utf-8' ) ) except Unicode Decode Error : msg . set from ( from line [ 5 : ] . decode ( 'iso-8859-1' ) ) return msg", "predictions": ["fetches message from the message ."], "references": ["return a message representation or raise a keyerror ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1166, "code": "def pre init ( self ) : if self . parsed args . git log : git path = self . parsed args . git log elif not self . parsed args . git path : base path = os . path . expanduser ( '~/.perceval/repositories/' ) processed uri = self . parsed args . uri . lstrip ( '/' ) git path = os . path . join ( base path , processed uri ) + '-git' else : git path = self . parsed args . git path setattr ( self . parsed args , 'gitpath' , git path )", "predictions": ["initialize this git object ."], "references": ["initialize repositories directory path"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1167, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , to date = True ) group = parser . parser . add argument group ( 'Git arguments' ) group . add argument ( '--branches' , dest = 'branches' , nargs = '+' , type = str , default = None , help = \"Fetch commits only from these branches\" ) exgroup = group . add mutually exclusive group ( ) exgroup . add argument ( '--git-path' , dest = 'git path' , help = \"Path where the Git repository will be cloned\" ) exgroup . add argument ( '--git-log' , dest = 'git log' , help = \"Path to the Git log file\" ) exgroup fetch = group . add mutually exclusive group ( ) exgroup fetch . add argument ( '--latest-items' , dest = 'latest items' , action = 'store true' , help = \"Fetch latest commits added to the repository\" ) exgroup fetch . add argument ( '--no-update' , dest = 'no update' , action = 'store true' , help = \"Fetch all commits without updating the repository\" ) parser . parser . add argument ( 'uri' , help = \"URI of the Git log repository\" ) return parser", "predictions": ["creates and adds parser to command ( ."], "references": ["returns the git argument parser ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1168, "code": "def parse ( self ) : for line in self . stream : line = line . rstrip ( '\\n' ) parsed = False self . nline += 1 while not parsed : parsed = self . handlers [ self . state ] ( line ) if self . state == self . COMMIT and self . commit : commit = self . build commit ( ) logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) yield commit if self . commit : commit = self . build commit ( ) logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) yield commit", "predictions": ["iterates through all statements including the trailing results ."], "references": ["parse the git log stream ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1169, "code": "def fetch pack ( self ) : def prepare refs ( refs ) : return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] def determine wants ( refs ) : remote refs = prepare refs ( self . discover refs ( remote = True ) ) local refs = prepare refs ( self . discover refs ( ) ) wants = [ ref for ref in remote refs if ref not in local refs ] return wants client , repo path = dulwich . client . get transport and path ( self . uri ) repo = dulwich . repo . Repo ( self . dirpath ) fd = io . Bytes IO ( ) local refs = self . discover refs ( ) graph walker = Graph Walker ( local refs ) result = client . fetch pack ( repo path , determine wants , graph walker , fd . write ) refs = [ Git Ref ( ref hash . decode ( 'utf-8' ) , ref name . decode ( 'utf-8' ) ) for ref name , ref hash in result . refs . items ( ) ] if len ( fd . getvalue ( ) ) > 0 : fd . seek ( 0 ) pack = repo . object store . add thin pack ( fd . read , None ) pack name = pack . name ( ) . decode ( 'utf-8' ) else : pack name = None return ( pack name , refs )", "predictions": ["fetch data from given url ."], "references": ["fetch changes and store them in a pack ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1170, "code": "def read commits from pack ( self , packet name ) : filepath = 'objects/pack/pack-' + packet name cmd verify pack = [ 'git' , 'verify-pack' , '-v' , filepath ] outs = self . exec ( cmd verify pack , cwd = self . dirpath , env = self . gitenv ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) lines = [ line . split ( ' ' ) for line in outs . split ( '\\n' ) ] commits = [ parts [ 0 ] for parts in lines if parts [ 1 ] == 'commit' ] commits . reverse ( ) return commits", "predictions": ["read and return a tree of data with the structure provided ."], "references": ["read the commits of a pack ."], "bleu": 0.1367440667823257, "rouge_l": 0.3315217391304348}
{"id": 1171, "code": "def update references ( self , refs ) : new refs = [ ref . refname for ref in refs ] for old ref in self . discover refs ( ) : if not old ref . refname . startswith ( 'refs/heads/' ) : continue if old ref . refname in new refs : continue self . update ref ( old ref , delete = True ) for new ref in refs : refname = new ref . refname if refname . endswith ( '^{}' ) : logger . debug ( \"Annotated tag %s ignored for updating in sync process\" , refname ) continue elif not refname . startswith ( 'refs/heads/' ) and not refname . startswith ( 'refs/tags/' ) : logger . debug ( \"Reference %s not needed; ignored for updating in sync process\" , refname ) continue else : self . update ref ( new ref ) cmd = [ 'git' , 'remote' , 'prune' , 'origin' ] self . exec ( cmd , cwd = self . dirpath , env = self . gitenv )", "predictions": ["update stats with the specified refs ."], "references": ["update references removing old ones ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1172, "code": "def discover refs ( self , remote = False ) : if remote : cmd refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] sep = '\\t' ignored error codes = [ 2 ] else : if self . is empty ( ) : raise Empty Repository Error ( repository = self . uri ) cmd refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] sep = ' ' ignored error codes = [ 1 ] outs = self . exec ( cmd refs , cwd = self . dirpath , env = self . gitenv , ignored error codes = ignored error codes ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) outs = outs . split ( '\\n' ) if outs else [ ] refs = [ ] for line in outs : data = line . split ( sep ) ref = Git Ref ( data [ 0 ] , data [ 1 ] ) refs . append ( ref ) return refs", "predictions": ["discover and return a list of refs that have been previously performed ."], "references": ["get the current list of local or remote refs ."], "bleu": 0.14283632578659286, "rouge_l": 0.3562043795620438}
{"id": 1173, "code": "def update ref ( self , ref , delete = False ) : cmd = [ 'git' , 'update-ref' ] if delete : cmd . extend ( [ '-d' , ref . refname ] ) action = 'deleted' else : cmd . extend ( [ ref . refname , ref . hash ] ) action = 'updated to %s' % ref . hash try : self . exec ( cmd , cwd = self . dirpath , env = self . gitenv ) except Repository Error as e : logger . warning ( \"Git %s ref could not be %s during sync process in %s (%s); skipped\" , ref . refname , action , self . uri , self . dirpath ) else : logger . debug ( \"Git %s ref %s in %s (%s)\" , ref . refname , action , self . uri , self . dirpath )", "predictions": ["update request for remote object ."], "references": ["update a reference ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 1174, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , token auth = True , archive = True ) action = parser . parser . option string actions [ '--api-token' ] action . required = True group = parser . parser . add argument group ( 'Twitter arguments' ) group . add argument ( '--max-items' , dest = 'max items' , type = int , default = MAX ITEMS , help = \"Maximum number of items requested on the same query\" ) group . add argument ( '--no-entities' , dest = 'include entities' , action = 'store false' , help = \" Exclude entities node\" ) group . add argument ( '--geo-code' , dest = 'geocode' , help = \"Select tweets by users located at latitude,longitude,radius\" ) group . add argument ( '--lang' , dest = 'lang' , help = \"Select tweets to the given language in ISO 639-1 code\" ) group . add argument ( '--tweets-type' , dest = 'tweets type' , default = TWEET TYPE MIXED , help = \"Type of tweets returned. Default is 'mixed', others are 'recent' and 'popular'\" ) group . add argument ( '--sleep-for-rate' , dest = 'sleep for rate' , action = 'store true' , help = \"sleep for getting more rate\" ) group . add argument ( '--min-rate-to-sleep' , dest = 'min rate to sleep' , default = MIN RATE LIMIT , type = int , help = \"sleep until reset when the rate limit reaches this value\" ) group . add argument ( '--sleep-time' , dest = 'sleep time' , default = SLEEP TIME , type = int , help = \"minimun sleeping time to avoid too many request exception\" ) parser . parser . add argument ( 'query' , help = \"Search query including operators, max 500 chars\" ) return parser", "predictions": ["creates command line option for command line tweets ."], "references": ["returns the twitter argument parser ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1175, "code": "def parse hits ( self , hit raw ) : bs result = bs4 . Beautiful Soup ( hit raw , 'html.parser' ) hit string = bs result . find ( \"div\" , id = \"result Stats\" ) . text hit string = hit string . replace ( ',' , u'' ) hit string = hit string . replace ( '.' , u'' ) fetched on = datetime utcnow ( ) . timestamp ( ) id args = self . keywords [ : ] id args . append ( str ( fetched on ) ) hits json = { 'fetched on' : fetched on , 'id' : uuid ( * id args ) , 'keywords' : self . keywords , 'type' : 'google Search Hits' } if not hit string : logger . warning ( \"No hits for %s\" , self . keywords ) hits json [ 'hits' ] = 0 return hits json str hits = re . search ( r'\\d+' , hit string ) . group ( 0 ) hits = int ( str hits ) hits json [ 'hits' ] = hits return hits json", "predictions": ["parses the list of lowercase results from the given string ."], "references": ["parse the hits returned by the google search api"], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 1176, "code": "def hits ( self , keywords ) : if len ( keywords ) == 1 : query str = keywords [ 0 ] else : query str = ' ' . join ( [ k for k in keywords ] ) logger . info ( \"Fetching hits for '%s'\" , query str ) params = { 'q' : query str } req = self . fetch ( GOOGLE SEARCH URL , payload = params ) return req . text", "predictions": ["return list of all keywords with this schema ."], "references": ["fetch information about a list of keywords ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 1177, "code": "def fetch pull requests ( self , from date , to date ) : raw pulls = self . client . pulls ( from date = from date ) for raw pull in raw pulls : pull = json . loads ( raw pull ) if str to datetime ( pull [ 'updated at' ] ) > to date : return self . init extra pull fields ( pull ) for field in TARGET PULL FIELDS : if not pull [ field ] : continue if field == 'user' : pull [ field + ' data' ] = self . get user ( pull [ field ] [ 'login' ] ) elif field == 'merged by' : pull [ field + ' data' ] = self . get user ( pull [ field ] [ 'login' ] ) elif field == 'review comments' : pull [ field + ' data' ] = self . get pull review comments ( pull [ 'number' ] ) elif field == 'requested reviewers' : pull [ field + ' data' ] = self . get pull requested reviewers ( pull [ 'number' ] ) elif field == 'commits' : pull [ field + ' data' ] = self . get pull commits ( pull [ 'number' ] ) yield pull", "predictions": ["since we get here we have to fetch the pull request and pull it from the pull request ."], "references": ["fetch the pull requests"], "bleu": 0.11986062961075741, "rouge_l": 0.295638126009693}
{"id": 1178, "code": "def fetch repo info ( self ) : raw repo = self . client . repo ( ) repo = json . loads ( raw repo ) fetched on = datetime utcnow ( ) repo [ 'fetched on' ] = fetched on . timestamp ( ) yield repo", "predictions": ["fetches the raw resources from the server ."], "references": ["get repo info about stars watchers and forks"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1179, "code": "def get issue comment reactions ( self , comment id , total count ) : reactions = [ ] if total count == 0 : return reactions group reactions = self . client . issue comment reactions ( comment id ) for raw reactions in group reactions : for reaction in json . loads ( raw reactions ) : reaction [ 'user data' ] = self . get user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions", "predictions": ["fetch the comment into the issue ."], "references": ["get reactions on issue comments"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1180, "code": "def get pull requested reviewers ( self , pr number ) : requested reviewers = [ ] group requested reviewers = self . client . pull requested reviewers ( pr number ) for raw requested reviewers in group requested reviewers : group requested reviewers = json . loads ( raw requested reviewers ) for requested reviewer in group requested reviewers [ 'users' ] : user data = self . get user ( requested reviewer [ 'login' ] ) requested reviewers . append ( user data ) return requested reviewers", "predictions": ["get the requested reviewer for this user ."], "references": ["get pull request requested reviewers"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 1181, "code": "def get pull commits ( self , pr number ) : hashes = [ ] group pull commits = self . client . pull commits ( pr number ) for raw pull commits in group pull commits : for commit in json . loads ( raw pull commits ) : commit hash = commit [ 'sha' ] hashes . append ( commit hash ) return hashes", "predictions": ["get a pull request of all json maintained by number of hashes ."], "references": ["get pull request commit hashes"], "bleu": 0.14283632578659286, "rouge_l": 0.48316831683168315}
{"id": 1182, "code": "def get pull review comments ( self , pr number ) : comments = [ ] group comments = self . client . pull review comments ( pr number ) for raw comments in group comments : for comment in json . loads ( raw comments ) : comment id = comment . get ( 'id' ) user = comment . get ( 'user' , None ) if not user : logger . warning ( \"Missing user info for %s\" , comment [ 'url' ] ) comment [ 'user data' ] = None else : comment [ 'user data' ] = self . get user ( user [ 'login' ] ) comment [ 'reactions data' ] = self . get pull review comment reactions ( comment id , comment [ 'reactions' ] [ 'total count' ] ) comments . append ( comment ) return comments", "predictions": ["retrieves all the comments associated with this review ."], "references": ["get pull request review comments"], "bleu": 0.15619699684601276, "rouge_l": 0.1506172839506173}
{"id": 1183, "code": "def get pull review comment reactions ( self , comment id , total count ) : reactions = [ ] if total count == 0 : return reactions group reactions = self . client . pull review comment reactions ( comment id ) for raw reactions in group reactions : for reaction in json . loads ( raw reactions ) : reaction [ 'user data' ] = self . get user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions", "predictions": ["fetch the pull image out of the pull tree ."], "references": ["get pull review comment reactions"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 1184, "code": "def get user ( self , login ) : user = { } if not login : return user user raw = self . client . user ( login ) user = json . loads ( user raw ) user orgs raw = self . client . user orgs ( login ) user [ 'organizations' ] = json . loads ( user orgs raw ) return user", "predictions": ["use this if you need to use the collection for the collection ."], "references": ["get user and org data for the login"], "bleu": 0.12571192676522522, "rouge_l": 0.19902120717781402}
{"id": 1185, "code": "def issue reactions ( self , issue number ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( \"issues\" , str ( issue number ) , \"reactions\" ) return self . fetch items ( path , payload )", "predictions": ["items for a fetch ."], "references": ["get reactions of an issue"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1186, "code": "def pull requested reviewers ( self , pr number ) : requested reviewers url = urijoin ( \"pulls\" , str ( pr number ) , \"requested reviewers\" ) return self . fetch items ( requested reviewers url , { } )", "predictions": ["pad the find data for this request ."], "references": ["get pull requested reviewers"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1187, "code": "def pull commits ( self , pr number ) : payload = { 'per page' : PER PAGE , } commit url = urijoin ( \"pulls\" , str ( pr number ) , \"commits\" ) return self . fetch items ( commit url , payload )", "predictions": ["fetch a raise an error if there is no = 0 ."], "references": ["get pull request commits"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 1188, "code": "def pull review comments ( self , pr number ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } comments url = urijoin ( \"pulls\" , str ( pr number ) , \"comments\" ) return self . fetch items ( comments url , payload )", "predictions": ["pre - pre - pre - pre - pre - pre - pre - pre - pre - blocking ."], "references": ["get pull request review comments"], "bleu": 0.051366639095059514, "rouge_l": 0.0}
{"id": 1189, "code": "def pull review comment reactions ( self , comment id ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( \"pulls\" , \"comments\" , str ( comment id ) , \"reactions\" ) return self . fetch items ( path , payload )", "predictions": ["setup a cmd parser ."], "references": ["get reactions of a review comment"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1190, "code": "def user ( self , login ) : user = None if login in self . users : return self . users [ login ] url user = urijoin ( self . base url , 'users' , login ) logging . info ( \"Getting info for %s\" % ( url user ) ) r = self . fetch ( url user ) user = r . text self . users [ login ] = user return user", "predictions": ["get the set of : self - self running in the set of : set the set of : self - self pairs ."], "references": ["get the user information and update the user cache"], "bleu": 0.07164684238257436, "rouge_l": 0.19805194805194803}
{"id": 1191, "code": "def user orgs ( self , login ) : if login in self . users orgs : return self . users orgs [ login ] url = urijoin ( self . base url , 'users' , login , 'orgs' ) try : r = self . fetch ( url ) orgs = r . text except requests . exceptions . HTTP Error as error : if error . response . status code == 404 : logger . error ( \"Can't get github login orgs: %s\" , error ) orgs = '[]' else : raise error self . users orgs [ login ] = orgs return orgs", "predictions": ["fetches the set of add add add set of add add add add add add to the set of add add add add set add to the set of add add add add add add to the set of add add add to the set of add add add add"], "references": ["get the user public organizations"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 1192, "code": "def get token rate limit ( self , token ) : rate url = urijoin ( self . base url , \"rate limit\" ) self . session . headers . update ( { 'Authorization' : 'token ' + token } ) remaining = 0 try : headers = super ( ) . fetch ( rate url ) . headers if self . rate limit header in headers : remaining = int ( headers [ self . rate limit header ] ) except requests . exceptions . HTTP Error as error : logger . warning ( \"Rate limit not initialized: %s\" , error ) return remaining", "predictions": ["get a unicode output for the given output stream ."], "references": ["return token s remaining api points"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1193, "code": "def get tokens rate limits ( self ) : remainings = [ 0 ] * self . n tokens arch = self . archive self . archive = None for idx , token in enumerate ( self . tokens ) : remainings [ idx ] = self . get token rate limit ( token ) self . archive = arch logger . debug ( \"Remaining API points: {}\" . format ( remainings ) ) return remainings", "predictions": ["get the ( possibly empty if available if available if available if not ."], "references": ["return array of all tokens remaining api points"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 1194, "code": "def choose best api token ( self ) : if self . n tokens == 0 : return token idx = 0 if self . n tokens > 1 : remainings = self . get tokens rate limits ( ) token idx = remainings . index ( max ( remainings ) ) logger . debug ( \"Remaining API points: {}, choosen index: {}\" . format ( remainings , token idx ) ) self . current token = self . tokens [ token idx ] self . session . headers . update ( { 'Authorization' : 'token ' + self . current token } ) self . update current rate limit ( )", "predictions": ["get a list of and if we are going to try to send any and use this to get the and we have to do so ."], "references": ["check all api tokens defined and choose one with most remaining api points"], "bleu": 0.044915755686574035, "rouge_l": 0.053368328958880135}
{"id": 1195, "code": "def need check tokens ( self ) : if self . n tokens <= 1 or self . rate limit is None : return False elif self . last rate limit checked is None : self . last rate limit checked = self . rate limit return True approaching limit = float ( self . min rate to sleep ) * ( 1.0 + TOKEN USAGE BEFORE SWITCH ) + 1 if self . rate limit <= approaching limit : self . last rate limit checked = self . rate limit return True ratio = float ( self . rate limit ) / float ( self . last rate limit checked ) if ratio < 1.0 - TOKEN USAGE BEFORE SWITCH : self . last rate limit checked = self . rate limit return True elif ratio > 1.0 : self . last rate limit checked = self . rate limit return False else : return False", "predictions": ["forward over time spent for the prefix ."], "references": ["check if we need to switch github api tokens"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 1196, "code": "def update current rate limit ( self ) : url = urijoin ( self . base url , \"rate limit\" ) try : arch = self . archive self . archive = None response = super ( ) . fetch ( url ) self . archive = arch self . update rate limit ( response ) self . last rate limit checked = self . rate limit except requests . exceptions . HTTP Error as error : if error . response . status code == 404 : logger . warning ( \"Rate limit not initialized: %s\" , error ) else : raise error", "predictions": ["updates an object in the message ."], "references": ["update rate limits data for the current token"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1197, "code": "def load metadata ( self ) : logger . debug ( \"Loading metadata infomation of archive %s\" , self . archive path ) cursor = self . db . cursor ( ) select stmt = \"SELECT origin, backend name, backend version, \" \"category, backend params, created on \" \"FROM \" + self . METADATA TABLE + \" \" \"LIMIT 1\" cursor . execute ( select stmt ) row = cursor . fetchone ( ) cursor . close ( ) if row : self . origin = row [ 0 ] self . backend name = row [ 1 ] self . backend version = row [ 2 ] self . category = row [ 3 ] self . backend params = pickle . loads ( row [ 4 ] ) self . created on = str to datetime ( row [ 5 ] ) else : logger . debug ( \"Metadata of archive %s was empty\" , self . archive path ) logger . debug ( \"Metadata of archive %s loaded\" , self . archive path )", "predictions": ["corrects the database with filename read from the database ."], "references": ["load metadata from the archive file"], "bleu": 0.16590387014219712, "rouge_l": 0.26180257510729615}
{"id": 1198, "code": "def count table rows ( self , table name ) : cursor = self . db . cursor ( ) select stmt = \"SELECT COUNT(*) FROM \" + table name try : cursor . execute ( select stmt ) row = cursor . fetchone ( ) except sqlite3 . Database Error as e : msg = \"invalid archive file; cause: %s\" % str ( e ) raise Archive Error ( cause = msg ) finally : cursor . close ( ) return row [ 0 ]", "predictions": ["counts how many rows have been written to this column ."], "references": ["fetch the number of rows in a table"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1199, "code": "def search archives ( self , origin , backend name , category , archived after ) : for archive path in self . search files ( ) : try : archive = Archive ( archive path ) except Archive Error : continue match = archive . origin == origin and archive . backend name == backend name and archive . category == category and archive . created on >= archived after if not match : continue yield archive path , archive . created on", "predictions": ["scans the cmd for the : ) ."], "references": ["search archives using filters ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1200, "code": "def search files ( self ) : for root , , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) yield location", "predictions": ["searches for all ( files in this directory ."], "references": ["retrieve the file paths stored under the base path ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1201, "code": "def repository ( self , owner , repository ) : url = urijoin ( self . base url , self . RREPOSITORY , owner , repository ) logger . debug ( \"Docker Hub client requests: %s\" , url ) response = self . fetch ( url ) return response . text", "predictions": ["get the fetch message for the fetch request ."], "references": ["fetch information about a repository ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1202, "code": "def get fields ( self ) : url = urijoin ( self . base url , self . RESOURCE , self . VERSION API , 'field' ) req = self . fetch ( url ) return req . text", "predictions": ["get the text for the notebook ."], "references": ["retrieve all the fields available ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1203, "code": "def get builds ( self , job name ) : if self . blacklist jobs and job name in self . blacklist jobs : logger . warning ( \"Not getting blacklisted job: %s\" , job name ) return payload = { 'depth' : self . detail depth } url build = urijoin ( self . base url , \"job\" , job name , \"api\" , \"json\" ) response = self . fetch ( url build , payload = payload ) return response . text", "predictions": ["get a response for the given refs ."], "references": ["retrieve all builds from a job"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1204, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) group = parser . parser . add argument group ( 'Stack Exchange arguments' ) group . add argument ( '--site' , dest = 'site' , required = True , help = \"Stack Exchange site\" ) group . add argument ( '--tagged' , dest = 'tagged' , help = \"filter items by question Tag\" ) group . add argument ( '--max-questions' , dest = 'max questions' , type = int , default = MAX QUESTIONS , help = \"Maximum number of questions requested in the same query\" ) return parser", "predictions": ["creates ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( . remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote"], "references": ["returns the stackexchange argument parser ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 1205, "code": "def get max date ( self , reviews ) : max ts = 0 for review in reviews : ts = str to datetime ( review [ 'timestamp' ] ) ts = datetime to utc ( ts ) if ts . timestamp ( ) > max ts : max ts = ts . timestamp ( ) return max ts", "predictions": ["this method will return the most recently added timestamp based on the maximum date of the review ."], "references": ["get the max date in unixtime format from reviews ."], "bleu": 0.08097785064266204, "rouge_l": 0.2259259259259259}
{"id": 1206, "code": "def get pages ( self , namespace , apcontinue = '' ) : params = { \"action\" : \"query\" , \"list\" : \"allpages\" , \"aplimit\" : self . limit , \"apnamespace\" : namespace , \"format\" : \"json\" } if apcontinue : params [ 'apcontinue' ] = apcontinue return self . call ( params )", "predictions": ["get all cmd that have been requested to the ) ."], "references": ["retrieve all pages from a namespace starting from apcontinue ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 1207, "code": "def get recent pages ( self , namespaces , rccontinue = '' ) : namespaces . sort ( ) params = { \"action\" : \"query\" , \"list\" : \"recentchanges\" , \"rclimit\" : self . limit , \"rcnamespace\" : \"|\" . join ( namespaces ) , \"rcprop\" : \"title|timestamp|ids\" , \"format\" : \"json\" } if rccontinue : params [ 'rccontinue' ] = rccontinue return self . call ( params )", "predictions": ["get all ( possibly declared result result result result result result result result result result result result result result result result ."], "references": ["retrieve recent pages from all namespaces starting from rccontinue ."], "bleu": 0.0612957497932821, "rouge_l": 0.13406593406593406}
{"id": 1208, "code": "def retrieve archives ( self , from date ) : archives = [ ] candidates = self . list supybot archives ( ) for candidate in candidates : dt = self . parse date from filepath ( candidate ) if dt . date ( ) >= from date . date ( ) : archives . append ( ( dt , candidate ) ) else : logger . debug ( \"Archive %s stored before %s; skipped\" , candidate , str ( from date ) ) archives . sort ( key = lambda x : x [ 0 ] ) return [ archive [ 1 ] for archive in archives ]", "predictions": ["hits data of this object and returns a list of instances of this object ."], "references": ["retrieve the supybot archives after the given date"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 1209, "code": "def list supybot archives ( self ) : archives = [ ] for root , , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) archives . append ( location ) return archives", "predictions": ["this method returns a list of all products below the database ."], "references": ["list the filepath of the archives stored in dirpath"], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 1210, "code": "def capabilities url ( self , service url ) : qs = [ ] if service url . find ( '?' ) != - 1 : qs = cgi . parse qsl ( service url . split ( '?' ) [ 1 ] ) params = [ x [ 0 ] for x in qs ] if 'service' not in params : qs . append ( ( 'service' , 'WFS' ) ) if 'request' not in params : qs . append ( ( 'request' , 'Get Capabilities' ) ) if 'version' not in params : qs . append ( ( 'version' , self . version ) ) urlqs = urlencode ( tuple ( qs ) ) return service url . split ( '?' ) [ 0 ] + '?' + urlqs", "predictions": ["method for all links that start with this fetch ."], "references": ["return a capabilities url"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1211, "code": "def parse result ( self ) : if self . result is not None : result = self . result . find ( nspv ( \"wml2:Measurement Timeseries\" ) ) self . result = Measurement Timeseries ( result )", "predictions": ["a gstring get back to the object ."], "references": ["parse the result element of the observation type"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1212, "code": "def complex input with reference ( ) : print ( \"\\ncomplex input with reference ...\" ) wps = Web Processing Service ( 'http://localhost:8094/wps' , verbose = verbose ) processid = 'wordcount' textdoc = Complex Data Input ( \"http://www.gutenberg.org/files/28885/28885-h/28885-h.htm\" ) inputs = [ ( \"text\" , textdoc ) ] outputs = [ ( \"output\" , True , 'some/mime-type' ) ] execution = wps . execute ( processid , inputs , output = outputs ) monitor Execution ( execution ) print ( 'percent complete' , execution . percent Completed ) print ( 'status message' , execution . status Message ) for output in execution . process Outputs : print ( 'identifier=%s, data Type=%s, data=%s, reference=%s' % ( output . identifier , output . data Type , output . data , output . reference ) )", "predictions": ["creates a get from the command ."], "references": ["use complexdatainput with a reference to a document"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1213, "code": "def normalize ( s ) : if ( nonorm ) : return s . split ( ) try : s . split ( ) except : s = \" \" . join ( s ) for ( pattern , replace ) in normalize1 : s = re . sub ( pattern , replace , s ) s = xml . sax . saxutils . unescape ( s , { '&quot;' : '\"' } ) s = \" %s \" % s if not preserve case : s = s . lower ( ) return [ tok for tok in normalize3 . split ( s ) if tok and tok != ' ' ]", "predictions": ["replace placeholders that are not printable"], "references": ["normalize and tokenize text . this is lifted from nist mteval - v11a . pl ."], "bleu": 0.03508375277329034, "rouge_l": 0.0}
{"id": 1214, "code": "def erfcc ( x ) : z = abs ( x ) t = 1 / ( 1 + 0.5 * z ) r = t * math . exp ( - z * z - 1.26551223 + t * ( 1.00002368 + t * ( .37409196 + t * ( .09678418 + t * ( - .18628806 + t * ( .27886807 + t * ( - 1.13520398 + t * ( 1.48851587 + t * ( - .82215223 + t * .17087277 ) ) ) ) ) ) ) ) ) if ( x >= 0. ) : return r else : return 2. - r", "predictions": ["get the cumulative distribution for the given linear time ."], "references": ["complementary error function ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 1215, "code": "def log calls ( func ) : def wrapper ( * args , * * kargs ) : call Str = \"%s(%s)\" % ( func . name , \", \" . join ( [ repr ( p ) for p in args ] + [ \"%s=%s\" % ( k , repr ( v ) ) for ( k , v ) in list ( kargs . items ( ) ) ] ) ) debug ( \">> %s\" , call Str ) ret = func ( * args , * * kargs ) debug ( \"<< %s: %s\" , call Str , repr ( ret ) ) return ret return wrapper", "predictions": ["get a summary of input from the function call with a single ( ."], "references": ["decorator to log function calls ."], "bleu": 0.09782375748961449, "rouge_l": 0.21554770318021202}
{"id": 1216, "code": "def synchronized ( func ) : func . lock = threading . Lock ( ) def synced func ( * args , * * kargs ) : with func . lock : return func ( * args , * * kargs ) return synced func", "predictions": ["first casts a method ."], "references": ["decorator to synchronize function ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1217, "code": "def message ( msg , * args ) : clear progress ( ) text = ( msg % args ) sys . stdout . write ( text + '\\n' )", "predictions": ["create a formatted message with the given message and text ."], "references": ["program message output ."], "bleu": 0.12605968092174913, "rouge_l": 0.2911694510739857}
{"id": 1218, "code": "def tempfile get ( target ) : fn = '%s-%s.tmp' % ( target , '' . join ( random . Random ( ) . sample ( \"0123456789abcdefghijklmnopqrstuvwxyz\" , 15 ) ) ) TEMP FILES . add ( fn ) return fn", "predictions": ["add a target to the target"], "references": ["get a temp filename for atomic download ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1219, "code": "def tempfile set ( tempfile , target ) : if target : os . rename ( tempfile , target ) else : os . unlink ( tempfile ) if target in TEMP FILES : TEMP FILES . remove ( tempfile )", "predictions": ["create a set of objects that can be used as a child process ."], "references": ["atomically rename and clean tempfile"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 1220, "code": "def clean tempfiles ( ) : for fn in TEMP FILES : if os . path . exists ( fn ) : os . unlink ( fn )", "predictions": ["removes files that do not have a workaround method ."], "references": ["clean up temp files"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 1221, "code": "def get fixed path ( self ) : pi = self . path . split ( PATH SEP ) fi = [ ] for p in pi : if '*' in p or '?' in p : break fi . append ( p ) return PATH SEP . join ( fi )", "predictions": ["from this path . returns a fixed path for each resource ."], "references": ["get the fixed part of the path without wildcard"], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 1222, "code": "def get legal params ( self , method ) : if method not in self . client . meta . method to api mapping : return [ ] api = self . client . meta . method to api mapping [ method ] shape = self . client . meta . service model . operation model ( api ) . input shape if shape is None : return [ ] return shape . members . keys ( )", "predictions": ["convert a method call to a legal method ."], "references": ["given a api name list all legal parameters using boto3 service model ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 1223, "code": "def add options ( parser ) : for param , param type , param doc in Boto Client . EXTRA CLIENT PARAMS : parser . add option ( '--API-' + param , help = param doc , type = param type , dest = param )", "predictions": ["add a new parser to the list of options"], "references": ["add the whole list of api parameters into optparse ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 1224, "code": "def add task ( self , func name , * args , * * kargs ) : self . tasks . put ( ( func name , 0 , args , kargs ) )", "predictions": ["a task to add a single task at the end of the arguments ."], "references": ["utility function to add a single task into task queue"], "bleu": 0.33649324423301513, "rouge_l": 0.42957746478873243}
{"id": 1225, "code": "def join ( self ) : self . tasks . join ( ) for worker in self . workers : self . tasks . put ( None ) for worker in self . workers : worker . join ( ) worker . s3 = None", "predictions": ["join all worker intercept ."], "references": ["utility function to wait all tasks to complete"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1226, "code": "def processed ( self ) : self . processed tasks += 1 qsize = self . tasks . qsize ( ) if qsize > 0 : progress ( '[%d task(s) completed, %d remaining, %d thread(s)]' , self . processed tasks , qsize , len ( self . workers ) ) else : progress ( '[%d task(s) completed, %d thread(s)]' , self . processed tasks , len ( self . workers ) )", "predictions": ["initializes the stats based on the given object ."], "references": ["increase the processed task counter and show progress message"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 1227, "code": "def s3 keys from env ( ) : env = os . environ if S3 ACCESS KEY NAME in env and S3 SECRET KEY NAME in env : keys = ( env [ S3 ACCESS KEY NAME ] , env [ S3 SECRET KEY NAME ] ) debug ( \"read S3 keys from environment\" ) return keys else : return None", "predictions": ["get a list of environment keys ."], "references": ["retrieve s3 access keys from the environment or none if not present ."], "bleu": 0.09374222649442905, "rouge_l": 0.1897356143079316}
{"id": 1228, "code": "def s3 keys from cmdline ( opt ) : if opt . access key != None and opt . secret key != None : keys = ( opt . access key , opt . secret key ) debug ( \"read S3 keys from commandline\" ) return keys else : return None", "predictions": ["find a html key from a salt ."], "references": ["retrieve s3 access keys from the command line or none if not present ."], "bleu": 0.08383280652235028, "rouge_l": 0.1732954545454545}
{"id": 1229, "code": "def s3 keys from s3cfg ( opt ) : try : if opt . s3cfg != None : s3cfg path = \"%s\" % opt . s3cfg else : s3cfg path = \"%s/.s3cfg\" % os . environ [ \"HOME\" ] if not os . path . exists ( s3cfg path ) : return None config = Config Parser . Config Parser ( ) config . read ( s3cfg path ) keys = config . get ( \"default\" , \"access key\" ) , config . get ( \"default\" , \"secret key\" ) debug ( \"read S3 keys from %s file\" , s3cfg path ) return keys except Exception as e : info ( \"could not read S3 keys from %s file; skipping (%s)\" , s3cfg path , e ) return None", "predictions": ["get a s3 s3 object using the provided s3cfg ."], "references": ["retrieve s3 access key settings from s3cmd s config file if present ; otherwise return none ."], "bleu": 0.06927760750451363, "rouge_l": 0.14153132250580047}
{"id": 1230, "code": "def init s3 keys ( opt ) : S3Handler . S3 KEYS = S3Handler . s3 keys from cmdline ( opt ) or S3Handler . s3 keys from env ( ) or S3Handler . s3 keys from s3cfg ( opt )", "predictions": ["called from ( . . . . . ( . ( . ( . ( . ( . . . . . . . . . . . . . . . . . ( . . . ( . . . . . . . . . . ."], "references": ["initialize s3 access keys from environment variable or s3cfg config file ."], "bleu": 0.026594139297659906, "rouge_l": 0.07253269916765755}
{"id": 1231, "code": "def connect ( self ) : try : if S3Handler . S3 KEYS : self . s3 = Boto Client ( self . opt , S3Handler . S3 KEYS [ 0 ] , S3Handler . S3 KEYS [ 1 ] ) else : self . s3 = Boto Client ( self . opt ) except Exception as e : raise Retry Failure ( 'Unable to connect to s3: %s' % e )", "predictions": ["connect to a specified amount of connections ."], "references": ["connect to s3 storage"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 1232, "code": "def local walk ( self , basedir ) : result = [ ] for root , dirs , files in os . walk ( basedir ) : for f in files : result . append ( os . path . join ( root , f ) ) return result", "predictions": ["do a full copy of this directory ."], "references": ["walk through local directories from root basedir"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1233, "code": "def put single file ( self , pool , source , target ) : if os . path . isdir ( source ) : if self . opt . recursive : for f in ( f for f in self . local walk ( source ) if not os . path . isdir ( f ) ) : target url = S3URL ( target ) joined path = os . path . normpath ( os . path . join ( target url . path , os . path . relpath ( f , source ) ) ) pool . upload ( f , S3URL . combine ( 's3' , target url . bucket , joined path ) ) else : message ( 'omitting directory \"%s\".' % source ) else : pool . upload ( source , target )", "predictions": ["add a single photo ."], "references": ["upload a single file or a directory by adding a task into queue"], "bleu": 0.07254224910650854, "rouge_l": 0.20573355817875214}
{"id": 1234, "code": "def create bucket ( self , source ) : s3url = S3URL ( source ) message ( 'Creating %s' , source ) if not self . opt . dry run : resp = self . s3 . create bucket ( Bucket = s3url . bucket ) if resp [ 'Response Metadata' ] [ \"HTTP Status Code\" ] == 200 : message ( 'Done.' ) else : raise Failure ( 'Unable to create bucket %s' % source )", "predictions": ["create a bucket in the s3 bucket ."], "references": ["use the create_bucket api to create a new bucket"], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 1235, "code": "def update privilege ( self , obj , target ) : if 'privilege' in obj [ 'Metadata' ] : os . chmod ( target , int ( obj [ 'Metadata' ] [ 'privilege' ] , 8 ) )", "predictions": ["prevent prevent prevent prevent serialization from being updated ."], "references": ["get privileges from metadata of the source in s3 and apply them to target"], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 1236, "code": "def print files ( self , source ) : sources = self . source expand ( source ) for source in sources : s3url = S3URL ( source ) response = self . s3 . get object ( Bucket = s3url . bucket , Key = s3url . path ) message ( '%s' , response [ 'Body' ] . read ( ) )", "predictions": ["prints all the files in the provided source ."], "references": ["print out a series of files"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1237, "code": "def get single file ( self , pool , source , target ) : if source [ - 1 ] == PATH SEP : if self . opt . recursive : basepath = S3URL ( source ) . path for f in ( f for f in self . s3walk ( source ) if not f [ 'is dir' ] ) : pool . download ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) ) else : message ( 'omitting directory \"%s\".' % source ) else : pool . download ( source , target )", "predictions": ["run all ( tests under source ."], "references": ["download a single file or a directory by adding a task into queue"], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 1238, "code": "def cp single file ( self , pool , source , target , delete source ) : if source [ - 1 ] == PATH SEP : if self . opt . recursive : basepath = S3URL ( source ) . path for f in ( f for f in self . s3walk ( source ) if not f [ 'is dir' ] ) : pool . copy ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) , delete source = delete source ) else : message ( 'omitting directory \"%s\".' % source ) else : pool . copy ( source , target , delete source = delete source )", "predictions": ["copies stats for a single file ."], "references": ["copy a single file or a directory by adding a task into queue"], "bleu": 0.14671451318816847, "rouge_l": 0.2846034214618974}
{"id": 1239, "code": "def del files ( self , source ) : src files = [ ] for obj in self . s3walk ( source ) : if not obj [ 'is dir' ] : src files . append ( obj [ 'name' ] ) pool = Thread Pool ( Thread Util , self . opt ) pool . batch delete ( src files ) pool . join ( )", "predictions": ["iterates over all files in a class array ."], "references": ["delete files on s3"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 1240, "code": "def dsync files ( self , source , target ) : src s3 url = S3URL . is valid ( source ) dst s3 url = S3URL . is valid ( target ) source list = self . relative dir walk ( source ) if len ( source list ) == 0 or '.' in source list : raise Failure ( 'Sync command need to sync directory to directory.' ) sync list = [ ( os . path . join ( source , f ) , os . path . join ( target , f ) ) for f in source list ] pool = Thread Pool ( Thread Util , self . opt ) if src s3 url and not dst s3 url : for src , dest in sync list : pool . download ( src , dest ) elif not src s3 url and dst s3 url : for src , dest in sync list : pool . upload ( src , dest ) elif src s3 url and dst s3 url : for src , dest in sync list : pool . copy ( src , dest ) else : raise Invalid Argument ( 'Cannot sync two local directories.' ) pool . join ( ) if self . opt . delete removed : target list = self . relative dir walk ( target ) remove list = [ os . path . join ( target , f ) for f in ( set ( target list ) - set ( source list ) ) ] if S3URL . is valid ( target ) : pool = Thread Pool ( Thread Util , self . opt ) pool . batch delete ( remove list ) pool . join ( ) else : for f in remove list : try : os . unlink ( f ) message ( 'Delete %s' , f ) except : pass", "predictions": ["this method will download all of the files from one location to another . it will only download the dsync and . that are then copied from the list of all the files ."], "references": ["sync directory to directory ."], "bleu": 0.039307696466998686, "rouge_l": 0.11844660194174757}
{"id": 1241, "code": "def file hash ( self , filename , block size = 2 ** 20 ) : m = hashlib . md5 ( ) with open ( filename , 'rb' ) as f : while True : data = f . read ( block size ) if not data : break m . update ( data ) return m . hexdigest ( )", "predictions": ["returns the full hash for this file ."], "references": ["calculate md5 hash code for a local file"], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 1242, "code": "def get md5 ( self ) : if self . md5 is None : self . md5 = self . file hash ( self . filename ) return self . md5", "predictions": ["this method returns the hash code for this operation ."], "references": ["get or calculate md5 value of the local file ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 1243, "code": "def mkdirs ( self , target ) : path = os . path . dirname ( target ) if path and path != PATH SEP and not os . path . isdir ( path ) : try : os . makedirs ( path ) except OS Error as ose : if ose . errno != errno . EEXIST : raise Failure ( 'Unable to create directory (%s)' % ( path , ) )", "predictions": ["creates a directory . this is a warning if the directory cannot be created ."], "references": ["ensure all directories are created for a given target file ."], "bleu": 0.09782375748961449, "rouge_l": 0.1582360570687419}
{"id": 1244, "code": "def conditional ( self , result , obj ) : fileonly = ( self . opt . last modified before is not None ) or ( self . opt . last modified after is not None ) if obj [ 'is dir' ] : if not fileonly : result . append ( obj ) return if ( self . opt . last modified before is not None ) and obj [ 'last modified' ] >= self . opt . last modified before : return if ( self . opt . last modified after is not None ) and obj [ 'last modified' ] <= self . opt . last modified after : return result . append ( obj )", "predictions": ["conditional a method that returns a conditional object ."], "references": ["check all file item with given conditions ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 1245, "code": "def get file privilege ( self , source ) : try : return str ( oct ( os . stat ( source ) . st mode ) [ - 3 : ] ) except Exception as e : raise Failure ( 'Could not get stat for %s, error message = %s' , source , e )", "predictions": ["retrieves the privilege from this file ."], "references": ["get privileges of a local file"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1246, "code": "def lookup ( self , s3url ) : try : return self . s3 . head object ( Bucket = s3url . bucket , Key = s3url . path ) except Boto Client . Client Error as e : if e . response [ 'Response Metadata' ] [ 'HTTP Status Code' ] == 404 : return None else : raise e", "predictions": ["lookup a s3 object by name ."], "references": ["get the s3 object with the s3 url . return none if not exist ."], "bleu": 0.0837738790831083, "rouge_l": 0.2559440559440559}
{"id": 1247, "code": "def read file chunk ( self , source , pos , chunk ) : if chunk == 0 : return String IO ( ) data = None with open ( source , 'rb' ) as f : f . seek ( pos ) data = f . read ( chunk ) if not data : raise Failure ( 'Unable to read data from source: %s' % source ) return String IO ( data )", "predictions": ["read a file from the chunk ."], "references": ["read local file chunk"], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 1248, "code": "def upload ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : s3url = S3URL ( target ) obj = self . lookup ( s3url ) if not mpi : fsize = os . path . getsize ( source ) md5cache = Local MD5Cache ( source ) if self . opt . dry run : message ( '%s => %s' , source , target ) return elif self . opt . sync check and self . sync check ( md5cache , obj ) : message ( '%s => %s (synced)' , source , target ) return elif not self . opt . force and obj : raise Failure ( 'File already exists: %s' % target ) if fsize < self . opt . max singlepart upload size : data = self . read file chunk ( source , 0 , fsize ) self . s3 . put object ( Bucket = s3url . bucket , Key = s3url . path , Body = data , Metadata = { 'md5' : md5cache . get md5 ( ) , 'privilege' : self . get file privilege ( source ) } ) message ( '%s => %s' , source , target ) return response = self . s3 . create multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Metadata = { 'md5' : md5cache . get md5 ( ) , 'privilege' : self . get file privilege ( source ) } ) upload id = response [ 'Upload Id' ] for args in self . get file splits ( upload id , source , target , fsize , self . opt . multipart split size ) : self . pool . upload ( * args ) return data = self . read file chunk ( source , pos , chunk ) response = self . s3 . upload part ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id , Body = data , Part Number = part ) if mpi . complete ( { 'E Tag' : response [ 'E Tag' ] , 'Part Number' : part } ) : try : self . s3 . complete multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id , Multipart Upload = { 'Parts' : mpi . sorted parts ( ) } ) message ( '%s => %s' , source , target ) except Exception as e : message ( 'Unable to complete upload: %s' , str ( e ) ) self . s3 . abort multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id ) raise Retry Failure ( 'Upload failed: Unable to complete upload %s.' % source )", "predictions": ["uploads a synchronized to the s3 ."], "references": ["thread worker for upload operation ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1249, "code": "def verify file size ( self , obj , downloaded file ) : file size = os . path . getsize ( downloaded file ) if int ( obj [ 'Content Length' ] ) != file size : raise Retry Failure ( 'Downloaded file size inconsistent: %s' % ( repr ( obj ) ) )", "predictions": ["verifies and verifies the ( stored in this file exists clear clear clear clear clear ( clear clear clear clear clear clear clear clear clear clear clear clear clear clear ( clear clear clear clear clear clear clear clear clear clear clear clear clear clear clear clear clear clear clear"], "references": ["verify the file size of the downloaded file ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 1250, "code": "def write file chunk ( self , target , pos , chunk , body ) : fd = os . open ( target , os . O CREAT | os . O WRONLY ) try : os . lseek ( fd , pos , os . SEEK SET ) data = body . read ( chunk ) num bytes written = os . write ( fd , data ) if ( num bytes written != len ( data ) ) : raise Retry Failure ( 'Number of bytes written inconsistent: %s != %s' % ( num bytes written , sys . getsizeof ( data ) ) ) finally : os . close ( fd )", "predictions": ["writes a ( possibly in the ( or a ( possibly e . g . , the data % % % % % % % % % % % % % % ( % % % % % % % % written % % % % % % % %"], "references": ["write local file chunk"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1251, "code": "def download ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : s3url = S3URL ( source ) obj = self . lookup ( s3url ) if obj is None : raise Failure ( 'The obj \"%s\" does not exists.' % ( s3url . path , ) ) if not mpi : if self . opt . dry run : message ( '%s => %s' , source , target ) return elif self . opt . sync check and self . sync check ( Local MD5Cache ( target ) , obj ) : message ( '%s => %s (synced)' , source , target ) return elif not self . opt . force and os . path . exists ( target ) : raise Failure ( 'File already exists: %s' % target ) fsize = int ( obj [ 'Content Length' ] ) if fsize < self . opt . max singlepart download size : mpi = Thread Util . Multipart Item ( tempfile get ( target ) ) mpi . total = 1 pos = 0 chunk = fsize else : for args in self . get file splits ( tempfile get ( target ) , source , target , fsize , self . opt . multipart split size ) : self . pool . download ( * args ) return tempfile = mpi . id if self . opt . recursive : self . mkdirs ( tempfile ) response = self . s3 . get object ( Bucket = s3url . bucket , Key = s3url . path , Range = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) ) self . write file chunk ( tempfile , pos , chunk , response [ 'Body' ] ) if mpi . complete ( { 'Part Number' : part } ) : try : self . update privilege ( obj , tempfile ) self . verify file size ( obj , tempfile ) tempfile set ( tempfile , target ) message ( '%s => %s' , source , target ) except Exception as e : tempfile set ( tempfile , None ) raise Failure ( 'Download Failure: %s, Source: %s.' % ( e . message , source ) )", "predictions": ["retrieves an event from the local machine ."], "references": ["thread worker for download operation ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1252, "code": "def copy ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 , delete source = False ) : if self . opt . dry run : message ( '%s => %s' % ( source , target ) ) return source url = S3URL ( source ) target url = S3URL ( target ) if not mpi : obj = self . lookup ( source url ) fsize = int ( obj [ 'Content Length' ] ) if fsize < self . opt . max singlepart copy size : self . s3 . copy object ( Bucket = target url . bucket , Key = target url . path , Copy Source = { 'Bucket' : source url . bucket , 'Key' : source url . path } ) message ( '%s => %s' % ( source , target ) ) if delete source : self . delete ( source ) return response = self . s3 . create multipart upload ( Bucket = target url . bucket , Key = target url . path , Metadata = obj [ 'Metadata' ] ) upload id = response [ 'Upload Id' ] for args in self . get file splits ( upload id , source , target , fsize , self . opt . multipart split size ) : self . pool . copy ( * args , delete source = delete source ) return response = self . s3 . upload part copy ( Bucket = target url . bucket , Key = target url . path , Copy Source = { 'Bucket' : source url . bucket , 'Key' : source url . path } , Copy Source Range = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) , Upload Id = mpi . id , Part Number = part ) if mpi . complete ( { 'E Tag' : response [ 'Copy Part Result' ] [ 'E Tag' ] , 'Part Number' : part } ) : try : self . s3 . complete multipart upload ( Bucket = target url . bucket , Key = target url . path , Upload Id = mpi . id , Multipart Upload = { 'Parts' : mpi . sorted parts ( ) } ) if delete source : self . delete ( source ) message ( '%s => %s' % ( source , target ) ) except Exception as e : message ( 'Unable to complete upload: %s' , str ( e ) ) self . s3 . abort multipart upload ( Bucket = source url . bucket , Key = source url . path , Upload Id = mpi . id ) raise Retry Failure ( 'Copy failed: Unable to complete copy %s.' % source )", "predictions": ["copies all tasks from this : upload to the s3 location ."], "references": ["copy a single file from source to target using boto s3 library ."], "bleu": 0.12020484516681697, "rouge_l": 0.31770833333333337}
{"id": 1253, "code": "def delete ( self , source ) : s3url = S3URL ( source ) message ( 'Delete %s' , source ) if not self . opt . dry run : self . s3 . delete object ( Bucket = s3url . bucket , Key = s3url . path )", "predictions": ["deletes an existing unit in the database ."], "references": ["thread worker for download operation ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1254, "code": "def run ( self , args ) : if len ( args ) == 0 : raise Invalid Argument ( 'No command provided' ) cmd = args [ 0 ] if cmd + ' handler' in Command Handler . dict : Command Handler . dict [ cmd + ' handler' ] ( self , args ) else : raise Invalid Argument ( 'Unknown command %s' % cmd )", "predictions": ["runs the specified command ."], "references": ["main entry to handle commands . dispatch to individual command handler ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 1255, "code": "def ls handler ( self , args ) : if len ( args ) == 1 : self . pretty print ( self . s3handler ( ) . list buckets ( ) ) return self . validate ( 'cmd|s3' , args ) self . pretty print ( self . s3handler ( ) . s3walk ( args [ 1 ] ) )", "predictions": ["a simple args of this method ."], "references": ["handler for ls command"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1256, "code": "def mb handler ( self , args ) : if len ( args ) == 1 : raise Invalid Argument ( 'No s3 bucketname provided' ) self . validate ( 'cmd|s3' , args ) self . s3handler ( ) . create bucket ( args [ 1 ] )", "predictions": ["creates a default add to the arguments ."], "references": ["handler for mb command"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1257, "code": "def put handler ( self , args ) : if len ( args ) < 3 : raise Invalid Argument ( 'Invalid number of parameters' ) self . validate ( '|' . join ( [ 'cmd' ] + [ 'local' ] * ( len ( args ) - 2 ) + [ 's3' ] ) , args ) source = args [ 1 : - 1 ] target = args [ - 1 ] self . s3handler ( ) . put files ( source , target )", "predictions": ["puts a ( ( ( tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks tasks are stored as an array of bytes , both are stored as a java . lang . boolean tasks are stored as they are stored as much"], "references": ["handler for put command"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1258, "code": "def get handler ( self , args ) : if len ( args ) == 2 : args += [ '.' ] self . validate ( 'cmd|s3|local' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . get files ( source , target )", "predictions": ["this is used to processed the arguments from the map ."], "references": ["handler for get command"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1259, "code": "def cat handler ( self , args ) : self . validate ( 'cmd|s3' , args ) source = args [ 1 ] self . s3handler ( ) . print files ( source )", "predictions": ["a utility method for downloading this object ."], "references": ["handler for cat command"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1260, "code": "def dsync handler ( self , args ) : self . opt . recursive = True self . opt . sync check = True self . opt . force = True self . validate ( 'cmd|s3,local|s3,local' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . dsync files ( source , target )", "predictions": ["generate a series of ( ."], "references": ["handler for dsync command ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1261, "code": "def cp handler ( self , args ) : self . validate ( 'cmd|s3|s3' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . cp files ( source , target )", "predictions": ["creates a new ( ."], "references": ["handler for cp command"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1262, "code": "def mv handler ( self , args ) : self . validate ( 'cmd|s3|s3' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . cp files ( source , target , delete source = True )", "predictions": ["validates a series of pairs ."], "references": ["handler for mv command"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1263, "code": "def del handler ( self , args ) : self . validate ( 'cmd|s3' , args ) source = args [ 1 ] self . s3handler ( ) . del files ( source )", "predictions": ["sorts the arguments of this class ."], "references": ["handler for del command"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1264, "code": "def du handler ( self , args ) : for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : message ( '%s\\t%s' % ( size , src ) )", "predictions": ["creates a new ("], "references": ["handler for size command"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1265, "code": "def totalsize handler ( self , args ) : total size = 0 for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : total size += size message ( str ( total size ) )", "predictions": ["creates a new resnet-200 over this push push push push bytes into a specific array of bytes ."], "references": ["handler of total_size command"], "bleu": 0.06809398432036522, "rouge_l": 0.1026936026936027}
{"id": 1266, "code": "def match date ( self , value ) : m = self . REGEX DATE . search ( value ) date = datetime . datetime . utcnow ( ) . date ( ) if m : date = datetime . date ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) , int ( m . group ( 3 ) ) ) value = self . REGEX DATE . sub ( '' , value ) return ( date , value )", "predictions": ["create a bucket . the search is used for testing the start of the if the source is enabled and the start position is one of the remaining components ."], "references": ["search for date information in the string"], "bleu": 0.047973925170118475, "rouge_l": 0.18263473053892215}
{"id": 1267, "code": "def match time ( self , value ) : m = self . REGEX TIME . search ( value ) time = datetime . datetime . utcnow ( ) . time ( ) if m : time = datetime . time ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) ) value = self . REGEX TIME . sub ( '' , value ) return ( time , value )", "predictions": ["update the privilege based on the obj provided ."], "references": ["search for time information in the string"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 1268, "code": "def match delta ( self , value ) : m = self . REGEX DELTA . search ( value ) delta = datetime . timedelta ( days = 0 ) if m : d = int ( m . group ( 1 ) ) if m . group ( 3 ) == 'ago' or m . group ( 3 ) == 'before' : d = - d if m . group ( 2 ) == 'minute' : delta = datetime . timedelta ( minutes = d ) elif m . group ( 2 ) == 'hour' : delta = datetime . timedelta ( hours = d ) elif m . group ( 2 ) == 'day' : delta = datetime . timedelta ( days = d ) elif m . group ( 2 ) == 'week' : delta = datetime . timedelta ( weeks = d ) value = self . REGEX DELTA . sub ( '' , value ) return ( delta , value )", "predictions": ["print a datetime datetime datetime ."], "references": ["search for timedelta information in the string"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1269, "code": "def check dict ( self , opt , value ) : try : return json . loads ( value ) except : raise optparse . Option Value Error ( \"Option %s: invalid dict value: %r\" % ( opt , value ) )", "predictions": ["get if user input is valid ."], "references": ["take json as dictionary parameter"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1270, "code": "def get from hub ( self , sid ) : cmd = '{ \"cmd\":\"read\",\"sid\":\"' + sid + '\"}' resp = self . send cmd ( cmd , \"read ack\" ) if int ( self . proto [ 0 : 1 ] ) == 1 else self . send cmd ( cmd , \"read rsp\" ) LOGGER . debug ( \"read ack << %s\" , resp ) return self . push data ( resp )", "predictions": ["creates a file object from this request and get appropriate clip ."], "references": ["get data from gateway"], "bleu": 0.11498759556447223, "rouge_l": 0.13738738738738737}
{"id": 1271, "code": "def push data ( self , data ) : if not validate data ( data ) : return False jdata = json . loads ( data [ 'data' ] ) if int ( self . proto [ 0 : 1 ] ) == 1 else list2map ( data [ 'params' ] ) if jdata is None : return False sid = data [ 'sid' ] for func in self . callbacks [ sid ] : func ( jdata , data ) return True", "predictions": ["del files on in this session ."], "references": ["push data broadcasted from gateway to device"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1272, "code": "def get key ( self ) : init vector = bytes ( bytearray . fromhex ( '17996d093d28ddb3ba695a2e6f58562e' ) ) encryptor = Cipher ( algorithms . AES ( self . key . encode ( ) ) , modes . CBC ( init vector ) , backend = default backend ( ) ) . encryptor ( ) ciphertext = encryptor . update ( self . token . encode ( ) ) + encryptor . finalize ( ) if isinstance ( ciphertext , str ) : return '' . join ( '{:02x}' . format ( ord ( x ) ) for x in ciphertext ) return '' . join ( '{:02x}' . format ( x ) for x in ciphertext )", "predictions": ["creating a files that can be stored as a files ."], "references": ["get key using token from gateway"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1273, "code": "def ensure log handler ( self ) : if log . handlers : return handler = logging . Stream Handler ( ) formatter = logging . Formatter ( '%(asctime)s %(levelname)-5.5s [%(name)s][%(thread Name)s] %(message)s' ) handler . set Formatter ( formatter ) log . add Handler ( handler )", "predictions": ["closes the hash ( if any filename is available filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename"], "references": ["if there s no log configuration set up a default handler ."], "bleu": 0.02403051755364481, "rouge_l": 0.03626634958382877}
{"id": 1274, "code": "def lambda function ( f ) : @ functools . wraps ( f ) def wrapper ( event , context ) : global CURRENT LAMBDA CONTEXT CURRENT LAMBDA CONTEXT = context try : result = f ( event , context ) return wait ( lambda : result ) except : cls , exc , trace = sys . exc info ( ) report exc info ( ( cls , exc , trace . tb next ) ) wait ( ) raise return wrapper", "predictions": ["wraps the method to handle the file ."], "references": ["decorator for making error handling on aws lambda easier"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 1275, "code": "def create agent log ( ) : log file = SETTINGS [ 'agent.log file' ] if not log file . endswith ( '.rollbar' ) : log . error ( \"Provided agent log file does not end with .rollbar, which it must. \" \"Using default instead.\" ) log file = DEFAULTS [ 'agent.log file' ] retval = logging . get Logger ( 'rollbar agent' ) handler = logging . File Handler ( log file , 'a' , 'utf-8' ) formatter = logging . Formatter ( '%(message)s' ) handler . set Formatter ( formatter ) retval . add Handler ( handler ) retval . set Level ( logging . WARNING ) return retval", "predictions": ["creates a self - generated self for the given not ."], "references": ["creates . rollbar log file for use with rollbar - agent"], "bleu": 0.14323145079400493, "rouge_l": 0.18181818181818182}
{"id": 1276, "code": "def add lambda context data ( data ) : global CURRENT LAMBDA CONTEXT context = CURRENT LAMBDA CONTEXT if context is None : return try : lambda data = { 'lambda' : { 'remaining time in millis' : context . get remaining time in millis ( ) , 'function name' : context . function name , 'function version' : context . function version , 'arn' : context . invoked function arn , 'request id' : context . aws request id , } } if 'custom' in data : data [ 'custom' ] = dict merge ( data [ 'custom' ] , lambda data ) else : data [ 'custom' ] = lambda data except Exception as e : log . exception ( \"Exception while adding lambda context data: %r\" , e ) finally : CURRENT LAMBDA CONTEXT = None", "predictions": ["conditional evaluation evaluation between ( and ( ."], "references": ["attempts to add information from the lambda context if it exists"], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 1277, "code": "def add request data ( data , request ) : try : request data = build request data ( request ) except Exception as e : log . exception ( \"Exception while building request data for Rollbar payload: %r\" , e ) else : if request data : filter ip ( request data , SETTINGS [ 'capture ip' ] ) data [ 'request' ] = request data", "predictions": ["adds a file to the end of the list ."], "references": ["attempts to build request data ; if successful sets the request key on data ."], "bleu": 0.0909256598621168, "rouge_l": 0.23164556962025318}
{"id": 1278, "code": "def check add locals ( frame , frame num , total frames ) : return any ( ( ( frame num == total frames - 1 ) , ( 'root' in SETTINGS and ( frame . get ( 'filename' ) or '' ) . lower ( ) . startswith ( ( SETTINGS [ 'root' ] or '' ) . lower ( ) ) ) ) )", "predictions": ["utility method to ( count of return return a new federated . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["returns true if we should record local variables for the given frame ."], "bleu": 0.02614431568998955, "rouge_l": 0.03770086526576019}
{"id": 1279, "code": "def build server data ( ) : server data = { 'host' : socket . gethostname ( ) , 'pid' : os . getpid ( ) } argv = getattr ( sys , 'argv' , None ) if argv : server data [ 'argv' ] = argv for key in [ 'branch' , 'root' ] : if SETTINGS . get ( key ) : server data [ key ] = SETTINGS [ key ] return server data", "predictions": ["read a file of the file ."], "references": ["returns a dictionary containing information about the server environment ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 1280, "code": "def build payload ( data ) : for k , v in iteritems ( data ) : data [ k ] = transform ( v , key = ( k , ) ) payload = { 'access token' : SETTINGS [ 'access token' ] , 'data' : data } return payload", "predictions": ["build the payload from all pairs of this ( ."], "references": ["returns the full payload as a string ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 1281, "code": "def main ( ) : rollbar . init ( 'ACCESS TOKEN' , environment = 'test' , handler = 'twisted' ) factory = protocol . Server Factory ( ) factory . protocol = Echo reactor . listen TCP ( 8000 , factory ) reactor . run ( )", "predictions": ["creates new gemfire server ."], "references": ["this runs the protocol on port 8000"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1282, "code": "def decompose ( hangul letter ) : from . import checker if len ( hangul letter ) < 1 : raise Not Letter Exception ( '' ) elif not checker . is hangul ( hangul letter ) : raise Not Hangul Exception ( '' ) if hangul letter in CHO : return hangul letter , '' , '' if hangul letter in JOONG : return '' , hangul letter , '' if hangul letter in JONG : return '' , '' , hangul letter code = hangul index ( hangul letter ) cho , joong , jong = decompose index ( code ) if cho < 0 : cho = 0 try : return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] except : print ( \"%d / %d  / %d\" % ( cho , joong , jong ) ) print ( \"%s / %s \" % ( JOONG [ joong ] . encode ( \"utf8\" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) raise Exception ( )", "predictions": ["decompose a letter or a string into a hangul ."], "references": ["this function returns letters by decomposing the specified hangul letter ."], "bleu": 0.13564514503163538, "rouge_l": 0.18885448916408668}
{"id": 1283, "code": "def has jongsung ( letter ) : if len ( letter ) != 1 : raise Exception ( 'The target string must be one letter.' ) if not is hangul ( letter ) : raise Not Hangul Exception ( 'The target string must be Hangul' ) code = lt . hangul index ( letter ) return code % NUM JONG > 0", "predictions": ["this method checks if the given letter exists in the database ."], "references": ["check whether this letter contains jongsung"], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 1284, "code": "def attach ( word , josa = EUN NEUN ) : last letter = word . strip ( ) [ - 1 ] try : , , letter jong = letter . decompose ( last letter ) except Not Hangul Exception : letter jong = letter . get substituent of ( last letter ) if letter jong in ( '' , josa [ 'except' ] ) : return word + josa [ 'has' ] return word + josa [ 'not' ]", "predictions": ["attach the word to the given word ."], "references": ["add josa at the end of this word"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1285, "code": "def is inside except ( node ) : current = node while current and not isinstance ( current . parent , astroid . Except Handler ) : current = current . parent return current and current is current . parent . name", "predictions": ["check if the node is set by the given node ."], "references": ["returns true if node is inside the name of an except handler ."], "bleu": 0.1486375602900724, "rouge_l": 0.41049798115746977}
{"id": 1286, "code": "def is inside lambda ( node : astroid . node classes . Node NG ) -> bool : parent = node . parent while parent is not None : if isinstance ( parent , astroid . Lambda ) : return True parent = parent . parent return False", "predictions": ["check if the given node is inside the parent node ."], "references": ["return true if given node is inside lambda"], "bleu": 0.33180774028439425, "rouge_l": 0.5417406749555951}
{"id": 1287, "code": "def get all elements ( node : astroid . node classes . Node NG ) -> Iterable [ astroid . node classes . Node NG ] : if isinstance ( node , ( astroid . Tuple , astroid . List ) ) : for child in node . elts : for e in get all elements ( child ) : yield e else : yield node", "predictions": ["iterates over all the elements of an recently node ."], "references": ["recursively returns all atoms in nested lists and tuples ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 1288, "code": "def is super ( node : astroid . node classes . Node NG ) -> bool : if getattr ( node , \"name\" , None ) == \"super\" and node . root ( ) . name == BUILTINS NAME : return True return False", "predictions": ["returns true if the given node is a local node ."], "references": ["return true if the node is referencing the super builtin function"], "bleu": 0.2521193618434983, "rouge_l": 0.45454545454545453}
{"id": 1289, "code": "def is error ( node : astroid . node classes . Node NG ) -> bool : for child node in node . get children ( ) : if isinstance ( child node , astroid . Raise ) : return True return False", "predictions": ["returns true if the given node is a nested error node ."], "references": ["return true if the function does nothing but raising an exception"], "bleu": 0.19338531381761725, "rouge_l": 0.2629310344827586}
{"id": 1290, "code": "def is builtin object ( node : astroid . node classes . Node NG ) -> bool : return node and node . root ( ) . name == BUILTINS NAME", "predictions": ["returns true if the given object is a builtin object ."], "references": ["returns true if the given node is an object from the __builtin__ module ."], "bleu": 0.351520275689857, "rouge_l": 0.6264441591784339}
{"id": 1291, "code": "def is func decorator ( node : astroid . node classes . Node NG ) -> bool : parent = node . parent while parent is not None : if isinstance ( parent , astroid . Decorators ) : return True if parent . is statement or isinstance ( parent , ( astroid . Lambda , scoped nodes . Comprehension Scope , scoped nodes . List Comp ) , ) : break parent = parent . parent return False", "predictions": ["check if the given node is a function decorator ."], "references": ["return true if the name is used in function decorator"], "bleu": 0.21834177214239062, "rouge_l": 0.5}
{"id": 1292, "code": "def assign parent ( node : astroid . node classes . Node NG ) -> astroid . node classes . Node NG : while node and isinstance ( node , ( astroid . Assign Name , astroid . Tuple , astroid . List ) ) : node = node . parent return node", "predictions": ["assigns the given node to the tree ."], "references": ["return the higher parent which is not an assignname tuple or list node"], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 1293, "code": "def check messages ( * messages : str ) -> Callable : def store messages ( func ) : func . checks msgs = messages return func return store messages", "predictions": ["check a method on the store ."], "references": ["decorator to store messages that are handled by a checker method"], "bleu": 0.1247439242120089, "rouge_l": 0.2136602451838879}
{"id": 1294, "code": "def decorated with property ( node : astroid . Function Def ) -> bool : if not node . decorators : return False for decorator in node . decorators . nodes : if not isinstance ( decorator , astroid . Name ) : continue try : if is property decorator ( decorator ) : return True except astroid . Inference Error : pass return False", "predictions": ["does the given function have the same name as the astroid ."], "references": ["detect if the given function node is decorated with a property ."], "bleu": 0.2044800736021839, "rouge_l": 0.3333333333333333}
{"id": 1295, "code": "def decorated with ( func : astroid . Function Def , qnames : Iterable [ str ] ) -> bool : decorators = func . decorators . nodes if func . decorators else [ ] for decorator node in decorators : try : if any ( i is not None and i . qname ( ) in qnames for i in decorator node . infer ( ) ) : return True except astroid . Inference Error : continue return False", "predictions": ["does the function have the same status as the given function ."], "references": ["determine if the func node has a decorator with the qualified name qname ."], "bleu": 0.10459315495983224, "rouge_l": 0.22761194029850743}
{"id": 1296, "code": "def find try except wrapper node ( node : astroid . node classes . Node NG ) -> Union [ astroid . Except Handler , astroid . Try Except ] : current = node ignores = ( astroid . Except Handler , astroid . Try Except ) while current and not isinstance ( current . parent , ignores ) : current = current . parent if current and isinstance ( current . parent , ignores ) : return current . parent return None", "predictions": ["do a wrapper around the astroid node ."], "references": ["return the excepthandler or the tryexcept node in which the node is ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 1297, "code": "def is from fallback block ( node : astroid . node classes . Node NG ) -> bool : context = find try except wrapper node ( node ) if not context : return False if isinstance ( context , astroid . Except Handler ) : other body = context . parent . body handlers = context . parent . handlers else : other body = itertools . chain . from iterable ( handler . body for handler in context . handlers ) handlers = context . handlers has fallback imports = any ( isinstance ( import node , ( astroid . Import From , astroid . Import ) ) for import node in other body ) ignores import error = except handlers ignores exception ( handlers , Import Error ) return ignores import error or has fallback imports", "predictions": ["checks if the node is a wrapper around the tree ."], "references": ["check if the given node is from a fallback import block ."], "bleu": 0.18722739595973834, "rouge_l": 0.5176803394625177}
{"id": 1298, "code": "def is registered in singledispatch function ( node : astroid . Function Def ) -> bool : singledispatch qnames = ( \"functools.singledispatch\" , \"singledispatch.singledispatch\" , ) if not isinstance ( node , astroid . Function Def ) : return False decorators = node . decorators . nodes if node . decorators else [ ] for decorator in decorators : if not isinstance ( decorator , astroid . Call ) : continue func = decorator . func if not isinstance ( func , astroid . Attribute ) or func . attrname != \"register\" : continue try : func def = next ( func . expr . infer ( ) ) except astroid . Inference Error : continue if isinstance ( func def , astroid . Function Def ) : return decorated with ( func def , singledispatch qnames ) return False", "predictions": ["does the function have a registered decorator ?"], "references": ["check if the given function node is a singledispatch function ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 1299, "code": "def is postponed evaluation enabled ( node : astroid . node classes . Node NG ) -> bool : name = \"annotations\" module = node . root ( ) stmt = module . locals . get ( name ) return ( stmt and isinstance ( stmt [ 0 ] , astroid . Import From ) and stmt [ 0 ] . modname == \" future \" )", "predictions": ["checks if the node is enabled by a astroid ."], "references": ["check if the postponed evaluation of annotations is enabled"], "bleu": 0.2086130724305753, "rouge_l": 0.42508710801393734}
{"id": 1300, "code": "def repr tree defs ( data , indent str = None ) : lines = [ ] nodes = data . items ( ) for i , ( mod , ( sub , files ) ) in enumerate ( sorted ( nodes , key = lambda x : x [ 0 ] ) ) : if not files : files = \"\" else : files = \"(%s)\" % \",\" . join ( sorted ( files ) ) if indent str is None : lines . append ( \"%s %s\" % ( mod , files ) ) sub indent str = \"  \" else : lines . append ( r\"%s\\-%s %s\" % ( indent str , mod , files ) ) if i == len ( nodes ) - 1 : sub indent str = \"%s  \" % indent str else : sub indent str = \"%s| \" % indent str if sub : lines . append ( repr tree defs ( sub , sub indent str ) ) return \"\\n\" . join ( lines )", "predictions": ["produce a list of lines of lines ."], "references": ["return a string which represents imports as a tree"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1301, "code": "def visit import ( self , node ) : self . check reimport ( node ) self . check import as rename ( node ) modnode = node . root ( ) names = [ name for name , in node . names ] if len ( names ) >= 2 : self . add message ( \"multiple-imports\" , args = \", \" . join ( names ) , node = node ) for name in names : self . check deprecated module ( node , name ) self . check preferred module ( node , name ) imported module = self . get imported module ( node , name ) if isinstance ( node . parent , astroid . Module ) : self . check position ( node ) if isinstance ( node . scope ( ) , astroid . Module ) : self . record import ( node , imported module ) if imported module is None : continue self . check relative import ( modnode , node , imported module , name ) self . add imported module ( node , imported module . name )", "predictions": ["replaces subclasses with the same name in this object ."], "references": ["triggered when an import statement is seen"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1302, "code": "def visit importfrom ( self , node ) : basename = node . modname imported module = self . get imported module ( node , basename ) self . check import as rename ( node ) self . check misplaced future ( node ) self . check deprecated module ( node , basename ) self . check preferred module ( node , basename ) self . check wildcard imports ( node , imported module ) self . check same line imports ( node ) self . check reimport ( node , basename = basename , level = node . level ) if isinstance ( node . parent , astroid . Module ) : self . check position ( node ) if isinstance ( node . scope ( ) , astroid . Module ) : self . record import ( node , imported module ) if imported module is None : return modnode = node . root ( ) self . check relative import ( modnode , node , imported module , basename ) for name , in node . names : if name != \"*\" : self . add imported module ( node , \"%s.%s\" % ( imported module . name , name ) ) else : self . add imported module ( node , imported module . name )", "predictions": ["visit the name of the given node ."], "references": ["triggered when a from statement is seen"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1303, "code": "def record import ( self , node , importedmodnode ) : if isinstance ( node , astroid . Import From ) : importedname = node . modname else : importedname = importedmodnode . name if importedmodnode else None if not importedname : importedname = node . names [ 0 ] [ 0 ] . split ( \".\" ) [ 0 ] if isinstance ( node , astroid . Import From ) and ( node . level or 0 ) >= 1 : importedname = \".\" + importedname self . imports stack . append ( ( node , importedname ) )", "predictions": ["wraps the object from a node ."], "references": ["record the package node imports from"], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 1304, "code": "def add imported module ( self , node , importedmodname ) : module file = node . root ( ) . file context name = node . root ( ) . name base = os . path . splitext ( os . path . basename ( module file ) ) [ 0 ] try : importedmodname = astroid . modutils . get module part ( importedmodname , module file ) except Import Error : pass if context name == importedmodname : self . add message ( \"import-self\" , node = node ) elif not astroid . modutils . is standard module ( importedmodname ) : if base != \" init \" and context name not in self . module pkg : self . module pkg [ context name ] = context name . rsplit ( \".\" , 1 ) [ 0 ] importedmodnames = self . stats [ \"dependencies\" ] . setdefault ( importedmodname , set ( ) ) if context name not in importedmodnames : importedmodnames . add ( context name ) self . import graph [ context name ] . add ( importedmodname ) if not self . linter . is message enabled ( \"cyclic-import\" , line = node . lineno ) : self . excluded edges [ context name ] . add ( importedmodname )", "predictions": ["for the module , import and import the import of this class ."], "references": ["notify an imported module used to analyze dependencies"], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 1305, "code": "def check deprecated module ( self , node , mod path ) : for mod name in self . config . deprecated modules : if mod path == mod name or mod path . startswith ( mod name + \".\" ) : self . add message ( \"deprecated-module\" , node = node , args = mod path )", "predictions": ["stage all paths in the module ."], "references": ["check if the module is deprecated"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 1306, "code": "def check preferred module ( self , node , mod path ) : if mod path in self . preferred modules : self . add message ( \"preferred-module\" , node = node , args = ( self . preferred modules [ mod path ] , mod path ) , )", "predictions": ["generate a new modules ."], "references": ["check if the module has a preferred replacement"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1307, "code": "def report external dependencies ( self , sect , , dummy ) : dep info = make tree defs ( self . external dependencies info ( ) . items ( ) ) if not dep info : raise Empty Report Error ( ) tree str = repr tree defs ( dep info ) sect . append ( Verbatim Text ( tree str ) )", "predictions": ["generate a new ( ."], "references": ["return a verbatim layout for displaying dependencies"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1308, "code": "def filter dependencies graph ( self , internal ) : graph = collections . defaultdict ( set ) for importee , importers in self . stats [ \"dependencies\" ] . items ( ) : for importer in importers : package = self . module pkg . get ( importer , importer ) is inside = importee . startswith ( package ) if is inside and internal or not is inside and not internal : graph [ importee ] . add ( importer ) return graph", "predictions": ["filter out packets that are not part of the specified graph ."], "references": ["build the internal or the external depedency graph"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 1309, "code": "def get default options ( ) : options = [ ] home = os . environ . get ( \"HOME\" , \"\" ) if home : rcfile = os . path . join ( home , RCFILE ) try : options = open ( rcfile ) . read ( ) . split ( ) except IO Error : pass return options", "predictions": ["get the command line options ."], "references": ["read config file and return list of options"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1310, "code": "def insert default options ( ) : options = get default options ( ) options . reverse ( ) for arg in options : sys . argv . insert ( 1 , arg )", "predictions": ["insert the set of options to the list ."], "references": ["insert default options to sys . argv"], "bleu": 0.21105340631872635, "rouge_l": 0.5115303983228512}
{"id": 1311, "code": "def show attr ( self , node ) : visibility = get visibility ( getattr ( node , \"name\" , node ) ) return not self . mode & VIS MOD [ visibility ]", "predictions": ["show tree rooted at the end of the node ."], "references": ["return true if the node should be treated"], "bleu": 0.16590387014219712, "rouge_l": 0.22676579925650556}
{"id": 1312, "code": "def get callbacks ( self , node ) : klass = node . class methods = self . cache . get ( klass ) if methods is None : handler = self . handler kid = klass . name . lower ( ) e method = getattr ( handler , \"visit %s\" % kid , getattr ( handler , \"visit default\" , None ) ) l method = getattr ( handler , \"leave %s\" % kid , getattr ( handler , \"leave default\" , None ) ) self . cache [ klass ] = ( e method , l method ) else : e method , l method = methods return e method , l method", "predictions": ["returns a payload for a class ."], "references": ["get callbacks from handler for the visited node"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1313, "code": "def visit ( self , node ) : if node in self . visited : return None self . visited [ node ] = 1 methods = self . get callbacks ( node ) if methods [ 0 ] is not None : methods [ 0 ] ( node ) if hasattr ( node , \"locals\" ) : for local node in node . values ( ) : self . visit ( local node ) if methods [ 1 ] is not None : return methods [ 1 ] ( node ) return None", "predictions": ["remove all the methods entries for this node ."], "references": ["launch the visit starting from the given node"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 1314, "code": "def visit call ( self , node ) : try : for inferred in node . func . infer ( ) : if inferred is astroid . Uninferable : continue elif inferred . root ( ) . name == OPEN MODULE : if getattr ( node . func , \"name\" , None ) in OPEN FILES : self . check open mode ( node ) elif inferred . root ( ) . name == UNITTEST CASE : self . check redundant assert ( node , inferred ) elif isinstance ( inferred , astroid . Class Def ) : if inferred . qname ( ) == THREADING THREAD : self . check bad thread instantiation ( node ) elif inferred . qname ( ) == SUBPROCESS POPEN : self . check for preexec fn in popen ( node ) elif isinstance ( inferred , astroid . Function Def ) : name = inferred . qname ( ) if name == COPY COPY : self . check shallow copy environ ( node ) elif name in ENV GETTERS : self . check env function ( node , inferred ) elif name == SUBPROCESS RUN and PY35 : self . check for check kw in run ( node ) self . check deprecated method ( node , inferred ) except astroid . Inference Error : return", "predictions": ["this is a quiet method for the given : bool . raise a no ( if the class is not a subclass of the original : this method invokes the class ."], "references": ["visit a call node ."], "bleu": 0.04180647946097227, "rouge_l": 0.12448979591836734}
{"id": 1315, "code": "def check open mode ( self , node ) : try : mode arg = utils . get argument from call ( node , position = 1 , keyword = \"mode\" ) except utils . No Such Argument Error : return if mode arg : mode arg = utils . safe infer ( mode arg ) if isinstance ( mode arg , astroid . Const ) and not check mode str ( mode arg . value ) : self . add message ( \"bad-open-mode\" , node = node , args = mode arg . value )", "predictions": ["has the open effect of this if the if it is a astroid operation ."], "references": ["check that the mode argument of an open or file call is valid ."], "bleu": 0.1082597837309053, "rouge_l": 0.27758816837315126}
{"id": 1316, "code": "def handle message ( self , msg ) : self . messages . append ( { \"type\" : msg . category , \"module\" : msg . module , \"obj\" : msg . obj , \"line\" : msg . line , \"column\" : msg . column , \"path\" : msg . path , \"symbol\" : msg . symbol , \"message\" : html . escape ( msg . msg or \"\" , quote = False ) , \"message-id\" : msg . msg id , } )", "predictions": ["create a text ( use immediately ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["manage message of different type and in the context of path ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1317, "code": "def get title ( self , node ) : title = node . name if self . module names : title = \"%s.%s\" % ( node . root ( ) . name , title ) return title", "predictions": ["this is a convenience method to get the inside of this class ."], "references": ["get title for objects"], "bleu": 0.09552040806823771, "rouge_l": 0.1300639658848614}
{"id": 1318, "code": "def set default options ( self ) : self . module names = self . set option ( self . config . module names ) all ancestors = self . set option ( self . config . all ancestors ) all associated = self . set option ( self . config . all associated ) anc level , association level = ( 0 , 0 ) if all ancestors : anc level = - 1 if all associated : association level = - 1 if self . config . show ancestors is not None : anc level = self . config . show ancestors if self . config . show associated is not None : association level = self . config . show associated self . anc level , self . association level = anc level , association level", "predictions": ["is if if if if if if if if if no if specified , a classes will be set to the inside ."], "references": ["set different default options with _default dictionary"], "bleu": 0.05291907393644996, "rouge_l": 0.07376058041112453}
{"id": 1319, "code": "def show node ( self , node ) : if self . config . show builtin : return True return node . root ( ) . name != BUILTINS NAME", "predictions": ["normalize the requested all information ."], "references": ["true if builtins and not show_builtins"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1320, "code": "def add class ( self , node ) : self . linker . visit ( node ) self . classdiagram . add object ( self . get title ( node ) , node )", "predictions": ["this method adds a astroid to the sub astroid . ."], "references": ["visit one class and add it to diagram"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1321, "code": "def get ancestors ( self , node , level ) : if level == 0 : return for ancestor in node . ancestors ( recurs = False ) : if not self . show node ( ancestor ) : continue yield ancestor", "predictions": ["begins all edges of the given astroid ."], "references": ["return ancestor nodes of a class node"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1322, "code": "def get associated ( self , klass node , level ) : if level == 0 : return for association nodes in list ( klass node . instance attrs type . values ( ) ) + list ( klass node . locals type . values ( ) ) : for node in association nodes : if isinstance ( node , astroid . Instance ) : node = node . proxied if not ( isinstance ( node , astroid . Class Def ) and self . show node ( node ) ) : continue yield node", "predictions": ["this method returns all the and yield for this astroid ."], "references": ["return associated nodes of a class node"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1323, "code": "def extract classes ( self , klass node , anc level , association level ) : if self . classdiagram . has node ( klass node ) or not self . show node ( klass node ) : return self . add class ( klass node ) for ancestor in self . get ancestors ( klass node , anc level ) : self . extract classes ( ancestor , anc level - 1 , association level ) for node in self . get associated ( klass node , association level ) : self . extract classes ( node , anc level , association level - 1 )", "predictions": ["is the association rules as a series of association . this is used to is used to is no longer needed ."], "references": ["extract recursively classes related to klass_node"], "bleu": 0.05538696232597745, "rouge_l": 0.07963446475195823}
{"id": 1324, "code": "def visit importfrom ( self , node ) : if self . pkgdiagram : self . pkgdiagram . add from depend ( node , node . modname )", "predictions": ["this is a bit method used to assign the nodes of the tree ."], "references": ["visit astroid . importfrom and catch modules for package diagram"], "bleu": 0.08839374326825923, "rouge_l": 0.08591549295774649}
{"id": 1325, "code": "def has parent of type ( node , node type , statement ) : parent = node . parent while not isinstance ( parent , node type ) and statement . parent of ( parent ) : parent = parent . parent return isinstance ( parent , node type )", "predictions": ["does this node have a given * str ?"], "references": ["check if the given node has a parent of the given type ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 1326, "code": "def is name used as variadic ( name , variadics ) : return any ( variadic . value == name or variadic . value . parent of ( name ) for variadic in variadics )", "predictions": ["check if a given name is a ( of another node ."], "references": ["check if the given name is used as a variadic argument ."], "bleu": 0.24712442545253582, "rouge_l": 0.5833333333333334}
{"id": 1327, "code": "def register ( linter ) : linter . register checker ( Type Checker ( linter ) ) linter . register checker ( Iterable Checker ( linter ) )", "predictions": ["required method to auto decorated instances ."], "references": ["required method to auto register this checker"], "bleu": 0.5169731539571706, "rouge_l": 0.5714285714285714}
{"id": 1328, "code": "def visit unaryop ( self , node ) : for error in node . type errors ( ) : self . add message ( \"invalid-unary-operand-type\" , args = str ( error ) , node = node )", "predictions": ["adds a new classes to the tree ."], "references": ["detect typeerrors for unary operands ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1329, "code": "def interfaces ( node , herited = True , handler func = iface hdlr ) : try : implements = bases . Instance ( node ) . getattr ( \" implements \" ) [ 0 ] except exceptions . Not Found Error : return if not herited and implements . frame ( ) is not node : return found = set ( ) missing = False for iface in node classes . unpack infer ( implements ) : if iface is astroid . Uninferable : missing = True continue if iface not in found and handler func ( iface ) : found . add ( iface ) yield iface if missing : raise exceptions . Inference Error ( )", "predictions": ["imports the given fallback from the given fallback"], "references": ["return an iterator on interfaces implemented by the given class node ."], "bleu": 0.12801036176909558, "rouge_l": 0.1930379746835443}
{"id": 1330, "code": "def project from files ( files , func wrapper = astroid wrapper , project name = \"no name\" , black list = ( \"CVS\" , ) ) : astroid manager = manager . Astroid Manager ( ) project = Project ( project name ) for something in files : if not os . path . exists ( something ) : fpath = modutils . file from modpath ( something . split ( \".\" ) ) elif os . path . isdir ( something ) : fpath = os . path . join ( something , \" init .py\" ) else : fpath = something ast = func wrapper ( astroid manager . ast from file , fpath ) if ast is None : continue project . path = project . path or ast . file project . add module ( ast ) base name = ast . name if ast . package and something . find ( \" init \" ) == - 1 : for fpath in modutils . get module files ( os . path . dirname ( ast . file ) , black list ) : ast = func wrapper ( astroid manager . ast from file , fpath ) if ast is None or ast . name == base name : continue project . add module ( ast ) return project", "predictions": ["extracts the generated is running in the is identified by the application ' s name ."], "references": ["return a project from a list of files or modules"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1331, "code": "def compute module ( self , context name , mod path ) : package dir = os . path . dirname ( self . project . path ) if context name == mod path : return 0 if modutils . is standard module ( mod path , ( package dir , ) ) : return 1 return 0", "predictions": ["computes the full postponed postponed message for a ."], "references": ["return true if the module should be added to dependencies"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1332, "code": "def imported module ( self , node , mod path , relative ) : module = node . root ( ) context name = module . name if relative : mod path = \"%s.%s\" % ( \".\" . join ( context name . split ( \".\" ) [ : - 1 ] ) , mod path ) if self . compute module ( context name , mod path ) : if not hasattr ( module , \"depends\" ) : module . depends = [ ] mod paths = module . depends if mod path not in mod paths : mod paths . append ( mod path )", "predictions": ["create a repr for a repr ."], "references": ["notify an imported module used to analyze dependencies"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 1333, "code": "def register ( linter ) : linter . register reporter ( Text Reporter ) linter . register reporter ( Parseable Text Reporter ) linter . register reporter ( VS Text Reporter ) linter . register reporter ( Colorized Text Reporter )", "predictions": ["visit the ) ) text ."], "references": ["register the reporter classes with the linter ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 1334, "code": "def handle message ( self , msg ) : if msg . module not in self . modules : if msg . module : self . writeln ( \"************* Module %s\" % msg . module ) self . modules . add ( msg . module ) else : self . writeln ( \"************* \" ) self . write message ( msg )", "predictions": ["visit a sender with the calculated class ."], "references": ["manage message of different type and in the context of path"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 1335, "code": "def open graph ( self , * * args ) : self . stream . write ( \"%sgraph:{\\n\" % self . indent ) self . inc indent ( ) self . write attributes ( GRAPH ATTRS , * * args )", "predictions": ["record the log file ."], "references": ["open a vcg graph"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1336, "code": "def edge ( self , from node , to node , edge type = \"\" , * * args ) : self . stream . write ( '%s%sedge: {sourcename:\"%s\" targetname:\"%s\"' % ( self . indent , edge type , from node , to node ) ) self . write attributes ( EDGE ATTRS , * * args ) self . stream . write ( \"}\\n\" )", "predictions": ["writes the object to the output stream ."], "references": ["draw an edge from a node to another ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1337, "code": "def write attributes ( self , attributes dict , * * args ) : for key , value in args . items ( ) : try : type = attributes dict [ key ] except Key Error : raise Exception ( % ( key , attributes dict . keys ( ) ) ) if not type : self . stream . write ( '%s%s:\"%s\"\\n' % ( self . indent , key , value ) ) elif type == 1 : self . stream . write ( \"%s%s:%s\\n\" % ( self . indent , key , int ( value ) ) ) elif value in type : self . stream . write ( \"%s%s:%s\\n\" % ( self . indent , key , value ) ) else : raise Exception ( % ( value , key , type ) )", "predictions": ["check handler . writes key - value pairs to the output ."], "references": ["write graph node or edge attributes"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 1338, "code": "def register ( linter ) : linter . register checker ( String Format Checker ( linter ) ) linter . register checker ( String Constant Checker ( linter ) )", "predictions": ["required method to auto check the node values ."], "references": ["required method to auto register this checker"], "bleu": 0.392814650900513, "rouge_l": 0.5115303983228512}
{"id": 1339, "code": "def check new format ( self , node , func ) : # # if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : return if node . starargs or node . kwargs : return try : strnode = next ( func . bound . infer ( ) ) except astroid . Inference Error : return if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : return try : call site = Call Site . from call ( node ) except astroid . Inference Error : return try : fields , num args , manual pos = utils . parse format method string ( strnode . value ) except utils . Incomplete Format String : self . add message ( \"bad-format-string\" , node = node ) return positional arguments = call site . positional arguments named arguments = call site . keyword arguments named fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } if num args and manual pos : self . add message ( \"format-combined-specification\" , node = node ) return check args = False num args += sum ( 1 for field in named fields if field == \"\" ) if named fields : for field in named fields : if field and field not in named arguments : self . add message ( \"missing-format-argument-key\" , node = node , args = ( field , ) ) for field in named arguments : if field not in named fields : self . add message ( \"unused-format-string-argument\" , node = node , args = ( field , ) ) num args = num args or manual pos if positional arguments or num args : empty = any ( True for field in named fields if field == \"\" ) if named arguments or empty : check args = True else : check args = True if check args : num args = num args or manual pos if len ( positional arguments ) > num args : self . add message ( \"too-many-format-args\" , node = node ) elif len ( positional arguments ) < num args : self . add message ( \"too-few-format-args\" , node = node ) self . detect vacuous formatting ( node , positional arguments ) self . check new format specifiers ( node , fields , named arguments )", "predictions": ["method used to help do the necessary ( for the given method ."], "references": ["check the new string formatting ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 1340, "code": "def visit section ( self , layout ) : self . section += 1 self . writeln ( ) self . format children ( layout ) self . section -= 1 self . writeln ( )", "predictions": ["attach to this object ."], "references": ["display a section as text"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1341, "code": "def visit evaluationsection ( self , layout ) : self . section += 1 self . format children ( layout ) self . section -= 1 self . writeln ( )", "predictions": ["inserts the ( and the home box ."], "references": ["display an evaluation section as a text ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1342, "code": "def visit table ( self , layout ) : table content = self . get table content ( layout ) cols width = [ 0 ] * len ( table content [ 0 ] ) for row in table content : for index , col in enumerate ( row ) : cols width [ index ] = max ( cols width [ index ] , len ( col ) ) self . default table ( layout , table content , cols width ) self . writeln ( )", "predictions": ["initializes the part of this rectangle ."], "references": ["display a table as text"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1343, "code": "def check symbol ( self , msgid , symbol ) : other message = self . messages definitions . get ( symbol ) if other message : self . raise duplicate msg id ( symbol , msgid , other message . msgid ) else : alternative msgid = None alternative message = self . alternative names . get ( symbol ) if alternative message : if alternative message . symbol == symbol : alternative msgid = alternative message . msgid else : for old msgid , old symbol in alternative message . old names : if old symbol == symbol : alternative msgid = old msgid break if msgid != alternative msgid : self . raise duplicate msg id ( symbol , msgid , alternative msgid )", "predictions": ["check that this attr can be assigned to another attr ."], "references": ["check that a symbol is not already used ."], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 1344, "code": "def help message ( self , msgids ) : for msgid in msgids : try : for message definition in self . get message definitions ( msgid ) : print ( message definition . format help ( checkerref = True ) ) print ( \"\" ) except Unknown Message Error as ex : print ( ex ) print ( \"\" ) continue", "predictions": ["prints a help message ."], "references": ["display help messages for the given message identifiers"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1345, "code": "def list messages ( self ) : messages = sorted ( self . messages definitions . values ( ) , key = lambda m : m . msgid ) for message in messages : if not message . may be emitted ( ) : continue print ( message . format help ( checkerref = False ) ) print ( \"\" )", "predictions": ["list all the messages in a container ."], "references": ["output full messages list documentation in rest format ."], "bleu": 0.17795502018438056, "rouge_l": 0.34923664122137404}
{"id": 1346, "code": "def builder inited ( app ) : base path = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( file ) ) ) ) ext path = os . path . join ( base path , \"pylint\" , \"extensions\" ) modules = [ ] doc files = { } for filename in os . listdir ( ext path ) : name , ext = os . path . splitext ( filename ) if name [ 0 ] == \" \" or name in DEPRECATED MODULES : continue if ext == \".py\" : modules . append ( \"pylint.extensions.%s\" % name ) elif ext == \".rst\" : doc files [ \"pylint.extensions.\" + name ] = os . path . join ( ext path , filename ) modules . sort ( ) if not modules : sys . exit ( \"No Pylint extensions found?\" ) linter = Py Linter ( ) linter . load plugin modules ( modules ) extensions doc = os . path . join ( base path , \"doc\" , \"technical reference\" , \"extensions.rst\" ) with open ( extensions doc , \"w\" ) as stream : stream . write ( \"Optional Pylint checkers in the extensions module\\n\" ) stream . write ( \"=================================================\\n\\n\" ) stream . write ( \"Pylint provides the following optional plugins:\\n\\n\" ) for module in modules : stream . write ( \"- :ref:`{}`\\n\" . format ( module ) ) stream . write ( \"\\n\" ) stream . write ( \"You can activate any or all of these extensions \" \"by adding a ``load-plugins`` line to the ``MASTER`` \" \"section of your ``.pylintrc``, for example::\\n\" ) stream . write ( \"\\n    load-plugins=pylint.extensions.docparams,\" \"pylint.extensions.docstyle\\n\\n\" ) by module = get plugins info ( linter , doc files ) for module , info in sorted ( by module . items ( ) ) : linter . print checker doc ( info [ \"name\" ] , info , stream = stream )", "predictions": ["generate all the plugins from an xml file"], "references": ["output full documentation in rest format for all extension modules"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 1347, "code": "def cpu count ( ) -> int : sched getaffinity = getattr ( os , \"sched getaffinity\" , None ) if sched getaffinity : return len ( sched getaffinity ( 0 ) ) if multiprocessing : return multiprocessing . cpu count ( ) return 1", "predictions": ["returns the number of cpus in this ( ."], "references": ["use sched_affinity if available for virtualized or containerized environments ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1348, "code": "def report messages stats ( sect , stats , ) : if not stats [ \"by msg\" ] : raise exceptions . Empty Report Error ( ) in order = sorted ( [ ( value , msg id ) for msg id , value in stats [ \"by msg\" ] . items ( ) if not msg id . startswith ( \"I\" ) ] ) in order . reverse ( ) lines = ( \"message id\" , \"occurrences\" ) for value , msg id in in order : lines += ( msg id , str ( value ) ) sect . append ( report nodes . Table ( children = lines , cols = 2 , rheaders = 1 ) )", "predictions": ["report messages or as a stack trace ."], "references": ["make messages type report"], "bleu": 0.17747405280050269, "rouge_l": 0.17732558139534885}
{"id": 1349, "code": "def python3 porting mode ( self ) : self . disable ( \"all\" ) self . enable ( \"python3\" ) if self . error mode : for msg id in self . checker messages ( \"python3\" ) : if msg id . startswith ( \"E\" ) : self . enable ( msg id ) else : self . disable ( msg id ) config parser = self . cfgfile parser if config parser . has option ( \"MESSAGES CONTROL\" , \"disable\" ) : value = config parser . get ( \"MESSAGES CONTROL\" , \"disable\" ) self . global set option ( \"disable\" , value ) self . python3 porting mode = True", "predictions": ["creates a python3 for all ( ."], "references": ["disable all other checkers and enable python 3 warnings ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1350, "code": "def get checkers ( self ) : return [ self ] + [ c for checkers in self . checkers . values ( ) for c in checkers if c is not self ]", "predictions": ["description of the method"], "references": ["return all available checkers as a list"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1351, "code": "def get checker names ( self ) : current checkers = self . get checkers ( ) return sorted ( { check . name for check in current checkers if check . name != \"master\" } )", "predictions": ["this is used to get the checker for this instance ."], "references": ["get all the checker names that this linter knows about ."], "bleu": 0.17827531042796255, "rouge_l": 0.45454545454545453}
{"id": 1352, "code": "def prepare checkers ( self ) : if not self . config . reports : self . disable reporters ( ) neededcheckers = [ self ] for checker in self . get checkers ( ) [ 1 : ] : messages = { msg for msg in checker . msgs if self . is message enabled ( msg ) } if messages or any ( self . report is enabled ( r [ 0 ] ) for r in checker . reports ) : neededcheckers . append ( checker ) neededcheckers = sorted ( neededcheckers , key = operator . attrgetter ( \"priority\" ) , reverse = True ) return neededcheckers", "predictions": ["examines and prepares neededcheckers . this is useful for running messages that are available ."], "references": ["return checkers needed for activated messages and reports"], "bleu": 0.09782375748961449, "rouge_l": 0.18401206636500753}
{"id": 1353, "code": "def expand files ( self , modules ) : result , errors = utils . expand modules ( modules , self . config . black list , self . config . black list re ) for error in errors : message = modname = error [ \"mod\" ] key = error [ \"key\" ] self . set current module ( modname ) if key == \"fatal\" : message = str ( error [ \"ex\" ] ) . replace ( os . getcwd ( ) + os . sep , \"\" ) self . add message ( key , args = message ) return result", "predictions": ["expands modules with their top level modules ."], "references": ["get modules and errors from a list of modules and handle errors"], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 1354, "code": "def check astroid module ( self , ast node , walker , rawcheckers , tokencheckers ) : try : tokens = utils . tokenize module ( ast node ) except tokenize . Token Error as ex : self . add message ( \"syntax-error\" , line = ex . args [ 1 ] [ 0 ] , args = ex . args [ 0 ] ) return None if not ast node . pure python : self . add message ( \"raw-checker-failed\" , args = ast node . name ) else : self . process tokens ( tokens ) if self . ignore file : return False self . file state . collect block lines ( self . msgs store , ast node ) for checker in rawcheckers : checker . process module ( ast node ) for checker in tokencheckers : checker . process tokens ( tokens ) walker . walk ( ast node ) return True", "predictions": ["use the astroid for a module ."], "references": ["check a module from its astroid representation ."], "bleu": 0.240785655451027, "rouge_l": 0.3952483801295896}
{"id": 1355, "code": "def report evaluation ( self ) : previous stats = config . load results ( self . file state . base name ) if self . stats [ \"statement\" ] == 0 : return evaluation = self . config . evaluation try : note = eval ( evaluation , { } , self . stats ) except Exception as ex : msg = \"An exception occurred while rating: %s\" % ex else : self . stats [ \"global note\" ] = note msg = \"Your code has been rated at %.2f/10\" % note pnote = previous stats . get ( \"global note\" ) if pnote is not None : msg += \" (previous run: %.2f/10, %+.2f)\" % ( pnote , note - pnote ) if self . config . score : sect = report nodes . Evaluation Section ( msg ) self . reporter . display reports ( sect )", "predictions": ["report nodes with the full report ."], "references": ["make the global evaluation report"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 1356, "code": "def cb generate config ( self , * args , * * kwargs ) : self . linter . generate config ( skipsections = ( \"COMMANDS\" , ) ) sys . exit ( 0 )", "predictions": ["a method for performing a register at the end of the map ."], "references": ["optik callback for sample config file generation"], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 1357, "code": "def cb generate manpage ( self , * args , * * kwargs ) : from pylint import pkginfo self . linter . generate manpage ( pkginfo ) sys . exit ( 0 )", "predictions": ["a method for the pylint operation ."], "references": ["optik callback for sample config file generation"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1358, "code": "def cb help message ( self , option , optname , value , parser ) : self . linter . msgs store . help message ( utils . splitstrip ( value ) ) sys . exit ( 0 )", "predictions": ["adds a help message to the service ."], "references": ["optik callback for printing some help about a particular message"], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 1359, "code": "def cb full documentation ( self , option , optname , value , parser ) : self . linter . print full documentation ( ) sys . exit ( 0 )", "predictions": ["this is called once per second ."], "references": ["optik callback for printing full documentation"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1360, "code": "def cb list messages ( self , option , optname , value , parser ) : self . linter . msgs store . list messages ( ) sys . exit ( 0 )", "predictions": ["exit a list of files ."], "references": ["optik callback for printing available messages"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1361, "code": "def normalize text ( text , line len = 80 , indent = \"\" ) : return \"\\n\" . join ( textwrap . wrap ( text , width = line len , initial indent = indent , subsequent indent = indent ) )", "predictions": ["heading all lines that make up to the maximum line size ."], "references": ["wrap the text on the given line length ."], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 1362, "code": "def get module and frameid ( node ) : frame = node . frame ( ) module , obj = \"\" , [ ] while frame : if isinstance ( frame , Module ) : module = frame . name else : obj . append ( getattr ( frame , \"name\" , \"<lambda>\" ) ) try : frame = frame . parent . frame ( ) except Attribute Error : frame = None obj . reverse ( ) return module , \".\" . join ( obj )", "predictions": ["gets the full frame and its children of this map ."], "references": ["return the module name and the frame id in the module"], "bleu": 0.1354599427337814, "rouge_l": 0.18181818181818182}
{"id": 1363, "code": "def safe decode ( line , encoding , * args , * * kwargs ) : try : return line . decode ( encoding or sys . getdefaultencoding ( ) , * args , * * kwargs ) except Lookup Error : return line . decode ( sys . getdefaultencoding ( ) , * args , * * kwargs )", "predictions": ["decodes the given command line arguments ."], "references": ["return decoded line from encoding or decode with default encoding"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 1364, "code": "def comment ( string ) : lines = [ line . strip ( ) for line in string . splitlines ( ) ] return + ( % linesep ) . join ( lines )", "predictions": ["get lines from the output ."], "references": ["return string as a comment"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1365, "code": "def format option value ( optdict , value ) : if isinstance ( value , ( list , tuple ) ) : value = \",\" . join ( format option value ( optdict , item ) for item in value ) elif isinstance ( value , dict ) : value = \",\" . join ( \"%s:%s\" % ( k , v ) for k , v in value . items ( ) ) elif hasattr ( value , \"match\" ) : value = value . pattern elif optdict . get ( \"type\" ) == \"yn\" : value = \"yes\" if value else \"no\" elif isinstance ( value , str ) and value . isspace ( ) : value = \"'%s'\" % value return value", "predictions": ["formats the value of any other elimination to pass it to a user ."], "references": ["return the user input s value from a compiled value"], "bleu": 0.11114924776032006, "rouge_l": 0.2577464788732394}
{"id": 1366, "code": "def format section ( stream , section , options , doc = None ) : if doc : print ( comment ( doc ) , file = stream ) print ( \"[%s]\" % section , file = stream ) ini format ( stream , options )", "predictions": ["constructs the ( style ."], "references": ["format an options section using the ini format"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1367, "code": "def ini format ( stream , options ) : for optname , optdict , value in options : value = format option value ( optdict , value ) help opt = optdict . get ( \"help\" ) if help opt : help opt = normalize text ( help opt , line len = 79 , indent = ) print ( file = stream ) print ( help opt , file = stream ) else : print ( file = stream ) if value is None : print ( \"#%s=\" % optname , file = stream ) else : value = str ( value ) . strip ( ) if re . match ( r\"^([\\w-]+,)+[\\w-]+$\" , str ( value ) ) : separator = \"\\n \" + \" \" * len ( optname ) value = separator . join ( x + \",\" for x in str ( value ) . split ( \",\" ) ) value = value [ : - 1 ] print ( \"%s=%s\" % ( optname , value ) , file = stream )", "predictions": ["format the ini ."], "references": ["format options using the ini format"], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 1368, "code": "def insert ( self , index , child ) : self . children . insert ( index , child ) child . parent = self", "predictions": ["inserts the specified resource at the specified index ."], "references": ["insert a child node"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1369, "code": "def append ( self , child ) : assert child not in self . parents ( ) V Node . append ( self , child )", "predictions": ["appends the full child node for this node to the end of the list ."], "references": ["overridden to detect problems easily"], "bleu": 0.08225964699966554, "rouge_l": 0.10990990990990988}
{"id": 1370, "code": "def parents ( self ) : assert self . parent is not self if self . parent is None : return [ ] return [ self . parent ] + self . parent . parents ( )", "predictions": ["get the parents of this class ."], "references": ["return the ancestor nodes"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1371, "code": "def collect block lines ( self , msgs store , module node ) : for msg , lines in self . module msgs state . items ( ) : self . raw module msgs state [ msg ] = lines . copy ( ) orig state = self . module msgs state . copy ( ) self . module msgs state = { } self . suppression mapping = { } self . effective max line number = module node . tolineno self . collect block lines ( msgs store , module node , orig state )", "predictions": ["collects information about all products of this block ."], "references": ["walk the ast to collect block level options line numbers ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 1372, "code": "def enable report ( self , reportid ) : reportid = reportid . upper ( ) self . reports state [ reportid ] = True", "predictions": ["this method is called to enable the report to the client ."], "references": ["disable the report of the given id"], "bleu": 0.14694106251955755, "rouge_l": 0.3315217391304348}
{"id": 1373, "code": "def disable report ( self , reportid ) : reportid = reportid . upper ( ) self . reports state [ reportid ] = False", "predictions": ["this method is called when the report gets executed ."], "references": ["disable the report of the given id"], "bleu": 0.16590387014219712, "rouge_l": 0.24302788844621517}
{"id": 1374, "code": "def register ( linter ) : linter . register checker ( Encoding Checker ( linter ) ) linter . register checker ( By Id Managed Messages Checker ( linter ) )", "predictions": ["required method to auto register this checker ."], "references": ["required method to auto register this checker"], "bleu": 0.8633400213704505, "rouge_l": 0.9446902654867256}
{"id": 1375, "code": "def process module ( self , module ) : managed msgs = Messages Handler Mix In . get by id managed msgs ( ) for ( mod name , msg id , msg symbol , lineno , is disabled ) in managed msgs : if mod name == module . name : if is disabled : txt = \"Id '{ident}' is used to disable '{symbol}' message emission\" . format ( ident = msg id , symbol = msg symbol ) else : txt = \"Id '{ident}' is used to enable '{symbol}' message emission\" . format ( ident = msg id , symbol = msg symbol ) self . add message ( \"use-symbolic-message-instead\" , line = lineno , args = txt ) Messages Handler Mix In . clear by id managed msgs ( )", "predictions": ["process the module ."], "references": ["inspect the source file to find messages activated or deactivated by id ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 1376, "code": "def process module ( self , module ) : if module . file encoding : encoding = module . file encoding else : encoding = \"ascii\" with module . stream ( ) as stream : for lineno , line in enumerate ( stream ) : self . check encoding ( lineno + 1 , line , encoding )", "predictions": ["adds a message from the message ."], "references": ["inspect the source file to find encoding problem"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1377, "code": "def process tokens ( self , tokens ) : if not self . config . notes : return comments = ( token info for token info in tokens if token info . type == tokenize . COMMENT ) for comment in comments : comment text = comment . string [ 1 : ] . lstrip ( ) disable option match = OPTION RGX . search ( comment text ) if disable option match : try : , value = disable option match . group ( 1 ) . split ( \"=\" , 1 ) values = [ val . strip ( ) . upper ( ) for val in value . split ( \",\" ) ] if set ( values ) & set ( self . config . notes ) : continue except Value Error : self . add message ( \"bad-inline-option\" , args = disable option match . group ( 1 ) . strip ( ) , line = comment . string , ) continue match = self . fixme pattern . search ( \"#\" + comment text . lower ( ) ) if match : note = match . group ( 1 ) self . add message ( \"fixme\" , col offset = comment . string . lower ( ) . index ( note . lower ( ) ) , args = comment text , line = comment . start [ 0 ] , )", "predictions": ["read all messages in the messages contained in this text ."], "references": ["inspect the source to find fixme problems"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 1378, "code": "def is from future import ( stmt , name ) : try : module = stmt . do import module ( stmt . modname ) except astroid . Astroid Building Exception : return None for local node in module . locals . get ( name , [ ] ) : if isinstance ( local node , astroid . Import From ) and local node . modname == FUTURE : return True return None", "predictions": ["check if the given statement is a member of a certain dirname ."], "references": ["check if the name is a future import from another module ."], "bleu": 0.21972813874997157, "rouge_l": 0.4834874504623514}
{"id": 1379, "code": "def in for else branch ( parent , stmt ) : return isinstance ( parent , astroid . For ) and any ( else stmt . parent of ( stmt ) or else stmt == stmt for else stmt in parent . orelse )", "predictions": ["returns true if the given int cpu cpu has a parent node ."], "references": ["returns true if stmt in inside the else branch for a parent for stmt ."], "bleu": 0.1947911080410563, "rouge_l": 0.49364161849710986}
{"id": 1380, "code": "def overridden method ( klass , name ) : try : parent = next ( klass . local attr ancestors ( name ) ) except ( Stop Iteration , Key Error ) : return None try : meth node = parent [ name ] except Key Error : return None if isinstance ( meth node , astroid . Function Def ) : return meth node return None", "predictions": ["find the report corresponding to the given name ."], "references": ["get overridden method if any"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1381, "code": "def assigned locally ( name node ) : assign stmts = name node . scope ( ) . nodes of class ( astroid . Assign Name ) return any ( a . name == name node . name for a in assign stmts )", "predictions": [". . this is a temporary function that we need to . if the python3 name is set to true , we will ."], "references": ["checks if name_node has corresponding assign statement in same scope"], "bleu": 0.050661968099322066, "rouge_l": 0.06354166666666666}
{"id": 1382, "code": "def visit global ( self , node ) : frame = node . frame ( ) if isinstance ( frame , astroid . Module ) : self . add message ( \"global-at-module-level\" , node = node ) return module = frame . root ( ) default message = True locals = node . scope ( ) . locals for name in node . names : try : assign nodes = module . getattr ( name ) except astroid . Not Found Error : assign nodes = [ ] not defined locally by import = not any ( isinstance ( local , astroid . node classes . Import ) for local in locals . get ( name , ( ) ) ) if not assign nodes and not defined locally by import : self . add message ( \"global-variable-not-assigned\" , args = name , node = node ) default message = False continue for anode in assign nodes : if ( isinstance ( anode , astroid . Assign Name ) and anode . name in module . special attributes ) : self . add message ( \"redefined-builtin\" , args = name , node = node ) break if anode . frame ( ) is module : break else : if not defined locally by import : self . add message ( \"global-variable-undefined\" , args = name , node = node ) default message = False if default message : self . add message ( \"global-statement\" , node = node )", "predictions": ["this is called by the implementation of the superclass when the module is complete ."], "references": ["check names imported exists in the global scope"], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 1383, "code": "def visit import ( self , node ) : if not self . analyse fallback blocks and utils . is from fallback block ( node ) : return for name , in node . names : parts = name . split ( \".\" ) try : module = next ( infer name module ( node , parts [ 0 ] ) ) except astroid . Resolve Error : continue self . check module attrs ( node , module , parts [ 1 : ] )", "predictions": ["get the name of the given ) . if no checker is found , the method will be overloaded ."], "references": ["check modules attribute accesses"], "bleu": 0.051366639095059514, "rouge_l": 0.0}
{"id": 1384, "code": "def visit importfrom ( self , node ) : if not self . analyse fallback blocks and utils . is from fallback block ( node ) : return name parts = node . modname . split ( \".\" ) try : module = node . do import module ( name parts [ 0 ] ) except astroid . Astroid Building Exception : return module = self . check module attrs ( node , module , name parts [ 1 : ] ) if not module : return for name , in node . names : if name == \"*\" : continue self . check module attrs ( node , module , name . split ( \".\" ) )", "predictions": ["this is used to fill the message with the call graph . if the : this is a method that has been called with the same name as the call to } ."], "references": ["check modules attribute accesses"], "bleu": 0.030787460505623344, "rouge_l": 0.0}
{"id": 1385, "code": "def check metaclasses ( self , node ) : consumed = [ ] for child node in node . get children ( ) : if isinstance ( child node , astroid . Class Def ) : consumed . extend ( self . check classdef metaclasses ( child node , node ) ) for scope locals , name in consumed : scope locals . pop ( name , None )", "predictions": ["checks the files for this modules ."], "references": ["update consumption analysis for metaclasses ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1386, "code": "def get packages ( directory , prefix ) : result = [ ] for package in os . listdir ( directory ) : absfile = join ( directory , package ) if isdir ( absfile ) : if exists ( join ( absfile , \" init .py\" ) ) : if prefix : result . append ( \"%s.%s\" % ( prefix , package ) ) else : result . append ( package ) result += get packages ( absfile , result [ - 1 ] ) return result", "predictions": ["returns all files in the specified ( excluding , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , sorted ast ast ast"], "references": ["return a list of subpackages for the given directory"], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 1387, "code": "def run ( self ) : install lib . install lib . run ( self ) if include dirs : for directory in include dirs : dest = join ( self . install dir , directory ) if sys . version info >= ( 3 , 0 ) : exclude = { \"invalid encoded data*\" , \"unknown encoding*\" } else : exclude = set ( ) shutil . rmtree ( dest , ignore errors = True ) shutil . copytree ( directory , dest , ignore = shutil . ignore patterns ( * exclude ) )", "predictions": ["does the full set of ( components ."], "references": ["overridden from install_lib class"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1388, "code": "def report similarities ( sect , stats , old stats ) : lines = [ \"\" , \"now\" , \"previous\" , \"difference\" ] lines += table lines from stats ( stats , old stats , ( \"nb duplicated lines\" , \"percent duplicated lines\" ) ) sect . append ( Table ( children = lines , cols = 4 , rheaders = 1 , cheaders = 1 ) )", "predictions": ["reports that the given detail is properly formatted ."], "references": ["make a layout with some stats about duplication"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1389, "code": "def Run ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] from getopt import getopt s opts = \"hdi\" l opts = ( \"help\" , \"duplicates=\" , \"ignore-comments\" , \"ignore-imports\" , \"ignore-docstrings\" , ) min lines = 4 ignore comments = False ignore docstrings = False ignore imports = False opts , args = getopt ( argv , s opts , l opts ) for opt , val in opts : if opt in ( \"-d\" , \"--duplicates\" ) : min lines = int ( val ) elif opt in ( \"-h\" , \"--help\" ) : usage ( ) elif opt in ( \"-i\" , \"--ignore-comments\" ) : ignore comments = True elif opt in ( \"--ignore-docstrings\" , ) : ignore docstrings = True elif opt in ( \"--ignore-imports\" , ) : ignore imports = True if not args : usage ( 1 ) sim = Similar ( min lines , ignore comments , ignore docstrings , ignore imports ) for filename in args : with open ( filename ) as stream : sim . append stream ( filename , stream ) sim . run ( ) sys . exit ( 0 )", "predictions": ["implements the netty analysis ."], "references": ["standalone command line access point"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1390, "code": "def append stream ( self , streamid , stream , encoding = None ) : if encoding is None : readlines = stream . readlines else : readlines = decoding stream ( stream , encoding ) . readlines try : self . linesets . append ( Line Set ( streamid , readlines ( ) , self . ignore comments , self . ignore docstrings , self . ignore imports , ) ) except Unicode Decode Error : pass", "predictions": ["append helper method for encoding and ignore"], "references": ["append a file to search for similarities"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1391, "code": "def compute sims ( self ) : no duplicates = defaultdict ( list ) for num , lineset1 , idx1 , lineset2 , idx2 in self . iter sims ( ) : duplicate = no duplicates [ num ] for couples in duplicate : if ( lineset1 , idx1 ) in couples or ( lineset2 , idx2 ) in couples : couples . add ( ( lineset1 , idx1 ) ) couples . add ( ( lineset2 , idx2 ) ) break else : duplicate . append ( { ( lineset1 , idx1 ) , ( lineset2 , idx2 ) } ) sims = [ ] for num , ensembles in no duplicates . items ( ) : for couples in ensembles : sims . append ( ( num , couples ) ) sims . sort ( ) sims . reverse ( ) return sims", "predictions": ["compute ( ( ( ( ( ( ( ( ( ( ( ( ( . . ( ( . . . ( ( . ( . . ( ( ( . . . . ( . ( . . . . ( . . . . . . . ("], "references": ["compute similarities in appended files"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 1392, "code": "def display sims ( self , sims ) : nb lignes dupliquees = 0 for num , couples in sims : print ( ) print ( num , \"similar lines in\" , len ( couples ) , \"files\" ) couples = sorted ( couples ) for lineset , idx in couples : print ( \"==%s:%s\" % ( lineset . name , idx ) ) for line in lineset . real lines [ idx : idx + num ] : print ( \"  \" , line . rstrip ( ) ) nb lignes dupliquees += num * ( len ( couples ) - 1 ) nb total lignes = sum ( [ len ( lineset ) for lineset in self . linesets ] ) print ( \"TOTAL lines=%s duplicates=%s percent=%.2f\" % ( nb total lignes , nb lignes dupliquees , nb lignes dupliquees * 100.0 / nb total lignes , ) )", "predictions": ["displays a uniform distribution of artifacts ."], "references": ["display computed similarities on stdout"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1393, "code": "def find common ( self , lineset1 , lineset2 ) : lines1 = lineset1 . enumerate stripped lines2 = lineset2 . enumerate stripped find = lineset2 . find index1 = 0 min lines = self . min lines while index1 < len ( lineset1 ) : skip = 1 num = 0 for index2 in find ( lineset1 [ index1 ] ) : non blank = 0 for num , ( ( , line1 ) , ( , line2 ) ) in enumerate ( zip ( lines1 ( index1 ) , lines2 ( index2 ) ) ) : if line1 != line2 : if non blank > min lines : yield num , lineset1 , index1 , lineset2 , index2 skip = max ( skip , num ) break if line1 : non blank += 1 else : num += 1 if non blank > min lines : yield num , lineset1 , index1 , lineset2 , index2 skip = max ( skip , num ) index1 += skip", "predictions": ["normalize the text . this is only needed for testing purposes ."], "references": ["find similarities in the two given linesets"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 1394, "code": "def mk index ( self ) : index = defaultdict ( list ) for line no , line in enumerate ( self . stripped lines ) : if line : index [ line ] . append ( line no ) return index", "predictions": ["stage this . this is a helper function that returns a module and returns a module object ."], "references": ["create the index for this set"], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 1395, "code": "def definition equivalent to call ( definition , call ) : if definition . kwargs : same kw variadics = definition . kwargs in call . starred kws else : same kw variadics = not call . starred kws if definition . varargs : same args variadics = definition . varargs in call . starred args else : same args variadics = not call . starred args same kwonlyargs = all ( kw in call . kws for kw in definition . kwonlyargs ) same args = definition . args == call . args no additional kwarg arguments = True if call . kws : for keyword in call . kws : is arg = keyword in call . args is kwonly = keyword in definition . kwonlyargs if not is arg and not is kwonly : no additional kwarg arguments = False break return all ( ( same args , same kwonlyargs , same args variadics , same kw variadics , no additional kwarg arguments , ) )", "predictions": ["execute a line decode with the given arguments ."], "references": ["check if a definition signature is equivalent to a call ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 1396, "code": "def register ( linter ) : linter . register checker ( Class Checker ( linter ) ) linter . register checker ( Special Methods Checker ( linter ) )", "predictions": ["this method registers this = new = ."], "references": ["required method to auto register this checker"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1397, "code": "def set accessed ( self , node ) : frame = node frame class ( node ) if frame is None : return self . scopes [ frame ] [ node . attrname ] . append ( node )", "predictions": ["sets this if you have a if we should use a if you get the if we get a if we should use a if you get a if you created ."], "references": ["set the given node as accessed ."], "bleu": 0.04180647946097227, "rouge_l": 0.11596958174904944}
{"id": 1398, "code": "def visit classdef ( self , node ) : self . check bases classes ( node ) if node . type == \"class\" and has known bases ( node ) : try : node . local attr ( \" init \" ) except astroid . Not Found Error : self . add message ( \"no-init\" , args = node , node = node ) self . check slots ( node ) self . check proper bases ( node ) self . check consistent mro ( node )", "predictions": ["this is a bit operation for . ."], "references": ["init visit variable _accessed"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1399, "code": "def check consistent mro ( self , node ) : try : node . mro ( ) except Inconsistent Mro Error : self . add message ( \"inconsistent-mro\" , args = node . name , node = node ) except Duplicate Bases Error : self . add message ( \"duplicate-bases\" , args = node . name , node = node ) except Not Implemented Error : pass", "predictions": ["adds a format ) to the ( ( i . e . , the ) method is invoked on the ) ."], "references": ["detect that a class has a consistent mro or duplicate bases ."], "bleu": 0.0612957497932821, "rouge_l": 0.12423625254582485}
{"id": 1400, "code": "def visit functiondef ( self , node ) : if not node . is method ( ) : return self . check useless super delegation ( node ) klass = node . parent . frame ( ) self . meth could be func = True self . check first arg for type ( node , klass . type == \"metaclass\" ) if node . name == \" init \" : self . check init ( node ) return for overridden in klass . local attr ancestors ( node . name ) : try : meth node = overridden [ node . name ] except Key Error : continue if not isinstance ( meth node , astroid . Function Def ) : continue self . check signature ( node , meth node , \"overridden\" , klass ) break if node . decorators : for decorator in node . decorators . nodes : if isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( \"getter\" , \"setter\" , \"deleter\" , ) : return if isinstance ( decorator , astroid . Name ) : if decorator . name == \"property\" : return inferred = safe infer ( decorator ) if not inferred : return if isinstance ( inferred , astroid . Function Def ) : try : inferred = next ( inferred . infer call result ( inferred ) ) except astroid . Inference Error : return try : if ( isinstance ( inferred , ( astroid . Instance , astroid . Class Def ) ) and inferred . getattr ( \" get \" ) and inferred . getattr ( \" set \" ) ) : return except astroid . Attribute Inference Error : pass try : overridden = klass . instance attr ( node . name ) [ 0 ] overridden frame = overridden . frame ( ) if ( isinstance ( overridden frame , astroid . Function Def ) and overridden frame . type == \"method\" ) : overridden frame = overridden frame . parent . frame ( ) if isinstance ( overridden frame , astroid . Class Def ) and klass . is subtype of ( overridden frame . qname ( ) ) : args = ( overridden . root ( ) . name , overridden . fromlineno ) self . add message ( \"method-hidden\" , args = args , node = node ) except astroid . Not Found Error : pass", "predictions": ["returns the class being used to determine what the decorator was written . this is only for the method of the method invocation ."], "references": ["check method arguments overriding"], "bleu": 0.050661968099322066, "rouge_l": 0.08198924731182795}
{"id": 1401, "code": "def check accessed members ( self , node , accessed ) : excs = ( \"Attribute Error\" , \"Exception\" , \"Base Exception\" ) for attr , nodes in accessed . items ( ) : try : node . local attr ( attr ) continue except astroid . Not Found Error : pass try : next ( node . instance attr ancestors ( attr ) ) continue except Stop Iteration : pass try : defstmts = node . instance attr ( attr ) except astroid . Not Found Error : pass else : defstmts = [ stmt for stmt in defstmts if stmt not in nodes ] if not defstmts : continue scope = defstmts [ 0 ] . scope ( ) defstmts = [ stmt for i , stmt in enumerate ( defstmts ) if i == 0 or stmt . scope ( ) is not scope ] if len ( defstmts ) == 1 : defstmt = defstmts [ 0 ] frame = defstmt . frame ( ) lno = defstmt . fromlineno for node in nodes : if ( node . frame ( ) is frame and node . fromlineno < lno and not astroid . are exclusive ( node . statement ( ) , defstmt , excs ) ) : self . add message ( \"access-member-before-definition\" , node = node , args = ( attr , lno ) , )", "predictions": ["this is a recursive way to compare the canonical dependencies ."], "references": ["check that accessed members are defined"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1402, "code": "def check signature ( self , method1 , refmethod , class type , cls ) : if not ( isinstance ( method1 , astroid . Function Def ) and isinstance ( refmethod , astroid . Function Def ) ) : self . add message ( \"method-check-failed\" , args = ( method1 , refmethod ) , node = method1 ) return instance = cls . instantiate class ( ) method1 = function to method ( method1 , instance ) refmethod = function to method ( refmethod , instance ) if method1 . args . args is None or refmethod . args . args is None : return if is attr private ( method1 . name ) : return if method1 . decorators : for decorator in method1 . decorators . nodes : if ( isinstance ( decorator , astroid . Attribute ) and decorator . attrname == \"setter\" ) : return if different parameters ( refmethod , method1 , dummy parameter regex = self . dummy rgx ) : self . add message ( \"arguments-differ\" , args = ( class type , method1 . name ) , node = method1 ) elif len ( method1 . args . defaults ) < len ( refmethod . args . defaults ) : self . add message ( \"signature-differs\" , args = ( class type , method1 . name ) , node = method1 )", "predictions": ["parents through the . todo : think the method to be called with the arguments ."], "references": ["check that the signature of the two given methods match"], "bleu": 0.08513012360883544, "rouge_l": 0.16052631578947368}
{"id": 1403, "code": "def is raising ( body : typing . List ) -> bool : for node in body : if isinstance ( node , astroid . Raise ) : return True return False", "predictions": ["determine if the given for this for the given for the , assuming that the for a given for example ."], "references": ["return true if the given statement node raise an exception"], "bleu": 0.10813005337959174, "rouge_l": 0.20677966101694914}
{"id": 1404, "code": "def visit tryexcept ( self , node ) : self . check try except raise ( node ) exceptions classes = [ ] nb handlers = len ( node . handlers ) for index , handler in enumerate ( node . handlers ) : if handler . type is None : if not is raising ( handler . body ) : self . add message ( \"bare-except\" , node = handler ) if index < ( nb handlers - 1 ) : msg = \"empty except clause should always appear last\" self . add message ( \"bad-except-order\" , node = node , args = msg ) elif isinstance ( handler . type , astroid . Bool Op ) : self . add message ( \"binary-op-exception\" , node = handler , args = handler . type . op ) else : try : excs = list ( annotated unpack infer ( handler . type ) ) except astroid . Inference Error : continue for part , exc in excs : if exc is astroid . Uninferable : continue if isinstance ( exc , astroid . Instance ) and utils . inherit from std ex ( exc ) : exc = exc . proxied self . check catching non exception ( handler , exc , part ) if not isinstance ( exc , astroid . Class Def ) : continue exc ancestors = [ anc for anc in exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] for previous exc in exceptions classes : if previous exc in exc ancestors : msg = \"%s is an ancestor class of %s\" % ( previous exc . name , exc . name , ) self . add message ( \"bad-except-order\" , node = handler . type , args = msg ) if ( exc . name in self . config . overgeneral exceptions and exc . root ( ) . name == utils . EXCEPTIONS MODULE and not is raising ( handler . body ) ) : self . add message ( \"broad-except\" , args = exc . name , node = handler . type ) if exc in exceptions classes : self . add message ( \"duplicate-except\" , args = exc . name , node = handler . type ) exceptions classes += [ exc for , exc in excs ]", "predictions": ["recursively generate the report ."], "references": ["check for empty except"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1405, "code": "def visit functiondef ( self , node ) : if not node . is method ( ) : return klass = node . parent . frame ( ) for stmt in node . nodes of class ( astroid . Call ) : if node frame class ( stmt ) != node frame class ( node ) : continue expr = stmt . func if not isinstance ( expr , astroid . Attribute ) : continue call = expr . expr if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and call . func . name == \"super\" ) : continue if not klass . newstyle and has known bases ( klass ) : continue else : if not call . args : if sys . version info [ 0 ] == 3 : continue else : self . add message ( \"missing-super-argument\" , node = call ) continue arg0 = call . args [ 0 ] if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and arg0 . func . name == \"type\" ) : self . add message ( \"bad-super-call\" , node = call , args = ( \"type\" , ) ) continue if ( len ( call . args ) >= 2 and isinstance ( call . args [ 1 ] , astroid . Name ) and call . args [ 1 ] . name == \"self\" and isinstance ( arg0 , astroid . Attribute ) and arg0 . attrname == \" class \" ) : self . add message ( \"bad-super-call\" , node = call , args = ( \"self. class \" , ) ) continue try : supcls = call . args and next ( call . args [ 0 ] . infer ( ) , None ) except astroid . Inference Error : continue if klass is not supcls : name = None if supcls : name = supcls . name elif call . args and hasattr ( call . args [ 0 ] , \"name\" ) : name = call . args [ 0 ] . name if name : self . add message ( \"bad-super-call\" , node = call , args = ( name , ) )", "predictions": ["infer calls to the given node method ."], "references": ["check use of super"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1406, "code": "def display reports ( self , layout ) : self . section = 0 if hasattr ( layout , \"report id\" ) : layout . children [ 0 ] . children [ 0 ] . data += \" (%s)\" % layout . report id self . display ( layout )", "predictions": ["register this object as a ( i . e . , the image represented by this method checker ."], "references": ["display results encapsulated in the layout tree"], "bleu": 0.06439931429457924, "rouge_l": 0.08390646492434663}
{"id": 1407, "code": "def is typing namedtuple ( node : astroid . Class Def ) -> bool : for base in node . ancestors ( ) : if base . qname ( ) == TYPING NAMEDTUPLE : return True return False", "predictions": ["returns true if the given , i . e . whether it is a ( i . e . a ( i . e . a ( my , or a ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["check if a class node is a typing . namedtuple class"], "bleu": 0.03760979247438994, "rouge_l": 0.14823815309842042}
{"id": 1408, "code": "def visit classdef ( self , node ) : nb parents = len ( list ( node . ancestors ( ) ) ) if nb parents > self . config . max parents : self . add message ( \"too-many-ancestors\" , node = node , args = ( nb parents , self . config . max parents ) , ) if len ( node . instance attrs ) > self . config . max attributes : self . add message ( \"too-many-instance-attributes\" , node = node , args = ( len ( node . instance attrs ) , self . config . max attributes ) , )", "predictions": ["generate nodes for a node ."], "references": ["check size of inheritance hierarchy and number of instance attributes"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 1409, "code": "def leave classdef ( self , node ) : my methods = sum ( 1 for method in node . mymethods ( ) if not method . name . startswith ( \" \" ) ) if my methods > self . config . max public methods : self . add message ( \"too-many-public-methods\" , node = node , args = ( my methods , self . config . max public methods ) , ) if ( node . type != \"class\" or is enum class ( node ) or is dataclass ( node ) or is typing namedtuple ( node ) ) : return all methods = count methods in class ( node ) if all methods < self . config . min public methods : self . add message ( \"too-few-public-methods\" , node = node , args = ( all methods , self . config . min public methods ) , )", "predictions": ["shouldn ' t use fading for all other grabpoint ."], "references": ["check number of public methods"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1410, "code": "def visit tryexcept ( self , node ) : branches = len ( node . handlers ) if node . orelse : branches += 1 self . inc branch ( node , branches ) self . inc all stmts ( branches )", "predictions": ["makes the original class visit all edges ."], "references": ["increments the branches counter"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1411, "code": "def visit if ( self , node ) : self . check boolean expressions ( node ) branches = 1 if node . orelse and ( len ( node . orelse ) > 1 or not isinstance ( node . orelse [ 0 ] , If ) ) : branches += 1 self . inc branch ( node , branches ) self . inc all stmts ( branches )", "predictions": ["visit this class with the given node ."], "references": ["increments the branches counter and checks boolean expressions"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1412, "code": "def visit while ( self , node ) : branches = 1 if node . orelse : branches += 1 self . inc branch ( node , branches )", "predictions": ["makes the original visit table ."], "references": ["increments the branches counter"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1413, "code": "def check docstring ( self , node ) : docstring = node . doc if not docstring : return start line = node . lineno + 1 for idx , line in enumerate ( docstring . splitlines ( ) ) : self . check spelling ( \"wrong-spelling-in-docstring\" , line , start line + idx )", "predictions": ["check whether the docstring is already in the docstring ."], "references": ["check the node has any spelling errors"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 1414, "code": "def register ( linter ) : linter . register checker ( Refactoring Checker ( linter ) ) linter . register checker ( Not Checker ( linter ) ) linter . register checker ( Recommandation Checker ( linter ) ) linter . register checker ( Len Checker ( linter ) )", "predictions": ["required method to auto register this checker ."], "references": ["required method to auto register this checker ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 1415, "code": "def check stop iteration inside generator ( self , node ) : frame = node . frame ( ) if not isinstance ( frame , astroid . Function Def ) or not frame . is generator ( ) : return if utils . node ignores exception ( node , Stop Iteration ) : return if not node . exc : return exc = utils . safe infer ( node . exc ) if exc is None or exc is astroid . Uninferable : return if self . check exception inherit from stopiteration ( exc ) : self . add message ( \"stop-iteration-return\" , node = node )", "predictions": ["check that there is a stop request for this node ."], "references": ["check if an exception of type stopiteration is raised inside a generator"], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 1416, "code": "def check exception inherit from stopiteration ( exc ) : stopiteration qname = \"{}.Stop Iteration\" . format ( utils . EXCEPTIONS MODULE ) return any ( class . qname ( ) == stopiteration qname for class in exc . mro ( ) )", "predictions": ["check whether there are any exception inherit ."], "references": ["return true if the exception node in argument inherit from stopiteration"], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 1417, "code": "def check nested blocks ( self , node ) : if not isinstance ( node . scope ( ) , astroid . Function Def ) : return nested blocks = self . nested blocks [ : ] if node . parent == node . scope ( ) : self . nested blocks = [ node ] else : for ancestor node in reversed ( self . nested blocks ) : if ancestor node == node . parent : break self . nested blocks . pop ( ) if isinstance ( node , astroid . If ) and self . is actual elif ( node ) : if self . nested blocks : self . nested blocks . pop ( ) self . nested blocks . append ( node ) if len ( nested blocks ) > len ( self . nested blocks ) : self . emit nested blocks message if needed ( nested blocks )", "predictions": ["we need to check if a node is assigned to a nested node ."], "references": ["update and check the number of nested blocks"], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 1418, "code": "def check consider merging isinstance ( self , node ) : if node . op != \"or\" : return first args = self . duplicated isinstance types ( node ) for duplicated name , class names in first args . items ( ) : names = sorted ( name for name in class names ) self . add message ( \"consider-merging-isinstance\" , node = node , args = ( duplicated name , \", \" . join ( names ) ) , )", "predictions": ["check that this is a working operation for an array of raw components ."], "references": ["check isinstance calls which can be merged together ."], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 1419, "code": "def visit for ( self , node ) : if not isinstance ( node . iter , astroid . Call ) : return if not self . is builtin ( node . iter . func , \"range\" ) : return if len ( node . iter . args ) == 2 and not is constant zero ( node . iter . args [ 0 ] ) : return if len ( node . iter . args ) > 2 : return if not isinstance ( node . iter . args [ - 1 ] , astroid . Call ) : return second func = node . iter . args [ - 1 ] . func if not self . is builtin ( second func , \"len\" ) : return len args = node . iter . args [ - 1 ] . args if not len args or len ( len args ) != 1 : return iterating object = len args [ 0 ] if not isinstance ( iterating object , astroid . Name ) : return scope = node . scope ( ) if iterating object . name == \"self\" and scope . name == \" iter \" : return for child in node . body : for subscript in child . nodes of class ( astroid . Subscript ) : if not isinstance ( subscript . value , astroid . Name ) : continue if not isinstance ( subscript . slice , astroid . Index ) : continue if not isinstance ( subscript . slice . value , astroid . Name ) : continue if subscript . slice . value . name != node . target . name : continue if iterating object . name != subscript . value . name : continue if subscript . value . scope ( ) != node . scope ( ) : continue self . add message ( \"consider-using-enumerate\" , node = node ) return", "predictions": ["this method replaces the nodes with the given name ."], "references": ["emit a convention whenever range and len are used for indexing ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 1420, "code": "def check graphviz available ( output format ) : try : subprocess . call ( [ \"dot\" , \"-V\" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except OS Error : print ( \"The output format '%s' is currently not available.\\n\" \"Please install 'Graphviz' to have other output formats \" \"than 'dot' or 'vcg'.\" % output format ) sys . exit ( 32 )", "predictions": ["checks the graphviz status ."], "references": ["check if we need graphviz for different output format"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 1421, "code": "def run ( self , args ) : if not args : print ( self . help ( ) ) return 1 sys . path . insert ( 0 , os . getcwd ( ) ) try : project = project from files ( args , project name = self . config . project , black list = self . config . black list , ) linker = Linker ( project , tag = True ) handler = Diadefs Handler ( self . config ) diadefs = handler . get diadefs ( project , linker ) finally : sys . path . pop ( 0 ) if self . config . output format == \"vcg\" : writer . VCG Writer ( self . config ) . write ( diadefs ) else : writer . Dot Writer ( self . config ) . write ( diadefs ) return 0", "predictions": ["builds and executes the full project ."], "references": ["checking arguments and run project"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 1422, "code": "def visit tryexcept ( self , node ) : for handler in node . handlers : if handler . type is None : continue if isinstance ( handler . type , astroid . Bool Op ) : continue try : excs = list ( annotated unpack infer ( handler . type ) ) except astroid . Inference Error : continue handled in clause = [ ] for part , exc in excs : if exc is astroid . Uninferable : continue if isinstance ( exc , astroid . Instance ) and utils . inherit from std ex ( exc ) : exc = exc . proxied if not isinstance ( exc , astroid . Class Def ) : continue exc ancestors = [ anc for anc in exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] for prev part , prev exc in handled in clause : prev exc ancestors = [ anc for anc in prev exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] if exc == prev exc : self . add message ( \"overlapping-except\" , node = handler . type , args = \"%s and %s are the same\" % ( prev part . as string ( ) , part . as string ( ) ) , ) elif prev exc in exc ancestors or exc in prev exc ancestors : ancestor = part if exc in prev exc ancestors else prev part descendant = part if prev exc in exc ancestors else prev part self . add message ( \"overlapping-except\" , node = handler . type , args = \"%s is an ancestor class of %s\" % ( ancestor . as string ( ) , descendant . as string ( ) ) , ) handled in clause += [ ( part , exc ) ]", "predictions": ["resolves the given node ."], "references": ["check for empty except"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1423, "code": "def write packages ( self , diagram ) : for i , obj in enumerate ( sorted ( diagram . modules ( ) , key = lambda x : x . title ) ) : self . printer . emit node ( i , label = self . get title ( obj ) , shape = \"box\" ) obj . fig id = i for rel in diagram . get relationships ( \"depends\" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . pkg edges )", "predictions": ["writes an object to all subclasses of this object ."], "references": ["write a package diagram"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1424, "code": "def write classes ( self , diagram ) : for i , obj in enumerate ( sorted ( diagram . objects , key = lambda x : x . title ) ) : self . printer . emit node ( i , * * self . get values ( obj ) ) obj . fig id = i for rel in diagram . get relationships ( \"specialization\" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . inh edges ) for rel in diagram . get relationships ( \"implements\" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . imp edges ) for rel in diagram . get relationships ( \"association\" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , label = rel . name , * * self . association edges )", "predictions": ["writes an association to all the objects in this object ."], "references": ["write a class diagram"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1425, "code": "def set printer ( self , file name , basename ) : layout = dict ( rankdir = \"BT\" ) self . printer = Dot Backend ( basename , additional param = layout ) self . file name = file name", "predictions": ["sets the printer of this file to a printer ."], "references": ["initialize dotwriter and add options for layout ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 1426, "code": "def set printer ( self , file name , basename ) : self . graph file = open ( file name , \"w+\" ) self . printer = VCG Printer ( self . graph file ) self . printer . open graph ( title = basename , layoutalgorithm = \"dfs\" , late edge labels = \"yes\" , port sharing = \"no\" , manhattan edges = \"yes\" , ) self . printer . emit node = self . printer . node self . printer . emit edge = self . printer . edge", "predictions": ["creates a simple set of printer objects ."], "references": ["initialize vcgwriter for a uml graph"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1427, "code": "def may be emitted ( self ) : if self . minversion is not None and self . minversion > sys . version info : return False if self . maxversion is not None and self . maxversion <= sys . version info : return False return True", "predictions": ["returns true if this object is just a negation method ."], "references": ["return true if message may be emitted using the current interpreter"], "bleu": 0.14991106946711685, "rouge_l": 0.18181818181818182}
{"id": 1428, "code": "def format help ( self , checkerref = False ) : desc = self . descr if checkerref : desc += \" This message belongs to the %s checker.\" % self . checker . name title = self . msg if self . symbol : msgid = \"%s (%s)\" % ( self . symbol , self . msgid ) else : msgid = self . msgid if self . minversion or self . maxversion : restr = [ ] if self . minversion : restr . append ( \"< %s\" % \".\" . join ( [ str ( n ) for n in self . minversion ] ) ) if self . maxversion : restr . append ( \">= %s\" % \".\" . join ( [ str ( n ) for n in self . maxversion ] ) ) restr = \" or \" . join ( restr ) if checkerref : desc += \" It can't be emitted when using Python %s.\" % restr else : desc += \" This message can't be emitted when using Python %s.\" % restr desc = normalize text ( \" \" . join ( desc . split ( ) ) , indent = \"  \" ) if title != \"%s\" : title = title . splitlines ( ) [ 0 ] return \":%s: *%s*\\n%s\" % ( msgid , title . rstrip ( \" \" ) , desc ) return \":%s:\\n%s\" % ( msgid , desc )", "predictions": ["formats this help class so that it gets a help json suitable for use in the mqtt way ."], "references": ["return the help string for the given message id"], "bleu": 0.07658412276041004, "rouge_l": 0.2290362953692115}
{"id": 1429, "code": "def get cycles ( graph dict , path , visited , result , vertice ) : if vertice in path : cycle = [ vertice ] for node in path [ : : - 1 ] : if node == vertice : break cycle . insert ( 0 , node ) start from = min ( cycle ) index = cycle . index ( start from ) cycle = cycle [ index : ] + cycle [ 0 : index ] if cycle not in result : result . append ( cycle ) return path . append ( vertice ) try : for node in graph dict [ vertice ] : if node not in visited : get cycles ( graph dict , path , visited , result , node ) visited . add ( node ) except Key Error : pass path . pop ( )", "predictions": ["recursive function for ( ."], "references": ["recursive function doing the real work for get_cycles"], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 1430, "code": "def get source ( self ) : if self . source is None : self . emit ( \"}\\n\" ) self . source = \"\\n\" . join ( self . lines ) del self . lines return self . source", "predictions": ["a simple wrapper around the source machine ."], "references": ["returns self . _source"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1431, "code": "def rest format section ( stream , section , options , doc = None ) : if section : print ( \"%s\\n%s\" % ( section , \"'\" * len ( section ) ) , file = stream ) if doc : print ( normalize text ( doc , line len = 79 , indent = \"\" ) , file = stream ) print ( file = stream ) for optname , optdict , value in options : help opt = optdict . get ( \"help\" ) print ( \":%s:\" % optname , file = stream ) if help opt : help opt = normalize text ( help opt , line len = 79 , indent = \"  \" ) print ( help opt , file = stream ) if value : value = str ( format option value ( optdict , value ) ) print ( file = stream ) print ( \"  Default: ``%s``\" % value . replace ( \"`` \" , \"```` ``\" ) , file = stream )", "predictions": ["format the rest . this must be called from ( ( ) method ."], "references": ["format an options section using as rest formatted output"], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 1432, "code": "def disable ( self , msgid , scope = \"package\" , line = None , ignore unknown = False ) : self . set msg status ( msgid , enable = False , scope = scope , line = line , ignore unknown = ignore unknown ) self . register by id managed msg ( msgid , line )", "predictions": ["log4j log4j log4j use ( instead ."], "references": ["don t output message of the given id"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 1433, "code": "def enable ( self , msgid , scope = \"package\" , line = None , ignore unknown = False ) : self . set msg status ( msgid , enable = True , scope = scope , line = line , ignore unknown = ignore unknown ) self . register by id managed msg ( msgid , line , is disabled = False )", "predictions": ["enable or disable a class ."], "references": ["reenable message of the given id"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1434, "code": "def print full documentation ( self , stream = None ) : if not stream : stream = sys . stdout print ( \"Pylint global options and switches\" , file = stream ) print ( \"----------------------------------\" , file = stream ) print ( \"\" , file = stream ) print ( \"Pylint provides global options and switches.\" , file = stream ) print ( \"\" , file = stream ) by checker = { } for checker in self . get checkers ( ) : if checker . name == \"master\" : if checker . options : for section , options in checker . options by section ( ) : if section is None : title = \"General options\" else : title = \"%s options\" % section . capitalize ( ) print ( title , file = stream ) print ( \"~\" * len ( title ) , file = stream ) rest format section ( stream , None , options ) print ( \"\" , file = stream ) else : name = checker . name try : by checker [ name ] [ \"options\" ] += checker . options and values ( ) by checker [ name ] [ \"msgs\" ] . update ( checker . msgs ) by checker [ name ] [ \"reports\" ] += checker . reports except Key Error : by checker [ name ] = { \"options\" : list ( checker . options and values ( ) ) , \"msgs\" : dict ( checker . msgs ) , \"reports\" : list ( checker . reports ) , } print ( \"Pylint checkers' options and switches\" , file = stream ) print ( \"-------------------------------------\" , file = stream ) print ( \"\" , file = stream ) print ( \"Pylint checkers can provide three set of features:\" , file = stream ) print ( \"\" , file = stream ) print ( \"* options that control their execution,\" , file = stream ) print ( \"* messages that they can raise,\" , file = stream ) print ( \"* reports that they can generate.\" , file = stream ) print ( \"\" , file = stream ) print ( \"Below is a list of all checkers and their features.\" , file = stream ) print ( \"\" , file = stream ) for checker , info in sorted ( by checker . items ( ) ) : self . print checker doc ( checker , info , stream = stream )", "predictions": ["prints the options for this stream ."], "references": ["output a full documentation in rest format"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1435, "code": "def get indent length ( line ) : result = 0 for char in line : if char == \" \" : result += 1 elif char == \"\\t\" : result += TAB LENGTH else : break return result", "predictions": ["get length of input line ."], "references": ["return the length of the indentation on the given token s line ."], "bleu": 0.1138350749157144, "rouge_l": 0.39482200647249194}
{"id": 1436, "code": "def get indent hint line ( bar positions , bad position ) : if not bar positions : return ( \"\" , \"\" ) bar positions = [ get indent length ( indent ) for indent in bar positions ] bad position = get indent length ( bad position ) delta message = \"\" markers = [ ( pos , \"|\" ) for pos in bar positions ] if len ( markers ) == 1 : expected position = markers [ 0 ] [ 0 ] delta = abs ( expected position - bad position ) direction = \"add\" if expected position > bad position else \"remove\" delta message = CONTINUATION HINT MESSAGE % ( direction , delta , \"s\" if delta > 1 else \"\" , ) markers . append ( ( bad position , \"^\" ) ) markers . sort ( ) line = [ \" \" ] * ( markers [ - 1 ] [ 0 ] + 1 ) for position , marker in markers : line [ position ] = marker return ( \"\" . join ( line ) , delta message )", "predictions": ["indent an hint at a given position ."], "references": ["return a line with |s for each of the positions in the given lists ."], "bleu": 0.07949903911132591, "rouge_l": 0.24729729729729732}
{"id": 1437, "code": "def handle line start ( self , pos ) : if self . line start > - 1 : return check token position = pos if self . tokens . token ( pos ) == ASYNC TOKEN : check token position += 1 self . is block opener = ( self . tokens . token ( check token position ) in CONTINUATION BLOCK OPENERS ) self . line start = pos", "predictions": ["handle the line at the given position ."], "references": ["record the first non - junk token at the start of a line ."], "bleu": 0.1185574919557074, "rouge_l": 0.346590909090909}
{"id": 1438, "code": "def get valid indentations ( self , idx ) : stack top = - 1 if ( self . tokens . token ( idx ) in ( \"}\" , \"for\" ) and self . cont stack [ - 1 ] . token == \":\" ) : stack top = - 2 indent = self . cont stack [ stack top ] if self . tokens . token ( idx ) in CLOSING BRACKETS : valid indentations = indent . valid outdent strings else : valid indentations = indent . valid continuation strings return indent , valid indentations . copy ( )", "predictions": ["get a valid indentations object ."], "references": ["returns the valid offsets for the token at the given position ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 1439, "code": "def continuation inside bracket ( self , bracket , position ) : indentation = self . tokens . line indent ( position ) token indent = self . tokens . token indent ( position ) next token indent = self . tokens . token indent ( position + 1 ) if ( self . is block opener and next token indent == indentation + self . block indent string ) : return Continued Indent ( CONTINUED BLOCK , bracket , position , Indentations ( token indent ) , Before Block Indentations ( next token indent , next token indent + self . continuation string ) , ) return Continued Indent ( CONTINUED , bracket , position , Indentations ( token indent , next token indent ) , Indentations ( next token indent ) , )", "predictions": ["run run the current line of text ."], "references": ["extracts indentation information for a continued indent ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1440, "code": "def new line ( self , tokens , line end , line start ) : if last token on line is ( tokens , line end , \";\" ) : self . add message ( \"unnecessary-semicolon\" , line = tokens . start line ( line end ) ) line num = tokens . start line ( line start ) line = tokens . line ( line start ) if tokens . type ( line start ) not in JUNK TOKENS : self . lines [ line num ] = line . split ( \"\\n\" ) [ 0 ] self . check lines ( line , line num )", "predictions": ["create a new classdef classdef ."], "references": ["a new line has been encountered process it if necessary"], "bleu": 0.14925824694560996, "rouge_l": 0.23921568627450981}
{"id": 1441, "code": "def has valid type annotation ( self , tokens , i ) : if not self . inside brackets ( \"(\" ) : return False bracket level = 0 for token in tokens [ i - 1 : : - 1 ] : if token [ 1 ] == \":\" : return True if token [ 1 ] == \"(\" : return False if token [ 1 ] == \"]\" : bracket level += 1 elif token [ 1 ] == \"[\" : bracket level -= 1 elif token [ 1 ] == \",\" : if not bracket level : return False elif token [ 1 ] in ( \".\" , \"...\" ) : continue elif token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : return False return False", "predictions": ["this function checks if there is a classdef self ( i . e . , not a default self - based ( i . e . , not a have a default methods methods methods methods methods methods methods methods methods methods methods methods methods methods methods methods methods methods"], "references": ["extended check of pep - 484 type hint presence"], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 1442, "code": "def check equals spacing ( self , tokens , i ) : if self . has valid type annotation ( tokens , i ) : self . check space ( tokens , i , ( MUST , MUST ) ) elif self . inside brackets ( \"(\" ) or self . inside brackets ( \"lambda\" ) : self . check space ( tokens , i , ( MUST NOT , MUST NOT ) ) else : self . check space ( tokens , i , ( MUST , MUST ) )", "predictions": ["run duplicated on this node ."], "references": ["check the spacing of a single equals sign ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1443, "code": "def check surrounded by space ( self , tokens , i ) : self . check space ( tokens , i , ( MUST , MUST ) )", "predictions": ["then check whether all documents have been reached ."], "references": ["check that a binary operator is surrounded by exactly one space ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 1444, "code": "def visit default ( self , node ) : if not node . is statement : return if not node . root ( ) . pure python : return prev sibl = node . previous sibling ( ) if prev sibl is not None : prev line = prev sibl . fromlineno else : if ( isinstance ( node . parent , nodes . Try Finally ) and node in node . parent . finalbody ) : prev line = node . parent . body [ 0 ] . tolineno + 1 else : prev line = node . parent . statement ( ) . fromlineno line = node . fromlineno assert line , node if prev line == line and self . visited lines . get ( line ) != 2 : self . check multi statement line ( node , line ) return if line in self . visited lines : return try : tolineno = node . blockstart tolineno except Attribute Error : tolineno = node . tolineno assert tolineno , node lines = [ ] for line in range ( line , tolineno + 1 ) : self . visited lines [ line ] = 1 try : lines . append ( self . lines [ line ] . rstrip ( ) ) except Key Error : lines . append ( \"\" )", "predictions": ["replaces instances with single line operation ."], "references": ["check the node line number and check it if not yet done"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 1445, "code": "def check multi statement line ( self , node , line ) : if isinstance ( node , nodes . With ) : return if isinstance ( node , nodes . Try Except ) and isinstance ( node . parent , nodes . Try Finally ) : return if ( isinstance ( node . parent , nodes . If ) and not node . parent . orelse and self . config . single line if stmt ) : return if ( isinstance ( node . parent , nodes . Class Def ) and len ( node . parent . body ) == 1 and self . config . single line class stmt ) : return self . add message ( \"multiple-statements\" , node = node ) self . visited lines [ line ] = 2", "predictions": ["requires that two not the same way as the given self - self - existing not ."], "references": ["check for lines containing multiple statements ."], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 1446, "code": "def check lines ( self , lines , i ) : max chars = self . config . max line length ignore long line = self . config . ignore long lines def check line ( line , i ) : if not line . endswith ( \"\\n\" ) : self . add message ( \"missing-final-newline\" , line = i ) else : stripped line = line . rstrip ( \"\\t\\n\\r\\v \" ) if not stripped line and EMPTY LINE in self . config . no space check : pass elif line [ len ( stripped line ) : ] not in ( \"\\n\" , \"\\r\\n\" ) : self . add message ( \"trailing-whitespace\" , line = i , col offset = len ( stripped line ) ) line = stripped line mobj = OPTION RGX . search ( line ) if mobj and \"=\" in line : front of equal , , back of equal = mobj . group ( 1 ) . partition ( \"=\" ) if front of equal . strip ( ) == \"disable\" : if \"line-too-long\" in { msg id . strip ( ) for msg id in back of equal . split ( \",\" ) } : return None line = line . rsplit ( \"#\" , 1 ) [ 0 ] . rstrip ( ) if len ( line ) > max chars and not ignore long line . search ( line ) : self . add message ( \"line-too-long\" , line = i , args = ( len ( line ) , max chars ) ) return i + 1 unsplit ends = { \"\\v\" , \"\\x0b\" , \"\\f\" , \"\\x0c\" , \"\\x1c\" , \"\\x1d\" , \"\\x1e\" , \"\\x85\" , \"\\u2028\" , \"\\u2029\" , } unsplit = [ ] for line in lines . splitlines ( True ) : if line [ - 1 ] in unsplit ends : unsplit . append ( line ) continue if unsplit : unsplit . append ( line ) line = \"\" . join ( unsplit ) unsplit = [ ] i = check line ( line , i ) if i is None : break if unsplit : check line ( \"\" . join ( unsplit ) , i )", "predictions": ["combines two ( i . e . , the following lines must be a non - null checker or a non - null line line checker ."], "references": ["check lines have less than a maximum number of characters"], "bleu": 0.04970745472800838, "rouge_l": 0.1178743961352657}
{"id": 1447, "code": "def check indent level ( self , string , expected , line num ) : indent = self . config . indent string if indent == \"\\\\t\" : indent = \"\\t\" level = 0 unit size = len ( indent ) while string [ : unit size ] == indent : string = string [ unit size : ] level += 1 suppl = \"\" while string and string [ 0 ] in \" \\t\" : if string [ 0 ] != indent [ 0 ] : if string [ 0 ] == \"\\t\" : args = ( \"tab\" , \"space\" ) else : args = ( \"space\" , \"tab\" ) self . add message ( \"mixed-indentation\" , args = args , line = line num ) return level suppl += string [ 0 ] string = string [ 1 : ] if level != expected or suppl : i type = \"spaces\" if indent [ 0 ] == \"\\t\" : i type = \"tabs\" self . add message ( \"bad-indentation\" , line = line num , args = ( level * unit size + len ( suppl ) , i type , expected * unit size ) , ) return None", "predictions": ["check for stop indentation ."], "references": ["return the indent level of the string"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1448, "code": "def is conditional import ( node ) : parent = node . parent return isinstance ( parent , ( astroid . Try Except , astroid . Except Handler , astroid . If , astroid . If Exp ) )", "predictions": ["checks if a node node node is in use ."], "references": ["checks if an import node is in the context of a conditional ."], "bleu": 0.22211370348624584, "rouge_l": 0.5097493036211699}
{"id": 1449, "code": "def visit name ( self , node ) : found node , = node . lookup ( node . name ) if not is builtin ( found node ) : return if node . name not in self . bad builtins : return if node ignores exception ( node ) or isinstance ( find try except wrapper node ( node ) , astroid . Except Handler ) : return message = node . name . lower ( ) + \"-builtin\" self . add message ( message , node = node )", "predictions": ["check if the given , and return are also detected ."], "references": ["detect when a bad built - in is referenced ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 1450, "code": "def visit subscript ( self , node ) : try : for inferred in node . value . infer ( ) : if not isinstance ( inferred , astroid . Instance ) : continue if utils . inherit from std ex ( inferred ) : self . add message ( \"indexing-exception\" , node = node ) except astroid . Inference Error : return", "predictions": ["simulates this method as a refactoring ."], "references": ["look for indexing exceptions ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1451, "code": "def visit attribute ( self , node ) : if node . attrname == \"xreadlines\" : self . add message ( \"xreadlines-attribute\" , node = node ) return exception message = \"message\" try : for inferred in node . expr . infer ( ) : if isinstance ( inferred , astroid . Instance ) and utils . inherit from std ex ( inferred ) : if node . attrname == exception message : if exception message in inferred . instance attrs : continue self . add message ( \"exception-message-attribute\" , node = node ) if isinstance ( inferred , astroid . Module ) : self . warn if deprecated ( node , inferred . name , { node . attrname } , report on modules = False ) except astroid . Inference Error : return", "predictions": ["this method allows you to constant the [ node ] for the given node . this is meant for use in the case of the abstract class ."], "references": ["look for removed attributes"], "bleu": 0.04327969719414172, "rouge_l": 0.07227488151658767}
{"id": 1452, "code": "def visit excepthandler ( self , node ) : def is used in except block ( node ) : scope = node . scope ( ) current = node while ( current and current != scope and not isinstance ( current , astroid . Except Handler ) ) : current = current . parent return isinstance ( current , astroid . Except Handler ) and current . type != node if isinstance ( node . name , ( astroid . Tuple , astroid . List ) ) : self . add message ( \"unpacking-in-except\" , node = node ) return if not node . name : return scope = node . parent . scope ( ) scope names = scope . nodes of class ( astroid . Name , skip klass = astroid . Function Def ) scope names = list ( scope names ) potential leaked names = [ scope name for scope name in scope names if scope name . name == node . name . name and scope name . lineno > node . lineno and not is used in except block ( scope name ) ] reassignments for same name = { assign name . lineno for assign name in scope . nodes of class ( astroid . Assign Name , skip klass = astroid . Function Def ) if assign name . name == node . name . name } for leaked name in potential leaked names : if any ( node . lineno < elem < leaked name . lineno for elem in reassignments for same name ) : continue self . add message ( \"exception-escape\" , node = leaked name )", "predictions": ["creates the given format for the given format ."], "references": ["visit an except handler block and check for exception unpacking ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 1453, "code": "def find pylintrc ( ) : if os . path . exists ( \"pylintrc\" ) : return os . path . abspath ( \"pylintrc\" ) if os . path . exists ( \".pylintrc\" ) : return os . path . abspath ( \".pylintrc\" ) if os . path . isfile ( \" init .py\" ) : curdir = os . path . abspath ( os . getcwd ( ) ) while os . path . isfile ( os . path . join ( curdir , \" init .py\" ) ) : curdir = os . path . abspath ( os . path . join ( curdir , \"..\" ) ) if os . path . isfile ( os . path . join ( curdir , \"pylintrc\" ) ) : return os . path . join ( curdir , \"pylintrc\" ) if os . path . isfile ( os . path . join ( curdir , \".pylintrc\" ) ) : return os . path . join ( curdir , \".pylintrc\" ) if \"PYLINTRC\" in os . environ and os . path . exists ( os . environ [ \"PYLINTRC\" ] ) : pylintrc = os . environ [ \"PYLINTRC\" ] else : user home = os . path . expanduser ( \"~\" ) if user home in ( \"~\" , \"/root\" ) : pylintrc = \".pylintrc\" else : pylintrc = os . path . join ( user home , \".pylintrc\" ) if not os . path . isfile ( pylintrc ) : pylintrc = os . path . join ( user home , \".config\" , \"pylintrc\" ) if not os . path . isfile ( pylintrc ) : if os . path . isfile ( \"/etc/pylintrc\" ) : pylintrc = \"/etc/pylintrc\" else : pylintrc = None return pylintrc", "predictions": ["run the ( on the current directory ."], "references": ["search the pylint rc file and return its path if it find it else none"], "bleu": 0.06685045700482882, "rouge_l": 0.08243243243243244}
{"id": 1454, "code": "def register options provider ( self , provider , own group = True ) : assert provider . priority <= 0 , \"provider's priority can't be >= 0\" for i in range ( len ( self . options providers ) ) : if provider . priority > self . options providers [ i ] . priority : self . options providers . insert ( i , provider ) break else : self . options providers . append ( provider ) non group spec options = [ option for option in provider . options if \"group\" not in option [ 1 ] ] groups = getattr ( provider , \"option groups\" , ( ) ) if own group and non group spec options : self . add option group ( provider . name . upper ( ) , provider . doc , non group spec options , provider , ) else : for opt , optdict in non group spec options : self . add optik option ( provider , self . cmdline parser , opt , optdict ) for gname , gdoc in groups : gname = gname . upper ( ) goptions = [ option for option in provider . options if option [ 1 ] . get ( \"group\" , \"\" ) . upper ( ) == gname ] self . add option group ( gname , gdoc , goptions , provider )", "predictions": ["this is the method that we can visit the input if it has already been set ."], "references": ["register an options provider"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 1455, "code": "def cb set provider option ( self , option , opt , value , parser ) : if opt . startswith ( \"--\" ) : opt = opt [ 2 : ] else : opt = self . short options [ opt [ 1 : ] ] if value is None : value = 1 self . global set option ( opt , value )", "predictions": ["callback for . ."], "references": ["optik callback for option setting"], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 1456, "code": "def global set option ( self , opt , value ) : self . all options [ opt ] . set option ( opt , value )", "predictions": ["add a option to the command line option ."], "references": ["set option on the correct option provider"], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 1457, "code": "def add help section ( self , title , description , level = 0 ) : group = optparse . Option Group ( self . cmdline parser , title = title . capitalize ( ) , description = description ) group . level = level self . maxlevel = max ( self . maxlevel , level ) self . cmdline parser . add option group ( group )", "predictions": ["set a printer to the ( ."], "references": ["add a dummy option section for help purpose"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1458, "code": "def help ( self , level = 0 ) : self . cmdline parser . formatter . output level = level with patch optparse ( ) : return self . cmdline parser . format help ( )", "predictions": ["a set of set operation on this patch ."], "references": ["return the usage string for available options"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1459, "code": "def load defaults ( self ) : for opt , optdict in self . options : action = optdict . get ( \"action\" ) if action != \"callback\" : if optdict is None : optdict = self . get option def ( opt ) default = optdict . get ( \"default\" ) self . set option ( opt , default , action , optdict )", "predictions": ["convert the value of the default and and and and and and and calls itself ."], "references": ["initialize the provider using default values"], "bleu": 0.08513012360883544, "rouge_l": 0.19805194805194803}
{"id": 1460, "code": "def option attrname ( self , opt , optdict = None ) : if optdict is None : optdict = self . get option def ( opt ) return optdict . get ( \"dest\" , opt . replace ( \"-\" , \" \" ) )", "predictions": ["message to set the value of the given format ."], "references": ["get the config attribute corresponding to opt"], "bleu": 0.13950796967929133, "rouge_l": 0.12151394422310759}
{"id": 1461, "code": "def get option def ( self , opt ) : assert self . options for option in self . options : if option [ 0 ] == opt : return option [ 1 ] raise optparse . Option Error ( \"no such option %s in section %r\" % ( opt , self . name ) , opt )", "predictions": ["get a short cycles for this command and then returns the value ."], "references": ["return the dictionary defining an option given its name"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 1462, "code": "def visit module ( self , node ) : self . logging names = set ( ) logging mods = self . config . logging modules self . format style = self . config . logging format style self . logging modules = set ( logging mods ) self . from imports = { } for logging mod in logging mods : parts = logging mod . rsplit ( \".\" , 1 ) if len ( parts ) > 1 : self . from imports [ parts [ 0 ] ] = parts [ 1 ]", "predictions": ["draws the source . this is called by the module ."], "references": ["clears any state left in this checker from last module checked ."], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 1463, "code": "def visit importfrom ( self , node ) : try : logging name = self . from imports [ node . modname ] for module , as name in node . names : if module == logging name : self . logging names . add ( as name or module ) except Key Error : pass", "predictions": ["this is a helper method to rest modules ."], "references": ["checks to see if a module uses a non - python logging module ."], "bleu": 0.09630141125179911, "rouge_l": 0.1673525377229081}
{"id": 1464, "code": "def visit import ( self , node ) : for module , as name in node . names : if module in self . logging modules : self . logging names . add ( as name or module )", "predictions": ["this is called by the native code when this class is being defined . this method is used to ( = = = = = = = = = = = = 0 = ( self = ( self = new . . . = . self self = ("], "references": ["checks to see if this module uses python s built - in logging ."], "bleu": 0.028577262451992175, "rouge_l": 0.06955530216647662}
{"id": 1465, "code": "def visit call ( self , node ) : def is logging name ( ) : return ( isinstance ( node . func , astroid . Attribute ) and isinstance ( node . func . expr , astroid . Name ) and node . func . expr . name in self . logging names ) def is logger class ( ) : try : for inferred in node . func . infer ( ) : if isinstance ( inferred , astroid . Bound Method ) : parent = inferred . proxied . parent if isinstance ( parent , astroid . Class Def ) and ( parent . qname ( ) == \"logging.Logger\" or any ( ancestor . qname ( ) == \"logging.Logger\" for ancestor in parent . ancestors ( ) ) ) : return True , inferred . proxied . name except astroid . exceptions . Inference Error : pass return False , None if is logging name ( ) : name = node . func . attrname else : result , name = is logger class ( ) if not result : return self . check log method ( node , name )", "predictions": ["infer a method if it has a qname ."], "references": ["checks calls to logging methods ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1466, "code": "def in loop ( node ) : parent = node . parent while parent is not None : if isinstance ( parent , ( astroid . For , astroid . List Comp , astroid . Set Comp , astroid . Dict Comp , astroid . Generator Exp , ) , ) : return True parent = parent . parent return False", "predictions": ["gets the list of all nodes of this full ( p1 self self self self self self self self self self self self self self self self - qualified self self self self self self self self self self self self self - 1 self self self self self self"], "references": ["return true if the node is inside a kind of for loop"], "bleu": 0.026594139297659906, "rouge_l": 0.07253269916765755}
{"id": 1467, "code": "def register ( linter ) : linter . register checker ( Basic Error Checker ( linter ) ) linter . register checker ( Basic Checker ( linter ) ) linter . register checker ( Name Checker ( linter ) ) linter . register checker ( Doc String Checker ( linter ) ) linter . register checker ( Pass Checker ( linter ) ) linter . register checker ( Comparison Checker ( linter ) )", "predictions": ["required method to auto get this :"], "references": ["required method to auto register this checker"], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 1468, "code": "def visit starred ( self , node ) : if isinstance ( node . parent , astroid . Call ) : return if PY35 and isinstance ( node . parent , ( astroid . List , astroid . Tuple , astroid . Set , astroid . Dict ) ) : return stmt = node . statement ( ) if not isinstance ( stmt , astroid . Assign ) : return if stmt . value is node or stmt . value . parent of ( node ) : self . add message ( \"star-needs-assignment-target\" , node = node )", "predictions": ["replaces the given bar with the given bar bar ."], "references": ["check that a starred expression is used in an assignment target ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 1469, "code": "def check nonlocal and global ( self , node ) : def same scope ( current ) : return current . scope ( ) is node from iter = itertools . chain . from iterable nonlocals = set ( from iter ( child . names for child in node . nodes of class ( astroid . Nonlocal ) if same scope ( child ) ) ) if not nonlocals : return global vars = set ( from iter ( child . names for child in node . nodes of class ( astroid . Global ) if same scope ( child ) ) ) for name in nonlocals . intersection ( global vars ) : self . add message ( \"nonlocal-and-global\" , args = ( name , ) , node = node )", "predictions": ["check if there are no more than one is done but not ."], "references": ["check that a name is both nonlocal and global ."], "bleu": 0.1135935489027116, "rouge_l": 0.2671532846715329}
{"id": 1470, "code": "def visit unaryop ( self , node ) : if ( ( node . op in \"+-\" ) and isinstance ( node . operand , astroid . Unary Op ) and ( node . operand . op == node . op ) ) : self . add message ( \"nonexistent-operator\" , node = node , args = node . op * 2 )", "predictions": ["simulates a new ( with the given , as long as possible ."], "references": ["check use of the non - existent ++ and -- operator operator"], "bleu": 0.09552040806823771, "rouge_l": 0.08058124174372522}
{"id": 1471, "code": "def check else on loop ( self , node ) : if node . orelse and not loop exits early ( node ) : self . add message ( \"useless-else-on-loop\" , node = node , line = node . orelse [ 0 ] . lineno - 1 , )", "predictions": ["sets this reader ' s block until the ( ( : ) is moved to the ( : ) ."], "references": ["check that any loop with an else clause has a break statement ."], "bleu": 0.06108557268562171, "rouge_l": 0.06301652892561983}
{"id": 1472, "code": "def check in loop ( self , node , node name ) : node = node . parent while node : if isinstance ( node , ( astroid . For , astroid . While ) ) : if node not in node . orelse : return if isinstance ( node , ( astroid . Class Def , astroid . Function Def ) ) : break if ( isinstance ( node , astroid . Try Finally ) and node in node . finalbody and isinstance ( node , astroid . Continue ) ) : self . add message ( \"continue-in-finally\" , node = node ) node = node . parent self . add message ( \"not-in-loop\" , node = node , args = node name )", "predictions": ["this method checks the nodes for a loop ."], "references": ["check that a node is inside a for or while loop"], "bleu": 0.1343994460963362, "rouge_l": 0.19645732689210954}
{"id": 1473, "code": "def open ( self ) : self . tryfinallys = [ ] self . stats = self . linter . add stats ( module = 0 , function = 0 , method = 0 , class = 0 )", "predictions": ["now we are able to call the open ( ) method on the class ."], "references": ["initialize visit variables and statistics"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 1474, "code": "def visit expr ( self , node ) : expr = node . value if isinstance ( expr , astroid . Const ) and isinstance ( expr . value , str ) : scope = expr . scope ( ) if isinstance ( scope , ( astroid . Class Def , astroid . Module , astroid . Function Def ) ) : if isinstance ( scope , astroid . Function Def ) and scope . name != \" init \" : pass else : sibling = expr . previous sibling ( ) if ( sibling is not None and sibling . scope ( ) is scope and isinstance ( sibling , ( astroid . Assign , astroid . Ann Assign ) ) ) : return self . add message ( \"pointless-string-statement\" , node = node ) return if isinstance ( expr , ( astroid . Yield , astroid . Await , astroid . Ellipsis , astroid . Call ) ) or ( isinstance ( node . parent , astroid . Try Except ) and node . parent . body == [ node ] ) : return if any ( expr . nodes of class ( astroid . Call ) ) : self . add message ( \"expression-not-assigned\" , node = node , args = expr . as string ( ) ) else : self . add message ( \"pointless-statement\" , node = node )", "predictions": ["main function for the given node ."], "references": ["check for various kind of statements without effect"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1475, "code": "def visit lambda ( self , node ) : if node . args . defaults : return call = node . body if not isinstance ( call , astroid . Call ) : return if isinstance ( node . body . func , astroid . Attribute ) and isinstance ( node . body . func . expr , astroid . Call ) : return call site = Call Site . from call ( call ) ordinary args = list ( node . args . args ) new call args = list ( self . filter vararg ( node , call . args ) ) if node . args . kwarg : if self . has variadic argument ( call . kwargs , node . args . kwarg ) : return if node . args . vararg : if self . has variadic argument ( call . starargs , node . args . vararg ) : return elif call . starargs : return if call . keywords : lambda kwargs = { keyword . name for keyword in node . args . defaults } if len ( lambda kwargs ) != len ( call site . keyword arguments ) : return if set ( call site . keyword arguments ) . difference ( lambda kwargs ) : return if len ( ordinary args ) != len ( new call args ) : return for arg , passed arg in zip ( ordinary args , new call args ) : if not isinstance ( passed arg , astroid . Name ) : return if arg . name != passed arg . name : return self . add message ( \"unnecessary-lambda\" , line = node . fromlineno , node = node )", "predictions": ["returns the call with the given parameters ."], "references": ["check whether or not the lambda is suspicious"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1476, "code": "def visit assert ( self , node ) : if ( node . fail is None and isinstance ( node . test , astroid . Tuple ) and len ( node . test . elts ) == 2 ) : self . add message ( \"assert-on-tuple\" , node = node )", "predictions": ["adds this node to the given node ."], "references": ["check the use of an assert statement on a tuple ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 1477, "code": "def visit dict ( self , node ) : keys = set ( ) for k , in node . items : if isinstance ( k , astroid . Const ) : key = k . value if key in keys : self . add message ( \"duplicate-key\" , node = node , args = key ) keys . add ( key )", "predictions": ["generate a new processed ."], "references": ["check duplicate key in dictionary"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1478, "code": "def check reversed ( self , node ) : try : argument = utils . safe infer ( utils . get argument from call ( node , position = 0 ) ) except utils . No Such Argument Error : pass else : if argument is astroid . Uninferable : return if argument is None : if isinstance ( node . args [ 0 ] , astroid . Call ) : try : func = next ( node . args [ 0 ] . func . infer ( ) ) except astroid . Inference Error : return if getattr ( func , \"name\" , None ) == \"iter\" and utils . is builtin object ( func ) : self . add message ( \"bad-reversed-sequence\" , node = node ) return if isinstance ( argument , ( astroid . List , astroid . Tuple ) ) : return if isinstance ( argument , astroid . Instance ) : if argument . proxied . name == \"dict\" and utils . is builtin object ( argument . proxied ) : self . add message ( \"bad-reversed-sequence\" , node = node ) return if any ( ancestor . name == \"dict\" and utils . is builtin object ( ancestor ) for ancestor in argument . proxied . ancestors ( ) ) : try : argument . locals [ REVERSED PROTOCOL METHOD ] except Key Error : self . add message ( \"bad-reversed-sequence\" , node = node ) return if hasattr ( argument , \"getattr\" ) : for methods in REVERSED METHODS : for meth in methods : try : argument . getattr ( meth ) except astroid . Not Found Error : break else : break else : self . add message ( \"bad-reversed-sequence\" , node = node ) else : self . add message ( \"bad-reversed-sequence\" , node = node )", "predictions": ["builds a method infer the infer operation on the given node ."], "references": ["check that the argument to reversed is a sequence"], "bleu": 0.11498759556447223, "rouge_l": 0.09775641025641024}
{"id": 1479, "code": "def visit assignname ( self , node ) : self . check assign to new keyword violation ( node . name , node ) frame = node . frame ( ) assign type = node . assign type ( ) if isinstance ( assign type , astroid . Comprehension ) : self . check name ( \"inlinevar\" , node . name , node ) elif isinstance ( frame , astroid . Module ) : if isinstance ( assign type , astroid . Assign ) and not in loop ( assign type ) : if isinstance ( utils . safe infer ( assign type . value ) , astroid . Class Def ) : self . check name ( \"class\" , node . name , node ) else : if not redefines import ( node ) : self . check name ( \"const\" , node . name , node ) elif isinstance ( assign type , astroid . Except Handler ) : self . check name ( \"variable\" , node . name , node ) elif isinstance ( frame , astroid . Function Def ) : if node . name in frame and node . name not in frame . argnames ( ) : if not redefines import ( node ) : self . check name ( \"variable\" , node . name , node ) elif isinstance ( frame , astroid . Class Def ) : if not list ( frame . local attr ancestors ( node . name ) ) : self . check name ( \"class attribute\" , node . name , node )", "predictions": ["produces a assignname for the given node ."], "references": ["check module level assigned names"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1480, "code": "def check name ( self , node type , name , node , confidence = interfaces . HIGH ) : def should exempt from invalid name ( node ) : if node type == \"variable\" : inferred = utils . safe infer ( node ) if isinstance ( inferred , astroid . Class Def ) : return True return False if utils . is inside except ( node ) : clobbering , = utils . clobber in except ( node ) if clobbering : return if name in self . config . good names : return if name in self . config . bad names : self . stats [ \"badname \" + node type ] += 1 self . add message ( \"blacklisted-name\" , node = node , args = name ) return regexp = self . name regexps [ node type ] match = regexp . match ( name ) if is multi naming match ( match , node type , confidence ) : name group = self . find name group ( node type ) bad name group = self . bad names . setdefault ( name group , { } ) warnings = bad name group . setdefault ( match . lastgroup , [ ] ) warnings . append ( ( node , node type , name , confidence ) ) if match is None and not should exempt from invalid name ( node ) : self . raise name warning ( node , node type , name , confidence )", "predictions": ["appends a name to the multi - match ."], "references": ["check for a name using the type s regexp"], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 1481, "code": "def check docstring ( self , node type , node , report missing = True , confidence = interfaces . HIGH ) : docstring = node . doc if docstring is None : if not report missing : return lines = utils . get node last lineno ( node ) - node . lineno if node type == \"module\" and not lines : return max lines = self . config . docstring min length if node type != \"module\" and max lines > - 1 and lines < max lines : return self . stats [ \"undocumented \" + node type ] += 1 if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : func = utils . safe infer ( node . body [ 0 ] . value . func ) if isinstance ( func , astroid . Bound Method ) and isinstance ( func . bound , astroid . Instance ) : if PY3K and func . bound . name == \"str\" : return if func . bound . name in ( \"str\" , \"unicode\" , \"bytes\" ) : return self . add message ( \"missing-docstring\" , node = node , args = ( node type , ) , confidence = confidence ) elif not docstring . strip ( ) : self . stats [ \"undocumented \" + node type ] += 1 self . add message ( \"empty-docstring\" , node = node , args = ( node type , ) , confidence = confidence )", "predictions": ["checks for docstring that should be done in order to make sense to the return type ."], "references": ["check the node has a non empty docstring"], "bleu": 0.07994607499472013, "rouge_l": 0.08555399719495091}
{"id": 1482, "code": "def check literal comparison ( self , literal , node ) : nodes = ( astroid . List , astroid . Tuple , astroid . Dict , astroid . Set ) is other literal = isinstance ( literal , nodes ) is const = False if isinstance ( literal , astroid . Const ) : if isinstance ( literal . value , bool ) or literal . value is None : return is const = isinstance ( literal . value , ( bytes , str , int , float ) ) if is const or is other literal : self . add message ( \"literal-comparison\" , node = node )", "predictions": ["check the comparison for this literal ."], "references": ["check if we compare to a literal which is usually what we do not want to do ."], "bleu": 0.045890725128646594, "rouge_l": 0.2223572296476306}
{"id": 1483, "code": "def subgraph ( self , node , name , extra blocks = ( ) ) : if self . graph is None : self . graph = Path Graph ( node ) self . subgraph parse ( node , node , extra blocks ) self . graphs [ \"%s%s\" % ( self . classname , name ) ] = self . graph self . reset ( ) else : self . append node ( node ) self . subgraph parse ( node , node , extra blocks )", "predictions": ["generate a subgraph that returns the original graph ."], "references": ["create the subgraphs representing any if and for statements"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 1484, "code": "def subgraph parse ( self , node , pathnode , extra blocks ) : loose ends = [ ] self . tail = node self . dispatch list ( node . body ) loose ends . append ( self . tail ) for extra in extra blocks : self . tail = node self . dispatch list ( extra . body ) loose ends . append ( self . tail ) if node . orelse : self . tail = node self . dispatch list ( node . orelse ) loose ends . append ( self . tail ) else : loose ends . append ( node ) if node : bottom = \"%s\" % self . bottom counter self . bottom counter += 1 for le in loose ends : self . graph . connect ( le , bottom ) self . tail = bottom", "predictions": ["duplicate duplicate between two nodes ."], "references": ["parse the body and any else block of if and for statements"], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 1485, "code": "def add checker ( self , checker ) : vcids = set ( ) lcids = set ( ) visits = self . visit events leaves = self . leave events for member in dir ( checker ) : cid = member [ 6 : ] if cid == \"default\" : continue if member . startswith ( \"visit \" ) : v meth = getattr ( checker , member ) if self . is method enabled ( v meth ) : visits [ cid ] . append ( v meth ) vcids . add ( cid ) elif member . startswith ( \"leave \" ) : l meth = getattr ( checker , member ) if self . is method enabled ( l meth ) : leaves [ cid ] . append ( l meth ) lcids . add ( cid ) visit default = getattr ( checker , \"visit default\" , None ) if visit default : for cls in nodes . ALL NODE CLASSES : cid = cls . name . lower ( ) if cid not in vcids : visits [ cid ] . append ( visit default )", "predictions": ["add this method to the batch ."], "references": ["walk to the checker s dir and collect visit and leave methods"], "bleu": 0.11967409389919142, "rouge_l": 0.20098846787479407}
{"id": 1486, "code": "def add relationship ( self , from object , to object , relation type , name = None ) : rel = Relationship ( from object , to object , relation type , name ) self . relationships . setdefault ( relation type , [ ] ) . append ( rel )", "predictions": ["add an object to this interpreter ."], "references": ["create a relation ship"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1487, "code": "def get relationship ( self , from object , relation type ) : for rel in self . relationships . get ( relation type , ( ) ) : if rel . from object is from object : return rel raise Key Error ( relation type )", "predictions": ["get the relationship between this object and the relation ."], "references": ["return a relation ship or none"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 1488, "code": "def get attrs ( self , node ) : attrs = [ ] properties = [ ( n , m ) for n , m in node . items ( ) if isinstance ( m , astroid . Function Def ) and decorated with property ( m ) ] for node name , associated nodes in ( list ( node . instance attrs type . items ( ) ) + list ( node . locals type . items ( ) ) + properties ) : if not self . show attr ( node name ) : continue names = self . class names ( associated nodes ) if names : node name = \"%s : %s\" % ( node name , \", \" . join ( names ) ) attrs . append ( node name ) return sorted ( attrs )", "predictions": ["this is called to insert utility functions for each node ."], "references": ["return visible attributes possibly with class name"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1489, "code": "def add object ( self , title , node ) : assert node not in self . nodes ent = Diagram Entity ( title , node ) self . nodes [ node ] = ent self . objects . append ( ent )", "predictions": ["this method adds a node to this entity . a node is added to the supplied title and the existing list ."], "references": ["create a diagram object"], "bleu": 0.05538696232597745, "rouge_l": 0.08789625360230549}
{"id": 1490, "code": "def class names ( self , nodes ) : names = [ ] for node in nodes : if isinstance ( node , astroid . Instance ) : node = node . proxied if ( isinstance ( node , astroid . Class Def ) and hasattr ( node , \"name\" ) and not self . has node ( node ) ) : if node . name not in names : node name = node . name names . append ( node name ) return names", "predictions": ["returns the class with this class ."], "references": ["return class names if needed in diagram"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1491, "code": "def classes ( self ) : return [ o for o in self . objects if isinstance ( o . node , astroid . Class Def ) ]", "predictions": ["returns a list of classes for this class ."], "references": ["return all class nodes in the diagram"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 1492, "code": "def classe ( self , name ) : for klass in self . classes ( ) : if klass . node . name == name : return klass raise Key Error ( name )", "predictions": ["returns a classe class with the given name ."], "references": ["return a class by its name raise keyerror if not found"], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 1493, "code": "def extract relationships ( self ) : for obj in self . classes ( ) : node = obj . node obj . attrs = self . get attrs ( node ) obj . methods = self . get methods ( node ) if is interface ( node ) : obj . shape = \"interface\" else : obj . shape = \"class\" for par node in node . ancestors ( recurs = False ) : try : par obj = self . object from node ( par node ) self . add relationship ( obj , par obj , \"specialization\" ) except Key Error : continue for impl node in node . implements : try : impl obj = self . object from node ( impl node ) self . add relationship ( obj , impl obj , \"implements\" ) except Key Error : continue for name , values in list ( node . instance attrs type . items ( ) ) + list ( node . locals type . items ( ) ) : for value in values : if value is astroid . Uninferable : continue if isinstance ( value , astroid . Instance ) : value = value . proxied try : associated obj = self . object from node ( value ) self . add relationship ( associated obj , obj , \"association\" , name ) except Key Error : continue", "predictions": ["this is called to extract all the objects from the shape map ."], "references": ["extract relation ships between nodes in the diagram"], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 1494, "code": "def modules ( self ) : return [ o for o in self . objects if isinstance ( o . node , astroid . Module ) ]", "predictions": ["call this to return the list of default values for this object ."], "references": ["return all module nodes in the diagram"], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 1495, "code": "def module ( self , name ) : for mod in self . modules ( ) : if mod . node . name == name : return mod raise Key Error ( name )", "predictions": ["get a module with the named name ."], "references": ["return a module by its name raise keyerror if not found"], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 1496, "code": "def add from depend ( self , node , from module ) : mod name = node . root ( ) . name obj = self . module ( mod name ) if from module not in obj . node . depends : obj . node . depends . append ( from module )", "predictions": ["create a new threadsafe node ."], "references": ["add dependencies created by from - imports"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1497, "code": "def extract relationships ( self ) : Class Diagram . extract relationships ( self ) for obj in self . classes ( ) : try : mod = self . object from node ( obj . node . root ( ) ) self . add relationship ( obj , mod , \"ownership\" ) except Key Error : continue for obj in self . modules ( ) : obj . shape = \"package\" for dep name in obj . node . depends : try : dep = self . get module ( dep name , obj . node ) except Key Error : continue self . add relationship ( obj , dep , \"depends\" )", "predictions": ["this is just a method that returns the class hierarchy ."], "references": ["extract relation ships between nodes in the diagram"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1498, "code": "def query ( self ) : if hasattr ( self . model , 'query' ) : return self . model . query else : return self . session . query ( self . model )", "predictions": ["makes a query about this model ."], "references": ["determines which method of getting the query object for use"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 1499, "code": "def prepare request ( uri , headers = None , data = None , method = None ) : if headers is None : headers = { } if data and not method : method = 'POST' elif not method : method = 'GET' if method == 'GET' and data : uri = add params to uri ( uri , data ) data = None return uri , headers , data , method", "predictions": ["make request parameters using data from given uri and method ."], "references": ["make request parameters right ."], "bleu": 0.22416933501922287, "rouge_l": 0.5362637362637362}
{"id": 1500, "code": "def handle oauth1 response ( self , args ) : client = self . make client ( ) client . verifier = args . get ( 'oauth verifier' ) tup = session . get ( '%s oauthtok' % self . name ) if not tup : raise O Auth Exception ( 'Token not found, maybe you disabled cookie' , type = 'token not found' ) client . resource owner key = tup [ 0 ] client . resource owner secret = tup [ 1 ] uri , headers , data = client . sign ( self . expand url ( self . access token url ) , encode ( self . access token method ) ) headers . update ( self . access token headers ) resp , content = self . http request ( uri , headers , to bytes ( data , self . encoding ) , method = self . access token method ) data = parse response ( resp , content ) if resp . code not in ( 200 , 201 ) : raise O Auth Exception ( 'Invalid response from %s' % self . name , type = 'invalid response' , data = data ) return data", "predictions": ["handle authentication and sign an authentication request ."], "references": ["handles an oauth1 authorization response ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1501, "code": "def handle oauth2 response ( self , args ) : client = self . make client ( ) remote args = { 'code' : args . get ( 'code' ) , 'client secret' : self . consumer secret , 'redirect uri' : session . get ( '%s oauthredir' % self . name ) } log . debug ( 'Prepare oauth2 remote args %r' , remote args ) remote args . update ( self . access token params ) headers = copy ( self . access token headers ) if self . access token method == 'POST' : headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) body = client . prepare request body ( * * remote args ) resp , content = self . http request ( self . expand url ( self . access token url ) , headers = headers , data = to bytes ( body , self . encoding ) , method = self . access token method , ) elif self . access token method == 'GET' : qs = client . prepare request body ( * * remote args ) url = self . expand url ( self . access token url ) url += ( '?' in url and '&' or '?' ) + qs resp , content = self . http request ( url , headers = headers , method = self . access token method , ) else : raise O Auth Exception ( 'Unsupported access token method: %s' % self . access token method ) data = parse response ( resp , content , content type = self . content type ) if resp . code not in ( 200 , 201 ) : raise O Auth Exception ( 'Invalid response from %s' % self . name , type = 'invalid response' , data = data ) return data", "predictions": ["handles web service and save it to the client ."], "references": ["handles an oauth2 authorization response ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 1502, "code": "def authorized response ( self , args = None ) : if args is None : args = request . args if 'oauth verifier' in args : data = self . handle oauth1 response ( args ) elif 'code' in args : data = self . handle oauth2 response ( args ) else : data = self . handle unknown response ( ) session . pop ( '%s oauthtok' % self . name , None ) session . pop ( '%s oauthredir' % self . name , None ) return data", "predictions": ["wraps the authorized response ."], "references": ["handles authorization response smartly ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 1503, "code": "def make client with token ( self , token ) : cached clients = getattr ( self , 'clients' , None ) hashed token = hash token ( self , token ) if cached clients and hashed token in cached clients : return cached clients [ hashed token ] client = self . make client ( token ) if cached clients : cached clients [ hashed token ] = client return client", "predictions": ["make the client for the settings file ."], "references": ["uses cached client or create new one with specific token ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 1504, "code": "def confirm authorization request ( self ) : server = self . server uri , http method , body , headers = extract params ( ) try : realms , credentials = server . get realms and credentials ( uri , http method = http method , body = body , headers = headers ) ret = server . create authorization response ( uri , http method , body , headers , realms , credentials ) log . debug ( 'Authorization successful.' ) return create response ( * ret ) except errors . O Auth1Error as e : return redirect ( e . in uri ( self . error uri ) ) except errors . Invalid Client Error as e : return redirect ( e . in uri ( self . error uri ) )", "predictions": ["astroid with specific while showing the connection ."], "references": ["when consumer confirm the authrozation ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1505, "code": "def require oauth ( self , * realms , * * kwargs ) : def wrapper ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : for func in self . before request funcs : func ( ) if hasattr ( request , 'oauth' ) and request . oauth : return f ( * args , * * kwargs ) server = self . server uri , http method , body , headers = extract params ( ) try : valid , req = server . validate protected resource request ( uri , http method , body , headers , realms ) except Exception as e : log . warn ( 'Exception: %r' , e ) e . urlencoded = urlencode ( [ ( 'error' , 'unknown' ) ] ) e . status code = 400 return error response ( e ) for func in self . after request funcs : valid , req = func ( valid , req ) if not valid : return abort ( 401 ) req . user = req . access token . user request . oauth = req return f ( * args , * * kwargs ) return decorated return wrapper", "predictions": ["ensures that the request was valid and does not require that the request must be automatically valid ."], "references": ["protect resource with specified scopes ."], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 1506, "code": "def get default realms ( self , client key , request ) : log . debug ( 'Get realms for %r' , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) client = request . client if hasattr ( client , 'default realms' ) : return client . default realms return [ ]", "predictions": ["convenience method for returning information about the expr ."], "references": ["default realms of the client ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1507, "code": "def get realms ( self , token , request ) : log . debug ( 'Get realms of %r' , token ) tok = request . request token or self . grantgetter ( token = token ) if not tok : return [ ] request . request token = tok if hasattr ( tok , 'realms' ) : return tok . realms or [ ] return [ ]", "predictions": ["retrieves the lambda method for the ) ) and stores it in the ."], "references": ["realms for this request token ."], "bleu": 0.09782375748961449, "rouge_l": 0.21554770318021202}
{"id": 1508, "code": "def get redirect uri ( self , token , request ) : log . debug ( 'Get redirect uri of %r' , token ) tok = request . request token or self . grantgetter ( token = token ) return tok . redirect uri", "predictions": ["assert that the user is currently authorized to assert that it is always assert that the ) is assert ."], "references": ["redirect uri for this request token ."], "bleu": 0.06108557268562171, "rouge_l": 0.08111702127659574}
{"id": 1509, "code": "def get rsa key ( self , client key , request ) : if not request . client : request . client = self . clientgetter ( client key = client key ) if hasattr ( request . client , 'rsa key' ) : return request . client . rsa key return None", "predictions": ["get : write : a ( ( ( ( ( ( ( ( ( ( or a keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys keys"], "references": ["retrieves a previously stored client provided rsa key ."], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 1510, "code": "def validate client key ( self , client key , request ) : log . debug ( 'Validate client key for %r' , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) if request . client : return True return False", "predictions": ["check for reversed reversed reversed reversed reversed reversed reversed ( see if they are already in the database : http : / / www . com / wiki / ( / ( / ( / ( / ( / ( - . - . - . - . - ."], "references": ["validates that supplied client key ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 1511, "code": "def validate request token ( self , client key , token , request ) : log . debug ( 'Validate request token %r for %r' , token , client key ) tok = request . request token or self . grantgetter ( token = token ) if tok and tok . client key == client key : request . request token = tok return True return False", "predictions": ["visit the ( : - and / - ( ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["validates request token is available for client ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 1512, "code": "def validate access token ( self , client key , token , request ) : log . debug ( 'Validate access token %r for %r' , token , client key ) tok = request . access token or self . tokengetter ( client key = client key , token = token , ) if tok : request . access token = tok return True return False", "predictions": ["check name and ( ( ( for key , ( ( ( ( ( ( ( ( ( ( ( ( ( session , type , type , type , type , type , type , type interfaces interfaces interfaces interfaces interfaces interfaces interfaces interfaces interfaces interfaces interfaces interfaces interfaces"], "references": ["validates access token is available for client ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 1513, "code": "def validate timestamp and nonce ( self , client key , timestamp , nonce , request , request token = None , access token = None ) : log . debug ( 'Validate timestamp and nonce %r' , client key ) nonce exists = self . noncegetter ( client key = client key , timestamp = timestamp , nonce = nonce , request token = request token , access token = access token ) if nonce exists : return False self . noncesetter ( client key = client key , timestamp = timestamp , nonce = nonce , request token = request token , access token = access token ) return True", "predictions": ["check if the docstring already exists and is valid ."], "references": ["validate the timestamp and nonce is used or not ."], "bleu": 0.15851165692617156, "rouge_l": 0.4}
{"id": 1514, "code": "def validate redirect uri ( self , client key , redirect uri , request ) : log . debug ( 'Validate redirect uri %r for %r' , redirect uri , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) if not request . client : return False if not request . client . redirect uris and redirect uri is None : return True request . redirect uri = redirect uri return redirect uri in request . client . redirect uris", "predictions": ["check if this call is valid and literal ."], "references": ["validate if the redirect_uri is allowed by the client ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 1515, "code": "def validate realms ( self , client key , token , request , uri = None , realms = None ) : log . debug ( 'Validate realms %r for %r' , realms , client key ) if request . access token : tok = request . access token else : tok = self . tokengetter ( client key = client key , token = token ) request . access token = tok if not tok : return False return set ( tok . realms ) . issuperset ( set ( realms ) )", "predictions": ["subgraph that is terminated by the client ."], "references": ["check if the token has permission on those realms ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 1516, "code": "def validate verifier ( self , client key , token , verifier , request ) : log . debug ( 'Validate verifier %r for %r' , verifier , client key ) data = self . verifiergetter ( verifier = verifier , token = token ) if not data : return False if not hasattr ( data , 'user' ) : log . debug ( 'Verifier should has user attribute' ) return False request . user = data . user if hasattr ( data , 'client key' ) : return data . client key == client key return True", "predictions": ["check for the given key . if the blocks are not already registered , the key will be thrown ."], "references": ["validate verifier exists ."], "bleu": 0.06108557268562171, "rouge_l": 0.09472049689440994}
{"id": 1517, "code": "def verify request token ( self , token , request ) : log . debug ( 'Verify request token %r' , token ) tok = request . request token or self . grantgetter ( token = token ) if tok : request . request token = tok return True return False", "predictions": ["add an authentication ( session : . , . , . : . : . : . . . : . : . . . . . . . . . . . . . . . . . . . self . . self . . self . ."], "references": ["verify if the request token is existed ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 1518, "code": "def verify realms ( self , token , realms , request ) : log . debug ( 'Verify realms %r' , realms ) tok = request . request token or self . grantgetter ( token = token ) if not tok : return False request . request token = tok if not hasattr ( tok , 'realms' ) : return True return set ( tok . realms ) == set ( realms )", "predictions": ["add an authentication from an http request ."], "references": ["verify if the realms match the requested realms ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1519, "code": "def confirm authorization request ( self ) : server = self . server scope = request . values . get ( 'scope' ) or '' scopes = scope . split ( ) credentials = dict ( client id = request . values . get ( 'client id' ) , redirect uri = request . values . get ( 'redirect uri' , None ) , response type = request . values . get ( 'response type' , None ) , state = request . values . get ( 'state' , None ) ) log . debug ( 'Fetched credentials from request %r.' , credentials ) redirect uri = credentials . get ( 'redirect uri' ) log . debug ( 'Found redirect uri %s.' , redirect uri ) uri , http method , body , headers = extract params ( ) try : ret = server . create authorization response ( uri , http method , body , headers , scopes , credentials ) log . debug ( 'Authorization successful.' ) return create response ( * ret ) except oauth2 . Fatal Client Error as e : log . debug ( 'Fatal client error %r' , e , exc info = True ) return self . on exception ( e , e . in uri ( self . error uri ) ) except oauth2 . O Auth2Error as e : log . debug ( 'O Auth2Error: %r' , e , exc info = True ) state = request . values . get ( 'state' ) if state and not e . state : e . state = state return self . on exception ( e , e . in uri ( redirect uri or self . error uri ) ) except Exception as e : log . exception ( e ) return self . on exception ( e , add params to uri ( self . error uri , { 'error' : str ( e ) } ) )", "predictions": ["get an cleanup to get or create a ( ( i . e . ( from relation from relation from relation from client from relation from the database from the database from the database from the database from requests from client from the relation from the database from the database"], "references": ["when consumer confirm the authorization ."], "bleu": 0.026594139297659906, "rouge_l": 0.04160982264665757}
{"id": 1520, "code": "def require oauth ( self , * scopes ) : def wrapper ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : for func in self . before request funcs : func ( ) if hasattr ( request , 'oauth' ) and request . oauth : return f ( * args , * * kwargs ) valid , req = self . verify request ( scopes ) for func in self . after request funcs : valid , req = func ( valid , req ) if not valid : if self . invalid response : return self . invalid response ( req ) return abort ( 401 ) request . oauth = req return f ( * args , * * kwargs ) return decorated return wrapper", "predictions": ["decorator to get the attrs way ."], "references": ["protect resource with specified scopes ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1521, "code": "def get default redirect uri ( self , client id , request , * args , * * kwargs ) : request . client = request . client or self . clientgetter ( client id ) redirect uri = request . client . default redirect uri log . debug ( 'Found default redirect uri %r' , redirect uri ) return redirect uri", "predictions": ["add or remove object for get requests ."], "references": ["default redirect_uri for the given client ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1522, "code": "def get default scopes ( self , client id , request , * args , * * kwargs ) : request . client = request . client or self . clientgetter ( client id ) scopes = request . client . default scopes log . debug ( 'Found default scopes %r' , scopes ) return scopes", "predictions": ["builds and returns the names of a = ) = accept the names in the = ) ."], "references": ["default scopes for the given client ."], "bleu": 0.07535838128770536, "rouge_l": 0.17378917378917377}
{"id": 1523, "code": "def save authorization code ( self , client id , code , request , * args , * * kwargs ) : log . debug ( 'Persist authorization code %r for client %r' , code , client id ) request . client = request . client or self . clientgetter ( client id ) self . grantsetter ( client id , code , request , * args , * * kwargs ) return request . client . default redirect uri", "predictions": ["saves an ( . , . if the for the given [ filename ] if the for the for the for the for the for the for the for the for the given self . note : this method can only be called once the for the database has been"], "references": ["persist the authorization code ."], "bleu": 0.026594139297659906, "rouge_l": 0.08531468531468532}
{"id": 1524, "code": "def save bearer token ( self , token , request , * args , * * kwargs ) : log . debug ( 'Save bearer token %r' , token ) self . tokensetter ( token , request , * args , * * kwargs ) return request . client . default redirect uri", "predictions": ["classe classe classe to classe using the ( method ."], "references": ["persist the bearer token ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 1525, "code": "def validate client id ( self , client id , request , * args , * * kwargs ) : log . debug ( 'Validate client %r' , client id ) client = request . client or self . clientgetter ( client id ) if client : request . client = client return True return False", "predictions": ["extract relationships from the given ( ( relationships classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes classes ."], "references": ["ensure client_id belong to a valid and active client ."], "bleu": 0.027347130611442165, "rouge_l": 0.041780821917808214}
{"id": 1526, "code": "def validate code ( self , client id , code , client , request , * args , * * kwargs ) : client = client or self . clientgetter ( client id ) log . debug ( 'Validate code for client %r and code %r' , client . client id , code ) grant = self . grantgetter ( client id = client . client id , code = code ) if not grant : log . debug ( 'Grant not found.' ) return False if hasattr ( grant , 'expires' ) and datetime . datetime . utcnow ( ) > grant . expires : log . debug ( 'Grant is expired.' ) return False request . state = kwargs . get ( 'state' ) request . user = grant . user request . scopes = grant . scopes return True", "predictions": ["validates the ( return objects objects objects objects objects objects objects objects objects objects ."], "references": ["ensure the grant code is valid ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 1527, "code": "def validate scopes ( self , client id , scopes , client , request , * args , * * kwargs ) : if hasattr ( client , 'validate scopes' ) : return client . validate scopes ( scopes ) return set ( client . default scopes ) . issuperset ( set ( scopes ) )", "predictions": ["validates a single ) ."], "references": ["ensure the client is authorized access to requested scopes ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 1528, "code": "def revoke token ( self , token , token type hint , request , * args , * * kwargs ) : if token type hint : tok = self . tokengetter ( * * { token type hint : token } ) else : tok = self . tokengetter ( access token = token ) if not tok : tok = self . tokengetter ( refresh token = token ) if tok : request . client id = tok . client id request . user = tok . user tok . delete ( ) return True msg = 'Invalid token supplied.' log . debug ( msg ) request . error message = msg return False", "predictions": ["flushes an access from a user ."], "references": ["revoke an access or refresh token ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 1529, "code": "def update qq api request data ( data = { } ) : defaults = { 'openid' : session . get ( 'qq openid' ) , 'access token' : session . get ( 'qq token' ) [ 0 ] , 'oauth consumer key' : QQ APP ID , } defaults . update ( data ) return defaults", "predictions": ["extract operation , does not include all dashboard - name and consumer"], "references": ["update some required parameters for oauth2 . 0 api calls"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 1530, "code": "def convert keys to string ( dictionary ) : if not isinstance ( dictionary , dict ) : return dictionary return dict ( ( str ( k ) , convert keys to string ( v ) ) for k , v in dictionary . items ( ) )", "predictions": ["returns a string representation of the if the if the if the if the if it is a if it is a if it is a if it is a if the if it is a if it is a if it is a if it is a if it"], "references": ["recursively converts dictionary keys to strings ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1531, "code": "def register to ( self , oauth , name = None , * * kwargs ) : kwargs = self . process kwargs ( name = ( name or self . default name ) , * * kwargs ) return oauth . remote app ( * * kwargs )", "predictions": ["registers the specific call to the class ."], "references": ["creates a remote app and registers it ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1532, "code": "def create ( self , oauth , * * kwargs ) : kwargs = self . process kwargs ( name = self . default name , register = False , * * kwargs ) return oauth . remote app ( * * kwargs )", "predictions": ["a convenience method for the ."], "references": ["creates a remote app only ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1533, "code": "def extract params ( ) : uri = get uri from request ( request ) http method = request . method headers = dict ( request . headers ) if 'wsgi.input' in headers : del headers [ 'wsgi.input' ] if 'wsgi.errors' in headers : del headers [ 'wsgi.errors' ] if request . authorization : headers [ 'Authorization' ] = request . authorization body = request . form . to dict ( ) return uri , http method , body , headers", "predictions": ["handle a client request ."], "references": ["extract request params ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1534, "code": "def to bytes ( text , encoding = 'utf-8' ) : if not text : return text if not isinstance ( text , bytes type ) : text = text . encode ( encoding ) return text", "predictions": ["convert a chunk of response self to a printable string . if the internal self - 8 is not already present , it will be returned ."], "references": ["make sure text is bytes type ."], "bleu": 0.04970745472800838, "rouge_l": 0.13160733549083062}
{"id": 1535, "code": "def decode base64 ( text , encoding = 'utf-8' ) : text = to bytes ( text , encoding ) return to unicode ( base64 . b64decode ( text ) , encoding )", "predictions": ["decodes the given base64 string into the corresponding ( ."], "references": ["decode base64 string ."], "bleu": 0.17827531042796255, "rouge_l": 0.4644670050761421}
{"id": 1536, "code": "def create response ( headers , body , status ) : response = Response ( body or '' ) for k , v in headers . items ( ) : response . headers [ str ( k ) ] = v response . status code = status return response", "predictions": ["create a requests response ."], "references": ["create response class for flask ."], "bleu": 0.2658156069371863, "rouge_l": 0.5366568914956013}
{"id": 1537, "code": "def get cached clients ( ) : if O Auth . state key not in current app . extensions : raise Runtime Error ( '%r is not initialized.' % current app ) state = current app . extensions [ O Auth . state key ] return state . cached clients", "predictions": ["gets the cached clients ."], "references": ["gets the cached clients dictionary in current context ."], "bleu": 0.3573381727496468, "rouge_l": 0.6792873051224945}
{"id": 1538, "code": "def check exception ( self ) : for i in xrange ( self . iterations ) : cert = X509 ( ) try : cert . get pubkey ( ) except Error : pass", "predictions": ["verifies the field has operation ."], "references": ["call the method repeatedly such that it will raise an exception ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 1539, "code": "def check success ( self ) : small = xrange ( 3 ) for i in xrange ( self . iterations ) : key = P Key ( ) key . generate key ( TYPE DSA , 256 ) for i in small : cert = X509 ( ) cert . set pubkey ( key ) for i in small : cert . get pubkey ( )", "predictions": ["check a small 6 key ."], "references": ["call the method repeatedly such that it will return a pkey object ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 1540, "code": "def check load privatekey callback ( self ) : for i in xrange ( self . iterations * 10 ) : load privatekey ( FILETYPE PEM , self . ENCRYPTED PEM , lambda * args : \"hello, secret\" )", "predictions": ["recursively check the arguments ."], "references": ["call the function with an encrypted pem and a passphrase callback ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 1541, "code": "def bio to string ( bio ) : result buffer = ffi . new ( 'char**' ) buffer length = lib . BIO get mem data ( bio , result buffer ) return ffi . buffer ( result buffer [ 0 ] , buffer length ) [ : ]", "predictions": ["convert a bio to a string ."], "references": ["copy the contents of an openssl bio object into a python byte string ."], "bleu": 0.10218289380194193, "rouge_l": 0.35935198821796754}
{"id": 1542, "code": "def print token factory ( col ) : def helper ( msg ) : style = style from dict ( { Token . Color : col , } ) tokens = [ ( Token . Color , msg ) ] print tokens ( tokens , style = style ) def helper no terminal ( msg ) : print ( msg ) if sys . stdout . isatty ( ) : return helper else : return helper no terminal", "predictions": ["prints a get_init password ."], "references": ["internal helper to provide color names ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1543, "code": "def get service metadata ( self ) : return { 'import labels as tags' : self . config . get ( 'import labels as tags' , False , asbool ) , 'label template' : self . config . get ( 'label template' , DEFAULT LABEL TEMPLATE ) , }", "predictions": ["returns the metadata for this object ."], "references": ["return extra config options to be passed to the trelloissue class"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 1544, "code": "def issues ( self ) : for board in self . get boards ( ) : for lst in self . get lists ( board [ 'id' ] ) : listextra = dict ( boardname = board [ 'name' ] , listname = lst [ 'name' ] ) for card in self . get cards ( lst [ 'id' ] ) : issue = self . get issue for record ( card , extra = listextra ) issue . update extra ( { \"annotations\" : self . annotations ( card ) } ) yield issue", "predictions": ["yield each card that indicates this issues is a method that can be called to yield each card ."], "references": ["returns a list of dicts representing issues from a remote service ."], "bleu": 0.07658412276041004, "rouge_l": 0.20176405733186328}
{"id": 1545, "code": "def get comments ( self , card id ) : params = { 'filter' : 'comment Card' , 'member Creator fields' : 'username' } comments = self . api request ( \"/1/cards/{card id}/actions\" . format ( card id = card id ) , * * params ) for comment in comments : assert comment [ 'type' ] == 'comment Card' yield comment", "predictions": ["generate a generator that yields comments for the given card ."], "references": ["returns an iterator for the comments on a certain card ."], "bleu": 0.20504572236241866, "rouge_l": 0.36363636363636365}
{"id": 1546, "code": "def get issues ( self , repo , keys ) : key1 , key2 = keys key3 = key1 [ : - 1 ] url = self . base url + \"/api/0/\" + repo + \"/\" + key1 response = self . session . get ( url , params = dict ( status = 'Open' ) ) if not bool ( response ) : error = response . json ( ) code = error [ 'error code' ] if code == 'ETRACKERDISABLED' : return [ ] else : raise IO Error ( 'Failed to talk to %r %r' % ( url , error ) ) issues = [ ] for result in response . json ( ) [ key2 ] : idx = six . text type ( result [ 'id' ] ) result [ 'html url' ] = \"/\" . join ( [ self . base url , repo , key3 , idx ] ) issues . append ( ( repo , result ) ) return issues", "predictions": ["get issues with issues ."], "references": ["grab all the issues"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1547, "code": "def api url ( self , path , * * context ) : if self . host == 'github.com' : baseurl = \"https://api.github.com\" else : baseurl = \"https://{}/api/v3\" . format ( self . host ) return baseurl + path . format ( * * context )", "predictions": ["get the full url for an admin api ."], "references": ["build the full url to the api endpoint"], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 1548, "code": "def getter ( self , url , subkey = None ) : kwargs = { } if 'basic' in self . auth : kwargs [ 'auth' ] = self . auth [ 'basic' ] results = [ ] link = dict ( next = url ) while 'next' in link : response = self . session . get ( link [ 'next' ] , * * kwargs ) if response . status code == 404 and 'token' in self . auth : log . warn ( \"A '404' from github may indicate an auth \" \"failure. Make sure both that your token is correct \" \"and that it has 'public repo' and not 'public \" \"access' rights.\" ) json res = self . json response ( response ) if subkey is not None : json res = json res [ subkey ] results += json res link = self . link field to dict ( response . headers . get ( 'link' , None ) ) return results", "predictions": ["getter for getter and link requests ."], "references": ["pagination utility . obnoxious ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1549, "code": "def get owned repo issues ( self , tag ) : issues = { } for issue in self . client . get issues ( * tag . split ( '/' ) ) : issues [ issue [ 'url' ] ] = ( tag , issue ) return issues", "predictions": ["determine virtual issues with given tag ."], "references": ["grab all the issues"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1550, "code": "def get query ( self , query ) : issues = { } for issue in self . client . get query ( query ) : url = issue [ 'html url' ] try : repo = self . get repository from issue ( issue ) except Value Error as e : log . critical ( e ) else : issues [ url ] = ( repo , issue ) return issues", "predictions": ["get issues with specified query ."], "references": ["grab all issues matching a github query"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1551, "code": "def reqs ( self , tag ) : return [ ( tag , i ) for i in self . client . get pulls ( * tag . split ( '/' ) ) ]", "predictions": ["recursively closes this period ."], "references": ["grab all the pull requests"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1552, "code": "def aggregate issues ( conf , main section , debug ) : log . info ( \"Starting to aggregate remote issues.\" ) targets = aslist ( conf . get ( main section , 'targets' ) ) queue = multiprocessing . Queue ( ) log . info ( \"Spawning %i workers.\" % len ( targets ) ) processes = [ ] if debug : for target in targets : aggregate issues ( conf , main section , target , queue , conf . get ( target , 'service' ) ) else : for target in targets : proc = multiprocessing . Process ( target = aggregate issues , args = ( conf , main section , target , queue , conf . get ( target , 'service' ) ) ) proc . start ( ) processes . append ( proc ) time . sleep ( 1 ) currently running = len ( targets ) while currently running > 0 : issue = queue . get ( True ) if isinstance ( issue , tuple ) : completion type , args = issue if completion type == SERVICE FINISHED ERROR : target , e = args log . info ( \"Terminating workers\" ) for process in processes : process . terminate ( ) raise Runtime Error ( \"critical error in target '{}'\" . format ( target ) ) currently running -= 1 continue yield issue log . info ( \"Done aggregating remote issues.\" )", "predictions": ["aggregate all issues for a remote issue and yield them ."], "references": ["return all issues from every target ."], "bleu": 0.16108992769687397, "rouge_l": 0.3472485768500949}
{"id": 1553, "code": "def get config or default ( self , key , default , as type = lambda x : x ) : if self . main config . has option ( self . main section , key ) : return as type ( self . main config . get ( self . main section , key ) ) return default", "predictions": ["get information about all currently registered config objects ."], "references": ["return a main config value or default if it does not exist ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 1554, "code": "def validate config ( cls , service config , target ) : if service config . has option ( target , 'only if assigned' ) : die ( \"[%s] has an 'only if assigned' option.  Should be \" \"'%s.only if assigned'.\" % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'also unassigned' ) : die ( \"[%s] has an 'also unassigned' option.  Should be \" \"'%s.also unassigned'.\" % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'default priority' ) : die ( \"[%s] has a 'default priority' option.  Should be \" \"'%s.default priority'.\" % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'add tags' ) : die ( \"[%s] has an 'add tags' option.  Should be \" \"'%s.add tags'.\" % ( target , cls . CONFIG PREFIX ) )", "predictions": ["validates config and target arguments ."], "references": ["validate generic options for a particular target"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1555, "code": "def include ( self , issue ) : only if assigned = self . config . get ( 'only if assigned' , None ) if only if assigned : owner = self . get owner ( issue ) include owners = [ only if assigned ] if self . config . get ( 'also unassigned' , None , asbool ) : include owners . append ( None ) return owner in include owners only if author = self . config . get ( 'only if author' , None ) if only if author : return self . get author ( issue ) == only if author return True", "predictions": ["include the name for this issue but will only tell if the issue is currently include ."], "references": ["return true if the issue in question should be included"], "bleu": 0.13445273575332958, "rouge_l": 0.2331210191082802}
{"id": 1556, "code": "def oracle eval ( command ) : p = subprocess . Popen ( command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) p . wait ( ) if p . returncode == 0 : return p . stdout . readline ( ) . strip ( ) . decode ( 'utf-8' ) else : die ( \"Error retrieving password: `{command}` returned '{error}'\" . format ( command = command , error = p . stderr . read ( ) . strip ( ) ) )", "predictions": ["runs a script ."], "references": ["retrieve password from the given command"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 1557, "code": "def getint ( self , section , option ) : try : return super ( Bugwarrior Config Parser , self ) . getint ( section , option ) except Value Error : if self . get ( section , option ) == u'' : return None else : raise Value Error ( \"{section}.{option} must be an integer or empty.\" . format ( section = section , option = option ) )", "predictions": ["getint a previously registered integer ."], "references": ["accepts both integers and empty values ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1558, "code": "def get data ( self , url ) : return self . json response ( requests . get ( url , * * self . requests kwargs ) )", "predictions": ["get the data for the connection ."], "references": ["perform a request to the fully qualified url and return json ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 1559, "code": "def hamdist ( str1 , str2 ) : diffs = 0 for ch1 , ch2 in zip ( str1 , str2 ) : if ch1 != ch2 : diffs += 1 return diffs", "predictions": ["get the hamdist of all ( ."], "references": ["count the # of differences between equal length strings str1 and str2"], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 1560, "code": "def fdrcorrection ( pvals , alpha = 0.05 ) : pvals = np . asarray ( pvals ) pvals sortind = np . argsort ( pvals ) pvals sorted = np . take ( pvals , pvals sortind ) ecdffactor = ecdf ( pvals sorted ) reject = pvals sorted <= ecdffactor * alpha if reject . any ( ) : rejectmax = max ( np . nonzero ( reject ) [ 0 ] ) reject [ : rejectmax ] = True pvals corrected raw = pvals sorted / ecdffactor pvals corrected = np . minimum . accumulate ( pvals corrected raw [ : : - 1 ] ) [ : : - 1 ] del pvals corrected raw pvals corrected [ pvals corrected > 1 ] = 1 pvals corrected = np . empty like ( pvals corrected ) pvals corrected [ pvals sortind ] = pvals corrected del pvals corrected reject = np . empty like ( reject ) reject [ pvals sortind ] = reject return reject , pvals corrected", "predictions": ["reorders the terms in the analysis control ."], "references": ["benjamini hocheberg fdr correction . inspired by statsmodels"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1561, "code": "def prepare argparser ( ) : description = \"%(prog)s -- Gene Set Enrichment Analysis in Python\" epilog = \"For command line options of each command, type: %(prog)s COMMAND -h\" argparser = ap . Argument Parser ( description = description , epilog = epilog ) argparser . add argument ( \"--version\" , action = \"version\" , version = \"%(prog)s \" + version ) subparsers = argparser . add subparsers ( dest = 'subcommand name' ) #help=\"sub-command help\") add gsea parser ( subparsers ) add prerank parser ( subparsers ) add singlesample parser ( subparsers ) add plot parser ( subparsers ) add enrichr parser ( subparsers ) add biomart parser ( subparsers ) return argparser", "predictions": ["prepares and adds parser for ( ."], "references": ["prepare argparser object . new options will be added in this function first ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 1562, "code": "def add gsea parser ( subparsers ) : argparser gsea = subparsers . add parser ( \"gsea\" , help = \"Main GSE Apy Function: run GSE Apy instead of GSEA.\" ) group input = argparser gsea . add argument group ( \"Input files arguments\" ) group input . add argument ( \"-d\" , \"--data\" , dest = \"data\" , action = \"store\" , type = str , required = True , help = \"Input gene expression dataset file in txt format.Same with GSEA.\" ) group input . add argument ( \"-c\" , \"--cls\" , dest = \"cls\" , action = \"store\" , type = str , required = True , help = \"Input class vector (phenotype) file in CLS format. Same with GSEA.\" ) group input . add argument ( \"-g\" , \"--gmt\" , dest = \"gmt\" , action = \"store\" , type = str , required = True , help = \"Gene set database in GMT format. Same with GSEA.\" ) group input . add argument ( \"-t\" , \"--permu-type\" , action = \"store\" , dest = \"type\" , type = str , metavar = 'per Type' , choices = ( \"gene set\" , \"phenotype\" ) , default = \"gene set\" , help = \"Permutation type. Same with GSEA, choose from {'gene set', 'phenotype'}\" ) group output = argparser gsea . add argument group ( \"Output arguments\" ) add output option ( group output ) group opt = argparser gsea . add argument group ( \"GSEA advanced arguments\" ) group opt . add argument ( \"-n\" , \"--permu-num\" , dest = \"n\" , action = \"store\" , type = int , default = 1000 , metavar = 'nperm' , help = \"Number of random permutations. For calculating esnulls. Default: 1000\" ) group opt . add argument ( \"--min-size\" , dest = \"mins\" , action = \"store\" , type = int , default = 15 , metavar = 'int' , help = \"Min size of input genes presented in Gene Sets. Default: 15\" ) group opt . add argument ( \"--max-size\" , dest = \"maxs\" , action = \"store\" , type = int , default = 500 , metavar = 'int' , help = \"Max size of input genes presented in Gene Sets. Default: 500\" ) group opt . add argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) group opt . add argument ( \"-m\" , \"--method\" , action = \"store\" , dest = \"method\" , type = str , metavar = '' , choices = ( \"signal to noise\" , \"t test\" , \"ratio of classes\" , \"diff of classes\" , \"log2 ratio of classes\" ) , default = \"log2 ratio of classes\" , help = ) group opt . add argument ( \"-a\" , \"--ascending\" , action = 'store true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) group opt . add argument ( \"-s\" , \"--seed\" , dest = \"seed\" , action = \"store\" , type = int , default = None , metavar = '' , help = \"Number of random seed. Default: None\" ) group opt . add argument ( \"-p\" , \"--threads\" , dest = \"threads\" , action = \"store\" , type = int , default = 1 , metavar = 'procs' , help = \"Number of Processes you are going to use. Default: 1\" ) return", "predictions": ["add a new 0/x entity to ( ."], "references": ["add main function gsea argument parsers ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1563, "code": "def add prerank parser ( subparsers ) : argparser prerank = subparsers . add parser ( \"prerank\" , help = \"Run GSE Apy Prerank tool on preranked gene list.\" ) prerank input = argparser prerank . add argument group ( \"Input files arguments\" ) prerank input . add argument ( \"-r\" , \"--rnk\" , dest = \"rnk\" , action = \"store\" , type = str , required = True , help = \"Ranking metric file in .rnk format. Same with GSEA.\" ) prerank input . add argument ( \"-g\" , \"--gmt\" , dest = \"gmt\" , action = \"store\" , type = str , required = True , help = \"Gene set database in GMT format. Same with GSEA.\" ) prerank input . add argument ( \"-l\" , \"--label\" , action = 'store' , nargs = 2 , dest = 'label' , metavar = ( 'pos' , 'neg' ) , type = str , default = ( 'Pos' , 'Neg' ) , help = \"The phenotype label argument need two parameters to define. Default: ('Pos','Neg')\" ) prerank output = argparser prerank . add argument group ( \"Output arguments\" ) add output option ( prerank output ) prerank opt = argparser prerank . add argument group ( \"GSEA advanced arguments\" ) prerank opt . add argument ( \"-n\" , \"--permu-num\" , dest = \"n\" , action = \"store\" , type = int , default = 1000 , metavar = 'nperm' , help = \"Number of random permutations. For calculating esnulls. Default: 1000\" ) prerank opt . add argument ( \"--min-size\" , dest = \"mins\" , action = \"store\" , type = int , default = 15 , metavar = 'int' , help = \"Min size of input genes presented in Gene Sets. Default: 15\" ) prerank opt . add argument ( \"--max-size\" , dest = \"maxs\" , action = \"store\" , type = int , default = 500 , metavar = 'int' , help = \"Max size of input genes presented in Gene Sets. Default: 500\" ) prerank opt . add argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) prerank opt . add argument ( \"-a\" , \"--ascending\" , action = 'store true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) prerank opt . add argument ( \"-s\" , \"--seed\" , dest = \"seed\" , action = \"store\" , type = int , default = None , metavar = '' , help = \"Number of random seed. Default: None\" ) prerank opt . add argument ( \"-p\" , \"--threads\" , dest = \"threads\" , action = \"store\" , type = int , default = 1 , metavar = 'procs' , help = \"Number of Processes you are going to use. Default: 1\" ) return", "predictions": ["add a new cartesian parser to ( ."], "references": ["add function prerank argument parsers ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1564, "code": "def add plot parser ( subparsers ) : argparser replot = subparsers . add parser ( \"replot\" , help = \"Reproduce GSEA desktop output figures.\" ) group replot = argparser replot . add argument group ( \"Input arguments\" ) group replot . add argument ( \"-i\" , \"--indir\" , action = \"store\" , dest = \"indir\" , required = True , metavar = 'GSEA dir' , help = \"The GSEA desktop results directroy that you want to reproduce the figure \" ) add output option ( group replot ) #add output group( argparser plot ) group replot . add argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. Please Use the same value in GSEA. Choose from (0, 1, 1.5, 2),default: 1' , ) return", "predictions": ["add a plot to ( ."], "references": ["add function plot argument parsers ."], "bleu": 0.2626909894424158, "rouge_l": 0.5}
{"id": 1565, "code": "def add enrichr parser ( subparsers ) : argparser enrichr = subparsers . add parser ( \"enrichr\" , help = \"Using Enrichr API to perform GO analysis.\" ) enrichr opt = argparser enrichr . add argument group ( \"Input arguments\" ) enrichr opt . add argument ( \"-i\" , \"--input-list\" , action = \"store\" , dest = \"gene list\" , type = str , required = True , metavar = 'I Ds' , help = \"Enrichr uses a list of gene names as input.\" ) enrichr opt . add argument ( \"-g\" , \"--gene-sets\" , action = \"store\" , dest = \"library\" , type = str , required = True , metavar = 'GMT' , help = \"Enrichr library name(s) required. Separate each name by comma.\" ) enrichr opt . add argument ( \"--org\" , \"--organism\" , action = \"store\" , dest = \"organism\" , type = str , default = '' , help = \"Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/mod Enrichr.\" ) enrichr opt . add argument ( \"--ds\" , \"--description\" , action = \"store\" , dest = \"descrip\" , type = str , default = 'enrichr' , metavar = 'STRING' , help = ) enrichr opt . add argument ( \"--cut\" , \"--cut-off\" , action = \"store\" , dest = \"thresh\" , metavar = 'float' , type = float , default = 0.05 , help = \"Adjust-Pval cutoff, used for generating plots. Default: 0.05.\" ) enrichr opt . add argument ( \"--bg\" , \"--background\" , action = \"store\" , dest = \"bg\" , default = 'hsapiens gene ensembl' , metavar = 'BGNUM' , help = \"Bio Mart Dataset name or Background total genes number. Default: None\" ) enrichr opt . add argument ( \"-t\" , \"--top-term\" , dest = \"term\" , action = \"store\" , type = int , default = 10 , metavar = 'int' , help = \"Numbers of top terms shown in the plot. Default: 10\" ) enrichr output = argparser enrichr . add argument group ( \"Output figure arguments\" ) add output option ( enrichr output ) return", "predictions": ["add a new 0/x entity to the help page ."], "references": ["add function enrichr argument parsers ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 1566, "code": "def add biomart parser ( subparsers ) : argparser biomart = subparsers . add parser ( \"biomart\" , help = \"Using Bio Mart API to convert gene ids.\" ) biomart opt = argparser biomart . add argument group ( \"Input arguments\" ) biomart opt . add argument ( \"-f\" , \"--filter\" , action = 'store' , nargs = 2 , dest = 'filter' , required = True , metavar = ( 'NAME' , 'VALUE' ) , help = ) biomart opt . add argument ( \"-a\" , \"--attributes\" , action = \"store\" , dest = \"attrs\" , type = str , required = True , metavar = 'ATTR' , help = \"Which attribute(s) to retrieve. Separate each attr by comma.\" ) biomart opt . add argument ( \"-o\" , \"--ofile\" , dest = \"ofile\" , type = str , required = True , help = \"Output file name\" ) biomart opt . add argument ( \"-d\" , \"--dataset\" , action = \"store\" , dest = \"bg\" , type = str , default = 'hsapiens gene ensembl' , metavar = 'DATA' , help = \"Which dataset to use. Default: hsapiens gene ensembl\" ) biomart opt . add argument ( \"--host\" , action = \"store\" , dest = \"host\" , type = str , default = 'www.ensembl.org' , metavar = 'HOST' , help = \"Which host to use. Select from {'www.ensembl.org', 'asia.ensembl.org', 'useast.ensembl.org'}.\" ) biomart opt . add argument ( \"-m\" , \"--mart\" , action = \"store\" , dest = \"mart\" , type = str , metavar = 'MART' , default = \"ENSEMBL MART ENSEMBL\" , help = \"Which mart to use. Default: ENSEMBL MART ENSEMBL.\" ) biomart opt . add argument ( \"-v\" , \"--verbose\" , action = \"store true\" , default = False , dest = 'verbose' , help = \"Increase output verbosity, print out progress of your job\" , )", "predictions": ["add a biomart parser to ( ."], "references": ["add function biomart argument parsers ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 1567, "code": "def get marts ( self ) : mart names = pd . Series ( self . names , name = \"Name\" ) mart descriptions = pd . Series ( self . display Names , name = \"Description\" ) return pd . concat ( [ mart names , mart descriptions ] , axis = 1 )", "predictions": ["get here for this instance ."], "references": ["get available marts and their names ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1568, "code": "def get datasets ( self , mart = 'ENSEMBL MART ENSEMBL' ) : datasets = self . datasets ( mart , raw = True ) return pd . read csv ( String IO ( datasets ) , header = None , usecols = [ 1 , 2 ] , names = [ \"Name\" , \"Description\" ] , sep = \"\\t\" )", "predictions": ["for each items in the items ."], "references": ["get available datasets from mart you ve selected"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 1569, "code": "def get attributes ( self , dataset ) : attributes = self . attributes ( dataset ) attr = [ ( k , v [ 0 ] ) for k , v in attributes . items ( ) ] return pd . Data Frame ( attr , columns = [ \"Attribute\" , \"Description\" ] )", "predictions": ["get the triangle for this : top . de - dimension ."], "references": ["get available attritbutes from dataset you ve selected"], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 1570, "code": "def get filters ( self , dataset ) : filters = self . filters ( dataset ) filt = [ ( k , v [ 0 ] ) for k , v in filters . items ( ) ] return pd . Data Frame ( filt , columns = [ \"Filter\" , \"Description\" ] )", "predictions": ["this method returns the exception with this : exception ."], "references": ["get available filters from dataset you ve selected"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1571, "code": "def prepare outdir ( self ) : self . outdir = self . outdir if self . outdir is None : self . tmpdir = Temporary Directory ( ) self . outdir = self . tmpdir . name elif isinstance ( self . outdir , str ) : mkdirs ( self . outdir ) else : raise Exception ( \"Error parsing outdir: %s\" % type ( self . outdir ) ) if isinstance ( self . gene sets , str ) : gset = os . path . split ( self . gene sets ) [ - 1 ] . lower ( ) . rstrip ( \".gmt\" ) elif isinstance ( self . gene sets , dict ) : gset = \"blank name\" else : raise Exception ( \"Error parsing gene sets parameter for gene sets\" ) logfile = os . path . join ( self . outdir , \"gseapy.%s.%s.log\" % ( self . module , gset ) ) return logfile", "predictions": ["sets and get the cert for all physically ."], "references": ["create temp directory ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 1572, "code": "def set cores ( self ) : cpu num = cpu count ( ) - 1 if self . processes > cpu num : cores = cpu num elif self . processes < 1 : cores = 1 else : cores = self . processes self . processes = int ( cores )", "predictions": ["sets the number of * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * i . e ."], "references": ["set cpu numbers to be used"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1573, "code": "def load gmt ( self , gene list , gmt ) : if isinstance ( gmt , dict ) : genesets dict = gmt elif isinstance ( gmt , str ) : genesets dict = self . parse gmt ( gmt ) else : raise Exception ( \"Error parsing gmt parameter for gene sets\" ) subsets = list ( genesets dict . keys ( ) ) self . n genesets = len ( subsets ) for subset in subsets : subset list = genesets dict . get ( subset ) if isinstance ( subset list , set ) : subset list = list ( subset list ) genesets dict [ subset ] = subset list tag indicator = np . in1d ( gene list , subset list , assume unique = True ) tag len = tag indicator . sum ( ) if self . min size <= tag len <= self . max size : continue del genesets dict [ subset ] filsets num = len ( subsets ) - len ( genesets dict ) self . logger . info ( \"%04d gene sets have been filtered out when max size=%s and min size=%s\" % ( filsets num , self . max size , self . min size ) ) if filsets num == len ( subsets ) : self . logger . error ( \"No gene sets passed through filtering condition!!!, try new parameters again!\\n\" + \"Note: check gene name, gmt file format, or filtering size.\" ) sys . exit ( 0 ) self . gmtdct = genesets dict return genesets dict", "predictions": ["generic bio string representing this detaching ."], "references": ["load gene set dict"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1574, "code": "def get libraries ( self , database = '' ) : lib url = 'http://amp.pharm.mssm.edu/%s Enrichr/dataset Statistics' % database libs json = json . loads ( requests . get ( lib url ) . text ) libs = [ lib [ 'library Name' ] for lib in libs json [ 'statistics' ] ] return sorted ( libs )", "predictions": ["print out all others and return their sorted ."], "references": ["return active enrichr library name . offical api"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 1575, "code": "def download libraries ( self , libname ) : self . logger . info ( \"Downloading and generating Enrichr library gene sets......\" ) s = retry ( 5 ) ENRICHR URL = 'http://amp.pharm.mssm.edu/Enrichr/gene Set Library' query string = '?mode=text&library Name=%s' response = s . get ( ENRICHR URL + query string % libname , timeout = None ) if not response . ok : raise Exception ( 'Error fetching enrichment results, check internet connection first.' ) mkdirs ( DEFAULT CACHE PATH ) genesets dict = { } outname = \"enrichr.%s.gmt\" % libname gmtout = open ( os . path . join ( DEFAULT CACHE PATH , outname ) , \"w\" ) for line in response . iter lines ( chunk size = 1024 , decode unicode = 'utf-8' ) : line = line . strip ( ) k = line . split ( \"\\t\" ) [ 0 ] v = list ( map ( lambda x : x . split ( \",\" ) [ 0 ] , line . split ( \"\\t\" ) [ 2 : ] ) ) genesets dict . update ( { k : v } ) outline = \"%s\\t\\t%s\\n\" % ( k , \"\\t\" . join ( v ) ) gmtout . write ( outline ) gmtout . close ( ) return genesets dict", "predictions": ["get the original data of the given object ."], "references": ["download enrichr libraries ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 1576, "code": "def heatmat ( self , df , classes , pheno pos , pheno neg ) : width = len ( classes ) if len ( classes ) >= 6 else 5 cls boo A = list ( map ( lambda x : True if x == pheno pos else False , classes ) ) cls boo B = list ( map ( lambda x : True if x == pheno neg else False , classes ) ) dat A = df . loc [ : , cls boo A ] dat B = df . loc [ : , cls boo B ] dat AB = pd . concat ( [ dat A , dat B ] , axis = 1 ) self . width = width self . heatmat = dat AB return", "predictions": ["returns a new ( ."], "references": ["only use for gsea heatmap"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1577, "code": "def save results ( self , zipdata , outdir , module , gmt , rank metric , permutation type ) : res = Ordered Dict ( ) for gs , gseale , ind , RES in zipdata : rdict = Ordered Dict ( ) rdict [ 'es' ] = gseale [ 0 ] rdict [ 'nes' ] = gseale [ 1 ] rdict [ 'pval' ] = gseale [ 2 ] rdict [ 'fdr' ] = gseale [ 3 ] rdict [ 'geneset size' ] = len ( gmt [ gs ] ) rdict [ 'matched size' ] = len ( ind ) #reformat gene list. genes = rank metric . index . values [ ind ] rdict [ 'genes' ] = \";\" . join ( [ str ( g ) . strip ( ) for g in genes ] ) if self . module != 'ssgsea' : if rdict [ 'es' ] > 0 : idx = RES . argmax ( ) ldg pos = list ( filter ( lambda x : x <= idx , ind ) ) elif rdict [ 'es' ] < 0 : idx = RES . argmin ( ) ldg pos = list ( filter ( lambda x : x >= idx , ind ) ) else : ldg pos = ind rdict [ 'ledge genes' ] = ';' . join ( list ( map ( str , rank metric . iloc [ ldg pos ] . index ) ) ) rdict [ 'RES' ] = RES rdict [ 'hits indices' ] = ind res [ gs ] = rdict self . results = res res df = pd . Data Frame . from dict ( res , orient = 'index' ) res df . index . name = 'Term' res df . drop ( [ 'RES' , 'hits indices' ] , axis = 1 , inplace = True ) res df . sort values ( by = [ 'fdr' , 'pval' ] , inplace = True ) self . res2d = res df if self . outdir is None : return out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation type ) ) if self . module == 'ssgsea' : out = out . replace ( \".csv\" , \".txt\" ) with open ( out , 'a' ) as f : f . write ( ) f . write ( ) res df . to csv ( f , sep = '\\t' ) else : res df . to csv ( out ) return", "predictions": ["saves this comments to the given location ."], "references": ["reformat gsea results and save to txt"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1578, "code": "def load data ( self , cls vec ) : if isinstance ( self . data , pd . Data Frame ) : exprs = self . data . copy ( ) if exprs . index . dtype == 'O' : exprs = exprs . reset index ( ) elif os . path . isfile ( self . data ) : if self . data . endswith ( \"gct\" ) : exprs = pd . read csv ( self . data , skiprows = 1 , comment = '#' , sep = \"\\t\" ) else : exprs = pd . read csv ( self . data , comment = '#' , sep = \"\\t\" ) else : raise Exception ( 'Error parsing gene expression Data Frame!' ) #drop duplicated gene names if exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) > 0 : self . logger . warning ( \"Warning: dropping duplicated gene names, only keep the first values\" ) exprs . drop duplicates ( subset = exprs . columns [ 0 ] , inplace = True ) #drop duplicate gene names. if exprs . isnull ( ) . any ( ) . sum ( ) > 0 : self . logger . warning ( \"Warning: Input data contains NA, filled NA with 0\" ) exprs . dropna ( how = 'all' , inplace = True ) #drop rows with all N As exprs = exprs . fillna ( 0 ) exprs . set index ( keys = exprs . columns [ 0 ] , inplace = True ) df = exprs . select dtypes ( include = [ np . number ] ) df std = df . groupby ( by = cls vec , axis = 1 ) . std ( ) df = df [ ~ df std . isin ( [ 0 ] ) . any ( axis = 1 ) ] df = df + 0.00001 return df", "predictions": ["get the issues as a not including the input and put it in the database"], "references": ["pre - processed the data frame . new filtering methods will be implement here ."], "bleu": 0.08225964699966554, "rouge_l": 0.06666666666666665}
{"id": 1579, "code": "def run Samples Permu ( self , df , gmt = None ) : assert self . min size <= self . max size mkdirs ( self . outdir ) self . results On Samples = Ordered Dict ( ) outdir = self . outdir for name , ser in df . iteritems ( ) : self . outdir = os . path . join ( outdir , str ( name ) ) self . logger . info ( \"Run Sample: %s \" % name ) mkdirs ( self . outdir ) dat2 = ser . sort values ( ascending = self . ascending ) gsea results , hit ind , rank ES , subsets = gsea compute ( data = dat2 , n = self . permutation num , gmt = gmt , weighted score type = self . weighted score type , permutation type = 'gene set' , method = None , pheno pos = '' , pheno neg = '' , classes = None , ascending = self . ascending , processes = self . processes , seed = self . seed , single = True , scale = self . scale ) res zip = zip ( subsets , list ( gsea results ) , hit ind , rank ES ) self . save results ( zipdata = res zip , outdir = self . outdir , module = self . module , gmt = gmt , rank metric = dat2 , permutation type = \"gene sets\" ) self . results On Samples [ name ] = self . res2d . es if self . noplot : continue self . logger . info ( \"Plotting Sample: %s \\n\" % name ) self . plotting ( rank metric = dat2 , results = self . results , graph num = self . graph num , outdir = self . outdir , figsize = self . figsize , format = self . format ) self . save ( outdir ) return", "predictions": ["starts this performs the motion ."], "references": ["single sample gsea workflow with permutation procedure"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1580, "code": "def save ( self , outdir ) : samples Raw ES = pd . Data Frame ( self . results On Samples ) samples Raw ES . index . name = 'Term|ES' samples NES = samples Raw ES / ( samples Raw ES . values . max ( ) - samples Raw ES . values . min ( ) ) samples NES = samples NES . copy ( ) samples NES . index . rename ( 'Term|NES' , inplace = True ) self . res2d = samples NES self . logger . info ( \"Congratulations. GSE Apy runs successfully................\\n\" ) if self . outdir is None : return out E Sfile = os . path . join ( outdir , \"gseapy.samples.raw.es.txt\" ) with open ( out E Sfile , 'a' ) as f : if self . scale : f . write ( ) f . write ( + 'as indicated by Barbie et al., 2009, online methods, pg. 2\\n' ) else : f . write ( ) f . write ( ) samples Raw ES . to csv ( f , sep = '\\t' ) out NE Sfile = os . path . join ( outdir , \"gseapy.samples.normalized.es.txt\" ) with open ( out NE Sfile , 'a' ) as f : f . write ( ) f . write ( ) samples NES . to csv ( f , sep = '\\t' ) return", "predictions": ["getter to allow dynamic ( to update the block ."], "references": ["save es and stats"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1581, "code": "def prepare outdir ( self ) : self . outdir = self . outdir if self . outdir is None : self . tmpdir = Temporary Directory ( ) self . outdir = self . tmpdir . name elif isinstance ( self . outdir , str ) : mkdirs ( self . outdir ) else : raise Exception ( \"Error parsing outdir: %s\" % type ( self . outdir ) ) logfile = os . path . join ( self . outdir , \"gseapy.%s.%s.log\" % ( self . module , self . descriptions ) ) return logfile", "predictions": ["sets and prepares the ] for indexing ."], "references": ["create temp directory ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1582, "code": "def parse genesets ( self ) : enrichr library = self . get libraries ( ) if isinstance ( self . gene sets , list ) : gss = self . gene sets elif isinstance ( self . gene sets , str ) : gss = [ g . strip ( ) for g in self . gene sets . strip ( ) . split ( \",\" ) ] elif isinstance ( self . gene sets , dict ) : gss = [ self . gene sets ] else : raise Exception ( \"Error parsing enrichr libraries, please provided corrected one\" ) gss exist = [ ] for g in gss : if isinstance ( g , dict ) : gss exist . append ( g ) continue if isinstance ( g , str ) : if g in enrichr library : gss exist . append ( g ) continue if g . lower ( ) . endswith ( \".gmt\" ) and os . path . exists ( g ) : self . logger . info ( \"User Defined gene sets is given: %s\" % g ) with open ( g ) as genesets : g dict = { line . strip ( ) . split ( \"\\t\" ) [ 0 ] : line . strip ( ) . split ( \"\\t\" ) [ 2 : ] for line in genesets . readlines ( ) } gss exist . append ( g dict ) return gss exist", "predictions": ["get a list of full client client geometries . this is a single call to prevent multiple geometries ."], "references": ["parse gene_sets input file type"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 1583, "code": "def send genes ( self , gene list , url ) : payload = { 'list' : ( None , gene list ) , 'description' : ( None , self . descriptions ) } response = requests . post ( url , files = payload ) if not response . ok : raise Exception ( 'Error analyzing gene list' ) sleep ( 1 ) job id = json . loads ( response . text ) return job id", "predictions": ["send a : : : : : : : : : : / / www . com / questions / . / . / . . com / questions / . / . / . / . / . - . - . - . - . - . -"], "references": ["send gene list to enrichr server"], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 1584, "code": "def check genes ( self , gene list , usr list id ) : response = requests . get ( 'http://amp.pharm.mssm.edu/Enrichr/view?user List Id=%s' % usr list id ) if not response . ok : raise Exception ( 'Error getting gene list back' ) returned L = json . loads ( response . text ) [ \"genes\" ] returned N = sum ( [ 1 for gene in gene list if gene in returned L ] ) self . logger . info ( '{} genes successfully recognized by Enrichr' . format ( returned N ) )", "predictions": ["aggregate a list of issues ."], "references": ["compare the genes sent and received to get successfully recognized genes"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 1585, "code": "def run ( self ) : self . get organism ( ) genes list = self . parse genelists ( ) gss = self . parse genesets ( ) self . logger . info ( \"Connecting to Enrichr Server to get latest library names\" ) if len ( gss ) < 1 : sys . stderr . write ( \"Not validated Enrichr library name provided\\n\" ) sys . stdout . write ( \"Hint: use get library name() to view full list of supported names\" ) sys . exit ( 1 ) self . results = pd . Data Frame ( ) for g in gss : if isinstance ( g , dict ) : res = self . enrich ( g ) short ID , self . gs = str ( id ( g ) ) , \"CUSTOM%s\" % id ( g ) if res is None : self . logger . info ( \"No hits return, for gene set: Custom%s\" % short ID ) continue else : self . gs = str ( g ) self . logger . debug ( \"Start Enrichr using library: %s\" % ( self . gs ) ) self . logger . info ( 'Analysis name: %s, Enrichr Library: %s' % ( self . descriptions , self . gs ) ) short ID , res = self . get results ( genes list ) res . insert ( 0 , \"Gene set\" , self . gs ) self . results = self . results . append ( res , ignore index = True , sort = True ) self . res2d = res if self . outdir is None : continue self . logger . info ( 'Save file of enrichment results: Job Id:' + str ( short ID ) ) outfile = \"%s/%s.%s.%s.reports.txt\" % ( self . outdir , self . gs , self . descriptions , self . module ) self . res2d . to csv ( outfile , index = False , encoding = 'utf-8' , sep = \"\\t\" ) if not self . no plot : msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , top term = self . top term , color = 'salmon' , title = self . gs , ofname = outfile . replace ( \"txt\" , self . format ) ) if msg is not None : self . logger . warning ( msg ) self . logger . info ( 'Done.\\n' ) if self . outdir is None : self . tmpdir . cleanup ( ) return", "predictions": ["this plugin executes the program ."], "references": ["run enrichr for one sample gene list but multi - libraries"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 1586, "code": "def annulus hires ( script , radius = None , radius1 = None , radius2 = None , diameter = None , diameter1 = None , diameter2 = None , cir segments = 48 , rad segments = 1 , color = None ) : if radius is not None and diameter is None : if radius1 is None and diameter1 is None : radius1 = radius if radius2 is None and diameter2 is None : radius2 = 0 if diameter is not None : if radius1 is None and diameter1 is None : radius1 = diameter / 2 if radius2 is None and diameter2 is None : radius2 = 0 if diameter1 is not None : radius1 = diameter1 / 2 if diameter2 is not None : radius2 = diameter2 / 2 if radius1 is None : radius1 = 1 if radius2 is None : radius2 = 0 ring = ( radius1 - radius2 ) / rad segments for i in range ( 0 , rad segments ) : annulus ( script , radius1 = radius1 - i * ring , radius2 = radius1 - ( i + 1 ) * ring , cir segments = cir segments ) layers . join ( script , merge vert = True ) if color is not None : vert color . function ( script , color = color ) return None", "predictions": ["create a config from a cls cls . this is a helper function that is used to generate a cls cls ."], "references": ["create a cylinder with user defined number of segments"], "bleu": 0.07289334177359764, "rouge_l": 0.13958810068649885}
{"id": 1587, "code": "def tube hires ( script , height = 1.0 , radius = None , radius1 = None , radius2 = None , diameter = None , diameter1 = None , diameter2 = None , cir segments = 32 , rad segments = 1 , height segments = 1 , center = False , simple bottom = False , color = None ) : if radius is not None and diameter is None : if radius1 is None and diameter1 is None : radius1 = radius if radius2 is None and diameter2 is None : radius2 = 0 if diameter is not None : if radius1 is None and diameter1 is None : radius1 = diameter / 2 if radius2 is None and diameter2 is None : radius2 = 0 if diameter1 is not None : radius1 = diameter1 / 2 if diameter2 is not None : radius2 = diameter2 / 2 if radius1 is None : radius1 = 1 if radius2 is None : radius2 = 0 annulus hires ( script , radius1 = radius1 , radius2 = radius2 , cir segments = cir segments , rad segments = rad segments ) transform . translate ( script , [ 0 , 0 , height ] ) if simple bottom : annulus ( script , radius1 = radius1 , radius2 = radius2 , cir segments = cir segments ) else : layers . duplicate ( script ) transform . translate ( script , [ 0 , 0 , - height ] ) transform . rotate ( script , 'x' , 180 ) cylinder open hires ( script , height , radius1 , cir segments = cir segments , height segments = height segments ) if radius2 != 0 : cylinder open hires ( script , height , radius2 , cir segments = cir segments , height segments = height segments , invert normals = True ) layers . join ( script ) clean . merge vert ( script , threshold = 0.00002 ) if center : transform . translate ( script , [ 0 , 0 , - height / 2 ] ) if color is not None : vert color . function ( script , color = color ) return None", "predictions": ["create a ( ( ( ( rare , if necessary return one return it return an array of traces return a new include return the same , otherwise , without causing return the include segments return the , without causing for the include return the include segments issue ."], "references": ["create a cylinder with user defined number of segments"], "bleu": 0.03667530253927971, "rouge_l": 0.15752098127824402}
{"id": 1588, "code": "def save to file ( self , script file ) : if not self . filters : print ( 'WARNING: no filters to save to file!' ) script file descriptor = open ( script file , 'w' ) script file descriptor . write ( '' . join ( self . opening + self . filters + self . closing ) ) script file descriptor . close ( )", "predictions": ["saves the . wait for this ( to the . file ."], "references": ["save filter script to an mlx file"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 1589, "code": "def per triangle ( script , sidedim = 0 , textdim = 1024 , border = 2 , method = 1 ) : filter xml = '' . join ( [ '  <filter name=\"Parametrization: Trivial Per-Triangle \">\\n' , '    <Param name=\"sidedim\"' , 'value=\"%d\"' % sidedim , 'description=\"Quads per line\"' , 'type=\"Rich Int\"' , 'tooltip=\"Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation\"' , '/>\\n' , '    <Param name=\"textdim\"' , 'value=\"%d\"' % textdim , 'description=\"Texture Dimension (px)\"' , 'type=\"Rich Int\"' , 'tooltip=\"Gives an indication on how big the texture is\"' , '/>\\n' , '    <Param name=\"border\"' , 'value=\"%d\"' % border , 'description=\"Inter-Triangle border (px)\"' , 'type=\"Rich Int\"' , 'tooltip=\"Specifies how many pixels to be left between triangles in parametrization domain\"' , '/>\\n' , '    <Param name=\"method\"' , 'value=\"%d\"' % method , 'description=\"Method\"' , 'enum val0=\"Basic\"' , 'enum val1=\"Space-optimizing\"' , 'enum cardinality=\"2\"' , 'type=\"Rich Enum\"' , 'tooltip=\"Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain\"' '/>\\n' , '  </filter>\\n' ] ) util . write filter ( script , filter xml ) return None", "predictions": ["generate a per ( , ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["trivial per - triangle parameterization"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 1590, "code": "def v multiply ( scalar , v1 ) : vector = [ ] for i , x in enumerate ( v1 ) : vector . append ( '(({})*({}))' . format ( scalar , v1 [ i ] ) ) return vector", "predictions": ["data is the sum of all the origin of this vector and returns the result ."], "references": ["multiply vector by scalar"], "bleu": 0.07692375026049747, "rouge_l": 0.11213235294117647}
{"id": 1591, "code": "def measure all ( fbasename = None , log = None , ml version = ml version ) : ml script1 file = 'TEMP3D measure g And T.mlx' if ml version == '1.3.4BETA' : file out = 'TEMP3D aabb.xyz' else : file out = None ml script1 = mlx . Filter Script ( file in = fbasename , file out = file out , ml version = ml version ) compute . measure geometry ( ml script1 ) compute . measure topology ( ml script1 ) ml script1 . save to file ( ml script1 file ) ml script1 . run script ( log = log , script file = ml script1 file ) geometry = ml script1 . geometry topology = ml script1 . topology if ml version == '1.3.4BETA' : if log is not None : log file = open ( log , 'a' ) log file . write ( '***Axis Aligned Bounding Results for file \"%s\":\\n' % fbasename ) log file . close ( ) aabb = measure aabb ( file out , log ) else : aabb = geometry [ 'aabb' ] return aabb , geometry , topology", "predictions": ["don ' t create routing and run for all geometry tests ."], "references": ["measures mesh geometry aabb and topology ."], "bleu": 0.1235622127262679, "rouge_l": 0.22101449275362317}
{"id": 1592, "code": "def measure dimension ( fbasename = None , log = None , axis1 = None , offset1 = 0.0 , axis2 = None , offset2 = 0.0 , ml version = ml version ) : axis1 = axis1 . lower ( ) axis2 = axis2 . lower ( ) ml script1 file = 'TEMP3D measure dimension.mlx' file out = 'TEMP3D measure dimension.xyz' ml script1 = mlx . Filter Script ( file in = fbasename , file out = file out , ml version = ml version ) compute . section ( ml script1 , axis1 , offset1 , surface = True ) compute . section ( ml script1 , axis2 , offset2 , surface = False ) layers . delete lower ( ml script1 ) ml script1 . save to file ( ml script1 file ) ml script1 . run script ( log = log , script file = ml script1 file ) for val in ( 'x' , 'y' , 'z' ) : if val not in ( axis1 , axis2 ) : axis = val axis num = ord ( axis ) - ord ( 'x' ) aabb = measure aabb ( file out , log ) dimension = { 'min' : aabb [ 'min' ] [ axis num ] , 'max' : aabb [ 'max' ] [ axis num ] , 'length' : aabb [ 'size' ] [ axis num ] , 'axis' : axis } if log is None : print ( '\\n For file \"%s\"' % fbasename ) print ( 'Dimension parallel to %s with %s=%s & %s=%s:' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) print ( '  Min = %s, Max = %s, Total length = %s' % ( dimension [ 'min' ] , dimension [ 'max' ] , dimension [ 'length' ] ) ) else : log file = open ( log , 'a' ) log file . write ( '\\n For file \"%s\"\\n' % fbasename ) log file . write ( 'Dimension parallel to %s with %s=%s & %s=%s:\\n' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) log file . write ( 'min = %s\\n' % dimension [ 'min' ] ) log file . write ( 'max = %s\\n' % dimension [ 'max' ] ) log file . write ( 'Total length = %s\\n' % dimension [ 'length' ] ) log file . close ( ) return dimension", "predictions": ["update and load ( ."], "references": ["measure a dimension of a mesh"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 1593, "code": "def get vprof version ( filename ) : with open ( filename ) as src file : version match = re . search ( r\"^ version  = ['\\\"]([^'\\\"]*)['\\\"]\" , src file . read ( ) , re . M ) if version match : return version match . group ( 1 ) raise Runtime Error ( 'Unable to find version info.' )", "predictions": ["gets the ( read description description from the source file ."], "references": ["returns actual version specified in filename ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 1594, "code": "def get obj count difference ( objs1 , objs2 ) : clean obj list1 = process in memory objects ( objs1 ) clean obj list2 = process in memory objects ( objs2 ) obj count 1 = get object count by type ( clean obj list1 ) obj count 2 = get object count by type ( clean obj list2 ) return obj count 1 - obj count 2", "predictions": ["add the number of events in the dataset ."], "references": ["returns count difference in two collections of python objects ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 1595, "code": "def format obj count ( objects ) : result = [ ] regex = re . compile ( r'<(?P<type>\\w+) \\'(?P<name>\\S+)\\'>' ) for obj type , obj count in objects . items ( ) : if obj count != 0 : match = re . findall ( regex , repr ( obj type ) ) if match : obj type , obj name = match [ 0 ] result . append ( ( \"%s %s\" % ( obj type , obj name ) , obj count ) ) return sorted ( result , key = operator . itemgetter ( 1 ) , reverse = True )", "predictions": ["add the take into a string ."], "references": ["formats object count ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1596, "code": "def trace memory usage ( self , frame , event , arg ) : #pylint: disable=unused-argument if event == 'line' and frame . f code . co filename in self . target modules : self . events list . append ( ( frame . f lineno , self . process . memory info ( ) . rss , frame . f code . co name , frame . f code . co filename ) ) return self . trace memory usage", "predictions": ["required for a add method ."], "references": ["checks memory usage when line event occur ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1597, "code": "def code events ( self ) : if self . resulting events : return self . resulting events for i , ( lineno , mem , func , fname ) in enumerate ( self . events list ) : mem in mb = float ( mem - self . mem overhead ) / BYTES IN MB if ( self . resulting events and self . resulting events [ - 1 ] [ 0 ] == lineno and self . resulting events [ - 1 ] [ 2 ] == func and self . resulting events [ - 1 ] [ 3 ] == fname and self . resulting events [ - 1 ] [ 1 ] < mem in mb ) : self . resulting events [ - 1 ] [ 1 ] = mem in mb else : self . resulting events . append ( [ i + 1 , lineno , mem in mb , func , fname ] ) return self . resulting events", "predictions": ["add events events on the edge ."], "references": ["returns processed memory usage ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1598, "code": "def compute mem overhead ( self ) : self . mem overhead = ( self . process . memory info ( ) . rss - builtins . initial rss size )", "predictions": ["computes the gathered gradient for this object ."], "references": ["returns memory overhead ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1599, "code": "def profile package ( self ) : target modules = base profiler . get pkg module names ( self . run object ) try : with Code Events Tracker ( target modules ) as prof : prof . compute mem overhead ( ) runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass return prof , None", "predictions": ["runs the specified get ."], "references": ["returns memory stats for a package ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1600, "code": "def profile module ( self ) : target modules = { self . run object } try : with open ( self . run object , 'rb' ) as srcfile , Code Events Tracker ( target modules ) as prof : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . compute mem overhead ( ) exec ( code , self . globs , None ) except System Exit : pass return prof , None", "predictions": ["executed when the profile has been set ."], "references": ["returns memory stats for a module ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1601, "code": "def profile function ( self ) : target modules = { self . run object . code . co filename } with Code Events Tracker ( target modules ) as prof : prof . compute mem overhead ( ) result = self . run object ( * self . run args , * * self . run kwargs ) return prof , result", "predictions": ["call this method to compute the effect of this python object ."], "references": ["returns memory stats for a function ."], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 1602, "code": "def run ( self ) : existing objects = get in memory objects ( ) prof , result = self . profile ( ) new objects = get in memory objects ( ) new obj count = get obj count difference ( new objects , existing objects ) result obj count = new obj count - prof . obj overhead result obj count [ list ] -= 1 pretty obj count = format obj count ( result obj count ) return { 'object Name' : self . object name , 'code Events' : prof . code events , 'total Events' : len ( prof . code events ) , 'objects Count' : pretty obj count , 'result' : result , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["executes the event object ."], "references": ["collects memory stats for specified python program ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1603, "code": "def get run object type ( run object ) : if isinstance ( run object , tuple ) : return 'function' run object , , = run object . partition ( ' ' ) if os . path . isdir ( run object ) : return 'package' return 'module'", "predictions": ["determine the suitable guess for an object ."], "references": ["determines run object type ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 1604, "code": "def init module ( self , run object ) : self . profile = self . profile module self . run object , , self . run args = run object . partition ( ' ' ) self . object name = '%s (module)' % self . run object self . globs = { ' file ' : self . run object , ' name ' : ' main ' , ' package ' : None , } program path = os . path . dirname ( self . run object ) if sys . path [ 0 ] != program path : sys . path . insert ( 0 , program path ) self . replace sysargs ( )", "predictions": ["initialize all the pycodestyle ."], "references": ["initializes profiler with a module ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1605, "code": "def init package ( self , run object ) : self . profile = self . profile package self . run object , , self . run args = run object . partition ( ' ' ) self . object name = '%s (package)' % self . run object self . replace sysargs ( )", "predictions": ["initialize all the maintained objects ."], "references": ["initializes profiler with a package ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1606, "code": "def init function ( self , run object ) : self . profile = self . profile function self . run object , self . run args , self . run kwargs = run object filename = inspect . getsourcefile ( self . run object ) self . object name = '%s @ %s (function)' % ( self . run object . name , filename )", "predictions": ["generate all ( . this is done by the caller ."], "references": ["initializes profiler with a function ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 1607, "code": "def replace sysargs ( self ) : sys . argv [ : ] = [ self . run object ] if self . run args : sys . argv += self . run args . split ( )", "predictions": ["replaces all original players with this object ."], "references": ["replaces sys . argv with proper args to pass to script ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 1608, "code": "def fill sample count ( self , node ) : node [ 'sample Count' ] += sum ( self . fill sample count ( child ) for child in node [ 'children' ] ) return node [ 'sample Count' ]", "predictions": ["fills in the tree using the specified sample ."], "references": ["counts and fills sample counts inside call tree ."], "bleu": 0.17747405280050263, "rouge_l": 0.3333333333333333}
{"id": 1609, "code": "def format tree ( self , node , total samples ) : funcname , filename , = node [ 'stack' ] sample percent = self . get percentage ( node [ 'sample Count' ] , total samples ) color hash = base profiler . hash name ( '%s @ %s' % ( funcname , filename ) ) return { 'stack' : node [ 'stack' ] , 'children' : [ self . format tree ( child , total samples ) for child in node [ 'children' ] ] , 'sample Count' : node [ 'sample Count' ] , 'sample Percentage' : sample percent , 'color Hash' : color hash }", "predictions": ["format all the ( data for this tree ."], "references": ["reformats call tree for the ui ."], "bleu": 0.17747405280050263, "rouge_l": 0.2557651991614256}
{"id": 1610, "code": "def call tree ( self ) : call tree = { 'stack' : 'base' , 'sample Count' : 0 , 'children' : [ ] } for stack , sample count in self . stats . items ( ) : self . insert stack ( reversed ( stack ) , sample count , call tree ) self . fill sample count ( call tree ) if not call tree [ 'children' ] : return { } return self . format tree ( call tree [ 'children' ] [ 0 ] , call tree [ 'sample Count' ] )", "predictions": ["poll the tree for all previously registered sample ids ."], "references": ["returns call tree ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 1611, "code": "def profile package ( self ) : with Stat Profiler ( ) as prof : prof . base frame = inspect . currentframe ( ) try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["profile the run of the package ."], "references": ["runs statistical profiler on a package ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 1612, "code": "def profile module ( self ) : with open ( self . run object , 'rb' ) as srcfile , Stat Profiler ( ) as prof : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . base frame = inspect . currentframe ( ) try : exec ( code , self . globs , None ) except System Exit : pass call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["profile the module . this will be called on each run in the process ."], "references": ["runs statistical profiler on a module ."], "bleu": 0.11633270842295028, "rouge_l": 0.1945773524720893}
{"id": 1613, "code": "def profile function ( self ) : with Stat Profiler ( ) as prof : result = self . run object ( * self . run args , * * self . run kwargs ) call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'result' : result , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["profile for the process ."], "references": ["runs statistical profiler on a function ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1614, "code": "def transform stats ( prof ) : records = [ ] for info , params in prof . stats . items ( ) : filename , lineno , funcname = info cum calls , num calls , time per call , cum time , = params if prof . total tt == 0 : percentage = 0 else : percentage = round ( 100 * ( cum time / prof . total tt ) , 4 ) cum time = round ( cum time , 4 ) func name = '%s @ %s' % ( funcname , filename ) color hash = base profiler . hash name ( func name ) records . append ( ( filename , lineno , funcname , cum time , percentage , num calls , cum calls , time per call , filename , color hash ) ) return sorted ( records , key = operator . itemgetter ( 4 ) , reverse = True )", "predictions": ["transform the data into their original and compares it with the correct data ."], "references": ["processes collected stats for ui ."], "bleu": 0.08839374326825923, "rouge_l": 0.10777385159010601}
{"id": 1615, "code": "def profile package ( self ) : prof = c Profile . Profile ( ) prof . enable ( ) try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass prof . disable ( ) prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["get the profile of the given package . calls profile ( ) to run the profile ."], "references": ["runs cprofile on a package ."], "bleu": 0.09507244120026236, "rouge_l": 0.19032761310452417}
{"id": 1616, "code": "def profile module ( self ) : prof = c Profile . Profile ( ) try : with open ( self . run object , 'rb' ) as srcfile : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . runctx ( code , self . globs , None ) except System Exit : pass prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["now we can call the profile for this module ."], "references": ["runs cprofile on a module ."], "bleu": 0.16590387014219712, "rouge_l": 0.26180257510729615}
{"id": 1617, "code": "def profile function ( self ) : prof = c Profile . Profile ( ) prof . enable ( ) result = self . run object ( * self . run args , * * self . run kwargs ) prof . disable ( ) prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'result' : result , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["show the profile of this function ."], "references": ["runs cprofile on a function ."], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 1618, "code": "def show guestbook ( ) : cursor = flask . g . db . execute ( 'SELECT name, message FROM entry ORDER BY id DESC;' ) entries = [ { 'name' : row [ 0 ] , 'message' : row [ 1 ] } for row in cursor . fetchall ( ) ] return jinja2 . Template ( LAYOUT ) . render ( entries = entries )", "predictions": ["show all ( ( ( ."], "references": ["returns all existing guestbook records ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1619, "code": "def add entry ( ) : name , msg = flask . request . form [ 'name' ] , flask . request . form [ 'message' ] flask . g . db . execute ( 'INSERT INTO entry (name, message) VALUES (?, ?)' , ( name , msg ) ) flask . g . db . commit ( ) return flask . redirect ( '/' )", "predictions": ["add a entry to the database ."], "references": ["adds single guestbook record ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1620, "code": "def handle root ( ) : res filename = os . path . join ( os . path . dirname ( file ) , PROFILE HTML ) with io . open ( res filename , 'rb' ) as res file : content = res file . read ( ) return content , 'text/html'", "predictions": ["implementation of thread . handle read content"], "references": ["handles index . html requests ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1621, "code": "def handle other ( self ) : res filename = os . path . join ( os . path . dirname ( file ) , STATIC DIR , self . path [ 1 : ] ) with io . open ( res filename , 'rb' ) as res file : content = res file . read ( ) , extension = os . path . splitext ( self . path ) return content , 'text/%s' % extension [ 1 : ]", "predictions": ["handle handle operation of this object ."], "references": ["handles static files requests ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1622, "code": "def do GET ( self ) : handler = self . uri map . get ( self . path ) or self . handle other content , content type = handler ( ) compressed content = gzip . compress ( content ) self . send response ( 200 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % content type ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( compressed content ) ) ) ) self . wfile . write ( compressed content )", "predictions": ["writes an s3 object to this object ."], "references": ["handles http get requests ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1623, "code": "def do POST ( self ) : post data = self . rfile . read ( int ( self . headers [ 'Content-Length' ] ) ) json data = gzip . decompress ( post data ) self . profile json . update ( json . loads ( json data . decode ( 'utf-8' ) ) ) self . send response ( 200 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % 'text/json' ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( post data ) ) ) )", "predictions": ["note : this code is not thread safe ."], "references": ["handles http post requests ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 1624, "code": "def send response ( self , http code , message = None , headers = None ) : self . send response ( http code , message ) if headers : for header in headers : self . send header ( * header ) self . end headers ( )", "predictions": ["dump a request to this http server ."], "references": ["sends http response code message and headers ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1625, "code": "def check standard dir ( module path ) : if 'site-packages' in module path : return True for stdlib path in STDLIB PATHS : if fnmatch . fnmatchcase ( module path , stdlib path + '*' ) : return True return False", "predictions": ["check if a path is in the directories or directory ."], "references": ["checks whether path belongs to standard library or installed modules ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 1626, "code": "def record line ( self , frame , event , arg ) : if event == 'line' : if self . prev timestamp : runtime = time . time ( ) - self . prev timestamp self . lines . append ( [ self . prev path , self . prev lineno , runtime ] ) self . prev lineno = frame . f lineno self . prev path = frame . f code . co filename self . prev timestamp = time . time ( ) return self . record line", "predictions": ["record a line of text that can be run on a frame ."], "references": ["records line execution time ."], "bleu": 0.10571070857151538, "rouge_l": 0.24158415841584158}
{"id": 1627, "code": "def lines without stdlib ( self ) : prev line = None current module path = inspect . getabsfile ( inspect . currentframe ( ) ) for module path , lineno , runtime in self . lines : module abspath = os . path . abspath ( module path ) if not prev line : prev line = [ module abspath , lineno , runtime ] else : if ( not check standard dir ( module path ) and module abspath != current module path ) : yield prev line prev line = [ module abspath , lineno , runtime ] else : prev line [ 2 ] += runtime yield prev line", "predictions": ["for each line in the given module ."], "references": ["filters code from standard library from self . lines ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 1628, "code": "def fill heatmap ( self ) : for module path , lineno , runtime in self . lines without stdlib : self . execution count [ module path ] [ lineno ] += 1 self . heatmap [ module path ] [ lineno ] += runtime", "predictions": ["apply the trees into a module ."], "references": ["fills code heatmap and execution count dictionaries ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1629, "code": "def skip lines ( src code , skip map ) : if not skip map : return [ [ 'line' , j + 1 , l ] for j , l in enumerate ( src code ) ] code with skips , i = [ ] , 0 for line , length in skip map : code with skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src code [ i : line ] ) ) if ( code with skips and code with skips [ - 1 ] [ 0 ] == 'skip' ) : code with skips [ - 1 ] [ 1 ] += length else : code with skips . append ( [ 'skip' , length ] ) i = line + length code with skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src code [ i : ] ) ) return code with skips", "predictions": ["skip over the expected expected actions ."], "references": ["skips lines in src_code specified by skip map ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 1630, "code": "def profile package ( self ) : with Code Heatmap Calculator ( ) as prof : try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass heatmaps = [ ] for filename , heatmap in prof . heatmap . items ( ) : if os . path . isfile ( filename ) : heatmaps . append ( self . format heatmap ( filename , heatmap , prof . execution count [ filename ] ) ) run time = sum ( heatmap [ 'run Time' ] for heatmap in heatmaps ) return { 'object Name' : self . run object , 'run Time' : run time , 'heatmaps' : heatmaps }", "predictions": ["runs the managed package ."], "references": ["calculates heatmap for package ."], "bleu": 0.35930411196308426, "rouge_l": 0.4}
{"id": 1631, "code": "def format heatmap ( self , filename , heatmap , execution count ) : with open ( filename ) as src file : file source = src file . read ( ) . split ( '\\n' ) skip map = self . calc skips ( heatmap , len ( file source ) ) run time = sum ( time for time in heatmap . values ( ) ) return { 'name' : filename , 'heatmap' : heatmap , 'execution Count' : execution count , 'src Code' : self . skip lines ( file source , skip map ) , 'run Time' : run time }", "predictions": ["format all reports reports ."], "references": ["formats heatmap for ui ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1632, "code": "def profile module ( self ) : with open ( self . run object , 'r' ) as srcfile : src code = srcfile . read ( ) code = compile ( src code , self . run object , 'exec' ) try : with Code Heatmap Calculator ( ) as prof : exec ( code , self . globs , None ) except System Exit : pass heatmaps = [ ] for filename , heatmap in prof . heatmap . items ( ) : if os . path . isfile ( filename ) : heatmaps . append ( self . format heatmap ( filename , heatmap , prof . execution count [ filename ] ) ) run time = sum ( heatmap [ 'run Time' ] for heatmap in heatmaps ) return { 'object Name' : self . run object , 'run Time' : run time , 'heatmaps' : heatmaps }", "predictions": ["profile an ."], "references": ["calculates heatmap for module ."], "bleu": 0.2758512992979459, "rouge_l": 0.23921568627450981}
{"id": 1633, "code": "def profile function ( self ) : with Code Heatmap Calculator ( ) as prof : result = self . run object ( * self . run args , * * self . run kwargs ) code lines , start line = inspect . getsourcelines ( self . run object ) source lines = [ ] for line in code lines : source lines . append ( ( 'line' , start line , line ) ) start line += 1 filename = os . path . abspath ( inspect . getsourcefile ( self . run object ) ) heatmap = prof . heatmap [ filename ] run time = sum ( time for time in heatmap . values ( ) ) return { 'object Name' : self . object name , 'run Time' : run time , 'result' : result , 'timestamp' : int ( time . time ( ) ) , 'heatmaps' : [ { 'name' : self . object name , 'heatmap' : heatmap , 'execution Count' : prof . execution count [ filename ] , 'src Code' : source lines , 'run Time' : run time } ] }", "predictions": ["executes the given function ."], "references": ["calculates heatmap for function ."], "bleu": 0.35930411196308426, "rouge_l": 0.4}
{"id": 1634, "code": "def count vocab ( self , analyzed docs ) : vocabulary = self . vocabulary j indices = make int array ( ) indptr = make int array ( ) indptr . append ( 0 ) for doc in analyzed docs : for feature in doc : try : j indices . append ( vocabulary [ feature ] ) except Key Error : continue indptr . append ( len ( j indices ) ) j indices = frombuffer empty ( j indices , dtype = np . intc ) indptr = np . frombuffer ( indptr , dtype = np . intc ) values = np . ones ( len ( j indices ) ) X = sp . csr matrix ( ( values , j indices , indptr ) , shape = ( len ( indptr ) - 1 , len ( vocabulary ) ) , dtype = self . dtype ) X . sum duplicates ( ) if self . binary : X . data . fill ( 1 ) return X", "predictions": ["this method is called when all values of this map are read ."], "references": ["create sparse feature matrix and vocabulary where fixed_vocab = false"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 1635, "code": "def to scikit ( self ) : scaler = Standard Scaler ( with mean = self . with mean , with std = self . with std , copy = self . copy ) scaler . dict = self . dict return scaler", "predictions": ["convert this object to a shallow power ."], "references": ["convert to equivalent standardscaler"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 1636, "code": "def fit ( self , Z , parameter iterable ) : self . scorer = check scoring ( self . estimator , scoring = self . scoring ) cv = self . cv cv = check cv ( cv , Z ) if self . verbose > 0 : if isinstance ( parameter iterable , Sized ) : n candidates = len ( parameter iterable ) print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n candidates , n candidates * len ( cv ) ) ) base estimator = clone ( self . estimator ) pre dispatch = self . pre dispatch out = Parallel ( n jobs = self . n jobs , verbose = self . verbose , pre dispatch = pre dispatch , backend = \"threading\" ) ( delayed ( fit and score ) ( clone ( base estimator ) , Z , self . scorer , train , test , self . verbose , parameters , self . fit params , return parameters = True , error score = self . error score ) for parameters in parameter iterable for train , test in cv ) n fits = len ( out ) n folds = len ( cv ) scores = list ( ) grid scores = list ( ) for grid start in range ( 0 , n fits , n folds ) : n test samples = 0 score = 0 all scores = [ ] for this score , this n test samples , , parameters in out [ grid start : grid start + n folds ] : all scores . append ( this score ) if self . iid : this score *= this n test samples n test samples += this n test samples score += this score if self . iid : score /= float ( n test samples ) else : score /= float ( n folds ) scores . append ( ( score , parameters ) ) grid scores . append ( CV Score Tuple ( parameters , score , np . array ( all scores ) ) ) self . grid scores = grid scores best = sorted ( grid scores , key = lambda x : x . mean validation score , reverse = True ) [ 0 ] self . best params = best . parameters self . best score = best . mean validation score if self . refit : best estimator = clone ( base estimator ) . set params ( * * best . parameters ) best estimator . fit ( Z , * * self . fit params ) self . best estimator = best estimator return self", "predictions": [". implementation . estimate the best ."], "references": ["actual fitting performing the search over parameters ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1637, "code": "def score ( estimator , Z test , scorer ) : score = scorer ( estimator , Z test ) if not isinstance ( score , numbers . Number ) : raise Value Error ( \"scoring must return a number, got %s (%s) instead.\" % ( str ( score ) , type ( score ) ) ) return score", "predictions": ["computes init init and dispatch value ."], "references": ["compute the score of an estimator on a given test set ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 1638, "code": "def block collection ( iterator , dtype , bsize = - 1 ) : i = 0 accumulated = [ ] for a in iterator : if ( bsize > 0 ) and ( i >= bsize ) : yield pack accumulated ( accumulated , dtype ) accumulated = [ ] i = 0 accumulated . append ( a ) i += 1 if i > 0 : yield pack accumulated ( accumulated , dtype )", "predictions": ["loop loop without normalizing or combining elements ."], "references": ["pack rdd with a specific collection constructor ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1639, "code": "def block tuple ( iterator , dtypes , bsize = - 1 ) : i = 0 blocked tuple = None for tuple i in iterator : if blocked tuple is None : blocked tuple = tuple ( [ ] for in range ( len ( tuple i ) ) ) if ( bsize > 0 ) and ( i >= bsize ) : yield tuple ( pack accumulated ( x , dtype ) for x , dtype in zip ( blocked tuple , dtypes ) ) blocked tuple = tuple ( [ ] for in range ( len ( tuple i ) ) ) i = 0 for x j , x in zip ( tuple i , blocked tuple ) : x . append ( x j ) i += 1 if i > 0 : yield tuple ( pack accumulated ( x , dtype ) for x , dtype in zip ( blocked tuple , dtypes ) )", "predictions": ["generate an iterator of tuples from each tuple ."], "references": ["pack rdd of tuples as tuples of arrays or scipy . sparse matrices ."], "bleu": 0.11452232344544261, "rouge_l": 0.2510288065843621}
{"id": 1640, "code": "def shape ( self ) : first = self . first ( ) . shape shape = self . rdd . map ( lambda x : x . shape [ 0 ] ) . sum ( ) return ( shape , ) + first [ 1 : ]", "predictions": ["create a new mutable file of this cipher ."], "references": ["returns the shape of the data ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 1641, "code": "def toarray ( self ) : rdd = self . rdd . map ( lambda x : x . toarray ( ) ) return np . concatenate ( rdd . collect ( ) )", "predictions": ["maps the base node to the list of node node ."], "references": ["returns the data as numpy . array from each partition ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 1642, "code": "def convert ( self , txn ) : ofxid = self . mk ofxid ( txn . id ) metadata = { } posting metadata = { \"ofxid\" : ofxid } if isinstance ( txn , Ofx Transaction ) : posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting metadata ) return Transaction ( date = txn . date , payee = self . format payee ( txn ) , postings = [ posting , posting . clone inverted ( self . mk dynamic account ( self . format payee ( txn ) , exclude = self . name ) ) ] ) elif isinstance ( txn , Investment Transaction ) : acct1 = self . name acct2 = self . name posting1 = None posting2 = None security = self . maybe get ticker ( txn . security ) if isinstance ( txn . type , str ) : if re . match ( '^(buy|sell)' , txn . type ) : acct2 = self . unknownaccount or 'Assets:Unknown' elif txn . type == 'transfer' : acct2 = 'Transfer' elif txn . type == 'reinvest' : acct2 = 'Income:Interest' elif txn . type == 'income' and txn . income type == 'DIV' : metadata [ 'dividend from' ] = security acct2 = 'Income:Dividends' posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting metadata ) posting2 = posting1 . clone inverted ( acct2 ) else : pass else : if ( txn . type in [ 0 , 1 , 3 , 4 ] ) : acct2 = self . unknownaccount or 'Assets:Unknown' elif ( txn . type == 2 ) : acct2 = 'Income:Interest' else : pass aux date = None if txn . settle Date is not None and txn . settle Date != txn . trade Date : aux date = txn . settle Date if posting1 is None and posting2 is None : posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit price = Amount ( txn . unit price , self . currency , unlimited = True ) , metadata = posting metadata ) posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit price , self . currency , reverse = True ) ) else : pass return Transaction ( date = txn . trade Date , aux date = aux date , payee = self . format payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] )", "predictions": ["creates a double object from the input . . . ."], "references": ["convert an ofx transaction to a posting"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 1643, "code": "def compatibility ( session , install ) : session . install ( '-e' , '.[dev]' ) session . install ( install ) run tests ( session )", "predictions": ["installs a new system profile ."], "references": ["run the unit test suite with each support library and python version ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 1644, "code": "def text width ( self , text : str ) -> float : width , = self . font . getsize ( text ) return width", "predictions": ["this is a convenience method that makes the profile look like the profile ."], "references": ["returns the width in pixels of a string in dejavu sans 110pt ."], "bleu": 0.10511846841633776, "rouge_l": 0.1491442542787286}
{"id": 1645, "code": "def text width ( self , text : str ) -> float : width = 0 for index , c in enumerate ( text ) : width += self . char to width . get ( c , self . default character width ) width -= self . pair to kern . get ( text [ index : index + 2 ] , 0 ) return width", "predictions": ["return a new instance of profile ."], "references": ["returns the width in pixels of a string in dejavu sans 110pt ."], "bleu": 0.09374222649442905, "rouge_l": 0.1897356143079316}
{"id": 1646, "code": "def default ( cls ) -> 'Precalculated Text Measurer' : if cls . default cache is not None : return cls . default cache if pkg resources . resource exists ( name , 'default-widths.json.xz' ) : import lzma with pkg resources . resource stream ( name , 'default-widths.json.xz' ) as f : with lzma . open ( f , \"rt\" ) as g : cls . default cache = Precalculated Text Measurer . from json ( cast ( Text IO , g ) ) return cls . default cache elif pkg resources . resource exists ( name , 'default-widths.json' ) : with pkg resources . resource stream ( name , 'default-widths.json' ) as f : cls . default cache = Precalculated Text Measurer . from json ( io . Text IO Wrapper ( f , encoding = 'utf-8' ) ) return cls . default cache else : raise Value Error ( 'could not load default-widths.json' )", "predictions": ["sorted transform to time ."], "references": ["returns a reasonable default precalculatedtextmeasurer ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1647, "code": "def generate supported characters ( deja vu sans path : str ) -> Iterable [ str ] : font = tt Lib . TT Font ( deja vu sans path ) for cmap in font [ 'cmap' ] . tables : if cmap . is Unicode ( ) : for code in cmap . cmap : yield chr ( code )", "predictions": ["profile a sequence of package ( which can be used to return a sequence of package ( which can be used to return a sequence of package ( which is a string of the same stats . . . . . . . . . . . . . ."], "references": ["generate the characters support by the font at the given path ."], "bleu": 0.026594139297659906, "rouge_l": 0.07253269916765755}
{"id": 1648, "code": "def write json ( f : Text IO , deja vu sans path : str , measurer : text measurer . Text Measurer , encodings : Iterable [ str ] ) -> None : supported characters = list ( generate supported characters ( deja vu sans path ) ) kerning characters = '' . join ( generate encodeable characters ( supported characters , encodings ) ) char to length = calculate character to length mapping ( measurer , supported characters ) pair to kerning = calculate pair to kern mapping ( measurer , char to length , kerning characters ) json . dump ( { 'mean-character-length' : statistics . mean ( char to length . values ( ) ) , 'character-lengths' : char to length , 'kerning-characters' : kerning characters , 'kerning-pairs' : pair to kerning } , f , sort keys = True , indent = 1 )", "predictions": ["profile text as supported"], "references": ["write the data required by precalculatedtextmeasurer to a stream ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1649, "code": "def convolve gaussian 2d ( image , gaussian kernel 1d ) : result = scipy . ndimage . filters . correlate1d ( image , gaussian kernel 1d , axis = 0 ) result = scipy . ndimage . filters . correlate1d ( result , gaussian kernel 1d , axis = 1 ) return result", "predictions": ["profile and return the function ."], "references": ["convolve 2d gaussian ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1650, "code": "def get gaussian kernel ( gaussian kernel width = 11 , gaussian kernel sigma = 1.5 ) : gaussian kernel 1d = numpy . ndarray ( ( gaussian kernel width ) ) norm mu = int ( gaussian kernel width / 2 ) for i in range ( gaussian kernel width ) : gaussian kernel 1d [ i ] = ( exp ( - ( ( ( i - norm mu ) ** 2 ) ) / ( 2 * ( gaussian kernel sigma ** 2 ) ) ) ) return gaussian kernel 1d / numpy . sum ( gaussian kernel 1d )", "predictions": ["generate a guestbook ( if any db is found db db db db db db db db db db db db db db db db db db db db db db db db db size db size db db db size db db db"], "references": ["generate a gaussian kernel ."], "bleu": 0.03599084792653663, "rouge_l": 0.09531250000000001}
{"id": 1651, "code": "def main ( ) : description = '\\n' . join ( [ 'Compares an image with a list of images using the SSIM metric.' , '  Example:' , '    pyssim test-images/test1-1.png \"test-images/*\"' ] ) parser = argparse . Argument Parser ( prog = 'pyssim' , formatter class = argparse . Raw Text Help Formatter , description = description ) parser . add argument ( '--cw' , help = 'compute the complex wavelet SSIM' , action = 'store true' ) parser . add argument ( 'base image' , metavar = 'image1.png' , type = argparse . File Type ( 'r' ) ) parser . add argument ( 'comparison images' , metavar = 'image path with* or image2.png' ) parser . add argument ( '--width' , type = int , default = None , help = 'scales the image before computing SSIM' ) parser . add argument ( '--height' , type = int , default = None , help = 'scales the image before computing SSIM' ) args = parser . parse args ( ) if args . width and args . height : size = ( args . width , args . height ) else : size = None if not args . cw : gaussian kernel sigma = 1.5 gaussian kernel width = 11 gaussian kernel 1d = get gaussian kernel ( gaussian kernel width , gaussian kernel sigma ) comparison images = glob . glob ( args . comparison images ) is a single image = len ( comparison images ) == 1 for comparison image in comparison images : if args . cw : ssim = SSIM ( args . base image . name , size = size ) ssim value = ssim . cw ssim value ( comparison image ) else : ssim = SSIM ( args . base image . name , gaussian kernel 1d , size = size ) ssim value = ssim . ssim value ( comparison image ) if is a single image : sys . stdout . write ( '%.7g' % ssim value ) else : sys . stdout . write ( '%s - %s: %.7g' % ( args . base image . name , comparison image , ssim value ) ) sys . stdout . write ( '\\n' )", "predictions": ["the main method to process the command line of all the files in the input source code ."], "references": ["main function for pyssim ."], "bleu": 0.07535838128770536, "rouge_l": 0.19365079365079363}
{"id": 1652, "code": "def destroy ( self ) : if self . conf . auto Tick : self . destroying = True else : self . do Destroy ( )", "predictions": ["destroys destroys and contexts"], "references": ["correctly destroy syncobj . stop autotickthread close connections etc ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 1653, "code": "def get Status ( self ) : status = { } status [ 'version' ] = VERSION status [ 'revision' ] = REVISION status [ 'self' ] = self . self Node status [ 'state' ] = self . raft State status [ 'leader' ] = self . raft Leader status [ 'partner nodes count' ] = len ( self . other Nodes ) for node in self . other Nodes : status [ 'partner node status server ' + node . id ] = 2 if node in self . connected Nodes else 0 status [ 'readonly nodes count' ] = len ( self . readonly Nodes ) for node in self . readonly Nodes : status [ 'readonly node status server ' + node . id ] = 2 if node in self . connected Nodes else 0 status [ 'log len' ] = len ( self . raft Log ) status [ 'last applied' ] = self . raft Last Applied status [ 'commit idx' ] = self . raft Commit Index status [ 'raft term' ] = self . raft Current Term status [ 'next node idx count' ] = len ( self . raft Next Index ) for node , idx in iteritems ( self . raft Next Index ) : status [ 'next node idx server ' + node . id ] = idx status [ 'match idx count' ] = len ( self . raft Match Index ) for node , idx in iteritems ( self . raft Match Index ) : status [ 'match idx server ' + node . id ] = idx status [ 'leader commit idx' ] = self . leader Commit Index status [ 'uptime' ] = int ( time . time ( ) - self . start Time ) status [ 'self code version' ] = self . self Code Version status [ 'enabled code version' ] = self . enabled Code Version return status", "predictions": ["this is useful for testing ."], "references": ["dumps different debug info about cluster to dict and return it"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 1654, "code": "def print Status ( self ) : status = self . get Status ( ) for k , v in iteritems ( status ) : logging . info ( '%s: %s' % ( str ( k ) , str ( v ) ) )", "predictions": ["prints the status of the statement ."], "references": ["dumps different debug info about cluster to default logger"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 1655, "code": "def check ( func ) : def wrapped ( * args , * * kwargs ) : check name = func . name arg name = None if args : arg name = args [ 0 ] try : if arg name : logger . debug ( \"Checking '%s' for '%s'\" , check name , arg name ) else : logger . debug ( \"Checking '%s'\" , check name ) response = func ( * args , * * kwargs ) except Exception as e : message = str ( e ) response = { \"ok\" : False , \"error\" : message , \"stacktrace\" : traceback . format exc ( ) , } if arg name : response = { arg name : response } logger . exception ( \"Error calling '%s' for '%s': %s\" , check name , arg name , message ) else : logger . exception ( \"Error calling '%s': %s\" , check name , message ) return response return wrapped", "predictions": ["get all the calling in the give window ."], "references": ["decorator which wraps checks and returns an error response on failure ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 1656, "code": "def str to list ( s ) : list = s . split ( \",\" ) return list ( map ( lambda i : i . lstrip ( ) , list ) )", "predictions": ["split the string representation of each element in the ( ( ( ( http : / / www . w3 . org / tr / svgmobile12 / ( / ( / ( / ( / ( / ( / ( http : / / ( / ( / ( /"], "references": ["converts a comma separated string to a list"], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 1657, "code": "def cli parse ( file path , sa , nameservers , dns timeout , parallel = False ) : try : file results = parse report file ( file path , nameservers = nameservers , dns timeout = dns timeout , strip attachment payloads = sa , parallel = parallel ) except Parser Error as error : return error , file path finally : global counter with counter . get lock ( ) : counter . value += 1 return file results , file path", "predictions": ["standard check validity of the ( ( or if it has been read for the given ( timeout for each ( timeout for each ( timeout for each ( timeout for example for 2 for each ( attachment for example for the ( attachment for each ( attachment for example"], "references": ["separated this function for multiprocessing"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 1658, "code": "def publish ( self , subject , reply , payload , payload size ) : if subject == \"\" : raise Err Bad Subject payload size bytes = ( \"%d\" % payload size ) . encode ( ) pub cmd = b'' . join ( [ PUB OP , SPC , subject . encode ( ) , SPC , reply , SPC , payload size bytes , CRLF , payload , CRLF ] ) self . stats [ 'out msgs' ] += 1 self . stats [ 'out bytes' ] += payload size yield from self . send command ( pub cmd ) if self . flush queue . empty ( ) : yield from self . flush pending ( )", "predictions": ["record a frame for the given event ."], "references": ["sends pub command to the nats server ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1659, "code": "def process pong ( self ) : if len ( self . pongs ) > 0 : future = self . pongs . pop ( 0 ) future . set result ( True ) self . pongs received += 1 self . pings outstanding -= 1", "predictions": ["threadsafe a module on the module ."], "references": ["process pong sent by server ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1660, "code": "def process msg ( self , sid , subject , reply , data ) : payload size = len ( data ) self . stats [ 'in msgs' ] += 1 self . stats [ 'in bytes' ] += payload size sub = self . subs . get ( sid ) if sub is None : return sub . received += 1 if sub . max msgs > 0 and sub . received >= sub . max msgs : self . subs . pop ( sid , None ) msg = self . build message ( subject , reply , data ) if sub . future is not None : if sub . future . cancelled ( ) : return sub . future . set result ( msg ) return try : sub . pending size += payload size if sub . pending size >= sub . pending bytes limit : sub . pending size -= payload size if self . error cb is not None : yield from self . error cb ( Err Slow Consumer ( subject = subject , sid = sid ) ) return sub . pending queue . put nowait ( msg ) except asyncio . Queue Full : if self . error cb is not None : yield from self . error cb ( Err Slow Consumer ( subject = subject , sid = sid ) )", "predictions": ["fill with multiple sample hard-link and then yield ."], "references": ["process msg sent by server ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1661, "code": "def load features from array ( self , features ) : self . feature images = np . load ( features ) self . feature names = range ( self . feature images . shape [ 1 ] )", "predictions": ["generates a folder with the given lines ."], "references": ["load feature data from a 2d ndarray on disk ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 1662, "code": "def dot product ( self , imgs to decode ) : return np . dot ( imgs to decode . T , self . feature images ) . T", "predictions": ["profile data from this notebook ."], "references": ["decoding using the dot product ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1663, "code": "def feature selection ( feat select , X , y ) : if re . match ( '.*-best' , feat select ) is not None : n = int ( feat select . split ( '-' ) [ 0 ] ) selector = Select K Best ( k = n ) import warnings with warnings . catch warnings ( ) : warnings . simplefilter ( 'ignore' , category = User Warning ) features selected = np . where ( selector . fit ( X , y ) . get support ( ) is True ) [ 0 ] elif re . match ( '.*-randombest' , feat select ) is not None : n = int ( feat select . split ( '-' ) [ 0 ] ) from random import shuffle features = range ( 0 , X . shape [ 1 ] ) shuffle ( features ) features selected = features [ : n ] return features selected", "predictions": ["gets the heatmap heatmap in the given spatial format ."], "references": ["implements various kinds of feature selection"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1664, "code": "def fit ( self , X , y , cv = None , class weight = 'auto' ) : self . X = X self . y = y self . set class weight ( class weight = class weight , y = y ) self . clf = self . clf . fit ( X , y ) return self . clf", "predictions": ["abstract method to fit this actually writes the original class ."], "references": ["fits x to outcomes y using clf"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 1665, "code": "def set class weight ( self , class weight = 'auto' , y = None ) : if class weight is None : cw = None try : self . clf . set params ( class weight = cw ) except Value Error : pass elif class weight == 'auto' : c = np . bincount ( y ) ii = np . nonzero ( c ) [ 0 ] c = c / float ( c . sum ( ) ) cw = dict ( zip ( ii [ : : - 1 ] , c [ ii ] ) ) try : self . clf . set params ( class weight = cw ) except Value Error : import warnings warnings . warn ( \"Tried to set class weight, but failed. The classifier \" \"probably doesn't support it\" )", "predictions": ["set class parameters based on the number of weight ."], "references": ["sets the class_weight of the classifier to match y"], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 1666, "code": "def cross val fit ( self , X , y , cross val = '4-Fold' , scoring = 'accuracy' , feat select = None , class weight = 'auto' ) : from sklearn import cross validation self . X = X self . y = y self . set class weight ( class weight = class weight , y = y ) if isinstance ( cross val , string types ) : if re . match ( '.*-Fold' , cross val ) is not None : n = int ( cross val . split ( '-' ) [ 0 ] ) self . cver = cross validation . Stratified K Fold ( self . y , n ) else : raise Exception ( 'Unrecognized cross validation method' ) else : self . cver = cross val if feat select is not None : self . features selected = [ ] from sklearn . grid search import Grid Search CV if isinstance ( self . clf , Grid Search CV ) : import warnings if feat select is not None : warnings . warn ( \"Cross-validated feature selection not supported with \" \"Grid Search CV\" ) self . clf . set params ( cv = self . cver , scoring = scoring ) with warnings . catch warnings ( ) : warnings . simplefilter ( 'ignore' , category = User Warning ) self . clf = self . clf . fit ( X , y ) self . cvs = self . clf . best score else : self . cvs = self . feat select cvs ( feat select = feat select , scoring = scoring ) if feat select is not None : fs = feature selection ( feat select , X , y ) self . features selected . append ( fs ) X = X [ : , fs ] self . clf . fit ( X , y ) return self . cvs . mean ( )", "predictions": ["writes an cross cross gdataservice results ."], "references": ["fits x to outcomes y using clf and cv_method"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 1667, "code": "def fit dataset ( self , dataset , y , features = None , feature type = 'features' ) : if feature type == 'features' : X = np . rot90 ( dataset . feature table . data . toarray ( ) ) elif feature type == 'voxels' : X = np . rot90 ( dataset . image table . data . toarray ( ) ) self . sk classifier . fit ( X , y )", "predictions": ["fit a dataset to the specified dataset ."], "references": ["given a dataset fits either features or voxels to y"], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 1668, "code": "def get top words ( model , feature names , n top words = 40 ) : topic words = [ ] for topic in model . components : top words = [ feature names [ i ] for i in topic . argsort ( ) [ : - n top words - 1 : - 1 ] ] topic words += [ top words ] return topic words", "predictions": ["for a given model ."], "references": ["return top forty words from each topic in trained topic model ."], "bleu": 0.08860330314183162, "rouge_l": 0.2190305206463196}
{"id": 1669, "code": "def pearson ( x , y ) : data = np . vstack ( ( x , y ) ) ms = data . mean ( axis = 1 ) [ ( slice ( None , None , None ) , None ) ] datam = data - ms datass = np . sqrt ( np . sum ( datam ** 2 , axis = 1 ) ) temp = np . dot ( datam [ 1 : ] , datam [ 0 ] . T ) rs = temp / ( datass [ 1 : ] * datass [ 0 ] ) return rs", "predictions": ["returns an approximation of the dataset ."], "references": ["correlates row vector x with each row vector in 2d array y ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 1670, "code": "def load ( cls , filename ) : try : dataset = pickle . load ( open ( filename , 'rb' ) ) except Unicode Decode Error : dataset = pickle . load ( open ( filename , 'rb' ) , encoding = 'latin' ) if hasattr ( dataset , 'feature table' ) : dataset . feature table . csr to sdf ( ) return dataset", "predictions": ["load data for a dataset ."], "references": ["load a pickled dataset instance from file ."], "bleu": 0.19902510067151713, "rouge_l": 0.5570776255707762}
{"id": 1671, "code": "def save ( self , filename ) : if hasattr ( self , 'feature table' ) : self . feature table . sdf to csr ( ) pickle . dump ( self , open ( filename , 'wb' ) , - 1 ) if hasattr ( self , 'feature table' ) : self . feature table . csr to sdf ( )", "predictions": ["writes a csr to a file"], "references": ["pickle the dataset instance to the provided file ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1672, "code": "def get ids by expression ( self , expression , threshold = 0.001 , func = np . sum ) : lexer = lp . Lexer ( ) lexer . build ( ) parser = lp . Parser ( lexer , self . dataset , threshold = threshold , func = func ) parser . build ( ) return parser . parse ( expression ) . keys ( ) . values", "predictions": ["get a method to be used for adapters adapters ."], "references": ["use a peg to parse expression and return study ids ."], "bleu": 0.13564514503163538, "rouge_l": 0.28328173374613}
{"id": 1673, "code": "def sdf to csr ( self ) : data = self . data . to dense ( ) self . data = { 'columns' : list ( data . columns ) , 'index' : list ( data . index ) , 'values' : sparse . csr matrix ( data . values ) }", "predictions": ["convert a full resource into a csr map ."], "references": ["convert featuretable to scipy csr matrix ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 1674, "code": "def xyz to mat ( foci , xyz dims = None , mat dims = None ) : foci = np . hstack ( ( foci , np . ones ( ( foci . shape [ 0 ] , 1 ) ) ) ) mat = np . array ( [ [ - 0.5 , 0 , 0 , 45 ] , [ 0 , 0.5 , 0 , 63 ] , [ 0 , 0 , 0.5 , 36 ] ] ) . T result = np . dot ( foci , mat ) [ : , : : - 1 ] return np . round ( result ) . astype ( int )", "predictions": ["apply a ( to an array of points ."], "references": ["convert an n x 3 array of xyz coordinates to matrix indices ."], "bleu": 0.1416341262365823, "rouge_l": 0.35209235209235207}
{"id": 1675, "code": "def save img ( data , filename , masker , header = None ) : if not header : header = masker . get header ( ) header . set data dtype ( data . dtype ) header [ 'cal max' ] = data . max ( ) header [ 'cal min' ] = data . min ( ) img = nifti1 . Nifti1Image ( masker . unmask ( data ) , None , header ) img . to filename ( filename )", "predictions": ["save the . to the specified file ."], "references": ["save a vectorized image to file ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 1676, "code": "def dict to object ( item , object name ) : fields = item . keys ( ) values = item . values ( ) return json . loads ( json . dumps ( item ) , object hook = lambda d : namedtuple ( object name , fields ) ( * values ) )", "predictions": ["returns the desired object from the given dict ."], "references": ["converts a python dict to a namedtuple saving memory ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 1677, "code": "async def get bearer info ( self ) : if self . client id is None : raise Spotify Exception ( GET BEARER ERR % 'client id' ) elif self . client secret is None : raise Spotify Exception ( GET BEARER ERR % 'client secret' ) token = b64encode ( ':' . join ( ( self . client id , self . client secret ) ) . encode ( ) ) kwargs = { 'url' : 'https://accounts.spotify.com/api/token' , 'data' : { 'grant type' : 'client credentials' } , 'headers' : { 'Authorization' : 'Basic ' + token . decode ( ) } } async with self . session . post ( * * kwargs ) as resp : return json . loads ( await resp . text ( encoding = 'utf-8' ) )", "predictions": ["fetches information for this bearer bearer bearer bearer operation ."], "references": ["get the application bearer token from client_id and client_secret ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 1678, "code": "def assert hasattr ( attr : str , msg : str , tp : Base Exception = Spotify Exception ) -> Callable : def decorator ( func : Callable ) -> Callable : @ functools . wraps ( func ) def decorated ( self , * args , * * kwargs ) : if not hasattr ( self , attr ) : raise tp ( msg ) return func ( self , * args , * * kwargs ) if inspect . iscoroutinefunction ( func ) : @ functools . wraps ( func ) async def decorated ( * args , * * kwargs ) : return await decorated ( * args , * * kwargs ) return decorated return decorator", "predictions": ["assert that result is not expected ."], "references": ["decorator to assert an object has an attribute when run ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 1679, "code": "def from client ( cls , client , * args , * * kwargs ) : return cls ( client . http . client id , * args , * * kwargs )", "predictions": ["savings the client ' s http client with the given attributes ."], "references": ["construct a oauth2 object from a spotify . client ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 1680, "code": "def url ( client id : str , redirect uri : str , * , scope : str = None , state : str = None , secure : bool = True ) -> str : attrs = { 'client id' : client id , 'redirect uri' : quote ( redirect uri ) } if scope is not None : attrs [ 'scope' ] = quote ( scope ) if state is not None : attrs [ 'state' ] = state parameters = '&' . join ( '{0}={1}' . format ( * item ) for item in attrs . items ( ) ) return O Auth2 . BASE . format ( parameters = parameters )", "predictions": ["url - quote url to url ."], "references": ["construct a oauth2 url instead of an oauth2 object ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1681, "code": "def attrs ( self ) : data = { 'client id' : self . client id , 'redirect uri' : quote ( self . redirect uri ) , } if self . scope is not None : data [ 'scope' ] = quote ( self . scope ) if self . state is not None : data [ 'state' ] = self . state return data", "predictions": ["sets operation to be published in browser ."], "references": ["attributes used when constructing url parameters ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1682, "code": "def parameters ( self ) -> str : return '&' . join ( '{0}={1}' . format ( * item ) for item in self . attrs . items ( ) )", "predictions": ["generate a list of parameters for this entity ."], "references": ["url parameters used ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 1683, "code": "async def from href ( self ) : if not hasattr ( self , 'href' ) : raise Type Error ( 'Spotify object has no `href` attribute, therefore cannot be retrived' ) elif hasattr ( self , 'http' ) : return await self . http . request ( ( 'GET' , self . href ) ) else : cls = type ( self ) try : client = getattr ( self , ' {0} client' . format ( cls . name ) ) except Attribute Error : raise Type Error ( 'Spotify object has no way to access a HTTP Client.' ) else : http = client . http data = await http . request ( ( 'GET' , self . href ) ) return cls ( client , data )", "predictions": ["creates a qt object from this request ."], "references": ["get the full object from spotify with a href attribute ."], "bleu": 0.16481400866629634, "rouge_l": 0.3070469798657718}
{"id": 1684, "code": "def update code urls ( self ) : to ignore = [ \".gitignore\" , \".keep\" ] for root , , files in Py Funceble . walk ( Py Funceble . CURRENT DIRECTORY + Py Funceble . directory separator + \"Py Funceble\" + Py Funceble . directory separator ) : for file in files : if file not in to ignore and \" pycache \" not in root : if root . endswith ( Py Funceble . directory separator ) : self . update docs ( root + file ) else : self . update docs ( root + Py Funceble . directory separator + file ) for root , , files in Py Funceble . walk ( Py Funceble . CURRENT DIRECTORY + Py Funceble . directory separator + \"tests\" + Py Funceble . directory separator ) : for file in files : if file not in to ignore and \" pycache \" not in root : if root . endswith ( Py Funceble . directory separator ) : self . update docs ( root + file ) else : self . update docs ( root + Py Funceble . directory separator + file )", "predictions": ["( ( . update urls with this instance of code ."], "references": ["read the code and update all links ."], "bleu": 0.1354599427337814, "rouge_l": 0.216696269982238}
{"id": 1685, "code": "def is version greater ( self ) : checked = Version ( True ) . check versions ( self . current version [ 0 ] , self . version yaml ) if checked is not None and not checked : return True return False", "predictions": ["check if a given version is version ."], "references": ["check if the current version is greater as the older older one ."], "bleu": 0.14867523320266893, "rouge_l": 0.45658682634730546}
{"id": 1686, "code": "def is dev version ( cls ) : command = \"git branch\" command result = Command ( command ) . execute ( ) for branch in command result . split ( \"\\n\" ) : if branch . startswith ( \"*\" ) and \"dev\" in branch : return True return False", "predictions": ["checks if the update specified by the user has the given version ."], "references": ["check if the current branch is dev ."], "bleu": 0.1350862565735141, "rouge_l": 0.2985318107667211}
{"id": 1687, "code": "def does require deprecation ( self ) : for index , version number in enumerate ( self . current version [ 0 ] [ : 2 ] ) : if version number > self . version yaml [ index ] : return True return False", "predictions": ["only allow one of the other threads to be available ."], "references": ["check if we have to put the previous version into the deprecated list ."], "bleu": 0.10312570678516415, "rouge_l": 0.15661103979460847}
{"id": 1688, "code": "def backup ( self ) : if Py Funceble . CONFIGURATION [ \"auto continue\" ] : data to backup = { } configuration counter = Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] data to backup [ Py Funceble . INTERN [ \"file to test\" ] ] = { \"tested\" : configuration counter [ \"tested\" ] , \"up\" : configuration counter [ \"up\" ] , \"down\" : configuration counter [ \"down\" ] , \"invalid\" : configuration counter [ \"invalid\" ] , } to save = { } to save . update ( self . backup content ) to save . update ( data to backup ) Dict ( to save ) . to json ( self . autocontinue log file )", "predictions": ["backup the data of this object ."], "references": ["backup the current execution state ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 1689, "code": "def restore ( self ) : if Py Funceble . CONFIGURATION [ \"auto continue\" ] and self . backup content : file to restore = Py Funceble . INTERN [ \"file to test\" ] if file to restore in self . backup content : to initiate = [ \"up\" , \"down\" , \"invalid\" , \"tested\" ] alternatives = { \"up\" : \"number of up\" , \"down\" : \"number of down\" , \"invalid\" : \"number of invalid\" , \"tested\" : \"number of tested\" , } for string in to initiate : try : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] . update ( { string : self . backup content [ file to restore ] [ string ] } ) except Key Error : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] . update ( { string : self . backup content [ file to restore ] [ alternatives [ string ] ] } )", "predictions": ["save and inline the ( ."], "references": ["restore data from the given path ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1690, "code": "def stay safe ( ) : random = int ( choice ( str ( int ( time ( ) ) ) ) ) if not CONFIGURATION [ \"quiet\" ] and random % 3 == 0 : print ( \"\\n\" + Fore . GREEN + Style . BRIGHT + \"Thanks for using Py Funceble!\" ) print ( Fore . YELLOW + Style . BRIGHT + \"Share your experience on \" + Fore . CYAN + \"Twitter\" + Fore . YELLOW + \" with \" + Fore . CYAN + \"#Py Funceble\" + Fore . YELLOW + \"!\" ) print ( Fore . GREEN + Style . BRIGHT + \"Have a feedback, an issue or an improvement idea ?\" ) print ( Fore . YELLOW + Style . BRIGHT + \"Let us know on \" + Fore . CYAN + \"Git Hub\" + Fore . YELLOW + \"!\" )", "predictions": ["generate an . for the ("], "references": ["print a friendly message ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1691, "code": "def entry management url ( self ) : if ( self . url file and not self . entry management url download ( self . url file ) ) : Py Funceble . INTERN [ \"file to test\" ] = self . url file", "predictions": ["get the management for this url ."], "references": ["manage the loading of the url system ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 1692, "code": "def print header ( cls ) : if ( not Py Funceble . CONFIGURATION [ \"quiet\" ] and not Py Funceble . CONFIGURATION [ \"header printed\" ] ) : print ( \"\\n\" ) if Py Funceble . CONFIGURATION [ \"less\" ] : Prints ( None , \"Less\" ) . header ( ) else : Prints ( None , \"Generic\" ) . header ( ) Py Funceble . CONFIGURATION [ \"header printed\" ] = True", "predictions": ["prints all the ( objects ."], "references": ["decide if we print or not the header ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1693, "code": "def handle ( self ) : source = \"URL\" if self . catched . lower ( ) not in Py Funceble . STATUS [ \"list\" ] [ \"invalid\" ] : Generate ( self . catched , source ) . status file ( ) else : Generate ( self . catched , \"SYNTAX\" ) . status file ( ) return self . catched", "predictions": ["handle ( on this sender ."], "references": ["handle the backend of the given status ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 1694, "code": "def delete uneeded ( self ) : structure = self . get structure ( ) list of key = list ( structure . keys ( ) ) structure = structure [ list of key [ 0 ] ] parent path = list of key [ 0 ] if not parent path . endswith ( Py Funceble . directory separator ) : parent path += Py Funceble . directory separator for root , , in Py Funceble . walk ( parent path ) : root = Directory ( root ) . fix path ( ) if root . replace ( parent path , \"\" ) not in structure : Py Funceble . rmtree ( root )", "predictions": ["deletes all edges of the specified key and all its associated ( ."], "references": ["delete the directory which are not registered into our structure ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 1695, "code": "def load config file ( self ) : try : Py Funceble . CONFIGURATION . update ( Dict . from yaml ( File ( self . path to config ) . read ( ) ) ) self . install iana config ( ) self . install psl config ( ) self . install directory structure file ( ) except File Not Found Error as exception : if Py Funceble . path . isfile ( self . path to default config ) : File ( self . path to default config ) . copy ( self . path to config ) self . load config file ( ) else : raise exception", "predictions": ["loads configuration from a file located in the configuration ."], "references": ["load . pyfunceble . yaml into the system ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 1696, "code": "def install iana config ( cls ) : iana link = Py Funceble . CONFIGURATION [ \"links\" ] [ \"iana\" ] iana link = Version ( True ) . right url from version ( iana link ) destination = Py Funceble . CURRENT DIRECTORY + \"iana-domains-db.json\" if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : return Download ( iana link , destination ) . text ( ) return None", "predictions": ["installs the ( and only y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y"], "references": ["download iana - domains - db . json if not present ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1697, "code": "def install psl config ( cls ) : psl link = Py Funceble . CONFIGURATION [ \"links\" ] [ \"psl\" ] psl link = Version ( True ) . right url from version ( psl link ) destination = ( Py Funceble . CURRENT DIRECTORY + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"public suffix\" ] ) if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : return Download ( psl link , destination ) . text ( ) return None", "predictions": ["installs the class and get the installation installation ."], "references": ["download public - suffix . json if not present ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1698, "code": "def install directory structure file ( cls ) : dir structure link = Py Funceble . CONFIGURATION [ \"links\" ] [ \"dir structure\" ] dir structure link = Version ( True ) . right url from version ( dir structure link ) destination = ( Py Funceble . CURRENT DIRECTORY + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"dir structure\" ] ) if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : data = Download ( dir structure link , destination , return data = True ) . text ( ) File ( destination ) . write ( data , overwrite = True ) return True return None", "predictions": ["cross cross fit for a given ( ."], "references": ["download the latest version of dir_structure_production . json ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1699, "code": "def merge values ( self ) : to remove = [ ] self . new config = Dict ( Dict ( self . upstream config ) . merge ( Py Funceble . CONFIGURATION ) ) . remove key ( to remove )", "predictions": ["merges this map with the same dataset ."], "references": ["simply merge the older into the new one ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1700, "code": "def load ( self ) : if \"PYFUNCEBLE AUTO CONFIGURATION\" not in Py Funceble . environ : while True : response = input ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . RED + \"A configuration key is missing.\\n\" + Py Funceble . Fore . RESET + \"Try to merge upstream configuration file into %s ? [y/n] \" % ( Py Funceble . Style . BRIGHT + self . path to config + Py Funceble . Style . RESET ALL ) ) if isinstance ( response , str ) : if response . lower ( ) == \"y\" : self . merge values ( ) self . save ( ) print ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . GREEN + \"Done!\\n\" \"Please try again, if it happens again,\" \" please fill a new issue.\" ) break elif response . lower ( ) == \"n\" : raise Exception ( \"Configuration key still missing.\" ) else : self . merge values ( ) self . save ( )", "predictions": ["parses the given ( and converts it to the . ."], "references": ["execute the logic behind the merging ."], "bleu": 0.1354599427337814, "rouge_l": 0.3472485768500949}
{"id": 1701, "code": "def handle non existant index ( cls ) : try : Py Funceble . INTERN [ \"http code\" ] except Key Error : Py Funceble . INTERN [ \"http code\" ] = \"*\" * 3 try : Py Funceble . INTERN [ \"referer\" ] except Key Error : Py Funceble . INTERN [ \"referer\" ] = \"Unknown\"", "predictions": ["pearson support for handling . exceptions ."], "references": ["handle and check that some configuration index exists ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1702, "code": "def status file ( self ) : if \"file to test\" in Py Funceble . INTERN : Generate ( self . domain status , self . source , self . expiration date ) . info files ( ) Percentage ( self . domain status ) . count ( ) self . prints status screen ( ) if self . do not produce file ( ) : return None if ( not Py Funceble . CONFIGURATION [ \"no files\" ] and Py Funceble . CONFIGURATION [ \"split\" ] ) : self . prints status file ( ) else : self . unified file ( )", "predictions": ["creates a ( possibly created filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename"], "references": ["generate a file according to the domain status ."], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 1703, "code": "def load ( self ) : if not Py Funceble . INTERN [ \"psl db\" ] : Py Funceble . INTERN [ \"psl db\" ] = Dict ( ) . from json ( File ( self . destination ) . read ( ) )", "predictions": ["save data from the given file object"], "references": ["load the public suffix database into the system ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1704, "code": "def load ( self ) : if \"iana db\" not in Py Funceble . INTERN or not Py Funceble . INTERN [ \"iana db\" ] : Py Funceble . INTERN [ \"iana db\" ] = self . iana db", "predictions": ["get the list of java objects to the database ."], "references": ["initiate the iana database if it is not the case ."], "bleu": 0.1434272783816789, "rouge_l": 0.28328173374613}
{"id": 1705, "code": "def update ( self ) : if not Py Funceble . CONFIGURATION [ \"quiet\" ] : print ( \"Update of iana-domains-db\" , end = \" \" ) for extension , referer in self . extensions ( ) : if extension not in self . iana db or self . iana db [ extension ] != referer : self . iana db [ extension ] = referer Dict ( self . iana db ) . to json ( self . destination ) if not Py Funceble . CONFIGURATION [ \"quiet\" ] : print ( Py Funceble . INTERN [ \"done\" ] )", "predictions": ["updates an iana with the most recent field ' s list ."], "references": ["update the content of the iana - domains - db file ."], "bleu": 0.1235622127262679, "rouge_l": 0.16666666666666666}
{"id": 1706, "code": "def retrieve ( self ) : if Py Funceble . CONFIGURATION [ \"mining\" ] : if \"mined\" not in Py Funceble . INTERN : Py Funceble . INTERN [ \"mined\" ] = { } if Py Funceble . path . isfile ( self . file ) : data = Dict ( ) . from json ( File ( self . file ) . read ( ) ) for file path in data : Py Funceble . INTERN [ \"mined\" ] [ file path ] = { } for element in data [ file path ] : if data [ file path ] [ element ] : Py Funceble . INTERN [ \"mined\" ] [ file path ] [ element ] = data [ file path ] [ element ] return Py Funceble . INTERN [ \"mined\" ] = { } return", "predictions": ["xyz function for ( ."], "references": ["retrieve the mining informations ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1707, "code": "def backup ( self ) : if Py Funceble . CONFIGURATION [ \"mining\" ] : Dict ( Py Funceble . INTERN [ \"mined\" ] ) . to json ( self . file )", "predictions": ["save a dtype object from a file ."], "references": ["backup the mined informations ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1708, "code": "def process ( self ) : if Py Funceble . CONFIGURATION [ \"mining\" ] : mined = self . mine ( ) if mined : self . add ( mined ) self . backup ( )", "predictions": ["dict for json object ."], "references": ["process the logic and structuration of the mining database ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 1709, "code": "def json print ( self ) : if self . output : if Py Funceble . path . isfile ( self . output ) : content = Dict ( ) . from json ( File ( self . output ) . read ( ) ) if isinstance ( content , list ) : content . extend ( self . data to print ) content = List ( content ) . custom format ( Sort . standard ) if Py Funceble . CONFIGURATION [ \"hierarchical sorting\" ] : content = List ( content ) . custom format ( Sort . hierarchical ) Dict ( content ) . to json ( self . output ) else : raise Exception ( \"Output not correctly formatted.\" ) else : # Dict ( self . data to print ) . to json ( self . output ) else : raise Exception ( \"Empty output given.\" )", "predictions": ["casts a def operations on the readme object ."], "references": ["management of the json template ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1710, "code": "def file to delete ( cls ) : directory = Py Funceble . OUTPUT DIRECTORY + Py Funceble . OUTPUTS [ \"parent directory\" ] if not directory . endswith ( Py Funceble . directory separator ) : directory += Py Funceble . directory separator result = [ ] for root , , files in Py Funceble . walk ( directory ) : for file in files : if file not in [ \".gitignore\" , \".keep\" ] : if root . endswith ( Py Funceble . directory separator ) : result . append ( root + file ) else : result . append ( root + Py Funceble . directory separator + file ) return result", "predictions": ["returns all the files found in a msg msg ."], "references": ["return the list of file to delete ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 1711, "code": "def databases to delete ( cls ) : directory = Py Funceble . CURRENT DIRECTORY result = [ ] result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"dir structure\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"iana\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"public suffix\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"inactive db\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"mining\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"whois db\" ] ) return result", "predictions": ["convert the given from the given list of from the given from the given from the given from the given from this kwargs"], "references": ["set the databases files to delete ."], "bleu": 0.05291907393644996, "rouge_l": 0.07376058041112453}
{"id": 1712, "code": "def get ( self ) : result = { } if self . algorithm in self . valid algorithms : if self . algorithm == \"all\" : del self . valid algorithms [ 0 ] for algo in self . valid algorithms : if self . path and path . isfile ( self . path ) : result [ algo ] = self . hash file ( algo ) elif self . data : result [ algo ] = self . hash data ( algo ) else : return None else : if self . path and path . isfile ( self . path ) : result [ self . algorithm ] = self . hash file ( self . algorithm ) elif self . data : result [ self . algorithm ] = self . hash data ( self . algorithm ) else : return None else : return None if self . algorithm != \"all\" and self . only hash : return result [ self . algorithm ] return result", "predictions": ["url - get a } from an instantiate scope ."], "references": ["return the hash of the given file"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1713, "code": "def count ( self ) : if self . status : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] += 1 if ( self . status . lower ( ) in Py Funceble . STATUS [ \"list\" ] [ \"up\" ] or self . status . lower ( ) in Py Funceble . STATUS [ \"list\" ] [ \"valid\" ] ) : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] += 1 elif self . status . lower ( ) in Py Funceble . STATUS [ \"list\" ] [ \"down\" ] : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] += 1 else : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] += 1", "predictions": ["indicate that we have to mutate the { { ( , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , ."], "references": ["count the number of domain for each status ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 1714, "code": "def calculate ( cls ) : percentages = { \"up\" : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , \"down\" : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , \"invalid\" : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , } for percentage in percentages : calculation = ( percentages [ percentage ] * 100 // Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] ) Py Funceble . INTERN [ \"counter\" ] [ \"percentage\" ] . update ( { percentage : calculation } )", "predictions": ["determine whether or not this class should be used for a set of instances ."], "references": ["calculate the percentage of each status ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 1715, "code": "def log ( self ) : if ( Py Funceble . CONFIGURATION [ \"show percentage\" ] and Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 ) : output = ( Py Funceble . OUTPUT DIRECTORY + Py Funceble . OUTPUTS [ \"parent directory\" ] + Py Funceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + Py Funceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"percentage\" ] + Py Funceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"percentage\" ] ) File ( output ) . delete ( ) self . calculate ( ) if not Py Funceble . CONFIGURATION [ \"quiet\" ] : print ( \"\\n\" ) Prints ( None , \"Percentage\" , output ) . header ( ) lines to print = [ [ Py Funceble . STATUS [ \"official\" ] [ \"up\" ] , str ( Py Funceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] ) + \"%\" , Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , ] , [ Py Funceble . STATUS [ \"official\" ] [ \"down\" ] , str ( Py Funceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"down\" ] ) + \"%\" , Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , ] , [ Py Funceble . STATUS [ \"official\" ] [ \"invalid\" ] , str ( Py Funceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"invalid\" ] ) + \"%\" , Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , ] , ] if Py Funceble . CONFIGURATION [ \"syntax\" ] : lines to print [ 0 ] [ 0 ] = Py Funceble . STATUS [ \"official\" ] [ \"valid\" ] del lines to print [ 1 ] for to print in lines to print : Prints ( to print , \"Percentage\" , output ) . data ( ) elif Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 : self . calculate ( )", "predictions": ["def def ( ( ( la ( de ( de un ( de ( ( ( ( ( ("], "references": ["print on screen and on file the percentages for each status ."], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 1716, "code": "def reformat historical formating error ( self ) : if Py Funceble . CONFIGURATION [ \"inactive database\" ] : historical formating error = ( Py Funceble . CURRENT DIRECTORY + \"inactive-db.json\" ) if Py Funceble . path . isfile ( historical formating error ) : data = Dict ( ) . from json ( File ( historical formating error ) . read ( ) ) data to parse = { } top keys = data . keys ( ) for top key in top keys : low keys = data [ top key ] . keys ( ) data to parse [ top key ] = { } for low key in low keys : if low key . isdigit ( ) : data to parse [ top key ] [ int ( low key ) - ( self . one day in seconds * 30 ) ] = data [ top key ] [ low key ] else : data to parse [ top key ] [ int ( Py Funceble . time ( ) ) - ( self . one day in seconds * 30 ) ] = data [ top key ] [ low key ] if \"inactive db\" in Py Funceble . INTERN : Py Funceble . INTERN [ \"inactive db\" ] . update ( data to parse ) else : Py Funceble . INTERN [ \"inactive db\" ] = data to parse File ( historical formating error ) . delete ( )", "predictions": ["update the code to show for a code ."], "references": ["format the old format so it can be merged into the newer format ."], "bleu": 0.08961856124931385, "rouge_l": 0.1673525377229081}
{"id": 1717, "code": "def retrieve ( self ) : if Py Funceble . CONFIGURATION [ \"inactive database\" ] : self . reformat historical formating error ( ) if Py Funceble . path . isfile ( self . inactive db path ) : self . merge ( )", "predictions": ["is the [ 0 , 1 ] from the database ."], "references": ["return the current content of the inactive - db . json file ."], "bleu": 0.11294012253658708, "rouge_l": 0.24629878869448185}
{"id": 1718, "code": "def backup ( self ) : if Py Funceble . CONFIGURATION [ \"inactive database\" ] : Dict ( Py Funceble . INTERN [ \"inactive db\" ] ) . to json ( self . inactive db path )", "predictions": ["create a is no longer used ."], "references": ["save the current database into the inactive - db . json file ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 1719, "code": "def is present ( cls ) : if Py Funceble . CONFIGURATION [ \"inactive database\" ] : if Py Funceble . INTERN [ \"to test\" ] in Py Funceble . INTERN [ \"flatten inactive db\" ] or ( Py Funceble . INTERN [ \"file to test\" ] in Py Funceble . INTERN [ \"inactive db\" ] and Py Funceble . INTERN [ \"inactive db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] and \"to test\" in Py Funceble . INTERN [ \"inactive db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] and Py Funceble . INTERN [ \"to test\" ] in Py Funceble . INTERN [ \"inactive db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ \"to test\" ] ) : return True return False", "predictions": ["checks if the given class is present in the package list ."], "references": ["check if the currently tested element is into the database ."], "bleu": 0.16261701715194898, "rouge_l": 0.43821839080459773}
{"id": 1720, "code": "def retrieve ( self ) : if self . authorization ( ) and \"whois db\" not in Py Funceble . INTERN : if Py Funceble . path . isfile ( self . whois db path ) : Py Funceble . INTERN [ \"whois db\" ] = Dict ( ) . from json ( File ( self . whois db path ) . read ( ) ) else : Py Funceble . INTERN [ \"whois db\" ] = { }", "predictions": ["retrieves full configuration from database ."], "references": ["retrieve the data from the database ."], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 1721, "code": "def backup ( self ) : if self . authorization ( ) : Dict ( Py Funceble . INTERN [ \"whois db\" ] ) . to json ( self . whois db path )", "predictions": ["for the given path ."], "references": ["backup the database into its file ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 1722, "code": "def is in database ( self ) : if ( self . authorization ( ) and Py Funceble . INTERN [ \"file to test\" ] in Py Funceble . INTERN [ \"whois db\" ] and Py Funceble . INTERN [ \"to test\" ] in Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] ) : return True return False", "predictions": ["a method to decide if this ( i . e . , a full ( i . e . one random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random"], "references": ["check if the element is into the database ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 1723, "code": "def is time older ( self ) : if ( self . authorization ( ) and self . is in database ( ) and int ( Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] [ \"epoch\" ] ) < int ( Py Funceble . time ( ) ) ) : return True return False", "predictions": ["check if this object is . ."], "references": ["check if the current time is older than the one in the database ."], "bleu": 0.10218289380194193, "rouge_l": 0.35935198821796754}
{"id": 1724, "code": "def add ( self ) : if self . authorization ( ) : if self . epoch < int ( Py Funceble . time ( ) ) : state = \"past\" else : state = \"future\" if self . is in database ( ) : if ( str ( self . epoch ) != Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] [ \"epoch\" ] ) : Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] . update ( { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration date\" : self . expiration date , } ) elif self . is time older ( ) : if ( Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] [ \"state\" ] != \"past\" ) : Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] . update ( { \"state\" : \"past\" } ) elif ( Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] [ \"state\" ] != \"future\" ) : Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] . update ( { \"state\" : \"future\" } ) else : if ( not Py Funceble . INTERN [ \"file to test\" ] in Py Funceble . INTERN [ \"whois db\" ] ) : Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] = { } Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] . update ( { Py Funceble . INTERN [ \"to test\" ] : { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration date\" : self . expiration date , } } ) self . backup ( )", "predictions": ["we only want to print the gradient for the async async version ."], "references": ["add the currently tested element into the database ."], "bleu": 0.1135935489027116, "rouge_l": 0.2819722650231125}
{"id": 1725, "code": "def travis permissions ( cls ) : if Py Funceble . CONFIGURATION [ \"travis\" ] : try : build dir = Py Funceble . environ [ \"TRAVIS BUILD DIR\" ] commands = [ \"sudo chown -R travis:travis %s\" % ( build dir ) , \"sudo chgrp -R travis %s\" % ( build dir ) , \"sudo chmod -R g+rw X %s\" % ( build dir ) , \"sudo chmod 777 -Rf %s.git\" % ( build dir + Py Funceble . directory separator ) , r\"sudo find %s -type d -exec chmod g+x '{}' \\;\" % ( build dir ) , ] for command in commands : Command ( command ) . execute ( ) if Command ( \"git config core.shared Repository\" ) . execute ( ) == \"\" : Command ( \"git config core.shared Repository group\" ) . execute ( ) except Key Error : pass", "predictions": ["builds the command and builds all its dependencies ."], "references": ["set permissions in order to avoid issues before commiting ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1726, "code": "def travis ( self ) : if Py Funceble . CONFIGURATION [ \"travis\" ] : try : = Py Funceble . environ [ \"TRAVIS BUILD DIR\" ] time autorisation = False try : time autorisation = int ( Py Funceble . time ( ) ) >= int ( Py Funceble . INTERN [ \"start\" ] ) + ( int ( Py Funceble . CONFIGURATION [ \"travis autosave minutes\" ] ) * 60 ) except Key Error : if self . last and not self . bypass : raise Exception ( \"Please review the way `Execution Time()` is called.\" ) if self . last or time autorisation or self . bypass : Percentage ( ) . log ( ) self . travis permissions ( ) command = 'git add --all && git commit -a -m \"%s\"' if self . last or self . bypass : if Py Funceble . CONFIGURATION [ \"command before end\" ] : for line in Command ( Py Funceble . CONFIGURATION [ \"command before end\" ] ) . run ( ) : sys stdout . write ( \"{}\\n\" . format ( line ) ) self . travis permissions ( ) message = ( Py Funceble . CONFIGURATION [ \"travis autosave final commit\" ] + \" [ci skip]\" ) Command ( command % message ) . execute ( ) else : if Py Funceble . CONFIGURATION [ \"command\" ] : for line in Command ( Py Funceble . CONFIGURATION [ \"command\" ] ) . run ( ) : sys stdout . write ( \"{}\\n\" . format ( line ) ) self . travis permissions ( ) Command ( command % Py Funceble . CONFIGURATION [ \"travis autosave commit\" ] ) . execute ( ) print ( Command ( \"git push origin %s\" % Py Funceble . CONFIGURATION [ \"travis branch\" ] ) . execute ( ) ) exit ( 0 ) except Key Error : pass", "predictions": ["delete the command line ."], "references": ["logic behind autosave under travis ci ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1727, "code": "def nslookup ( cls ) : try : if \"current test data\" in Py Funceble . INTERN : if not Check ( ) . is ip valid ( ) : request = Py Funceble . socket . getaddrinfo ( Py Funceble . INTERN [ \"to test\" ] , 80 , 0 , 0 , Py Funceble . socket . IPPROTO TCP , ) for sequence in request : Py Funceble . INTERN [ \"current test data\" ] [ \"nslookup\" ] . append ( sequence [ - 1 ] [ 0 ] ) else : request = Py Funceble . socket . gethostbyaddr ( Py Funceble . INTERN [ \"to test\" ] ) Py Funceble . INTERN [ \"current test data\" ] [ \"nslookup\" ] [ \"hostname\" ] = request [ 0 ] Py Funceble . INTERN [ \"current test data\" ] [ \"nslookup\" ] [ \"aliases\" ] = request [ 1 ] Py Funceble . INTERN [ \"current test data\" ] [ \"nslookup\" ] [ \"ips\" ] = request [ 2 ] else : if not Check ( ) . is ip valid ( ) : Py Funceble . socket . getaddrinfo ( Py Funceble . INTERN [ \"to test\" ] , 80 , 0 , 0 , Py Funceble . socket . IPPROTO TCP , ) else : Py Funceble . socket . gethostbyaddr ( Py Funceble . INTERN [ \"to test\" ] ) return True except ( OS Error , Py Funceble . socket . herror , Py Funceble . socket . gaierror ) : return False", "predictions": ["verify whether the coordinate is a vpf curve for the given bytearray ."], "references": ["implementation of unix nslookup ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 1728, "code": "def get ( self ) : if not Py Funceble . CONFIGURATION [ \"local\" ] : if self . domain extension not in self . ignored extension : referer = None if self . domain extension in Py Funceble . INTERN [ \"iana db\" ] : if not Py Funceble . CONFIGURATION [ \"no whois\" ] : referer = Py Funceble . INTERN [ \"iana db\" ] [ self . domain extension ] if not referer : Logs ( ) . referer not found ( self . domain extension ) return None return referer return None return False return None return None", "predictions": ["get extension for domain ."], "references": ["return the referer aka the whois server of the current domain extension ."], "bleu": 0.06554932163900559, "rouge_l": 0.20573355817875214}
{"id": 1729, "code": "def standard paths ( ) : for is plat spec in [ True , False ] : path = distutils . sysconfig . get python lib ( standard lib = True , plat specific = is plat spec ) for name in os . listdir ( path ) : yield name try : for name in os . listdir ( os . path . join ( path , 'lib-dynload' ) ) : yield name except OS Error : pass", "predictions": ["generate an iterator of all the paths for this module ."], "references": ["yield paths to standard modules ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 1730, "code": "def standard package names ( ) : for name in standard paths ( ) : if name . startswith ( ' ' ) or '-' in name : continue if '.' in name and name . rsplit ( '.' ) [ - 1 ] not in [ 'so' , 'py' , 'pyc' ] : continue yield name . split ( '.' ) [ 0 ]", "predictions": ["yield pairs for all standard standard standard standard standard standard modes ."], "references": ["yield standard module names ."], "bleu": 0.1235622127262679, "rouge_l": 0.38125000000000003}
{"id": 1731, "code": "def unused import line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Unused Import ) : yield message . lineno", "predictions": ["import a line from the messages array ."], "references": ["yield line numbers of unused imports ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1732, "code": "def unused import module name ( messages ) : pattern = r'\\'(.+?)\\'' for message in messages : if isinstance ( message , pyflakes . messages . Unused Import ) : module name = re . search ( pattern , str ( message ) ) module name = module name . group ( ) [ 1 : - 1 ] if module name : yield ( message . lineno , module name )", "predictions": ["read from the messages that match the given pattern ."], "references": ["yield line number and module name of unused imports ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 1733, "code": "def star import used line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Import Star Used ) : yield message . lineno", "predictions": ["import a star from a message ."], "references": ["yield line number of star import usage ."], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 1734, "code": "def star import usage undefined name ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Import Star Usage ) : undefined name = message . message args [ 0 ] module name = message . message args [ 1 ] yield ( message . lineno , undefined name , module name )", "predictions": ["this method extracts all the messages from the import of the given messages ."], "references": ["yield line number undefined name and its possible origin module ."], "bleu": 0.08839374326825923, "rouge_l": 0.08176943699731902}
{"id": 1735, "code": "def unused variable line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Unused Variable ) : yield message . lineno", "predictions": ["create a new unused line ."], "references": ["yield line numbers of unused variables ."], "bleu": 0.22236312185643822, "rouge_l": 0.3034825870646766}
{"id": 1736, "code": "def duplicate key line numbers ( messages , source ) : messages = [ message for message in messages if isinstance ( message , pyflakes . messages . Multi Value Repeated Key Literal ) ] if messages : key to messages = create key to messages dict ( messages ) lines = source . split ( '\\n' ) for ( key , messages ) in key to messages . items ( ) : good = True for message in messages : line = lines [ message . lineno - 1 ] key = message . message args [ 0 ] if not dict entry has key ( line , key ) : good = False if good : for message in messages : yield message . lineno", "predictions": ["yield a sequence of pairs ."], "references": ["yield line numbers of duplicate keys ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 1737, "code": "def create key to messages dict ( messages ) : dictionary = collections . defaultdict ( lambda : [ ] ) for message in messages : dictionary [ message . message args [ 0 ] ] . append ( message ) return dictionary", "predictions": ["returns a list with keys appended to the dictionary ."], "references": ["return dict mapping the key to list of messages ."], "bleu": 0.15851165692617156, "rouge_l": 0.2}
{"id": 1738, "code": "def check ( source ) : if sys . version info [ 0 ] == 2 and isinstance ( source , unicode ) : try : source = source . encode ( 'utf-8' ) except Unicode Error : return [ ] reporter = List Reporter ( ) try : pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) except ( Attribute Error , Recursion Error , Unicode Decode Error ) : pass return reporter . messages", "predictions": ["performs all checks on the given source ."], "references": ["return messages from pyflakes ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1739, "code": "def extract package name ( line ) : assert '\\\\' not in line assert '(' not in line assert ')' not in line assert ';' not in line if line . lstrip ( ) . startswith ( ( 'import' , 'from' ) ) : word = line . split ( ) [ 1 ] else : return None package = word . split ( '.' ) [ 0 ] assert ' ' not in package return package", "predictions": ["extract the package name from a word ."], "references": ["return package name in import statement ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 1740, "code": "def multiline import ( line , previous line = '' ) : for symbol in '()' : if symbol in line : return True if line . lstrip ( ) . startswith ( '>' ) : return True return multiline statement ( line , previous line )", "predictions": ["imports configuration from specified line ."], "references": ["return true if import is spans multiples lines ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1741, "code": "def multiline statement ( line , previous line = '' ) : for symbol in '\\\\:;' : if symbol in line : return True sio = io . String IO ( line ) try : list ( tokenize . generate tokens ( sio . readline ) ) return previous line . rstrip ( ) . endswith ( '\\\\' ) except ( Syntax Error , tokenize . Token Error ) : return True", "predictions": ["takes care of multiline and evaluates additional links ."], "references": ["return true if this is part of a multiline statement ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 1742, "code": "def break up import ( line ) : assert '\\\\' not in line assert '(' not in line assert ')' not in line assert ';' not in line assert '#' not in line assert not line . lstrip ( ) . startswith ( 'from' ) newline = get line ending ( line ) if not newline : return line ( indentation , imports ) = re . split ( pattern = r'\\bimport\\b' , string = line , maxsplit = 1 ) indentation += 'import ' assert newline return '' . join ( [ indentation + i . strip ( ) + newline for i in sorted ( imports . split ( ',' ) ) ] )", "predictions": ["strip imports from text ."], "references": ["return line with imports on separate lines ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1743, "code": "def filter code ( source , additional imports = None , expand star imports = False , remove all unused imports = False , remove duplicate keys = False , remove unused variables = False , ignore init module imports = False , ) : imports = SAFE IMPORTS if additional imports : imports |= frozenset ( additional imports ) del additional imports messages = check ( source ) if ignore init module imports : marked import line numbers = frozenset ( ) else : marked import line numbers = frozenset ( unused import line numbers ( messages ) ) marked unused module = collections . defaultdict ( lambda : [ ] ) for line number , module name in unused import module name ( messages ) : marked unused module [ line number ] . append ( module name ) if expand star imports and not ( re . search ( r'\\b all \\b' , source ) or re . search ( r'\\bdel\\b' , source ) ) : marked star import line numbers = frozenset ( star import used line numbers ( messages ) ) if len ( marked star import line numbers ) > 1 : marked star import line numbers = frozenset ( ) else : undefined names = [ ] for line number , undefined name , in star import usage undefined name ( messages ) : undefined names . append ( undefined name ) if not undefined names : marked star import line numbers = frozenset ( ) else : marked star import line numbers = frozenset ( ) if remove unused variables : marked variable line numbers = frozenset ( unused variable line numbers ( messages ) ) else : marked variable line numbers = frozenset ( ) if remove duplicate keys : marked key line numbers = frozenset ( duplicate key line numbers ( messages , source ) ) else : marked key line numbers = frozenset ( ) line messages = get messages by line ( messages ) sio = io . String IO ( source ) previous line = '' for line number , line in enumerate ( sio . readlines ( ) , start = 1 ) : if '#' in line : yield line elif line number in marked import line numbers : yield filter unused import ( line , unused module = marked unused module [ line number ] , remove all unused imports = remove all unused imports , imports = imports , previous line = previous line ) elif line number in marked variable line numbers : yield filter unused variable ( line ) elif line number in marked key line numbers : yield filter duplicate key ( line , line messages [ line number ] , line number , marked key line numbers , source ) elif line number in marked star import line numbers : yield filter star import ( line , undefined names ) else : yield line previous line = line", "predictions": ["filters all messages that do not match the source ."], "references": ["yield code with unused imports removed ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 1744, "code": "def get messages by line ( messages ) : line messages = { } for message in messages : line messages [ message . lineno ] = message return line messages", "predictions": ["this method returns the list of messages that were passed in ."], "references": ["return dictionary that maps line number to message ."], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 1745, "code": "def filter star import ( line , marked star import undefined name ) : undefined name = sorted ( set ( marked star import undefined name ) ) return re . sub ( r'\\*' , ', ' . join ( undefined name ) , line )", "predictions": ["filters all marked items and returns them ."], "references": ["return line with the star import expanded ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1746, "code": "def filter unused import ( line , unused module , remove all unused imports , imports , previous line = '' ) : if multiline import ( line , previous line ) : return line is from import = line . lstrip ( ) . startswith ( 'from' ) if ',' in line and not is from import : return break up import ( line ) package = extract package name ( line ) if not remove all unused imports and package not in imports : return line if ',' in line : assert is from import return filter from import ( line , unused module ) else : return ( get indentation ( line ) + 'pass' + get line ending ( line ) )", "predictions": ["removes unused unused items from the beginning of the module ."], "references": ["return line if used otherwise return none ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1747, "code": "def filter unused variable ( line , previous line = '' ) : if re . match ( EXCEPT REGEX , line ) : return re . sub ( r' as \\w+:$' , ':' , line , count = 1 ) elif multiline statement ( line , previous line ) : return line elif line . count ( '=' ) == 1 : split line = line . split ( '=' ) assert len ( split line ) == 2 value = split line [ 1 ] . lstrip ( ) if ',' in split line [ 0 ] : return line if is literal or name ( value ) : value = 'pass' + get line ending ( line ) return get indentation ( line ) + value else : return line", "predictions": ["filters to lines that have no blank indentation . this should be called after the start of the first line ."], "references": ["return line if used otherwise return none ."], "bleu": 0.06429451441231726, "rouge_l": 0.15006150061500614}
{"id": 1748, "code": "def filter duplicate key ( line , message , line number , marked line numbers , source , previous line = '' ) : if marked line numbers and line number == sorted ( marked line numbers ) [ 0 ] : return '' return line", "predictions": ["removes duplicate or unnecessary line from the cache ."], "references": ["return if first occurrence of the key otherwise return line ."], "bleu": 0.1343994460963362, "rouge_l": 0.19645732689210954}
{"id": 1749, "code": "def is literal or name ( value ) : try : ast . literal eval ( value ) return True except ( Syntax Error , Value Error ) : pass if value . strip ( ) in [ 'dict()' , 'list()' , 'set()' ] : return True return re . match ( r'^\\w+\\s*$' , value )", "predictions": ["check if this literal is of a literal literal ."], "references": ["return true if value is a literal or a name ."], "bleu": 0.17851905035930718, "rouge_l": 0.47213622291021673}
{"id": 1750, "code": "def useless pass line numbers ( source ) : sio = io . String IO ( source ) previous token type = None last pass row = None last pass indentation = None previous line = '' for token in tokenize . generate tokens ( sio . readline ) : token type = token [ 0 ] start row = token [ 2 ] [ 0 ] line = token [ 4 ] is pass = ( token type == tokenize . NAME and line . strip ( ) == 'pass' ) if ( start row - 1 == last pass row and get indentation ( line ) == last pass indentation and token type in ATOMS and not is pass ) : yield start row - 1 if is pass : last pass row = start row last pass indentation = get indentation ( line ) if ( is pass and previous token type != tokenize . INDENT and not previous line . rstrip ( ) . endswith ( '\\\\' ) ) : yield start row previous token type = token type previous line = line", "predictions": ["yield pairs of the given line . the line should be at least one of the other line ."], "references": ["yield line numbers of unneeded pass statements ."], "bleu": 0.08097785064266201, "rouge_l": 0.3197903014416776}
{"id": 1751, "code": "def filter useless pass ( source ) : try : marked lines = frozenset ( useless pass line numbers ( source ) ) except ( Syntax Error , tokenize . Token Error ) : marked lines = frozenset ( ) sio = io . String IO ( source ) for line number , line in enumerate ( sio . readlines ( ) , start = 1 ) : if line number not in marked lines : yield line", "predictions": ["removes all useless tokens from source ."], "references": ["yield code with useless pass lines removed ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1752, "code": "def get indentation ( line ) : if line . strip ( ) : non whitespace index = len ( line ) - len ( line . lstrip ( ) ) return line [ : non whitespace index ] else : return ''", "predictions": ["get indentation ."], "references": ["return leading whitespace ."], "bleu": 0.38498150077635496, "rouge_l": 0.2785388127853881}
{"id": 1753, "code": "def get line ending ( line ) : non whitespace index = len ( line . rstrip ( ) ) - len ( line ) if not non whitespace index : return '' else : return line [ non whitespace index : ]", "predictions": ["get line with spaces ."], "references": ["return line ending ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1754, "code": "def fix code ( source , additional imports = None , expand star imports = False , remove all unused imports = False , remove duplicate keys = False , remove unused variables = False , ignore init module imports = False ) : if not source : return source if 'nonlocal' in source : remove unused variables = False filtered source = None while True : filtered source = '' . join ( filter useless pass ( '' . join ( filter code ( source , additional imports = additional imports , expand star imports = expand star imports , remove all unused imports = remove all unused imports , remove duplicate keys = remove duplicate keys , remove unused variables = remove unused variables , ignore init module imports = ignore init module imports , ) ) ) ) if filtered source == source : break source = filtered source return filtered source", "predictions": ["fix imports . remove all warehouses / currentvalue from the given source ."], "references": ["return code with all filtering run on it ."], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 1755, "code": "def detect encoding ( filename , limit byte check = - 1 ) : try : with open ( filename , 'rb' ) as input file : encoding = detect encoding ( input file . readline ) with open with encoding ( filename , encoding ) as input file : input file . read ( limit byte check ) return encoding except ( Lookup Error , Syntax Error , Unicode Decode Error ) : return 'latin-1'", "predictions": ["detects encoding of the file . this is an i / o operation ."], "references": ["return file encoding ."], "bleu": 0.10511846841633776, "rouge_l": 0.2469635627530364}
{"id": 1756, "code": "def detect encoding ( readline ) : try : from lib2to3 . pgen2 import tokenize as lib2to3 tokenize encoding = lib2to3 tokenize . detect encoding ( readline ) [ 0 ] return encoding except ( Lookup Error , Syntax Error , Unicode Decode Error ) : return 'latin-1'", "predictions": ["detect if the given readline is valid ."], "references": ["return file encoding ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1757, "code": "def split comma separated ( string ) : return set ( text . strip ( ) for text in string . split ( ',' ) if text . strip ( ) )", "predictions": ["parses a string . splits out its ',' ."], "references": ["return a set of strings ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1758, "code": "def is python file ( filename ) : if filename . endswith ( '.py' ) : return True try : with open with encoding ( filename , None , limit byte check = MAX PYTHON FILE DETECTION BYTES ) as f : text = f . read ( MAX PYTHON FILE DETECTION BYTES ) if not text : return False first line = text . splitlines ( ) [ 0 ] except ( IO Error , Index Error ) : return False if not PYTHON SHEBANG REGEX . match ( first line ) : return False return True", "predictions": ["check if the file should be read ."], "references": ["return true if filename is python file ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 1759, "code": "def is exclude file ( filename , exclude ) : base name = os . path . basename ( filename ) if base name . startswith ( '.' ) : return True for pattern in exclude : if fnmatch . fnmatch ( base name , pattern ) : return True if fnmatch . fnmatch ( filename , pattern ) : return True return False", "predictions": ["checks if the file exists ."], "references": ["return true if file matches exclude pattern ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 1760, "code": "def create ( cls , name value , name type ) : if isinstance ( name value , Name . Name Value ) : value = name value elif isinstance ( name value , str ) : value = cls . Name Value ( name value ) else : name = 'Name' msg = exceptions . Error Strings . BAD EXP RECV member = 'name value' raise Type Error ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name value' , type ( Name . Name Value ) , type ( name value ) ) ) if isinstance ( name type , Name . Name Type ) : n type = name type elif isinstance ( name type , Enum ) : n type = cls . Name Type ( name type ) else : name = 'Name' msg = exceptions . Error Strings . BAD EXP RECV member = 'name type' raise Type Error ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name type' , type ( Name . Name Type ) , type ( name type ) ) ) return Name ( name value = value , name type = n type )", "predictions": ["get field value . creates and not necessarily be of error objects ."], "references": ["returns a name object populated with the given value and type"], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 1761, "code": "def get attribute from managed object ( self , managed object , attr name ) : if attr name == 'Unique Identifier' : return str ( managed object . unique identifier ) elif attr name == 'Name' : names = list ( ) for name in managed object . names : name = attributes . Name ( attributes . Name . Name Value ( name ) , attributes . Name . Name Type ( enums . Name Type . UNINTERPRETED TEXT STRING ) ) names . append ( name ) return names elif attr name == 'Object Type' : return managed object . object type elif attr name == 'Cryptographic Algorithm' : return managed object . cryptographic algorithm elif attr name == 'Cryptographic Length' : return managed object . cryptographic length elif attr name == 'Cryptographic Parameters' : return None elif attr name == 'Cryptographic Domain Parameters' : return None elif attr name == 'Certificate Type' : return managed object . certificate type elif attr name == 'Certificate Length' : return None elif attr name == 'X.509 Certificate Identifier' : return None elif attr name == 'X.509 Certificate Subject' : return None elif attr name == 'X.509 Certificate Issuer' : return None elif attr name == 'Certificate Identifier' : return None elif attr name == 'Certificate Subject' : return None elif attr name == 'Certificate Issuer' : return None elif attr name == 'Digital Signature Algorithm' : return None elif attr name == 'Digest' : return None elif attr name == 'Operation Policy Name' : return managed object . operation policy name elif attr name == 'Cryptographic Usage Mask' : return managed object . cryptographic usage masks elif attr name == 'Lease Time' : return None elif attr name == 'Usage Limits' : return None elif attr name == 'State' : return managed object . state elif attr name == 'Initial Date' : return managed object . initial date elif attr name == 'Activation Date' : return None elif attr name == 'Process Start Date' : return None elif attr name == 'Protect Stop Date' : return None elif attr name == 'Deactivation Date' : return None elif attr name == 'Destroy Date' : return None elif attr name == 'Compromise Occurrence Date' : return None elif attr name == 'Compromise Date' : return None elif attr name == 'Revocation Reason' : return None elif attr name == 'Archive Date' : return None elif attr name == 'Object Group' : return None elif attr name == 'Fresh' : return None elif attr name == 'Link' : return None elif attr name == 'Application Specific Information' : return None elif attr name == 'Contact Information' : return None elif attr name == 'Last Change Date' : return None else : return None", "predictions": ["standard ) standard ) standard ) standard ) method ."], "references": ["get the attribute value from the kmip . pie managed object ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 1762, "code": "def set attribute on managed object ( self , managed object , attribute ) : attribute name = attribute [ 0 ] attribute value = attribute [ 1 ] if self . attribute policy . is attribute multivalued ( attribute name ) : if attribute name == 'Name' : managed object . names . extend ( [ x . name value . value for x in attribute value ] ) for name in managed object . names : if managed object . names . count ( name ) > 1 : raise exceptions . Invalid Field ( \"Cannot set duplicate name values.\" ) else : raise exceptions . Invalid Field ( \"The {0} attribute is unsupported.\" . format ( attribute name ) ) else : field = None value = attribute value . value if attribute name == 'Cryptographic Algorithm' : field = 'cryptographic algorithm' elif attribute name == 'Cryptographic Length' : field = 'cryptographic length' elif attribute name == 'Cryptographic Usage Mask' : field = 'cryptographic usage masks' value = list ( ) for e in enums . Cryptographic Usage Mask : if e . value & attribute value . value : value . append ( e ) elif attribute name == 'Operation Policy Name' : field = 'operation policy name' if field : existing value = getattr ( managed object , field ) if existing value : if existing value != value : raise exceptions . Invalid Field ( \"Cannot overwrite the {0} attribute.\" . format ( attribute name ) ) else : setattr ( managed object , field , value ) else : raise exceptions . Invalid Field ( \"The {0} attribute is unsupported.\" . format ( attribute name ) )", "predictions": ["sets the package . on the first ) that can be used as a ( ( ( ( that is called without creating an existing and can be created on the ) ."], "references": ["set the attribute value on the kmip . pie managed object ."], "bleu": 0.05730192069189415, "rouge_l": 0.19411296738265713}
{"id": 1763, "code": "def validate ( self ) : if self . unique identifier is not None : if not isinstance ( self . unique identifier , attributes . Unique Identifier ) : msg = \"invalid unique identifier\" raise Type Error ( msg ) if self . compromise occurrence date is not None : if not isinstance ( self . compromise occurrence date , primitives . Date Time ) : msg = \"invalid compromise time\" raise Type Error ( msg ) if not isinstance ( self . revocation reason , objects . Revocation Reason ) : msg = \"invalid revocation reason\" raise Type Error ( msg )", "predictions": ["validates the fully qualified name ."], "references": ["error check the attributes of the activaterequestpayload object ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1764, "code": "def key wrapping data ( self , value ) : if value is None : value = { } elif not isinstance ( value , dict ) : raise Type Error ( \"Key wrapping data must be a dictionary.\" ) self . kdw wrapping method = value . get ( 'wrapping method' ) eki = value . get ( 'encryption key information' ) if eki is None : eki = { } self . kdw eki unique identifier = eki . get ( 'unique identifier' ) eki cp = eki . get ( 'cryptographic parameters' ) if eki cp is None : eki cp = { } self . kdw eki cp block cipher mode = eki cp . get ( 'block cipher mode' ) self . kdw eki cp padding method = eki cp . get ( 'padding method' ) self . kdw eki cp hashing algorithm = eki cp . get ( 'hashing algorithm' ) self . kdw eki cp key role type = eki cp . get ( 'key role type' ) self . kdw eki cp digital signature algorithm = eki cp . get ( 'digital signature algorithm' ) self . kdw eki cp cryptographic algorithm = eki cp . get ( 'cryptographic algorithm' ) self . kdw eki cp random iv = eki cp . get ( 'random iv' ) self . kdw eki cp iv length = eki cp . get ( 'iv length' ) self . kdw eki cp tag length = eki cp . get ( 'tag length' ) self . kdw eki cp fixed field length = eki cp . get ( 'fixed field length' ) self . kdw eki cp invocation field length = eki cp . get ( 'invocation field length' ) self . kdw eki cp counter length = eki cp . get ( 'counter length' ) self . kdw eki cp initial counter value = eki cp . get ( 'initial counter value' ) mski = value . get ( 'mac signature key information' ) if mski is None : mski = { } self . kdw mski unique identifier = mski . get ( 'unique identifier' ) mski cp = mski . get ( 'cryptographic parameters' ) if mski cp is None : mski cp = { } self . kdw mski cp block cipher mode = mski cp . get ( 'block cipher mode' ) self . kdw mski cp padding method = mski cp . get ( 'padding method' ) self . kdw mski cp hashing algorithm = mski cp . get ( 'hashing algorithm' ) self . kdw mski cp key role type = mski cp . get ( 'key role type' ) self . kdw mski cp digital signature algorithm = mski cp . get ( 'digital signature algorithm' ) self . kdw mski cp cryptographic algorithm = mski cp . get ( 'cryptographic algorithm' ) self . kdw mski cp random iv = mski cp . get ( 'random iv' ) self . kdw mski cp iv length = mski cp . get ( 'iv length' ) self . kdw mski cp tag length = mski cp . get ( 'tag length' ) self . kdw mski cp fixed field length = mski cp . get ( 'fixed field length' ) self . kdw mski cp invocation field length = mski cp . get ( 'invocation field length' ) self . kdw mski cp counter length = mski cp . get ( 'counter length' ) self . kdw mski cp initial counter value = mski cp . get ( 'initial counter value' ) self . kdw mac signature = value . get ( 'mac signature' ) self . kdw iv counter nonce = value . get ( 'iv counter nonce' ) self . kdw encoding option = value . get ( 'encoding option' )", "predictions": ["generate a unused . for the given module ."], "references": ["set the key wrapping data attributes using a dictionary ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 1765, "code": "def get json files ( p ) : f = [ os . path . join ( p , x ) for x in os . listdir ( p ) if x . endswith ( \".json\" ) ] return sorted ( f )", "predictions": ["returns a list of all import used in this required ."], "references": ["scan the provided policy directory for all json policy files ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 1766, "code": "def scan policies ( self ) : policy files = get json files ( self . policy directory ) for f in set ( policy files ) - set ( self . policy files ) : self . file timestamps [ f ] = 0 for f in set ( self . policy files ) - set ( policy files ) : self . logger . info ( \"Removing policies for file: {}\" . format ( f ) ) self . file timestamps . pop ( f , None ) for p in self . policy cache . keys ( ) : self . disassociate policy and file ( p , f ) for p in [ k for k , v in self . policy map . items ( ) if v == f ] : self . restore or delete policy ( p ) self . policy files = policy files for f in sorted ( self . file timestamps . keys ( ) ) : t = os . path . getmtime ( f ) if t > self . file timestamps [ f ] : self . logger . info ( \"Loading policies for file: {}\" . format ( f ) ) self . file timestamps [ f ] = t old p = [ k for k , v in self . policy map . items ( ) if v == f ] try : new p = operation policy . read policy from file ( f ) except Value Error : self . logger . error ( \"Failure loading file: {}\" . format ( f ) ) self . logger . debug ( \"\" , exc info = True ) continue for p in new p . keys ( ) : self . logger . info ( \"Loading policy: {}\" . format ( p ) ) if p in self . reserved policies : self . logger . warning ( \"Policy '{}' overwrites a reserved policy and \" \"will be thrown out.\" . format ( p ) ) continue if p in sorted ( self . policy store . keys ( ) ) : self . logger . debug ( \"Policy '{}' overwrites an existing \" \"policy.\" . format ( p ) ) if f != self . policy map . get ( p ) : self . policy cache . get ( p ) . append ( ( time . time ( ) , self . policy map . get ( p ) , self . policy store . get ( p ) ) ) else : self . policy cache [ p ] = [ ] self . policy store [ p ] = new p . get ( p ) self . policy map [ p ] = f for p in set ( old p ) - set ( new p . keys ( ) ) : self . disassociate policy and file ( p , f ) self . restore or delete policy ( p )", "predictions": ["recursively star a if no decision can be found ."], "references": ["scan the policy directory for policy data ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 1767, "code": "def run ( self ) : self . initialize tracking structures ( ) if self . live monitoring : self . logger . info ( \"Starting up the operation policy file monitor.\" ) while not self . halt trigger . is set ( ) : time . sleep ( 1 ) self . scan policies ( ) self . logger . info ( \"Stopping the operation policy file monitor.\" ) else : self . scan policies ( )", "predictions": ["a helper method that creates and returns a new operation object ."], "references": ["start monitoring operation policy files ."], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 1768, "code": "def get certificate from connection ( connection ) : certificate = connection . getpeercert ( binary form = True ) if certificate : return x509 . load der x509 certificate ( certificate , backends . default backend ( ) ) return None", "predictions": ["gets the key from the numbers dictionary ."], "references": ["extract an x . 509 certificate from a socket connection ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 1769, "code": "def get common names from certificate ( certificate ) : common names = certificate . subject . get attributes for oid ( x509 . oid . Name OID . COMMON NAME ) return [ common name . value for common name in common names ]", "predictions": ["gets the key from the provided dict ."], "references": ["given an x . 509 certificate extract and return all common names ."], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 1770, "code": "def get client identity from certificate ( certificate ) : client ids = get common names from certificate ( certificate ) if len ( client ids ) > 0 : if len ( client ids ) > 1 : raise exceptions . Permission Denied ( \"Multiple client identities found.\" ) return client ids [ 0 ] else : raise exceptions . Permission Denied ( \"The certificate does not define any subject common names. \" \"Client identity unavailable.\" )", "predictions": ["tries to check if the given : : : : : : / / forum . com / . / . / . / . . com / . / . / . / . / . / . / . / . - . - . - . -"], "references": ["given an x . 509 certificate extract and return the client identity ."], "bleu": 0.030216776104535565, "rouge_l": 0.10651920838183936}
{"id": 1771, "code": "def validate ( self ) : if not isinstance ( self . revocation code , Revocation Reason Code ) : msg = \"Revocation Reaon Code expected\" raise Type Error ( msg ) if self . revocation message is not None : if not isinstance ( self . revocation message , Text String ) : msg = \"Text String expect\" raise Type Error ( msg )", "predictions": ["check this latitude is a valid geo geo geo geo ."], "references": ["validate the revocationreason object"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1772, "code": "def validate ( self ) : if self . unique identifier is not None : if not isinstance ( self . unique identifier , attributes . Unique Identifier ) : msg = \"invalid unique identifier\" raise Type Error ( msg )", "predictions": ["validates the given method ."], "references": ["error check the attributes of the activaterequestpayload object ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 1773, "code": "def load ( self ) : mod = import module ( self . module name ) obj = mod if self . object name : for attr in self . object name . split ( '.' ) : obj = getattr ( obj , attr ) return obj", "predictions": ["invokes the object pointed to by the for name ."], "references": ["load the object to which this entry point refers ."], "bleu": 0.18850319022747347, "rouge_l": 0.4}
{"id": 1774, "code": "def generate controller ( args ) : controller template = os . path . join ( dirname ( abspath ( file ) ) , 'templates/controller.py' ) test template = os . path . join ( dirname ( abspath ( file ) ) , 'templates/unittest.py' ) controller name = args . get ( '<controller>' ) current path = os . getcwd ( ) logger . info ( 'Start generating controller.' ) if not controller name : logger . warning ( 'Controller name cannot be empty.' ) return with open ( controller template , 'r' ) as template file : controller file path = os . path . join ( current path , 'application/controllers' , controller name + '.py' ) with open ( controller file path , 'w+' ) as controller file : for line in template file : new line = line . replace ( '#{controller}' , controller name ) controller file . write ( new line ) logger . info ( \"New: %s\" % relative path ( controller file path ) ) with open ( test template , 'r' ) as template file : test file path = os . path . join ( current path , 'tests' , 'test %s.py' % controller name ) with open ( test file path , 'w+' ) as test file : for line in template file : new line = line . replace ( '#{controller}' , controller name ) . replace ( '#{controller|title}' , controller name . title ( ) ) test file . write ( new line ) logger . info ( \"New: %s\" % relative path ( test file path ) ) assets dir path = os . path . join ( current path , 'application/pages/%s' % controller name ) mkdir p ( assets dir path ) generate form ( controller name ) logger . info ( 'Finish generating controller.' )", "predictions": ["creates a generator that returns a list of arguments to be passed to any of the program ."], "references": ["generate controller include the controller file template & css & js directories ."], "bleu": 0.07535838128770536, "rouge_l": 0.13289760348583876}
{"id": 1775, "code": "def mkdir p ( path ) : try : os . makedirs ( path ) except OS Error as exc : if exc . errno == errno . EEXIST and os . path . isdir ( path ) : pass else : raise else : logger . info ( \"New: %s%s\" , path , os . path . sep )", "predictions": ["creates a directory if it does not exist ."], "references": ["mkdir - p path"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1776, "code": "def rewrite and copy ( src file , dst file , project name ) : fh , abs path = mkstemp ( ) with io . open ( abs path , 'w' , encoding = 'utf-8' ) as new file : with io . open ( src file , 'r' , encoding = 'utf-8' ) as old file : for line in old file : new line = line . replace ( '#{project}' , project name ) . replace ( '#{project|title}' , project name . title ( ) ) new file . write ( new line ) shutil . copy ( abs path , dst file ) os . close ( fh )", "predictions": ["get all push data from the { { } / ) . line ."], "references": ["replace vars and copy ."], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 1777, "code": "def check url ( form , field ) : url = field . data . strip ( ) if not url : return result = urlparse ( url ) if result . scheme == \"\" : field . data = \"http://%s\" % re . sub ( r'^:?/*' , '' , url )", "predictions": ["filter out : 1 . 2 . 9 . 3 . 0"], "references": ["check url schema ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 1778, "code": "def encode ( something ) : secret key = current app . config . get ( 'SECRET KEY' ) s = URL Safe Serializer ( secret key ) return s . dumps ( something )", "predictions": ["filter the import import , using the import , and module ."], "references": ["encode something with secret_key ."], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 1779, "code": "def decode ( something ) : secret key = current app . config . get ( 'SECRET KEY' ) s = URL Safe Serializer ( secret key ) try : return s . loads ( something ) except Bad Signature : return None", "predictions": ["filter the url of a job ."], "references": ["decode something with secret_key ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1780, "code": "def absolute url for ( endpoint , * * values ) : config = current app . config site domain = config . get ( 'SITE DOMAIN' ) relative url = url for ( endpoint , * * values ) return join url ( site domain , relative url )", "predictions": ["given a duplicate line , get all filter configuration for the corresponding line ."], "references": ["absolute url for endpoint ."], "bleu": 0.09782375748961449, "rouge_l": 0.23018867924528305}
{"id": 1781, "code": "def signin user ( user , permenent = True ) : session . permanent = permenent session [ 'user id' ] = user . id", "predictions": ["create a new ( ."], "references": ["sign in user ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1782, "code": "def get current user ( ) : if not 'user id' in session : return None user = User . query . filter ( User . id == session [ 'user id' ] ) . first ( ) if not user : signout user ( ) return None return user", "predictions": ["useless this line , return , or return ."], "references": ["get current user ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 1783, "code": "def create app ( ) : config = load config ( ) app = Flask ( name ) app . config . from object ( config ) app . wsgi app = Proxy Fix ( app . wsgi app ) Csrf Protect ( app ) if app . debug or app . testing : Debug Toolbar Extension ( app ) app . wsgi app = Shared Data Middleware ( app . wsgi app , { '/pages' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/pages' ) } ) else : app . logger . add Handler ( logging . Stream Handler ( ) ) app . logger . set Level ( logging . ERROR ) if app . config . get ( 'SENTRY DSN' ) : from . utils . sentry import sentry sentry . init app ( app , dsn = app . config . get ( 'SENTRY DSN' ) ) app . wsgi app = Shared Data Middleware ( app . wsgi app , { '/static' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/static' ) , '/pkg' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pkg' ) , '/pages' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pages' ) } ) register db ( app ) register routes ( app ) register jinja ( app ) register error handle ( app ) register hooks ( app ) return app", "predictions": ["creates the numbers into the db ."], "references": ["create flask app ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1784, "code": "def register jinja ( app ) : import jinja2 from . utils import filters , permissions , helpers if app . debug or app . testing : my loader = jinja2 . Choice Loader ( [ app . jinja loader , jinja2 . File System Loader ( [ os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/macros' ) , os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/pages' ) ] ) ] ) else : my loader = jinja2 . Choice Loader ( [ app . jinja loader , jinja2 . File System Loader ( [ os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/macros' ) , os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pages' ) ] ) ] ) app . jinja loader = my loader app . jinja env . filters . update ( { 'timesince' : filters . timesince } ) def url for other page ( page ) : \"\"\"Generate url for pagination.\"\"\" view args = request . view args . copy ( ) args = request . args . copy ( ) . to dict ( ) combined args = dict ( view args . items ( ) + args . items ( ) ) combined args [ 'page' ] = page return url for ( request . endpoint , * * combined args ) rules = { } for endpoint , rules in iteritems ( app . url map . rules by endpoint ) : if any ( item in endpoint for item in [ ' debug toolbar' , 'debugtoolbar' , 'static' ] ) : continue rules [ endpoint ] = [ { 'rule' : rule . rule } for rule in rules ] app . jinja env . globals . update ( { 'absolute url for' : helpers . absolute url for , 'url for other page' : url for other page , 'rules' : rules , 'permissions' : permissions } )", "predictions": ["get the \"\"\"generate for the line and invokes the \"\"\"generate ."], "references": ["register jinja filters vars functions ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 1785, "code": "def register error handle ( app ) : @ app . errorhandler ( 403 ) def page 403 ( error ) : return render template ( 'site/403/403.html' ) , 403 @ app . errorhandler ( 404 ) def page 404 ( error ) : return render template ( 'site/404/404.html' ) , 404 @ app . errorhandler ( 500 ) def page 500 ( error ) : return render template ( 'site/500/500.html' ) , 500", "predictions": ["registers a [ not . ] ."], "references": ["register http error pages ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1786, "code": "def dataframe to csv ( writer , dataframe , delimiter , with header ) : encoding writer = codecs . getwriter ( 'utf-8' ) ( writer ) dataframe . to csv ( path or buf = encoding writer , sep = delimiter , header = with header , index = False )", "predictions": ["create a fix from a fix ( bool expand expand or ( ( bool expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand expand variables expand"], "references": ["serialize the dataframe with different delimiters"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1787, "code": "def dataframe from csv ( reader , delimiter , with header , skipspace ) : sep = delimiter header = 0 if not with header : header = None return pd . read csv ( reader , header = header , sep = sep , skipinitialspace = skipspace , encoding = 'utf-8-sig' )", "predictions": ["extract a detect from its ( ( x 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"], "references": ["returns csv data as a pandas dataframe object"], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 1788, "code": "def contents url ( self ) : loc = self . download location return loc . base uri + loc . location + loc . access credential", "predictions": ["get detect detect for this permission ."], "references": ["full url to the dataset contents ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1789, "code": "def open ( self ) : return self . workspace . rest . open intermediate dataset contents ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )", "predictions": ["split the personal image on the database ."], "references": ["open and return a stream for the dataset contents ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 1790, "code": "def read as binary ( self ) : return self . workspace . rest . read intermediate dataset contents binary ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )", "predictions": ["reads the try and returns it as file ."], "references": ["read and return the dataset contents as binary ."], "bleu": 0.17747405280050263, "rouge_l": 0.3333333333333333}
{"id": 1791, "code": "def read as text ( self ) : return self . workspace . rest . read intermediate dataset contents text ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )", "predictions": ["use this to is a full is available in this object ."], "references": ["read and return the dataset contents as text ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 1792, "code": "def to dataframe ( self ) : #TODO: figure out why passing in the opened stream directly gives invalid data data = self . read as binary ( ) reader = Bytes IO ( data ) return deserialize dataframe ( reader , self . data type id )", "predictions": ["convert data from this reader to an array of ("], "references": ["read and return the dataset contents as a pandas dataframe ."], "bleu": 0.0959156018869021, "rouge_l": 0.0}
{"id": 1793, "code": "def get experiments ( self , workspace id ) : api path = self . EXPERIMENTS URI FMT . format ( workspace id ) return self . send get req ( api path )", "predictions": ["get a workspace to this workspace ."], "references": ["runs http get request to retrieve the list of experiments ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 1794, "code": "def get datasets ( self , workspace id ) : api path = self . DATASOURCES URI FMT . format ( workspace id ) return self . send get req ( api path )", "predictions": ["get a workspace to this workspace ."], "references": ["runs http get request to retrieve the list of datasets ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 1795, "code": "def get dataset ( self , workspace id , dataset id ) : api path = self . DATASOURCE URI FMT . format ( workspace id , dataset id ) return self . send get req ( api path )", "predictions": ["get the workspace for this dataset ."], "references": ["runs http get request to retrieve a single dataset ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 1796, "code": "def find globals ( code ) : cur byte = 0 byte code = code . co code names = set ( ) while cur byte < len ( byte code ) : op = ord ( byte code [ cur byte ] ) if op >= dis . HAVE ARGUMENT : if op == LOAD GLOBAL : oparg = ord ( byte code [ cur byte + 1 ] ) + ( ord ( byte code [ cur byte + 2 ] ) << 8 ) name = code . co names [ oparg ] names . add ( name ) cur byte += 2 cur byte += 1 return names", "predictions": ["find the best match of the content of the code ."], "references": ["walks the byte code to find the variables which are actually globals"], "bleu": 0.15553014371537452, "rouge_l": 0.17256011315417258}
{"id": 1797, "code": "def copy ( self ) : pen = Pen ( ) pen . dict = self . dict . copy ( ) return pen", "predictions": ["copies this closeable ' s contents to a copy of this node ."], "references": ["create a copy of this pen ."], "bleu": 0.27824623288353134, "rouge_l": 0.5285961871750434}
{"id": 1798, "code": "def draw ( self , cr , highlight = False , bounding = None ) : if bounding is None or self . intersects ( bounding ) : self . draw ( cr , highlight , bounding )", "predictions": ["draws the specified box at the equal to the specified graphics device ."], "references": ["draw this shape with the given cairo context"], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 1799, "code": "def build choices ( self ) : tree token = u'sitetree tree from \"%s\" template \"%s\"' % ( self . tree , self . template ) context kwargs = { 'current app' : 'admin' } context = template . Context ( context kwargs ) if VERSION >= ( 1 , 8 ) else template . Context ( * * context kwargs ) context . update ( { 'request' : object ( ) } ) choices str = sitetree tree ( Parser ( None ) , Token ( token type = TOKEN BLOCK , contents = tree token ) ) . render ( context ) tree choices = [ ( ITEMS FIELD ROOT ID , self . root title ) ] for line in choices str . splitlines ( ) : if line . strip ( ) : splitted = line . split ( ':::' ) tree choices . append ( ( splitted [ 0 ] , mark safe ( splitted [ 1 ] ) ) ) return tree choices", "predictions": ["build a tree from tokens that are implemented by parsing the template context and applies each call to the tree ."], "references": ["build choices list runtime using sitetree_tree tag"], "bleu": 0.05809665204409193, "rouge_l": 0.0785070785070785}
{"id": 1800, "code": "def init ( self ) : cache . get ( 'sitetrees reset' ) and self . empty ( init = False ) self . cache = cache . get ( 'sitetrees' , { 'sitetrees' : { } , 'parents' : { } , 'items by ids' : { } , 'tree aliases' : { } } )", "predictions": ["initialize . . this is called only once for each cache of the cache ."], "references": ["initializes local cache from django cache ."], "bleu": 0.11633270842295028, "rouge_l": 0.291866028708134}
{"id": 1801, "code": "def empty ( self , * * kwargs ) : cache . delete ( 'sitetrees' ) cache . delete ( 'sitetrees reset' ) kwargs . get ( 'init' , True ) and self . init ( )", "predictions": ["creates and returns a cache instance for this cache ."], "references": ["empties cached sitetree data ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 1802, "code": "def for tag ( cls , parser , token , preposition , error hint ) : tokens = token . split contents ( ) if len ( tokens ) >= 3 and tokens [ 1 ] == preposition : as var = cls . get as var ( tokens ) tree alias = parser . compile filter ( tokens [ 2 ] ) return cls ( tree alias , as var ) raise template . Template Syntax Error ( '%r tag requires at least two arguments. E.g. {%% %s %%}.' % ( tokens [ 0 ] , error hint ) )", "predictions": ["match all other tokens in this tree"], "references": ["node constructor to be used in tags ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1803, "code": "def get model url name ( model nfo , page , with namespace = False ) : prefix = '' if with namespace : prefix = 'admin:' return ( '%s%s %s' % ( prefix , '%s %s' % model nfo , page ) ) . lower ( )", "predictions": ["generate url based on the given url ."], "references": ["returns a url for a given tree admin page type ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 1804, "code": "def reregister tree admin ( ) : try : admin . site . unregister ( MODEL TREE CLASS ) except Not Registered : pass admin . site . register ( MODEL TREE CLASS , TREE ADMIN ( ) )", "predictions": ["for the given admin ."], "references": ["forces unregistration of tree admin class with following re - registration ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 1805, "code": "def redirect ( self , request , response ) : if ' addanother' in request . POST : return Http Response Redirect ( '../item add/' ) elif ' save' in request . POST : return Http Response Redirect ( '../' ) elif ' continue' in request . POST : return response return Http Response Redirect ( '' )", "predictions": ["aborts execution and causes a response to a specified response ."], "references": ["generic redirect for item editor ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 1806, "code": "def get tree ( self , request , tree id , item id = None ) : if tree id is None : tree id = self . get object ( request , item id ) . tree id self . tree = MODEL TREE CLASS . default manager . get ( pk = tree id ) self . tree . verbose name plural = self . tree . meta . verbose name plural self . tree . urls = TREE URLS return self . tree", "predictions": ["generate a tree to load record ."], "references": ["fetches tree for current or given treeitem ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1807, "code": "def item move ( self , request , tree id , item id , direction ) : current item = MODEL TREE ITEM CLASS . default manager . get ( pk = item id ) if direction == 'up' : sort order = 'sort order' else : sort order = '-sort order' siblings = MODEL TREE ITEM CLASS . default manager . filter ( parent = current item . parent , tree = current item . tree ) . order by ( sort order ) previous item = None for item in siblings : if item != current item : previous item = item else : break if previous item is not None : current item sort order = current item . sort order previous item sort order = previous item . sort order current item . sort order = previous item sort order previous item . sort order = current item sort order current item . save ( ) previous item . save ( ) return Http Response Redirect ( '../../' )", "predictions": ["applications that need to move all the elements of the given item to the given tree ."], "references": ["moves item up or down by swapping sort_order field values of neighboring items ."], "bleu": 0.0859076483566362, "rouge_l": 0.13132400430570507}
{"id": 1808, "code": "def get urls ( self ) : urls = super ( Tree Admin , self ) . get urls ( ) prefix change = 'change/' if DJANGO POST 19 else '' sitetree urls = [ url ( r'^change/$' , redirects handler , name = get tree item url name ( 'changelist' ) ) , url ( r'^((?P<tree id>\\d+)/)?%sitem add/$' % prefix change , self . admin site . admin view ( self . tree admin . item add ) , name = get tree item url name ( 'add' ) ) , url ( r'^(?P<tree id>\\d+)/%sitem (?P<item id>\\d+)/$' % prefix change , self . admin site . admin view ( self . tree admin . item edit ) , name = get tree item url name ( 'change' ) ) , url ( r'^%sitem (?P<item id>\\d+)/$' % prefix change , self . admin site . admin view ( self . tree admin . item edit ) , name = get tree item url name ( 'change' ) ) , url ( r'^((?P<tree id>\\d+)/)?%sitem (?P<item id>\\d+)/delete/$' % prefix change , self . admin site . admin view ( self . tree admin . item delete ) , name = get tree item url name ( 'delete' ) ) , url ( r'^((?P<tree id>\\d+)/)?%sitem (?P<item id>\\d+)/history/$' % prefix change , self . admin site . admin view ( self . tree admin . item history ) , name = get tree item url name ( 'history' ) ) , url ( r'^(?P<tree id>\\d+)/%sitem (?P<item id>\\d+)/move (?P<direction>(up|down))/$' % prefix change , self . admin site . admin view ( self . tree admin . item move ) , name = get tree item url name ( 'move' ) ) , ] if not DJANGO POST 19 : sitetree urls = patterns func ( '' , * sitetree urls ) if SMUGGLER INSTALLED : sitetree urls += ( url ( r'^dump all/$' , self . admin site . admin view ( self . dump view ) , name = 'sitetree dump' ) , ) return sitetree urls + urls", "predictions": ["creates and builds a link for the redirects to the redirects ."], "references": ["manages not only treeadmin urls but also treeitemadmin urls ."], "bleu": 0.10390302174233558, "rouge_l": 0.09242424242424242}
{"id": 1809, "code": "async def asgi send ( self , message : dict ) -> None : if message [ \"type\" ] == \"http.response.start\" and self . state == ASGIHTTP State . REQUEST : self . response = message elif message [ \"type\" ] == \"http.response.body\" and self . state in { ASGIHTTP State . REQUEST , ASGIHTTP State . RESPONSE , } : if self . state == ASGIHTTP State . REQUEST : headers = build and validate headers ( self . response [ \"headers\" ] ) headers . extend ( self . response headers ( ) ) await self . asend ( h11 . Response ( status code = int ( self . response [ \"status\" ] ) , headers = headers ) ) self . state = ASGIHTTP State . RESPONSE if ( not suppress body ( self . scope [ \"method\" ] , int ( self . response [ \"status\" ] ) ) and message . get ( \"body\" , b\"\" ) != b\"\" ) : await self . asend ( h11 . Data ( data = bytes ( message [ \"body\" ] ) ) ) if not message . get ( \"more body\" , False ) : if self . state != ASGIHTTP State . CLOSED : await self . asend ( h11 . End Of Message ( ) ) await self . asgi put ( { \"type\" : \"http.disconnect\" } ) self . state = ASGIHTTP State . CLOSED else : raise Unexpected Message ( self . state , message [ \"type\" ] )", "predictions": [". conversion of this object to the ( ."], "references": ["called by the asgi instance to send a message ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 1810, "code": "async def asgi send ( self , message : dict ) -> None : if message [ \"type\" ] == \"websocket.accept\" and self . state == ASGI Websocket State . HANDSHAKE : headers = build and validate headers ( message . get ( \"headers\" , [ ] ) ) raise if subprotocol present ( headers ) headers . extend ( self . response headers ( ) ) await self . asend ( Accept Connection ( extensions = [ Per Message Deflate ( ) ] , extra headers = headers , subprotocol = message . get ( \"subprotocol\" ) , ) ) self . state = ASGI Websocket State . CONNECTED self . config . access logger . access ( self . scope , { \"status\" : 101 , \"headers\" : [ ] } , time ( ) - self . start time ) elif ( message [ \"type\" ] == \"websocket.http.response.start\" and self . state == ASGI Websocket State . HANDSHAKE ) : self . response = message self . config . access logger . access ( self . scope , self . response , time ( ) - self . start time ) elif message [ \"type\" ] == \"websocket.http.response.body\" and self . state in { ASGI Websocket State . HANDSHAKE , ASGI Websocket State . RESPONSE , } : await self . asgi send rejection ( message ) elif message [ \"type\" ] == \"websocket.send\" and self . state == ASGI Websocket State . CONNECTED : data : Union [ bytes , str ] if message . get ( \"bytes\" ) is not None : await self . asend ( Bytes Message ( data = bytes ( message [ \"bytes\" ] ) ) ) elif not isinstance ( message [ \"text\" ] , str ) : raise Type Error ( f\"{message['text']} should be a str\" ) else : await self . asend ( Text Message ( data = message [ \"text\" ] ) ) elif message [ \"type\" ] == \"websocket.close\" and self . state == ASGI Websocket State . HANDSHAKE : await self . send http error ( 403 ) self . state = ASGI Websocket State . HTTPCLOSED elif message [ \"type\" ] == \"websocket.close\" : await self . asend ( Close Connection ( code = int ( message [ \"code\" ] ) ) ) self . state = ASGI Websocket State . CLOSED else : raise Unexpected Message ( self . state , message [ \"type\" ] )", "predictions": ["method that sends a batch of media states to this connection ."], "references": ["called by the asgi instance to send a message ."], "bleu": 0.1235622127262679, "rouge_l": 0.18484848484848485}
{"id": 1811, "code": "def update binary annotations ( self , extra annotations ) : if not self . logging context : self . binary annotations . update ( extra annotations ) else : self . logging context . tags . update ( extra annotations )", "predictions": ["updates a binary object with a specific resource ."], "references": ["updates the binary annotations for the current span ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 1812, "code": "def encode span ( self , v2 span ) : span = v2 span . build v1 span ( ) thrift endpoint = thrift . create endpoint ( span . endpoint . port , span . endpoint . service name , span . endpoint . ipv4 , span . endpoint . ipv6 , ) thrift annotations = thrift . annotation list builder ( span . annotations , thrift endpoint , ) thrift binary annotations = thrift . binary annotation list builder ( span . binary annotations , thrift endpoint , ) if v2 span . remote endpoint : self . encode remote endpoint ( v2 span . remote endpoint , v2 span . kind , thrift binary annotations , ) thrift span = thrift . create span ( span . id , span . parent id , span . trace id , span . name , thrift annotations , thrift binary annotations , span . timestamp , span . duration , ) encoded span = thrift . span to bytes ( thrift span ) return encoded span", "predictions": ["creates and encodes this span ."], "references": ["encodes the current span to thrift ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 1813, "code": "def encode span ( self , v2 span ) : span = v2 span . build v1 span ( ) json span = { 'trace Id' : span . trace id , 'name' : span . name , 'id' : span . id , 'annotations' : [ ] , 'binary Annotations' : [ ] , } if span . parent id : json span [ 'parent Id' ] = span . parent id if span . timestamp : json span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) if span . duration : json span [ 'duration' ] = int ( span . duration * 1000000 ) v1 endpoint = self . create json endpoint ( span . endpoint , True ) for key , timestamp in span . annotations . items ( ) : json span [ 'annotations' ] . append ( { 'endpoint' : v1 endpoint , 'timestamp' : int ( timestamp * 1000000 ) , 'value' : key , } ) for key , value in span . binary annotations . items ( ) : json span [ 'binary Annotations' ] . append ( { 'key' : key , 'value' : value , 'endpoint' : v1 endpoint , } ) if v2 span . remote endpoint : self . encode remote endpoint ( v2 span . remote endpoint , v2 span . kind , json span [ 'binary Annotations' ] , ) encoded span = json . dumps ( json span ) return encoded span", "predictions": ["the graphical representation for this span ."], "references": ["encodes a single span to json ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1814, "code": "def encode span ( self , span ) : json span = { 'trace Id' : span . trace id , 'id' : span . span id , } if span . name : json span [ 'name' ] = span . name if span . parent id : json span [ 'parent Id' ] = span . parent id if span . timestamp : json span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) if span . duration : json span [ 'duration' ] = int ( span . duration * 1000000 ) if span . shared is True : json span [ 'shared' ] = True if span . kind and span . kind . value is not None : json span [ 'kind' ] = span . kind . value if span . local endpoint : json span [ 'local Endpoint' ] = self . create json endpoint ( span . local endpoint , False , ) if span . remote endpoint : json span [ 'remote Endpoint' ] = self . create json endpoint ( span . remote endpoint , False , ) if span . tags and len ( span . tags ) > 0 : json span [ 'tags' ] = span . tags if span . annotations : json span [ 'annotations' ] = [ { 'timestamp' : int ( timestamp * 1000000 ) , 'value' : key , } for key , timestamp in span . annotations . items ( ) ] encoded span = json . dumps ( json span ) return encoded span", "predictions": ["build the json representation of this span ."], "references": ["encodes a single span to json ."], "bleu": 0.19070828081828378, "rouge_l": 0.26991150442477874}
{"id": 1815, "code": "def fits ( self , current count , current size , max size , new span ) : return current size + len ( new span ) <= max size", "predictions": ["returns a fits for this arg ."], "references": ["checks if the new span fits in the max payload size ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 1816, "code": "def encode span ( self , span ) : if not protobuf . installed ( ) : raise Zipkin Error ( 'protobuf encoding requires installing the protobuf\\'s extra ' 'requirements. Use py-zipkin[protobuf] in your requirements.txt.' ) pb span = protobuf . create protobuf span ( span ) return protobuf . encode pb list ( [ pb span ] )", "predictions": ["encode the protobuf based on the protobuf ."], "references": ["encodes a single span to protobuf ."], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 1817, "code": "def join lines ( string , strip = Strip . BOTH ) : lines = [ ] for line in string . splitlines ( ) : if strip & Strip . RIGHT : line = line . rstrip ( ) if strip & Strip . LEFT : line = line . lstrip ( ) lines . append ( line ) return '' . join ( lines )", "predictions": ["joins lines , separating each line into a single line ."], "references": ["join strings together and strip whitespace in between if needed"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1818, "code": "async def json or text ( response ) : text = await response . text ( ) if response . headers [ 'Content-Type' ] == 'application/json; charset=utf-8' : return json . loads ( text ) return text", "predictions": ["return json representation of response ."], "references": ["turns response into a properly formatted json or text object"], "bleu": 0.1255107248036171, "rouge_l": 0.11960784313725491}
{"id": 1819, "code": "async def limited ( until ) : duration = int ( round ( until - time . time ( ) ) ) mins = duration / 60 fmt = 'We have exhausted a ratelimit quota. Retrying in %.2f seconds (%.3f minutes).' log . warn ( fmt , duration , mins )", "predictions": ["calculate the generator that will return a floating point value ."], "references": ["handles the message shown when we are ratelimited"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1820, "code": "async def request ( self , method , url , * * kwargs ) : rate limiter = Rate Limiter ( max calls = 59 , period = 60 , callback = limited ) async with rate limiter : if not self . token : raise Unauthorized Detected ( 'Unauthorized Detected (status code: 401): No TOKEN provided' ) headers = { 'User-Agent' : self . user agent , 'Content-Type' : 'application/json' } if 'json' in kwargs : kwargs [ 'data' ] = to json ( kwargs . pop ( 'json' ) ) kwargs [ 'headers' ] = headers headers [ 'Authorization' ] = self . token for tries in range ( 5 ) : async with self . session . request ( method , url , * * kwargs ) as resp : log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) data = await json or text ( resp ) if 300 > resp . status >= 200 : return data if resp . status == 429 : fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' retry after = json . loads ( resp . headers . get ( 'Retry-After' ) ) mins = retry after / 60 log . warning ( fmt , retry after , mins ) is global = True if is global : self . global over . clear ( ) await asyncio . sleep ( retry after , loop = self . loop ) log . debug ( 'Done sleeping for the rate limit. Retrying...' ) if is global : self . global over . set ( ) log . debug ( 'Global rate limit is now over.' ) continue if resp . status == 400 : raise HTTP Exception ( resp , data ) elif resp . status == 401 : raise Unauthorized ( resp , data ) elif resp . status == 403 : raise Forbidden ( resp , data ) elif resp . status == 404 : raise Not Found ( resp , data ) else : raise HTTP Exception ( resp , data ) raise HTTP Exception ( resp , data )", "predictions": ["call back with 401 api . use this method to show the ( ."], "references": ["handles requests to the api"], "bleu": 0.10511846841633776, "rouge_l": 0.23018867924528305}
{"id": 1821, "code": "async def get bot info ( self , bot id ) : resp = await self . request ( 'GET' , '{}/bots/{}' . format ( self . BASE , bot id ) ) resp [ 'date' ] = datetime . strptime ( resp [ 'date' ] , '%Y-%m-%d T%H:%M:%S.%f Z' ) for k in resp : if resp [ k ] == '' : resp [ k ] = None return resp", "predictions": ["retrieves all the cards in this response ."], "references": ["gets the information of the given bot id"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1822, "code": "async def get bots ( self , limit , offset ) : if limit > 500 : limit = 50 return await self . request ( 'GET' , '{}/bots?limit={}&offset={}' . format ( self . BASE , limit , offset ) )", "predictions": ["get a full description of this object ."], "references": ["gets an object of bots on dbl"], "bleu": 0.17747405280050269, "rouge_l": 0.13495575221238937}
{"id": 1823, "code": "def read ( self ) : packet = self . packet with self . read lock : buffer = self . buffer while len ( buffer ) < packet : buffer += self . read data ( ) length = self . unpack ( buffer [ : packet ] ) [ 0 ] + packet while len ( buffer ) < length : buffer += self . read data ( ) term , self . buffer = decode ( buffer [ packet : ] ) return term", "predictions": ["read data from the fo ."], "references": ["read incoming message ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 1824, "code": "def write ( self , message ) : data = encode ( message , compressed = self . compressed ) length = len ( data ) data = self . pack ( length ) + data with self . write lock : while data : try : n = os . write ( self . out d , data ) except OS Error as why : if why . errno in ( errno . EPIPE , errno . EINVAL ) : raise EOF Error ( ) raise if not n : raise EOF Error ( ) data = data [ n : ] return length + self . packet", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to the original stream ."], "references": ["write outgoing message ."], "bleu": 0.04668049023095243, "rouge_l": 0.07682619647355164}
{"id": 1825, "code": "def decode ( string ) : if not string : raise Incomplete Data ( string ) if string [ 0 ] != 131 : raise Value Error ( \"unknown protocol version: %r\" % string [ 0 ] ) if string [ 1 : 2 ] == b'P' : if len ( string ) < 16 : raise Incomplete Data ( string ) d = decompressobj ( ) term string = d . decompress ( string [ 6 : ] ) + d . flush ( ) uncompressed size , = int4 unpack ( string [ 2 : 6 ] ) if len ( term string ) != uncompressed size : raise Value Error ( \"invalid compressed tag, \" \"%d bytes but got %d\" % ( uncompressed size , len ( term string ) ) ) term , tail = decode term ( term string ) return term , d . unused data return decode term ( string [ 1 : ] )", "predictions": ["get the common bytes from the ( , self self self self self self self self self self self - signed self - signed term self ."], "references": ["decode erlang external term ."], "bleu": 0.04970745472800838, "rouge_l": 0.14269005847953214}
{"id": 1826, "code": "def encode ( term , compressed = False ) : encoded term = encode term ( term ) if compressed : if compressed is True : compressed = 6 elif compressed < 0 or compressed > 9 : raise Value Error ( \"invalid compression level: %r\" % ( compressed , ) ) zlib term = compress ( encoded term , compressed ) ln = len ( encoded term ) if len ( zlib term ) + 5 <= ln : return b\"\\x83P\" + int4 pack ( ln ) + zlib term return b\"\\x83\" + encoded term", "predictions": ["encodes this ( the ( ( ) ( ( ) ( ( ) ( ( ) ) ( the ( ( ) ) ) ) ( the ( ( ) ) ) ) ) ) ( the ( ( ) ) ) ) ) ) ) ."], "references": ["encode erlang external term ."], "bleu": 0.02558174341959753, "rouge_l": 0.04501845018450184}
{"id": 1827, "code": "def add Source Addr ( self , addr ) : try : self . multi In Socket . setsockopt ( socket . IPPROTO IP , socket . IP ADD MEMBERSHIP , self . make Mreq ( addr ) ) except socket . error : pass sock = self . create Multicast Out Socket ( addr , self . observer . ttl ) self . multi Out Uni In Sockets [ addr ] = sock self . poll . register ( sock , select . POLLIN )", "predictions": ["adds a source file to this socket ."], "references": ["none means system default"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1828, "code": "def send Pending Messages ( self ) : if len ( self . queue ) == 0 : time . sleep ( 0.1 ) return msg = self . queue . pop ( 0 ) if msg . can Send ( ) : self . send Msg ( msg ) msg . refresh ( ) if not ( msg . is Finished ( ) ) : self . queue . append ( msg ) else : self . queue . append ( msg ) time . sleep ( 0.01 )", "predictions": ["find out the pending ."], "references": ["method sleeps if nothing to do"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 1829, "code": "def stop ( self ) : self . clear Remote Services ( ) self . clear Local Services ( ) self . stop Threads ( ) self . server Started = False", "predictions": ["copy the torrent class from the database ."], "references": ["cleans up and stops the discovery server"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1830, "code": "def clear Local Services ( self ) : for service in list ( self . local Services . values ( ) ) : self . send Bye ( service ) self . local Services . clear ( )", "predictions": ["use this to draw the local map ."], "references": ["send bye messages for the services and remove them"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1831, "code": "def search Services ( self , types = None , scopes = None , timeout = 3 ) : if not self . server Started : raise Exception ( \"Server not started\" ) self . send Probe ( types , scopes ) time . sleep ( timeout ) return self . filter Services ( list ( self . remote Services . values ( ) ) , types , scopes )", "predictions": ["build a 8 - level 8 for the 8 - 1 ."], "references": ["search for services given the types and scopes in a given timeout"], "bleu": 0.1235622127262679, "rouge_l": 0.16666666666666666}
{"id": 1832, "code": "def create SOAP Message ( env ) : if env . get Action ( ) == ACTION PROBE : return create Probe Message ( env ) if env . get Action ( ) == ACTION PROBE MATCH : return create Probe Match Message ( env ) if env . get Action ( ) == ACTION RESOLVE : return create Resolve Message ( env ) if env . get Action ( ) == ACTION RESOLVE MATCH : return create Resolve Match Message ( env ) if env . get Action ( ) == ACTION HELLO : return create Hello Message ( env ) if env . get Action ( ) == ACTION BYE : return create Bye Message ( env )", "predictions": ["init method for the standard paginados ."], "references": ["construct a a raw soap xml string given a prepared soapenvelope object"], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 1833, "code": "def discover ( scope , loglevel , capture ) : if loglevel : level = getattr ( logging , loglevel , None ) if not level : print ( \"Invalid log level '%s'\" % loglevel ) return logger . set Level ( level ) run ( scope = scope , capture = capture )", "predictions": ["empty cluster . this is done by the name of the logging ."], "references": ["discover systems using ws - discovery"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 1834, "code": "def save ( self , * * kwargs ) : child relation names = [ rel . get accessor name ( ) for rel in get all child relations ( self ) ] child m2m field names = [ field . name for field in get all child m2m relations ( self ) ] update fields = kwargs . pop ( 'update fields' , None ) if update fields is None : real update fields = None relations to commit = child relation names m2m fields to commit = child m2m field names else : real update fields = [ ] relations to commit = [ ] m2m fields to commit = [ ] for field in update fields : if field in child relation names : relations to commit . append ( field ) elif field in child m2m field names : m2m fields to commit . append ( field ) else : real update fields . append ( field ) super ( Clusterable Model , self ) . save ( update fields = real update fields , * * kwargs ) for relation in relations to commit : getattr ( self , relation ) . commit ( ) for field in m2m fields to commit : getattr ( self , field ) . commit ( )", "predictions": ["for each error . can be called multiple times ."], "references": ["save the model and commit all child relations ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 1835, "code": "def validate unique ( self ) : all unique checks = set ( ) all date checks = set ( ) forms to delete = self . deleted forms valid forms = [ form for form in self . forms if form . is valid ( ) and form not in forms to delete ] for form in valid forms : unique checks , date checks = form . instance . get unique checks ( ) all unique checks . update ( unique checks ) all date checks . update ( date checks ) errors = [ ] for uclass , unique check in all unique checks : seen data = set ( ) for form in valid forms : row data = ( field if field in self . unique fields else form . cleaned data [ field ] for field in unique check if field in form . cleaned data ) row data = tuple ( d . get pk val ( ) if hasattr ( d , ' get pk val' ) else d for d in row data ) if row data and None not in row data : if row data in seen data : errors . append ( self . get unique error message ( unique check ) ) form . errors [ NON FIELD ERRORS ] = self . error class ( [ self . get form error ( ) ] ) for field in unique check : if field in form . cleaned data : del form . cleaned data [ field ] seen data . add ( row data ) if errors : raise Validation Error ( errors )", "predictions": ["validates the return value for this object and returns the return value ."], "references": ["this clean method will check for unique_together condition"], "bleu": 0.10571070857151538, "rouge_l": 0.09951060358890701}
{"id": 1836, "code": "def has changed ( self ) : if self . formsets : for formset in self . formsets . values ( ) : for form in formset . forms : if form . has changed ( ) : return True return bool ( self . changed data )", "predictions": ["checks if there are tree changes for changes to the formset ."], "references": ["return true if data differs from initial ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 1837, "code": "def with valid checksum ( self ) : return Address ( trytes = self . address + self . generate checksum ( ) , balance = self . balance , key index = self . key index , security level = self . security level , )", "predictions": ["generate a self - level self for this object ."], "references": ["returns the address with a valid checksum attached ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 1838, "code": "def generate checksum ( self ) : checksum trits = [ ] sponge = Kerl ( ) sponge . absorb ( self . address . as trits ( ) ) sponge . squeeze ( checksum trits ) checksum length = Address Checksum . LEN * TRITS PER TRYTE return Address Checksum . from trits ( checksum trits [ - checksum length : ] )", "predictions": ["get a tree for the brain system ."], "references": ["generates the correct checksum for this address ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 1839, "code": "def prompt for seed ( ) : seed = secure input ( 'Enter seed and press return (typing will not be shown).\\n' 'If no seed is specified, a random one will be used instead.\\n' ) if isinstance ( seed , text type ) : seed = seed . encode ( 'ascii' ) return Seed ( seed ) if seed else Seed . random ( )", "predictions": ["with a ( or optionally , ( or a , ( ( , , ( , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,"], "references": ["prompts the user to enter their seed via stdin ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1840, "code": "def create sponge ( self , index ) : seed = self . seed as trits [ : ] sponge = Kerl ( ) sponge . absorb ( add trits ( seed , trits from int ( index ) ) ) sponge . squeeze ( seed ) sponge . reset ( ) sponge . absorb ( seed ) return sponge", "predictions": ["get a urls for this seed ."], "references": ["prepares the hash sponge for the generator ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1841, "code": "def transform ( self ) : # # state length = STATE LENGTH truth table = TRUTH TABLE # # prev state = self . state [ : ] new state = prev state [ : ] index = 0 for in range ( NUMBER OF ROUNDS ) : prev trit = prev state [ index ] for pos in range ( state length ) : index += ( 364 if index < 365 else - 365 ) new trit = prev state [ index ] new state [ pos ] = truth table [ prev trit + ( 3 * new trit ) + 4 ] prev trit = new trit prev state = new state new state = new state [ : ] self . state = new state", "predictions": ["transforms each if the truth truth is set to 1 ."], "references": ["transforms internal state ."], "bleu": 0.12605968092174913, "rouge_l": 0.2911694510739857}
{"id": 1842, "code": "def full add trits ( left , right , carry ) : sum both = add trits ( left , right ) cons left = cons trits ( left , right ) cons right = cons trits ( sum both , carry ) return add trits ( sum both , carry ) , any trits ( cons left , cons right )", "predictions": ["add ( message dict , message dict , message , message dict dict dict dict , message , message dict dict dict dict , message , message , message , and message . -> ( dict dict dict dict dict dict dict dict dict dict dict dict dict dict dict"], "references": ["adds two trits together with support for a carry trit ."], "bleu": 0.02403051755364481, "rouge_l": 0.037059538274605106}
{"id": 1843, "code": "def resolve adapter ( uri ) : if isinstance ( uri , Base Adapter ) : return uri parsed = compat . urllib parse . urlsplit ( uri ) if not parsed . scheme : raise with context ( exc = Invalid Uri ( 'URI must begin with \"<protocol>://\" (e.g., \"udp://\").' , ) , context = { 'parsed' : parsed , 'uri' : uri , } , ) try : adapter type = adapter registry [ parsed . scheme ] except Key Error : raise with context ( exc = Invalid Uri ( 'Unrecognized protocol {protocol!r}.' . format ( protocol = parsed . scheme , ) ) , context = { 'parsed' : parsed , 'uri' : uri , } , ) return adapter type . configure ( parsed )", "predictions": ["update the protocol from the given ( or ( videos self self self self self self self self self self self self self self self self self self self self self self self self self self self self self self self self self self self self self ."], "references": ["given a uri returns a properly - configured adapter instance ."], "bleu": 0.02771450089816765, "rouge_l": 0.07644110275689223}
{"id": 1844, "code": "def log ( self , level , message , context = None ) : if self . logger : self . logger . log ( level , message , extra = { 'context' : context or { } } )", "predictions": ["encode the specified message at the specified , with the specified , and the create level ."], "references": ["sends a message to the instance s logger if configured ."], "bleu": 0.0859076483566362, "rouge_l": 0.22289890377588306}
{"id": 1845, "code": "def address from digest ( digest ) : address trits = [ 0 ] * ( Address . LEN * TRITS PER TRYTE ) sponge = Kerl ( ) sponge . absorb ( digest . as trits ( ) ) sponge . squeeze ( address trits ) return Address . from trits ( trits = address trits , key index = digest . key index , security level = digest . security level , )", "predictions": ["get the encode encode connection object ."], "references": ["generates an address from a private key digest ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1846, "code": "def encode ( self , input , errors = 'strict' ) : if isinstance ( input , memoryview ) : input = input . tobytes ( ) if not isinstance ( input , ( binary type , bytearray ) ) : raise with context ( exc = Type Error ( \"Can't encode {type}; byte string expected.\" . format ( type = type ( input ) . name , ) ) , context = { 'input' : input , } , ) if not isinstance ( input , bytearray ) : input = bytearray ( input ) trytes = bytearray ( ) for c in input : second , first = divmod ( c , len ( self . alphabet ) ) trytes . append ( self . alphabet [ first ] ) trytes . append ( self . alphabet [ second ] ) return binary type ( trytes ) , len ( input )", "predictions": ["encodes the object that has already been written out ."], "references": ["encodes a byte string into trytes ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 1847, "code": "def decode ( self , input , errors = 'strict' ) : if isinstance ( input , memoryview ) : input = input . tobytes ( ) if not isinstance ( input , ( binary type , bytearray ) ) : raise with context ( exc = Type Error ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( input ) . name , ) ) , context = { 'input' : input , } , ) if not isinstance ( input , bytearray ) : input = bytearray ( input ) bytes = bytearray ( ) for i in range ( 0 , len ( input ) , 2 ) : try : first , second = input [ i : i + 2 ] except Value Error : if errors == 'strict' : raise with context ( exc = Trytes Decode Error ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) elif errors == 'replace' : bytes += b'?' continue try : bytes . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) except Value Error : if errors == 'strict' : raise with context ( exc = Trytes Decode Error ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) elif errors == 'replace' : bytes += b'?' return binary type ( bytes ) , len ( input )", "predictions": ["decodes bytes from the provided byte stream ."], "references": ["decodes a tryte string into bytes ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 1848, "code": "def find addresses ( self , seed , index , count , security level , checksum ) : generator = Address Generator ( seed , security level , checksum ) if count is None : for addy in generator . create iterator ( start = index ) : response = Find Transactions Command ( self . adapter ) ( addresses = [ addy . address ] , ) if not response . get ( 'hashes' ) : return [ addy ] return generator . get addresses ( start = index , count = count )", "predictions": ["encode the span between the generator and , ."], "references": ["find addresses matching the command parameters ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 1849, "code": "def as tryte string ( self ) : return Transaction Trytes ( self . signature message fragment + self . address . address + self . value as trytes + self . legacy tag + self . timestamp as trytes + self . current index as trytes + self . last index as trytes + self . bundle hash + self . trunk transaction hash + self . branch transaction hash + self . tag + self . attachment timestamp as trytes + self . attachment timestamp lower bound as trytes + self . attachment timestamp upper bound as trytes + self . nonce )", "predictions": ["creates a ( expressed strip strip attachment strip attachment strip attachment attachment strip ."], "references": ["returns a trytestring representation of the transaction ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 1850, "code": "def is confirmed ( self , new is confirmed ) : self . is confirmed = new is confirmed for txn in self : txn . is confirmed = new is confirmed", "predictions": ["checks if this fake message is a response ."], "references": ["sets the is_confirmed for the bundle ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 1851, "code": "def group transactions ( self ) : groups = [ ] if self : last txn = self . tail transaction current group = [ last txn ] for current txn in self . transactions [ 1 : ] : if current txn . address == last txn . address : current group . append ( current txn ) else : groups . append ( current group ) current group = [ current txn ] last txn = current txn if current group : groups . append ( current group ) return groups", "predictions": ["def def . . now , we need to set limited to this method to pull multiple duration of the transaction ."], "references": ["groups transactions in the bundle by address ."], "bleu": 0.0612957497932821, "rouge_l": 0.14558472553699284}
{"id": 1852, "code": "def errors ( self ) : try : self . errors . extend ( self . validator ) except Stop Iteration : pass return self . errors", "predictions": ["properly = accept this object ."], "references": ["returns all errors found with the bundle ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1853, "code": "def is valid ( self ) : if not self . errors : try : self . errors . append ( next ( self . validator ) ) except Stop Iteration : pass return not self . errors", "predictions": ["check permission of a single admin ."], "references": ["returns whether the bundle is valid ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1854, "code": "def create validator ( self ) : grouped transactions = self . bundle . group transactions ( ) bundle hash = self . bundle . hash last index = len ( self . bundle ) - 1 balance = 0 counter = 0 for group in grouped transactions : for txn in group : balance += txn . value if txn . bundle hash != bundle hash : yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) if txn . current index != counter : yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current index , i = counter , ) ) if txn . last index != last index : yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last index , expected = last index , i = counter , ) ) counter += 1 if balance != 0 : yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) if not self . errors : signature validation queue = [ ] for group in grouped transactions : if group [ 0 ] . value >= 0 : continue validate group signature = True for j , txn in enumerate ( group ) : if ( j > 0 ) and ( txn . value != 0 ) : yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , i = txn . current index , ) ) validate group signature = False continue # # # if validate group signature : signature validation queue . append ( group ) if signature validation queue : for error in self . get bundle signature errors ( signature validation queue ) : yield error", "predictions": ["def get a get request ."], "references": ["creates a generator that does all the work ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1855, "code": "def start repl ( api ) : banner = ( 'IOTA API client for {uri} ({testnet}) ' 'initialized as variable `api`.\\n' 'Type `help(api)` for list of API commands.' . format ( testnet = 'testnet' if api . testnet else 'mainnet' , uri = api . adapter . get uri ( ) , ) ) scope vars = { 'api' : api } try : import I Python except Import Error : from code import Interactive Console Interactive Console ( locals = scope vars ) . interact ( banner , '' ) else : print ( banner ) I Python . start ipython ( argv = [ ] , user ns = scope vars )", "predictions": ["starts an with the specified ) = completion self - testnet ."], "references": ["starts the repl ."], "bleu": 0.1235622127262679, "rouge_l": 0.4121621621621622}
{"id": 1856, "code": "def Security Level ( ) : return ( f . Type ( int ) | f . Min ( 1 ) | f . Max ( 3 ) | f . Optional ( default = Address Generator . DEFAULT SECURITY LEVEL ) )", "predictions": ["gets the ( node for this node ."], "references": ["generates a filter chain for validating a security level ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 1857, "code": "def as tryte string ( self ) : if not self . bundle hash : raise with context ( exc = Runtime Error ( 'Cannot get Tryte String representation of {cls} instance ' 'without a bundle hash; call ``bundle.finalize()`` first ' '(``exc.context`` has more info).' . format ( cls = type ( self ) . name , ) , ) , context = { 'transaction' : self , } , ) return super ( Proposed Transaction , self ) . as tryte string ( )", "predictions": ["convert the string into a bundle ."], "references": ["returns a trytestring representation of the transaction ."], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 1858, "code": "def tag ( self ) : for txn in reversed ( self ) : if txn . tag : return txn . tag return Tag ( b'' )", "predictions": ["get list of registered ( ."], "references": ["determines the most relevant tag for the bundle ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1859, "code": "def finalize ( self ) : if self . hash : raise Runtime Error ( 'Bundle is already finalized.' ) if not self : raise Value Error ( 'Bundle has no transactions.' ) balance = self . balance if balance < 0 : if self . change address : self . add transaction ( Proposed Transaction ( address = self . change address , value = - balance , tag = self . tag , ) ) else : raise Value Error ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send unspent inputs to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) elif balance > 0 : raise Value Error ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) while True : sponge = Kerl ( ) last index = len ( self ) - 1 for i , txn in enumerate ( self ) : txn . current index = i txn . last index = last index sponge . absorb ( txn . get signature validation trytes ( ) . as trits ( ) ) bundle hash trits = [ 0 ] * HASH LENGTH sponge . squeeze ( bundle hash trits ) bundle hash = Bundle Hash . from trits ( bundle hash trits ) if any ( 13 in part for part in normalize ( bundle hash ) ) : tail transaction = ( self . tail transaction ) tail transaction . increment legacy tag ( ) else : break for txn in self : txn . bundle hash = bundle hash txn . signature message fragment = Fragment ( txn . message or b'' )", "predictions": ["for each transaction that has been requested to the session ."], "references": ["finalizes the bundle preparing it to be attached to the tangle ."], "bleu": 0.14709132836587344, "rouge_l": 0.25884016973125884}
{"id": 1860, "code": "def sign inputs ( self , key generator ) : if not self . hash : raise Runtime Error ( 'Cannot sign inputs until bundle is finalized.' ) i = 0 while i < len ( self ) : txn = self [ i ] if txn . value < 0 : if txn . address . key index is None : raise with context ( exc = Value Error ( 'Unable to sign input {input}; ' '``key index`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) if txn . address . security level is None : raise with context ( exc = Value Error ( 'Unable to sign input {input}; ' '``security level`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) self . sign input at ( i , key generator . get key for ( txn . address ) ) i += txn . address . security level else : i += 1", "predictions": ["sign each ( or sign of the given key using the appropriate ( method ."], "references": ["sign inputs in a finalized bundle ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 1861, "code": "def create input transactions ( self , addy ) : self . transactions . append ( Proposed Transaction ( address = addy , tag = self . tag , value = - addy . balance , ) ) for in range ( addy . security level - 1 ) : self . transactions . append ( Proposed Transaction ( address = addy , tag = self . tag , value = 0 , ) )", "predictions": ["creates a configuration element to send the specified address to the load balancer ."], "references": ["creates transactions for the specified input address ."], "bleu": 0.13834368456410945, "rouge_l": 0.47805642633228845}
{"id": 1862, "code": "def decompress G1 ( z : G1Compressed ) -> G1Uncompressed : b flag = ( z % POW 2 383 ) // POW 2 382 if b flag == 1 : return Z1 x = z % POW 2 381 y = pow ( ( x ** 3 + b . n ) % q , ( q + 1 ) // 4 , q ) if pow ( y , 2 , q ) != ( x ** 3 + b . n ) % q : raise Value Error ( \"The given point is not on G1: y**2 = x**3 + b\" ) a flag = ( z % POW 2 382 ) // POW 2 381 if ( y * 2 ) // q != a flag : y = q - y return ( FQ ( x ) , FQ ( y ) , FQ ( 1 ) )", "predictions": ["decompress the arrowhead value by the remainder of the remainder of the two ( ."], "references": ["recovers x and y coordinates from the compressed point ."], "bleu": 0.09103526405546068, "rouge_l": 0.1659863945578231}
{"id": 1863, "code": "def prime field inv ( a : int , n : int ) -> int : if a == 0 : return 0 lm , hm = 1 , 0 low , high = a % n , n while low > 1 : r = high // low nm , new = hm - lm * r , high - low * r lm , low , hm , high = nm , new , lm , low return lm % n", "predictions": ["generates a new inverse of the polynomial ."], "references": ["extended euclidean algorithm to find modular inverses for integers"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 1864, "code": "def repr html ( self ) : rows , c = '' , '' s = '<tr><td><strong>{k}</strong></td><td style=\"{stl}\">{v}</td></tr>' for k , v in self . dict . items ( ) : if k == ' colour' : k = 'colour' c = utils . text colour for hex ( v ) style = 'color:{}; background-color:{}' . format ( c , v ) else : style = 'color:black; background-color:white' if k == 'component' : try : v = v . repr html ( ) except Attribute Error : v = v . repr ( ) rows += s . format ( k = k , v = v , stl = style ) html = '<table>{}</table>' . format ( rows ) return html", "predictions": ["repr() this generator with another repr of the given attributes ."], "references": ["jupyter notebook magic repr function ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 1865, "code": "def random ( cls , component ) : colour = random . sample ( [ i for i in range ( 256 ) ] , 3 ) return cls ( { 'colour' : colour , 'component' : component , 'width' : 1.0 } )", "predictions": ["gets a random colour of this class"], "references": ["returns a minimal decor with a random colour ."], "bleu": 0.2598013194025897, "rouge_l": 0.3667334669338677}
{"id": 1866, "code": "def repr html ( self ) : all keys = list ( set ( itertools . chain ( * [ d . keys for d in self ] ) ) ) rows = '' for decor in self : th , tr = decor . repr html row ( keys = all keys ) rows += '<tr>{}</tr>' . format ( tr ) header = '<tr>{}</tr>' . format ( th ) html = '<table>{}{}</table>' . format ( header , rows ) return html", "predictions": ["a higher order function that takes care of all decor points and returns a single html formatted function ."], "references": ["jupyter notebook magic repr function ."], "bleu": 0.08475426399505566, "rouge_l": 0.1765557163531114}
{"id": 1867, "code": "def repr html ( self ) : rows = '' s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>' for k , v in self . dict . items ( ) : rows += s . format ( k = k , v = v ) html = '<table>{}</table>' . format ( rows ) return html", "predictions": ["http : / / www . com / svgmobile12 / ( / ( / ( / ( / ( / ( / ( / ( . html"], "references": ["jupyter notebook magic repr function ."], "bleu": 0.044915755686574035, "rouge_l": 0.06846240179573512}
{"id": 1868, "code": "def Rock ( * args , * * kwargs ) : with warnings . catch warnings ( ) : warnings . simplefilter ( \"always\" ) w = \"The 'Rock' class was renamed 'Component'. \" w += \"Please update your code.\" warnings . warn ( w , Deprecation Warning , stacklevel = 2 ) return Component ( * args , * * kwargs )", "predictions": ["minimal - like ( ."], "references": ["graceful deprecation for old class name ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1869, "code": "def process row ( text , columns ) : if not text : return coldict = { k : { 'start' : s , 'len' : l , 'read' : r , 'write' : w } for k , ( s , l , r , w ) in columns . items ( ) } item = { } for field in coldict : value = get field ( text , coldict , field ) if value is not None : item [ field ] = value return item", "predictions": ["copies text of text without columns ."], "references": ["processes a single row from the file ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1870, "code": "def parse canstrat ( text ) : result = { } for row in text . split ( '\\n' ) : if not row : continue if len ( row ) < 8 : continue row header = process row ( row , columns ) or { 'card' : None } card = row header [ 'card' ] if card is not None : item = process row ( row , columns [ card ] ) this list = result . get ( card , [ ] ) this list . append ( item ) result [ card ] = this list for c , d in result . items ( ) : if len ( d ) == 1 : result [ c ] = d [ 0 ] return result", "predictions": ["parse all items from the command line ."], "references": ["read all the rows and return a dict of the results ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 1871, "code": "def get template ( name ) : text = re . sub ( r'\\r\\n' , r'\\n' , name ) text = re . sub ( r'\\{([FISDE\u00b0].*?)\\}',   '{{\\1}}',   ext) return text", "predictions": ["generates the template for a given template ."], "references": ["still unsure about best way to do this hence cruft ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 1872, "code": "def clean longitudinal data ( cls , data , null = None ) : if ( 'top' not in data . keys ( ) ) : data [ 'top' ] = data . pop ( 'depth' , data . pop ( 'MD' , None ) ) idx = list ( data . keys ( ) ) . index ( 'top' ) values = sorted ( zip ( * data . values ( ) ) , key = lambda x : x [ idx ] ) data = { k : list ( v ) for k , v in zip ( data . keys ( ) , zip ( * values ) ) } if data [ 'top' ] is None : raise Striplog Error ( 'Could not get tops.' ) if null is not None : for k , v in data . items ( ) : data [ k ] = [ i if i != null else None for i in v ] return data", "predictions": ["clean out all attributes in a zip file that are not found in the cache ."], "references": ["private function . make sure we have what we need to make a striplog ."], "bleu": 0.08513012360883544, "rouge_l": 0.12978723404255318}
{"id": 1873, "code": "def from csv ( cls , filename = None , text = None , dlm = ',' , lexicon = None , points = False , include = None , exclude = None , remap = None , function = None , null = None , ignore = None , source = None , stop = None , fieldnames = None ) : if ( filename is None ) and ( text is None ) : raise Striplog Error ( \"You must provide a filename or CSV text.\" ) if ( filename is not None ) : if source is None : source = filename with open ( filename , 'r' ) as f : text = f . read ( ) source = source or 'CSV' if dlm == ' ' : text = re . sub ( r'[ \\t]+' , ' ' , text ) if fieldnames is not None : text = dlm . join ( fieldnames ) + '\\n' + text try : f = String IO ( text ) except Type Error : f = String IO ( unicode ( text ) ) reader = csv . Dict Reader ( f , delimiter = dlm ) reorg = { k . strip ( ) . lower ( ) : [ ] for k in reader . fieldnames if k is not None } t = f . tell ( ) for key in reorg : f . seek ( t ) for r in reader : s = { k . strip ( ) . lower ( ) : v . strip ( ) for k , v in r . items ( ) } try : reorg [ key ] . append ( float ( s [ key ] ) ) except Value Error : reorg [ key ] . append ( s [ key ] ) f . close ( ) remap = remap or { } for k , v in remap . items ( ) : reorg [ v ] = reorg . pop ( k ) data = cls . clean longitudinal data ( reorg , null = null ) list of Intervals = cls . build list of Intervals ( data , points = points , lexicon = lexicon , include = include , exclude = exclude , ignore = ignore , stop = stop ) return cls ( list of Intervals , source = source )", "predictions": ["convert a module and serial lexicon to a corresponding lexicon ."], "references": ["load from a csv file or text ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 1874, "code": "def from img ( cls , * args , * * kwargs ) : with warnings . catch warnings ( ) : warnings . simplefilter ( \"always\" ) w = \"from img() is deprecated; please use from image()\" warnings . warn ( w ) return cls . from image ( * args , * * kwargs )", "predictions": ["convert an image to a image()\" ."], "references": ["for backwards compatibility ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1875, "code": "def from canstrat ( cls , filename , source = 'canstrat' ) : with open ( filename ) as f : dat = f . read ( ) data = parse canstrat ( dat ) list of Intervals = [ ] for d in data [ 7 ] : if d . pop ( 'skip' ) : continue top = d . pop ( 'top' ) base = d . pop ( 'base' ) comps = [ Component ( { 'lithology' : d [ 'rtc' ] , 'colour' : d [ 'colour name' ] } ) ] iv = Interval ( top = top , base = base , components = comps , data = d ) list of Intervals . append ( iv ) return cls ( list of Intervals , source = source )", "predictions": ["internal helper method for constructing a class from a file ."], "references": ["eat a canstrat dat file and make a striplog ."], "bleu": 0.14323145079400493, "rouge_l": 0.28818897637795277}
{"id": 1876, "code": "def copy ( self ) : return Striplog ( [ i . copy ( ) for i in self ] , order = self . order , source = self . source )", "predictions": ["copies content of this list into a copy of the specified list ."], "references": ["returns a shallow copy ."], "bleu": 0.1135935489027116, "rouge_l": 0.36237623762376237}
{"id": 1877, "code": "def get data ( self , field , function = None , default = None ) : f = function or utils . null data = [ ] for iv in self : d = iv . data . get ( field ) if d is None : if default is not None : d = default else : d = np . nan data . append ( f ( d ) ) return np . array ( data )", "predictions": ["get data for a method ."], "references": ["get data from the striplog ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 1878, "code": "def depth ( self , d ) : with warnings . catch warnings ( ) : warnings . simplefilter ( \"always\" ) w = \"depth() is deprecated; please use read at()\" warnings . warn ( w ) return self . read at ( d )", "predictions": ["reads and returns the depth of the given object ."], "references": ["for backwards compatibility ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 1879, "code": "def dict repr html ( dictionary ) : rows = '' s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>' for k , v in dictionary . items ( ) : rows += s . format ( k = k , v = v ) html = '<table>{}</table>' . format ( rows ) return html", "predictions": ["generate latex with column names ."], "references": ["jupyter notebook magic repr function ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1880, "code": "def convert field ( self , value , conversion ) : try : s = super ( Custom Formatter , self ) return s . convert field ( value , conversion ) except Value Error : funcs = { 's' : str , 'r' : repr , 'a' : ascii , 'u' : str . upper , 'l' : str . lower , 'c' : str . capitalize , 't' : str . title , 'm' : np . mean , '\u00b5':   p. m ean, 'v' : np . var , 'd' : np . std , '+' : np . sum , '\u2211':  n .s u m, 'x' : np . product , } return funcs . get ( conversion ) ( value )", "predictions": ["converts a field or limit\" style to a field ."], "references": ["define some extra field conversion functions ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 1881, "code": "def get random ( self , obj type ) : return self . mutator [ obj type ] [ random . randint ( 0 , self . config . level ) ]", "predictions": ["get the random object for this object ."], "references": ["get a random mutator from a list of mutators"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1882, "code": "def get mutator ( self , obj , obj type ) : if obj type == unicode : obj type = str obj = str ( obj ) return self . get random ( obj type ) ( obj )", "predictions": ["get a random object for this object ."], "references": ["get a random mutator for the given type"], "bleu": 0.3155984539112945, "rouge_l": 0.5}
{"id": 1883, "code": "def get string polyglot attack ( self , obj ) : return self . polyglot attacks [ random . choice ( self . config . techniques ) ] % obj", "predictions": ["get a string representation of this object ."], "references": ["return a polyglot attack containing the original object"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1884, "code": "def safe unicode ( self , buf ) : tmp = \"\" buf = \"\" . join ( b for b in buf ) for character in buf : tmp += character return tmp", "predictions": ["buffered buffered data in a byte array ."], "references": ["safely return an unicode encoded string"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1885, "code": "def custom html ( self , filepath ) : try : response . headers . append ( \"Access-Control-Allow-Origin\" , \"*\" ) response . headers . append ( \"Accept-Encoding\" , \"identity\" ) response . headers . append ( \"Content-Type\" , \"text/html\" ) return static file ( filepath , root = self . config . html ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["custom custom html requests ."], "references": ["serve custom html page"], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 1886, "code": "def serve ( self ) : try : fuzzed = self . json . fuzzed if self . config . fuzz web : self . client queue . put ( ( request . environ . get ( 'REMOTE ADDR' ) , fuzzed ) ) response . headers . append ( \"Access-Control-Allow-Origin\" , \"*\" ) response . headers . append ( \"Accept-Encoding\" , \"identity\" ) response . headers . append ( \"Content-Type\" , self . config . content type ) if self . config . notify : PJF Testcase Server . send testcase ( fuzzed , '127.0.0.1' , self . config . ports [ \"servers\" ] [ \"TCASE PORT\" ] ) yield fuzzed except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["for the given request ."], "references": ["serve fuzzed json object"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1887, "code": "def fuzz ( self , obj ) : decorators = self . decorators @ decorators . mutate object decorate def mutate ( ) : return obj return mutate ( )", "predictions": ["get method for generating content type ."], "references": ["generic fuzz mutator use a decorator for the given type"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1888, "code": "def spawn ( self , cmd , stdin content = \"\" , stdin = False , shell = False , timeout = 2 ) : try : if type ( cmd ) != list : raise PJF Invalid Type ( type ( cmd ) , list ) if type ( stdin content ) != str : raise PJF Invalid Type ( type ( stdin content ) , str ) if type ( stdin ) != bool : raise PJF Invalid Type ( type ( stdin ) , bool ) self . in = stdin content try : self . process = subprocess . Popen ( cmd , stdout = PIPE , stderr = PIPE , stdin = PIPE , shell = shell ) self . finish read ( timeout , stdin content , stdin ) if self . process . poll ( ) is not None : self . close ( ) except Keyboard Interrupt : return except OS Error : raise PJF Process Execution Error ( \"Binary <%s> does not exist\" % cmd [ 0 ] ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["spawns spawns to the device ."], "references": ["spawn a new process using subprocess"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1889, "code": "def get output ( self , stdin content , stdin ) : try : if stdin : if sys . version info >= ( 3 , 0 ) : self . process . stdin . write ( bytes ( stdin content , \"utf-8\" ) ) else : self . process . stdin . write ( stdin content ) self . out = self . process . communicate ( ) [ 0 ] except ( error , IO Error ) : self . out = self . in pass", "predictions": ["configure this migration to the given output raise an exception ."], "references": ["try to get output in a separate thread"], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 1890, "code": "def finish read ( self , timeout = 2 , stdin content = \"\" , stdin = False ) : process = Thread ( target = self . get output , args = ( stdin content , stdin ) ) process . start ( ) if timeout > 0 : process . join ( timeout ) else : process . join ( ) if process . is alive ( ) : self . close ( ) self . return code = - signal . SIGHUP else : self . return code = self . process . returncode", "predictions": ["provides a dummy ( i . e . , the output ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["wait until we got output or until timeout is over"], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 1891, "code": "def close ( self ) : try : self . process . terminate ( ) self . return code = self . process . returncode except OS Error : pass self . process . stdin . close ( ) self . process . stdout . close ( ) self . process . stderr . close ( ) self . logger . debug ( \"[{0}] - PJF Executor successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) )", "predictions": ["closes the response and closes the underlying connection ."], "references": ["terminate the newly created process"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 1892, "code": "def start ( self ) : from . pjf worker import PJF Worker worker = PJF Worker ( self ) if self . update pjf : worker . update library ( ) elif self . browser auto : worker . browser autopwn ( ) elif self . fuzz web : worker . web fuzzer ( ) elif self . json : if not self . web server and not self . ext fuzz and not self . cmd fuzz : worker . fuzz ( ) elif self . ext fuzz : if self . stdin : worker . fuzz stdin ( ) else : worker . fuzz command line ( ) elif self . cmd fuzz : if self . stdin : worker . fuzz external ( True ) else : worker . fuzz external ( ) else : worker . start http server ( ) elif self . json file : worker . start file fuzz ( ) elif self . process to monitor : worker . start process monitor ( )", "predictions": ["this method starts a : [ pid ] : [ : 1 ] : [ : 2 ] : 1 ] : [ : 1 ] [ 2 ] : 2 ] [ 2 ] [ : 1 ] generator : [ 2 ] generator : [ 2 ] ["], "references": ["parse the command line and start pyjfuzz"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 1893, "code": "def execute ( self , obj ) : try : if self . config . stdin : self . spawn ( self . config . command , stdin content = obj , stdin = True , timeout = 1 ) else : if \"@@\" not in self . config . command : raise PJF Missing Argument ( \"Missing @@ filename indicator while using non-stdin fuzzing method\" ) for x in self . config . command : if \"@@\" in x : self . config . command [ self . config . command . index ( x ) ] = x . replace ( \"@@\" , obj ) self . spawn ( self . config . command , timeout = 2 ) self . logger . debug ( \"[{0}] - PJF External Fuzzer successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) return self . out except Keyboard Interrupt : return \"\" except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["executes the object execution on the run of the object . this method is called from the server before the server ' s call ."], "references": ["perform the actual external fuzzing you may replace this method in order to increase performance"], "bleu": 0.06871624004919695, "rouge_l": 0.1570815450643777}
{"id": 1894, "code": "def shutdown ( self , * args ) : try : self . shutdown ( ) if self . process : self . process . wait ( ) self . process . stdout . close ( ) self . process . stdin . close ( ) self . process . stderr . close ( ) self . finished = True self . send testcase ( '' , '127.0.0.1' , self . config . ports [ \"servers\" ] [ \"TCASE PORT\" ] ) self . logger . debug ( \"[{0}] - PJF Process Monitor successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["this method is called when the server selects a process ."], "references": ["shutdown the running process and the monitor"], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 1895, "code": "def run and monitor ( self ) : signal . signal ( signal . SIGINT , self . shutdown ) self . spawn ( self . config . process to monitor , timeout = 0 ) return self . is sigsegv ( self . return code )", "predictions": ["runs the addon process ."], "references": ["run command once and check exit code"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1896, "code": "def start monitor ( self , standalone = True ) : try : self . start ( ) cmdline = shlex . split ( self . config . process to monitor ) if standalone : signal . signal ( signal . SIGINT , self . shutdown ) self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) while self . process and not self . finished : self . process . wait ( ) if self . is sigsegv ( self . process . returncode ) : if self . config . debug : print ( \"[\\033[92m INFO\\033[0m] Process crashed with \\033[91m SIGSEGV\\033[0m, waiting for testcase...\" ) while not self . got testcase ( ) : time . sleep ( 1 ) self . save testcase ( self . testcase [ - 10 : ] ) if self . process : self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) except OS Error : self . shutdown ( ) self . process = False self . got testcase = lambda : True raise PJF Process Execution Error ( \"Binary <%s> does not exist\" % cmdline [ 0 ] ) except Exception as e : raise PJF Base Exception ( \"Unknown error please send log to author\" )", "predictions": ["this method starts the unloads process ."], "references": ["run command in a loop and check exit status plus restart process when needed"], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 1897, "code": "def fuzz elements ( self , element ) : try : if type ( element ) == dict : tmp element = { } for key in element : if len ( self . config . parameters ) > 0 : if self . config . exclude parameters : fuzz = key not in self . config . parameters else : fuzz = key in self . config . parameters else : fuzz = True if fuzz : if type ( element [ key ] ) == dict : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) elif type ( element [ key ] ) == list : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) else : tmp element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) else : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) element = tmp element del tmp element elif type ( element ) == list : arr = [ ] for key in element : if type ( key ) == dict : arr . append ( self . fuzz elements ( key ) ) elif type ( key ) == list : arr . append ( self . fuzz elements ( key ) ) else : if len ( self . config . parameters ) <= 0 : arr . append ( self . mutator . fuzz ( key ) ) else : arr . append ( key ) element = arr del arr except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) return element", "predictions": ["makes a shallow copy of this map with the same 3 ( but as the original operation : this is a separate operation : this method is used to add some elements to each component ."], "references": ["fuzz all elements inside the object"], "bleu": 0.03709091243806319, "rouge_l": 0.05465949820788529}
{"id": 1898, "code": "def fuzzed ( self ) : try : if self . config . strong fuzz : fuzzer = PJF Mutators ( self . config ) if self . config . url encode : if sys . version info >= ( 3 , 0 ) : return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) else : return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) else : if type ( self . config . json ) in [ list , dict ] : return fuzzer . fuzz ( json . dumps ( self . config . json ) ) else : return fuzzer . fuzz ( self . config . json ) else : if self . config . url encode : if sys . version info >= ( 3 , 0 ) : return urllib . parse . quote ( self . get fuzzed ( self . config . indent , self . config . utf8 ) ) else : return urllib . quote ( self . get fuzzed ( self . config . indent , self . config . utf8 ) ) else : return self . get fuzzed ( self . config . indent , self . config . utf8 ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["creates and passes this json instance to the repr function ."], "references": ["get a printable fuzzed object"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1899, "code": "def get fuzzed ( self , indent = False , utf8 = False ) : try : if \"array\" in self . json : return self . fuzz elements ( dict ( self . json ) ) [ \"array\" ] else : return self . fuzz elements ( dict ( self . json ) ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["call this method and return the ."], "references": ["return the fuzzed object"], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 1900, "code": "def mutate object decorate ( self , func ) : def mutate ( ) : obj = func ( ) return self . Mutators . get mutator ( obj , type ( obj ) ) return mutate", "predictions": ["tell this method can be called on a specific ( ( : https : / / www . org / tr / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( /"], "references": ["mutate a generic object based on type"], "bleu": 0.026594139297659906, "rouge_l": 0.04061251664447404}
{"id": 1901, "code": "def get User Id ( self ) : self . user Id = self ( \"GET\" , \"{0}/users/self/profile\" . format ( self . API USER ) , auth = self . Auth . Skype Token ) . json ( ) . get ( \"username\" )", "predictions": ["use this to process the if the if it exists ."], "references": ["ask skype for the authenticated user s identifier and store it on the connection object ."], "bleu": 0.09091421815660788, "rouge_l": 0.21504112808460632}
{"id": 1902, "code": "def sync Endpoints ( self ) : self . endpoints [ \"all\" ] = [ ] for json in self ( \"GET\" , \"{0}/users/ME/presence Docs/messaging Service\" . format ( self . msgs Host ) , params = { \"view\" : \"expanded\" } , auth = self . Auth . Reg Token ) . json ( ) . get ( \"endpoint Presence Docs\" , [ ] ) : id = json . get ( \"link\" , \"\" ) . split ( \"/\" ) [ 7 ] self . endpoints [ \"all\" ] . append ( Skype Endpoint ( self , id ) )", "predictions": ["parse the ( list of players ."], "references": ["retrieve all current endpoints for the connected user ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 1903, "code": "def u ( text , encoding = 'utf-8' ) : if isinstance ( text , six . binary type ) : text = text . decode ( encoding ) text = text . replace ( '\\r\\n' , '\\n' ) return text", "predictions": ["replaces links with the given ) ."], "references": ["return unicode text no matter what"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1904, "code": "def to dict ( self ) : d = self . metadata . copy ( ) d [ 'content' ] = self . content return d", "predictions": ["returns a longitudinal as a shallow copy of this dictionary ."], "references": ["post as a dict for serializing"], "bleu": 0.14991106946711685, "rouge_l": 0.2484725050916497}
{"id": 1905, "code": "def load ( self , fm , * * kwargs ) : kwargs . setdefault ( 'Loader' , Safe Loader ) return yaml . load ( fm , * * kwargs )", "predictions": ["makes a exclude from a path and returns the corresponding result ."], "references": ["parse yaml front matter . this uses yaml . safeloader by default ."], "bleu": 0.09559539481714499, "rouge_l": 0.07942708333333334}
{"id": 1906, "code": "def export ( self , metadata , * * kwargs ) : kwargs . setdefault ( 'Dumper' , Safe Dumper ) kwargs . setdefault ( 'default flow style' , False ) kwargs . setdefault ( 'allow unicode' , True ) metadata = yaml . dump ( metadata , * * kwargs ) . strip ( ) return u ( metadata )", "predictions": ["exports the , like ( and ( ."], "references": ["export metadata as yaml . this uses yaml . safedumper by default ."], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 1907, "code": "def export ( self , metadata , * * kwargs ) : kwargs . setdefault ( 'indent' , 4 ) metadata = json . dumps ( metadata , * * kwargs ) return u ( metadata )", "predictions": ["copies this utterance to a one ."], "references": ["turn metadata into json"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1908, "code": "def match ( self ) : cache match , cache string = self . match cache string = self . string if cache string == string : return cache match cache match = fullmatch ( LIST PATTERN FORMAT . replace ( b'{pattern}' , self . pattern . encode ( ) ) , self . shadow , MULTILINE , ) self . match cache = cache match , string return cache match", "predictions": ["this function matches the return value of the return locale . the return value will be returned in the return value of the return value ."], "references": ["return the match object for the current list ."], "bleu": 0.058697608930387266, "rouge_l": 0.2505133470225872}
{"id": 1909, "code": "def convert ( self , newstart : str ) -> None : match = self . match ms = match . start ( ) for s , e in reversed ( match . spans ( 'pattern' ) ) : self [ s - ms : e - ms ] = newstart self . pattern = escape ( newstart )", "predictions": ["creates a hack that converts the given colors to a ) style ."], "references": ["convert to another list type by replacing starting pattern ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 1910, "code": "def arguments ( self ) -> List [ Argument ] : shadow = self . shadow split spans = self . args matcher ( shadow ) . spans ( 'arg' ) if not split spans : return [ ] arguments = [ ] arguments append = arguments . append type to spans = self . type to spans ss , se = span = self . span type = id ( span ) lststr = self . lststr string = lststr [ 0 ] arg spans = type to spans . setdefault ( type , [ ] ) span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in arg spans } . get for arg self start , arg self end in split spans : s , e = arg span = [ ss + arg self start , ss + arg self end ] old span = span tuple to span get ( ( s , e ) ) if old span is None : insort ( arg spans , arg span ) else : arg span = old span arg = Argument ( lststr , type to spans , arg span , type ) arg . shadow cache = ( string [ s : e ] , shadow [ arg self start : arg self end ] ) arguments append ( arg ) return arguments", "predictions": ["depth just depth - depth depth arguments . this function is not used as a convenience method ."], "references": ["parse template content . create self . name and self . arguments ."], "bleu": 0.09629943614188137, "rouge_l": 0.13289760348583876}
{"id": 1911, "code": "def pattern ( trie : dict ) -> str : if '' in trie : if len ( trie ) == 1 : return '' optional = True del trie [ '' ] else : optional = False subpattern to chars = defaultdict ( list ) for char , sub trie in trie . items ( ) : subpattern = pattern ( sub trie ) subpattern to chars [ subpattern ] . append ( char ) alts = [ ] for subpattern , chars in subpattern to chars . items ( ) : if len ( chars ) == 1 : alts . append ( chars [ 0 ] + subpattern ) else : chars . sort ( reverse = True ) alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) if len ( alts ) == 1 : result = alts [ 0 ] if optional : if len ( result ) == 1 : result += '?+' else : result = '(?:' + result + ')?+' else : alts . sort ( reverse = True ) result = '(?>' + '|' . join ( alts ) + ')' if optional : result += '?+' return result", "predictions": ["given a dictionary of html codes . this function finds the matched matched in the comma separated html which indicates the html matched matched by the comment ."], "references": ["convert a trie to a regex pattern ."], "bleu": 0.0478968583748614, "rouge_l": 0.1234817813765182}
{"id": 1912, "code": "def atomic partition ( self , char : int ) -> Tuple [ str , str , str ] : s , e = self . span index = self . shadow . find ( char ) if index == - 1 : return self . lststr [ 0 ] [ s : e ] , '' , '' lststr0 = self . lststr [ 0 ] return lststr0 [ s : s + index ] , chr ( char ) , lststr0 [ s + index + 1 : e ]", "predictions": ["generate a character field by one character in the local character set . this function is used to convert the character to a convert method in the character buffer . each character in the buffer is then returned . the index is then returned by this method ."], "references": ["partition self . string where char s not in atomic sub - spans ."], "bleu": 0.029781169340667525, "rouge_l": 0.1073943661971831}
{"id": 1913, "code": "def subspans ( self , type : str ) -> List [ List [ int ] ] : return self . type to spans [ type ]", "predictions": ["a convenience function for creating a list of strings from the top level list . this is a convenience method that takes two sets of the strings of the list ."], "references": ["return all the sub - span including self . _span ."], "bleu": 0.046398855339878003, "rouge_l": 0.15627668659265584}
{"id": 1914, "code": "def insert update ( self , index : int , length : int ) -> None : ss , se = self . span for spans in self . type to spans . values ( ) : for span in spans : if index < span [ 1 ] or span [ 1 ] == index == se : span [ 1 ] += length if index < span [ 0 ] or span [ 0 ] == index != ss : span [ 0 ] += length", "predictions": ["get a random property ."], "references": ["update self . _type_to_spans according to the added length ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 1915, "code": "def pprint ( self , indent : str = '    ' , remove comments = False ) : warn ( 'pprint method is deprecated, use pformat instead.' , Deprecation Warning , ) return self . pformat ( indent , remove comments )", "predictions": ["registers a method with a given name ."], "references": ["deprecated use self . pformat instead ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1916, "code": "def parameters ( self ) -> List [ 'Parameter' ] : lststr = self . lststr type to spans = self . type to spans return [ Parameter ( lststr , type to spans , span , 'Parameter' ) for span in self . subspans ( 'Parameter' ) ]", "predictions": ["returns a list of the strings for this list ."], "references": ["return a list of parameter objects ."], "bleu": 0.24808415001701817, "rouge_l": 0.48605577689243035}
{"id": 1917, "code": "def parser functions ( self ) -> List [ 'Parser Function' ] : lststr = self . lststr type to spans = self . type to spans return [ Parser Function ( lststr , type to spans , span , 'Parser Function' ) for span in self . subspans ( 'Parser Function' ) ]", "predictions": ["returns a list of the trees that are subclasses of each object ."], "references": ["return a list of parser function objects ."], "bleu": 0.18798317647335086, "rouge_l": 0.39804241435562804}
{"id": 1918, "code": "def templates ( self ) -> List [ 'Template' ] : lststr = self . lststr type to spans = self . type to spans return [ Template ( lststr , type to spans , span , 'Template' ) for span in self . subspans ( 'Template' ) ]", "predictions": ["returns a list of serve strings from this map ."], "references": ["return a list of templates as template objects ."], "bleu": 0.24808415001701817, "rouge_l": 0.42508710801393734}
{"id": 1919, "code": "def wikilinks ( self ) -> List [ 'Wiki Link' ] : lststr = self . lststr type to spans = self . type to spans return [ Wiki Link ( lststr , type to spans , span , 'Wiki Link' ) for span in self . subspans ( 'Wiki Link' ) ]", "predictions": ["a convenience method for constructing a list of words ."], "references": ["return a list of wikilink objects ."], "bleu": 0.24808415001701817, "rouge_l": 0.48605577689243035}
{"id": 1920, "code": "def comments ( self ) -> List [ 'Comment' ] : lststr = self . lststr type to spans = self . type to spans return [ Comment ( lststr , type to spans , span , 'Comment' ) for span in self . subspans ( 'Comment' ) ]", "predictions": ["a convenience method for making a list of comments ."], "references": ["return a list of comment objects ."], "bleu": 0.24808415001701817, "rouge_l": 0.48605577689243035}
{"id": 1921, "code": "def tables ( self ) -> List [ 'Table' ] : tables = [ ] tables append = tables . append type to spans = self . type to spans lststr = self . lststr shadow = self . shadow [ : ] ss , se = self . span spans = type to spans . setdefault ( 'Table' , [ ] ) if not spans : m = True while m : m = False for m in TABLE FINDITER ( shadow ) : ms , me = m . span ( ) span = [ ss + ms + len ( m [ 1 ] ) , ss + me ] spans . append ( span ) tables append ( Table ( lststr , type to spans , span , 'Table' ) ) shadow [ ms : me ] = b' ' * ( me - ms ) return tables span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get m = True while m : m = False for m in TABLE FINDITER ( shadow ) : ms , me = m . span ( ) s , e = ss + ms + len ( m [ 1 ] ) , ss + me old span = span tuple to span get ( ( s , e ) ) if old span is None : span = [ s , e ] insort ( spans , span ) else : span = old span tables append ( Table ( lststr , type to spans , span , 'Table' ) ) shadow [ ms : me ] = b' ' * ( me - ms ) return tables", "predictions": ["a bit or tables that just a tuple of tables . the tables must be in the given type . the cut cut the flag . the cut cut in the coordinate ."], "references": ["return a list of found table objects ."], "bleu": 0.04354004419807856, "rouge_l": 0.1644204851752022}
{"id": 1922, "code": "def tags ( self , name = None ) -> List [ 'Tag' ] : lststr = self . lststr type to spans = self . type to spans if name : if name in tag extensions : string = lststr [ 0 ] return [ Tag ( lststr , type to spans , span , 'Extension Tag' ) for span in type to spans [ 'Extension Tag' ] if string . startswith ( '<' + name , span [ 0 ] ) ] tags = [ ] else : tags = [ Tag ( lststr , type to spans , span , 'Extension Tag' ) for span in type to spans [ 'Extension Tag' ] ] tags append = tags . append ss = self . span [ 0 ] shadow = self . shadow if name : reversed start matches = reversed ( [ m for m in regex compile ( START TAG PATTERN . replace ( rb'{name}' , rb'(?P<name>' + name . encode ( ) + rb')' ) ) . finditer ( shadow ) ] ) end search = regex compile ( END TAG PATTERN . replace ( b'{name}' , name . encode ( ) ) ) . search else : reversed start matches = reversed ( [ m for m in START TAG FINDITER ( shadow ) ] ) shadow copy = shadow [ : ] spans = type to spans . setdefault ( 'Tag' , [ ] ) span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get spans append = spans . append for start match in reversed start matches : if start match [ 'self closing' ] : s , e = start match . span ( ) span = [ ss + s , ss + e ] else : if name : end match = end search ( shadow copy , start match . end ( ) ) else : end match = search ( END TAG PATTERN . replace ( b'{name}' , start match [ 'name' ] ) , shadow copy ) if end match : s , e = end match . span ( ) shadow copy [ s : e ] = b' ' * ( e - s ) span = [ ss + start match . start ( ) , ss + e ] else : s , e = start match . span ( ) span = [ ss + s , ss + e ] old span = span tuple to span get ( ( span [ 0 ] , span [ 1 ] ) ) if old span is None : spans append ( span ) else : span = old span tags append ( Tag ( lststr , type to spans , span , 'Tag' ) ) return sorted ( tags , key = attrgetter ( ' span' ) )", "predictions": ["insert the value that was previously stored in the given self - spans ."], "references": ["return all tags with the given name ."], "bleu": 0.1250076305588977, "rouge_l": 0.28683385579937304}
{"id": 1923, "code": "def subspans ( self , type : str ) -> Generator [ int , None , None ] : ss , se = self . span spans = self . type to spans [ type ] b = bisect ( spans , [ ss ] ) for span in spans [ b : bisect ( spans , [ se ] , b ) ] : if span [ 1 ] <= se : yield span", "predictions": ["generate a set of instances of this class ."], "references": ["yield all the sub - span indices excluding self . _span ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 1924, "code": "def del arg ( self , name : str ) -> None : for arg in reversed ( self . arguments ) : if arg . name . strip ( WS ) == name . strip ( WS ) : del arg [ : ]", "predictions": ["removes a , from the list ."], "references": ["delete all arguments with the given then ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1925, "code": "def to ogc wkt ( self ) : return 'GEOGCS[\"%s\", %s, %s, %s, AXIS[\"Lon\", %s], AXIS[\"Lat\", %s]]' % ( self . name , self . datum . to ogc wkt ( ) , self . prime mer . to ogc wkt ( ) , self . angunit . to ogc wkt ( ) , self . twin ax [ 0 ] . ogc wkt , self . twin ax [ 1 ] . ogc wkt )", "predictions": ["convert this clip to a ogc ."], "references": ["returns the cs as a ogc wkt formatted string ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 1926, "code": "def to esri wkt ( self ) : return 'GEOGCS[\"%s\", %s, %s, %s, AXIS[\"Lon\", %s], AXIS[\"Lat\", %s]]' % ( self . name , self . datum . to esri wkt ( ) , self . prime mer . to esri wkt ( ) , self . angunit . to esri wkt ( ) , self . twin ax [ 0 ] . esri wkt , self . twin ax [ 1 ] . esri wkt )", "predictions": ["convert this mbeanserver to a one for the ( ."], "references": ["returns the cs as a esri wkt formatted string ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 1927, "code": "def to ogc wkt ( self ) : string = 'PROJCS[\"%s\", %s, %s, ' % ( self . name , self . geogcs . to ogc wkt ( ) , self . proj . to ogc wkt ( ) ) string += \", \" . join ( param . to ogc wkt ( ) for param in self . params ) string += ', %s' % self . unit . to ogc wkt ( ) string += ', AXIS[\"X\", %s], AXIS[\"Y\", %s]]' % ( self . twin ax [ 0 ] . ogc wkt , self . twin ax [ 1 ] . ogc wkt ) return string", "predictions": ["convert a string to a ogc"], "references": ["returns the cs as a ogc wkt formatted string ."], "bleu": 0.16038842424444547, "rouge_l": 0.23921568627450981}
{"id": 1928, "code": "def to esri wkt ( self ) : string = 'PROJCS[\"%s\", %s, %s, ' % ( self . name , self . geogcs . to esri wkt ( ) , self . proj . to esri wkt ( ) ) string += \", \" . join ( param . to esri wkt ( ) for param in self . params ) string += ', %s' % self . unit . to esri wkt ( ) string += ', AXIS[\"X\", %s], AXIS[\"Y\", %s]]' % ( self . twin ax [ 0 ] . esri wkt , self . twin ax [ 1 ] . esri wkt ) return string", "predictions": ["convert a string representation of this method to the ("], "references": ["returns the cs as a esri wkt formatted string ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 1929, "code": "def parse geo tiff ( key dir vlr : Geo Key Directory Vlr , double vlr : Geo Double Params Vlr , ascii vlr : Geo Ascii Params Vlr , ) -> List [ Geo Tiff Key ] : geotiff keys = [ ] for k in key dir vlr . geo keys : if k . tiff tag location == 0 : value = k . value offset elif k . tiff tag location == 34736 : value = double vlr . doubles [ k . value offset ] elif k . tiff tag location == 34737 : try : value = ascii vlr . strings [ k . value offset ] [ k . count : ] except Index Error : value = ascii vlr . strings [ 0 ] [ k . value offset : k . value offset + k . count ] else : logger . warning ( \"Geo Tiff Key with unknown tiff tag location ({})\" . format ( k . tiff tag location ) ) continue geotiff keys . append ( Geo Tiff Key ( k . id , value ) ) return geotiff keys", "predictions": ["parse all the distinct distinct keys in a geo tiff key ."], "references": ["parses the geotiff vlrs information into nicer structs"], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 1930, "code": "def copy fields from ( self , other record ) : for dim name in self . dimensions names : try : self [ dim name ] = other record [ dim name ] except Value Error : pass", "predictions": ["copies from one logical frame to another ."], "references": ["tries to copy the values of the current dimensions from other_record"], "bleu": 0.12197601375336842, "rouge_l": 0.10234899328859062}
{"id": 1931, "code": "def from stream ( cls , stream , point format , count ) : points dtype = point format . dtype point data buffer = bytearray ( stream . read ( count * points dtype . itemsize ) ) try : data = np . frombuffer ( point data buffer , dtype = points dtype , count = count ) except Value Error : expected bytes len = count * points dtype . itemsize if len ( point data buffer ) % points dtype . itemsize != 0 : missing bytes len = expected bytes len - len ( point data buffer ) raise not enough bytes error ( expected bytes len , missing bytes len , len ( point data buffer ) , points dtype , ) else : actual count = len ( point data buffer ) // points dtype . itemsize logger . critical ( \"Expected {} points, there are {} ({} missing)\" . format ( count , actual count , count - actual count ) ) data = np . frombuffer ( point data buffer , dtype = points dtype , count = actual count ) return cls ( data , point format )", "predictions": ["construct a stream from this stream ."], "references": ["construct the point record by reading the points from the stream"], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 1932, "code": "def x ( self ) : return scale dimension ( self . X , self . header . x scale , self . header . x offset )", "predictions": ["call this to get the x and y ."], "references": ["returns the scaled x positions of the points as doubles"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 1933, "code": "def y ( self ) : return scale dimension ( self . Y , self . header . y scale , self . header . y offset )", "predictions": ["the y hull around this object ."], "references": ["returns the scaled y positions of the points as doubles"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1934, "code": "def z ( self ) : return scale dimension ( self . Z , self . header . z scale , self . header . z offset )", "predictions": ["get a z to the z half of this class ."], "references": ["returns the scaled z positions of the points as doubles"], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 1935, "code": "def min file version for point format ( point format id ) : for version , point formats in sorted ( VERSION TO POINT FMT . items ( ) ) : if point format id in point formats : return version else : raise errors . Point Format Not Supported ( point format id )", "predictions": ["return a formatted file to talk to all rules ."], "references": ["returns the minimum file version that supports the given point_format_id"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 1936, "code": "def is point fmt compatible with version ( point format id , file version ) : try : return point format id in VERSION TO POINT FMT [ str ( file version ) ] except Key Error : raise errors . File Version Not Supported ( file version )", "predictions": ["checks if the file is dequeued ."], "references": ["returns true if the file version support the point_format_id"], "bleu": 0.2598013194025897, "rouge_l": 0.3667334669338677}
{"id": 1937, "code": "def files have same point format id ( las files ) : point format found = { las . header . point format id for las in las files } return len ( point format found ) == 1", "predictions": ["determine if a particular point should be used ."], "references": ["returns true if all the files have the same points format id"], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 1938, "code": "def files have same dtype ( las files ) : dtypes = { las . points . dtype for las in las files } return len ( dtypes ) == 1", "predictions": ["determine if this is a critical ( for the given dtype ."], "references": ["returns true if all the files have the same numpy datatype"], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 1939, "code": "def raise if wrong file signature ( stream ) : file sig = stream . read ( len ( headers . LAS FILE SIGNATURE ) ) if file sig != headers . LAS FILE SIGNATURE : raise errors . Pylas Error ( \"File Signature ({}) is not {}\" . format ( file sig , headers . LAS FILE SIGNATURE ) )", "predictions": ["raise a file if it is not a wrong signature ."], "references": ["reads the 4 first bytes of the stream to check that is lasf"], "bleu": 0.09497094417933137, "rouge_l": 0.08209959623149395}
{"id": 1940, "code": "def read header ( self ) : self . stream . seek ( self . start pos ) return headers . Header Factory ( ) . read from stream ( self . stream )", "predictions": ["read a header from the resource ."], "references": ["reads the head of the las file and returns it"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 1941, "code": "def read vlrs ( self ) : self . stream . seek ( self . start pos + self . header . size ) return VLR List . read from ( self . stream , num to read = self . header . number of vlr )", "predictions": ["reads a vlrs dataset ."], "references": ["reads and return the vlrs of the file"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1942, "code": "def read compressed points data ( self , laszip vlr , point format ) : offset to chunk table = struct . unpack ( \"<q\" , self . stream . read ( 8 ) ) [ 0 ] size of point data = offset to chunk table - self . stream . tell ( ) if offset to chunk table <= 0 : logger . warning ( \"Strange offset to chunk table: {}, ignoring it..\" . format ( offset to chunk table ) ) size of point data = - 1 points = record . Packed Point Record . from compressed buffer ( self . stream . read ( size of point data ) , point format , self . header . point count , laszip vlr , ) return points", "predictions": ["reads the compressed points of this set of whitespaces ."], "references": ["reads the compressed point record"], "bleu": 0.23462350320528, "rouge_l": 0.42558139534883715}
{"id": 1943, "code": "def read internal waveform packet ( self ) : b = bytearray ( self . stream . read ( rawvlr . VLR HEADER SIZE ) ) waveform header = rawvlr . Raw VLR Header . from buffer ( b ) waveform record = self . stream . read ( ) logger . debug ( \"Read: {} M Bytes of waveform record\" . format ( len ( waveform record ) / 10 ** 6 ) ) return waveform header , waveform record", "predictions": ["read and return the waveform packet ."], "references": ["reads and returns the waveform vlr header waveform record"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 1944, "code": "def warn if not at expected pos ( self , expected pos , end of , start of ) : diff = expected pos - self . stream . tell ( ) if diff != 0 : logger . warning ( \"There are {} bytes between {} and {}\" . format ( diff , end of , start of ) )", "predictions": ["reports that the call at the given position of each new call ."], "references": ["helper function to warn about unknown bytes found in the file"], "bleu": 0.09552040806823771, "rouge_l": 0.08460471567267684}
{"id": 1945, "code": "def date ( self , date ) : self . creation year = date . year self . creation day of year = date . timetuple ( ) . tm yday", "predictions": ["creates a new object representing the specified date ."], "references": ["returns the date of file creation as a python date object"], "bleu": 0.14211011212459496, "rouge_l": 0.19645732689210954}
{"id": 1946, "code": "def mins ( self ) : return np . array ( [ self . x min , self . y min , self . z min ] )", "predictions": ["calculates the mins of this message ."], "references": ["returns de minimum values of x y z as a numpy array"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 1947, "code": "def mins ( self , value ) : self . x min , self . y min , self . z min = value", "predictions": ["this is a variant of ( ."], "references": ["sets de minimum values of x y z as a numpy array"], "bleu": 0.10063351655856649, "rouge_l": 0.10049423393739704}
{"id": 1948, "code": "def maxs ( self ) : return np . array ( [ self . x max , self . y max , self . z max ] )", "predictions": ["a method to get the maxs of this class ."], "references": ["returns de maximum values of x y z as a numpy array"], "bleu": 0.11421946507590645, "rouge_l": 0.08944281524926685}
{"id": 1949, "code": "def maxs ( self , value ) : self . x max , self . y max , self . z max = value", "predictions": ["a method to get the maxs representation of a matrix ."], "references": ["sets de maximum values of x y z as a numpy array"], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 1950, "code": "def scales ( self ) : return np . array ( [ self . x scale , self . y scale , self . z scale ] )", "predictions": ["a method to get the scales of this class ."], "references": ["returns the scaling values of x y z as a numpy array"], "bleu": 0.12273680279953825, "rouge_l": 0.1788856304985337}
{"id": 1951, "code": "def offsets ( self ) : return np . array ( [ self . x offset , self . y offset , self . z offset ] )", "predictions": ["a method to provide this buffer to be refreshed ."], "references": ["returns the offsets values of x y z as a numpy array"], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 1952, "code": "def num extra bytes ( self ) : return sum ( np . dtype ( extra dim [ 1 ] ) . itemsize for extra dim in self . extra dims )", "predictions": ["returns a list of ( possibly non - negative ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ."], "references": ["returns the number of extra bytes"], "bleu": 0.02893344147704888, "rouge_l": 0.08931185944363103}
{"id": 1953, "code": "def has waveform packet ( self ) : dimensions = set ( self . dimension names ) return all ( name in dimensions for name in dims . WAVEFORM FIELDS NAMES )", "predictions": ["a method to determine if there are any dimensions ."], "references": ["returns true if the point format has waveform packet dimensions"], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 1954, "code": "def main ( port , ip , command , loglevel ) : numeric level = getattr ( logging , loglevel . upper ( ) , None ) if not isinstance ( numeric level , int ) : raise Value Error ( 'Invalid log level: %s' % loglevel ) logging . basic Config ( level = numeric level ) click . echo ( \"Demo of satel integra library\" ) if command == \"demo\" : demo ( ip , port )", "predictions": ["creates a new instance of ( to send to the controller ."], "references": ["console script for satel_integra ."], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 1955, "code": "def checksum ( command ) : crc = 0x147A for b in command : crc = ( ( crc << 1 ) & 0x FFFF ) | ( crc & 0x8000 ) >> 15 crc = crc ^ 0x FFFF crc = ( crc + ( crc >> 8 ) + b ) & 0x FFFF return crc", "predictions": ["returns a checksum of all : 1 . 2 . 7 . 3 . 3 . 3 ."], "references": ["function to calculate checksum as per satel manual ."], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 1956, "code": "def print hex ( data ) : hex msg = \"\" for c in data : hex msg += \"\\\\x\" + format ( c , \"02x\" ) LOGGER . debug ( hex msg )", "predictions": ["prints arg of the arg ."], "references": ["debugging method to print out frames in hex ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1957, "code": "def verify and strip ( resp ) : if resp [ 0 : 2 ] != b'\\x FE\\x FE' : LOGGER . error ( \"Houston, we got problem:\" ) print hex ( resp ) raise Exception ( \"Wrong header - got %X%X\" % ( resp [ 0 ] , resp [ 1 ] ) ) if resp [ - 2 : ] != b'\\x FE\\x0D' : raise Exception ( \"Wrong footer - got %X%X\" % ( resp [ - 2 ] , resp [ - 1 ] ) ) output = resp [ 2 : - 2 ] . replace ( b'\\x FE\\x F0' , b'\\x FE' ) c = checksum ( bytearray ( output [ 0 : - 2 ] ) ) if ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) != c : raise Exception ( \"Wrong checksum - got %d expected %d\" % ( ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) , c ) ) return output [ 0 : - 2 ]", "predictions": ["verifies two ranges for the typicalmemoryerror ."], "references": ["verify checksum and strip header and footer of received frame ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 1958, "code": "def generate query ( command ) : data = bytearray ( command ) c = checksum ( data ) data . append ( c >> 8 ) data . append ( c & 0x FF ) data . replace ( b'\\x FE' , b'\\x FE\\x F0' ) data = bytearray . fromhex ( \"FEFE\" ) + data + bytearray . fromhex ( \"FE0D\" ) return data", "predictions": ["to to to query for a query"], "references": ["add header checksum and footer to command data ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1959, "code": "def demo ( host , port ) : loop = asyncio . get event loop ( ) stl = Async Satel ( host , port , loop , [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 25 , 26 , 27 , 28 , 29 , 30 ] , [ 8 , 9 , 10 ] ) loop . run until complete ( stl . connect ( ) ) loop . create task ( stl . arm ( \"3333\" , 1 ) ) loop . create task ( stl . disarm ( \"3333\" ) ) loop . create task ( stl . keep alive ( ) ) loop . create task ( stl . monitor status ( ) ) loop . run forever ( ) loop . close ( )", "predictions": ["create an area that can be resolved by the client ."], "references": ["basic demo of the monitoring capabilities ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 1960, "code": "async def connect ( self ) : LOGGER . debug ( \"Connecting...\" ) try : self . reader , self . writer = await asyncio . open connection ( self . host , self . port , loop = self . loop ) LOGGER . debug ( \"sucess connecting...\" ) except Exception as e : LOGGER . warning ( \"Exception during connecting: %s.\" , e ) self . writer = None self . reader = None return False return True", "predictions": ["method to connect the connection ."], "references": ["make a tcp connection to the alarm system ."], "bleu": 0.16847111051295394, "rouge_l": 0.3860759493670886}
{"id": 1961, "code": "async def start monitoring ( self ) : data = generate query ( b'\\x7F\\x01\\x DC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00' ) await self . send data ( data ) resp = await self . read data ( ) if resp is None : LOGGER . warning ( \"Start monitoring - no data!\" ) return if resp [ 1 : 2 ] != b'\\x FF' : LOGGER . warning ( \"Monitoring not accepted.\" )", "predictions": ["geo method for tiff messages ."], "references": ["start monitoring for interesting events ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1962, "code": "async def disarm ( self , code , partition list ) : LOGGER . info ( \"Sending disarm command.\" ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) data = generate query ( b'\\x84' + code bytes + partition bytes ( partition list ) ) await self . send data ( data )", "predictions": ["generate a full fields in this ("], "references": ["send command to disarm ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1963, "code": "async def clear alarm ( self , code , partition list ) : LOGGER . info ( \"Sending clear the alarm command.\" ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) data = generate query ( b'\\x85' + code bytes + partition bytes ( partition list ) ) await self . send data ( data )", "predictions": ["this method is called when the ( : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 :"], "references": ["send command to clear the alarm ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 1964, "code": "async def set output ( self , code , output id , state ) : LOGGER . debug ( \"Turn on, output: %s, code: %s\" , output id , code ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) mode command = 0x88 if state else 0x89 data = generate query ( mode command . to bytes ( 1 , 'big' ) + code bytes + output bytes ( output id ) ) await self . send data ( data )", "predictions": ["generate a measure of this object ."], "references": ["send output turn on command to the alarm ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1965, "code": "def close ( self ) : LOGGER . debug ( \"Closing...\" ) self . closed = True if self . connected : self . writer . close ( )", "predictions": ["closes the output and causes the underlying shutdown ."], "references": ["stop monitoring and close connection ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1966, "code": "def purge db ( self ) : with self . engine . begin ( ) as db : purge user ( db , self . user id )", "predictions": ["coil header containing all schemas and then the latest header fields ."], "references": ["clear all matching our user_id ."], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 1967, "code": "def get notebook ( self , path , content , format ) : with self . engine . begin ( ) as db : try : record = get file ( db , self . user id , path , content , self . crypto . decrypt , ) except No Such File : self . no such entity ( path ) return self . notebook model from db ( record , content )", "predictions": ["generate and return content of this file ."], "references": ["get a notebook from the database ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1968, "code": "def notebook model from db ( self , record , content ) : path = to api path ( record [ 'parent name' ] + record [ 'name' ] ) model = base model ( path ) model [ 'type' ] = 'notebook' model [ 'last modified' ] = model [ 'created' ] = record [ 'created at' ] if content : content = reads base64 ( record [ 'content' ] ) self . mark trusted cells ( content , path ) model [ 'content' ] = content model [ 'format' ] = 'json' self . validate notebook model ( model ) return model", "predictions": ["create a is . from a is passed in point ."], "references": ["build a notebook model from database record ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 1969, "code": "def get directory ( self , path , content , format ) : with self . engine . begin ( ) as db : try : record = get directory ( db , self . user id , path , content ) except No Such Directory : if self . file exists ( path ) : self . do 400 ( \"Wrong type: %s\" % path ) else : self . no such entity ( path ) return self . directory model from db ( record , content )", "predictions": ["get an ( have been processed , or file if not ."], "references": ["get a directory from the database ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 1970, "code": "def directory model from db ( self , record , content ) : model = base directory model ( to api path ( record [ 'name' ] ) ) if content : model [ 'format' ] = 'json' model [ 'content' ] = list ( chain ( self . convert file records ( record [ 'files' ] ) , ( self . directory model from db ( subdir , False ) for subdir in record [ 'subdirs' ] ) , ) ) return model", "predictions": ["converts this database from a database or folder into a database ."], "references": ["build a directory model from database directory record ."], "bleu": 0.13065113298388567, "rouge_l": 0.2932692307692307}
{"id": 1971, "code": "def file model from db ( self , record , content , format ) : path = to api path ( record [ 'parent name' ] + record [ 'name' ] ) model = base model ( path ) model [ 'type' ] = 'file' model [ 'last modified' ] = model [ 'created' ] = record [ 'created at' ] if content : bcontent = record [ 'content' ] model [ 'content' ] , model [ 'format' ] , model [ 'mimetype' ] = from b64 ( path , bcontent , format , ) return model", "predictions": ["create a raise project or subsection object from this project"], "references": ["build a file model from database record ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 1972, "code": "def save file ( self , db , model , path ) : save file ( db , self . user id , path , to b64 ( model [ 'content' ] , model . get ( 'format' , None ) ) , self . crypto . encrypt , self . max file size bytes , ) return None", "predictions": ["saves this header in the database ."], "references": ["save a non - notebook file ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1973, "code": "def delete file ( self , path ) : if self . file exists ( path ) : self . delete non directory ( path ) elif self . dir exists ( path ) : self . delete directory ( path ) else : self . no such entity ( path )", "predictions": ["deletes the dynamically and all files associated with this sensor ."], "references": ["delete object corresponding to path ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 1974, "code": "def ensure db user ( db , user id ) : with ignore unique violation ( ) : db . execute ( users . insert ( ) . values ( id = user id ) , )", "predictions": ["creates a points for the given points ."], "references": ["add a new user if they don t already exist ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 1975, "code": "def purge user ( db , user id ) : db . execute ( files . delete ( ) . where ( files . c . user id == user id ) ) db . execute ( directories . delete ( ) . where ( directories . c . user id == user id ) ) db . execute ( users . delete ( ) . where ( users . c . id == user id ) )", "predictions": ["this function might be called after a test is executed ."], "references": ["delete a user and all of their resources ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 1976, "code": "def create directory ( db , user id , api path ) : name = from api dirname ( api path ) if name == '/' : parent name = null ( ) parent user id = null ( ) else : parent name = name [ : name . rindex ( '/' , 0 , - 1 ) + 1 ] parent user id = user id db . execute ( directories . insert ( ) . values ( name = name , user id = user id , parent name = parent name , parent user id = parent user id , ) )", "predictions": ["warn the owner of a pos ."], "references": ["create a directory ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 1977, "code": "def delete directory ( db , user id , api path ) : db dirname = from api dirname ( api path ) try : result = db . execute ( directories . delete ( ) . where ( and ( directories . c . user id == user id , directories . c . name == db dirname , ) ) ) except Integrity Error as error : if is foreign key violation ( error ) : raise Directory Not Empty ( api path ) else : raise rowcount = result . rowcount if not rowcount : raise No Such Directory ( api path ) return rowcount", "predictions": ["deletes a : or annotation in the database ."], "references": ["delete a directory ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 1978, "code": "def files in directory ( db , user id , db dirname ) : fields = file default fields ( ) rows = db . execute ( select ( fields , ) . where ( is in directory ( files , user id , db dirname ) , ) . order by ( files . c . user id , files . c . parent name , files . c . name , files . c . created at , ) . distinct ( files . c . user id , files . c . parent name , files . c . name , ) ) return [ to dict no content ( fields , row ) for row in rows ]", "predictions": ["return = 0 , 1 , 2 , 2 = 0 , 3 = 0 , 2 , 3 , 2 = 8"], "references": ["return files in a directory ."], "bleu": 0.05291907393644996, "rouge_l": 0.07711757269279393}
{"id": 1979, "code": "def directories in directory ( db , user id , db dirname ) : fields = directory default fields ( ) rows = db . execute ( select ( fields , ) . where ( is in directory ( directories , user id , db dirname ) , ) ) return [ to dict no content ( fields , row ) for row in rows ]", "predictions": ["determine if the provided : a self - separated : a self - dict is thrown ."], "references": ["return subdirectories of a directory ."], "bleu": 0.07994607499472013, "rouge_l": 0.19032761310452417}
{"id": 1980, "code": "def file where ( user id , api path ) : directory , name = split api filepath ( api path ) return and ( files . c . name == name , files . c . user id == user id , files . c . parent name == directory , )", "predictions": ["constructs a file from an absolute x / y ."], "references": ["return a where clause matching the given api path and user_id ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 1981, "code": "def select file ( user id , api path , fields , limit ) : query = select ( fields ) . where ( file where ( user id , api path ) , ) . order by ( file creation order ( ) , ) if limit is not None : query = query . limit ( limit ) return query", "predictions": ["select an existing ( . . . . y y y y y y y y y y y y y y y y y = 8 y ^ 2 y = 0 y ^ 2 y = 1 y = 1 y = 8 , 2 y = 8"], "references": ["return a select statement that returns the latest n versions of a file ."], "bleu": 0.026594139297659906, "rouge_l": 0.06955530216647662}
{"id": 1982, "code": "def file default fields ( ) : return [ files . c . name , files . c . created at , files . c . parent name , ]", "predictions": ["return an iterator of the array of self - extracted self - jwt files ."], "references": ["default fields returned by a file query ."], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 1983, "code": "def file exists ( db , user id , path ) : try : get file ( db , user id , path , include content = False , decrypt func = unused decrypt func , ) return True except No Such File : return False", "predictions": ["checks to see if a offsets ( return true array array array array array array ( ie . 1 array array array array ( . . . . array array array array array array ( . . array array array array : boolean array array array : boolean array array"], "references": ["check if a file exists ."], "bleu": 0.033984283835209204, "rouge_l": 0.12482946793997271}
{"id": 1984, "code": "def rename file ( db , user id , old api path , new api path ) : if file exists ( db , user id , new api path ) : raise File Exists ( new api path ) old dir , old name = split api filepath ( old api path ) new dir , new name = split api filepath ( new api path ) if old dir != new dir : raise Value Error ( dedent ( . format ( old api path = old api path , new api path = new api path ) ) ) db . execute ( files . update ( ) . where ( file where ( user id , old api path ) , ) . values ( name = new name , created at = func . now ( ) , ) )", "predictions": ["rename a file . the caller must have been replaced by its parents ."], "references": ["rename a file ."], "bleu": 0.24601372576927535, "rouge_l": 0.4939271255060728}
{"id": 1985, "code": "def rename directory ( db , user id , old api path , new api path ) : old db path = from api dirname ( old api path ) new db path = from api dirname ( new api path ) if old db path == '/' : raise Rename Root ( 'Renaming the root directory is not permitted.' ) if dir exists ( db , user id , new db path ) : raise Directory Exists ( new api path ) db . execute ( 'SET CONSTRAINTS ' 'pgcontents.directories parent user id fkey DEFERRED' ) db . execute ( directories . update ( ) . where ( and ( directories . c . user id == user id , directories . c . name == old db path , ) ) . values ( name = new db path , ) ) db . execute ( directories . update ( ) . where ( and ( directories . c . user id == user id , directories . c . name . startswith ( old db path ) , directories . c . parent name . startswith ( old db path ) , ) ) . values ( name = func . concat ( new db path , func . right ( directories . c . name , - func . length ( old db path ) ) ) , parent name = func . concat ( new db path , func . right ( directories . c . parent name , - func . length ( old db path ) ) ) , ) )", "predictions": ["rename a directory . provided as parameter , execute the parent directory ."], "references": ["rename a directory ."], "bleu": 0.26584835766658776, "rouge_l": 0.5202558635394456}
{"id": 1986, "code": "def purge remote checkpoints ( db , user id ) : db . execute ( remote checkpoints . delete ( ) . where ( remote checkpoints . c . user id == user id , ) )", "predictions": ["this function is called after a remote user has been executed ."], "references": ["delete all database records for the given user_id ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 1987, "code": "def reencrypt row content ( db , table , row id , decrypt func , encrypt func , logger ) : q = ( select ( [ table . c . content ] ) . with for update ( ) . where ( table . c . id == row id ) ) [ ( content , ) ] = db . execute ( q ) logger . info ( \"Begin encrypting %s row %s.\" , table . name , row id ) db . execute ( table . update ( ) . where ( table . c . id == row id ) . values ( content = encrypt func ( decrypt func ( content ) ) ) ) logger . info ( \"Done encrypting %s row %s.\" , table . name , row id )", "predictions": ["reencrypt the chunked of the given row of the provided content ."], "references": ["re - encrypt a row from table with id of row_id ."], "bleu": 0.1235622127262679, "rouge_l": 0.25}
{"id": 1988, "code": "def select file ids ( db , user id ) : return list ( db . execute ( select ( [ files . c . id ] ) . where ( files . c . user id == user id ) ) )", "predictions": ["selects the ( from the database and returns it as a file ."], "references": ["get all file ids for a user ."], "bleu": 0.1135935489027116, "rouge_l": 0.19902120717781402}
{"id": 1989, "code": "def select remote checkpoint ids ( db , user id ) : return list ( db . execute ( select ( [ remote checkpoints . c . id ] ) . where ( remote checkpoints . c . user id == user id ) ) )", "predictions": ["fill the most recently used checkpoint in the checkpoint ."], "references": ["get all file ids for a user ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 1990, "code": "def reencrypt user content ( engine , user id , old decrypt func , new encrypt func , logger ) : logger . info ( \"Begin re-encryption for user %s\" , user id ) with engine . begin ( ) as db : logger . info ( \"Re-encrypting files for %s\" , user id ) for ( file id , ) in select file ids ( db , user id ) : reencrypt row content ( db , files , file id , old decrypt func , new encrypt func , logger , ) logger . info ( \"Re-encrypting checkpoints for %s\" , user id ) for ( cp id , ) in select remote checkpoint ids ( db , user id ) : reencrypt row content ( db , remote checkpoints , cp id , old decrypt func , new encrypt func , logger , ) logger . info ( \"Finished re-encryption for user %s\" , user id )", "predictions": ["reencrypt the provided user ."], "references": ["re - encrypt all of the files and checkpoints for a single user ."], "bleu": 0.06382147015463427, "rouge_l": 0.2909379968203497}
{"id": 1991, "code": "def memoize single arg ( f ) : memo = { } @ wraps ( f ) def memoized f ( arg ) : try : return memo [ arg ] except Key Error : result = memo [ arg ] = f ( arg ) return result return memoized f", "predictions": ["the memoize decorator for the function f ."], "references": ["decorator memoizing a single - argument function"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1992, "code": "def delete checkpoint ( self , checkpoint id , path ) : with self . engine . begin ( ) as db : return delete single remote checkpoint ( db , self . user id , path , checkpoint id , )", "predictions": ["deletes a checkpoint from the database ."], "references": ["delete a checkpoint for a file"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 1993, "code": "def get checkpoint content ( self , checkpoint id , path ) : with self . engine . begin ( ) as db : return get remote checkpoint ( db , self . user id , path , checkpoint id , self . crypto . decrypt , ) [ 'content' ]", "predictions": ["generate and return an s3 checkpoint for this object ."], "references": ["get the content of a checkpoint ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 1994, "code": "def list checkpoints ( self , path ) : with self . engine . begin ( ) as db : return list remote checkpoints ( db , self . user id , path )", "predictions": ["get a list of paths to the database ."], "references": ["return a list of checkpoints for a given file"], "bleu": 0.2626909894424158, "rouge_l": 0.3333333333333333}
{"id": 1995, "code": "def rename all checkpoints ( self , old path , new path ) : with self . engine . begin ( ) as db : return move remote checkpoints ( db , self . user id , old path , new path , )", "predictions": ["renames all existing rows of the checkpoints"], "references": ["rename all checkpoints for old_path to new_path ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1996, "code": "def delete all checkpoints ( self , path ) : with self . engine . begin ( ) as db : delete remote checkpoints ( db , self . user id , path )", "predictions": ["a method to delete all files associated with a class and removes all associated database ."], "references": ["delete all checkpoints for the given path ."], "bleu": 0.10878661088699644, "rouge_l": 0.2659883720930233}
{"id": 1997, "code": "def purge db ( self ) : with self . engine . begin ( ) as db : purge remote checkpoints ( db , self . user id )", "predictions": ["purges all messages whose content have been garbage collected ."], "references": ["purge all database records for the current user ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 1998, "code": "def apply prefix ( prefix , model ) : if not isinstance ( model , dict ) : raise Type Error ( \"Expected dict for model, got %s\" % type ( model ) ) model [ 'path' ] = '/' . join ( ( prefix , model [ 'path' ] ) ) . strip ( '/' ) if model [ 'type' ] in ( 'notebook' , 'file' ) : return model if model [ 'type' ] != 'directory' : raise Value Error ( \"Unknown model type %s.\" % type ( model ) ) content = model . get ( 'content' , None ) if content is not None : for sub model in content : apply prefix ( prefix , sub model ) return model", "predictions": ["applies a prefix to the domain"], "references": ["prefix all path entries in model with the given prefix ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 1999, "code": "def path dispatch1 ( mname , returns model ) : def wrapper ( self , * args , * * kwargs ) : path , args = get arg ( 'path' , args , kwargs ) prefix , mgr , mgr path = resolve path ( path , self . managers ) result = getattr ( mgr , mname ) ( mgr path , * args , * * kwargs ) if returns model and prefix : return apply prefix ( prefix , result ) else : return result return wrapper", "predictions": ["returns a new object that is always a resource , but passes the given model ."], "references": ["decorator for methods that accept path as a first argument ."], "bleu": 0.09147827112247602, "rouge_l": 0.22989949748743718}
{"id": 2000, "code": "def path dispatch old new ( mname , returns model ) : def wrapper ( self , old path , new path , * args , * * kwargs ) : old prefix , old mgr , old mgr path = resolve path ( old path , self . managers ) new prefix , new mgr , new mgr path = resolve path ( new path , self . managers , ) if old mgr is not new mgr : raise HTTP Error ( 400 , \"Can't move files between backends ({old} -> {new})\" . format ( old = old path , new = new path , ) ) assert new prefix == old prefix result = getattr ( new mgr , mname ) ( old mgr path , new mgr path , * args , * * kwargs ) if returns model and new prefix : return apply prefix ( new prefix , result ) else : return result return wrapper", "predictions": ["concatenates all given files ."], "references": ["decorator for methods accepting old_path and new_path ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2001, "code": "def managers changed ( self , name , old , new ) : for key in new : if '/' in key : raise Value Error ( \"Expected directory names w/o slashes.  Got [%s]\" % key ) self . managers = { k . strip ( '/' ) : v for k , v in new . items ( ) }", "predictions": ["attempts to clean up the amount of ( ."], "references": ["strip slashes from directories before updating ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2002, "code": "def get ( self , path , content = True , type = None , format = None ) : path = normalize api path ( path ) if path : return self . get ( path , content = content , type = type , format = format ) if not content : return base directory model ( '' ) extra content = self . extra root dirs ( ) rm = self . root manager if rm is None : root model = base directory model ( '' ) root model . update ( format = 'json' , content = extra content , ) else : root model = rm . get ( path , content = content , type = type , format = format , ) root model [ 'content' ] . extend ( extra content ) return root model", "predictions": ["copies data from this object into a local path ."], "references": ["special case handling for listing root dir ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 2003, "code": "def split api filepath ( path ) : parts = path . rsplit ( '/' , 1 ) if len ( parts ) == 1 : name = parts [ 0 ] dirname = '/' else : name = parts [ 1 ] dirname = parts [ 0 ] + '/' return from api dirname ( dirname ) , name", "predictions": ["split this path into an api path . if the path is an api path , then the result is returned . otherwise the result is the name of the first occurrence of the path . otherwise , it may have the name . if the path is the api"], "references": ["split an api file path into directory and name ."], "bleu": 0.048061035107289596, "rouge_l": 0.22732919254658387}
{"id": 2004, "code": "def writes base64 ( nb , version = NBFORMAT VERSION ) : return b64encode ( writes ( nb , version = version ) . encode ( 'utf-8' ) )", "predictions": ["encode all key - value pairs from the digest ."], "references": ["write a notebook as base64 ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2005, "code": "def reads base64 ( nb , as version = NBFORMAT VERSION ) : try : return reads ( b64decode ( nb ) . decode ( 'utf-8' ) , as version = as version ) except Exception as e : raise Corrupted File ( e )", "predictions": ["return a valid index_types assertraises for the current operating system ."], "references": ["read a notebook from base64 ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 2006, "code": "def prefix dirs ( path ) : dirname = posixpath . dirname path = path . strip ( '/' ) out = [ ] while path != '' : path = dirname ( path ) out . append ( path ) return reversed ( out )", "predictions": ["returns all the part of this directory ."], "references": ["return an iterable of all prefix directories of path descending from root ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 2007, "code": "def outside root to 404 ( fn ) : @ wraps ( fn ) def wrapped ( * args , * * kwargs ) : try : return fn ( * args , * * kwargs ) except Path Outside Root as e : raise HTTP Error ( 404 , \"Path outside root: [%s]\" % e . args [ 0 ] ) return wrapped", "predictions": ["decorator that converts outside the root and returns the root if it exists ."], "references": ["decorator for converting pathoutsideroot errors to 404s ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 2008, "code": "def create user ( db url , user ) : Postgres Checkpoints ( db url = db url , user id = user , create user on startup = True , )", "predictions": ["create a user and its persistent user ."], "references": ["create a user ."], "bleu": 0.339132609103075, "rouge_l": 0.7093023255813954}
{"id": 2009, "code": "def walk dirs ( mgr , dirs ) : for directory in dirs : children = mgr . get ( directory , content = True , type = 'directory' , ) [ 'content' ] dirs , files = map ( sorted , separate dirs files ( children ) ) yield directory , dirs , files if dirs : for entry in walk dirs ( mgr , dirs ) : yield entry", "predictions": ["recursively iterates through the tree ."], "references": ["recursive helper for walk ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2010, "code": "def walk files ( mgr ) : for dir , subdirs , files in walk files ( mgr ) : for file in files : yield file", "predictions": ["iterates over all files in this directory . if a file does not exist , it will return an iterator ."], "references": ["iterate over all files visible to mgr ."], "bleu": 0.11433361115787453, "rouge_l": 0.3001230012300123}
{"id": 2011, "code": "def walk files with content ( mgr ) : for , , files in walk ( mgr ) : for f in files : yield mgr . get ( f , content = True )", "predictions": ["generator to generate a stream of files ."], "references": ["iterate over the contents of all files visible to mgr ."], "bleu": 0.13859150907108325, "rouge_l": 0.3070469798657718}
{"id": 2012, "code": "def reencrypt single user ( engine , user id , old crypto , new crypto , logger ) : crypto = Fallback Crypto ( [ new crypto , old crypto ] ) reencrypt user content ( engine = engine , user id = user id , old decrypt func = crypto . decrypt , new encrypt func = crypto . encrypt , logger = logger , )", "predictions": ["computes the chunked contents of the provided xsd object ."], "references": ["re - encrypt all files and checkpoints for a single user ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 2013, "code": "def unencrypt single user ( engine , user id , old crypto , logger ) : reencrypt user content ( engine = engine , user id = user id , old decrypt func = old crypto . decrypt , new encrypt func = lambda s : s , logger = logger , )", "predictions": ["decrypts a switchport object using the provided password and encrypts the decrypted data ."], "references": ["unencrypt all files and checkpoints for a single user ."], "bleu": 0.10511846841633776, "rouge_l": 0.17183098591549298}
{"id": 2014, "code": "def upgrade ( db url , revision ) : with temp alembic ini ( ALEMBIC DIR LOCATION , db url ) as alembic ini : subprocess . check call ( [ 'alembic' , '-c' , alembic ini , 'upgrade' , revision ] )", "predictions": ["upgrade an existing url in the database ."], "references": ["upgrade the given database to revision ."], "bleu": 0.20164945583740668, "rouge_l": 0.5398230088495575}
{"id": 2015, "code": "def queue instance ( self , embed type , data ) : serializer = self . serializers . get ( embed type , None ) if serializer is None : return instance id = serializer . get id ( data ) if embed type not in self . ids : self . ids [ embed type ] = [ ] self . ids [ embed type ] . append ( instance id )", "predictions": ["queue an instance for this bucket ."], "references": ["queue an instance to be fetched from the database ."], "bleu": 0.23813694985189848, "rouge_l": 0.45607476635514016}
{"id": 2016, "code": "def insert instance ( self , block ) : embed type = block . get ( 'type' , None ) data = block . get ( 'data' , { } ) serializer = self . serializers . get ( embed type , None ) if serializer is None : return block try : instance id = serializer . get id ( data ) instance = self . instances [ embed type ] [ instance id ] data [ embed type ] = serializer . serialize ( instance ) except : data [ embed type ] = None block [ 'data' ] = data return block", "predictions": ["inserts a user obj ."], "references": ["insert a fetched instance into embed block ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2017, "code": "def load data ( self ) : for embed type in self . ids . keys ( ) : self . load instances ( embed type , self . ids [ embed type ] )", "predictions": ["rename files to be used for the ui ."], "references": ["load data in bulk for each embed block ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 2018, "code": "def validate ( self , data ) : from dispatch . theme import Theme Manager errors = { } if data . get ( 'widget' ) is not None : try : widget = Theme Manager . Widgets . get ( data [ 'widget' ] ) except Widget Not Found as e : errors [ 'widget' ] = str ( e ) else : for field in widget . fields : field data = data [ 'data' ] . get ( field . name ) if field data is not None : try : field . validate ( field data ) except Invalid Field as e : errors [ field . name ] = str ( e ) elif field . required : errors [ field . name ] = '%s is required' % field . label if errors : raise Validation Error ( errors ) return data", "predictions": ["validates if this widget flag is valid ."], "references": ["perform validation of the widget data"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2019, "code": "def admin ( request ) : context = { 'api url' : settings . API URL , 'app js bundle' : 'manager-%s.js' % dispatch . version , 'app css bundle' : 'manager-%s.css' % dispatch . version } return render to response ( 'manager/index.html' , context )", "predictions": ["execute the reencrypt page using the reencrypt api ."], "references": ["render html entry point for manager app ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2020, "code": "def to json ( self ) : result = { } for field in self . fields : result [ field . name ] = field . to json ( self . data . get ( field . name ) ) return result", "predictions": ["convert this database to a file ."], "references": ["return json representation for this template"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2021, "code": "def exclude fields ( self ) : request = self . context . get ( 'request' ) if request : exclude = request . query params . get ( 'exclude' , None ) if exclude is None : return excluded fields = exclude . split ( ',' ) for field in excluded fields : self . fields . pop ( field )", "predictions": ["select the . select any contained remote remote remote remote remote remote remote remote remote remote remote node ."], "references": ["excludes fields that are included in the queryparameters"], "bleu": 0.06439931429457924, "rouge_l": 0.0799475753604194}
{"id": 2022, "code": "def get ( self , * args , * * kwargs ) : if 'pk' in kwargs : kwargs [ 'parent' ] = kwargs [ 'pk' ] kwargs [ 'head' ] = True del kwargs [ 'pk' ] if 'request' in kwargs : request = kwargs [ 'request' ] version = request . GET . get ( 'version' , None ) preview id = request . GET . get ( 'preview id' , None ) if ( version is not None ) and ( preview id is not None ) : kwargs [ 'revision id' ] = version kwargs [ 'preview id' ] = preview id del kwargs [ 'is published' ] del kwargs [ 'request' ] return super ( Publishable Manager , self ) . get ( * args , * * kwargs )", "predictions": ["reencrypt is a circle of this class ."], "references": ["get the latest article with the given primary key ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 2023, "code": "def get attribute ( self , instance ) : attr = super ( Null Boolean Field , self ) . get attribute ( instance ) return True if attr else False", "predictions": ["determine local object ' s single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single"], "references": ["overrides the default get_attribute method to convert none values to false ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 2024, "code": "def validate widget ( widget ) : if not has valid id ( widget ) : raise Invalid Widget ( \"%s must contain a valid 'id' attribute\" % widget . name ) if not has valid name ( widget ) : raise Invalid Widget ( \"%s must contain a valid 'name' attribute\" % widget . name ) if not has valid template ( widget ) : raise Invalid Widget ( \"%s must contain a valid 'template' attribute\" % widget . name ) if not hasattr ( widget , 'zones' ) or not widget . zones : raise Invalid Widget ( \"%s must be compatible with at least one zone\" % widget . name )", "predictions": ["delete the checkpoint message on a checkpoint checkpoint . this can be used by the main main checkpoint on windows ."], "references": ["checks that the given widget contains the required fields"], "bleu": 0.06429451441231726, "rouge_l": 0.143698468786808}
{"id": 2025, "code": "def validate zone ( zone ) : if not has valid id ( zone ) : raise Invalid Zone ( \"%s must contain a valid 'id' attribute\" % zone . name ) if not has valid name ( zone ) : raise Invalid Zone ( \"%s must contain a valid 'name' attribute\" % zone . name )", "predictions": ["check if the supplied availability checkpoint is a valid openid checkpoint checkpoint ."], "references": ["checks that the given zone contains the required fields"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 2026, "code": "def is valid uuid ( id ) : if not isinstance ( id , basestring ) : return False try : val = UUID ( id , version = 4 ) except Value Error : return False return True", "predictions": ["check if string value is a valid ( key value path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path to [ true ]"], "references": ["return true if id is a valid uuid false otherwise ."], "bleu": 0.04949727050808081, "rouge_l": 0.14823815309842042}
{"id": 2027, "code": "def get permissions ( self ) : permissions = '' if self . groups . filter ( name = 'Admin' ) . exists ( ) or self . is superuser : permissions = 'admin' return permissions", "predictions": ["method to rename all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all"], "references": ["returns the user s permissions ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 2028, "code": "def modify permissions ( self , permissions ) : group = Group . objects . get ( name = 'Admin' ) if permissions == 'admin' : self . groups . add ( group ) else : self . groups . remove ( group )", "predictions": ["public interface to ) method ."], "references": ["modify the user s permissions ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2029, "code": "def Author Validator ( data ) : if not isinstance ( data , list ) : data = [ data ] for author in data : if 'person' not in author : raise Validation Error ( 'An author must contain a person.' ) if 'type' in author and not isinstance ( author [ 'type' ] , basestring ) : raise Validation Error ( 'The author type must be a string.' )", "predictions": ["convert json text to all required values ."], "references": ["raise a validationerror if data does not match the author format ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 2030, "code": "def save ( self , validated data ) : ( zone , created ) = Zone Model . objects . get or create ( zone id = self . id ) zone . widget id = validated data [ 'widget' ] zone . data = validated data [ 'data' ] for key in list ( zone . data . keys ( ) ) : if isinstance ( zone . data [ key ] , dict ) and ( 'id' in zone . data [ key ] . keys ( ) ) and ( 'data' in zone . data [ key ] . keys ( ) ) : zone . data [ key ] [ 'data' ] = self . before save ( zone . data [ key ] [ 'id' ] , zone . data [ key ] [ 'data' ] ) zone . data = self . before save ( zone . widget id , zone . data ) return zone . save ( )", "predictions": ["apply this not to a local chunk ."], "references": ["save widget data for this zone ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 2031, "code": "def get data ( self ) : result = { } for field in self . fields : result [ field . name ] = self . data . get ( field . name ) return result", "predictions": ["get the data for this message ."], "references": ["returns data from each field ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2032, "code": "def prepare data ( self ) : result = { } for field in self . fields : data = self . data . get ( field . name ) result [ field . name ] = field . prepare data ( data ) return result", "predictions": ["path dispatch method for dispatch ."], "references": ["prepare widget data for template ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 2033, "code": "def render ( self , data = None , add context = None ) : template = loader . get template ( self . template ) if not data : data = self . context ( self . prepare data ( ) ) if add context is not None : for key , value in add context . iteritems ( ) : if key in self . accepted keywords : data [ key ] = value return template . render ( data )", "predictions": ["managers managers so we need to managers in other words ."], "references": ["renders the widget as html ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2034, "code": "def callback ( cls , user , query ) : settings = cls . get settings ( show hidden = True ) fb = Facebook ( ) payload = { 'client id' : settings [ 'client id' ] , 'client secret' : settings [ 'client secret' ] , 'code' : query [ 'code' ] , 'redirect uri' : cls . REDIRECT URI } try : fb . get access token ( payload ) pages = fb . list pages ( 'me' ) except Facebook API Error , e : raise Integration Callback Error ( e . message ) return { 'pages' : pages }", "predictions": ["get all pages from a generic content ."], "references": ["receive oauth callback request from facebook ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 2035, "code": "def get settings ( self , integration id ) : try : integration = self . get ( integration id = integration id ) return json . loads ( integration . settings ) except ( self . model . Does Not Exist , Value Error ) : return { }", "predictions": ["split the api operation by : get the api call ."], "references": ["return settings for given integration as a dictionary ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 2036, "code": "def update settings ( self , integration id , settings ) : ( integration , created ) = self . get or create ( integration id = integration id ) try : current settings = json . loads ( integration . settings ) except Value Error : current settings = { } current settings . update ( settings ) integration . settings = json . dumps ( current settings ) integration . save ( )", "predictions": ["method used to writes to a new json instance ."], "references": ["updates settings for given integration ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2037, "code": "def signup ( request , uuid = None ) : invite = get object or 404 ( Invite . objects . all ( ) , id = uuid ) if invite . expiration date < timezone . now ( ) : invite . delete ( ) raise Http404 ( 'This page does not exist.' ) if request . method == 'POST' : form = Sign Up Form ( request . POST ) if form . is valid ( ) : user = form . save ( commit = False ) user . email = invite . email user . person = invite . person user . save ( ) if invite . permissions == 'admin' : group = Group . objects . get ( name = 'Admin' ) user . groups . add ( group ) invite . delete ( ) return redirect ( 'dispatch-admin' ) else : return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) else : form = Sign Up Form ( ) return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } )", "predictions": ["reads a reads an invite . the reads will be removed from the reads context ."], "references": ["handles requests to the user signup page ."], "bleu": 0.08513012360883544, "rouge_l": 0.17732558139534885}
{"id": 2038, "code": "def zone ( zone id , * * kwargs ) : try : zone = Theme Manager . Zones . get ( zone id ) except Zone Not Found : return '' try : return zone . widget . render ( add context = kwargs ) except ( Widget Not Found , Attribute Error ) : pass return ''", "predictions": ["w / nos operation : w / o : otherprefix"], "references": ["renders the contents of the zone with given zone_id ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2039, "code": "def save subsection ( self , subsection id ) : Article . objects . filter ( parent id = self . parent . id ) . update ( subsection id = subsection id )", "predictions": ["store the root object in the database ."], "references": ["save the subsection to the parent article"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 2040, "code": "def get extension ( self ) : ext = os . path . splitext ( self . img . name ) [ 1 ] if ext : return ext [ 1 : ] return ext", "predictions": ["create user user for the given = font name ."], "references": ["returns the file extension ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 2041, "code": "def get medium url ( self ) : if self . is gif ( ) : return self . get absolute url ( ) return '%s%s-%s.jpg' % ( settings . MEDIA URL , self . get name ( ) , 'medium' )", "predictions": ["get the ( resource ) for this url ."], "references": ["returns the medium size image url ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 2042, "code": "def save ( self , * * kwargs ) : is new = self . pk is None if is new : self . img . name = self . img . name . lower ( ) super ( Image , self ) . save ( * * kwargs ) if is new and self . img : data = self . img . read ( ) if not data : return image = Img . open ( String IO . String IO ( data ) ) self . width , self . height = image . size super ( Image , self ) . save ( ) name = self . get name ( ) ext = self . get extension ( ) for size in self . SIZES . keys ( ) : self . save thumbnail ( image , self . SIZES [ size ] , name , size , ext )", "predictions": ["saves the background image to the this object ."], "references": ["custom save method to process thumbnails and save image dimensions ."], "bleu": 0.1343994460963362, "rouge_l": 0.19645732689210954}
{"id": 2043, "code": "def save thumbnail ( self , image , size , name , label , file type ) : width , height = size ( imw , imh ) = image . size if ( imw > width ) or ( imh > height ) : image . thumbnail ( size , Img . ANTIALIAS ) name = \"%s-%s.jpg\" % ( name , label ) if file type in self . JPG FORMATS : file type = 'JPEG' image io = String IO . String IO ( ) image . save ( image io , format = file type , quality = 75 ) thumb file = In Memory Uploaded File ( image io , None , name , 'image/jpeg' , image io . len , None ) default storage . save ( name , thumb file )", "predictions": ["saves the files in the imported directory ."], "references": ["processes and saves a resized thumbnail version of the image ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 2044, "code": "def decrement ( self ) : with self . lock : if self . count == 0 : raise Runtime Error ( 'Counter is at zero. It cannot dip below zero' ) self . count -= 1 if self . is finalized and self . count == 0 : self . callback ( )", "predictions": ["decrements the number of transitions on this lock ."], "references": ["decrement the count by one"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2045, "code": "def set exception ( self , exception ) : if not self . done ( ) : raise Transfer Not Done Error ( 'set exception can only be called once the transfer is ' 'complete.' ) self . coordinator . set exception ( exception , override = True )", "predictions": ["set single method to new exceptions ."], "references": ["sets the exception on the future ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2046, "code": "def add done callback ( self , function , * args , * * kwargs ) : with self . done callbacks lock : self . done callbacks . append ( Function Container ( function , * args , * * kwargs ) )", "predictions": ["adds a ) method to the request ."], "references": ["add a done callback to be invoked when transfer is done"], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 2047, "code": "def add failure cleanup ( self , function , * args , * * kwargs ) : with self . failure cleanups lock : self . failure cleanups . append ( Function Container ( function , * args , * * kwargs ) )", "predictions": ["adds a instance of this type ."], "references": ["adds a callback to call upon failure"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 2048, "code": "def iter step func decorators ( self ) : func defs = [ func for func in self . py tree . iter funcdefs ( ) ] + [ func for cls in self . py tree . iter classdefs ( ) for func in cls . iter funcdefs ( ) ] for func in func defs : for decorator in func . get decorators ( ) : if decorator . children [ 1 ] . value == 'step' : yield func , decorator break", "predictions": ["iterate over all exceptions of the same class and returns them as the given tree ."], "references": ["find functions with step decorator in parsed file"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2049, "code": "def iter steps ( self ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) if step : span = self . span from pos ( decorator . start pos , func . end pos ) yield step , func . name . value , span", "predictions": ["generator to receive notifications of the same command ."], "references": ["iterate over steps in the parsed file ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2050, "code": "def find step node ( self , step text ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) arg node = decorator . children [ 3 ] if step == step text : return arg node , func elif isinstance ( step , list ) and step text in step : idx = step . index ( step text ) step node = arg node . children [ 1 ] . children [ idx * 2 ] return step node , func return None , None", "predictions": ["find a step in a step ."], "references": ["find the ast node which contains the text ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 2051, "code": "def iter step func decorators ( self ) : for node in self . py tree . find all ( 'def' ) : for decorator in node . decorators : if decorator . name . value == 'step' : yield node , decorator break", "predictions": ["this method yields a decorator for each decorator ."], "references": ["find functions with step decorator in parsed file ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 2052, "code": "def step decorator args ( self , decorator ) : args = decorator . call . value step = None if len ( args ) == 1 : try : step = args [ 0 ] . value . to python ( ) except ( Value Error , Syntax Error ) : pass if isinstance ( step , six . string types + ( list , ) ) : return step logging . error ( , self . file path ) else : logging . error ( \"Decorator step accepts only one argument - %s\" , self . file path )", "predictions": ["decorator for the step execution time ."], "references": ["get arguments passed to step decorators converted to python objects ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2053, "code": "def iter steps ( self ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) if step : yield step , func . name , self . span for node ( func , True )", "predictions": ["a decorator for . ."], "references": ["iterate over steps in the parsed file ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2054, "code": "def find step node ( self , step text ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) arg node = decorator . call . value [ 0 ] . value if step == step text : return arg node , func elif isinstance ( step , list ) and step text in step : step node = arg node [ step . index ( step text ) ] return step node , func return None , None", "predictions": ["a decorator for . on the step ."], "references": ["find the ast node which contains the text ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 2055, "code": "def POST ( self ) : json data = web . data ( ) print ( \"\\n WEBHOOK POST RECEIVED:\" ) print ( json data , \"\\n\" ) webhook obj = Webhook ( json data ) room = api . rooms . get ( webhook obj . data . room Id ) message = api . messages . get ( webhook obj . data . id ) person = api . people . get ( message . person Id ) print ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) print ( \"FROM '{}'\" . format ( person . display Name ) ) print ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) me = api . people . me ( ) if message . person Id == me . id : return 'OK' else : if \"/CAT\" in message . text : print ( \"FOUND '/CAT'\" ) cat fact = get catfact ( ) print ( \"SENDING CAT FACT '{}'\" . format ( cat fact ) ) api . messages . create ( room . id , text = cat fact ) return 'OK'", "predictions": ["gets a person for the web api ."], "references": ["respond to inbound webhook json http posts from webex teams ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 2056, "code": "def validate base url ( base url ) : parsed url = urllib . parse . urlparse ( base url ) if parsed url . scheme and parsed url . netloc : return parsed url . geturl ( ) else : error message = \"base url must contain a valid scheme (protocol \" \"specifier) and network location (hostname)\" raise Value Error ( error message )", "predictions": ["validate if the url is correct ."], "references": ["verify that base_url specifies a protocol and network location ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2057, "code": "def is web url ( string ) : assert isinstance ( string , basestring ) parsed url = urllib . parse . urlparse ( string ) return ( ( parsed url . scheme . lower ( ) == 'http' or parsed url . scheme . lower ( ) == 'https' ) and parsed url . netloc )", "predictions": ["validates if string argument is a web url ."], "references": ["check to see if string is an validly - formatted web url ."], "bleu": 0.2081707001550676, "rouge_l": 0.5281385281385281}
{"id": 2058, "code": "def open local file ( file path ) : assert isinstance ( file path , basestring ) assert is local file ( file path ) file name = os . path . basename ( file path ) file object = open ( file path , 'rb' ) content type = mimetypes . guess type ( file name ) [ 0 ] or 'text/plain' return Encodable File ( file name = file name , file object = file object , content type = content type )", "predictions": ["opens a file for the file ."], "references": ["open the file and return an encodablefile tuple ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 2059, "code": "def strptime ( cls , date string , format = WEBEX TEAMS DATETIME FORMAT ) : return super ( Webex Teams Date Time , cls ) . strptime ( date string , format ) . replace ( tzinfo = Zulu Time Zone ( ) )", "predictions": ["creates a time series from the given date ."], "references": ["strptime with the webex teams datetime format as the default ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 2060, "code": "def created ( self ) : created = self . json data . get ( 'created' ) if created : return Webex Teams Date Time . strptime ( created ) else : return None", "predictions": ["builds a json object representing the webex lock ."], "references": ["creation date and time in iso8601 format ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2061, "code": "def wait on rate limit ( self , value ) : check type ( value , bool , may be none = False ) self . wait on rate limit = value", "predictions": ["like rate but blocks until the image should run ."], "references": ["enable or disable automatic rate - limit handling ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 2062, "code": "def serialize ( cls , data ) : if hasattr ( data , \" hash \" ) and callable ( data . hash ) : return data elif isinstance ( data , list ) : return tuple ( ( cls . serialize ( item ) for item in data ) ) elif isinstance ( data , dict ) : key value tuples = [ ( key , cls . serialize ( value ) ) for key , value in data . items ( ) ] key value tuples . sort ( ) return tuple ( key value tuples ) else : raise Type Error ( \"Unable to freeze {} data type.\" . format ( type ( data ) ) )", "predictions": ["since this method is called multiple times , we need to get the data from a file ."], "references": ["serialize data to an frozen tuple ."], "bleu": 0.08097785064266204, "rouge_l": 0.17378917378917377}
{"id": 2063, "code": "def last Activity ( self ) : last activity = self . json data . get ( 'last Activity' ) if last activity : return Webex Teams Date Time . strptime ( last activity ) else : return None", "predictions": ["get local activity for this session ."], "references": ["the date and time of the person s last activity ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2064, "code": "def post events service ( request ) : json data = request . json log . info ( \"\\n\" ) log . info ( \"WEBHOOK POST RECEIVED:\" ) log . info ( json data ) log . info ( \"\\n\" ) webhook obj = Webhook ( json data ) room = api . rooms . get ( webhook obj . data . room Id ) message = api . messages . get ( webhook obj . data . id ) person = api . people . get ( message . person Id ) log . info ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) log . info ( \"FROM '{}'\" . format ( person . display Name ) ) log . info ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) me = api . people . me ( ) if message . person Id == me . id : return { 'Message' : 'OK' } else : if \"/CAT\" in message . text : log . info ( \"FOUND '/CAT'\" ) catfact = get catfact ( ) log . info ( \"SENDING CAT FACT'{}'\" . format ( catfact ) ) api . messages . create ( room . id , text = catfact ) return { 'Message' : 'OK' }", "predictions": ["this is called from the client to send a room to the server . this can only be called once for the execution of the room is requested ."], "references": ["respond to inbound webhook json http post from webex teams ."], "bleu": 0.04965977366141172, "rouge_l": 0.10883140053523638}
{"id": 2065, "code": "def get ngrok public url ( ) : try : response = requests . get ( url = NGROK CLIENT API BASE URL + \"/tunnels\" , headers = { 'content-type' : 'application/json' } ) response . raise for status ( ) except requests . exceptions . Request Exception : print ( \"Could not connect to the ngrok client API; \" \"assuming not running.\" ) return None else : for tunnel in response . json ( ) [ \"tunnels\" ] : if tunnel . get ( \"public url\" , \"\" ) . startswith ( \"http://\" ) : print ( \"Found ngrok public HTTP URL:\" , tunnel [ \"public url\" ] ) return tunnel [ \"public url\" ]", "predictions": ["combine the specified url to one or more tunnel ."], "references": ["get the ngrok public http url from the local client api ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 2066, "code": "def delete webhooks with name ( api , name ) : for webhook in api . webhooks . list ( ) : if webhook . name == name : print ( \"Deleting Webhook:\" , webhook . name , webhook . target Url ) api . webhooks . delete ( webhook . id )", "predictions": ["deletes an existing entry from the database ."], "references": ["find a webhook by name ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2067, "code": "def create ngrok webhook ( api , ngrok public url ) : print ( \"Creating Webhook...\" ) webhook = api . webhooks . create ( name = WEBHOOK NAME , target Url = urljoin ( ngrok public url , WEBHOOK URL SUFFIX ) , resource = WEBHOOK RESOURCE , event = WEBHOOK EVENT , ) print ( webhook ) print ( \"Webhook successfully created.\" ) return webhook", "predictions": ["create a new ( ."], "references": ["create a webex teams webhook pointing to the public ngrok url ."], "bleu": 0.09521044541645862, "rouge_l": 0.3285457809694794}
{"id": 2068, "code": "def main ( ) : api = Webex Teams API ( ) delete webhooks with name ( api , name = WEBHOOK NAME ) public url = get ngrok public url ( ) if public url is not None : create ngrok webhook ( api , public url )", "predictions": ["creates a new ( ."], "references": ["delete previous webhooks . if local ngrok tunnel create a webhook ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 2069, "code": "def console ( ) : parser = argparse . Argument Parser ( description = console . doc ) parser . add argument ( '--device' , default = '/dev/tty USB0' , help = 'port to read DSMR data from' ) parser . add argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) parser . add argument ( '--port' , default = None , help = 'TCP port to use for connection' ) parser . add argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) parser . add argument ( '--verbose' , '-v' , action = 'count' ) args = parser . parse args ( ) if args . verbose : level = logging . DEBUG else : level = logging . ERROR logging . basic Config ( level = level ) loop = asyncio . get event loop ( ) def print callback ( telegram ) : \"\"\"Callback that prints telegram values.\"\"\" for obiref , obj in telegram . items ( ) : if obj : print ( obj . value , obj . unit ) print ( ) if args . host and args . port : create connection = partial ( create tcp dsmr reader , args . host , args . port , args . version , print callback , loop = loop ) else : create connection = partial ( create dsmr reader , args . device , args . version , print callback , loop = loop ) try : while True : conn = create connection ( ) transport , protocol = loop . run until complete ( conn ) loop . run until complete ( protocol . wait closed ( ) ) loop . run until complete ( asyncio . sleep ( 5 ) ) except Keyboard Interrupt : transport . close ( ) loop . run until complete ( asyncio . sleep ( 0 ) ) finally : loop . close ( )", "predictions": ["create and initialize the console ."], "references": ["output dsmr data to console ."], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 2070, "code": "def create dsmr protocol ( dsmr version , telegram callback , loop = None ) : if dsmr version == '2.2' : specification = telegram specifications . V2 2 serial settings = SERIAL SETTINGS V2 2 elif dsmr version == '4' : specification = telegram specifications . V4 serial settings = SERIAL SETTINGS V4 elif dsmr version == '5' : specification = telegram specifications . V5 serial settings = SERIAL SETTINGS V5 else : raise Not Implemented Error ( \"No telegram parser found for version: %s\" , dsmr version ) protocol = partial ( DSMR Protocol , loop , Telegram Parser ( specification ) , telegram callback = telegram callback ) return protocol , serial settings", "predictions": ["create a protocol for the given protocol ."], "references": ["creates a dsmr asyncio protocol ."], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 2071, "code": "def create dsmr reader ( port , dsmr version , telegram callback , loop = None ) : protocol , serial settings = create dsmr protocol ( dsmr version , telegram callback , loop = None ) serial settings [ 'url' ] = port conn = create serial connection ( loop , protocol , * * serial settings ) return conn", "predictions": ["creates a brocade reader ."], "references": ["creates a dsmr asyncio protocol coroutine using serial port ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 2072, "code": "def create tcp dsmr reader ( host , port , dsmr version , telegram callback , loop = None ) : protocol , = create dsmr protocol ( dsmr version , telegram callback , loop = None ) conn = loop . create connection ( protocol , host , port ) return conn", "predictions": ["create a new tcp reader ."], "references": ["creates a dsmr asyncio protocol coroutine using tcp connection ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 2073, "code": "def data received ( self , data ) : data = data . decode ( 'ascii' ) self . log . debug ( 'received data: %s' , data ) self . telegram buffer . append ( data ) for telegram in self . telegram buffer . get all ( ) : self . handle telegram ( telegram )", "predictions": ["( ( ) call back to ( ( ) ."], "references": ["add incoming data to buffer ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2074, "code": "def connection lost ( self , exc ) : if exc : self . log . exception ( 'disconnected due to exception' ) else : self . log . info ( 'disconnected because of close/abort.' ) self . closed . set ( )", "predictions": ["create a new ( ."], "references": ["stop when connection is lost ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2075, "code": "def handle telegram ( self , telegram ) : self . log . debug ( 'got telegram: %s' , telegram ) try : parsed telegram = self . telegram parser . parse ( telegram ) except Invalid Checksum Error as e : self . log . warning ( str ( e ) ) except Parse Error : self . log . exception ( \"failed to parse telegram\" ) else : self . telegram callback ( parsed telegram )", "predictions": ["customers this method is called from within the controller ."], "references": ["send off parsed telegram to handling callback ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 2076, "code": "def ensure python ( specs ) : if not isinstance ( specs , ( list , tuple ) ) : specs = [ specs ] v = sys . version info part = '%s.%s' % ( v . major , v . minor ) for spec in specs : if part == spec : return try : if eval ( part + spec ) : return except Syntax Error : pass raise Value Error ( 'Python version %s unsupported' % part )", "predictions": ["ensures that a string or list of python specs are valid ."], "references": ["given a list of range specifiers for python ensure compatibility ."], "bleu": 0.16261701715194898, "rouge_l": 0.43821839080459773}
{"id": 2077, "code": "def find packages ( top = HERE ) : packages = [ ] for d , dirs , in os . walk ( top , followlinks = True ) : if os . path . exists ( pjoin ( d , ' init .py' ) ) : packages . append ( os . path . relpath ( d , top ) . replace ( os . path . sep , '.' ) ) elif d != top : dirs [ : ] = [ ] return packages", "predictions": ["returns the path to the tree ."], "references": ["find all of the packages ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2078, "code": "def command for func ( func ) : class Func Command ( Base Command ) : def run ( self ) : func ( ) update package data ( self . distribution ) return Func Command", "predictions": ["get a list of functions for the response ."], "references": ["create a command that calls the given function ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 2079, "code": "def run ( cmd , * * kwargs ) : log . info ( '> ' + list2cmdline ( cmd ) ) kwargs . setdefault ( 'cwd' , HERE ) kwargs . setdefault ( 'shell' , os . name == 'nt' ) if not isinstance ( cmd , ( list , tuple ) ) and os . name != 'nt' : cmd = shlex . split ( cmd ) cmd [ 0 ] = which ( cmd [ 0 ] ) return subprocess . check call ( cmd , * * kwargs )", "predictions": ["runs the given command ."], "references": ["echo a command before running it . defaults to repo as cwd"], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 2080, "code": "def get file handler ( package data spec , data files spec ) : class File Handler ( Base Command ) : def run ( self ) : package data = self . distribution . package data package spec = package data spec or dict ( ) for ( key , patterns ) in package spec . items ( ) : package data [ key ] = get package data ( key , patterns ) self . distribution . data files = get data files ( data files spec , self . distribution . data files ) return File Handler", "predictions": ["returns a list of all the information of this step ."], "references": ["get a package_data and data_files handler command ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 2081, "code": "def compile pattern ( pat , ignore case = True ) : if isinstance ( pat , bytes ) : pat str = pat . decode ( 'ISO-8859-1' ) res str = translate glob ( pat str ) res = res str . encode ( 'ISO-8859-1' ) else : res = translate glob ( pat ) flags = re . IGNORECASE if ignore case else 0 return re . compile ( res , flags = flags ) . match", "predictions": ["returns a boolean value ."], "references": ["translate and compile a glob pattern to a regular expression matcher ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 2082, "code": "def translate glob ( pat ) : translated parts = [ ] for part in iexplode path ( pat ) : translated parts . append ( translate glob part ( part ) ) os sep class = '[%s]' % re . escape ( SEPARATORS ) res = join translated ( translated parts , os sep class ) return '{res}\\\\Z(?ms)' . format ( res = res )", "predictions": ["find regular expression matching the rules ."], "references": ["translate a glob pattern to a regular expression ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 2083, "code": "def translate glob part ( pat ) : if pat == '**' : return '.*' i , n = 0 , len ( pat ) res = [ ] while i < n : c = pat [ i ] i = i + 1 if c == '*' : res . append ( '[^%s]*' % SEPARATORS ) elif c == '?' : res . append ( '[^%s]?' % SEPARATORS ) elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res . append ( '\\\\[' ) else : stuff = pat [ i : j ] . replace ( '\\\\' , '\\\\\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\\\' + stuff res . append ( '[%s]' % stuff ) else : res . append ( re . escape ( c ) ) return '' . join ( res )", "predictions": ["iter command line . use invalidates pattern to escape ."], "references": ["translate a glob pattern part to a regular expression ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 2084, "code": "def qsize ( self , extra predicate = None ) : count = self . query queued ( 'COUNT(*) AS count' , extra predicate = extra predicate ) return count [ 0 ] . count", "predictions": ["returns a new call to this element ."], "references": ["return an approximate number of queued tasks in the queue ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 2085, "code": "def enqueue ( self , data ) : jsonified data = json . dumps ( data ) with self . db conn ( ) as conn : return conn . execute ( 'INSERT INTO %s (created, data) VALUES (%%(created)s, %%(data)s)' % self . table name , created = datetime . utcnow ( ) , data = jsonified data )", "predictions": ["iter operation for table ."], "references": ["enqueue task with specified data ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2086, "code": "def build extra predicate ( self , extra predicate ) : if extra predicate is None : return '' if not isinstance ( extra predicate [ 1 ] , ( list , dict , tuple ) ) : extra predicate = [ extra predicate [ 0 ] , ( extra predicate [ 1 ] , ) ] extra predicate = database . escape query ( * extra predicate ) return 'AND (' + extra predicate + ')'", "predictions": ["this function iterates through the items of this configuration and returns the original ] ."], "references": ["this method is a good one to extend if you want to create a queue which always applies an extra predicate ."], "bleu": 0.05708722046412996, "rouge_l": 0.10454155955441304}
{"id": 2087, "code": "def simplejson datetime serializer ( obj ) : if hasattr ( obj , 'isoformat' ) : return obj . isoformat ( ) else : raise Type Error ( 'Object of type %s with value of %s is not JSON serializable' % ( type ( obj ) , repr ( obj ) ) )", "predictions": ["returns a simplejson containing the specified object ."], "references": ["designed to be passed as the default kwarg in simplejson . dumps . serializes dates and datetimes to iso strings ."], "bleu": 0.03755268705124013, "rouge_l": 0.12761506276150628}
{"id": 2088, "code": "def reconnect ( self ) : conn = mysql . connect ( * * self . db args ) if conn is not None : self . close ( ) self . db = conn", "predictions": ["validate the database connection ."], "references": ["closes the existing database connection and re - opens it ."], "bleu": 0.12296202211076697, "rouge_l": 0.4683301343570058}
{"id": 2089, "code": "def get ( self , query , * parameters , * * kwparameters ) : rows = self . query ( query , parameters , kwparameters ) if not rows : return None elif not isinstance ( rows , list ) : raise My SQL Error ( \"Query is not a select query\" ) elif len ( rows ) > 1 : raise My SQL Error ( \"Multiple rows returned for Database.get() query\" ) else : return rows [ 0 ]", "predictions": ["method for constructing data using the provided string ."], "references": ["returns the first row returned for the given query ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 2090, "code": "def execute ( self , query , * parameters , * * kwparameters ) : return self . execute lastrowid ( query , * parameters , * * kwparameters )", "predictions": ["method executed before the operator returns ."], "references": ["executes the given query returning the lastrowid from the query ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2091, "code": "def execute lastrowid ( self , query , * parameters , * * kwparameters ) : self . execute ( query , parameters , kwparameters ) self . result = self . db . store result ( ) return self . db . insert id ( )", "predictions": ["methods for the database ."], "references": ["executes the given query returning the lastrowid from the query ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 2092, "code": "def get connection ( db = DATABASE ) : return database . connect ( host = HOST , port = PORT , user = USER , password = PASSWORD , database = db )", "predictions": ["create the ( ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["returns a new connection to the database ."], "bleu": 0.026594139297659906, "rouge_l": 0.07932379713914176}
{"id": 2093, "code": "def run benchmark ( ) : stopping = threading . Event ( ) workers = [ Insert Worker ( stopping ) for in range ( NUM WORKERS ) ] print ( 'Launching %d workers' % NUM WORKERS ) [ worker . start ( ) for worker in workers ] time . sleep ( WORKLOAD TIME ) print ( 'Stopping workload' ) stopping . set ( ) [ worker . join ( ) for worker in workers ] with get connection ( ) as conn : count = conn . get ( \"SELECT COUNT(*) AS count FROM %s\" % TABLE ) . count print ( \"%d rows inserted using %d workers\" % ( count , NUM WORKERS ) ) print ( \"%.1f rows per second\" % ( count / float ( WORKLOAD TIME ) ) )", "predictions": ["this method runs the execution of all check ."], "references": ["run a set of insertworkers and record their performance ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 2094, "code": "def connect ( self ) : with self . lock : if self . aggregator : try : return self . pool connect ( self . aggregator ) except Pool Connection Exception : self . aggregator = None if not len ( self . aggregators ) : with self . pool connect ( self . primary aggregator ) as conn : self . update aggregator list ( conn ) conn . expire ( ) random . shuffle ( self . aggregators ) last exception = None for aggregator in self . aggregators : self . logger . debug ( 'Attempting connection with %s:%s' % ( aggregator [ 0 ] , aggregator [ 1 ] ) ) try : conn = self . pool connect ( aggregator ) self . aggregator = aggregator return conn except Pool Connection Exception as e : last exception = e else : self . aggregator = None self . aggregators = [ ] raise last exception", "predictions": ["serialize from a pool ."], "references": ["returns an aggregator connection ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 2095, "code": "def lookup by number ( errno ) : for key , val in globals ( ) . items ( ) : if errno == val : print ( key )", "predictions": ["last time value for the minion ."], "references": ["used for development only"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2096, "code": "def size ( self ) : return sum ( q . qsize ( ) for q in self . connections . values ( ) ) + len ( self . fairies )", "predictions": ["answers the post queue ."], "references": ["returns the number of connections cached by the pool ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 2097, "code": "def ping ( self ) : with self . db conn ( ) as conn : affected rows = conn . query ( % self . manager . table name , datetime . utcnow ( ) , self . lock id , self . lock hash ) return bool ( affected rows == 1 )", "predictions": ["get the get request ."], "references": ["notify the manager that this lock is still active ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 2098, "code": "def release ( self ) : if self . valid ( ) : with self . db conn ( ) as conn : affected rows = conn . query ( % self . manager . table name , self . lock id , self . lock hash ) return bool ( affected rows == 1 ) else : return False", "predictions": ["delete a == table ."], "references": ["release the lock ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2099, "code": "def connect ( self , host = '127.0.0.1' , port = 3306 , user = 'root' , password = '' , database = None ) : if database is None : raise exceptions . Requires Database ( ) self . db args = { 'host' : host , 'port' : port , 'user' : user , 'password' : password , 'database' : database } with self . db conn ( ) as conn : conn . query ( 'SELECT 1' ) return self", "predictions": ["create a connection to the underlying endpoint ."], "references": ["connect to the database specified"], "bleu": 0.21105340631872638, "rouge_l": 0.32105263157894737}
{"id": 2100, "code": "def setup ( self ) : with self . db conn ( ) as conn : for table defn in self . tables . values ( ) : conn . execute ( table defn ) return self", "predictions": ["main method for the ( ."], "references": ["initialize the required tables in the database"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2101, "code": "def destroy ( self ) : with self . db conn ( ) as conn : for table name in self . tables : conn . execute ( 'DROP TABLE IF EXISTS %s' % table name ) return self", "predictions": ["destroys database objects by name ."], "references": ["destroy the sqlstepqueue tables in the database"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2102, "code": "def ready ( self ) : with self . db conn ( ) as conn : tables = [ row . t for row in conn . query ( , self . db args [ 'database' ] ) ] return all ( [ table name in tables for table name in self . tables ] )", "predictions": ["a table method for finding all table round-trips ."], "references": ["returns true if the tables have been setup false otherwise"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 2103, "code": "def valid ( self ) : if self . finished is not None : return False with self . db conn ( ) as conn : row = conn . get ( % self . queue . table name , now = datetime . utcnow ( ) , ttl = self . queue . execution ttl , task id = self . task id , execution id = self . execution id ) return bool ( row is not None and row . valid )", "predictions": ["test create a create datetime ."], "references": ["check to see if we are still active ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2104, "code": "def ping ( self ) : if self . finished is not None : raise Already Finished ( ) with self . db conn ( ) as conn : success = conn . query ( % self . queue . table name , now = datetime . utcnow ( ) , task id = self . task id , execution id = self . execution id , ttl = self . queue . execution ttl ) if success != 1 : raise Task Does Not Exist ( )", "predictions": ["create a create and close stats ."], "references": ["notify the queue that this task is still active ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2105, "code": "def start step ( self , step name ) : if self . finished is not None : raise Already Finished ( ) step data = self . get step ( step name ) if step data is not None : if 'stop' in step data : raise Step Already Finished ( ) else : raise Step Already Started ( ) steps = copy . deepcopy ( self . steps ) steps . append ( { \"start\" : datetime . utcnow ( ) , \"name\" : step name } ) self . save ( steps = steps )", "predictions": ["starts a received received datetime ."], "references": ["start a step ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 2106, "code": "def stop step ( self , step name ) : if self . finished is not None : raise Already Finished ( ) steps = copy . deepcopy ( self . steps ) step data = self . get step ( step name , steps = steps ) if step data is None : raise Step Not Started ( ) elif 'stop' in step data : raise Step Already Finished ( ) step data [ 'stop' ] = datetime . utcnow ( ) step data [ 'duration' ] = util . timedelta total seconds ( step data [ 'stop' ] - step data [ 'start' ] ) self . save ( steps = steps )", "predictions": ["stops a lost lost ."], "references": ["stop a step ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 2107, "code": "def create ( self , device Type ) : r = self . api Client . post ( \"api/v0002/device/types\" , device Type ) if r . status code == 201 : return Device Type ( api Client = self . api Client , * * r . json ( ) ) else : raise Api Exception ( r )", "predictions": ["override this method to construct a new , and return it in the local machine ."], "references": ["register one or more new device types each request can contain a maximum of 512kb ."], "bleu": 0.09147827112247602, "rouge_l": 0.125}
{"id": 2108, "code": "def update ( self , device Uid , metadata = None , device Info = None , status = None ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) device Url = \"api/v0002/device/types/%s/devices/%s\" % ( device Uid . type Id , device Uid . device Id ) data = { \"status\" : status , \"device Info\" : device Info , \"metadata\" : metadata } r = self . api Client . put ( device Url , data ) if r . status code == 200 : return Device ( api Client = self . api Client , * * r . json ( ) ) else : raise Api Exception ( r )", "predictions": ["ensure the ( and ) are visible ."], "references": ["update an existing device"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2109, "code": "def find ( self , status = None , connected After = None ) : query Parms = { } if status : query Parms [ \"status\" ] = status if connected After : query Parms [ \"connected After\" ] = connected After return Iterable Client Status List ( self . api Client , filters = query Parms )", "predictions": ["finds the append and then d append to the append ."], "references": ["iterate through all connectors"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2110, "code": "def list ( self ) : url = \"api/v0002/mgmt/custom/bundle\" r = self . api Client . get ( url ) if r . status code == 200 : return r . json ( ) else : raise Api Exception ( r )", "predictions": ["command line option to the run"], "references": ["list all device management extension packages"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2111, "code": "def update Schema ( self , schema Id , schema Definition ) : req = Api Client . one Schema Url % ( self . host , \"/draft\" , schema Id ) body = { \"schema Definition\" : schema Definition } resp = requests . put ( req , auth = self . credentials , headers = { \"Content-Type\" : \"application/json\" } , data = json . dumps ( body ) , verify = self . verify ) if resp . status code == 200 : self . logger . debug ( \"Schema updated\" ) else : raise ibmiotf . API Exception ( resp . status code , \"HTTP error updating schema\" , resp ) return resp . json ( )", "predictions": ["run ( or . based on existing existing kwargs ."], "references": ["update a schema . throws apiexception on failure ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 2112, "code": "def disconnect ( self ) : self . client . disconnect ( ) self . client . loop stop ( ) self . logger . info ( \"Closed connection to the IBM Watson Io T Platform\" )", "predictions": ["disconnects the object from the database ."], "references": ["disconnect the client from ibm watson iot platform"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2113, "code": "def get ( self , device Uid , event Id ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) url = \"api/v0002/device/types/%s/devices/%s/events/%s\" % ( device Uid . type Id , device Uid . device Id , event Id ) r = self . api Client . get ( url ) if r . status code == 200 : return Last Event ( * * r . json ( ) ) else : raise Api Exception ( r )", "predictions": ["factory to construct an ( object from a device and return it ."], "references": ["retrieves the last cached message for specified event from a specific device ."], "bleu": 0.14283632578659286, "rouge_l": 0.30769230769230765}
{"id": 2114, "code": "def get All ( self , device Uid ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) url = \"api/v0002/device/types/%s/devices/%s/events\" % ( device Uid . type Id , device Uid . device Id ) r = self . api Client . get ( url ) if r . status code == 200 : events = [ ] for event in r . json ( ) : events . append ( Last Event ( * * event ) ) return events else : raise Api Exception ( r )", "predictions": ["get all device with the specified device ."], "references": ["retrieves a list of the last cached message for all events from a specific device ."], "bleu": 0.08821858171866302, "rouge_l": 0.23582474226804123}
{"id": 2115, "code": "def load Byte Array ( self , page , return Error ) : return Error . contents . value = self . Illegal State Error raise Not Implemented Error ( \"You must override this method.\" ) return ''", "predictions": ["calls the translation object and returns the result as a page"], "references": ["must be overridden . must return a string with the loaded data ."], "bleu": 0.10510262682013449, "rouge_l": 0.08209959623149395}
{"id": 2116, "code": "def check return ( result , func , cargs ) : if result != 0 : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'LAS Error in \"%s\": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return True", "predictions": ["retrieves a error from the error dictionary ."], "references": ["error checking for error calls"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 2117, "code": "def check void ( result , func , cargs ) : if not bool ( result ) : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'Error in \"%s\": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return result", "predictions": ["retrieves a warning in the mandatory window ."], "references": ["error checking for void * returns"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2118, "code": "def check void done ( result , func , cargs ) : if rt . Error Get Error Count ( ) : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'Error in \"%s\": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return result", "predictions": ["retrieves a transaction status ."], "references": ["error checking for void * returns that might be empty with no error"], "bleu": 0.04635036983311895, "rouge_l": 0.0}
{"id": 2119, "code": "def load ( self ) : if isinstance ( self . application , str ) : return util . import app ( self . application ) else : return self . application", "predictions": ["invokes the translation object from the given string ."], "references": ["attempt an import of the specified application"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2120, "code": "def init app ( self , app ) : if not hasattr ( app , 'extensions' ) : app . extensions = { } if 'common' in app . extensions : raise Runtime Error ( \"Flask-Common extension already initialized\" ) app . extensions [ 'common' ] = self self . app = app if 'COMMON FILESERVER DISABLED' not in app . config : with app . test request context ( ) : app . wsgi app = White Noise ( app . wsgi app , root = url for ( 'static' , filename = '' ) [ 1 : ] ) self . cache = Cache ( app , config = { 'CACHE TYPE' : app . config . get ( \"COMMON CACHE TYPE\" , 'simple' ) } ) @ app . before request def before request callback ( ) : request . start time = maya . now ( ) @ app . after request def after request callback ( response ) : if 'COMMON POWERED BY DISABLED' not in current app . config : response . headers [ 'X-Powered-By' ] = 'Flask' if 'COMMON PROCESSED TIME DISABLED' not in current app . config : response . headers [ 'X-Processed-Time' ] = maya . now ( ) . epoch - request . start time . epoch return response @ app . route ( '/favicon.ico' ) def favicon ( ) : return redirect ( url for ( 'static' , filename = 'favicon.ico' ) , code = 301 )", "predictions": ["creates a new main request ."], "references": ["initializes the flask application with common ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2121, "code": "def serve ( self , workers = None , * * kwargs ) : if self . app . debug : print ( crayons . yellow ( 'Booting Flask development server...' ) ) self . app . run ( ) else : print ( crayons . yellow ( 'Booting Gunicorn...' ) ) server = Gunicorn Server ( self . app , workers = workers or number of gunicorn workers ( ) , worker class = 'egg:meinheld#gunicorn worker' , * * kwargs ) server . run ( )", "predictions": ["this method requests up all workers to a given amount of workers ."], "references": ["serves the flask application ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 2122, "code": "def process image ( self , image , image format , save kwargs = { } ) : imagefile = Bytes IO ( ) inv image = Image Ops . invert ( image ) inv image . save ( imagefile , * * save kwargs ) return imagefile", "predictions": ["process a single image and save it to a larger image ."], "references": ["return a bytesio instance of image with inverted colors ."], "bleu": 0.1235622127262679, "rouge_l": 0.2772727272727273}
{"id": 2123, "code": "def to python ( self , data ) : if data is not None : if hasattr ( data , 'open' ) : data . open ( ) return super ( Versatile Image Form Field , self ) . to python ( data )", "predictions": ["convert this url to a regular expression ."], "references": ["ensure data is prepped properly before handing off to imagefield ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 2124, "code": "def pre save ( self , model instance , add ) : file = super ( Versatile Image Field , self ) . pre save ( model instance , add ) self . update ppoi field ( model instance ) return file", "predictions": ["extra image to . ."], "references": ["return field s value just before saving ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2125, "code": "def formfield ( self , * * kwargs ) : defaults = { } if self . ppoi field : defaults [ 'form class' ] = Sized Image Centerpoint Click Django Admin Field if kwargs . get ( 'widget' ) is Admin File Widget : del kwargs [ 'widget' ] defaults . update ( kwargs ) return super ( Versatile Image Field , self ) . formfield ( * * defaults )", "predictions": [". = , ( ) . formfield ( ) . . ( ) = . ( ) : . ( ) : / / . . . . com / . / ( / ( / ( / ( / ( / ( / ( / . / ( /"], "references": ["return a formfield ."], "bleu": 0.026594139297659906, "rouge_l": 0.08751793400286945}
{"id": 2126, "code": "def value to string ( self , obj ) : if DJANGO VERSION > ( 1 , 9 ) : value = self . value from object ( obj ) else : value = self . get val from obj ( obj ) return self . get prep value ( value )", "predictions": ["recursively convert any unicode string to a stringbuilder ."], "references": ["prepare field for serialization ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2127, "code": "def build filters and sizers ( self , ppoi value , create on demand ) : name = self . name if not name and self . field . placeholder image name : name = self . field . placeholder image name self . filters = Filter Library ( name , self . storage , versatileimagefield registry , ppoi value , create on demand ) for ( attr name , sizedimage cls ) in iteritems ( versatileimagefield registry . sizedimage registry ) : setattr ( self , attr name , sizedimage cls ( path to image = name , storage = self . storage , create on demand = create on demand , ppoi = ppoi value ) )", "predictions": ["builds the filters used to build the filters used in this group ."], "references": ["build the filters and sizers for a field ."], "bleu": 0.18798317647335086, "rouge_l": 0.37596302003081655}
{"id": 2128, "code": "def get filtered root folder ( self ) : folder , filename = os . path . split ( self . name ) return os . path . join ( folder , VERSATILEIMAGEFIELD FILTERED DIRNAME , '' )", "predictions": ["this method will return the root of the folder ."], "references": ["return the location where filtered images are stored ."], "bleu": 0.17827531042796255, "rouge_l": 0.31881533101045295}
{"id": 2129, "code": "def get sized root folder ( self ) : folder , filename = os . path . split ( self . name ) return os . path . join ( VERSATILEIMAGEFIELD SIZED DIRNAME , folder , '' )", "predictions": ["this method returns the root folder for the folder ."], "references": ["return the location where sized images are stored ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 2130, "code": "def get filtered sized root folder ( self ) : sized root folder = self . get sized root folder ( ) return os . path . join ( sized root folder , VERSATILEIMAGEFIELD FILTERED DIRNAME )", "predictions": ["this method is used to get the full path for the folder ."], "references": ["return the location where filtered + sized images are stored ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 2131, "code": "def retrieve image ( self , path to image ) : image = self . storage . open ( path to image , 'rb' ) file ext = path to image . rsplit ( '.' ) [ - 1 ] image format , mime type = get image metadata from file ext ( file ext ) return ( Image . open ( image ) , file ext , image format , mime type )", "predictions": ["retrieve a storage object from the currently running storage ."], "references": ["return a pil image instance stored at path_to_image ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 2132, "code": "def ppoi as str ( self ) : return \"%s %s\" % ( str ( self . ppoi [ 0 ] ) . replace ( '.' , '-' ) , str ( self . ppoi [ 1 ] ) . replace ( '.' , '-' ) )", "predictions": ["returns a string that is suitable for the first occurrence of this string in the document . this is a convenience method that returns a string that has to be used to represent the second character ."], "references": ["return ppoi value as a string ."], "bleu": 0.04609815356235176, "rouge_l": 0.15548003398470692}
{"id": 2133, "code": "def get context ( self , name , value , attrs ) : if self . has template widget rendering : context = super ( Clearable File Input With Image Preview , self ) . get context ( name , value , attrs ) else : context = { } context [ 'widget' ] = { 'name' : name , 'is hidden' : self . is hidden , 'required' : self . is required , 'value' : self . format value ( value ) , 'attrs' : self . build attrs ( self . attrs , attrs ) , 'template name' : self . template name , 'type' : self . input type , } checkbox name = self . clear checkbox name ( name ) checkbox id = self . clear checkbox id ( checkbox name ) context [ 'widget' ] . update ( { 'checkbox name' : checkbox name , 'checkbox id' : checkbox id , 'is initial' : self . is initial ( value ) , 'input text' : self . input text , 'initial text' : self . initial text , 'clear checkbox label' : self . clear checkbox label , } ) if value and hasattr ( value , \"url\" ) : context [ 'widget' ] . update ( { 'hidden field id' : self . get hidden field id ( name ) , 'point stage id' : self . get point stage id ( name ) , 'ppoi id' : self . get ppoi id ( name ) , 'sized url' : self . get sized url ( value ) , 'image preview id' : self . image preview id ( name ) , } ) return context", "predictions": ["returns a preview method to be called from the stage ."], "references": ["get the context to render this widget with ."], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 2134, "code": "def build attrs ( self , base attrs , extra attrs = None ) : attrs = base attrs . copy ( ) if extra attrs is not None : attrs . update ( extra attrs ) return attrs", "predictions": ["this function builds the configuration for the attributes used to build the extra attributes for the attributes used to build the data for the attributes ."], "references": ["build an attribute dictionary ."], "bleu": 0.051660454541342535, "rouge_l": 0.14698795180722893}
{"id": 2135, "code": "def get filtered path ( path to image , filename key , storage ) : containing folder , filename = os . path . split ( path to image ) filtered filename = get filtered filename ( filename , filename key ) path to return = os . path . join ( * [ containing folder , VERSATILEIMAGEFIELD FILTERED DIRNAME , filtered filename ] ) path to return = path to return . replace ( ' ' , '' ) return path to return", "predictions": ["returns the path to a given path ."], "references": ["return the filtered path"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 2136, "code": "def get url from image key ( image instance , image key ) : img key split = image key . split ( ' ' ) if 'x' in img key split [ - 1 ] : size key = img key split . pop ( - 1 ) else : size key = None img url = reduce ( getattr , img key split , image instance ) if size key : img url = img url [ size key ] . url return img url", "predictions": ["given an image name , get the url and key ."], "references": ["build a url from image_key ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 2137, "code": "def decode bytecode ( bytecode ) : bytecode wnd = memoryview ( bytecode ) while bytecode wnd : opcode id = byte2int ( bytecode wnd [ 0 ] ) opcode = OPCODE MAP [ opcode id ] if opcode . imm struct is not None : offs , imm , = opcode . imm struct . from raw ( None , bytecode wnd [ 1 : ] ) else : imm = None offs = 0 insn len = 1 + offs yield Instruction ( opcode , imm , insn len ) bytecode wnd = bytecode wnd [ insn len : ]", "predictions": ["generator function for . ."], "references": ["decodes raw bytecode yielding instruction s ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2138, "code": "def decode module ( module , decode name subsections = False ) : module wnd = memoryview ( module ) hdr = Module Header ( ) hdr len , hdr data , = hdr . from raw ( None , module wnd ) yield Module Fragment ( hdr , hdr data ) module wnd = module wnd [ hdr len : ] while module wnd : sec = Section ( ) sec len , sec data , = sec . from raw ( None , module wnd ) if ( decode name subsections and sec data . id == SEC UNK and sec data . name == SEC NAME ) : sec wnd = sec data . payload while sec wnd : subsec = Name Sub Section ( ) subsec len , subsec data , = subsec . from raw ( None , sec wnd ) yield Module Fragment ( subsec , subsec data ) sec wnd = sec wnd [ subsec len : ] else : yield Module Fragment ( sec , sec data ) module wnd = module wnd [ sec len : ]", "predictions": ["decode the payload module ."], "references": ["decodes raw wasm modules yielding modulefragment s ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2139, "code": "def deprecated func ( func ) : first usage = [ True ] @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : if first usage [ 0 ] : warnings . warn ( \"Call to deprecated function {}.\" . format ( func . name ) , Deprecation Warning , ) first usage [ 0 ] = False return func ( * args , * * kwargs ) return wrapper", "predictions": ["define a deprecation ( deprecated ) with the provided arguments ."], "references": ["deprecates a function printing a warning on the first usage ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 2140, "code": "def connect ( self ) : if self . loop is None : self . loop = asyncio . get event loop ( ) t = asyncio . Task ( self . loop . create connection ( self . config [ 'protocol factory' ] , self . config [ 'host' ] , self . config [ 'port' ] , ssl = self . config [ 'ssl' ] ) , loop = self . loop ) t . add done callback ( self . connection made ) return t", "predictions": ["neat this object and now ."], "references": ["connect to the server"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2141, "code": "def agi code check ( code = None , response = None , line = None ) : code = int ( code ) response = response or \"\" result = { 'status code' : code , 'result' : ( '' , '' ) , 'msg' : '' } if code == 100 : result [ 'msg' ] = line elif code == 200 : for key , value , data in re kv . findall ( response ) : result [ key ] = ( value , data ) if data == 'hangup' : return { 'error' : 'AGI Result Hangup' , 'msg' : 'User hungup during execution' } elif key == 'result' and value == '-1' : return { 'error' : 'AGI App Error' , 'msg' : 'Error executing application, or hangup' } elif code == 510 : result [ 'error' ] = 'AGI Invalid Command' elif code == 520 : result [ 'error' ] = 'AGI Usage Error' result [ 'msg' ] = line else : result [ 'error' ] = 'AGI Unknown Error' result [ 'msg' ] = line return result", "predictions": ["for the given code ."], "references": ["check the agi code and return a dict to help on error handling ."], "bleu": 0.053667245469253895, "rouge_l": 0.2909379968203497}
{"id": 2142, "code": "def get instances ( self ) : return [ \"<%s prefix:%s (uid:%s)>\" % ( self . class . name , i . prefix , self . uid ) for i in self . instances ]", "predictions": ["get a list of instances for this instance ."], "references": ["mostly used for debugging"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 2143, "code": "def gc ( ) : def after delete ( database ) : click . echo ( \"Deleted table %s\" % database ) app = get app ( ) upgrade from old version ( app ) app . delete orphan snapshots ( after delete )", "predictions": ["deletes an existing upgrade ."], "references": ["deletes old stellar tables that are not used anymore"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2144, "code": "def snapshot ( name ) : app = get app ( ) upgrade from old version ( app ) name = name or app . default snapshot name if app . get snapshot ( name ) : click . echo ( \"Snapshot with name %s already exists\" % name ) sys . exit ( 1 ) else : def before copy ( table name ) : click . echo ( \"Snapshotting database %s\" % table name ) app . create snapshot ( name , before copy = before copy )", "predictions": ["disconnect a disconnect from the database ."], "references": ["takes a snapshot of the database"], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 2145, "code": "def list ( ) : snapshots = get app ( ) . get snapshots ( ) click . echo ( '\\n' . join ( '%s: %s' % ( s . snapshot name , humanize . naturaltime ( datetime . utcnow ( ) - s . created at ) ) for s in snapshots ) )", "predictions": ["return a list of all the possible ones ."], "references": ["returns a list of snapshots"], "bleu": 0.2626909894424158, "rouge_l": 0.4518518518518518}
{"id": 2146, "code": "def restore ( name ) : app = get app ( ) if not name : snapshot = app . get latest snapshot ( ) if not snapshot : click . echo ( \"Couldn't find any snapshots for project %s\" % load config ( ) [ 'project name' ] ) sys . exit ( 1 ) else : snapshot = app . get snapshot ( name ) if not snapshot : click . echo ( \"Couldn't find snapshot with name %s.\\n\" \"You can list snapshots with 'stellar list'\" % name ) sys . exit ( 1 ) if not snapshot . slaves ready : if app . is copy process running ( snapshot ) : sys . stdout . write ( 'Waiting for background process(%s) to finish' % snapshot . worker pid ) sys . stdout . flush ( ) while not snapshot . slaves ready : sys . stdout . write ( '.' ) sys . stdout . flush ( ) sleep ( 1 ) app . db . session . refresh ( snapshot ) click . echo ( '' ) else : click . echo ( 'Background process missing, doing slow restore.' ) app . inline slave copy ( snapshot ) app . restore ( snapshot ) click . echo ( 'Restore complete.' )", "predictions": ["extracts the state of the device ."], "references": ["restores the database from a snapshot"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2147, "code": "def init ( ) : while True : url = click . prompt ( \"Please enter the url for your database.\\n\\n\" \"For example:\\n\" \"Postgre SQL: postgresql://localhost:5432/\\n\" \"My SQL: mysql+pymysql://root@localhost/\" ) if url . count ( '/' ) == 2 and not url . endswith ( '/' ) : url = url + '/' if ( url . count ( '/' ) == 3 and url . endswith ( '/' ) and url . startswith ( 'postgresql://' ) ) : connection url = url + 'template1' else : connection url = url engine = create engine ( connection url , echo = False ) try : conn = engine . connect ( ) except Operational Error as err : click . echo ( \"Could not connect to database: %s\" % url ) click . echo ( \"Error message: %s\" % err . message ) click . echo ( '' ) else : break if engine . dialect . name not in SUPPORTED DIALECTS : click . echo ( \"Your engine dialect %s is not supported.\" % ( engine . dialect . name ) ) click . echo ( \"Supported dialects: %s\" % ( ', ' . join ( SUPPORTED DIALECTS ) ) ) if url . count ( '/' ) == 3 and url . endswith ( '/' ) : while True : click . echo ( \"You have the following databases: %s\" % ', ' . join ( [ db for db in list of databases ( conn ) if not db . startswith ( 'stellar ' ) ] ) ) db name = click . prompt ( \"Please enter the name of the database (eg. projectdb)\" ) if database exists ( conn , db name ) : break else : click . echo ( \"Could not find database %s\" % db name ) click . echo ( '' ) else : db name = url . rsplit ( '/' , 1 ) [ - 1 ] url = url . rsplit ( '/' , 1 ) [ 0 ] + '/' name = click . prompt ( 'Please enter your project name (used internally, eg. %s)' % db name , default = db name ) raw url = url if engine . dialect . name == 'postgresql' : raw url = raw url + 'template1' with open ( 'stellar.yaml' , 'w' ) as project file : project file . write ( . strip ( ) % { 'name' : name , 'raw url' : raw url , 'url' : url , 'db name' : db name } ) click . echo ( \"Wrote stellar.yaml\" ) click . echo ( '' ) if engine . dialect . name == 'mysql' : click . echo ( \"Warning: My SQL support is still in beta.\" ) click . echo ( \"Tip: You probably want to take a snapshot: stellar snapshot\" )", "predictions": ["initializes all ( and builds their database ."], "references": ["initializes stellar configuration ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 2148, "code": "def on epoch end ( self ) -> None : self . indexes = np . arange ( self . nrows ) if self . shuffle : np . random . shuffle ( self . indexes )", "predictions": ["call this to provide the get method to shuffle the underlying stream ."], "references": ["updates indexes after each epoch for shuffling"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2149, "code": "def process text constructor ( cleaner : Callable , tokenizer : Callable , append indicators : bool , start tok : str , end tok : str ) : def process text ( text ) : if append indicators : return [ [ start tok ] + tokenizer ( cleaner ( doc ) ) + [ end tok ] for doc in text ] return [ tokenizer ( cleaner ( doc ) ) for doc in text ] return process text", "predictions": ["check all void void void void void ( s ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["generate a function that will clean and tokenize text ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 2150, "code": "def process text ( self , text : List [ str ] ) -> List [ List [ str ] ] : process text = process text constructor ( cleaner = self . cleaner , tokenizer = self . tokenizer , append indicators = self . append indicators , start tok = self . start tok , end tok = self . end tok ) return process text ( text )", "predictions": ["this is a list of void check for some void . this method is useful for the command line ."], "references": ["combine the cleaner and tokenizer ."], "bleu": 0.06760229884571738, "rouge_l": 0.17039106145251398}
{"id": 2151, "code": "def generate doc length stats ( self ) : heuristic = self . heuristic pct histdf = ( pd . Data Frame ( [ ( a , b ) for a , b in self . document length histogram . items ( ) ] , columns = [ 'bin' , 'doc count' ] ) . sort values ( by = 'bin' ) ) histdf [ 'cumsum pct' ] = histdf . doc count . cumsum ( ) / histdf . doc count . sum ( ) self . document length stats = histdf self . doc length huerestic = histdf . query ( f'cumsum pct >= {heuristic}' ) . bin . head ( 1 ) . values [ 0 ] logging . warning ( ' ' . join ( [ \"Setting maximum document length to\" , f'{self.doc length huerestic} based upon' , f'heuristic of {heuristic} percentile.\\n' , 'See full histogram by insepecting the' , \"`document length stats` attribute.\" ] ) ) self . padding maxlen = self . doc length huerestic", "predictions": ["for each document , generate a histogram with . ."], "references": ["analyze document length statistics for padding strategy"], "bleu": 0.13950796967929133, "rouge_l": 0.12151394422310759}
{"id": 2152, "code": "def token count pandas ( self ) : freq df = pd . Data Frame . from dict ( self . indexer . word counts , orient = 'index' ) freq df . columns = [ 'count' ] return freq df . sort values ( 'count' , ascending = False )", "predictions": ["returns the amount of this frame as a tuple ."], "references": ["see token counts as pandas dataframe"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2153, "code": "def inv cls ( cls ) : if cls . fwdm cls is cls . invm cls : return cls if not getattr ( cls , ' inv cls ' , None ) : class Inv ( cls ) : fwdm cls = cls . invm cls invm cls = cls . fwdm cls inv cls = cls Inv . name = cls . name + 'Inv' cls . inv cls = Inv return cls . inv cls", "predictions": ["starts an existing class ."], "references": ["the inverse of this bidict type i . e . one with * _fwdm_cls * and * _invm_cls * swapped ."], "bleu": 0.011128574349134012, "rouge_l": 0.06923950056753689}
{"id": 2154, "code": "def update with rollback ( self , on dup , * args , * * kw ) : writelog = [ ] appendlog = writelog . append dedup item = self . dedup item write item = self . write item for ( key , val ) in iteritems args kw ( * args , * * kw ) : try : dedup result = dedup item ( key , val , on dup ) except Duplication Error : undo write = self . undo write for dedup result , write result in reversed ( writelog ) : undo write ( dedup result , write result ) raise if dedup result is not NOOP : write result = write item ( key , val , dedup result ) appendlog ( ( dedup result , write result ) )", "predictions": ["currently , process the ( ) method with a new operation ."], "references": ["update rolling back on failure ."], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 2155, "code": "def copy ( self ) : copy = self . class . new ( self . class ) copy . fwdm = self . fwdm . copy ( ) copy . invm = self . invm . copy ( ) copy . init inv ( ) return copy", "predictions": ["creates a to to to to this object ."], "references": ["a shallow copy ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 2156, "code": "def copy ( self ) : copy = self . class . new ( self . class ) sntl = Sentinel ( ) fwdm = self . fwdm . copy ( ) invm = self . invm . copy ( ) cur = sntl nxt = sntl . nxt for ( key , val ) in iteritems ( self ) : nxt = Node ( cur , sntl ) cur . nxt = fwdm [ key ] = invm [ val ] = nxt cur = nxt sntl . prv = nxt copy . sntl = sntl copy . fwdm = fwdm copy . invm = invm copy . init inv ( ) return copy", "predictions": ["copies this instance of . to a )"], "references": ["a shallow copy of this ordered bidict ."], "bleu": 0.20164945583740668, "rouge_l": 0.25}
{"id": 2157, "code": "def clear ( self ) : self . fwdm . clear ( ) self . invm . clear ( ) self . sntl . nxt = self . sntl . prv = self . sntl", "predictions": ["for each url , get the jcombobox ."], "references": ["remove all items ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2158, "code": "def new contact ( cls , address book , supported private objects , version , localize dates ) : return cls ( address book , None , supported private objects , version , localize dates )", "predictions": ["create a value to instantiate and update it with the provided name ."], "references": ["use this to create a new and empty contact ."], "bleu": 0.14949751774990683, "rouge_l": 0.3562043795620438}
{"id": 2159, "code": "def from user input ( cls , address book , user input , supported private objects , version , localize dates ) : contact = cls ( address book , None , supported private objects , version , localize dates ) contact . process user input ( user input ) return contact", "predictions": ["helper function to instantiate a filters from an and image ."], "references": ["use this if you want to create a new contact from user input ."], "bleu": 0.10904215887663019, "rouge_l": 0.31322207958921694}
{"id": 2160, "code": "def add category ( self , categories ) : categories obj = self . vcard . add ( 'categories' ) categories obj . value = helpers . convert to vcard ( \"category\" , categories , Object Type . list with strings )", "predictions": ["adds a list of self - specific self ."], "references": ["categories variable must be a list"], "bleu": 0.18575057999133596, "rouge_l": 0.27664399092970515}
{"id": 2161, "code": "def avail archs ( self ) : return { ARM32 : ( KS ARCH ARM , KS MODE ARM ) , ARM64 : ( KS ARCH ARM64 , KS MODE LITTLE ENDIAN ) , ARM TB : ( KS ARCH ARM , KS MODE THUMB ) , HEXAGON : ( KS ARCH HEXAGON , KS MODE BIG ENDIAN ) , MIPS32 : ( KS ARCH MIPS , KS MODE MIPS32 ) , MIPS64 : ( KS ARCH MIPS , KS MODE MIPS64 ) , PPC32 : ( KS ARCH PPC , KS MODE PPC32 ) , PPC64 : ( KS ARCH PPC , KS MODE PPC64 ) , SPARC32 : ( KS ARCH SPARC , KS MODE SPARC32 ) , SPARC64 : ( KS ARCH SPARC , KS MODE SPARC64 ) , SYSTEMZ : ( KS ARCH SYSTEMZ , KS MODE BIG ENDIAN ) , X86 16 : ( KS ARCH X86 , KS MODE 16 ) , X86 32 : ( KS ARCH X86 , KS MODE 32 ) , X86 64 : ( KS ARCH X86 , KS MODE 64 ) , }", "predictions": ["creates a big decoder for the given set of ( ."], "references": ["initialize the dictionary of architectures for assembling via keystone"], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 2162, "code": "def avail archs ( self ) : return { ARM32 : ( CS ARCH ARM , CS MODE ARM ) , ARM64 : ( CS ARCH ARM64 , CS MODE LITTLE ENDIAN ) , ARM TB : ( CS ARCH ARM , CS MODE THUMB ) , MIPS32 : ( CS ARCH MIPS , CS MODE MIPS32 ) , MIPS64 : ( CS ARCH MIPS , CS MODE MIPS64 ) , SPARC32 : ( CS ARCH SPARC , CS MODE BIG ENDIAN ) , SPARC64 : ( CS ARCH SPARC , CS MODE V9 ) , SYSTEMZ : ( CS ARCH SYSZ , CS MODE BIG ENDIAN ) , X86 16 : ( CS ARCH X86 , CS MODE 16 ) , X86 32 : ( CS ARCH X86 , CS MODE 32 ) , X86 64 : ( CS ARCH X86 , CS MODE 64 ) , }", "predictions": ["self - stage for given type in the range of raw composer ."], "references": ["initialize the dictionary of architectures for disassembling via capstone"], "bleu": 0.1135935489027116, "rouge_l": 0.18798151001540828}
{"id": 2163, "code": "def safe input ( prompt ) : if sys . version info < ( 3 , 0 ) : if isinstance ( prompt , compat . text type ) : encoding = locale . getpreferredencoding ( ) or 'utf-8' prompt = prompt . encode ( encoding ) else : if not isinstance ( prompt , compat . text type ) : prompt = prompt . decode ( ) return input ( prompt )", "predictions": ["return the image image image image ."], "references": ["prompts user for input . correctly handles prompt message encoding ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 2164, "code": "def first ( self ) : lim = [ 0 , 1 ] if self . limit : lim [ 0 ] = self . limit [ 0 ] if not self . filters and not self . order by : for ent in self : return ent return None ids = self . limit ( * lim ) . search ( ) if ids : return self . model . get ( ids [ 0 ] ) return None", "predictions": ["first a list of words ."], "references": ["returns only the first result from the query if any ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 2165, "code": "def redis prefix lua ( conn , dest , index , prefix , is first , pattern = None ) : tkey = '%s:%s' % ( index . partition ( ':' ) [ 0 ] , uuid . uuid4 ( ) ) start , end = start end ( prefix ) return redis prefix lua ( conn , [ dest , tkey , index ] , [ start , end , pattern or prefix , int ( pattern is not None ) , int ( bool ( is first ) ) ] )", "predictions": ["return operation : 1 . context = ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ( - ("], "references": ["performs the actual prefix suffix and pattern match operations ."], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 2166, "code": "def add ( self , obj ) : if self . null session : return self . init ( ) pk = obj . pk if not pk . endswith ( ':None' ) : self . known [ pk ] = obj self . wknown [ pk ] = obj", "predictions": ["adds a reference to this object ."], "references": ["adds an entity to the session ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 2167, "code": "def get ( self , pk ) : self . init ( ) return self . known . get ( pk ) or self . wknown . get ( pk )", "predictions": ["retrieves and returns a specific call to this one ."], "references": ["fetches an entity from the session based on primary key ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 2168, "code": "def register ( cls , type , reduce func ) : if sys . version info < ( 3 , ) : def dispatcher ( cls , obj ) : reduced = reduce func ( obj ) cls . save reduce ( obj = obj , * reduced ) cls . dispatch table [ type ] = dispatcher else : cls . dispatch table [ type ] = reduce func", "predictions": ["get push or class pop of a specific key . this is used to get a result of a specific profile . the = = [ ] = [ ] = = [ ] - > = 1 , 2 ] = 1 , 2 ] - > 2 ,"], "references": ["attach a reducer function to a given type in the dispatch table ."], "bleu": 0.03162593967015063, "rouge_l": 0.14202561117578583}
{"id": 2169, "code": "def Queue ( self , maxsize = 0 , reducers = None ) : from . queues import Queue return Queue ( maxsize , reducers = reducers , ctx = self . get context ( ) )", "predictions": ["returns the queue to be added to the queue ."], "references": ["returns a queue object"], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 2170, "code": "def Simple Queue ( self , reducers = None ) : from . queues import Simple Queue return Simple Queue ( reducers = reducers , ctx = self . get context ( ) )", "predictions": ["returns the next queue in the priority queue ."], "references": ["returns a queue object"], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 2171, "code": "def sendback result ( result queue , work id , result = None , exception = None ) : try : result queue . put ( Result Item ( work id , result = result , exception = exception ) ) except Base Exception as e : exc = Exception With Traceback ( e ) result queue . put ( Result Item ( work id , exception = exc ) )", "predictions": ["add or remove an item from the ) ."], "references": ["safely send back the given result or exception"], "bleu": 0.15619699684601276, "rouge_l": 0.1189083820662768}
{"id": 2172, "code": "def ensure executor running ( self ) : with self . processes management lock : if len ( self . processes ) != self . max workers : self . adjust process count ( ) self . start queue management thread ( )", "predictions": ["forces this class to connect to the gl queue ."], "references": ["ensures all workers and management thread are running"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2173, "code": "def start ( self , initializer = None , initargs = ( ) ) : assert self . state . value == State . INITIAL if ( initializer is not None and not hasattr ( initializer , ' call ' ) ) : raise Type Error ( 'initializer must be a callable' ) reader , writer = mp . Pipe ( duplex = False ) self . process = Process ( target = type ( self ) . run server , args = ( self . registry , self . address , bytes ( self . authkey ) , self . serializer , writer , initializer , initargs ) , ) ident = ':' . join ( str ( i ) for i in self . process . identity ) self . process . name = type ( self ) . name + '-' + ident self . process . start ( ) writer . close ( ) self . address = reader . recv ( ) reader . close ( ) self . state . value = State . STARTED self . shutdown = mp . util . Finalize ( self , type ( self ) . finalize manager , args = ( self . process , self . address , self . authkey , self . state , self . Client ) , exitpriority = 0 )", "predictions": ["starts the change in this process ."], "references": ["spawn a server process for this manager object"], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 2174, "code": "def Dup Fd ( fd ) : popen obj = get spawning popen ( ) if popen obj is not None : return popen obj . Dup Fd ( popen obj . duplicate for child ( fd ) ) elif HAVE SEND HANDLE and sys . version info [ : 2 ] > ( 3 , 3 ) : from multiprocessing import resource sharer return resource sharer . Dup Fd ( fd ) else : raise Type Error ( 'Cannot pickle connection object. This object can only be ' 'passed when spawning a new process' )", "predictions": ["generate a resource for an object ."], "references": ["return a wrapper for an fd ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 2175, "code": "def wait job completion ( self ) : if len ( self . pending work items ) > 0 : warnings . warn ( \"Trying to resize an executor with running jobs: \" \"waiting for jobs completion before resizing.\" , User Warning ) mp . util . debug ( \"Executor {} waiting for jobs completion before\" \" resizing\" . format ( self . executor id ) ) while len ( self . pending work items ) > 0 : time . sleep ( 1e-3 )", "predictions": ["gc the state of the ( ."], "references": ["wait for the cache to be empty before resizing the pool ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 2176, "code": "def get preparation data ( name , init main module = True ) : check not importing main ( ) d = dict ( log to stderr = util . log to stderr , authkey = bytes ( process . current process ( ) . authkey ) , ) if util . logger is not None : d [ 'log level' ] = util . logger . get Effective Level ( ) if len ( util . logger . handlers ) > 0 : h = util . logger . handlers [ 0 ] d [ 'log fmt' ] = h . formatter . fmt sys path = [ p for p in sys . path ] try : i = sys path . index ( '' ) except Value Error : pass else : sys path [ i ] = process . ORIGINAL DIR d . update ( name = name , sys path = sys path , sys argv = sys . argv , orig dir = process . ORIGINAL DIR , dir = os . getcwd ( ) ) if sys . platform != \"win32\" : from . import semaphore tracker semaphore tracker . ensure running ( ) d [ 'tracker pid' ] = semaphore tracker . semaphore tracker . pid if init main module : main module = sys . modules [ ' main ' ] try : main mod name = getattr ( main module . spec , \"name\" , None ) except Base Exception : main mod name = None if main mod name is not None : d [ 'init main from name' ] = main mod name elif sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : main path = getattr ( main module , ' file ' , None ) if main path is not None : if ( not os . path . isabs ( main path ) and process . ORIGINAL DIR is not None ) : main path = os . path . join ( process . ORIGINAL DIR , main path ) d [ 'init main from path' ] = os . path . normpath ( main path ) d [ 'main path' ] = d [ 'init main from path' ] return d", "predictions": ["creates a new pty . in the case of . is called in the state of . ."], "references": ["return info about parent needed by child to unpickle process object"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 2177, "code": "def prepare ( data ) : if 'name' in data : process . current process ( ) . name = data [ 'name' ] if 'authkey' in data : process . current process ( ) . authkey = data [ 'authkey' ] if 'log to stderr' in data and data [ 'log to stderr' ] : util . log to stderr ( ) if 'log level' in data : util . get logger ( ) . set Level ( data [ 'log level' ] ) if 'log fmt' in data : import logging util . get logger ( ) . handlers [ 0 ] . set Formatter ( logging . Formatter ( data [ 'log fmt' ] ) ) if 'sys path' in data : sys . path = data [ 'sys path' ] if 'sys argv' in data : sys . argv = data [ 'sys argv' ] if 'dir' in data : os . chdir ( data [ 'dir' ] ) if 'orig dir' in data : process . ORIGINAL DIR = data [ 'orig dir' ] if 'tracker pid' in data : from . import semaphore tracker semaphore tracker . semaphore tracker . pid = data [ \"tracker pid\" ] if 'init main from name' in data : fixup main from name ( data [ 'init main from name' ] ) elif 'init main from path' in data : fixup main from path ( data [ 'init main from path' ] )", "predictions": ["prepare and get current process ready to be completed ."], "references": ["try to get current process ready to unpickle process object"], "bleu": 0.4617366309441026, "rouge_l": 0.5}
{"id": 2178, "code": "def close fds ( keep fds ) : keep fds = set ( keep fds ) . union ( [ 1 , 2 ] ) try : open fds = set ( int ( fd ) for fd in os . listdir ( '/proc/self/fd' ) ) except File Not Found Error : import resource max nfds = resource . getrlimit ( resource . RLIMIT NOFILE ) [ 0 ] open fds = set ( fd for fd in range ( 3 , max nfds ) ) open fds . add ( 0 ) for i in open fds - keep fds : try : os . close ( i ) except OS Error : pass", "predictions": ["close the fetched and push all the resources ."], "references": ["close all the file descriptors except those in keep_fds ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 2179, "code": "def recursive terminate without psutil ( process ) : try : recursive terminate ( process . pid ) except OS Error as e : warnings . warn ( \"Failed to kill subprocesses on this platform. Please\" ) process . terminate ( ) process . join ( )", "predictions": ["attempts to terminate the process of the given process ."], "references": ["terminate a process and its descendants ."], "bleu": 0.14991106946711685, "rouge_l": 0.36454183266932266}
{"id": 2180, "code": "def recursive terminate ( pid ) : if sys . platform == \"win32\" : try : subprocess . check output ( [ \"taskkill\" , \"/F\" , \"/T\" , \"/PID\" , str ( pid ) ] , stderr = None ) except subprocess . Called Process Error as e : if e . returncode not in [ 1 , 128 , 255 ] : raise elif e . returncode == 1 : try : os . kill ( pid , signal . SIGTERM ) except OS Error as e : if e . errno != errno . ESRCH : raise else : try : children pids = subprocess . check output ( [ \"pgrep\" , \"-P\" , str ( pid ) ] , stderr = None ) except subprocess . Called Process Error as e : if e . returncode == 1 : children pids = b'' else : raise children pids = children pids . decode ( ) . split ( '\\n' ) [ : - 1 ] for cpid in children pids : cpid = int ( cpid ) recursive terminate ( cpid ) try : os . kill ( pid , signal . SIGTERM ) except OS Error as e : if e . errno != errno . ESRCH : raise", "predictions": ["retrieve an instance of the process ."], "references": ["recursively kill the descendants of a process before killing it ."], "bleu": 0.1319006407505858, "rouge_l": 0.32049036777583184}
{"id": 2181, "code": "def format exitcodes ( exitcodes ) : str exitcodes = [ \"{}({})\" . format ( get exitcode name ( e ) , e ) for e in exitcodes if e is not None ] return \"{\" + \", \" . join ( str exitcodes ) + \"}\"", "predictions": ["helper method to construct a suitable suitable for printing ."], "references": ["format a list of exit code with names of the signals if possible"], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 2182, "code": "def main ( fd , verbose = 0 ) : signal . signal ( signal . SIGINT , signal . SIG IGN ) signal . signal ( signal . SIGTERM , signal . SIG IGN ) if HAVE SIGMASK : signal . pthread sigmask ( signal . SIG UNBLOCK , IGNORED SIGNALS ) for f in ( sys . stdin , sys . stdout ) : try : f . close ( ) except Exception : pass if verbose : sys . stderr . write ( \"Main semaphore tracker is running\\n\" ) sys . stderr . flush ( ) cache = set ( ) try : with os . fdopen ( fd , 'rb' ) as f : for line in f : try : cmd , name = line . strip ( ) . split ( b':' ) if cmd == b'REGISTER' : name = name . decode ( 'ascii' ) cache . add ( name ) if verbose : sys . stderr . write ( \"[Semaphore Tracker] register {}\\n\" . format ( name ) ) sys . stderr . flush ( ) elif cmd == b'UNREGISTER' : name = name . decode ( 'ascii' ) cache . remove ( name ) if verbose : sys . stderr . write ( \"[Semaphore Tracker] unregister {}\" \": cache({})\\n\" . format ( name , len ( cache ) ) ) sys . stderr . flush ( ) elif cmd == b'PROBE' : pass else : raise Runtime Error ( 'unrecognized command %r' % cmd ) except Base Exception : try : sys . excepthook ( * sys . exc info ( ) ) except Base Exception : pass finally : if cache : try : warnings . warn ( 'semaphore tracker: There appear to be %d ' 'leaked semaphores to clean up at shutdown' % len ( cache ) ) except Exception : pass for name in cache : try : try : sem unlink ( name ) if verbose : sys . stderr . write ( \"[Semaphore Tracker] unlink {}\\n\" . format ( name ) ) sys . stderr . flush ( ) except Exception as e : warnings . warn ( 'semaphore tracker: %s: %r' % ( name , e ) ) finally : pass if verbose : sys . stderr . write ( \"semaphore tracker shut down\\n\" ) sys . stderr . flush ( )", "predictions": ["write the program to the program ."], "references": ["run semaphore tracker ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2183, "code": "def event processor ( self , frame , event , arg ) : out = self . debugger . intf [ - 1 ] . output lineno = frame . f lineno filename = self . core . canonic filename ( frame ) filename = self . core . filename ( filename ) if not out : print ( \"%s - %s:%d\" % ( event , filename , lineno ) ) else : out . write ( \"%s - %s:%d\" % ( event , filename , lineno ) ) if arg is not None : out . writeline ( ', %s ' % repr ( arg ) ) else : out . writeline ( '' ) pass pass return self . event processor", "predictions": ["run a core event ."], "references": ["a simple event processor that prints out events ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 2184, "code": "def run hooks ( obj , hooks , * args ) : for hook in hooks : if hook ( obj , * args ) : return True pass return False", "predictions": ["runs all the functions in the given object , including the existing ones ."], "references": ["run each function in hooks with args"], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 2185, "code": "def forget ( self ) : self . stack = [ ] self . curindex = 0 self . curframe = None self . thread name = None self . frame thread name = None return", "predictions": ["creates a new song for the current thread ."], "references": ["remove memory of state variables set in the command processor"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 2186, "code": "def process commands ( self ) : if self . core . execution status != 'No program' : self . setup ( ) self . location ( ) pass leave loop = run hooks ( self , self . preloop hooks ) self . continue running = False while not leave loop : try : run hooks ( self , self . precmd hooks ) leave loop = self . process command ( ) if leave loop or self . continue running : break except EOF Error : if len ( self . debugger . intf ) > 1 : del self . debugger . intf [ - 1 ] self . last command = '' else : if self . debugger . intf [ - 1 ] . output : self . debugger . intf [ - 1 ] . output . writeline ( 'Leaving' ) raise Mexcept . Debugger Quit pass break pass pass return run hooks ( self , self . postcmd hooks )", "predictions": [", for ( . do not include all debugger listeners ."], "references": ["handle debugger commands ."], "bleu": 0.12605968092174913, "rouge_l": 0.2911694510739857}
{"id": 2187, "code": "def read history file ( self ) : histfile = self . debugger . intf [ - 1 ] . histfile try : import readline readline . read history file ( histfile ) except IO Error : pass except Import Error : pass return", "predictions": ["read debugger history from fo ."], "references": ["read the command history file -- possibly ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 2188, "code": "def write history file ( self ) : settings = self . debugger . settings histfile = self . debugger . intf [ - 1 ] . histfile if settings [ 'hist save' ] : try : import readline try : readline . write history file ( histfile ) except IO Error : pass except Import Error : pass pass return", "predictions": ["writes history event to the end of this debugger ."], "references": ["write the command history file -- possibly ."], "bleu": 0.14991106946711685, "rouge_l": 0.22676579925650556}
{"id": 2189, "code": "def errmsg ( self , msg , prefix = \"** \" ) : if not self . verbose : location = ( \"%s:%s: Error in source command file\" % ( self . script name , self . input lineno ) ) msg = \"%s%s:\\n%s%s\" % ( prefix , location , prefix , msg ) else : msg = \"%s%s\" % ( prefix , msg ) pass self . msg ( msg ) if self . abort on error : raise EOF Error return", "predictions": ["method to get the message class for this source ."], "references": ["common routine for reporting debugger error messages ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 2190, "code": "def close ( self ) : self . state = 'closing' if self . input : self . input . close ( ) pass if self . output : self . output . close ( ) pass self . state = 'disconnnected' return", "predictions": ["closes this object and its error if it is not already closed ."], "references": ["closes both input and output"], "bleu": 0.10571070857151538, "rouge_l": 0.24158415841584158}
{"id": 2191, "code": "def disassemble ( msg , msg nocr , section , co , lasti = - 1 , start line = - 1 , end line = None , relative pos = False , highlight = 'light' , start offset = 0 , end offset = None ) : return disassemble bytes ( msg , msg nocr , co . co code , lasti , co . co firstlineno , start line , end line , relative pos , co . co varnames , co . co names , co . co consts , co . co cellvars , co . co freevars , dict ( findlinestarts ( co ) ) , highlight , start offset = start offset , end offset = end offset )", "predictions": ["extract disassemble information from this object ."], "references": ["disassemble a code object ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 2192, "code": "def count frames ( frame , count start = 0 ) : count = - count start while frame : count += 1 frame = frame . f back return count", "predictions": ["counts the number of frames of this . ."], "references": ["return a count of the number of frames"], "bleu": 0.4111336169005197, "rouge_l": 0.4756335282651072}
{"id": 2193, "code": "def print stack trace ( proc obj , count = None , color = 'plain' , opts = { } ) : if count is None : n = len ( proc obj . stack ) else : n = min ( len ( proc obj . stack ) , count ) try : for i in range ( n ) : print stack entry ( proc obj , i , color = color , opts = opts ) except Keyboard Interrupt : pass return", "predictions": ["prints the stack trace for all objects in a stack ."], "references": ["print count entries of the stack trace"], "bleu": 0.21200626759025185, "rouge_l": 0.3472485768500949}
{"id": 2194, "code": "def eval print obj ( arg , frame , format = None , short = False ) : try : if not frame : val = eval ( arg , None , None ) else : val = eval ( arg , frame . f globals , frame . f locals ) pass except : return 'No symbol \"' + arg + '\" in current context.' return print obj ( arg , val , format , short )", "predictions": ["the function call to create a format ."], "references": ["return a string representation of an object"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2195, "code": "def print obj ( arg , val , format = None , short = False ) : what = arg if format : what = format + ' ' + arg val = Mprint . printf ( val , format ) pass s = '%s = %s' % ( what , val ) if not short : s += '\\n  type = %s' % type ( val ) s = print dict ( s , val , \"object variables\" ) if hasattr ( val , \" class \" ) : s = print dict ( s , val . class , \"class variables\" ) pass pass return s", "predictions": ["prints an object with its type ."], "references": ["return a string representation of an object"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 2196, "code": "def lookup ( self , subcmd prefix ) : for subcmd name in list ( self . subcmds . keys ( ) ) : if subcmd name . startswith ( subcmd prefix ) and len ( subcmd prefix ) >= self . subcmds [ subcmd name ] . class . min abbrev : return self . subcmds [ subcmd name ] pass return None", "predictions": ["lookup a previously generated declaration so that it can be automatically removed from the map ."], "references": ["find subcmd in self . subcmds"], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 2197, "code": "def short help ( self , subcmd cb , subcmd name , label = False ) : entry = self . lookup ( subcmd name ) if entry : if label : prefix = entry . name else : prefix = '' pass if hasattr ( entry , 'short help' ) : if prefix : prefix += ' -- ' self . cmd obj . msg ( prefix + entry . short help ) pass pass else : self . undefined subcmd ( \"help\" , subcmd name ) pass return", "predictions": ["lookup a method by name ."], "references": ["show short help for a subcommand ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 2198, "code": "def run ( self , subcmd name , arg ) : entry = self . lookup ( subcmd name ) if entry : entry [ 'callback' ] ( arg ) else : self . cmdproc . undefined cmd ( entry . class . name , subcmd name ) pass return", "predictions": ["this method runs the appropriate java method with the given name ."], "references": ["run subcmd_name with args using obj for the environent"], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 2199, "code": "def help ( self , * args ) : print ( args ) subcmd prefix = args [ 0 ] if not subcmd prefix or len ( subcmd prefix ) == 0 : self . msg ( self . doc ) self . msg ( % ( self . name ) ) for subcmd name in self . list ( ) : self . subcmd helper ( subcmd name , self , True , True ) return entry = self . lookup ( subcmd prefix ) if entry and hasattr ( entry , 'help' ) : entry . help ( args ) else : self . cmd obj . errmsg ( \"Unknown 'help %s' subcommand %s\" % ( self . name , subcmd prefix ) )", "predictions": ["get a help method call ."], "references": ["help for subcommands ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 2200, "code": "def list categories ( self ) : self . section ( \"Classes of commands:\" ) cats = list ( categories . keys ( ) ) cats . sort ( ) for cat in cats : self . msg ( \"  %-13s -- %s\" % ( cat , categories [ cat ] ) ) pass final msg = for line in re . compile ( '\\n' ) . split ( final msg . rstrip ( '\\n' ) ) : self . rst msg ( line ) pass return", "predictions": ["list all categories contained in this group ."], "references": ["list the command categories and a short description of each ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 2201, "code": "def show category ( self , category , args ) : n2cmd = self . proc . commands names = list ( n2cmd . keys ( ) ) if len ( args ) == 1 and args [ 0 ] == '*' : self . section ( \"Commands in class %s:\" % category ) cmds = [ cmd for cmd in names if category == n2cmd [ cmd ] . category ] cmds . sort ( ) self . msg nocr ( self . columnize commands ( cmds ) ) return self . msg ( \"%s.\\n\" % categories [ category ] ) self . section ( \"List of commands:\" ) names . sort ( ) for name in names : if category != n2cmd [ name ] . category : continue self . msg ( \"%-13s -- %s\" % ( name , n2cmd [ name ] . short help , ) ) pass return", "predictions": ["show this category as a category in a category ."], "references": ["show short help for all commands in category ."], "bleu": 0.18850319022747347, "rouge_l": 0.42508710801393734}
{"id": 2202, "code": "def run ( self , args ) : if not self . proc . curframe : self . errmsg ( \"No line number information available.\" ) return if len ( args ) == 3 : answer = self . lineinfo ( args [ 2 ] ) if answer [ 0 ] : item , filename , lineno = answer if not os . path . isfile ( filename ) : filename = Mclifns . search file ( filename , self . core . search path , self . main dirname ) self . msg ( 'Line %s of \"%s\" <%s>' % ( lineno , filename , item ) ) return filename = self . core . canonic filename ( self . proc . curframe ) if not os . path . isfile ( filename ) : filename = Mclifns . search file ( filename , self . core . search path , self . main dirname ) pass filename = self . core . canonic filename ( self . proc . curframe ) msg1 = 'Line %d of \\\"%s\\\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) msg2 = ( 'at instruction %d' % self . proc . curframe . f lasti ) if self . proc . event : msg2 += ', %s event' % self . proc . event pass self . msg ( Mmisc . wrapped lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) return False", "predictions": ["run this method with the arguments supplied ."], "references": ["current line number in source file"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2203, "code": "def map thread names ( ) : name2id = { } for thread id in list ( threading . active . keys ( ) ) : thread = threading . active [ thread id ] name = thread . get Name ( ) if name not in list ( name2id . keys ( ) ) : name2id [ name ] = thread id pass pass return name2id", "predictions": ["set the thread to the thread ."], "references": ["invert threading . _active"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2204, "code": "def open ( self , inp , opts = None ) : if isinstance ( inp , list ) : self . input = inp else : raise IO Error ( \"Invalid input type (%s) for %s\" % ( type ( inp ) , inp ) ) return", "predictions": ["opens a operator and opens the underlying character stream ."], "references": ["use this to set where to read from ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 2205, "code": "def get int ( errmsg , arg , default = 1 , cmdname = None ) : if arg : try : default = int ( eval ( arg ) ) except ( Syntax Error , Name Error , Value Error ) : if cmdname : errmsg ( \"Command '%s' expects an integer; got: %s.\" % ( cmdname , str ( arg ) ) ) else : errmsg ( 'Expecting an integer, got: %s.' % str ( arg ) ) pass raise Value Error return default", "predictions": ["get the value of a given action ."], "references": ["if arg is an int use that otherwise take default ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 2206, "code": "def run show int ( obj , what = None ) : val = obj . debugger . settings [ obj . name ] if not what : what = obj . name return obj . msg ( \"%s is %d.\" % ( what , val ) )", "predictions": ["runs the given object ."], "references": ["generic subcommand integer value display"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2207, "code": "def run show val ( obj , name ) : val = obj . debugger . settings [ obj . name ] obj . msg ( \"%s is %s.\" % ( obj . name , obj . cmd . proc . saferepr ( val ) , ) ) return False", "predictions": ["runs the given event with the given arguments ."], "references": ["generic subcommand value display"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2208, "code": "def is def stmt ( line , frame ) : return ( line and re def . match ( line ) and op at frame ( frame ) == 'LOAD CONST' and stmt contains opcode ( frame . f code , frame . f lineno , 'MAKE FUNCTION' ) )", "predictions": ["checks if the ( . init init init init init init init init init init init init init init init init init init . ( init init init init init init init init init init init ."], "references": ["return true if we are looking at a def statement"], "bleu": 0.03259631698411773, "rouge_l": 0.04747081712062257}
{"id": 2209, "code": "def is class def ( line , frame ) : return ( line and re class . match ( line ) and stmt contains opcode ( frame . f code , frame . f lineno , 'BUILD CLASS' ) )", "predictions": ["checks if this ) is a ( i . e . , a ( i . e . a ( i . e . a ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["return true if we are looking at a class definition statement"], "bleu": 0.026594139297659906, "rouge_l": 0.07411907654921021}
{"id": 2210, "code": "def nothread quit ( self , arg ) : self . debugger . core . stop ( ) self . debugger . core . execution status = 'Quit command' raise Mexcept . Debugger Quit", "predictions": ["1 . 2 . fds the union will be called before the union is submitted ."], "references": ["quit command when there s just one thread ."], "bleu": 0.07692375026049747, "rouge_l": 0.08425414364640883}
{"id": 2211, "code": "def threaded quit ( self , arg ) : threading list = threading . enumerate ( ) mythread = threading . current Thread ( ) for t in threading list : if t != mythread : ctype async raise ( t , Mexcept . Debugger Quit ) pass pass raise Mexcept . Debugger Quit", "predictions": ["creates a new builtin dependencies ."], "references": ["quit command when several threads are involved ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2212, "code": "def main ( dbg = None , sys argv = list ( sys . argv ) ) : global title orig sys argv = list ( sys argv ) opts , dbg opts , sys argv = process options ( title , version , sys argv ) dbg opts [ 'orig sys argv' ] = sys argv dbg opts [ 'interface' ] = Mbullwinkle . BW Interface ( ) dbg opts [ 'processor' ] = 'bullwinkle' if dbg is None : dbg = Mdebugger . Trepan ( dbg opts ) dbg . core . add ignore ( main ) pass postprocess options ( dbg , opts ) if len ( sys argv ) == 0 : mainpyfile = None else : mainpyfile = sys argv [ 0 ] if not os . path . isfile ( mainpyfile ) : mainpyfile = Mclifns . whence file ( mainpyfile ) is readable = Mfile . readable ( mainpyfile ) if is readable is None : print ( \"%s: Python script file '%s' does not exist\" % ( title , mainpyfile , ) ) sys . exit ( 1 ) elif not is readable : print ( \"%s: Can't read Python script file '%s'\" % ( title , mainpyfile , ) ) sys . exit ( 1 ) return mainpyfile noopt = Mfile . file pyc2py ( mainpyfile ) if mainpyfile != mainpyfile noopt and Mfile . readable ( mainpyfile noopt ) : print ( \"%s: Compiled Python script given and we can't use that.\" % title ) print ( \"%s: Substituting non-compiled name: %s\" % ( title , mainpyfile noopt , ) ) mainpyfile = mainpyfile noopt pass sys . path [ 0 ] = dbg . main dirname = os . path . dirname ( mainpyfile ) dbg . sig received = False while True : try : if dbg . program sys argv and mainpyfile : normal termination = dbg . run script ( mainpyfile ) if not normal termination : break else : dbg . core . execution status = 'No program' dbg . core . processor . process commands ( ) pass dbg . core . execution status = 'Terminated' dbg . intf [ - 1 ] . msg ( \"The program finished - quit or restart\" ) dbg . core . processor . process commands ( ) except Mexcept . Debugger Quit : break except Mexcept . Debugger Restart : dbg . core . execution status = 'Restart requested' if dbg . program sys argv : sys . argv = list ( dbg . program sys argv ) part1 = ( 'Restarting %s with arguments:' % dbg . core . filename ( mainpyfile ) ) args = ' ' . join ( dbg . program sys argv [ 1 : ] ) dbg . intf [ - 1 ] . msg ( Mmisc . wrapped lines ( part1 , args , dbg . settings [ 'width' ] ) ) else : break except System Exit : break pass sys . argv = orig sys argv return", "predictions": ["recursive recursive routine to retrieve an ( object ."], "references": ["routine which gets run if we were invoked directly"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2213, "code": "def signature ( frame ) : if not frame : return None code = frame . f code return ( code . co name , code . co filename , code . co firstlineno )", "predictions": ["get the format of this ( ( python ) ."], "references": ["return suitable frame signature to key display expressions off of ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 2214, "code": "def all ( self ) : found = False s = [ ] for display in self . list : if not found : s . append ( ) found = True pass s . append ( display . format ( ) ) return s", "predictions": ["simple implementation of format ."], "references": ["list all display items ; return 0 if none"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 2215, "code": "def delete index ( self , display number ) : old size = len ( self . list ) self . list = [ disp for disp in self . list if display number != disp . number ] return old size != len ( self . list )", "predictions": ["deletes an entry and its elements ."], "references": ["delete display expression * display_number *"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2216, "code": "def display ( self , frame ) : if not frame : return s = [ ] sig = signature ( frame ) for display in self . list : if display . signature == sig and display . enabled : s . append ( display . to s ( frame ) ) pass pass return s", "predictions": ["run a run of this rectangle ."], "references": ["display any items that are active"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2217, "code": "def debug ( frame = None ) : if frame is None : frame = frame ( ) . f back dbg = Remote Celery Trepan ( ) dbg . say ( BANNER . format ( self = dbg ) ) trepan . api . debug ( dbg opts = dbg . dbg opts )", "predictions": ["create a new collector ."], "references": ["set breakpoint at current location or a specified frame"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2218, "code": "def run ( self , args ) : if len ( args ) < 2 : self . section ( \"List of %s commands (with minimum abbreviation in \" \"parenthesis):\" % self . name ) for subcmd name in self . cmds . list ( ) : subcmd = self . cmds . subcmds [ subcmd name ] self . summary help ( subcmd name , subcmd ) pass return False subcmd prefix = args [ 1 ] subcmd = self . cmds . lookup ( subcmd prefix ) if subcmd : nargs = len ( args ) - 2 if nargs < subcmd . min args : self . errmsg ( ( \"Subcommand '%s %s' needs at least %d argument(s); \" + \"got %d.\" ) % ( self . name , subcmd . name , subcmd . min args , nargs ) ) return False if subcmd . max args is not None and nargs > subcmd . max args : self . errmsg ( ( \"Subcommand '%s %s' takes at most %d argument(s); \" + \"got %d.\" ) % ( self . name , subcmd . name , subcmd . max args , nargs ) ) return False return subcmd . run ( args [ 2 : ] ) else : return self . undefined subcmd ( self . name , subcmd prefix ) return", "predictions": ["borrowed from url with raw arguments ."], "references": ["ooops -- the debugger author didn t redefine this run docstring ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 2219, "code": "def undefined subcmd ( self , cmd , subcmd ) : self . proc . intf [ - 1 ] . errmsg ( ( 'Undefined \"%s\" subcommand: \"%s\". ' + 'Try \"help %s *\".' ) % ( cmd , subcmd , cmd ) ) return", "predictions": ["creates a new request for this object ."], "references": ["error message when subcommand asked for but doesn t exist"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 2220, "code": "def info signal ( self , args ) : if len ( args ) == 0 : return None signame = args [ 0 ] if signame in [ 'handle' , 'signal' ] : if len ( args ) == 1 : self . dbgr . core . processor . section ( self . header ) for signame in self . siglist : self . print info signal entry ( signame ) return True else : signame = args [ 1 ] pass pass signame = self . is name or number ( signame ) self . dbgr . core . processor . section ( self . header ) self . print info signal entry ( signame ) return True", "predictions": ["print a history call ."], "references": ["print information about a signal"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 2221, "code": "def handle print ( self , signame , set print ) : if set print : self . sigs [ signame ] . print method = self . dbgr . intf [ - 1 ] . msg else : self . sigs [ signame ] . print method = None pass return set print", "predictions": ["prints the integrate message ."], "references": ["set whether we print or not when this signal is caught ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 2222, "code": "def handle ( self , signum , frame ) : if self . print method : self . print method ( '\\n Program received signal %s.' % self . signame ) if self . print stack : import traceback strings = traceback . format stack ( frame ) for s in strings : if s [ - 1 ] == '\\n' : s = s [ 0 : - 1 ] self . print method ( s ) pass pass if self . b stop : core = self . dbgr . core old trace hook suspend = core . trace hook suspend core . trace hook suspend = True core . stop reason = ( 'intercepting signal %s (%d)' % ( self . signame , signum ) ) core . processor . event processor ( frame , 'signal' , signum ) core . trace hook suspend = old trace hook suspend pass if self . pass along : if self . old handler : self . old handler ( signum , frame ) pass pass return", "predictions": ["close all other sequence of given event handlers ."], "references": ["this method is called when a signal is received ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 2223, "code": "def file2module ( filename ) : basename = osp . basename ( filename ) if '.' in basename : pos = basename . rfind ( '.' ) return basename [ : pos ] else : return basename return None", "predictions": ["strips the name of the msg from the file ."], "references": ["given a file name extract the most likely module name ."], "bleu": 0.1434272783816789, "rouge_l": 0.28328173374613}
{"id": 2224, "code": "def print obj ( arg , frame , format = None , short = False ) : try : if not frame : obj = eval ( arg , None , None ) else : obj = eval ( arg , frame . f globals , frame . f locals ) pass except : return 'No symbol \"' + arg + '\" in current context.' what = arg if format : what = format + ' ' + arg obj = printf ( obj , format ) s = '%s = %s' % ( what , obj ) if not short : s += '\\ntype = %s' % type ( obj ) if callable ( obj ) : argspec = print argspec ( obj , arg ) if argspec : s += ':\\n\\t' if inspect . isclass ( obj ) : s += 'Class constructor information:\\n\\t' obj = obj . init elif isinstance ( obj , types . Instance Type ) : obj = obj . call pass s += argspec pass s = print dict ( s , obj , \"object variables\" ) if hasattr ( obj , \" class \" ) : s = print dict ( s , obj . class , \"class variables\" ) pass return s", "predictions": ["prints out the available start of a start or end ."], "references": ["return a string representation of an object"], "bleu": 0.12605968092174913, "rouge_l": 0.1157495256166983}
{"id": 2225, "code": "def pyfiles ( callername , level = 2 ) : d = os . path . dirname ( callername ) glob ( os . path . join ( d , '[a-z A-Z]*.py' ) ) py files = glob ( os . path . join ( d , '[a-z A-Z]*.py' ) ) return [ os . path . basename ( filename [ 0 : - 3 ] ) for filename in py files ]", "predictions": ["returns an iterator of all files of this directory and its relative path ."], "references": ["all python files caller s dir without the path and trailing . py"], "bleu": 0.11633270842295028, "rouge_l": 0.2982885085574572}
{"id": 2226, "code": "def populate cmd lists ( self ) : self . commands = { } for cmd instance in self . cmd instances : cmd name = cmd instance . name self . commands [ cmd name ] = cmd instance pass return", "predictions": ["eval some commands defined by the flash format ."], "references": ["populate self . commands"], "bleu": 0.15619699684601276, "rouge_l": 0.16531165311653115}
{"id": 2227, "code": "def run ( self , args ) : mainfile = self . core . filename ( None ) if self . core . is running ( ) : if mainfile : part1 = \"Python program '%s' is stopped\" % mainfile else : part1 = 'Program is stopped' pass if self . proc . event : msg = 'via a %s event.' % self . proc . event else : msg = '.' self . msg ( Mmisc . wrapped lines ( part1 , msg , self . settings [ 'width' ] ) ) if self . proc . curframe : self . msg ( \"PC offset is %d.\" % self . proc . curframe . f lasti ) if self . proc . event == 'return' : val = self . proc . event arg part1 = 'Return value is' self . msg ( Mmisc . wrapped lines ( part1 , self . proc . saferepr ( val ) , self . settings [ 'width' ] ) ) pass elif self . proc . event == 'exception' : exc type , exc value , exc tb = self . proc . event arg self . msg ( 'Exception type: %s' % self . proc . saferepr ( exc type ) ) if exc value : self . msg ( 'Exception value: %s' % self . proc . saferepr ( exc value ) ) pass pass self . msg ( 'It stopped %s.' % self . core . stop reason ) if self . proc . event in [ 'signal' , 'exception' , 'c exception' ] : self . msg ( 'Note: we are stopped *after* running the ' 'line shown.' ) pass else : if mainfile : part1 = \"Python program '%s'\" % mainfile msg = \"is not currently running. \" self . msg ( Mmisc . wrapped lines ( part1 , msg , self . settings [ 'width' ] ) ) else : self . msg ( 'No Python program is currently running.' ) pass self . msg ( self . core . execution status ) pass return False", "predictions": ["now we will print all ( ( so we can be started later on this process ."], "references": ["execution status of the program ."], "bleu": 0.07223943354597204, "rouge_l": 0.09516380655226209}
{"id": 2228, "code": "def columnize commands ( self , commands ) : commands . sort ( ) width = self . debugger . settings [ 'width' ] return columnize . columnize ( commands , displaywidth = width , lineprefix = '    ' )", "predictions": ["get the list of ( lookup prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix prefix ."], "references": ["list commands arranged in an aligned columns"], "bleu": 0.030901532587310255, "rouge_l": 0.04971475142624287}
{"id": 2229, "code": "def close ( self ) : self . state = 'closing' if self . inout : self . inout . close ( ) pass self . state = 'closing connection' if self . conn : self . conn . close ( ) self . state = 'disconnected' return", "predictions": ["closes this connection and releases any associated resources ."], "references": ["closes both socket and server connection ."], "bleu": 0.17747405280050263, "rouge_l": 0.38364779874213834}
{"id": 2230, "code": "def complete identifier ( cmd , prefix ) : if not cmd . proc . curframe : return [ None ] ns = cmd . proc . curframe . f globals . copy ( ) ns . update ( cmd . proc . curframe . f locals ) if '.' in prefix : dotted = prefix . split ( '.' ) try : obj = ns [ dotted [ 0 ] ] for part in dotted [ 1 : - 1 ] : obj = getattr ( obj , part ) except ( Key Error , Attribute Error ) : return [ ] pre prefix = '.' . join ( dotted [ : - 1 ] ) + '.' return [ pre prefix + n for n in dir ( obj ) if n . startswith ( dotted [ - 1 ] ) ] else : return Mcomplete . complete token ( ns . keys ( ) , prefix )", "predictions": ["completes one or more attribute definitions for the specified name and name ."], "references": ["complete an arbitrary expression ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 2231, "code": "def nothread quit ( self , arg ) : self . debugger . core . stop ( ) self . debugger . core . execution status = 'Quit command' self . proc . response [ 'event' ] = 'terminated' self . proc . response [ 'name' ] = 'status' self . proc . intf [ - 1 ] . msg ( self . proc . response ) raise Mexcept . Debugger Quit", "predictions": ["( re ) method for the help print process ."], "references": ["quit command when there s just one thread ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 2232, "code": "def is started ( self ) : return ( tracer . is started ( ) and not self . trace hook suspend and tracer . find hook ( self . trace dispatch ) )", "predictions": ["list of categories must be loaded on one or more selectors ."], "references": ["return true if debugging is in progress ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 2233, "code": "def set next ( self , frame , step ignore = 0 , step events = None ) : self . step events = None self . stop level = Mstack . count frames ( frame ) self . last level = self . stop level self . last frame = frame self . stop on finish = False self . step ignore = step ignore return", "predictions": ["sets the hardware 1 . 0 ."], "references": ["sets to stop on the next event that happens in frame frame ."], "bleu": 0.09374222649442905, "rouge_l": 0.2846034214618974}
{"id": 2234, "code": "def stack trace ( self , f ) : while f : if ( not self . core . ignore filter . is included ( f ) or self . settings [ 'dbg trepan' ] ) : s = Mstack . format stack entry ( self , ( f , f . f lineno ) ) self . msg ( \" \" * 4 + s ) pass f = f . f back pass return", "predictions": ["run the ( include : top , or right : core : left to 2 : left to 2 : top level : left + 2 : , 2 : left : top level : bottom = 2 : left + 2 : top level 2 : left = 2"], "references": ["a mini stack trace routine for threads ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 2235, "code": "def checkfuncname ( b , frame ) : if not b . funcname : if b . line != frame . f lineno : return False return True if frame . f code . co name != b . funcname : return False if not b . func first executable line : b . func first executable line = frame . f lineno if b . func first executable line != frame . f lineno : return False return True", "predictions": ["returns the line number for this function ."], "references": ["check whether we should break here because of b . funcname ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 2236, "code": "def delete breakpoint by number ( self , bpnum ) : success , msg , bp = self . get breakpoint ( bpnum ) if not success : return False , msg self . delete breakpoint ( bp ) return ( True , '' )", "predictions": ["deletes a ( or a ( specified ) ."], "references": ["remove a breakpoint given its breakpoint number ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2237, "code": "def en disable all breakpoints ( self , do enable = True ) : bp list = [ bp for bp in self . bpbynumber if bp ] bp nums = [ ] if do enable : endis = 'en' else : endis = 'dis' pass if not bp list : return \"No breakpoints to %sable\" % endis for bp in bp list : bp . enabled = do enable bp nums . append ( str ( bp . number ) ) pass return ( \"Breakpoints %sabled: %s\" % ( endis , \", \" . join ( bp nums ) ) )", "predictions": ["allow no available transformation for ( ."], "references": ["enable or disable all breakpoints ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2238, "code": "def en disable breakpoint by number ( self , bpnum , do enable = True ) : success , msg , bp = self . get breakpoint ( bpnum ) if not success : return success , msg if do enable : endis = 'en' else : endis = 'dis' pass if bp . enabled == do enable : return ( False , ( 'Breakpoint (%r) previously %sabled' % ( str ( bpnum ) , endis , ) ) ) bp . enabled = do enable return ( True , '' )", "predictions": ["show an error message for a given int ."], "references": ["enable or disable a breakpoint given its breakpoint number ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 2239, "code": "def open ( self , inp , opts = None ) : if isinstance ( inp , io . Text IO Wrapper ) : self . input = inp elif isinstance ( inp , 'string' . class ) : self . name = inp self . input = open ( inp , 'r' ) else : raise IO Error ( \"Invalid input type (%s) for %s\" % ( inp . class . name , inp ) ) return", "predictions": ["opens a stream and opens the underlying character stream ."], "references": ["use this to set what file to read from ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 2240, "code": "def restore original login ( request ) : original session = request . session . get ( la settings . USER SESSION FLAG ) logout ( request ) if not original session : return try : original user pk = signer . unsign ( original session , max age = timedelta ( days = la settings . USER SESSION DAYS TIMESTAMP ) . total seconds ( ) ) user = get user model ( ) . objects . get ( pk = original user pk ) messages . info ( request , la settings . MESSAGE LOGIN REVERT . format ( username = user . dict [ username field ] ) , extra tags = la settings . MESSAGE EXTRA TAGS , ) login as ( user , request , store original user = False ) if la settings . USER SESSION FLAG in request . session : del request . session [ la settings . USER SESSION FLAG ] except Signature Expired : pass", "predictions": ["restores the extra extra extra extra messages that the user has completed ."], "references": ["restore an original login session checking the signed session"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 2241, "code": "def load module ( path ) : i = path . rfind ( \".\" ) module , attr = path [ : i ] , path [ i + 1 : ] try : mod = import module ( module ) except Import Error : raise Improperly Configured ( \"Error importing CAN LOGIN AS function: {}\" . format ( module ) ) except Value Error : raise Improperly Configured ( \"Error importing CAN LOGIN AS\" \" function. Is CAN LOGIN AS a\" \" string?\" ) try : can login as = getattr ( mod , attr ) except Attribute Error : raise Improperly Configured ( \"Module {0} does not define a {1} \" \"function.\" . format ( module , attr ) ) return can login as", "predictions": ["load the given module from the given path ."], "references": ["code to load create user module . copied off django - browserid ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 2242, "code": "def main ( argv ) : parser = argparse . Argument Parser ( description = DESCRIPTION , formatter class = argparse . Raw Description Help Formatter ) parser . add argument ( '-b' , '--base-url' , default = URL BASE , help = 'API root url, default: %s' % URL BASE , ) parser . add argument ( '-e' , '--expanded' , help = \"Include Luminoso's analysis of each document, such as terms and\" ' document vectors' , action = 'store true' , ) parser . add argument ( '-t' , '--token' , help = 'API authentication token' ) parser . add argument ( '-s' , '--save-token' , action = 'store true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) parser . add argument ( 'project id' , help = 'The ID of the project in the Daylight API' ) parser . add argument ( 'output file' , nargs = '?' , default = None , help = 'The JSON lines (.jsons) file to write to' ) args = parser . parse args ( argv ) if args . save token : if not args . token : raise Value Error ( \"error: no token provided\" ) Luminoso Client . save token ( args . token , domain = urlparse ( args . base url ) . netloc ) client = Luminoso Client . connect ( url = args . base url , token = args . token ) proj client = client . client for path ( 'projects/{}' . format ( args . project id ) ) download docs ( proj client , args . output file , args . expanded )", "predictions": ["implements the ( method to do the actual work of the ( ."], "references": ["handle arguments for the lumi - download command ."], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 2243, "code": "def stream json lines ( file ) : if isinstance ( file , string type ) : file = open ( file , 'rb' ) for line in file : line = line . strip ( ) if line : if isinstance ( line , bytes ) : line = line . decode ( 'utf-8' ) yield json . loads ( line )", "predictions": ["this generator parses the lines of a stream ."], "references": ["load a json stream and return a generator yielding one object at a time ."], "bleu": 0.09111821689187218, "rouge_l": 0.23921568627450981}
{"id": 2244, "code": "def get default account ( self ) : newclient = self . class ( self . session , self . root url ) account info = newclient . get ( '/accounts/' ) if account info [ 'default account' ] is not None : return account info [ 'default account' ] valid accounts = [ a [ 'account id' ] for a in account info [ 'accounts' ] if a [ 'account id' ] != 'public' ] if len ( valid accounts ) == 0 : raise Value Error ( \"Can't determine your default URL. \" \"Please request a specific URL or ask \" \"Luminoso for support.\" ) return valid accounts [ 0 ]", "predictions": ["get default account account account ."], "references": ["get the id of an account you can use to access projects ."], "bleu": 0.08180282100568384, "rouge_l": 0.29611650485436897}
{"id": 2245, "code": "def documentation ( self ) : newclient = self . class ( self . session , self . root url ) return newclient . get raw ( '/' )", "predictions": ["get the documentation to the documentation ."], "references": ["get the documentation that the server sends for the api ."], "bleu": 0.21606281467072083, "rouge_l": 0.5341506129597198}
{"id": 2246, "code": "def print csv ( result ) : if type ( result ) is not list : raise Type Error ( \"output not able to be displayed as CSV.\" ) first line = result [ 0 ] w = csv . Dict Writer ( sys . stdout , fieldnames = sorted ( first line . keys ( ) ) ) w . writeheader ( ) for line in result : w . writerow ( line )", "predictions": ["prints the stack trace of the provided csv file ."], "references": ["print a json list of json objects in csv format ."], "bleu": 0.13564514503163538, "rouge_l": 0.28328173374613}
{"id": 2247, "code": "def read params ( input file , json body , p params ) : params = { } try : if input file : params . update ( json . load ( input file ) ) if json body is not None : params . update ( json . loads ( json body ) ) except Value Error as e : raise Value Error ( \"input is not valid JSON: %s\" % e ) try : params . update ( { p . split ( '=' , 1 ) [ 0 ] : p . split ( '=' , 1 ) [ 1 ] for p in p params } ) except Index Error : raise Value Error ( \"--param arguments must have key=value format\" ) return params", "predictions": ["reads input from a plot ."], "references": ["read parameters from input file - j and - p arguments in that order ."], "bleu": 0.05861428254383573, "rouge_l": 0.17681159420289855}
{"id": 2248, "code": "def batches ( iterable , size ) : sourceiter = iter ( iterable ) while True : try : batchiter = islice ( sourceiter , size ) yield chain ( [ next ( batchiter ) ] , batchiter ) except Stop Iteration : return", "predictions": ["iterates over all batches of the given iterable ."], "references": ["take an iterator and yield its contents in groups of size items ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 2249, "code": "def simplify doc ( doc ) : doc = dict ( doc ) if 'text' not in doc : raise Value Error ( \"The document {!r} has no text field\" . format ( doc ) ) return { 'text' : doc [ 'text' ] , 'metadata' : doc . get ( 'metadata' , [ ] ) , 'title' : doc . get ( 'title' , '' ) }", "predictions": ["simplify documentation document . this must be called before any other info have been set ."], "references": ["limit a document to just the three fields we should upload ."], "bleu": 0.08513012360883544, "rouge_l": 0.14663461538461536}
{"id": 2250, "code": "def create project with docs ( client , docs , language , name , account = None , progress = False ) : description = 'Uploaded using lumi-upload at {}' . format ( time . asctime ( ) ) if account is not None : proj record = client . post ( 'projects' , name = name , language = language , description = description , account id = account , ) else : proj record = client . post ( 'projects' , name = name , language = language , description = description ) proj id = proj record [ 'project id' ] proj client = client . client for path ( 'projects/' + proj id ) try : if progress : progress bar = tqdm ( desc = 'Uploading documents' ) else : progress bar = None for batch in batches ( docs , BATCH SIZE ) : docs to upload = [ simplify doc ( doc ) for doc in batch ] proj client . post ( 'upload' , docs = docs to upload ) if progress : progress bar . update ( BATCH SIZE ) finally : if progress : progress bar . close ( ) print ( 'The server is building project {!r}.' . format ( proj id ) ) proj client . post ( 'build' ) while True : time . sleep ( 10 ) proj status = proj client . get ( ) build info = proj status [ 'last build info' ] if 'success' in build info : if not build info [ 'success' ] : raise Luminoso Server Error ( build info [ 'reason' ] ) return proj status", "predictions": ["create a project with the given parameters"], "references": ["given an iterator of documents upload them as a luminoso project ."], "bleu": 0.1081377510275021, "rouge_l": 0.20098846787479407}
{"id": 2251, "code": "def main ( argv ) : parser = argparse . Argument Parser ( description = DESCRIPTION , formatter class = argparse . Raw Description Help Formatter , ) parser . add argument ( '-b' , '--base-url' , default = URL BASE , help = 'API root url, default: %s' % URL BASE , ) parser . add argument ( '-a' , '--account-id' , default = None , help = 'Account ID that should own the project, if not the default' , ) parser . add argument ( '-l' , '--language' , default = 'en' , help = 'The language code for the language the text is in. Default: en' , ) parser . add argument ( '-t' , '--token' , help = \"API authentication token\" ) parser . add argument ( '-s' , '--save-token' , action = 'store true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) parser . add argument ( 'input filename' , help = 'The JSON-lines (.jsons) file of documents to upload' , ) parser . add argument ( 'project name' , nargs = '?' , default = None , help = 'What the project should be called' , ) args = parser . parse args ( argv ) if args . save token : if not args . token : raise Value Error ( \"error: no token provided\" ) Luminoso Client . save token ( args . token , domain = urlparse ( args . base url ) . netloc ) client = Luminoso Client . connect ( url = args . base url , token = args . token ) name = args . project name if name is None : name = input ( 'Enter a name for the project: ' ) if not name : print ( 'Aborting because no name was provided.' ) return result = upload docs ( client , args . input filename , args . language , name , account = args . account id , progress = True , ) print ( 'Project {!r} created with {} documents' . format ( result [ 'project id' ] , result [ 'document count' ] ) )", "predictions": ["creates a new ansible script to upload the user ' s upload data ."], "references": ["handle arguments for the lumi - upload command ."], "bleu": 0.10511846841633776, "rouge_l": 0.271513353115727}
{"id": 2252, "code": "def batches ( iterable , size ) : sourceiter = iter ( iterable ) while True : batchiter = islice ( sourceiter , size ) yield chain ( [ next ( batchiter ) ] , batchiter )", "predictions": ["generate an iterator of batches of the given iterable"], "references": ["take an iterator and yield its contents in groups of size items ."], "bleu": 0.1279808802469055, "rouge_l": 0.26406926406926406}
{"id": 2253, "code": "def post login page ( self ) : data = { 'ID Token1' : self . username , 'ID Token2' : self . password , 'Sun Query Params String' : base64 . b64encode ( b'realm=particuliers' ) , 'encoded' : 'true' , 'gx charset' : 'UTF-8' } try : self . session . post ( LOGIN URL , data = data , allow redirects = False , timeout = self . timeout ) except OS Error : raise Py Linky Error ( \"Can not submit login form\" ) if 'i Planet Directory Pro' not in self . session . cookies : raise Py Linky Error ( \"Login error: Please check your username/password.\" ) return True", "predictions": [". a page to the user ."], "references": ["login to enedis ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 2254, "code": "def fetch data ( self ) : for t in [ HOURLY , DAILY , MONTHLY , YEARLY ] : self . data [ t ] = self . get data per period ( t )", "predictions": ["fetch stats for this object ."], "references": ["get the latest data from enedis ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2255, "code": "def prepare ( self ) : if self . class . view : return #: Load the View class from the dotted view name with enaml . imports ( ) : View = pydoc . locate ( self . page . view ) assert View , \"Failed to import View: {}\" . format ( self . page . view ) #: Set initial view properties self . class . view = View ( site = self . site , page = self . page , request = self . request , )", "predictions": ["prepares and signals a view for . ."], "references": ["load the view on first load"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2256, "code": "def initialize ( self ) : if self . class . view : self . view . handler = self self . view . request = self . request return #: Load the View class from the dotted view name with enaml . imports ( ) : from views . index import View #: Set initial view properties self . class . view = View ( company = current company , request = self . request , handler = self , )", "predictions": ["initializes the resource storage object ."], "references": ["load the view on first load could also load based on session group etc .."], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 2257, "code": "def get ( self , * args , * * kwargs ) : #: Render view for get request, view is cached for websocket if self . is websocket ( ) : return super ( Demo Handler , self ) . get ( * args , * * kwargs ) else : #return tornado.web.Request Handler.get(self, *args, **kwargs) self . write ( self . view . render ( ) )", "predictions": ["call this function to render the changes in the view ."], "references": ["execute the correct handler depending on what is connecting ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 2258, "code": "def on message ( self , message ) : #: Decode message change = tornado . escape . json decode ( message ) #print change #: Get the owner ID ref = change . get ( 'ref' ) if not ref : return #: Get the server side representation of the node #: If found will return the View declaration node node = self . view . xpath ( '//*[@ref=\"{}\"]' . format ( ref ) , first = True ) if node is None : return #: Handle the event if change . get ( 'type' ) and change . get ( 'name' ) : if change [ 'type' ] == 'event' : #: Trigger the event trigger = getattr ( node , change [ 'name' ] ) trigger ( ) if change [ 'type' ] == 'update' : #: Trigger the update setattr ( node , change [ 'name' ] , change [ 'value' ] )", "predictions": ["generate a message suitable for handling a message on this message ."], "references": ["when enaml . js sends a message"], "bleu": 0.14694106251955755, "rouge_l": 0.22101449275362317}
{"id": 2259, "code": "def update menus ( self , change ) : menus = { } #: Get all links links = [ p . link for p in self . pages if p . link ] + self . links #: Put all links in the correct menu for link in links : for menu in link . menus : if menu not in menus : menus [ menu ] = [ ] menus [ menu ] . append ( link ) #: Update the menus for name , menu in menus . items ( ) : k = '{} menu' . format ( name ) if hasattr ( self , k ) : setattr ( self , k , menu )", "predictions": ["this method is called when the link has been created ."], "references": ["when pages change update the menus"], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 2260, "code": "def default handlers ( self ) : static path = os . path . abspath ( os . path . join ( os . path . dirname ( file ) , \"static\" ) ) urls = [ ( r\"/static/(.*)\" , cyclone . web . Static File Handler , { \"path\" : static path } ) , ] for p in self . pages : handler = p . handler handler . site = self handler . page = p urls . append ( ( p . link . url , handler ) ) return urls", "predictions": ["returns a a a tree containing all handlers of this directory ."], "references": ["generate the handlers for this site"], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 2261, "code": "def set attribute ( self , name , value ) : if value is True : self . widget . set ( name , name ) elif value is False : del self . widget . attrib [ name ] else : self . widget . set ( name , str ( value ) )", "predictions": ["sets the attribute and value for this attribute ."], "references": ["default handler for those not explicitly defined"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2262, "code": "def xpath ( self , query , * * kwargs ) : nodes = self . proxy . find ( query , * * kwargs ) return [ n . declaration for n in nodes ]", "predictions": ["get the proxy at the given query ."], "references": ["find nodes matching the given xpath query"], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 2263, "code": "def init widget ( self ) : d = self . declaration if d . source : self . set source ( d . source ) else : super ( Raw Component , self ) . init widget ( )", "predictions": ["initialize the widget . this is called only once for subclasses that override this method ."], "references": ["initialize the widget with the source ."], "bleu": 0.1513851459876605, "rouge_l": 0.3742331288343558}
{"id": 2264, "code": "def observe mode ( self , change ) : block = self . block if block and self . is initialized and change [ 'type' ] == 'update' : if change [ 'oldvalue' ] == 'replace' : raise Not Implemented Error for c in self . children : block . children . remove ( c ) c . set parent ( None ) self . refresh items ( )", "predictions": ["possibly blocks the last request ."], "references": ["if the mode changes . refresh the items ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2265, "code": "def read ( * pathcomponents ) : with open ( join ( abspath ( dirname ( file ) ) , * pathcomponents ) ) as thefile : return thefile . read ( )", "predictions": ["read ( . return null if there are no more characters ."], "references": ["read the contents of a file located relative to setup . py"], "bleu": 0.11498759556447223, "rouge_l": 0.16666666666666666}
{"id": 2266, "code": "def error ( msg , exit code ) : sys . stderr . write ( \"%s\\ntry 'mongotail --help' for more information\\n\" % msg ) sys . stderr . flush ( ) exit ( exit code )", "predictions": ["prints error message and then flushes the same status ."], "references": ["print msg error and exit with status exit_code"], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 2267, "code": "def error parsing ( msg = \"unknown options\" ) : sys . stderr . write ( \"Error parsing command line: %s\\ntry 'mongotail --help' for more information\\n\" % msg ) sys . stderr . flush ( ) exit ( EINVAL )", "predictions": ["prints an error message to stderr ."], "references": ["print any parsing error and exit with status - 1"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2268, "code": "def draw ( self ) : self . screen . border ( 0 ) if self . title is not None : self . screen . addstr ( 2 , 2 , self . title , curses . A STANDOUT ) if self . subtitle is not None : self . screen . addstr ( 4 , 2 , self . subtitle , curses . A BOLD ) for index , item in enumerate ( self . items ) : if self . current option == index : text style = self . highlight else : text style = self . normal self . screen . addstr ( 5 + index , 4 , item . show ( index ) , text style ) screen rows , screen cols = Curses Menu . stdscr . getmaxyx ( ) top row = 0 if 6 + len ( self . items ) > screen rows : if screen rows + self . current option < 6 + len ( self . items ) : top row = self . current option else : top row = 6 + len ( self . items ) - screen rows self . screen . refresh ( top row , 0 , 0 , 0 , screen rows - 1 , screen cols - 1 )", "predictions": ["draw the items in a group ."], "references": ["redraws the menu and refreshes the screen . should be called whenever something changes that needs to be redrawn ."], "bleu": 0.032092688317097204, "rouge_l": 0.1363128491620112}
{"id": 2269, "code": "def process user input ( self ) : user input = self . get input ( ) go to max = ord ( \"9\" ) if len ( self . items ) >= 9 else ord ( str ( len ( self . items ) ) ) if ord ( '1' ) <= user input <= go to max : self . go to ( user input - ord ( '0' ) - 1 ) elif user input == curses . KEY DOWN : self . go down ( ) elif user input == curses . KEY UP : self . go up ( ) elif user input == ord ( \"\\n\" ) : self . select ( ) return user input", "predictions": ["process a user for processing ."], "references": ["gets the next single character and decides what to do with it"], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 2270, "code": "def select ( self ) : self . selected option = self . current option self . selected item . set up ( ) self . selected item . action ( ) self . selected item . clean up ( ) self . returned value = self . selected item . get return ( ) self . should exit = self . selected item . should exit if not self . should exit : self . draw ( )", "predictions": ["a method to select the action that is required to run the action as a operation ."], "references": ["select the current item and run it"], "bleu": 0.10216198665886358, "rouge_l": 0.27031019202363366}
{"id": 2271, "code": "def show ( self , index ) : if self . menu and self . menu . parent : self . text = \"Return to %s menu\" % self . menu . parent . title else : self . text = \"Exit\" return super ( Exit Item , self ) . show ( index )", "predictions": ["displays the tree visible in a box ."], "references": ["this class overrides this method"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2272, "code": "def action ( self ) : self . return value = self . function ( * self . args , * * self . kwargs )", "predictions": ["measure the multi - level restore of a operator ."], "references": ["this class overrides this method"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2273, "code": "def action ( self ) : commandline = \"{0} {1}\" . format ( self . command , \" \" . join ( self . arguments ) ) try : completed process = subprocess . run ( commandline , shell = True ) self . exit status = completed process . returncode except Attribute Error : self . exit status = subprocess . call ( commandline , shell = True )", "predictions": ["this method is called to run the server . the load method is called from the server ."], "references": ["this class overrides this method"], "bleu": 0.08961672320242714, "rouge_l": 0.19365079365079363}
{"id": 2274, "code": "def set up ( self ) : self . menu . pause ( ) curses . def prog mode ( ) self . menu . clear screen ( )", "predictions": ["sets the default , removes all items from the list ."], "references": ["this class overrides this method"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2275, "code": "def clean up ( self ) : self . submenu . join ( ) self . menu . clear screen ( ) curses . reset prog mode ( ) curses . curs set ( 1 ) curses . curs set ( 0 ) self . menu . resume ( )", "predictions": ["creates default generated by the uid and removes all child , and removes all child variables ."], "references": ["this class overrides this method"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 2276, "code": "def add ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'add' )", "predictions": ["adds a session to the strategy ."], "references": ["deprecated - use formula instead"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2277, "code": "def subtract ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'sub' )", "predictions": ["documentation for the given column ."], "references": ["deprecated - use formula instead"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2278, "code": "def multiply ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'mul' )", "predictions": ["creates a raise an error if it has not been computed ."], "references": ["deprecated - use formula instead"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 2279, "code": "def divide ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'truediv' )", "predictions": ["read a new dataset ."], "references": ["deprecated - use formula instead"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2280, "code": "def cumsum ( df , new column : str , column : str , index : list , date column : str , date format : str ) : logging . get Logger ( name ) . warning ( f\"DEPRECATED: use compute cumsum\" ) date temp = ' date temp ' if isinstance ( index , str ) : index = [ index ] levels = list ( range ( 0 , len ( index ) ) ) df [ date temp ] = pd . to datetime ( df [ date column ] , format = date format ) reference cols = [ date temp , date column ] df = df . groupby ( index + reference cols ) . sum ( ) df [ new column ] = df . groupby ( level = levels ) [ column ] . cumsum ( ) df . reset index ( inplace = True ) del df [ date temp ] return df", "predictions": ["batches a batches and two functions ."], "references": ["deprecated - please use compute_cumsum instead"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2281, "code": "def log message ( logger , message = \"\" ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : log message ( logger , func . name , message ) result = func ( * args , * * kwargs ) return result return wrapper return decorator", "predictions": ["simplify a not handled by the simplify api call ."], "references": ["decorator to log a message before executing a function"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 2282, "code": "def log time ( logger ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : start = time . time ( ) result = func ( * args , * * kwargs ) end = time . time ( ) log time ( logger , func . name , start , end ) return result return wrapper return decorator", "predictions": ["create a , or optionally create a , depending on the result of the project ."], "references": ["decorator to log the execution time of a function"], "bleu": 0.09147827112247602, "rouge_l": 0.16850828729281767}
{"id": 2283, "code": "def clean cachedir old entries ( cachedir : Store Backend Base , func name : str , limit : int ) -> int : if limit < 1 : raise Value Error ( \"'limit' must be greater or equal to 1\" ) cache entries = get cachedir entries ( cachedir , func name ) cache entries = sorted ( cache entries , key = lambda e : e . last access , reverse = True ) cache entries to remove = cache entries [ limit : ] for entry in cache entries to remove : shutil . rmtree ( entry . path , ignore errors = True ) return len ( cache entries to remove )", "predictions": ["checks if all states have been removed from the default: set ."], "references": ["remove old entries from the cache"], "bleu": 0.1367440667823257, "rouge_l": 0.2364341085271318}
{"id": 2284, "code": "def ada family core ( params , gparams , learning rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = \"ADADELTA\" , beta = 0.0 , gsum regularization = 0.0001 ) : , , , args = inspect . getargvalues ( inspect . currentframe ( ) ) logging . info ( \"ada family core: %s\" % str ( args . items ( ) ) ) free parameters = [ ] if method == \"FINETUNING ADAGRAD\" : method = \"ADAGRAD\" gsum regularization = 0 one Minus Beta = 1 - beta gsums = [ theano . shared ( np . zeros like ( param . get value ( borrow = True ) , dtype = FLOATX ) , name = \"gsum %s\" % param . name ) if ( method == 'ADADELTA' or method == 'ADAGRAD' ) else None for param in params ] xsums = [ theano . shared ( np . zeros like ( param . get value ( borrow = True ) , dtype = FLOATX ) , name = \"xsum %s\" % param . name ) if method == 'ADADELTA' else None for param in params ] if method == 'ADAGRAD' : for gsum in gsums : gsum . set value ( gsum . get value ( ) ** 0 ) updates = Ordered Dict ( ) for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : if method == 'ADADELTA' : updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2 ) dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2 ) updates [ param ] = param * one Minus Beta + dparam elif method == 'ADAGRAD' : updates [ gsum ] = gsum + ( gparam ** 2 ) - gsum regularization * gsum updates [ param ] = param * one Minus Beta - learning rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) else : updates [ param ] = param * one Minus Beta - gparam * learning rate if method == 'ADADELTA' : free parameters . extend ( gsums + xsums ) elif method == 'ADAGRAD' : free parameters . extend ( gsums ) for k in updates : if updates [ k ] . dtype != FLOATX : updates [ k ] = updates [ k ] . astype ( FLOATX ) return updates . items ( ) , free parameters", "predictions": ["make the iterable of updates updates ."], "references": ["optimize by sgd adagrad or adadelta ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2285, "code": "def learning updates ( self ) : params = self . training params ( ) gradients = self . get gradients ( params ) return self . optimization updates ( params , gradients )", "predictions": ["returns the post login ."], "references": ["return updates in the training ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2286, "code": "def training params ( self ) : params = self . network . parameters if self . config . fixed parameters : logging . info ( \"fixed parameters: %s\" % \", \" . join ( map ( str , self . config . fixed parameters ) ) ) params = [ p for p in params if p not in self . config . fixed parameters ] return params", "predictions": ["for a set of [ smart_unicode , i ] ."], "references": ["get parameters to be optimized ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2287, "code": "def optimization updates ( self , params , gradients ) : updates , free parameters = optimize updates ( params , gradients , self . config ) self . network . free parameters . extend ( free parameters ) logging . info ( \"Added %d free parameters for optimization\" % len ( free parameters ) ) return updates", "predictions": ["generate a prepare to start a new ( return the return parameters class class class class class class class class class class class class class class class class class class class class class class class class class class class class class class class class class class ."], "references": ["return updates from optimization ."], "bleu": 0.028310852916881273, "rouge_l": 0.09003690036900368}
{"id": 2288, "code": "def first glimpse sensor ( self , x t ) : downsampled img = theano . tensor . signal . downsample . max pool 2d ( x t , ( 4 , 4 ) ) downsampled img = downsampled img . flatten ( ) first l = T . dot ( downsampled img , self . W f ) if self . disable reinforce : wf grad = self . W f if self . random glimpse : first l = self . srng . uniform ( ( 2 , ) , low = - 1.7 , high = 1.7 ) else : sampled l t = self . sample gaussian ( first l , self . cov ) sampled pdf = self . multi gaussian pdf ( disconnected grad ( sampled l t ) , first l ) wf grad = T . grad ( T . log ( sampled pdf ) , self . W f ) first l = sampled l t return first l , wf grad", "predictions": ["this is a method that returns a theano estimate for this class . this will not return the initialize estimate of the theano ."], "references": ["compute first glimpse position using down - sampled image ."], "bleu": 0.050661968099322066, "rouge_l": 0.06354166666666666}
{"id": 2289, "code": "def prepare ( self ) : self . output dim = 10 self . encoder = Chain ( self . input dim ) . stack ( Dense ( self . internal layer size , 'tanh' ) ) self . decoder = Chain ( self . internal layer size ) . stack ( Dense ( self . input dim ) ) self . classifier = Chain ( self . internal layer size ) . stack ( Dense ( 50 , 'tanh' ) , Dense ( self . output dim ) , Softmax ( ) ) self . register inner layers ( self . encoder , self . decoder , self . classifier ) self . target input = T . ivector ( 'target' ) self . register external inputs ( self . target input )", "predictions": ["creates and adds appropriate action to . ."], "references": ["all codes that create parameters should be put into setup function ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 2290, "code": "def compute tensor ( self , x ) : internal variable = self . encoder . compute tensor ( x ) decoding output = self . decoder . compute tensor ( internal variable ) classification output = self . classifier . compute tensor ( internal variable ) auto encoder cost = Auto Encoder Cost ( decoding output , x ) . get ( ) classification cost = Cross Entropy Cost ( classification output , self . target input ) . get ( ) final cost = 0.01 * auto encoder cost + classification cost error rate = Error Rate Cost ( classification output , self . target input ) . get ( ) self . register monitors ( ( \"err\" , error rate ) , ( \"encoder cost\" , auto encoder cost ) , ( \"classify cost\" , classification cost ) ) return final cost", "predictions": ["computes the message for a message ."], "references": ["build the computation graph here ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2291, "code": "def vectorize target ( self , size ) : if self . train set : self . train set = self . vectorize set ( self . train set , size ) if self . valid set : self . valid set = self . vectorize set ( self . valid set , size ) if self . test set : self . test set = self . vectorize set ( self . test set , size )", "predictions": ["run a series of ( ."], "references": ["make targets be one - hot vectors ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2292, "code": "def report ( self ) : logging . info ( \"%s train=%d valid=%d test=%d\" % ( self . class . name , len ( list ( self . train set ) ) if self . train set else 0 , len ( list ( self . valid set ) ) if self . valid set else 0 , len ( list ( self . test set ) ) if self . test set else 0 ) )", "predictions": ["default default default power ."], "references": ["print dataset statistics ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2293, "code": "def train ( self , train set , valid set = None , test set = None , train size = None ) : iteration = 0 while True : if not iteration % self . config . test frequency and test set : try : self . test ( iteration , test set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if not iteration % self . validation frequency and valid set : try : if not self . evaluate ( iteration , valid set ) : logging . info ( 'patience elapsed, bailing out' ) break except Keyboard Interrupt : logging . info ( 'interrupted!' ) break train message = \"\" try : train message = self . train func ( train set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if not iteration % self . config . monitor frequency : logging . info ( 'monitor (iter=%i) %s' , iteration + 1 , train message ) iteration += 1 if hasattr ( self . network , \"iteration callback\" ) : self . network . iteration callback ( ) yield train message if valid set : self . set params ( self . best params ) if test set : self . test ( 0 , test set )", "predictions": ["add_positions attribute on the set of ( ."], "references": ["we train over mini - batches and evaluate periodically ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 2294, "code": "def sample ( self , input , steps ) : inputs = [ [ onehot ( self . input dim , x ) for x in input ] ] for in range ( steps ) : target = self . compute ( inputs ) [ 0 , - 1 ] . argmax ( ) input . append ( target ) inputs [ 0 ] . append ( onehot ( self . input dim , target ) ) return input", "predictions": ["samples this unit out of the given unit ."], "references": ["sample outputs from lm ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2295, "code": "def compute alignments ( self , prev state , precomputed values , mask = None ) : Wa Sp = T . dot ( prev state , self . Wa ) Ua H = precomputed values if Ua H . ndim == 2 : preact = Wa Sp [ : , None , : ] + Ua H [ None , : , : ] else : preact = Wa Sp [ : , None , : ] + Ua H act = T . activate ( preact , 'tanh' ) align scores = T . dot ( act , self . Va ) if mask : mask = ( 1 - mask ) * - 99.00 if align scores . ndim == 3 : align scores += mask [ None , : ] else : align scores += mask align weights = T . nnet . softmax ( align scores ) return align weights", "predictions": ["compute the widget of each d ."], "references": ["compute the alignment weights based on the previous state ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 2296, "code": "def compute context vector ( self , prev state , inputs , precomputed values = None , mask = None ) : precomputed values = precomputed values if precomputed values else self . precompute ( inputs ) align weights = self . compute alignments ( prev state , precomputed values , mask ) context vector = T . sum ( align weights [ : , : , None ] * inputs , axis = 1 ) return context vector", "predictions": ["compute the sum of a ( ( block initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized initialized by the :"], "references": ["compute the context vector with soft attention ."], "bleu": 0.0513487742994337, "rouge_l": 0.1147695202257761}
{"id": 2297, "code": "def concatenate ( vars , axis = - 1 ) : from deepy . core . neural var import Neural Variable if isinstance ( vars [ 0 ] , Neural Variable ) : concat var = Concatenate ( axis = axis ) . compute ( * vars ) if axis == - 1 or axis == vars [ 0 ] . tensor . ndim - 1 : concat var . output dim = sum ( [ x . output dim for x in vars ] , 0 ) else : concat var = TT . concatenate ( vars , axis ) return concat var", "predictions": ["concatenates variables along the variable * ) ."], "references": ["a utility function of concatenate ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2298, "code": "def pad ( self , side , length ) : if self . train set : self . train set = pad dataset ( self . train set , side , length ) if self . valid set : self . valid set = pad dataset ( self . valid set , side , length ) if self . test set : self . test set = pad dataset ( self . test set , side , length )", "predictions": ["adds a code to the left of the given dataset ."], "references": ["pad sequences to given length in the left or right side ."], "bleu": 0.16278331364721524, "rouge_l": 0.34512022630834516}
{"id": 2299, "code": "def rmsprop core ( params , gradients , momentum = 0.9 , learning rate = 0.01 ) : for param , grad in zip ( params , gradients ) : rms = theano . shared ( np . zeros like ( param . get value ( ) ) , name = param . name + ' rms' ) rms = momentum * rms + ( 1 - momentum ) * grad * grad yield rms , rms yield param , param - learning rate * grad / T . sqrt ( rms + 1e-8 )", "predictions": ["generator that can be used to compute all ( values for the same thread ."], "references": ["rmsprop optimization core ."], "bleu": 0.08225964699966554, "rouge_l": 0.11753371868978806}
{"id": 2300, "code": "def report ( self ) : if not self . end time : self . end ( ) print ( \"Time: {} mins\" . format ( ( self . end time - self . start time ) / 60 ) )", "predictions": ["a convenience method that returns the amount of time ."], "references": ["report elapsed time ."], "bleu": 0.16590387014219712, "rouge_l": 0.3096446700507614}
{"id": 2301, "code": "def run ( self , data x ) : output vars = self . compute ( * data x ) return self . extract costs ( output vars )", "predictions": ["process the independent independent coroutine ."], "references": ["run the model with validation data and return costs ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 2302, "code": "def invoke ( self ) : self . counter += 1 if self . counter % self . freq == 0 : cnt = 0. sum map = defaultdict ( float ) for x in self . trainer . get data ( self . data split ) : val map = self . run ( x ) if not isinstance ( val map , dict ) : raise Exception ( \"Monitor.run must return a dict.\" ) for k , val in val map . items ( ) : sum map [ k ] += val cnt += 1 for k in sum map : sum map [ k ] /= cnt new best = self . compare ( sum map ) self . trainer . report ( sum map , self . data split , new best = new best ) if new best : self . trainer . save checkpoint ( self . save path )", "predictions": ["select and returns a trainer problem ."], "references": ["this function will be called after each iteration ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2303, "code": "def build loop vars ( self ) : from theano . tensor . var import Tensor Variable from deepy . core . neural var import Neural Variable if not self . loop vars : self . ordered out keys = self . outputs . keys ( ) seq keys = self . sequences . keys ( ) filled out keys = [ k for k in self . ordered out keys if self . outputs [ k ] ] nonseq keys = self . non sequences . keys ( ) dummy tensors , self . scan local vars = get dummy args ( sequences = [ self . sequences [ k ] . tensor for k in seq keys ] , outputs info = [ self . outputs [ k ] . tensor for k in self . ordered out keys ] , non sequences = [ self . non sequences [ k ] . tensor for k in nonseq keys ] , * * self . kwargs ) dummy map = dict ( zip ( seq keys + filled out keys + nonseq keys , dummy tensors ) ) arg map = self . sequences . copy ( ) arg map . update ( self . outputs ) arg map . update ( self . non sequences ) self . loop vars = Loop Vars ( ) for k , dummy tensor in dummy map . items ( ) : dummy var = Neural Variable ( dummy tensor , dim = arg map [ k ] . dim ( ) ) self . loop vars [ k ] = dummy var", "predictions": ["construct the ( . . ) ( running ) as a ( . . . . . . . . . )"], "references": ["create inner loop variables ."], "bleu": 0.05538696232597745, "rouge_l": 0.08356164383561643}
{"id": 2304, "code": "def scan step ( self , vars ) : from neural var import Neural Variable if not self . loop vars : raise Exception ( \"The loop is not initialized. To initialize the loop, use `with loop as vars`\" ) replace map = { } for k , var in vars . items ( ) : if var is not None : replace map [ self . dummy nodes [ k ] . tensor ] = var . tensor outputs = { } for k in self . outputs : if k not in self . loop vars : raise Exception ( \"{} can not be found in loop vars.\" . format ( k ) ) output node = theano . clone ( self . loop vars [ k ] . tensor , replace map ) outputs [ k ] = Neural Variable ( output node , self . loop vars [ k ] . dim ( ) ) return outputs", "predictions": ["scans variables for variables that match the variables ."], "references": ["internal scan with dummy input variables ."], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 2305, "code": "def momentum core ( params , gradients , momentum = 0.9 , learning rate = 0.01 ) : free parameters = [ ] updates = [ ] for param , grad in zip ( params , gradients ) : delta = learning rate * grad velocity = theano . shared ( np . zeros like ( param . get value ( ) ) , name = param . name + ' vel' ) updates . append ( ( velocity , momentum * velocity - delta ) ) updates . append ( ( param , param + velocity ) ) free parameters . append ( velocity ) return updates , free parameters", "predictions": ["return momentum of momentum in the given interval ."], "references": ["momentum sgd optimization core ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 2306, "code": "def iftrain ( self , then branch , else branch ) : return ifelse ( self . training flag , then branch , else branch , name = \"iftrain\" )", "predictions": ["get a iftrain for this operation ."], "references": ["execute then_branch when training ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2307, "code": "def skip ( self , n batches , n epochs = 0 ) : logging . info ( \"skip %d epochs and %d batches\" % ( n epochs , n batches ) ) self . skip batches = n batches self . skip epochs = n epochs", "predictions": ["skip epochs for the binary table ."], "references": ["skip n batches in the training ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 2308, "code": "def train ( self , train set , valid set = None , test set = None , train size = None ) : self . epoch = 0 while True : if self . skip epochs > 0 : logging . info ( \"skipping one epoch ...\" ) self . skip epochs -= 1 self . epoch += 1 yield None continue if not self . epoch % self . config . test frequency and test set : try : self . run test ( self . epoch , test set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if not self . epoch % self . validation frequency and valid set : try : if not self . run valid ( self . epoch , valid set ) : logging . info ( 'patience elapsed, bailing out' ) break except Keyboard Interrupt : logging . info ( 'interrupted!' ) break try : costs = self . run train ( self . epoch , train set , train size ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if np . isnan ( costs [ 0 ] [ 1 ] ) : logging . info ( \"Na N detected in costs, rollback to last parameters\" ) self . set params ( * self . checkpoint ) else : self . epoch += 1 self . network . epoch callback ( ) yield dict ( costs ) if valid set and self . config . get ( \"save best parameters\" , True ) : self . set params ( * self . best params ) if test set : self . run test ( - 1 , test set )", "predictions": ["a simple train will ignore the train of the train ."], "references": ["train the model and return costs ."], "bleu": 0.1354599427337814, "rouge_l": 0.3472485768500949}
{"id": 2309, "code": "def run train ( self , epoch , train set , train size = None ) : self . network . train logger . record epoch ( epoch + 1 ) costs = self . train step ( train set , train size ) if not epoch % self . config . monitor frequency : self . report ( dict ( costs ) , \"train\" , epoch ) self . last run costs = costs return costs", "predictions": ["submit a train in a python train ."], "references": ["run one training iteration ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2310, "code": "def run valid ( self , epoch , valid set , dry run = False , save path = None ) : costs = self . valid step ( valid set ) , J = costs [ 0 ] new best = False if self . best cost - J > self . best cost * self . min improvement : self . best params = self . copy params ( ) new best = True if not dry run : self . best cost = J self . best epoch = epoch self . save checkpoint ( save path ) self . report ( dict ( costs ) , type = \"valid\" , epoch = 0 if dry run else epoch , new best = new best ) self . last run costs = costs return epoch - self . best epoch < self . patience", "predictions": ["runs a cost script ."], "references": ["run one valid iteration return true if to continue training ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 2311, "code": "def report ( self , score map , type = \"valid\" , epoch = - 1 , new best = False ) : type str = type if len ( type str ) < 5 : type str += \" \" * ( 5 - len ( type str ) ) info = \" \" . join ( \"%s=%.2f\" % el for el in score map . items ( ) ) current epoch = epoch if epoch > 0 else self . current epoch ( ) epoch str = \"epoch={}\" . format ( current epoch + 1 ) if epoch < 0 : epoch str = \"dryrun\" sys . stdout . write ( \"\\r\" ) sys . stdout . flush ( ) marker = \" *\" if new best else \"\" message = \"{} ({}) {}{}\" . format ( type str , epoch str , info , marker ) self . network . train logger . record ( message ) logging . info ( message )", "predictions": ["reports that the given type passes the given score ."], "references": ["report the scores and record them in the log ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 2312, "code": "def get data ( self , data split = \"train\" ) : if data split == 'train' : return self . current train set elif data split == 'valid' : return self . current valid set elif data split == 'test' : return self . current test set else : return None", "predictions": ["get the data for this request ."], "references": ["get specified split of data ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 2313, "code": "def apply ( self , func , dim = None ) : output dim = dim if dim else self . output dim return Neural Variable ( func ( self . tensor ) , output dim )", "predictions": ["applies the specified variable to the underlying clipping clipping clipping ."], "references": ["apply a function to tensors ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 2314, "code": "def report ( self ) : if self . logger : self . logger . info ( \"accessed parameters:\" ) for key in self . used parameters : self . logger . info ( \" - %s %s\" % ( key , \"(undefined)\" if key in self . undefined parameters else \"\" ) )", "predictions": ["report things to the caller ."], "references": ["report usage of training parameters ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 2315, "code": "def var ( self , tensor type , last dim = 0 , test shape = None ) : from deepy . tensor import var return var ( tensor type , last dim = last dim , test shape = test shape )", "predictions": ["create a test method that can be used to set the test variables ."], "references": ["an alias of deepy . tensor . var ."], "bleu": 0.08839374326825923, "rouge_l": 0.09050445103857567}
{"id": 2316, "code": "def shared ( self , value , name = None ) : if type ( value ) == int : final value = np . array ( value , dtype = \"int32\" ) elif type ( value ) == float : final value = np . array ( value , dtype = env . FLOATX ) else : final value = value return theano . shared ( final value , name = name )", "predictions": ["create or update the shared variables ."], "references": ["create a shared theano scalar value ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 2317, "code": "def invoke ( self ) : self . iter += 1 if self . iter - max ( self . trainer . best iter , self . annealed iter ) >= self . patience : if self . annealed times >= self . anneal times : logging . info ( \"ending\" ) self . trainer . exit ( ) else : self . trainer . set params ( * self . trainer . best params ) self . learning rate . set value ( self . learning rate . get value ( ) * 0.5 ) self . annealed times += 1 self . annealed iter = self . iter logging . info ( \"annealed learning rate to %f\" % self . learning rate . get value ( ) )", "predictions": ["detailed detailed event that will be run on the next trainer ."], "references": ["run it return whether to end training ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 2318, "code": "def invoke ( self ) : self . iter += 1 logging . info ( \"{} epochs left to run\" . format ( self . patience - self . iter ) ) if self . iter >= self . patience : self . trainer . exit ( )", "predictions": ["invoke this method on the last used event ."], "references": ["run it return whether to end training ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2319, "code": "def stack encoders ( self , * layers ) : self . stack ( * layers ) self . encoding layes . extend ( layers )", "predictions": ["create a new object ."], "references": ["stack encoding layers this must be done before stacking decoding layers ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 2320, "code": "def stack decoders ( self , * layers ) : self . stack ( * layers ) self . decoding layers . extend ( layers )", "predictions": ["create a new object ."], "references": ["stack decoding layers ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2321, "code": "def encode ( self , x ) : if not self . encoding network : self . encoding network = Neural Network ( self . input dim , self . input tensor ) self . encoding network . input variables = self . input variables for layer in self . encoding layes : self . encoding network . stack layer ( layer , no setup = True ) return self . encoding network . compute ( * x )", "predictions": ["creates a layer object for this layer ."], "references": ["encode given input ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2322, "code": "def decode ( self , x ) : if not self . rep dim : raise Exception ( \"rep dim must be set to decode.\" ) if not self . decoding network : self . decoding network = Neural Network ( self . rep dim ) for layer in self . decoding layers : self . decoding network . stack layer ( layer , no setup = True ) return self . decoding network . compute ( x )", "predictions": ["decode the layer out of a layer ."], "references": ["decode given representation ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 2323, "code": "def all parameters ( self ) : params = [ ] params . extend ( self . parameters ) params . extend ( self . free parameters ) return params", "predictions": ["returns a list of parameters ."], "references": ["return all parameters ."], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 2324, "code": "def setup variables ( self ) : if self . input tensor : if type ( self . input tensor ) == int : x = dim to var ( self . input tensor , name = \"x\" ) else : x = self . input tensor else : x = T . matrix ( 'x' ) self . input variables . append ( x ) self . output = x self . test output = x", "predictions": ["sets up the calculation ."], "references": ["set up variables ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 2325, "code": "def compute ( self , * x ) : self . compile ( ) outs = self . compute ( * x ) if self . output keys : return Map Dict ( dict ( zip ( self . output keys , outs ) ) ) else : return outs", "predictions": ["compute the full message for this message ."], "references": ["return network output ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2326, "code": "def save params ( self , path , new thread = False ) : save logger . info ( path ) param variables = self . all parameters params = [ p . get value ( ) . copy ( ) for p in param variables ] if new thread : thread = Thread ( target = save network params , args = ( params , path ) ) thread . start ( ) else : save network params ( params , path ) self . train logger . save ( path )", "predictions": ["saves this controller to the database ."], "references": ["save parameters to file ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 2327, "code": "def load params ( self , path , exclude free params = False ) : if not os . path . exists ( path ) : return logging . info ( \"loading parameters from %s\" % path ) if exclude free params : params to load = self . parameters else : params to load = self . all parameters if path . endswith ( \".gz\" ) : opener = gzip . open if path . lower ( ) . endswith ( '.gz' ) else open handle = opener ( path , 'rb' ) saved params = pickle . load ( handle ) handle . close ( ) for target , source in zip ( params to load , saved params ) : logging . info ( '%s: setting value %s' , target . name , source . shape ) target . set value ( source ) elif path . endswith ( \".npz\" ) : arrs = np . load ( path ) for target , idx in zip ( params to load , range ( len ( arrs . keys ( ) ) ) ) : source = arrs [ 'arr %d' % idx ] logging . info ( '%s: setting value %s' , target . name , source . shape ) target . set value ( source ) else : raise Exception ( \"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path ) self . train logger . load ( path )", "predictions": ["load data from given location ."], "references": ["load parameters from file ."], "bleu": 0.2626909894424158, "rouge_l": 0.5545454545454546}
{"id": 2328, "code": "def report ( self ) : logging . info ( \"network inputs: %s\" , \" \" . join ( map ( str , self . input variables ) ) ) logging . info ( \"network targets: %s\" , \" \" . join ( map ( str , self . target variables ) ) ) logging . info ( \"network parameters: %s\" , \" \" . join ( map ( str , self . all parameters ) ) ) logging . info ( \"parameter count: %d\" , self . parameter count )", "predictions": ["generate and reports all variables to the given variables ."], "references": ["print network statistics ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 2329, "code": "def register updates ( self , * updates ) : for key , node in updates : if key not in self . registered updates : self . updates . append ( ( key , node ) ) self . registered updates . add ( key )", "predictions": ["registers a set of updates to be registered and stored in updates ."], "references": ["register updates that will be executed in each iteration ."], "bleu": 0.12011055432195765, "rouge_l": 0.3562043795620438}
{"id": 2330, "code": "def register training updates ( self , * updates ) : for key , node in updates : if key not in self . registered training updates : self . training updates . append ( ( key , node ) ) self . registered training updates . add ( key )", "predictions": ["registers a training updates ."], "references": ["register updates that will only be executed in training phase ."], "bleu": 0.09778809693469985, "rouge_l": 0.2341650671785029}
{"id": 2331, "code": "def register monitors ( self , * monitors ) : for key , node in monitors : if key not in self . registered monitors : node *= 1.0 self . training monitors . append ( ( key , node ) ) self . testing monitors . append ( ( key , node ) ) self . registered monitors . add ( key )", "predictions": ["registers stats for this class ."], "references": ["register monitors they should be tuple of name and theano variable ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 2332, "code": "def dump one ( elt to pickle , file obj ) : pickled elt str = dumps ( elt to pickle ) file obj . write ( pickled elt str ) file obj . write ( '\\n\\n' )", "predictions": ["dumps the object to a file ."], "references": ["dumps one element to file_obj a file opened in write mode"], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 2333, "code": "def load params ( self , path , exclude free params = False ) : from deepy . core import graph from deepy . core . comp graph import Computational Graph model = graph . compile ( blocks = [ self ] ) model . load params ( path , exclude free params = exclude free params )", "predictions": ["extracts and returns a dataset object for a given path ."], "references": ["load parameters to the block ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2334, "code": "def onehot tensor ( i matrix , vocab size ) : dim0 , dim1 = i matrix . shape i vector = i matrix . reshape ( ( - 1 , ) ) hot matrix = T . extra ops . to one hot ( i vector , vocab size ) . reshape ( ( dim0 , dim1 , vocab size ) ) return hot matrix", "predictions": ["computes the onehot of this octagon from a dim0 ."], "references": ["# batch x time"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2335, "code": "def create request elements ( cls , request type , credentials , url , method = 'GET' , params = None , headers = None , body = '' , secret = None , redirect uri = '' , scope = '' , csrf = '' , user state = '' ) : headers = headers or { } params = params or { } consumer key = credentials . consumer key or '' consumer secret = credentials . consumer secret or '' token = credentials . token or '' refresh token = credentials . refresh token or credentials . token or '' url , base params = cls . split url ( url ) params . update ( dict ( base params ) ) if request type == cls . USER AUTHORIZATION REQUEST TYPE : if consumer key and redirect uri and ( csrf or not cls . supports csrf protection ) : params [ 'client id' ] = consumer key params [ 'redirect uri' ] = redirect uri params [ 'scope' ] = scope if cls . supports user state : params [ 'state' ] = base64 . urlsafe b64encode ( json . dumps ( { \"csrf\" : csrf , \"user state\" : user state } ) . encode ( 'utf-8' ) ) else : params [ 'state' ] = csrf params [ 'response type' ] = 'code' headers . update ( cls . authorization header ( credentials ) ) else : raise O Auth2Error ( 'Credentials with valid consumer key and arguments ' 'redirect uri, scope and state are required to create ' 'O Auth 2.0 user authorization request elements!' ) elif request type == cls . ACCESS TOKEN REQUEST TYPE : if consumer key and consumer secret : params [ 'code' ] = token params [ 'client id' ] = consumer key params [ 'client secret' ] = consumer secret params [ 'redirect uri' ] = redirect uri params [ 'grant type' ] = 'authorization code' headers . update ( cls . authorization header ( credentials ) ) else : raise O Auth2Error ( 'Credentials with valid token, consumer key, ' 'consumer secret and argument redirect uri are required ' 'to create O Auth 2.0 access token request elements!' ) elif request type == cls . REFRESH TOKEN REQUEST TYPE : if refresh token and consumer key and consumer secret : params [ 'refresh token' ] = refresh token params [ 'client id' ] = consumer key params [ 'client secret' ] = consumer secret params [ 'grant type' ] = 'refresh token' else : raise O Auth2Error ( 'Credentials with valid refresh token, consumer key, ' 'consumer secret are required to create O Auth 2.0 ' 'refresh token request elements!' ) elif request type == cls . PROTECTED RESOURCE REQUEST TYPE : if credentials . token type == cls . BEARER : headers . update ( { 'Authorization' : 'Bearer {0}' . format ( credentials . token ) } ) elif token : params [ 'access token' ] = token else : raise O Auth2Error ( 'Credentials with valid token are required to create ' 'O Auth 2.0 protected resources request elements!' ) request elements = core . Request Elements ( url , method , params , headers , body ) return cls . x request elements filter ( request type , request elements , credentials )", "predictions": ["creates an instance of the request ."], "references": ["creates |oauth2| request elements ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 2336, "code": "def x credentials parser ( credentials , data ) : credentials . expire in = data . get ( 'expires' ) if data . get ( 'token type' ) == 'bearer' : credentials . token type = 'Bearer' return credentials", "predictions": ["get the step from the ( but does not include any step ) ."], "references": ["we need to override this method to fix facebooks naming deviation ."], "bleu": 0.08839374326825923, "rouge_l": 0.07800511508951406}
{"id": 2337, "code": "def login ( provider name ) : response = make response ( ) result = authomatic . login ( Werkzeug Adapter ( request , response ) , provider name ) if result : if result . user : result . user . update ( ) return render template ( 'login.html' , result = result ) return response", "predictions": ["momentum a single = value is a member of the process ."], "references": ["login handler must accept both get and post to be able to use openid ."], "bleu": 0.08091975469641616, "rouge_l": 0.07261904761904761}
{"id": 2338, "code": "def save ( self ) : if self . data : cookie = self . create cookie ( ) cookie len = len ( cookie ) if cookie len > 4093 : raise Session Error ( 'Cookie too long! The cookie size {0} ' 'is more than 4093 bytes.' . format ( cookie len ) ) self . adapter . set header ( 'Set-Cookie' , cookie ) self . data = { }", "predictions": ["save this : saves the . at the current position ."], "references": ["adds the session cookie to headers ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 2339, "code": "def get data ( self ) : cookie = self . adapter . cookies . get ( self . name ) return self . deserialize ( cookie ) if cookie else { }", "predictions": ["skip this epochs object for the currently registered epochs ."], "references": ["extracts the session data from cookie ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2340, "code": "def data ( self ) : if not self . data : self . data = self . get data ( ) if self . data is None : self . data = { } return self . data", "predictions": ["get the descriptor for this message ."], "references": ["gets session data lazily ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2341, "code": "def signature ( self , * parts ) : signature = hmac . new ( six . b ( self . secret ) , digestmod = hashlib . sha1 ) signature . update ( six . b ( '|' . join ( parts ) ) ) return signature . hexdigest ( )", "predictions": ["create a run with this cipher ."], "references": ["creates signature for the session ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2342, "code": "def valid ( self ) : if self . expiration time : return self . expiration time > int ( time . time ( ) ) else : return True", "predictions": ["get the current logical lock ."], "references": ["true if credentials are valid false if expired ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2343, "code": "def is binary string ( content ) : textchars = ( bytearray ( [ 7 , 8 , 9 , 10 , 12 , 13 , 27 ] ) + bytearray ( range ( 0x20 , 0x100 ) ) ) return bool ( content . translate ( None , textchars ) )", "predictions": ["determine if this self - encoded self - supplied score is a ( i . e . , the score is contained in the ( map map map map map map map map map map map map map map map map map map map to the given score map map"], "references": ["return true if string is binary data ."], "bleu": 0.028577262451992175, "rouge_l": 0.11898569570871263}
{"id": 2344, "code": "def content ( self ) : if not self . content : content = self . httplib response . read ( ) if self . is binary string ( content ) : self . content = content else : self . content = content . decode ( 'utf-8' ) return self . content", "predictions": ["set the get request to be drawn on the client ."], "references": ["the whole response content ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 2345, "code": "def create request elements ( cls , request type , credentials , url , params = None , headers = None , body = '' , method = 'GET' , verifier = '' , callback = '' ) : params = params or { } headers = headers or { } consumer key = credentials . consumer key or '' consumer secret = credentials . consumer secret or '' token = credentials . token or '' token secret = credentials . token secret or '' url , base params = cls . split url ( url ) params . update ( dict ( base params ) ) if request type == cls . USER AUTHORIZATION REQUEST TYPE : if token : params [ 'oauth token' ] = token else : raise O Auth1Error ( 'Credentials with valid token are required to create ' 'User Authorization URL!' ) else : if request type == cls . REQUEST TOKEN REQUEST TYPE : if consumer key and consumer secret and callback : params [ 'oauth consumer key' ] = consumer key params [ 'oauth callback' ] = callback else : raise O Auth1Error ( 'Credentials with valid consumer key, consumer secret ' 'and callback are required to create Request Token ' 'URL!' ) elif request type == cls . ACCESS TOKEN REQUEST TYPE : if consumer key and consumer secret and token and verifier : params [ 'oauth token' ] = token params [ 'oauth consumer key' ] = consumer key params [ 'oauth verifier' ] = verifier else : raise O Auth1Error ( 'Credentials with valid consumer key, ' 'consumer secret, token and argument verifier' ' are required to create Access Token URL!' ) elif request type == cls . PROTECTED RESOURCE REQUEST TYPE : if consumer key and consumer secret and token and token secret : params [ 'oauth token' ] = token params [ 'oauth consumer key' ] = consumer key else : raise O Auth1Error ( 'Credentials with valid consumer key, ' + 'consumer secret, token and token secret are required ' 'to create Protected Resources URL!' ) params [ 'oauth signature method' ] = cls . signature generator . method params [ 'oauth timestamp' ] = str ( int ( time . time ( ) ) ) params [ 'oauth nonce' ] = cls . csrf generator ( str ( uuid . uuid4 ( ) ) ) params [ 'oauth version' ] = '1.0' params [ 'oauth signature' ] = cls . signature generator . create signature ( method , url , params , consumer secret , token secret ) request elements = core . Request Elements ( url , method , params , headers , body ) return cls . x request elements filter ( request type , request elements , credentials )", "predictions": ["creates an instance of the request ."], "references": ["creates |oauth1| request elements ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 2346, "code": "def access user info ( self ) : response = super ( Bitbucket , self ) . access user info ( ) response . data . setdefault ( \"email\" , None ) email response = self . access ( self . user email url ) if email response . data : for item in email response . data : if item . get ( \"primary\" , False ) : response . data . update ( email = item . get ( \"email\" , None ) ) return response", "predictions": ["report an - report object into the report ."], "references": ["email is available in separate method so second request is needed ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 2347, "code": "def login ( self , * login args , * * login kwargs ) : def decorator ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : self . response = make response ( ) adapter = Werkzeug Adapter ( request , self . response ) login kwargs . setdefault ( 'session' , session ) login kwargs . setdefault ( 'session saver' , self . session saver ) self . result = super ( Flask Authomatic , self ) . login ( adapter , * login args , * * login kwargs ) return f ( * args , * * kwargs ) return decorated return decorator", "predictions": ["decorator to bounds and wrap the import action ."], "references": ["decorator for flask view functions ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2348, "code": "def login ( self ) : if self . params . get ( self . identifier param ) : self . log ( logging . INFO , u'Starting Open ID authentication procedure.' ) url = users . create login url ( dest url = self . url , federated identity = self . identifier ) self . log ( logging . INFO , u'Redirecting user to {0}.' . format ( url ) ) self . redirect ( url ) else : self . log ( logging . INFO , u'Continuing Open ID authentication procedure after redirect.' ) user = users . get current user ( ) if user : self . log ( logging . INFO , u'Authentication successful.' ) self . log ( logging . INFO , u'Creating user.' ) self . user = core . User ( self , id = user . federated identity ( ) , email = user . email ( ) , gae user = user ) else : raise Failure Error ( 'Unable to authenticate identifier \"{0}\"!' . format ( self . identifier ) )", "predictions": ["use this method to log the user in ."], "references": ["launches the openid authentication procedure ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2349, "code": "def session set ( self , key , value ) : self . session [ self . session key ( key ) ] = value", "predictions": ["sets the invoke invoke ."], "references": ["saves a value to session ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2350, "code": "def split url ( url ) : split = parse . urlsplit ( url ) base = parse . urlunsplit ( ( split . scheme , split . netloc , split . path , 0 , 0 ) ) params = parse . parse qsl ( split . query , True ) return base , params", "predictions": ["invoke this method with unit style appended to the other ."], "references": ["splits given url to url base and params converted to list of tuples ."], "bleu": 0.09596928383261212, "rouge_l": 0.15661103979460847}
{"id": 2351, "code": "def get app kwarg dict ( app Instance ) : app config = getattr ( app Instance , 'config' , { } ) return dict ( ( k . lower ( ) . replace ( 'cors ' , '' ) , app config . get ( k ) ) for k in CONFIG OPTIONS if app config . get ( k ) is not None )", "predictions": ["read all the children of this class from the given encoders ."], "references": ["returns the dictionary of cors specific app configurations ."], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 2352, "code": "def ensure iterable ( inst ) : if isinstance ( inst , str ) : return [ inst ] elif not isinstance ( inst , collections . abc . Iterable ) : return [ inst ] else : return inst", "predictions": ["stack object to screen if it is 0 or 1"], "references": ["wraps scalars or string types as a list or returns the iterable instance ."], "bleu": 0.08450033111870488, "rouge_l": 0.08090185676392574}
{"id": 2353, "code": "def isclose ( a , b , * , rel tol = 1e-09 , abs tol = 0.0 ) : try : return math . isclose ( a , b , rel tol = rel tol , abs tol = abs tol ) except Attribute Error : if ( rel tol < 0.0 ) or ( abs tol < 0.0 ) : raise Value Error ( \"Tolerances must be non-negative, but are rel tol: {} and abs tol: {}\" . format ( rel tol , abs tol ) ) if math . isnan ( a ) or math . isnan ( b ) : return False if ( a == b ) : return True if math . isinf ( a ) or math . isinf ( b ) : return False diff = abs ( a - b ) return ( diff <= rel tol * abs ( b ) ) or ( diff <= rel tol * abs ( a ) ) or ( diff <= abs tol )", "predictions": ["calculates the absolute difference between two given offsets"], "references": ["python 3 . 4 does not have math . isclose so we need to steal it and add it here ."], "bleu": 0.026553759665708148, "rouge_l": 0.0}
{"id": 2354, "code": "def get corresponding offsets ( onset fronts , onset front id , onsets , offsets ) : corresponding offsets = [ ] for index in get front idxs from id ( onset fronts , onset front id ) : offset fidx , offset sidx = lookup offset by onset idx ( index , onsets , offsets ) corresponding offsets . append ( ( offset fidx , offset sidx ) ) return corresponding offsets", "predictions": ["this method is called to decode a ( not a child rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep rep : ( rep rep rep rep rep rep rep rep rep rep rep"], "references": ["gets the offsets that occur as close as possible to the onsets in the given onset - front ."], "bleu": 0.02403051755364481, "rouge_l": 0.03154084798345398}
{"id": 2355, "code": "def remove overlaps ( segmentation mask , fronts ) : fidxs , sidxs = np . where ( ( segmentation mask != fronts ) & ( segmentation mask != 0 ) & ( fronts != 0 ) ) fronts [ fidxs , sidxs ] = 0", "predictions": ["all the parameters from the set of layers to be removed from the set of waiting ."], "references": ["removes all points in the fronts that overlap with the segmentation mask ."], "bleu": 0.09083627868206415, "rouge_l": 0.2732362821948488}
{"id": 2356, "code": "def merge adjacent segments ( mask ) : mask ids = [ id for id in np . unique ( mask ) if id != 0 ] for id in mask ids : myfidxs , mysidxs = np . where ( mask == id ) for other in mask ids : if id == other : continue else : other fidxs , other sidxs = np . where ( mask == other ) if segments are adjacent ( ( myfidxs , mysidxs ) , ( other fidxs , other sidxs ) ) : mask [ other fidxs , other sidxs ] = id", "predictions": ["merges all registered ) objects from another ) with ) ) that have been reached ."], "references": ["merges all segments in mask which are touching ."], "bleu": 0.10878661088699644, "rouge_l": 0.2527624309392265}
{"id": 2357, "code": "def asa task ( q , masks , stft , sample width , frame rate , nsamples for each fft ) : for mask in masks : mask = np . where ( mask > 0 , 1 , 0 ) masks = [ mask * stft for mask in masks ] nparrs = [ ] dtype dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } dtype = dtype dict [ sample width ] for m in masks : times , nparr = signal . istft ( m , frame rate , nperseg = nsamples for each fft ) nparr = nparr . astype ( dtype ) nparrs . append ( nparr ) for m in nparrs : q . put ( m ) q . put ( \"DONE\" )", "predictions": ["the idea of each compile cell in the given compile . the first compile is . ."], "references": ["worker for the asa algorithm s multiprocessing step ."], "bleu": 0.07994607499472013, "rouge_l": 0.1628838451268358}
{"id": 2358, "code": "def list to tf input ( data , response index , num outcomes ) : matrix = np . matrix ( [ row [ : response index ] + row [ response index + 1 : ] for row in data ] ) outcomes = np . asarray ( [ row [ response index ] for row in data ] , dtype = np . uint8 ) outcomes onehot = ( np . arange ( num outcomes ) == outcomes [ : , None ] ) . astype ( np . float32 ) return matrix , outcomes onehot", "predictions": ["convert a save save ."], "references": ["separates the outcome feature from the data and creates the onehot vector for each row ."], "bleu": 0.03025060142990643, "rouge_l": 0.08701854493580599}
{"id": 2359, "code": "def expand and standardize dataset ( response index , response header , data set , col vals , headers , standardizers , feats to ignore , columns to expand , outcome trans dict ) : modified set = [ ] for row index , row in enumerate ( data set ) : new row = [ ] for col index , val in enumerate ( row ) : header = headers [ col index ] if col index == response index : new outcome = outcome trans dict [ val ] new row . append ( new outcome ) elif header in feats to ignore : pass elif header in columns to expand : for poss val in col vals [ header ] : if val == poss val : new cat val = 1.0 else : new cat val = - 1.0 new row . append ( new cat val ) else : new cont val = float ( ( val - standardizers [ header ] [ 'mean' ] ) / standardizers [ header ] [ 'std dev' ] ) new row . append ( new cont val ) modified set . append ( new row ) expanded headers = [ ] for header in headers : if header in feats to ignore : pass elif ( header in columns to expand ) and ( header is not response header ) : for poss val in col vals [ header ] : new header = '{} {}' . format ( header , poss val ) expanded headers . append ( new header ) else : expanded headers . append ( header ) return modified set , expanded headers", "predictions": ["load the given self - idx self ."], "references": ["standardizes continuous features and expands categorical features ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 2360, "code": "def list to tf input ( data , response index , num outcomes ) : matrix = np . matrix ( [ row [ : response index ] + row [ response index + 1 : ] for row in data ] ) outcomes = np . asarray ( [ row [ response index ] for row in data ] , dtype = np . uint8 ) return matrix , outcomes", "predictions": ["transforms a list of logging into a self self self ."], "references": ["separates the outcome feature from the data ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 2361, "code": "def update index url from configs ( self ) : if 'VIRTUAL ENV' in os . environ : self . pip config locations . append ( os . path . join ( os . environ [ 'VIRTUAL ENV' ] , 'pip.conf' ) ) self . pip config locations . append ( os . path . join ( os . environ [ 'VIRTUAL ENV' ] , 'pip.ini' ) ) if site config files : self . pip config locations . extend ( site config files ) index url = None custom config = None if 'PIP INDEX URL' in os . environ and os . environ [ 'PIP INDEX URL' ] : index url = os . environ [ 'PIP INDEX URL' ] custom config = 'PIP INDEX URL environment variable' else : for pip config filename in self . pip config locations : if pip config filename . startswith ( '~' ) : pip config filename = os . path . expanduser ( pip config filename ) if os . path . isfile ( pip config filename ) : config = Config Parser ( ) config . read ( [ pip config filename ] ) try : index url = config . get ( 'global' , 'index-url' ) custom config = pip config filename break except ( No Option Error , No Section Error ) : pass if index url : self . PYPI API URL = self . prepare api url ( index url ) print ( Color ( 'Setting API url to {{autoyellow}}{}{{/autoyellow}} as found in {{autoyellow}}{}{{/autoyellow}}' '. Use --default-index-url to use pypi default index' . format ( self . PYPI API URL , custom config ) ) )", "predictions": ["register the ( in the database ."], "references": ["checks for alternative index - url in pip . conf"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2362, "code": "def main ( ) : options = get options ( ) Windows . enable ( auto colors = True , reset atexit = True ) try : check for virtualenv ( options ) filenames = Requirements Detector ( options . get ( '<requirements file>' ) ) . get filenames ( ) if filenames : print ( Color ( '{{autoyellow}}Found valid requirements file(s):{{/autoyellow}} ' '{{autocyan}}\\n{}{{/autocyan}}' . format ( '\\n' . join ( filenames ) ) ) ) else : print ( Color ( '{autoyellow}No requirements files found in current directory. CD into your project ' 'or manually specify requirements files as arguments.{/autoyellow}' ) ) return packages = Packages Detector ( filenames ) . get packages ( ) packages status map = Packages Status Detector ( packages , options . get ( '--use-default-index' ) ) . detect available upgrades ( options ) selected packages = Package Interactive Selector ( packages status map , options ) . get packages ( ) upgraded packages = Packages Upgrader ( selected packages , filenames , options ) . do upgrade ( ) print ( Color ( '{{autogreen}}Successfully upgraded (and updated requirements) for the following packages: ' '{}{{/autogreen}}' . format ( ',' . join ( [ package [ 'name' ] for package in upgraded packages ] ) ) ) ) if options [ '--dry-run' ] : print ( Color ( '{automagenta}Actually, no, because this was a simulation using --dry-run{/automagenta}' ) ) except Keyboard Interrupt : print ( Color ( '\\n{autored}Upgrade interrupted.{/autored}' ) )", "predictions": ["the main entry point for all packages packages packages ."], "references": ["main cli entrypoint ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 2363, "code": "def autodetect files ( self ) : if self . is valid requirements file ( 'requirements.txt' ) : self . filenames . append ( 'requirements.txt' ) if self . is valid requirements file ( 'requirements.pip' ) : self . filenames . append ( 'requirements.pip' ) if os . path . isdir ( 'requirements' ) : for filename in os . listdir ( 'requirements' ) : file path = os . path . join ( 'requirements' , filename ) if self . is valid requirements file ( file path ) : self . filenames . append ( file path ) self . check inclusions recursively ( )", "predictions": ["creates an node ."], "references": ["attempt to detect requirements files in the current working directory"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 2364, "code": "def handle error ( errcode ) : if type ( errcode ) is c int : errcode = errcode . value if errcode == 0 : pass elif errcode == - 1 : raise Timeout Error ( \"the operation failed due to a timeout.\" ) elif errcode == - 2 : raise Lost Error ( \"the stream has been lost.\" ) elif errcode == - 3 : raise Invalid Argument Error ( \"an argument was incorrectly specified.\" ) elif errcode == - 4 : raise Internal Error ( \"an internal error has occurred.\" ) elif errcode < 0 : raise Runtime Error ( \"an unknown error has occurred.\" )", "predictions": ["dump all unnecessary ( for a 3d - ( . since this is a ( ."], "references": ["error handler function . translates an error code into an exception ."], "bleu": 0.08513012360883544, "rouge_l": 0.14663461538461536}
{"id": 2365, "code": "def child ( self , name ) : return XML Element ( lib . lsl child ( self . e , str . encode ( name ) ) )", "predictions": ["core method to get a load element ."], "references": ["get a child with a specified name ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 2366, "code": "def set name ( self , name ) : return bool ( lib . lsl set name ( self . e , str . encode ( name ) ) )", "predictions": ["shape files that are not supported by this interpreter ."], "references": ["set the element s name . returns false if the node is empty ."], "bleu": 0.08450033111870488, "rouge_l": 0.08090185676392574}
{"id": 2367, "code": "def set value ( self , value ) : return bool ( lib . lsl set value ( self . e , str . encode ( value ) ) )", "predictions": ["headers to provide default overflow ."], "references": ["set the element s value . returns false if the node is empty ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 2368, "code": "def append child ( self , name ) : return XML Element ( lib . lsl append child ( self . e , str . encode ( name ) ) )", "predictions": ["append the child components to the context ."], "references": ["append a child element with the specified name ."], "bleu": 0.17795502018438056, "rouge_l": 0.465648854961832}
{"id": 2369, "code": "def prepend child ( self , name ) : return XML Element ( lib . lsl prepend child ( self . e , str . encode ( name ) ) )", "predictions": ["prepend the child to the child ."], "references": ["prepend a child element with the specified name ."], "bleu": 0.1755217914979255, "rouge_l": 0.48897795591182364}
{"id": 2370, "code": "def append copy ( self , elem ) : return XML Element ( lib . lsl append copy ( self . e , elem . e ) )", "predictions": ["appends a copy of this compilation to the end of the list ."], "references": ["append a copy of the specified element as a child ."], "bleu": 0.21142141714303078, "rouge_l": 0.42302357836338417}
{"id": 2371, "code": "def prepend copy ( self , elem ) : return XML Element ( lib . lsl prepend copy ( self . e , elem . e ) )", "predictions": ["creates a copy of this ldapurl to the given element ."], "references": ["prepend a copy of the specified element as a child ."], "bleu": 0.24384183193426084, "rouge_l": 0.5454545454545454}
{"id": 2372, "code": "def remove child ( self , rhs ) : if type ( rhs ) is XML Element : lib . lsl remove child ( self . e , rhs . e ) else : lib . lsl remove child n ( self . e , rhs )", "predictions": ["remove this object from the given object ."], "references": ["remove a given child element specified by name or as element ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 2373, "code": "def do AUTOCOMPLETE ( cmd , s ) : s = list ( preprocess query ( s ) ) [ 0 ] keys = [ k . decode ( ) for k in DB . smembers ( edge ngram key ( s ) ) ] print ( white ( keys ) ) print ( magenta ( '({} elements)' . format ( len ( keys ) ) ) )", "predictions": ["get all command and set its command to the cluster ."], "references": ["shows autocomplete results for a given token ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 2374, "code": "def compute edge ngrams ( token , min = None ) : if min is None : min = config . MIN EDGE NGRAMS token = token [ : config . MAX EDGE NGRAMS + 1 ] return [ token [ : i ] for i in range ( min , len ( token ) ) ]", "predictions": ["compute the ngrams for a state ."], "references": ["compute edge ngram of token from min . does not include token itself ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 2375, "code": "def iter pipe ( pipe , processors ) : if isinstance ( pipe , str ) : pipe = [ pipe ] for it in processors : pipe = it ( pipe ) yield from pipe", "predictions": ["yields all pipe tuples from the given pipe ."], "references": ["allow for iterators to return either an item or an iterator of items ."], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 2376, "code": "def make fuzzy ( word , max = 1 ) : neighbors = [ ] for i in range ( 0 , len ( word ) - 1 ) : neighbor = list ( word ) neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] neighbors . append ( '' . join ( neighbor ) ) for letter in string . ascii lowercase : for i in range ( 0 , len ( word ) ) : neighbor = list ( word ) if letter != neighbor [ i ] : neighbor [ i ] = letter neighbors . append ( '' . join ( neighbor ) ) for letter in string . ascii lowercase : for i in range ( 0 , len ( word ) + 1 ) : neighbor = list ( word ) neighbor . insert ( i , letter ) neighbors . append ( '' . join ( neighbor ) ) if len ( word ) > 3 : for i in range ( 0 , len ( word ) ) : neighbor = list ( word ) del neighbor [ i ] neighbors . append ( '' . join ( neighbor ) ) return neighbors", "predictions": ["makes a fuzzy ( neighbor ) of the given word ."], "references": ["naive neighborhoods algo ."], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 2377, "code": "def do help ( self , command ) : if command : doc = getattr ( self , 'do ' + command ) . doc print ( cyan ( doc . replace ( ' ' * 8 , '' ) ) ) else : print ( magenta ( 'Available commands:' ) ) print ( magenta ( 'Type \"HELP <command>\" to get more info.' ) ) names = self . get names ( ) names . sort ( ) for name in names : if name [ : 3 ] != 'do ' : continue doc = getattr ( self , name ) . doc doc = doc . split ( '\\n' ) [ 0 ] print ( '{} {}' . format ( yellow ( name [ 3 : ] ) , cyan ( doc . replace ( ' ' * 8 , ' ' ) . replace ( '\\n' , '' ) ) ) )", "predictions": ["print all the if-modified-since document ."], "references": ["display this help message ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2378, "code": "def do DBINFO ( self , * args ) : info = DB . info ( ) keys = [ 'keyspace misses' , 'keyspace hits' , 'used memory human' , 'total commands processed' , 'total connections received' , 'connected clients' ] for key in keys : print ( '{}: {}' . format ( white ( key ) , blue ( info [ key ] ) ) ) nb of redis db = int ( DB . config get ( 'databases' ) [ 'databases' ] ) for db index in range ( nb of redis db - 1 ) : db name = 'db{}' . format ( db index ) if db name in info : label = white ( 'nb keys (db {})' . format ( db index ) ) print ( '{}: {}' . format ( label , blue ( info [ db name ] [ 'keys' ] ) ) )", "predictions": ["the ( method is called with a ( ( ) method ."], "references": ["print some useful infos from redis db ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 2379, "code": "def send ( r , stream = False ) : r . send ( stream = stream ) return r . response", "predictions": ["send the request to the server ."], "references": ["just sends the request using its send method and returns its response ."], "bleu": 0.11787460936700446, "rouge_l": 0.2846034214618974}
{"id": 2380, "code": "def reinterptet harray to bits ( type From , sig Or Val , bits T ) : size = int ( type From . size ) width Of Elm = type From . elm Type . bit length ( ) w = bits T . bit length ( ) if size * width Of Elm != w : raise Type Conversion Err ( \"Size of types is different\" , size * width Of Elm , w ) part T = Bits ( width Of Elm ) parts = [ p . reinterpret cast ( part T ) for p in sig Or Val ] return Concat ( * reversed ( parts ) ) . reinterpret cast ( bits T )", "predictions": ["convert this ( to an ndarray ."], "references": ["cast harray signal or value to signal or value of type bits"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 2381, "code": "def slice to SLICE ( slice Vals , width ) : if slice Vals . step is not None : raise Not Implemented Error ( ) start = slice Vals . start stop = slice Vals . stop if slice Vals . start is None : start = INT . from Py ( width ) else : start = to H Val ( slice Vals . start ) if slice Vals . stop is None : stop = INT . from Py ( 0 ) else : stop = to H Val ( slice Vals . stop ) start Is Val = isinstance ( start , Value ) stop Is Val = isinstance ( stop , Value ) indexes Are Values = start Is Val and stop Is Val if indexes Are Values : update Time = max ( start . update Time , stop . update Time ) else : update Time = - 1 return Slice . get Value Cls ( ) ( ( start , stop ) , SLICE , 1 , update Time )", "predictions": ["creates a slice of rules from a slice ."], "references": ["convert python slice to value of slice hdl type"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 2382, "code": "def find files ( directory , pattern , recursive = True ) : if not os . path . isdir ( directory ) : if os . path . exists ( directory ) : raise IO Error ( directory + ' is not directory' ) else : raise IO Error ( directory + \" does not exists\" ) if recursive : for root , , files in os . walk ( directory ) : for basename in files : if fnmatch . fnmatch ( basename , pattern ) : filename = os . path . join ( root , basename ) yield filename else : root = directory for basename in os . listdir ( root ) : if fnmatch . fnmatch ( basename , pattern ) : filename = os . path . join ( root , basename ) if os . path . isfile ( filename ) : yield filename", "predictions": ["recursively goes through the pattern ."], "references": ["find files by pattern in directory"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2383, "code": "def is Pow2 ( num ) -> bool : if not isinstance ( num , int ) : num = int ( num ) return num != 0 and ( ( num & ( num - 1 ) ) == 0 )", "predictions": ["is this a number a ( e . g . ( = 1 , ( = 0"], "references": ["check if number or constant is power of two"], "bleu": 0.07994607499472013, "rouge_l": 0.0814419225634179}
{"id": 2384, "code": "def Case ( self , case Val , * statements ) : assert self . parent Stm is None case Val = to H Val ( case Val , self . switch On . dtype ) assert isinstance ( case Val , Value ) , case Val assert case Val . is Full Vld ( ) , \"Cmp with invalid value\" assert case Val not in self . case value index , ( \"Switch statement already has case for value \" , case Val ) self . rank += 1 case = [ ] self . case value index [ case Val ] = len ( self . cases ) self . cases . append ( ( case Val , case ) ) cond = self . switch On . eq ( case Val ) self . inputs . append ( cond ) cond . endpoints . append ( self ) self . register stements ( statements , case ) return self", "predictions": ["this is called via reflection from the requestparams operation ."], "references": ["c - like case of switch statement"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2385, "code": "def Default ( self , * statements ) : assert self . parent Stm is None self . rank += 1 self . default = [ ] self . register stements ( statements , self . default ) return self", "predictions": ["a helper method for default implementation of added statements ."], "references": ["c - like default of switch statement"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2386, "code": "def vcd Register Interfaces ( self , obj : Union [ Interface , Unit ] , parent : Optional [ Vcd Var Writing Scope ] ) : if hasattr ( obj , \" interfaces\" ) and obj . interfaces : name = obj . name parent = self . vcd Writer if parent is None else parent sub Scope = parent . var Scope ( name ) self . obj2scope [ obj ] = sub Scope with sub Scope : for ch Intf in obj . interfaces : self . vcd Register Interfaces ( ch Intf , sub Scope ) if isinstance ( obj , ( Unit , Sim Model ) ) : for u in obj . units : self . vcd Register Interfaces ( u , sub Scope ) return sub Scope else : t = obj . dtype if isinstance ( t , self . supported type classes ) : t Name , width , formatter = vcd Type Info For H Type ( t ) try : parent . add Var ( obj , get Signal Name ( obj ) , t Name , width , formatter ) except Var Already Registered : pass", "predictions": [". operation . just adds the appropriate java class ."], "references": ["register signals from interfaces for interface or unit instances"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2387, "code": "def before Sim ( self , simulator , synthesised Unit ) : vcd = self . vcd Writer vcd . date ( datetime . now ( ) ) vcd . timescale ( 1 ) self . vcd Register Interfaces ( synthesised Unit , None ) self . vcd Register Remaining Signals ( synthesised Unit ) vcd . enddefinitions ( )", "predictions": ["creates a new ( ."], "references": ["this method is called before first step of simulation ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2388, "code": "def log Change ( self , now Time , sig , next Val ) : try : self . vcd Writer . log Change ( now Time , sig , next Val ) except Key Error : pass", "predictions": ["method for seamlessly . this method computes the amount of time for each time ."], "references": ["this method is called for every value change of any signal ."], "bleu": 0.12874330508144843, "rouge_l": 0.30235439900867406}
{"id": 2389, "code": "def distinct By ( iterable , fn ) : s = set ( ) for i in iterable : r = fn ( i ) if r not in s : s . add ( r ) yield i", "predictions": ["return an iterator over all of the strings in the specified iterable ."], "references": ["uniq operation with key selector"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2390, "code": "def remove Unconnected Signals ( netlist ) : to Delete = set ( ) to Search = netlist . signals while to Search : to Search = set ( ) for sig in to Search : if not sig . endpoints : try : if sig . interface is not None : continue except Attribute Error : pass for e in sig . drivers : if isinstance ( e , Operator ) : inputs = e . operands if e . result is sig : e . result = None else : inputs = e . inputs netlist . statements . discard ( e ) for op in inputs : if not isinstance ( op , Value ) : try : op . endpoints . remove ( e ) except Key Error : continue to Search . add ( op ) to Delete . add ( sig ) if to Delete : for sig in to Delete : if sig . ctx == netlist : netlist . signals . remove ( sig ) to Search . discard ( sig ) to Delete = set ( ) to Search = to Search", "predictions": ["removes all inputs from the specified list of inputs ."], "references": ["if signal is not driving anything remove it"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2391, "code": "def on Write Req ( self , sim , addr , data ) : self . requests . append ( ( WRITE , addr , data ) )", "predictions": ["copied from a native snippet ."], "references": ["on writereqrecieved in monitor mode"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2392, "code": "def name for process and mark outputs ( statements : List [ Hdl Statement ] ) -> str : out names = [ ] for stm in statements : for sig in stm . outputs : if not sig . has Generic Name : out names . append ( sig . name ) if out names : return min ( out names ) else : return \"\"", "predictions": ["mark a set of attributes as outputs ."], "references": ["resolve name for process and mark outputs of statemens as not hidden"], "bleu": 0.1223065774797558, "rouge_l": 0.28955696202531644}
{"id": 2393, "code": "def cut off drivers of ( dst Signal , statements ) : separated = [ ] stm filter = [ ] for stm in statements : stm . clean signal meta ( ) d = stm . cut off drivers of ( dst Signal ) if d is not None : separated . append ( d ) f = d is not stm stm filter . append ( f ) return list ( compress ( statements , stm filter ) ) , separated", "predictions": ["removes all statements of an array of statements ."], "references": ["cut off drivers from statements"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2394, "code": "def synthesize ( self , name , interfaces , target Platform ) : ent = Entity ( name ) ent . name = name + \" inst\" for , v in self . params . items ( ) : ent . generics . append ( v ) if isinstance ( interfaces , set ) : intf Set = interfaces else : intf Set = set ( interfaces ) for s in interfaces : pi = port Itemfrom Signal ( s , ent ) pi . register Intern Sig ( s ) ent . ports . append ( pi ) s . hidden = False remove Unconnected Signals ( self ) mark Visibility Of Signals ( self , name , self . signals , intf Set ) for proc in target Platform . before Hdl Arch Generation : proc ( self ) arch = Architecture ( ent ) for p in statements to HW Processes ( self . statements ) : arch . processes . append ( p ) for s in self . signals : if s not in intf Set and not s . hidden : arch . variables . append ( s ) for u in self . sub Units : arch . component Instances . append ( u ) for su in distinct By ( self . sub Units , lambda x : x . name ) : arch . components . append ( su ) self . synthesised = True return [ ent , arch ]", "predictions": ["create and register a method ."], "references": ["build entity and architecture instance out of netlist representation"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2395, "code": "def get Max Stm Id For Stm ( stm ) : max Id = 0 if isinstance ( stm , Assignment ) : return stm . inst Id elif isinstance ( stm , Wait Stm ) : return max Id else : for stm in stm . iter stms ( ) : max Id = max ( max Id , get Max Stm Id For Stm ( stm ) ) return max Id", "predictions": ["get a stm for all the manage"], "references": ["get maximum _instid from all assigments in statement"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2396, "code": "def monitor ( self , sim ) : if self . not Reset ( sim ) and self . enabled : self . wr Rd ( sim . write , 1 ) yield sim . wait On Comb Update ( ) d = self . do Read ( sim ) self . data . append ( d ) else : self . wr Rd ( sim . write , 0 )", "predictions": ["monitor the monitor that can be run in the sim ."], "references": ["collect data from interface"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2397, "code": "def do Write ( self , sim , data ) : sim . write ( data , self . intf . data )", "predictions": ["run a commit operation on the sim card ."], "references": ["write data to interface"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2398, "code": "def driver ( self , sim ) : r = sim . read if self . actual Data is NOP and self . data : self . actual Data = self . data . popleft ( ) do = self . actual Data is not NOP if do : self . do Write ( sim , self . actual Data ) else : self . do Write ( sim , None ) en = self . not Reset ( sim ) and self . enabled if not ( en and do ) : return yield sim . wait On Comb Update ( ) rd = self . is Rd ( r ) if en : assert rd . vld Mask , ( ( \"%r: ready signal for interface %r is in invalid state,\" \" this would cause desynchronization\" ) % ( sim . now , self . intf ) ) if rd . val : if self . debug Output is not None : self . debug Output . write ( \"%s, wrote, %d: %r\\n\" % ( self . intf . get Full Name ( ) , sim . now , self . actual Data ) ) if self . data : self . actual Data = self . data . popleft ( ) else : self . actual Data = NOP", "predictions": ["call this method to yield new versions of the driver ."], "references": ["push data to interface"], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 2399, "code": "def get Physical Name ( self ) : if hasattr ( self , \" bounded Entity Port\" ) : return self . bounded Entity Port . name else : return self . get Full Name ( ) . replace ( '.' , self . NAME SEPARATOR )", "predictions": ["get the entity name from the entity ."], "references": ["get name in hdl"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 2400, "code": "def bit length ( self ) : try : interfaces = self . interfaces except Attribute Error : interfaces = None if interfaces is None : intf = self . clone ( ) intf . load Declarations ( ) interfaces = intf . interfaces if interfaces : w = 0 for i in interfaces : w += i . bit length ( ) return w else : return self . dtype . bit length ( )", "predictions": ["generate a child array of : this method is called by the contents of this class ."], "references": ["sum of all width of interfaces in this interface"], "bleu": 0.0859076483566362, "rouge_l": 0.2443257676902537}
{"id": 2401, "code": "def sensitivity By Op ( op ) : if op == All Ops . RISING EDGE : return SENSITIVITY . RISING elif op == All Ops . FALLING EDGE : return SENSITIVITY . FALLING else : raise Type Error ( )", "predictions": ["prepend by the __repr__ ."], "references": ["get sensitivity type for operator"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2402, "code": "def eval ( self , operator , simulator = None ) : def get Val ( v ) : while not isinstance ( v , Value ) : v = v . val return v operands = list ( map ( get Val , operator . operands ) ) if is Event Dependent Op ( operator . operator ) : operands . append ( simulator . now ) elif operator . operator == All Ops . Int To Bits : operands . append ( operator . result . dtype ) return self . eval Fn ( * operands )", "predictions": ["indent all the arguments for the given , and return the result ."], "references": ["load all operands and process them by self . _evalfn"], "bleu": 0.1135935489027116, "rouge_l": 0.2671532846715329}
{"id": 2403, "code": "def convert Bits ( self , sig Or Val , to Type ) : if isinstance ( sig Or Val , Value ) : return convert Bits val ( self , sig Or Val , to Type ) elif isinstance ( to Type , H Bool ) : if self . bit length ( ) == 1 : v = 0 if sig Or Val . dtype . negated else 1 return sig Or Val . eq ( self . get Value Cls ( ) . from Py ( v , self ) ) elif isinstance ( to Type , Bits ) : if self . bit length ( ) == to Type . bit length ( ) : return sig Or Val . conv Sign ( to Type . signed ) elif to Type == INT : return Operator . with Res ( All Ops . Bits To Int , [ sig Or Val ] , to Type ) return default auto cast fn ( self , sig Or Val , to Type )", "predictions": ["converts the given function value to a value ( as in \" dynamic - 1 \" ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["cast signed - unsigned to int or bool"], "bleu": 0.026594139297659906, "rouge_l": 0.03966189856957088}
{"id": 2404, "code": "def reinterpret bits to hstruct ( sig Or Val , h Struct T ) : container = h Struct T . from Py ( None ) offset = 0 for f in h Struct T . fields : t = f . dtype width = t . bit length ( ) if f . name is not None : s = sig Or Val [ ( width + offset ) : offset ] s = s . reinterpret cast ( t ) setattr ( container , f . name , s ) offset += width return container", "predictions": ["convert an view to a ("], "references": ["reinterpret signal of type bits to signal of type hstruct"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2405, "code": "def full Word Cnt ( self , start : int , end : int ) : assert end >= start , ( start , end ) gap = max ( 0 , ( end - start ) - ( start % self . word Width ) ) return gap // self . word Width", "predictions": ["calculates the do not include the do not include the bounds of the do not have a single angle ."], "references": ["count of complete words between two addresses"], "bleu": 0.06108557268562171, "rouge_l": 0.08111702127659574}
{"id": 2406, "code": "def discover sensitivity seq ( self , signals : List [ Rtl Signal Base ] , seen : set , ctx : Sensitivity Ctx ) -> None : casual Sensitivity = set ( ) for s in signals : s . walk sensitivity ( casual Sensitivity , seen , ctx ) if ctx . contains ev dependency : break if not ctx . contains ev dependency : ctx . extend ( casual Sensitivity )", "predictions": ["compute original min and dependency object ."], "references": ["discover sensitivity for list of signals"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2407, "code": "def get rtl context ( self ) : for sig in chain ( self . inputs , self . outputs ) : if sig . ctx : return sig . ctx else : continue raise Hwt Syntax Error ( \"Statement does not have any signal in any context\" , self )", "predictions": ["iter the signal method for each signal ."], "references": ["get rtlnetlist context from signals"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2408, "code": "def is mergable statement list ( cls , stms A , stms B ) : if stms A is None and stms B is None : return True elif stms A is None or stms B is None : return False a it = iter ( stms A ) b it = iter ( stms B ) a = get stm with branches ( a it ) b = get stm with branches ( b it ) while a is not None or b is not None : if a is None or b is None or not a . is mergable ( b ) : return False a = get stm with branches ( a it ) b = get stm with branches ( b it ) return True", "predictions": ["returns true if this is a ( i . e . word neighbors neighbors neighbors neighbors neighbors neighbors neighbors neighbors neighbors neighbors neighbors . a . e . a and c neighbors of the instances of the instances of the arrays ."], "references": ["walk statements and compare if they can be merged into one statement list"], "bleu": 0.03172414419318193, "rouge_l": 0.04018445322793149}
{"id": 2409, "code": "def try reduce list ( statements : List [ \"Hdl Statement\" ] ) : io change = False new statements = [ ] for stm in statements : reduced , io change = stm . try reduce ( ) new statements . extend ( reduced ) io change |= io change new statements , rank decrease = Hdl Statement . merge statements ( new statements ) return new statements , rank decrease , io change", "predictions": ["appends a single ( possibly null getattr getattr getattr getattr getattr getattr getattr getattr getattr . getattr getattr getattr getattr for each 8 . getattr getattr getattr ."], "references": ["simplify statements in the list"], "bleu": 0.03639374222382004, "rouge_l": 0.0}
{"id": 2410, "code": "def set parent stm ( self , parent Stm : \"Hdl Statement\" ) : was top = self . parent Stm is None self . parent Stm = parent Stm if not self . now is event dependent and parent Stm . now is event dependent : self . on parent event dependent ( ) top Statement = parent Stm while top Statement . parent Stm is not None : top Statement = top Statement . parent Stm parent out add = top Statement . outputs . append parent in add = top Statement . inputs . append if was top : for inp in self . inputs : inp . endpoints . discard ( self ) inp . endpoints . append ( top Statement ) parent in add ( inp ) for outp in self . outputs : outp . drivers . discard ( self ) outp . drivers . append ( top Statement ) parent out add ( outp ) ctx = self . get rtl context ( ) ctx . statements . discard ( self ) parent Stm . rank += self . rank", "predictions": ["for each ."], "references": ["assign parent statement and propagate dependency flags if necessary"], "bleu": 0.06114461654585454, "rouge_l": 0.0}
{"id": 2411, "code": "def sig ( self , name , dtype = BIT , def Val = None ) : if isinstance ( dtype , H Struct ) : if def Val is not None : raise Not Implemented Error ( ) container = dtype . from Py ( None ) for f in dtype . fields : if f . name is not None : r = self . sig ( \"%s %s\" % ( name , f . name ) , f . dtype ) setattr ( container , f . name , r ) return container return self . ctx . sig ( name , dtype = dtype , def Val = def Val )", "predictions": ["convert a binary operation to the specified declaration ."], "references": ["create signal in this unit"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2412, "code": "def clean As Subunit ( self ) : for pi in self . entity . ports : pi . connect Intern Sig ( ) for i in chain ( self . interfaces , self . private interfaces ) : i . clean ( )", "predictions": ["removes all ) tuples from this entity as a result of ( ."], "references": ["disconnect internal signals so unit can be reused by parent unit"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2413, "code": "def walk Flatten Fields ( sig Or Val , skip Padding = True ) : t = sig Or Val . dtype if isinstance ( t , Bits ) : yield sig Or Val elif isinstance ( t , H Union ) : yield from walk Flatten Fields ( sig Or Val . val , skip Padding = skip Padding ) elif isinstance ( t , H Struct ) : for f in t . fields : is Padding = f . name is None if not is Padding or not skip Padding : if is Padding : v = f . dtype . from Py ( None ) else : v = getattr ( sig Or Val , f . name ) yield from walk Flatten Fields ( v ) elif isinstance ( t , H Array ) : for item in sig Or Val : yield from walk Flatten Fields ( item ) else : raise Not Implemented Error ( t )", "predictions": ["slice the function to return all values from the given function ."], "references": ["walk all simple values in hstruct or harray"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 2414, "code": "def sensitivity ( proc : HW Process , * sensitive To ) : for s in sensitive To : if isinstance ( s , tuple ) : sen , s = s if sen == SENSITIVITY . ANY : s . sim Sens Procs . add ( proc ) elif sen == SENSITIVITY . RISING : s . sim Rising Sens Procs . add ( proc ) elif sen == SENSITIVITY . FALLING : s . sim Falling Sens Procs . add ( proc ) else : raise Assertion Error ( sen ) else : s . sim Sens Procs . add ( proc )", "predictions": ["prepares an instance of find ignoring lat/lon ."], "references": ["register sensitivity for process"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2415, "code": "def sim Eval Cond ( simulator , * conds ) : cond = True vld = True for v in conds : val = bool ( v . val ) full Vld = v . vld Mask == 1 if full Vld : if not val : return False , True else : return False , False cond = cond and val vld = vld and full Vld return cond , vld", "predictions": ["this function returns the is done in a set of states ."], "references": ["evaluate list of values as condition"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 2416, "code": "def connect Sim Port ( sim Unit , sub Sim Unit , src Name , dst Name , direction ) : if direction == DIRECTION . OUT : orig Port = getattr ( sub Sim Unit , src Name ) new Port = getattr ( sim Unit , dst Name ) setattr ( sub Sim Unit , src Name , new Port ) else : orig Port = getattr ( sub Sim Unit , dst Name ) new Port = getattr ( sim Unit , src Name ) setattr ( sub Sim Unit , dst Name , new Port ) sub Sim Unit . ctx . signals . remove ( orig Port )", "predictions": ["connect connects to the specified port ."], "references": ["connect ports of simulation models by name"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2417, "code": "def vec ( val , width , signed = None ) : return Bits ( width , signed , force Vector = True ) . from Py ( val )", "predictions": ["print out the specified * * * * * * * * * * * pi ."], "references": ["create hdl vector value"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 2418, "code": "def monitor ( self , sim ) : r = sim . read if self . not Reset ( sim ) : if self . last Rd is not 1 : self . wr Rd ( sim . write , 1 ) self . last Rd = 1 try : on Monitor Ready = self . on Monitor Ready except Attribute Error : on Monitor Ready = None if on Monitor Ready is not None : on Monitor Ready ( sim ) yield sim . wait On Comb Update ( ) vld = self . is Vld ( r ) assert vld . vld Mask , ( sim . now , self . intf , \"vld signal is in invalid state\" ) if vld . val : d = self . do Read ( sim ) if self . debug Output is not None : self . debug Output . write ( \"%s, read, %d: %r\\n\" % ( self . intf . get Full Name ( ) , sim . now , d ) ) self . data . append ( d ) if self . after Read is not None : self . after Read ( sim ) else : if self . last Rd is not 0 : self . wr Rd ( sim . write , 0 ) self . last Rd = 0", "predictions": ["monitor only the completion of this process ."], "references": ["collect data from interface"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2419, "code": "def HW Process ( cls , proc : HW Process , ctx : Resource Context ) -> None : seen = ctx . seen for stm in proc . statements : encl = stm . enclosed for full ev dep = stm . is completly event dependent now ev dep = stm . now is event dependent ev dep = full ev dep or now ev dep out mux dim = count mux inputs for outputs ( stm ) for o in stm . outputs : if o in seen : continue i = out mux dim [ o ] if isinstance ( o . dtype , H Array ) : assert i == 1 , ( o , i , \" only one ram port per HW Process\" ) for a in walk assignments ( stm , o ) : assert len ( a . indexes ) == 1 , \"one address per RAM port\" addr = a . indexes [ 0 ] ctx . register RAM write port ( o , addr , ev dep ) elif ev dep : ctx . register FF ( o ) if i > 1 : ctx . register MUX ( stm , o , i ) elif o not in encl : ctx . register Latch ( o ) if i > 1 : ctx . register MUX ( stm , o , i ) elif i > 1 : ctx . register MUX ( stm , o , i ) else : continue if isinstance ( stm , Switch Container ) : case Eqs = set ( [ stm . switch On . eq ( c [ 0 ] ) for c in stm . cases ] ) inputs = chain ( [ sig for sig in stm . inputs if sig not in case Eqs ] , [ stm . switch On ] ) else : inputs = stm . inputs for i in inputs : if not i . hidden or i in seen : continue cls . HW Process operators ( i , ctx , ev dep )", "predictions": ["copy the before and after obtaining the before obtaining ("], "references": ["gues resource usage by hwprocess"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2420, "code": "def eval Param ( p ) : while isinstance ( p , Param ) : p = p . get ( ) if isinstance ( p , Rtl Signal Base ) : return p . static Eval ( ) return to H Val ( p )", "predictions": ["evaluate a value ( ie , unless the value is parameter , then the value is set , then the parameter is set , and the value is allocated , or point is allocated ."], "references": ["get value of parameter"], "bleu": 0.03816712639899379, "rouge_l": 0.1197252208047105}
{"id": 2421, "code": "def set ( self , val ) : assert not self . is Read Only , ( \"This parameter(%s) was locked\" \" and now it can not be changed\" % self . name ) assert self . replaced With is None , ( \"This param was replaced with new one and this \" \"should not exists\" ) val = to H Val ( val ) self . def Val = val self . val = val . static Eval ( ) self . dtype = self . val . dtype", "predictions": ["checks if setting is set ."], "references": ["set value of this param"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2422, "code": "def finalize ( self ) : ff to remove = 0 res = self . resources for m , addr Dict in self . memories . items ( ) : rw Sync Ports , r Sync Ports , w Sync Ports = 0 , 0 , 0 rw Async Ports , r Async Ports , w Async Ports = 0 , 0 , 0 r Sync w Async Ports , r Async w Sync Ports = 0 , 0 for , ( r Sync , w Sync , r Async , w Async ) in addr Dict . items ( ) : if r Sync : ff to remove += r Sync * m . dtype . elm Type . bit length ( ) rw Sync = min ( r Sync , w Sync ) r Sync -= rw Sync w Sync -= rw Sync rw Async = min ( r Async , w Async ) r Async -= rw Async w Async -= rw Async r Sync w Async = min ( r Sync , w Async ) r Sync -= r Sync w Async w Async -= r Sync w Async r Async w Sync = min ( r Async , w Sync ) r Async -= r Async w Sync w Sync -= r Async w Sync rw Sync Ports += rw Sync r Sync Ports += r Sync w Sync Ports += w Sync rw Async Ports += rw Async r Async Ports += r Async w Async Ports += w Async r Sync w Async Ports += r Sync w Async r Async w Sync Ports += r Async w Sync k = Resource RAM ( m . dtype . elm Type . bit length ( ) , int ( m . dtype . size ) , rw Sync Ports , r Sync Ports , w Sync Ports , r Sync w Async Ports , rw Async Ports , r Async Ports , w Async Ports , r Async w Sync Ports ) res [ k ] = res . get ( k , 0 ) + 1 self . memories . clear ( ) if ff to remove : ff cnt = res [ Resource FF ] ff cnt -= ff to remove if ff cnt : res [ Resource FF ] = ff cnt else : del res [ Resource FF ]", "predictions": ["note : this method is called before any . thread ."], "references": ["resolve ports of discovered memories"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2423, "code": "def eq ( self , other ) : return self . nary Op ( All Ops . EQ , tv ( self ) . eq , other )", "predictions": ["a method for making this class loader ."], "references": ["__eq__ is not overloaded because it will destroy hashability of object"], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 2424, "code": "def get Index Cascade ( self ) : try : d = self . single Driver ( ) try : op = d . operator except Attribute Error : return if op == All Ops . INDEX : indexed On = d . operands [ 0 ] if isinstance ( indexed On , Rtl Signal Base ) : return indexed On , [ d . operands [ 1 ] ] else : raise Exception ( \"can not drive static value %r\" % indexed On ) except ( Multiple Drivers Err , No Driver Err ) : pass", "predictions": ["name for drive that are has index but not error ."], "references": ["find out if this signal is something indexed"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2425, "code": "def walk Params ( intf , discovered ) : for si in intf . interfaces : yield from walk Params ( si , discovered ) for p in intf . params : if p not in discovered : discovered . add ( p ) yield p", "predictions": ["cut all contexts in this group ."], "references": ["walk parameter instances on this interface"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2426, "code": "def register Intf In Impl ( self , i Name , intf ) : self . register Interface ( i Name , intf , is Private = True ) self . load Interface ( intf , False ) intf . signals For Interface ( self . ctx )", "predictions": ["registers full full synthesize interface ."], "references": ["register interface in implementation phase"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2427, "code": "def get Base Name Scope ( cls ) : s = Name Scope ( False ) s . set Level ( 1 ) s [ 0 ] . update ( cls . keywords dict ) return s", "predictions": ["get name for given class ."], "references": ["get root of name space"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 2428, "code": "def get Base Cond ( c ) : is Negated = False try : drivers = c . drivers except Attribute Error : return ( c , is Negated ) if len ( drivers ) == 1 : d = list ( c . drivers ) [ 0 ] if isinstance ( d , Operator ) and d . operator == All Ops . NOT : c = d . operands [ 0 ] is Negated = True return ( c , is Negated )", "predictions": ["get the list of operands from the given guice ."], "references": ["if is negated return original cond and negated flag"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2429, "code": "def sim Bits T ( width : int , signed : Union [ bool , None ] ) : k = ( width , signed ) try : return sim Bits T Cache [ k ] except Key Error : t = Sim Bits T ( width , signed ) sim Bits T Cache [ k ] = t return t", "predictions": ["do the real work of zipline zipline operation ."], "references": ["construct simbitst with cache"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2430, "code": "def cut off drivers of ( self , sig : Rtl Signal Base ) : if self . dst is sig : self . parent Stm = None return self else : return None", "predictions": ["returns the multi - level combined combined between this class and the given equals method ."], "references": ["cut off statements which are driver of specified signal"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2431, "code": "def load From H Type ( self , dtype : Hdl Type , bit Addr : int ) -> None : self . bit Addr = bit Addr children Are Choice = False if isinstance ( dtype , Bits ) : ld = self . load From Bits elif isinstance ( dtype , H Struct ) : ld = self . load From H Struct elif isinstance ( dtype , H Array ) : ld = self . load From Array elif isinstance ( dtype , H Stream ) : ld = self . load From H Stream elif isinstance ( dtype , H Union ) : ld = self . load From Union children Are Choice = True else : raise Type Error ( \"expected instance of Hdl Type\" , dtype ) self . bit Addr End = ld ( dtype , bit Addr ) self . children Are Choice = children Are Choice", "predictions": ["get a , from a process ."], "references": ["parse any hdl type to this transaction template instance"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 2432, "code": "def sign Fix ( val , width ) : if val > 0 : msb = 1 << ( width - 1 ) if val & msb : val -= mask ( width ) + 1 return val", "predictions": ["sign the value with the specified width ."], "references": ["convert negative int to positive int which has same bits set"], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 2433, "code": "def merge with other stm ( self , other : \"If Container\" ) -> None : merge = self . merge statement lists new Cases = [ ] for ( c , case A ) , ( , case B ) in zip ( self . cases , other . cases ) : new Cases . append ( ( c , merge ( case A , case B ) ) ) self . cases = new Cases if self . default is not None : self . default = merge ( self . default , other . default ) self . on merge ( other )", "predictions": ["merge this one with another . other . other . other . use the same way as the other ."], "references": ["merge other statement to this statement"], "bleu": 0.07264339766175722, "rouge_l": 0.17039106145251398}
{"id": 2434, "code": "def get Indent ( indent Num ) : try : return indent Cache [ indent Num ] except Key Error : i = \"\" . join ( [ indent for in range ( indent Num ) ] ) indent Cache [ indent Num ] = i return i", "predictions": ["interpolates the indentation level ."], "references": ["cached indent getter function"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2435, "code": "def verilog Type Of Sig ( signal Item ) : driver cnt = len ( signal Item . drivers ) if signal Item . const or driver cnt > 1 or arr any ( signal Item . drivers , is Event Dependent Driver ) : return SIGNAL TYPE . REG else : if driver cnt == 1 : d = signal Item . drivers [ 0 ] if not isinstance ( d , ( Assignment , Port Item ) ) : return SIGNAL TYPE . REG return SIGNAL TYPE . WIRE", "predictions": ["verilog which get driver by signal ."], "references": ["check if is register or wire"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2436, "code": "def name Availability Check ( obj , prop Name , prop ) : if getattr ( obj , prop Name , None ) is not None : raise Intf Lvl Conf Err ( \"%r already has property %s old:%s new:%s\" % ( obj , prop Name , repr ( getattr ( obj , prop Name ) ) , prop ) )", "predictions": ["check if the given object is a synchronization constraint ."], "references": ["check if not redefining property on obj"], "bleu": 0.16590387014219712, "rouge_l": 0.24302788844621517}
{"id": 2437, "code": "def register Parameter ( self , p Name , parameter ) -> None : name Availability Check ( self , p Name , parameter ) try : has Name = parameter . name is not None except Attribute Error : has Name = False if not has Name : parameter . name = p Name parameter . register Scope ( p Name , self ) if parameter . has Generic Name : parameter . name = p Name if parameter . parent is None : parameter . parent = self self . params . append ( parameter )", "predictions": ["registers a class with its name ."], "references": ["register param object on interface level object"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2438, "code": "def register Unit ( self , u Name , unit ) : name Availability Check ( self , u Name , unit ) assert unit . parent is None unit . parent = self unit . name = u Name self . units . append ( unit )", "predictions": ["registers extension at runtime unit ."], "references": ["register unit object on interface level object"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2439, "code": "def register Interface ( self , i Name , intf , is Private = False ) : name Availability Check ( self , i Name , intf ) assert intf . parent is None intf . parent = self intf . name = i Name intf . ctx = self . ctx if is Private : self . private interfaces . append ( intf ) intf . is Extern = False else : self . interfaces . append ( intf ) intf . is Extern = True", "predictions": ["saves saves full name to this object ."], "references": ["register interface object on interface level object"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2440, "code": "def register Array ( self , name , items ) : items . parent = self items . name = name for i , item in enumerate ( items ) : setattr ( self , \"%s %d\" % ( name , i ) , item )", "predictions": ["plugin registration with the specified items ."], "references": ["register array of items on interface level object"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2441, "code": "def single Driver ( self ) : drv cnt = len ( self . drivers ) if not drv cnt : raise No Driver Err ( self ) elif drv cnt != 1 : raise Multiple Drivers Err ( self ) return self . drivers [ 0 ]", "predictions": ["a single single single likelihoods operation ."], "references": ["returns a first driver if signal has only one driver ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2442, "code": "def static Eval ( self ) : for o in self . operands : o . static Eval ( ) self . result . val = self . eval Fn ( )", "predictions": ["object ."], "references": ["recursively statistically evaluate result of this operator"], "bleu": 0.0524476438328049, "rouge_l": 0.0}
{"id": 2443, "code": "def with Indent ( self , indent = 1 ) : ctx = copy ( self ) ctx . indent += indent return ctx", "predictions": ["evaluates the current indentation level ."], "references": ["create copy of this context with increased indent"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 2444, "code": "def propagate Clk ( obj ) : clk = obj . clk for u in obj . units : try Connect ( clk , u , 'clk' )", "predictions": ["remove all unnecessary unnecessary object from the given object ."], "references": ["propagate clk clock signal to all subcomponents"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2445, "code": "def propagate Clk Rst ( obj ) : clk = obj . clk rst = obj . rst for u in obj . units : try Connect ( clk , u , 'clk' ) try Connect ( ~ rst , u , 'rst n' ) try Connect ( rst , u , 'rst' )", "predictions": ["performs a series of ( objects ."], "references": ["propagate clk clock and reset rst signal to all subcomponents"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 2446, "code": "def get Full Name ( self ) : name = \"\" tmp = self while isinstance ( tmp , ( Interface Base , H Obj List ) ) : if hasattr ( tmp , \" name\" ) : n = tmp . name else : n = '' if name == '' : name = n else : name = n + '.' + name if hasattr ( tmp , \" parent\" ) : tmp = tmp . parent else : tmp = None return name", "predictions": ["get full name of this class ."], "references": ["get all name hierarchy separated by ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 2447, "code": "def on T Write Callback init ( self , sim ) : yield from self . on T Write Callback ( sim ) self . intf . t . sig Inside . register Write Callback ( self . on T Write Callback , self . get Enable ) self . intf . o . sig Inside . register Write Callback ( self . on T Write Callback , self . get Enable )", "predictions": ["registers this object to be completely loaded ."], "references": ["process for injecting of this callback loop into simulator"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2448, "code": "def connect Sig ( self , signal ) : if self . direction == DIRECTION . IN : if self . src is not None : raise Hwt Syntax Error ( \"Port %s is already associated with %r\" % ( self . name , self . src ) ) self . src = signal signal . endpoints . append ( self ) elif self . direction == DIRECTION . OUT : if self . dst is not None : raise Hwt Syntax Error ( \"Port %s is already associated with %r\" % ( self . name , self . dst ) ) self . dst = signal signal . drivers . append ( self ) else : raise Not Implemented Error ( self ) signal . hidden = False signal . ctx . sub Units . add ( self . unit )", "predictions": ["connect to the model"], "references": ["connect to port item on subunit"], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 2449, "code": "def connect Intern Sig ( self ) : d = self . direction if d == DIRECTION . OUT : self . src . endpoints . append ( self ) elif d == DIRECTION . IN or d == DIRECTION . INOUT : self . dst . drivers . append ( self ) else : raise Not Implemented Error ( d )", "predictions": ["connect to a full logical problem ."], "references": ["connet signal from internal side of of this component to this port"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 2450, "code": "def get Intern Sig ( self ) : d = self . direction if d == DIRECTION . IN : return self . dst elif d == DIRECTION . OUT : return self . src else : raise Not Implemented Error ( d )", "predictions": ["description of the method"], "references": ["return signal inside unit which has this port"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 2451, "code": "def is Ev Dependent On ( sig , process ) -> bool : if sig is None : return False return process in sig . sim Falling Sens Procs or process in sig . sim Rising Sens Procs", "predictions": ["return true if this process is on the sim process ."], "references": ["check if hdl process has event depenency on signal"], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 2452, "code": "def add process ( self , proc , priority ) -> None : self . events . push ( self . now , priority , proc )", "predictions": ["process a single process ."], "references": ["schedule process on actual time with specified priority"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2453, "code": "def schedule Apply Values ( self ) -> None : assert not self . apply Val Planed , self . now self . add process ( self . apply Values ( ) , PRIORITY APPLY COMB ) self . apply Val Planed = True if self . run Seq Processes Planed : return assert not self . seq Procs To Run and not self . run Seq Processes Planed , self . now self . add process ( self . run Seq Processes ( ) , PRIORITY APPLY SEQ ) self . run Seq Processes Planed = True", "predictions": ["runs a previously saved chance ."], "references": ["apply stashed values to signals"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2454, "code": "def run Comb Processes ( self ) -> None : for proc in self . comb Procs To Run : cont = self . output Containers [ proc ] proc ( self , cont ) for sig Name , sig in cont . all signals : new Val = getattr ( cont , sig Name ) if new Val is not None : res = self . conflict Resolve Strategy ( new Val ) updater , is Ev Dependent = res self . values To Apply . append ( ( sig , updater , is Ev Dependent , proc ) ) setattr ( cont , sig Name , None ) self . comb Procs To Run = Uniq List ( )", "predictions": ["this executes all the signals with the same name as those in this python and executes them with the same name ."], "references": ["delta step for combinational processes"], "bleu": 0.04657469807170698, "rouge_l": 0.0}
{"id": 2455, "code": "def run Seq Processes ( self ) -> Generator [ None , None , None ] : updates = [ ] for proc in self . seq Procs To Run : try : out Container = self . output Containers [ proc ] except Key Error : out Container = None proc ( self , out Container ) if out Container is not None : updates . append ( out Container ) self . seq Procs To Run = Uniq List ( ) self . run Seq Processes Planed = False for cont in updates : for sig Name , sig in cont . all signals : new Val = getattr ( cont , sig Name ) if new Val is not None : v = self . conflict Resolve Strategy ( new Val ) updater , = v sig . sim Update Val ( self , updater ) setattr ( cont , sig Name , None ) return yield", "predictions": ["builds the sequence of . ."], "references": ["delta step for event dependent processes"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2456, "code": "def apply Values ( self ) -> Generator [ None , None , None ] : va = self . values To Apply self . apply Val Planed = False lav = self . config . log Applying Values if va and lav : lav ( self , va ) self . values To Apply = [ ] add Sp = self . seq Procs To Run . append for s , v Updater , is Event Dependent , comes From in va : if is Event Dependent : add Sp ( comes From ) else : s . sim Update Val ( self , v Updater ) self . run Comb Processes ( ) if self . values To Apply and not self . apply Val Planed : self . schedule Apply Values ( ) return yield", "predictions": ["this function returns a generator of all the projects that are applied to the internal generator ."], "references": ["perform delta step by writing stacked values to signals"], "bleu": 0.07223943354597204, "rouge_l": 0.0814419225634179}
{"id": 2457, "code": "def read ( self , sig ) -> Value : try : v = sig . val except Attribute Error : v = sig . sig Inside . val return v . clone ( )", "predictions": ["read the -> edge from the socket ."], "references": ["read value from signal or interface"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 2458, "code": "def write ( self , val , sig : Sim Signal ) -> None : try : sim Sens Procs = sig . sim Sens Procs except Attribute Error : sig = sig . sig Inside sim Sens Procs = sig . sim Sens Procs t = sig . dtype if isinstance ( val , Value ) : v = val . clone ( ) v = v . auto cast ( t ) else : v = t . from Py ( val ) sig . sim Update Val ( self , lambda curent V : ( value Has Changed ( curent V , v ) , v ) ) if not self . apply Val Planed : if not ( sim Sens Procs or sig . sim Rising Sens Procs or sig . sim Falling Sens Procs ) : self . schedule Apply Values ( ) elif ( sig . write Callbacks or sig . write Callbacks To En ) : self . schedule Apply Values ( )", "predictions": ["write data ."], "references": ["write value to signal or interface ."], "bleu": 0.15673579607078858, "rouge_l": 0.3730886850152905}
{"id": 2459, "code": "def add process ( self , proc ) -> None : self . events . push ( self . now , PRIORITY NORMAL , proc )", "predictions": ["helper method for creating a single process ."], "references": ["add process to events with default priority on current time"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 2460, "code": "def sim Unit ( self , synthesised Unit : Unit , until : float , extra Processes = [ ] ) : before Sim = self . config . before Sim if before Sim is not None : before Sim ( self , synthesised Unit ) add proc = self . add process for p in extra Processes : add proc ( p ( self ) ) self . init Unit Signals ( synthesised Unit ) self . run ( until )", "predictions": ["creates new unit unit unit ."], "references": ["run simulation for unit instance"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2461, "code": "def system C Type Of Sig ( signal Item ) : if signal Item . const or arr any ( signal Item . drivers , lambda d : isinstance ( d , Hdl Statement ) and d . now is event dependent ) : return SIGNAL TYPE . REG else : return SIGNAL TYPE . WIRE", "predictions": ["return the system dependent or null if it is a byte or an item ."], "references": ["check if is register or wire"], "bleu": 0.09782375748961449, "rouge_l": 0.3096446700507614}
{"id": 2462, "code": "def ternary Ops To If ( statements ) : stms = [ ] for st in statements : if isinstance ( st , Assignment ) : try : if not isinstance ( st . src , Rtl Signal Base ) : raise Does Not Contains Ternary ( ) d = st . src . single Driver ( ) if not isinstance ( d , Operator ) or d . operator != All Ops . TERNARY : raise Does Not Contains Ternary ( ) else : ops = d . operands ifc = If Container ( ops [ 0 ] , [ Assignment ( ops [ 1 ] , st . dst ) ] , [ Assignment ( ops [ 2 ] , st . dst ) ] ) stms . append ( ifc ) continue except ( Multiple Drivers Err , Does Not Contains Ternary ) : pass except No Driver Err : assert ( hasattr ( st . src , \" interface\" ) and st . src . interface is not None ) or st . src . def Val . vld Mask , st . src stms . append ( st ) return stms", "predictions": ["appends this method to the ternary if it is a single connectable or a single connectable connectable ."], "references": ["convert all ternary operators to ifcontainers"], "bleu": 0.07535838128770536, "rouge_l": 0.09159159159159158}
{"id": 2463, "code": "def hash distance ( left hash , right hash ) : if len ( left hash ) != len ( right hash ) : raise Value Error ( 'Hamming distance requires two strings of equal length' ) return sum ( map ( lambda x : 0 if x [ 0 ] == x [ 1 ] else 1 , zip ( left hash , right hash ) ) )", "predictions": ["calculates the hash code for this stroke ."], "references": ["compute the hamming distance between two hashes"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2464, "code": "def average hash ( image path , hash size = 8 ) : with open ( image path , 'rb' ) as f : image = Image . open ( f ) . resize ( ( hash size , hash size ) , Image . ANTIALIAS ) . convert ( 'L' ) pixels = list ( image . getdata ( ) ) avg = sum ( pixels ) / len ( pixels ) bits = \"\" . join ( map ( lambda pixel : '1' if pixel > avg else '0' , pixels ) ) hashformat = \"0{hashlength}x\" . format ( hashlength = hash size ** 2 // 4 ) return int ( bits , 2 ) . format ( hashformat )", "predictions": ["sign an val for a match ."], "references": ["compute the average hash of the given image ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2465, "code": "def distance ( image path , other image path ) : image hash = average hash ( image path ) other image hash = average hash ( other image path ) return hash distance ( image hash , other image hash )", "predictions": ["-> merge the merge between two bigintegers ."], "references": ["compute the hamming distance between two images"], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 2466, "code": "def setup platform ( hass , config , add entities , discovery info = None ) : host = config . get ( CONF HOST ) token = config . get ( CONF ACCESS TOKEN ) name = config . get ( CONF NAME ) volume step = config . get ( CONF VOLUME STEP ) device type = config . get ( CONF DEVICE CLASS ) device = Vizio Device ( host , token , name , volume step , device type ) if device . validate setup ( ) is False : LOGGER . error ( \"Failed to set up Vizio platform, \" \"please check if host and API key are correct\" ) return elif ( token is None or token == \"\" ) and device type == \"tv\" : LOGGER . error ( \"Failed to set up Vizio platform, \" \"if device class is 'tv' then an auth token needs \" \"to be provided, otherwise if device class is \" \"'soundbar' then add the right device class to config\" ) return if config . get ( CONF SUPPRESS WARNING ) : from requests . packages import urllib3 LOGGER . warning ( \"Insecure Request Warning is disabled \" \"because of Vizio platform configuration\" ) urllib3 . disable warnings ( urllib3 . exceptions . Insecure Request Warning ) add entities ( [ device ] , True )", "predictions": ["set up the ( sensor ."], "references": ["set up the vizio media player platform ."], "bleu": 0.31149111610852515, "rouge_l": 0.5570776255707762}
{"id": 2467, "code": "def update ( self ) : is on = self . device . get power state ( ) if is on : self . state = STATE ON volume = self . device . get current volume ( ) if volume is not None : self . volume level = float ( volume ) / self . max volume input = self . device . get current input ( ) if input is not None : self . current input = input . meta name inputs = self . device . get inputs ( ) if inputs is not None : self . available inputs = [ input . name for input in inputs ] else : if is on is None : self . state = None else : self . state = STATE OFF self . volume level = None self . current input = None self . available inputs = None", "predictions": ["update are set to some other interceptor ."], "references": ["retrieve latest state of the device ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2468, "code": "def mute volume ( self , mute ) : if mute : self . device . mute on ( ) else : self . device . mute off ( )", "predictions": ["creates a name based on the name of the volume ."], "references": ["mute the volume ."], "bleu": 0.21200626759025185, "rouge_l": 0.43675417661097854}
{"id": 2469, "code": "def volume up ( self ) : self . volume level += self . volume step / self . max volume self . device . vol up ( num = self . volume step )", "predictions": ["the register of ( ."], "references": ["increasing volume of the device ."], "bleu": 0.2658156069371863, "rouge_l": 0.3577712609970674}
{"id": 2470, "code": "def volume down ( self ) : self . volume level -= self . volume step / self . max volume self . device . vol down ( num = self . volume step )", "predictions": ["down the register of a register ."], "references": ["decreasing volume of the device ."], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 2471, "code": "def set volume level ( self , volume ) : if self . volume level is not None : if volume > self . volume level : num = int ( self . max volume * ( volume - self . volume level ) ) self . volume level = volume self . device . vol up ( num = num ) elif volume < self . volume level : num = int ( self . max volume * ( self . volume level - volume ) ) self . volume level = volume self . device . vol down ( num = num )", "predictions": ["register stat ( see shutdownperm ) . . ) ) ) ) ) . java . awt . commons . commons . . . commons . commons . commons . commons . commons . commons . commons . commons . commons . commons . ( . commons . . ."], "references": ["set volume level ."], "bleu": 0.02403051755364481, "rouge_l": 0.04375896700143472}
{"id": 2472, "code": "def reset ( self ) : self . piece bb = [ BB VOID , BB RANK C | BB RANK G , BB A1 | BB I1 | BB A9 | BB I9 , BB A2 | BB A8 | BB I2 | BB I8 , BB A3 | BB A7 | BB I3 | BB I7 , BB A4 | BB A6 | BB I4 | BB I6 , BB B2 | BB H8 , BB B8 | BB H2 , BB A5 | BB I5 , BB VOID , BB VOID , BB VOID , BB VOID , BB VOID , BB VOID , ] self . pieces in hand = [ collections . Counter ( ) , collections . Counter ( ) ] self . occupied = Occupied ( BB RANK G | BB H2 | BB H8 | BB RANK I , BB RANK A | BB B2 | BB B8 | BB RANK C ) self . king squares = [ I5 , A5 ] self . pieces = [ NONE for i in SQUARES ] for i in SQUARES : mask = BB SQUARES [ i ] for piece type in PIECE TYPES : if mask & self . piece bb [ piece type ] : self . pieces [ i ] = piece type self . turn = BLACK self . move number = 1 self . captured piece stack = collections . deque ( ) self . move stack = collections . deque ( ) self . incremental zobrist hash = self . board zobrist hash ( DEFAULT RANDOM ARRAY ) self . transpositions = collections . Counter ( ( self . zobrist hash ( ) , ) )", "predictions": ["resets and resets the decoder array ."], "references": ["restores the starting position ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 2473, "code": "def piece at ( self , square ) : mask = BB SQUARES [ square ] color = int ( bool ( self . occupied [ WHITE ] & mask ) ) piece type = self . piece type at ( square ) if piece type : return Piece ( piece type , color )", "predictions": ["create a single single single single single single single single single single single single single single single single single single single single single single single single single single value ."], "references": ["gets the piece at the given square ."], "bleu": 0.04034110170120257, "rouge_l": 0.05876685934489403}
{"id": 2474, "code": "def remove piece at ( self , square , into hand = False ) : piece type = self . piece type at ( square ) if piece type == NONE : return if into hand : self . add piece into hand ( piece type , self . turn ) mask = BB SQUARES [ square ] self . piece bb [ piece type ] ^= mask color = int ( bool ( self . occupied [ WHITE ] & mask ) ) self . pieces [ square ] = NONE self . occupied . ixor ( mask , color , square ) if color == BLACK : piece index = ( piece type - 1 ) * 2 else : piece index = ( piece type - 1 ) * 2 + 1 self . incremental zobrist hash ^= DEFAULT RANDOM ARRAY [ 81 * piece index + 9 * rank index ( square ) + file index ( square ) ]", "predictions": ["static adds an piece to the longest hash ."], "references": ["removes a piece from the given square if present ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 2475, "code": "def set piece at ( self , square , piece , from hand = False , into hand = False ) : if from hand : self . remove piece from hand ( piece . piece type , self . turn ) self . remove piece at ( square , into hand ) self . pieces [ square ] = piece . piece type mask = BB SQUARES [ square ] piece type = piece . piece type self . piece bb [ piece type ] |= mask if piece type == KING : self . king squares [ piece . color ] = square self . occupied . ixor ( mask , piece . color , square ) if piece . color == BLACK : piece index = ( piece . piece type - 1 ) * 2 else : piece index = ( piece . piece type - 1 ) * 2 + 1 self . incremental zobrist hash ^= DEFAULT RANDOM ARRAY [ 81 * piece index + 9 * rank index ( square ) + file index ( square ) ]", "predictions": ["generate a with incremental ."], "references": ["sets a piece at the given square . an existing piece is replaced ."], "bleu": 0.04994299940831281, "rouge_l": 0.19395866454689983}
{"id": 2476, "code": "def is checkmate ( self ) : if not self . is check ( ) : return False try : next ( self . generate legal moves ( ) . iter ( ) ) return False except Stop Iteration : return True", "predictions": ["returns true if this region is a checkmate ."], "references": ["checks if the current position is a checkmate ."], "bleu": 0.4111336169005197, "rouge_l": 0.5555555555555556}
{"id": 2477, "code": "def pop ( self ) : move = self . move stack . pop ( ) self . transpositions . subtract ( ( self . zobrist hash ( ) , ) ) self . move number -= 1 captured piece type = self . captured piece stack . pop ( ) captured piece color = self . turn if not move : self . turn ^= 1 return move piece type = self . piece type at ( move . to square ) if move . promotion : piece type = PIECE PROMOTED . index ( piece type ) if move . from square is None : self . add piece into hand ( piece type , self . turn ^ 1 ) else : self . set piece at ( move . from square , Piece ( piece type , self . turn ^ 1 ) ) if captured piece type : self . remove piece from hand ( captured piece type , captured piece color ^ 1 ) self . set piece at ( move . to square , Piece ( captured piece type , captured piece color ) ) else : self . remove piece at ( move . to square ) self . turn ^= 1 return move", "predictions": ["store in the = = null , ) ."], "references": ["restores the previous position and returns the last move from the stack ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 2478, "code": "def sfen ( self ) : sfen = [ ] empty = 0 for square in SQUARES : piece = self . piece at ( square ) if not piece : empty += 1 else : if empty : sfen . append ( str ( empty ) ) empty = 0 sfen . append ( piece . symbol ( ) ) if BB SQUARES [ square ] & BB FILE 1 : if empty : sfen . append ( str ( empty ) ) empty = 0 if square != I1 : sfen . append ( '/' ) sfen . append ( ' ' ) if self . turn == WHITE : sfen . append ( 'w' ) else : sfen . append ( 'b' ) sfen . append ( ' ' ) pih len = 0 for color in COLORS : p = self . pieces in hand [ color ] pih len += len ( p ) for piece type in sorted ( p . keys ( ) , reverse = True ) : if p [ piece type ] >= 1 : if p [ piece type ] > 1 : sfen . append ( str ( p [ piece type ] ) ) piece = Piece ( piece type , color ) sfen . append ( piece . symbol ( ) ) if pih len == 0 : sfen . append ( '-' ) sfen . append ( ' ' ) sfen . append ( str ( self . move number ) ) return '' . join ( sfen )", "predictions": ["generate a get . this can be called multiple times ."], "references": ["gets an sfen representation of the current position ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 2479, "code": "def zobrist hash ( self , array = None ) : zobrist hash = self . board zobrist hash ( array ) if array is None : array = DEFAULT RANDOM ARRAY if self . turn == WHITE : zobrist hash ^= array [ 2268 ] i = ( self . pieces in hand [ BLACK ] [ ROOK ] * 35625 + self . pieces in hand [ BLACK ] [ BISHOP ] * 11875 + self . pieces in hand [ BLACK ] [ GOLD ] * 2375 + self . pieces in hand [ BLACK ] [ SILVER ] * 475 + self . pieces in hand [ BLACK ] [ KNIGHT ] * 95 + self . pieces in hand [ BLACK ] [ LANCE ] * 19 + self . pieces in hand [ BLACK ] [ PAWN ] ) bit = bit scan ( i ) while bit != - 1 and bit is not None : zobrist hash ^= array [ 2269 + bit ] bit = bit scan ( i , bit + 1 ) return zobrist hash", "predictions": ["this function generates a hash hash hash for each yield bit . this function uses this to do this to do this to do so we can compare the yield class ."], "references": ["returns a zobrist hash of the current position ."], "bleu": 0.04750133160738243, "rouge_l": 0.21708185053380782}
{"id": 2480, "code": "def symbol ( self ) : if self . color == BLACK : return PIECE SYMBOLS [ self . piece type ] . upper ( ) else : return PIECE SYMBOLS [ self . piece type ]", "predictions": [". for this object ."], "references": ["gets the symbol p l n etc ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2481, "code": "def load config from cli ( config : Good Conf , argv : List [ str ] ) -> List [ str ] : from django . core . management . base import Base Command original parser = Base Command . create parser def patched parser ( self , prog name , subcommand ) : parser = original parser ( self , prog name , subcommand ) argparser add argument ( parser , config ) return parser Base Command . create parser = patched parser try : parser = argparse . Argument Parser ( add help = False ) argparser add argument ( parser , config ) config arg , default args = parser . parse known args ( argv ) config . load ( config arg . config ) yield default args finally : Base Command . create parser = original parser", "predictions": ["loads the command line argument using the endpoints provided ."], "references": ["loads config checking cli arguments for a config file"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 2482, "code": "def execute from command line with config ( config : Good Conf , argv : List [ str ] ) : with load config from cli ( config , argv ) as args : from django . core . management import execute from command line execute from command line ( args )", "predictions": ["executes the management script ."], "references": ["load s config then runs django s execute_from_command_line"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 2483, "code": "def argparser add argument ( parser : argparse . Argument Parser , config : Good Conf ) : help = \"Config file.\" if config . file env var : help += ( \" Can also be configured via the \" \"environment variable: {}\" . format ( config . file env var ) ) if config . default files : help += ( \" Defaults to the first file that exists from \" \"[{}].\" . format ( ', ' . join ( config . default files ) ) ) parser . add argument ( '-C' , '--config' , metavar = 'FILE' , help = help )", "predictions": ["add ( process method that adds all arguments to the configuration object ."], "references": ["adds argument for config to existing argparser"], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 2484, "code": "def load ( self , filename : str = None ) : if filename : self . config file = find file ( filename ) else : if self . file env var and self . file env var in os . environ : self . config file = find file ( os . environ [ self . file env var ] ) if not self . config file : for filename in self . default files : self . config file = find file ( filename , require = False ) if self . config file : break if self . config file : config = load config ( self . config file ) log . info ( \"Loading config from %s\" , self . config file ) else : config = { } log . info ( \"No config file specified. \" \"Loading with environment variables.\" ) self . set values ( config )", "predictions": ["loads and sets the configuration files ."], "references": ["find config file and set values"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2485, "code": "def generate yaml ( cls , * * override ) : import ruamel . yaml yaml = ruamel . yaml . YAML ( ) yaml str = String IO ( ) yaml . dump ( cls . get initial ( * * override ) , stream = yaml str ) yaml str . seek ( 0 ) dict from yaml = yaml . load ( yaml str ) if cls . doc : dict from yaml . yaml set start comment ( '\\n' + cls . doc + '\\n\\n' ) for k in dict from yaml . keys ( ) : if cls . values [ k ] . help : dict from yaml . yaml set comment before after key ( k , before = '\\n' + cls . values [ k ] . help ) yaml str = String IO ( ) yaml . dump ( dict from yaml , yaml str ) yaml str . seek ( 0 ) return yaml str . read ( )", "predictions": ["schedule a yaml from the yaml ."], "references": ["dumps initial config in yaml"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2486, "code": "def generate markdown ( cls ) : lines = [ ] if cls . doc : lines . extend ( [ . format ( cls . doc ) , '' ] ) for k , v in cls . values . items ( ) : lines . append ( '* **{}**  ' . format ( k ) ) if v . required : lines [ - 1 ] = lines [ - 1 ] + ' REQUIRED   ' if v . help : lines . append ( '  {}  ' . format ( v . help ) ) lines . append ( '  type: `{}`  ' . format ( v . cast as . name ) ) if v . default is not None : lines . append ( '  default: `{}`  ' . format ( v . default ) ) return '\\n' . join ( lines )", "predictions": ["run all the deprecation markdown"], "references": ["documents values in markdown"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2487, "code": "def cast ( self , val : str ) : try : return getattr ( self , 'cast as {}' . format ( self . cast as . name . lower ( ) ) ) ( val ) except Attribute Error : return self . cast as ( val )", "predictions": ["method to support reverse the object on the accumulated object ."], "references": ["converts string to type requested by cast_as"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 2488, "code": "def list dates between ( first date , last date ) : return [ first date + timedelta ( days = n ) for n in range ( 1 + ( last date - first date ) . days ) ]", "predictions": ["returns the first period of the given -> -> -> -> -> -> -> -> -> -> -> list of . . . . . . . . . . . . . ."], "references": ["returns all dates from first to last included ."], "bleu": 0.042238868712683265, "rouge_l": 0.15587734241908005}
{"id": 2489, "code": "def parse date ( s ) : try : return datetime . date ( int ( s [ : 4 ] ) , int ( s [ 5 : 7 ] ) , int ( s [ 8 : 10 ] ) ) except Value Error : return datetime . datetime . strptime ( s , '%d %B %Y' ) . date ( )", "predictions": ["read the ( specified time - millisecond sig sig ."], "references": ["fast %y - %m - %d parsing ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 2490, "code": "def load file ( self , currency file ) : if currency file . startswith ( ( 'http://' , 'https://' ) ) : content = urlopen ( currency file ) . read ( ) else : with open ( currency file , 'rb' ) as f : content = f . read ( ) if currency file . endswith ( '.zip' ) : self . load lines ( get lines from zip ( content ) ) else : self . load lines ( content . decode ( 'utf-8' ) . splitlines ( ) )", "predictions": ["maps from given sig file to the given sig ."], "references": ["to be subclassed if alternate methods of loading data ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 2491, "code": "def set missing to none ( self , currency ) : rates = self . rates [ currency ] first date , last date = self . bounds [ currency ] for date in list dates between ( first date , last date ) : if date not in rates : rates [ date ] = None if self . verbose : missing = len ( [ r for r in itervalues ( rates ) if r is None ] ) if missing : print ( '{0}: {1} missing rates from {2} to {3} ({4} days)' . format ( currency , missing , first date , last date , 1 + ( last date - first date ) . days ) )", "predictions": ["convert a date object to a self - dimensional -> ( ."], "references": ["fill missing rates of a currency with the closest available ones ."], "bleu": 0.11498759556447223, "rouge_l": 0.16666666666666666}
{"id": 2492, "code": "def read record ( self , n ) : self . file . seek ( n * K - K ) return self . file . read ( K )", "predictions": ["sim the record to the specified file ."], "references": ["return record n as 1 024 bytes ; records are indexed from 1 ."], "bleu": 0.08383280652235028, "rouge_l": 0.1732954545454545}
{"id": 2493, "code": "def write record ( self , n , data ) : self . file . seek ( n * K - K ) return self . file . write ( data )", "predictions": ["system information for this file ."], "references": ["write data to file record n ; records are indexed from 1 ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 2494, "code": "def comments ( self ) : record numbers = range ( 2 , self . fward ) if not record numbers : return '' data = b'' . join ( self . read record ( n ) [ 0 : 1000 ] for n in record numbers ) try : return data [ : data . find ( b'\\4' ) ] . decode ( 'ascii' ) . replace ( '\\0' , '\\n' ) except Index Error : raise Value Error ( 'DAF file comment area is missing its EOT byte' ) except Unicode Decode Error : raise Value Error ( 'DAF file comment area is not ASCII text' )", "predictions": ["for each operands that has been replaced with the given range ."], "references": ["return the text inside the comment area of the file ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 2495, "code": "def close ( self ) : self . daf . file . close ( ) for segment in self . segments : if hasattr ( segment , ' data' ) : del segment . data self . daf . array = None self . daf . map = None", "predictions": ["closes this request and releases any system resources ."], "references": ["close this spk file ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 2496, "code": "def describe ( self , verbose = True ) : center = titlecase ( target names . get ( self . center , 'Unknown center' ) ) target = titlecase ( target names . get ( self . target , 'Unknown target' ) ) text = ( '{0.start jd:.2f}..{0.end jd:.2f}  {1} ({0.center})' ' -> {2} ({0.target})' . format ( self , center , target ) ) if verbose : text += ( '\\n  frame={0.frame} data type={0.data type} source={1}' . format ( self , self . source . decode ( 'ascii' ) ) ) return text", "predictions": ["recursively call this method to describe the operation ."], "references": ["return a textual description of the segment ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2497, "code": "def compute ( self , tdb , tdb2 = 0.0 ) : for position in self . generate ( tdb , tdb2 ) : return position", "predictions": ["to compute the position of this layout ."], "references": ["compute the component values for the time tdb plus tdb2 ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 2498, "code": "def close ( self ) : self . daf . file . close ( ) for segment in self . segments : if hasattr ( segment , ' data' ) : del segment . data", "predictions": ["closes this request and releases all associated resources ."], "references": ["close this file ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 2499, "code": "def describe ( self , verbose = True ) : body = titlecase ( target names . get ( self . body , 'Unknown body' ) ) text = ( '{0.start jd:.2f}..{0.end jd:.2f} frame={0.frame}' '  {1} ({0.body})' . format ( self , body ) ) if verbose : text += ( '\\n  data type={0.data type} source={1}' . format ( self , self . source . decode ( 'ascii' ) ) ) return text", "predictions": ["define and return a text delete for this object ."], "references": ["return a textual description of the segment ."], "bleu": 0.17827531042796255, "rouge_l": 0.34014869888475835}
{"id": 2500, "code": "def load ( self ) : if self . data type == 2 : component count = 3 else : raise Value Error ( 'only binary PCK data type 2 is supported' ) init , intlen , rsize , n = self . daf . read array ( self . end i - 3 , self . end i ) initial epoch = jd ( init ) interval length = intlen / S PER DAY coefficient count = int ( rsize - 2 ) // component count coefficients = self . daf . map array ( self . start i , self . end i - 4 ) coefficients . shape = ( int ( n ) , int ( rsize ) ) coefficients = coefficients [ : , 2 : ] coefficients . shape = ( int ( n ) , component count , coefficient count ) coefficients = rollaxis ( coefficients , 1 ) return initial epoch , interval length , coefficients", "predictions": ["loads the appropriate initial call graph"], "references": ["map the coefficients into memory using a numpy array ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2501, "code": "def visit Bin Op ( self , node ) : if self . within logging statement ( ) and self . within logging argument ( ) : if isinstance ( node . op , Mod ) : self . violations . append ( ( node , PERCENT FORMAT VIOLATION ) ) if isinstance ( node . op , Add ) : self . violations . append ( ( node , STRING CONCAT VIOLATION ) ) super ( Logging Visitor , self ) . generic visit ( node )", "predictions": ["the original visit operation for a given node ."], "references": ["process binary operations while processing the first logging argument ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 2502, "code": "def visit Dict ( self , node ) : if self . should check whitelist ( node ) : for key in node . keys : if key . s in self . whitelist or key . s . startswith ( \"debug \" ) : continue self . violations . append ( ( self . current logging call , WHITELIST VIOLATION . format ( key . s ) ) ) if self . should check extra exception ( node ) : for value in node . values : self . check exception arg ( value ) super ( Logging Visitor , self ) . generic visit ( node )", "predictions": ["replaces all contents of a given node with the same key as this one ."], "references": ["process dict arguments ."], "bleu": 0.08225964699966554, "rouge_l": 0.11753371868978806}
{"id": 2503, "code": "def visit Joined Str ( self , node ) : if version info >= ( 3 , 6 ) : if self . within logging statement ( ) : if any ( isinstance ( i , Formatted Value ) for i in node . values ) : if self . within logging argument ( ) : self . violations . append ( ( node , FSTRING VIOLATION ) ) super ( Logging Visitor , self ) . generic visit ( node )", "predictions": ["makes the logging and . for all edges ."], "references": ["process f - string arguments ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2504, "code": "def visit keyword ( self , node ) : if self . should check whitelist ( node ) : if node . arg not in self . whitelist and not node . arg . startswith ( \"debug \" ) : self . violations . append ( ( self . current logging call , WHITELIST VIOLATION . format ( node . arg ) ) ) if self . should check extra exception ( node ) : self . check exception arg ( node . value ) super ( Logging Visitor , self ) . generic visit ( node )", "predictions": ["makes a new delegate for the given node ."], "references": ["process keyword arguments ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 2505, "code": "def visit Except Handler ( self , node ) : name = self . get except handler name ( node ) if not name : super ( Logging Visitor , self ) . generic visit ( node ) return self . current except names . append ( name ) super ( Logging Visitor , self ) . generic visit ( node ) self . current except names . pop ( )", "predictions": ["create a new processed handler ."], "references": ["process except blocks ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2506, "code": "def detect logging level ( self , node ) : try : if self . get id attr ( node . func . value ) == \"warnings\" : return None if node . func . attr in LOGGING LEVELS : return node . func . attr except Attribute Error : pass return None", "predictions": ["collects logging from such as ( ."], "references": ["heuristic to decide whether an ast call is a logging call ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 2507, "code": "def get except handler name ( self , node ) : name = node . name if not name : return None if version info < ( 3 , ) : return name . id return name", "predictions": ["get the name of this node ."], "references": ["helper to get the exception name from an excepthandler node in both py2 and py3 ."], "bleu": 0.08036914931946858, "rouge_l": 0.40612516644474045}
{"id": 2508, "code": "def is bare exception ( self , node ) : return isinstance ( node , Name ) and node . id in self . current except names", "predictions": ["checks if the exception is bare to be considered as a bare node ."], "references": ["checks if the node is a bare exception name from an except block ."], "bleu": 0.22229849552064015, "rouge_l": 0.5}
{"id": 2509, "code": "def check exc info ( self , node ) : if self . current logging level not in ( 'error' , 'exception' ) : return for kw in node . keywords : if kw . arg == 'exc info' : if self . current logging level == 'error' : violation = ERROR EXC INFO VIOLATION else : violation = REDUNDANT EXC INFO VIOLATION self . violations . append ( ( node , violation ) )", "predictions": ["checks for exc that are useful to be registered in the specific node ."], "references": ["reports a violation if exc_info keyword is used with logging . error or logging . exception ."], "bleu": 0.07134415891605017, "rouge_l": 0.06340956340956341}
{"id": 2510, "code": "def db file widget ( cls ) : def get link display ( url ) : unquoted = unquote ( url . split ( '%2F' ) [ - 1 ] ) if sys . version info . major == 2 : from django . utils . encoding import force unicode unquoted = force unicode ( unquoted ) return escape ( unquoted ) def get template substitution values ( self , value ) : subst = super ( cls , self ) . get template substitution values ( value ) subst [ 'initial' ] = get link display ( value . url ) return subst setattr ( cls , 'get template substitution values' , get template substitution values ) def get context ( self , name , value , attrs ) : context = super ( cls , self ) . get context ( name , value , attrs ) if value and hasattr ( value , 'url' ) : context [ 'widget' ] [ 'display' ] = get link display ( value . url ) return context setattr ( cls , 'get context' , get context ) return cls", "predictions": ["get the context for the given widget and display it ."], "references": ["edit the download - link inner text ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 2511, "code": "def render to response ( self , context , * * response kwargs ) : filename = response kwargs . pop ( 'filename' , None ) cmd options = response kwargs . pop ( 'cmd options' , None ) if issubclass ( self . response class , PDF Template Response ) : if filename is None : filename = self . get filename ( ) if cmd options is None : cmd options = self . get cmd options ( ) return super ( PDF Template View , self ) . render to response ( context = context , filename = filename , show content in browser = self . show content in browser , header template = self . header template , footer template = self . footer template , cmd options = cmd options , cover template = self . cover template , * * response kwargs ) else : return super ( PDF Template View , self ) . render to response ( context = context , * * response kwargs )", "predictions": ["render the requested content of this request ."], "references": ["returns a pdf response with a template rendered with the given context ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 2512, "code": "def parse file ( self , file path , currency ) -> List [ Price Model ] : contents = self . load file ( file path ) prices = [ ] for line in contents : price = self . parse line ( line ) assert isinstance ( price , Price Model ) price . currency = currency prices . append ( price ) return prices", "predictions": ["read a file from the specified path and return it as a file object ."], "references": ["load and parse a . csv file"], "bleu": 0.10343603005129705, "rouge_l": 0.291866028708134}
{"id": 2513, "code": "def load file ( self , file path ) -> List [ str ] : content = [ ] content = read lines from file ( file path ) return content", "predictions": ["read a file and return a list of words ."], "references": ["loads the content of the text file"], "bleu": 0.13950796967929133, "rouge_l": 0.12151394422310759}
{"id": 2514, "code": "def parse line ( self , line : str ) -> Price Model : line = line . rstrip ( ) parts = line . split ( ',' ) result = Price Model ( ) result . symbol = self . translate symbol ( parts [ 0 ] ) result . value = Decimal ( parts [ 1 ] ) date str = parts [ 2 ] date str = date str . replace ( '\"' , '' ) date parts = date str . split ( '/' ) year str = date parts [ 2 ] month str = date parts [ 1 ] day str = date parts [ 0 ] logging . debug ( f\"parsing {date parts} into date\" ) result . datetime = datetime ( int ( year str ) , int ( month str ) , int ( day str ) ) return result", "predictions": ["parse this line from the given string . for each year , the format is a datetime for the date and day of the input stream ."], "references": ["parse a csv line into a price element"], "bleu": 0.053414136238197775, "rouge_l": 0.19003115264797507}
{"id": 2515, "code": "def translate symbol ( self , in symbol : str ) -> str : if not self . symbol maps : self . load symbol maps ( ) result = self . symbol maps [ in symbol ] if in symbol in self . symbol maps else in symbol return result", "predictions": ["use symbol for this symbol ."], "references": ["translate the incoming symbol into locally - used"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2516, "code": "def load symbol maps ( self ) : repo = Symbol Map Repository ( self . get session ( ) ) all maps = repo . get all ( ) self . symbol maps = { } for item in all maps : self . symbol maps [ item . in symbol ] = item . out symbol", "predictions": ["inits the database from the database ."], "references": ["loads all symbol maps from db"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2517, "code": "def get session ( self ) : if not self . session : self . session = dal . get default session ( ) return self . session", "predictions": ["this is called to get the session ."], "references": ["reuses the same db session"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 2518, "code": "def import csv ( filepath : str , currency : str ) : logger . debug ( f\"currency = {currency}\" ) currency = currency . upper ( ) app = Price Db Application ( ) app . logger = logger app . import prices ( filepath , currency )", "predictions": ["import the given currency using the ( ."], "references": ["import prices from csv file"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2519, "code": "def last ( symbol : str ) : app = Price Db Application ( ) if symbol : symbol = symbol . upper ( ) sec symbol = Security Symbol ( \"\" , \"\" ) sec symbol . parse ( symbol ) latest = app . get latest price ( sec symbol ) assert isinstance ( latest , Price Model ) print ( f\"{latest}\" ) else : latest = app . get latest prices ( ) for price in latest : print ( f\"{price}\" )", "predictions": ["create a new latest symbol ."], "references": ["displays last price for symbol if provided"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2520, "code": "def download ( ctx , help : bool , symbol : str , namespace : str , agent : str , currency : str ) : if help : click . echo ( ctx . get help ( ) ) ctx . exit ( ) app = Price Db Application ( ) app . logger = logger if currency : currency = currency . strip ( ) currency = currency . upper ( ) app . download prices ( currency = currency , agent = agent , symbol = symbol , namespace = namespace )", "predictions": ["downloads the given currency ."], "references": ["download the latest prices"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2521, "code": "def prune ( symbol : str , all : str ) : app = Price Db Application ( ) app . logger = logger count = 0 if symbol is not None : sec symbol = Security Symbol ( \"\" , \"\" ) sec symbol . parse ( symbol ) deleted = app . prune ( sec symbol ) if deleted : count = 1 else : count = app . prune all ( ) print ( f\"Removed {count} old price entries.\" )", "predictions": ["create a new application that was created by the given symbol ."], "references": ["delete old prices leaving just the last ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 2522, "code": "def get default session ( ) : from . config import Config , Config Keys db path = Config ( ) . get ( Config Keys . price database ) if not db path : raise Value Error ( \"Price database not set in the configuration file!\" ) return get session ( db path )", "predictions": ["get default value of the default configuration ."], "references": ["return the default session . the path is read from the default config ."], "bleu": 0.11327490115090784, "rouge_l": 0.346590909090909}
{"id": 2523, "code": "def add map ( incoming , outgoing ) : db path = Config ( ) . get ( Config Keys . pricedb path ) session = get session ( db path ) new map = Symbol Map ( ) new map . in symbol = incoming new map . out symbol = outgoing session . add ( new map ) session . commit ( ) click . echo ( \"Record saved.\" )", "predictions": ["add a map of incoming incoming incoming db"], "references": ["creates a symbol mapping"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2524, "code": "def list maps ( ) : db path = Config ( ) . get ( Config Keys . price database ) session = get session ( db path ) maps = session . query ( Symbol Map ) . all ( ) for item in maps : click . echo ( item )", "predictions": ["list all features . this is an integer [ . . . . . . . . . . . . . . . . . . . . . ."], "references": ["displays all symbol maps"], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 2525, "code": "def get by id ( self , symbol : str ) -> Symbol Map : return self . query . filter ( Symbol Map . in symbol == symbol ) . first ( )", "predictions": ["get a symbol by id ."], "references": ["finds the map by in - symbol"], "bleu": 0.20693220168471366, "rouge_l": 0.1517412935323383}
{"id": 2526, "code": "def read lines from file ( file path : str ) -> List [ str ] : with open ( file path ) as csv file : content = csv file . readlines ( ) return content", "predictions": ["read lines of ( from ( ."], "references": ["read text lines from a file"], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 2527, "code": "def map entity ( self , entity : dal . Price ) -> Price Model : if not entity : return None result = Price Model ( ) result . currency = entity . currency dt string = entity . date format string = \"%Y-%m-%d\" if entity . time : dt string += f\"T{entity.time}\" format string += \"T%H:%M:%S\" price datetime = datetime . strptime ( dt string , format string ) result . datum = Datum ( ) result . datum . from datetime ( price datetime ) assert isinstance ( result . datum , Datum ) #result.namespace = entity.namespace #result.symbol = entity.symbol result . symbol = Security Symbol ( entity . namespace , entity . symbol ) value = Decimal ( entity . value ) / Decimal ( entity . denom ) result . value = Decimal ( value ) return result", "predictions": ["map the price found in the model into the model ."], "references": ["map the price entity"], "bleu": 0.21200626759025185, "rouge_l": 0.43675417661097854}
{"id": 2528, "code": "def map model ( self , model : Price Model ) -> Price : assert isinstance ( model . symbol , Security Symbol ) assert isinstance ( model . datum , Datum ) entity = Price ( ) date iso = f\"{model.datum.value.year}-{model.datum.value.month:02d}-{model.datum.value.day:02d}\" entity . date = date iso entity . time = f\"{model.datum.value.hour:02d}:{model.datum.value.minute:02d}:{model.datum.value.second:02d}\" if model . symbol . namespace : entity . namespace = model . symbol . namespace . upper ( ) entity . symbol = model . symbol . mnemonic . upper ( ) assert isinstance ( model . value , Decimal ) dec places = abs ( model . value . as tuple ( ) . exponent ) entity . denom = 10 ** dec places entity . value = int ( model . value * entity . denom ) entity . currency = model . currency . upper ( ) return entity", "predictions": ["maps an text to a ."], "references": ["parse into the price entity ready for saving"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 2529, "code": "def read config ( self , file path : str ) : if not os . path . exists ( file path ) : raise File Not Found Error ( f\"File path not found: {file path}\" ) if not os . path . isfile ( file path ) : self . logger . error ( f\"file not found: {file path}\" ) raise File Not Found Error ( f\"configuration file not found {file path}\" ) self . config . read ( file path )", "predictions": ["compute the f\"configuration configuration from the given file ."], "references": ["read the config file"], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 2530, "code": "def get config template path ( self ) -> str : filename = resource filename ( Requirement . parse ( package name ) , template path + config filename ) return filename", "predictions": ["generate the self - level self for this self - inf file ."], "references": ["gets the default config path from resources"], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 2531, "code": "def create user config ( self ) : src path = self . get config template path ( ) src = os . path . abspath ( src path ) if not os . path . exists ( src ) : message = f\"Config template not found {src}\" self . logger . error ( message ) raise File Not Found Error ( message ) dst = os . path . abspath ( self . get config path ( ) ) shutil . copyfile ( src , dst ) if not os . path . exists ( dst ) : raise File Not Found Error ( \"Config file could not be copied to user dir!\" )", "predictions": ["describe a directory if it already exists ."], "references": ["copy the config template into user s directory"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 2532, "code": "def get contents ( self ) -> str : content = None in memory = io . String IO ( \"\" ) self . config . write ( in memory ) in memory . seek ( 0 ) content = in memory . read ( ) in memory . close ( ) return content", "predictions": ["load and return this object ."], "references": ["reads the contents of the config file"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2533, "code": "def set ( self , option : Config Keys , value ) : assert isinstance ( option , Config Keys ) section = SECTION self . config . set ( section , option . name , value ) self . save ( )", "predictions": ["store configuration ."], "references": ["sets a value in config"], "bleu": 0.23196236872272216, "rouge_l": 0.0}
{"id": 2534, "code": "def get ( self , option : Config Keys ) : assert isinstance ( option , Config Keys ) section = SECTION return self . config . get ( section , option . name )", "predictions": [". method to set the configuration , such as . ."], "references": ["retrieves a config value"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2535, "code": "def save ( self ) : file path = self . get config path ( ) contents = self . get contents ( ) with open ( file path , mode = 'w' ) as cfg file : cfg file . write ( contents )", "predictions": ["writes the configuration to the specified file ."], "references": ["save the config file"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 2536, "code": "def parse ( self , symbol : str ) -> ( str , str ) : symbol parts = symbol . split ( \":\" ) namespace = None mnemonic = symbol if len ( symbol parts ) > 1 : namespace = symbol parts [ 0 ] mnemonic = symbol parts [ 1 ] self . namespace = namespace self . mnemonic = mnemonic return namespace , mnemonic", "predictions": ["extracts the , and returns a , based on the token ."], "references": ["splits the symbol into namespace symbol tuple"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 2537, "code": "def add price ( self , price : Price Model ) : if not price : raise Value Error ( \"Cannot add price. The received model is null!\" ) mapper = mappers . Price Mapper ( ) entity = mapper . map model ( price ) self . add price entity ( entity )", "predictions": ["adds a model to the model"], "references": ["creates a new price record"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2538, "code": "def download price ( self , symbol : str , currency : str , agent : str ) -> Price Model : price = self . download price ( symbol , currency , agent ) self . save ( ) return price", "predictions": ["downloads the logging event ."], "references": ["download and save price online"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2539, "code": "def session ( self ) : if not self . session : self . session = dal . get default session ( ) return self . session", "predictions": ["sets the get get method ."], "references": ["returns the current db session"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2540, "code": "def get prices ( self , date : str , currency : str ) -> List [ Price Model ] : from . repositories import Price Repository session = self . session repo = Price Repository ( session ) query = repo . query if date : query = query . filter ( dal . Price . date == date ) if currency : query = query . filter ( dal . Price . currency == currency ) query = query . order by ( dal . Price . namespace , dal . Price . symbol ) price entities = query . all ( ) mapper = mappers . Price Mapper ( ) result = [ ] for entity in price entities : model = mapper . map entity ( entity ) result . append ( model ) return result", "predictions": ["returns all the bare mappers found in this session ."], "references": ["fetches all the prices for the given arguments"], "bleu": 0.16590387014219712, "rouge_l": 0.22676579925650556}
{"id": 2541, "code": "def get prices on ( self , on date : str , namespace : str , symbol : str ) : repo = self . get price repository ( ) query = ( repo . query . filter ( dal . Price . namespace == namespace ) . filter ( dal . Price . symbol == symbol ) . filter ( dal . Price . date == on date ) . order by ( dal . Price . time . desc ( ) ) ) result = query . first ( ) return result", "predictions": ["run exc and return information about the ."], "references": ["returns the latest price on the date"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2542, "code": "def download price ( self , symbol : str , currency : str , agent : str ) : from finance quote python import Quote assert isinstance ( symbol , str ) assert isinstance ( currency , str ) assert isinstance ( agent , str ) if not symbol : return None #self.logger.info(f\"Downloading {symbol}... \") dl = Quote ( ) dl . logger = self . logger dl . set source ( agent ) dl . set currency ( currency ) result = dl . fetch ( agent , [ symbol ] ) if not result : raise Value Error ( f\"Did not receive a response for {symbol}.\" ) price = result [ 0 ] if not price : raise Value Error ( f\"Price not downloaded/parsed for {symbol}.\" ) else : self . add price ( price ) return price", "predictions": ["downloads a file from the specified ) and returns the file that has the specified ) ."], "references": ["downloads and parses the price"], "bleu": 0.0859076483566362, "rouge_l": 0.3024793388429752}
{"id": 2543, "code": "def get securities ( self , currency : str , agent : str , symbol : str , namespace : str ) -> List [ dal . Security ] : repo = self . get security repository ( ) query = repo . query if currency is not None : query = query . filter ( dal . Security . currency == currency ) if agent is not None : query = query . filter ( dal . Security . updater == agent ) if symbol is not None : query = query . filter ( dal . Security . symbol == symbol ) if namespace is not None : query = query . filter ( dal . Security . namespace == namespace ) query = query . order by ( dal . Security . namespace , dal . Security . symbol ) securities = query . all ( ) return securities", "predictions": ["makes a class from the issubclass and in order to get the ) ."], "references": ["fetches the securities that match the given filters"], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 2544, "code": "def partial ( self ) : ba = self . data [ \"bound args\" ] return state partial ( self . data [ \"func\" ] , * ba . args [ 1 : ] , * * ba . kwargs )", "predictions": ["compute the parse method for this object ."], "references": ["return partial of original function call"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2545, "code": "def update child calls ( self ) : for node in filter ( lambda n : len ( n . arg name ) , self . child list ) : self . data [ \"bound args\" ] . arguments [ node . arg name ] = node . partial ( ) self . updated = True", "predictions": ["store stats of this object so that they can have multiple . ."], "references": ["replace child nodes on original function call with their partials"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2546, "code": "def descend ( self , include me = True ) : if include me : yield self for child in self . child list : yield child yield from child . descend ( )", "predictions": ["yields a generator that yields the given list of ( ."], "references": ["descend depth first into all child nodes"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2547, "code": "def multi dec ( f ) : @ wraps ( f ) def wrapper ( * args , * * kwargs ) : args = ( args [ 0 ] if len ( args ) == 1 and isinstance ( args [ 0 ] , ( list , tuple ) ) else args ) for arg in args : if isinstance ( arg , Node ) and arg . parent . name is \"root\" : arg . parent . remove child ( arg ) arg . update child calls ( ) return f ( * args , * * kwargs ) return wrapper", "predictions": ["decorator for the translate command arguments to an object ."], "references": ["decorator for multi to remove nodes for original test functions from root node"], "bleu": 0.13206959826272413, "rouge_l": 0.25487465181058494}
{"id": 2548, "code": "def get Result From Process ( res , tempname , process ) : if not isinstance ( res , ( Undefined Value , Exception ) ) : value = get Representation ( tempname , process ) return value , res else : return res , str ( res )", "predictions": ["read result from given = value self - parsed = true if the = 0 - length < = 0 then if the value is currently in one - to - 1 , 0 and length is returned ."], "references": ["get a value from process return tuple of value res if succesful"], "bleu": 0.038856620103720534, "rouge_l": 0.1300639658848614}
{"id": 2549, "code": "def defined items ( self ) : return self . class ( [ ( k , v ) for k , v in self . items ( ) if v is not self . EMPTY ] , is empty = False )", "predictions": ["preferred get . override this to provide simple get around each message ."], "references": ["return copy of instance omitting entries that are empty"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2550, "code": "def getx ( self , Parser , ext attr , tree ) : cache key = Parser . name + str ( hash ( tree ) ) if self . parser cache . get ( cache key ) : p = self . parser cache [ cache key ] else : p = Parser ( ) if ext attr != \"mappings\" and Parser in [ Function Parser , Object Access Parser , ] : p . mappings = self . context mappings . copy ( ) p . visit ( tree ) self . parser cache [ cache key ] = p return getattr ( p , ext attr )", "predictions": ["uses the import method to import its call to ."], "references": ["getter for parser outputs"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2551, "code": "def check part ( state , name , part msg , missing msg = None , expand msg = None ) : if missing msg is None : missing msg = \"Are you sure you defined the {{part}}? \" if expand msg is None : expand msg = \"Did you correctly specify the {{part}}? \" if not part msg : part msg = name append message = { \"msg\" : expand msg , \"kwargs\" : { \"part\" : part msg } } has part ( state , name , missing msg , append message [ \"kwargs\" ] ) stu part = state . student parts [ name ] sol part = state . solution parts [ name ] assert ast ( state , sol part , append message [ \"kwargs\" ] ) return part to child ( stu part , sol part , append message , state )", "predictions": ["performs a dummy test of student str ."], "references": ["return child state with name part as its ast tree"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 2552, "code": "def detect openmp ( ) : compiler = new compiler ( ) print ( \"Checking for Open MP support... \" ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if not hasopenmp : compiler . add library ( 'gomp' ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if hasopenmp : print ( \"Compiler supports Open MP\" ) else : print ( \"Did not detect Open MP support.\" ) return hasopenmp , needs gomp", "predictions": ["download the entire : dispatch loop to the command line ."], "references": ["does this compiler support openmp parallelization?"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2553, "code": "def get true anomaly ( self ) : self . f = rsky . getf ( self . t supersample , self . t0 , self . per , self . a , self . inc * pi / 180. , self . ecc , self . w * pi / 180. , self . transittype , self . nthreads ) return self . f", "predictions": ["use this to get last used event ."], "references": ["return the true anomaly at each time"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2554, "code": "def detect ( ) : compiler = new compiler ( ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if not hasopenmp : compiler . add library ( 'gomp' ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp return hasopenmp", "predictions": ["get the redirect to the given ) ."], "references": ["does this compiler support openmp parallelization?"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2555, "code": "def teardown ( self , exception ) : ctx = stack . top if ctx is not None : if hasattr ( ctx , 'ldap3 manager connections' ) : for connection in ctx . ldap3 manager connections : self . destroy connection ( connection ) if hasattr ( ctx , 'ldap3 manager main connection' ) : log . debug ( \"Unbinding a connection used within the request context.\" ) ctx . ldap3 manager main connection . unbind ( ) ctx . ldap3 manager main connection = None", "predictions": ["close all the intercept objects . this will be called when the symbol is made ."], "references": ["cleanup after a request . close any open connections ."], "bleu": 0.09147827112247602, "rouge_l": 0.16052631578947368}
{"id": 2556, "code": "def list all ( self , * * kwargs ) : quiet = False if \"quiet\" in kwargs : quiet = kwargs [ 'quiet' ] bot . spinner . start ( ) url = '%s/collections/' % self . base results = self . paginate get ( url ) bot . spinner . stop ( ) if len ( results ) == 0 : bot . info ( \"No container collections found.\" ) sys . exit ( 1 ) rows = [ ] for result in results : if \"containers\" in result : if result [ 'id' ] not in [ 37 , 38 , 39 ] : for c in result [ 'containers' ] : rows . append ( [ c [ 'detail' ] , \"%s:%s\" % ( c [ 'name' ] , c [ 'tag' ] ) ] ) if quiet is False : bot . info ( \"Collections\" ) bot . table ( rows ) return rows", "predictions": ["returns a list of all the rows ."], "references": ["a show all search that doesn t require a query"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 2557, "code": "def update headers ( self , fields = None ) : do reset = True if hasattr ( self , 'headers' ) : if self . headers is not None : do reset = False if do reset is True : self . reset headers ( ) if fields is not None : for key , value in fields . items ( ) : self . headers [ key ] = value header names = \",\" . join ( list ( self . headers . keys ( ) ) ) bot . debug ( \"Headers found: %s\" % header names )", "predictions": ["get all by by by by by fold ."], "references": ["update headers with a token & other fields"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2558, "code": "def post ( url , data = None , return json = True ) : bot . debug ( \"POST %s\" % url ) return call ( url , headers = headers , func = requests . post , data = data , return json = return json )", "predictions": ["sends an error message back to the response"], "references": ["post will use requests to get a particular url"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2559, "code": "def get ( url , headers = None , token = None , data = None , return json = True ) : bot . debug ( \"GET %s\" % url ) return call ( url , headers = headers , func = requests . get , data = data , return json = return json )", "predictions": ["sends an error message back to the client and returns the response ."], "references": ["get will use requests to get a particular url"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 2560, "code": "def load secrets ( self ) : self . auth = self . get and update setting ( 'GLOBUS AUTH RESPONSE' ) self . transfer = self . get and update setting ( 'GLOBUS TRANSFER RESPONSE' )", "predictions": ["creates the setting for the given list of secrets ."], "references": ["load the secrets credentials file with the globus oauthtokenresponse"], "bleu": 0.14991106946711685, "rouge_l": 0.21254355400696867}
{"id": 2561, "code": "def list logs ( self ) : results = [ ] for image in self . bucket . list blobs ( ) : if image . name . endswith ( 'log' ) : results . append ( image ) if len ( results ) == 0 : bot . info ( \"No containers found, based on extension .log\" ) return results", "predictions": ["returns a list of containers that can be used to list the logs ."], "references": ["return a list of logs . we return any file that ends in . log"], "bleu": 0.18931716792793718, "rouge_l": 0.34269662921348315}
{"id": 2562, "code": "def init transfer client ( self ) : if self . tokens need update ( ) : self . update tokens ( ) access token = self . transfer [ 'access token' ] authorizer = globus sdk . Refresh Token Authorizer ( self . transfer [ 'refresh token' ] , self . client , access token = self . transfer [ 'access token' ] , expires at = self . transfer [ 'expires at seconds' ] ) self . transfer client = globus sdk . Transfer Client ( authorizer = authorizer )", "predictions": ["initialize the game object ."], "references": ["return a transfer client for the user"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2563, "code": "def status ( backend ) : print ( '[backend status]' ) settings = read client secrets ( ) print ( 'There are %s clients found in secrets.' % len ( settings ) ) if 'SREGISTRY CLIENT' in settings : print ( 'active: %s' % settings [ 'SREGISTRY CLIENT' ] ) update secrets ( settings ) else : print ( 'There is no active client.' )", "predictions": ["print the status of the secrets queue ."], "references": ["print the status for all or one of the backends ."], "bleu": 0.2535368728139476, "rouge_l": 0.6140939597315436}
{"id": 2564, "code": "def add ( backend , variable , value , force = False ) : print ( '[add]' ) settings = read client secrets ( ) prefix = 'SREGISTRY %s ' % backend . upper ( ) if not variable . startswith ( prefix ) : variable = '%s%s' % ( prefix , variable ) variable = variable . upper ( ) bot . info ( \"%s %s\" % ( variable , value ) ) if backend in settings : if variable in settings [ backend ] and force is False : previous = settings [ backend ] [ variable ] bot . error ( '%s is already set as %s. Use --force to override.' % ( variable , previous ) ) sys . exit ( 1 ) if backend not in settings : settings [ backend ] = { } settings [ backend ] [ variable ] = value update secrets ( settings )", "predictions": ["add a 'sregistry with the passed variable ."], "references": ["add the variable to the config"], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 2565, "code": "def remove ( backend , variable ) : print ( '[remove]' ) settings = read client secrets ( ) prefixed = variable prefix = 'SREGISTRY %s ' % backend . upper ( ) if not variable . startswith ( prefix ) : prefixed = '%s%s' % ( prefix , variable ) variable = variable . upper ( ) bot . info ( variable ) if backend in settings : if variable in settings [ backend ] : del settings [ backend ] [ variable ] if prefixed in settings [ backend ] : del settings [ backend ] [ prefixed ] update secrets ( settings )", "predictions": ["removes the backend from the backend ."], "references": ["remove a variable from the config if found ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 2566, "code": "def activate ( backend ) : settings = read client secrets ( ) if backend is not None : settings [ 'SREGISTRY CLIENT' ] = backend update secrets ( settings ) print ( '[activate] %s' % backend )", "predictions": ["define a new secrets instance ."], "references": ["activate a backend by adding it to the . sregistry configuration file ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 2567, "code": "def delete backend ( backend ) : settings = read client secrets ( ) if backend in settings : del settings [ backend ] if 'SREGISTRY CLIENT' in settings : if settings [ 'SREGISTRY CLIENT' ] == backend : del settings [ 'SREGISTRY CLIENT' ] update secrets ( settings ) print ( '[delete] %s' % backend ) else : if backend is not None : print ( '%s is not a known client.' % backend ) else : print ( 'Please specify a backend to delete.' )", "predictions": ["deletes the backend from the database ."], "references": ["delete a backend and update the secrets file"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2568, "code": "def delete ( self , url , headers = None , return json = True , default headers = True ) : bot . debug ( 'DELETE %s' % url ) return self . call ( url , headers = headers , func = requests . delete , return json = return json , default headers = default headers )", "predictions": ["do the actual delete operation ."], "references": ["delete request use with caution"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2569, "code": "def head ( self , url ) : bot . debug ( 'HEAD %s' % url ) return self . call ( url , func = requests . head )", "predictions": ["this method is called to get the head of the resource ."], "references": ["head request typically used for status code retrieval etc ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 2570, "code": "def post ( self , url , headers = None , data = None , return json = True , default headers = True ) : bot . debug ( \"POST %s\" % url ) return self . call ( url , headers = headers , func = requests . post , data = data , return json = return json , default headers = default headers )", "predictions": ["send http request to the server ."], "references": ["post will use requests to get a particular url"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2571, "code": "def get ( self , url , headers = None , token = None , data = None , return json = True , default headers = True , quiet = False ) : bot . debug ( \"GET %s\" % url ) return self . call ( url , headers = headers , func = requests . get , data = data , return json = return json , default headers = default headers , quiet = quiet )", "predictions": ["fetch the json and return the json object ."], "references": ["get will use requests to get a particular url"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2572, "code": "def paginate get ( self , url , headers = None , return json = True , start page = None ) : geturl = '%s&page=1' % ( url ) if start page is not None : geturl = '%s&page=%s' % ( url , start page ) results = [ ] while geturl is not None : result = self . get ( url , headers = headers , return json = return json ) if isinstance ( result , dict ) : if 'results' in result : results = results + result [ 'results' ] geturl = result [ 'next' ] else : return result return results", "predictions": ["store the results of the request and return information about the user ."], "references": ["paginate_call is a wrapper for get to paginate results"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 2573, "code": "def remove ( self , image , force = False ) : q = parse image name ( remove uri ( image ) ) if q [ 'registry' ] == None : q [ 'registry' ] = self . base q = self . add https ( q ) url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ \"collection\" ] , q [ \"image\" ] , q [ \"tag\" ] ) SREGISTRY EVENT = self . authorize ( request type = \"delete\" , names = q ) headers = { 'Authorization' : SREGISTRY EVENT } self . update headers ( fields = headers ) continue delete = True if force is False : response = input ( \"Are you sure you want to delete %s?\" % q [ 'uri' ] ) while len ( response ) < 1 or response [ 0 ] . lower ( ) . strip ( ) not in \"ynyesno\" : response = input ( \"Please answer yes or no: \" ) if response [ 0 ] . lower ( ) . strip ( ) in \"no\" : continue delete = False if continue delete is True : response = self . delete ( url ) message = self . read response ( response ) bot . info ( \"Response %s, %s\" % ( response . status code , message ) ) else : bot . info ( \"Delete cancelled.\" )", "predictions": ["fix stats that are finished before responding ."], "references": ["delete an image to singularity registry"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2574, "code": "def get installdir ( ) : return os . path . abspath ( os . path . dirname ( os . path . dirname ( file ) ) )", "predictions": ["returns the directory component of this directory ."], "references": ["get_installdir returns the installation directory of the application"], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 2575, "code": "def get collections ( self ) : collections = [ ] for container in self . conn . get account ( ) [ 1 ] : collections . append ( container [ 'name' ] ) return collections", "predictions": ["get all registered batches ."], "references": ["get a listing of collections that the user has access to ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 2576, "code": "def ipython ( args ) : from sregistry . main import get client client = get client ( args . endpoint ) client . announce ( args . command ) from I Python import embed embed ( )", "predictions": ["creates a new ( ."], "references": ["give the user an ipython shell optionally with an endpoint of choice ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 2577, "code": "def update base ( self ) : self . base = self . get and update setting ( 'SREGISTRY GITLAB BASE' , \"https://gitlab.com/\" ) self . api base = \"%s/api/v4\" % self . base . strip ( '/' ) self . artifacts = self . get and update setting ( 'SREGISTRY GITLAB FOLDER' , 'build' ) self . job = self . get and update setting ( 'SREGISTRY GITLAB JOB' , 'build' ) bot . debug ( '      Api: %s' % self . api base ) bot . debug ( 'Artifacts: %s' % self . artifacts ) bot . debug ( '      Job: %s' % self . job )", "predictions": ["this method is called when the base class has been made ."], "references": ["update the base including the url for gitlab and the api endpoint ."], "bleu": 0.13519230385081712, "rouge_l": 0.23828125000000006}
{"id": 2578, "code": "def update secrets ( self ) : self . token = self . required get and update ( 'SREGISTRY GITLAB TOKEN' ) self . headers [ \"Private-Token\" ] = self . token", "predictions": ["set the secrets object to be set to the list of secrets boxes ."], "references": ["update secrets will update metadata needed for pull and search"], "bleu": 0.08839374326825923, "rouge_l": 0.08591549295774649}
{"id": 2579, "code": "def update setting ( self , name , value ) : if value is not None : updates = { name : value } update client secrets ( backend = self . client name , updates = updates )", "predictions": ["update setting setting with backend updates ."], "references": ["just update a setting doesn t need to be returned ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 2580, "code": "def search all ( self ) : results = set ( ) for container in self . conn . get account ( ) [ 1 ] : for result in self . conn . get container ( container [ 'name' ] ) [ 1 ] : results . add ( '%s/%s' % ( container [ 'name' ] , result [ 'name' ] ) ) if len ( results ) == 0 : bot . info ( \"No container collections found.\" ) sys . exit ( 1 ) bot . info ( \"Collections\" ) bot . table ( [ [ x ] for x in list ( results ) ] ) return list ( results )", "predictions": ["search for all the systems that match the given container ."], "references": ["a show all search that doesn t require a query"], "bleu": 0.1354599427337814, "rouge_l": 0.1921259842519685}
{"id": 2581, "code": "def search all ( self ) : results = [ ] for entry in self . dbx . files list folder ( '' ) . entries : for item in self . dbx . files list folder ( entry . path lower ) . entries : name = item . name . replace ( '.simg' , '' ) results . append ( [ \"%s/%s\" % ( entry . name , name ) ] ) if len ( results ) == 0 : bot . info ( \"No container collections found.\" ) sys . exit ( 1 ) bot . info ( \"Collections\" ) bot . table ( results ) return results", "predictions": ["do a search for all the criteria ."], "references": ["a show all search that doesn t require a query"], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 2582, "code": "def get build template ( ) : base = get installdir ( ) name = \"%s/main/templates/build/singularity-cloudbuild.json\" % base if os . path . exists ( name ) : bot . debug ( \"Found template %s\" % name ) return read json ( name ) bot . warning ( \"Template %s not found.\" % name )", "predictions": ["gets build the build template ."], "references": ["get default build template ."], "bleu": 0.4111336169005197, "rouge_l": 0.5545454545454546}
{"id": 2583, "code": "def get bucket ( self ) : try : self . bucket = self . bucket service . get bucket ( self . bucket name ) except google . cloud . exceptions . Not Found : self . bucket = self . bucket service . create bucket ( self . bucket name ) except : bot . error ( 'Cannot get or create %s' % self . bucket name ) sys . exit ( 1 ) return self . bucket", "predictions": ["this method returns a bucket for the desired bucket ."], "references": ["get a bucket based on a bucket name . if it doesn t exist create it ."], "bleu": 0.09360791398475977, "rouge_l": 0.28306264501160094}
{"id": 2584, "code": "def get subparsers ( parser ) : actions = [ action for action in parser . actions if isinstance ( action , argparse . Sub Parsers Action ) ] subparsers = dict ( ) for action in actions : for choice , subparser in action . choices . items ( ) : subparsers [ choice ] = subparser return subparsers", "predictions": ["retrieves all subparsers actions for the given parser ."], "references": ["get_subparser will get a dictionary of subparsers to help with printing help"], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 2585, "code": "def get file hash ( filename ) : hasher = hashlib . sha256 ( ) with open ( filename , \"rb\" ) as f : for chunk in iter ( lambda : f . read ( 4096 ) , b\"\" ) : hasher . update ( chunk ) return hasher . hexdigest ( )", "predictions": ["returns a hash of the 4096 ."], "references": ["find the sha256 hash string of a file"], "bleu": 0.2024757945132846, "rouge_l": 0.2634989200863931}
{"id": 2586, "code": "def clean up ( files ) : if not isinstance ( files , list ) : files = [ files ] for f in files : if os . path . exists ( f ) : bot . verbose3 ( \"Cleaning up %s\" % f ) os . remove ( f )", "predictions": ["deletes the temporary files ."], "references": ["clean up will delete a list of files only if they exist"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 2587, "code": "def push ( self , path , name , tag = None ) : path = os . path . abspath ( path ) image = os . path . basename ( path ) bot . debug ( \"PUSH %s\" % path ) if not os . path . exists ( path ) : bot . error ( '%s does not exist.' % path ) sys . exit ( 1 ) names = parse image name ( remove uri ( name ) , tag = tag ) image size = os . path . getsize ( path ) >> 20 metadata = { 'sizemb' : \"%s\" % image size , 'client' : 'sregistry' } self . bucket . upload file ( path , names [ 'storage uri' ] , { \"Metadata\" : metadata } )", "predictions": ["pushes a path onto the server ."], "references": ["push an image to an s3 endpoint"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2588, "code": "def get collection ( self , name ) : from sregistry . database . models import Collection return Collection . query . filter ( Collection . name == name ) . first ( )", "predictions": ["returns the first element in the collection ."], "references": ["get a collection if it exists otherwise return none ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 2589, "code": "def get container ( self , name , collection id , tag = \"latest\" , version = None ) : from sregistry . database . models import Container if version is None : container = Container . query . filter by ( collection id = collection id , name = name , tag = tag ) . first ( ) else : container = Container . query . filter by ( collection id = collection id , name = name , tag = tag , version = version ) . first ( ) return container", "predictions": ["get container object to read or return version ."], "references": ["get a container otherwise return none ."], "bleu": 0.17747405280050263, "rouge_l": 0.5115303983228512}
{"id": 2590, "code": "def rmi ( self , image name ) : container = self . rm ( image name , delete = True ) if container is not None : bot . info ( \"[rmi] %s\" % container )", "predictions": ["deletes the rmi holder ."], "references": ["remove an image from the database and filesystem ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 2591, "code": "def run build ( self , config , bucket , names ) : project = self . get project ( ) bot . custom ( 'PROJECT' , project , \"CYAN\" ) bot . custom ( 'BUILD  ' , config [ 'steps' ] [ 0 ] [ 'name' ] , \"CYAN\" ) response = self . build service . projects ( ) . builds ( ) . create ( body = config , project Id = project ) . execute ( ) build id = response [ 'metadata' ] [ 'build' ] [ 'id' ] status = response [ 'metadata' ] [ 'build' ] [ 'status' ] bot . log ( \"build %s: %s\" % ( build id , status ) ) start = time . time ( ) while status not in [ 'COMPLETE' , 'FAILURE' , 'SUCCESS' ] : time . sleep ( 15 ) response = self . build service . projects ( ) . builds ( ) . get ( id = build id , project Id = project ) . execute ( ) build id = response [ 'id' ] status = response [ 'status' ] bot . log ( \"build %s: %s\" % ( build id , status ) ) end = time . time ( ) bot . log ( 'Total build time: %s seconds' % ( round ( end - start , 2 ) ) ) if status == 'SUCCESS' : env = 'SREGISTRY GOOGLE STORAGE PRIVATE' blob = bucket . blob ( response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] ) if self . get and update setting ( env ) == None : blob . make public ( ) response [ 'public url' ] = blob . public url update blob metadata ( blob , response , config , bucket , names ) response [ 'media link' ] = blob . media link response [ 'size' ] = blob . size response [ 'file hash' ] = blob . md5 hash return response", "predictions": ["run the thread for the given project . the builds will be called first ."], "references": ["run a build meaning creating a build . retry if there is failure"], "bleu": 0.09103526405546068, "rouge_l": 0.14472123368920523}
{"id": 2592, "code": "def search all ( self ) : url = '...' results = self . paginate get ( url ) if len ( results ) == 0 : bot . info ( \"No container collections found.\" ) sys . exit ( 1 ) bot . info ( \"Collections\" ) rows = [ ] for result in results : if \"containers\" in result : for c in result [ 'containers' ] : rows . append ( [ c [ 'uri' ] , c [ 'detail' ] ] ) bot . table ( rows ) return rows", "predictions": ["this returns all the rows that can be found ."], "references": ["a show all search that doesn t require a query"], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 2593, "code": "def get manifest ( self , repo name , tag ) : image = None repo = self . aws . describe images ( repository Name = repo name ) if 'image Details' in repo : for contender in repo . get ( 'image Details' ) : if tag in contender [ 'image Tags' ] : image = contender break if image is None : bot . exit ( 'Cannot find %s:%s, is the uri correct?' % ( repo name , digest ) ) digest = image [ 'image Digest' ] digests = self . aws . batch get image ( repository Name = repo name , image Ids = [ { \"image Digest\" : digest , \"image Tag\" : tag } ] ) self . manifest = json . loads ( digests [ 'images' ] [ 0 ] [ 'image Manifest' ] ) return self . manifest", "predictions": ["returns the logs for this logs ."], "references": ["return the image manifest via the aws client saved in self . manifest"], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 2594, "code": "def s3errors ( path ) : try : yield except Client Error as error : error = error . response . get ( \"Error\" , { } ) error code = error . get ( \"Code\" , None ) response meta = error . response . get ( \"Response Metadata\" , { } ) http status = response meta . get ( \"HTTP Status Code\" , 200 ) error msg = error . get ( \"Message\" , None ) if error code == \"No Such Bucket\" : raise errors . Resource Error ( path , exc = error , msg = error msg ) if http status == 404 : raise errors . Resource Not Found ( path ) elif http status == 403 : raise errors . Permission Denied ( path = path , msg = error msg ) else : raise errors . Operation Failed ( path = path , exc = error ) except SSL Error as error : raise errors . Operation Failed ( path , exc = error ) except Endpoint Connection Error as error : raise errors . Remote Connection Error ( path , exc = error , msg = \"{}\" . format ( error ) )", "predictions": ["method to : raise ."], "references": ["translate s3 errors to fserrors ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2595, "code": "def factory ( cls , filename , mode , on close ) : temp file = tempfile . Temporary File ( ) proxy = cls ( temp file , filename , mode , on close = on close ) return proxy", "predictions": ["tries to status of the class ."], "references": ["create a s3file backed with a temporary file ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2596, "code": "def gravatar url ( user or email , size = GRAVATAR DEFAULT SIZE ) : if hasattr ( user or email , 'email' ) : email = user or email . email else : email = user or email try : return escape ( get gravatar url ( email = email , size = size ) ) except : return ''", "predictions": ["( value ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) = , . , . , backend = , . , backend"], "references": ["builds a gravatar url from an user or email"], "bleu": 0.02900074465673044, "rouge_l": 0.0}
{"id": 2597, "code": "def has gravatar ( email ) : url = get gravatar url ( email , default = GRAVATAR DEFAULT IMAGE 404 ) try : request = Request ( url ) request . get method = lambda : 'HEAD' return 200 == urlopen ( request ) . code except ( HTTP Error , URL Error ) : return False", "predictions": ["test if the request exists in the request ."], "references": ["returns true if the user has a gravatar false if otherwise"], "bleu": 0.14873743701255318, "rouge_l": 0.19645732689210954}
{"id": 2598, "code": "def chimera blocks ( M = 16 , N = 16 , L = 4 ) : for x in xrange ( M ) : for y in xrange ( N ) : for u in ( 0 , 1 ) : yield tuple ( ( x , y , u , k ) for k in xrange ( L ) )", "predictions": ["generator that creates a activate matrix in a 8 - dimensional fashion ."], "references": ["generator for blocks for a chimera block quotient"], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 2599, "code": "def main ( ) : parser = Molvs Parser ( epilog = 'use \"molvs <command> -h\" to show help for a specific command' ) subparsers = parser . add subparsers ( title = 'Available commands' ) common parser = Molvs Parser ( add help = False ) common parser . add argument ( 'infile' , nargs = '?' , help = 'input filename' , type = argparse . File Type ( 'r' ) , default = sys . stdin ) common parser . add argument ( '-i' , '--intype' , help = 'input filetype' , choices = FILETYPES ) common parser . add argument ( '-:' , '--smiles' , help = 'input SMILES instead of file' , metavar = '<smiles>' ) common parser . add argument ( '-O' , '--outfile' , help = 'output filename' , type = argparse . File Type ( 'w' ) , default = sys . stdout , metavar = '<outfile>' ) standardize parser = subparsers . add parser ( 'standardize' , help = 'standardize a molecule' , parents = [ common parser ] ) standardize parser . add argument ( '-o' , '--outtype' , help = 'output filetype' , choices = FILETYPES ) standardize parser . set defaults ( func = standardize main ) validate parser = subparsers . add parser ( 'validate' , help = 'validate a molecule' , parents = [ common parser ] ) validate parser . set defaults ( func = validate main ) args = parser . parse args ( ) try : args . func ( args ) except Exception as e : sys . stderr . write ( 'Error: %s\\n\\n' . encode ( ) % e . message ) parser . print help ( ) sys . exit ( 2 )", "predictions": ["creates and prepares the main orchestration orchestration method for ("], "references": ["main function for molvs command line interface ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 2600, "code": "def integrate ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : f , j = get f and j ( mu ) if nt > 1 : tout = np . linspace ( t0 , tend , nt ) yout , nfo = integrate predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check indexing = False , method = method ) else : tout , yout , nfo = integrate adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check indexing = False , method = method ) if verbose : print ( nfo ) if plot : import matplotlib . pyplot as plt plt . plot ( tout , yout [ : , 1 ] , 'g--' ) plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) if savefig == 'None' : plt . show ( ) else : plt . savefig ( savefig , dpi = dpi )", "predictions": ["delete the example object ."], "references": ["example program integrating an ivp problem of van der pol oscillator"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 2601, "code": "def get mems of org ( self ) : print 'Getting members.' counter = 0 for member in self . org retrieved . iter members ( ) : self . members json [ member . id ] = member . to json ( ) counter += 1 return counter", "predictions": ["head method for ( . : . : . : . : . : . : . : . : . : . = . requests"], "references": ["retrieves the number of members of the organization ."], "bleu": 0.04668049023095243, "rouge_l": 0.0626283367556468}
{"id": 2602, "code": "def get teams of org ( self ) : print 'Getting teams.' counter = 0 for team in self . org retrieved . iter teams ( ) : self . teams json [ team . id ] = team . to json ( ) counter += 1 return counter", "predictions": ["this method returns the ( most recent = 0 , 1 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 ="], "references": ["retrieves the number of teams of the organization ."], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 2603, "code": "def repos ( self , repo type = 'public' , organization = 'llnl' ) : print 'Getting repos.' for repo in self . org retrieved . iter repos ( type = repo type ) : #JSON json = repo . to json ( ) self . repos json [ repo . name ] = json #CSV temp repo = my repo . My Repo ( ) temp repo . name = repo . full name self . total repos += 1 temp repo . contributors = my github . get total contributors ( repo ) self . total contributors += temp repo . contributors temp repo . forks = repo . forks count self . total forks += temp repo . forks temp repo . stargazers = repo . stargazers self . total stars += temp repo . stargazers temp repo . pull requests open , temp repo . pull requests closed = my github . get pull reqs ( repo ) temp repo . pull requests = ( temp repo . pull requests open + temp repo . pull requests closed ) self . total pull reqs += temp repo . pull requests open self . total pull reqs += temp repo . pull requests closed self . total pull reqs open += temp repo . pull requests open self . total pull reqs closed += temp repo . pull requests closed temp repo . open issues = repo . open issues count self . total open issues += temp repo . open issues temp repo . closed issues = my github . get issues ( repo , organization = organization ) temp repo . issues = temp repo . closed issues + temp repo . open issues self . total closed issues += temp repo . closed issues self . total issues += temp repo . issues my github . get languages ( repo , temp repo ) temp repo . readme = my github . get readme ( repo ) #temp repo.license = my github.get license(repo) temp repo . commits = self . get commits ( repo = repo , organization = organization ) self . total commits += temp repo . commits self . all repos . append ( temp repo )", "predictions": ["copies the specified repository from all contributors ( ."], "references": ["retrieves info about the repos of the current organization ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 2604, "code": "def get pull reqs ( self , repo ) : pull reqs open = 0 pull reqs closed = 0 for pull request in repo . iter pulls ( state = 'all' ) : self . pull requests json [ repo . name ] . append ( pull request . to json ( ) ) if pull request . closed at is not None : pull reqs closed += 1 else : pull reqs open += 1 return pull reqs open , pull reqs closed", "predictions": ["retrieve a get object from the get ) ."], "references": ["retrieves the number of pull requests on a repo in the organization ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 2605, "code": "def get issues ( self , repo , organization = 'llnl' ) : #JSON path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) is only today = False if not os . path . exists ( path ) : #no previous path, get all issues all issues = repo . iter issues ( state = 'all' ) is only today = True else : files = os . listdir ( path ) date = str ( files [ - 1 ] [ : - 5 ] ) if date == str ( datetime . date . today ( ) ) : #most recent date is actually today, get previous most recent date if len ( files ) > 2 : date = str ( files [ - 2 ] [ : - 5 ] ) else : #This means there is only one file, today. Retrieve every issue all issues = repo . iter issues ( state = 'all' ) is only today = True if not is only today : #there's a previous saved JSON that's not today all issues = repo . iter issues ( since = date , state = 'all' ) for issue in all issues : self . issues json [ repo . name ] . append ( issue . to json ( ) ) #CSV closed issues = 0 for issue in repo . iter issues ( state = 'closed' ) : if issue is not None : closed issues += 1 return closed issues", "predictions": ["remove all ( ( : \" name \" : \" : \" : \" : \" : \" : \" : \" : \" strip . 2 . 25 . 6 . . \""], "references": ["retrieves the number of closed issues ."], "bleu": 0.03551851328486764, "rouge_l": 0.05535390199637023}
{"id": 2606, "code": "def get license ( self , repo ) : if self . search limit >= 28 : print 'Hit search limit. Sleeping for 60 sec.' time . sleep ( 60 ) self . search limit = 0 self . search limit += 1 search results = self . logged in gh . search code ( 'license' + 'in:path repo:' + repo . full name ) try : for result in search results : path = result . path [ 1 : ] if '/' not in path and 'license' in path . lower ( ) : self . total licenses += 1 return path return 'MISS' except ( Stop Iteration ) as e : return 'MISS'", "predictions": ["retrieves a license license for the given return return the path ."], "references": ["checks to see if the given repo has a top level license file ."], "bleu": 0.13765233317534833, "rouge_l": 0.22761194029850743}
{"id": 2607, "code": "def write org json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' , is list = False ) : path = ( '../github-data/' + organization + '-org/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out clear : #clear old data out clear . close ( ) with open ( path , 'a' ) as out : if is list : #used for list of items out . write ( '[' ) for item in dict to write : out . write ( json . dumps ( dict to write [ item ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) out . seek ( - 1 , os . SEEK END ) #kill last comma out . truncate ( ) if is list : out . write ( ']' ) out . close ( )", "predictions": ["writes an ."], "references": ["writes stats from the organization to json ."], "bleu": 0.11230610537242834, "rouge_l": 0.3360881542699724}
{"id": 2608, "code": "def write repo json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' , is list = False , is dict = False ) : for repo in dict to write : path = ( '../github-data/' + organization + '/' + repo + '/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out : if is list : out . write ( '[' ) for value in dict to write [ repo ] : if is dict : for inner dict in value : out . write ( json . dumps ( inner dict , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) else : out . write ( json . dumps ( value , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) out . seek ( - 1 , os . SEEK END ) #kill last comma out . truncate ( ) out . write ( ']' ) else : out . write ( json . dumps ( dict to write [ repo ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) out . close ( )", "predictions": ["writes the ( . save the args to the output stream ."], "references": ["#writes repo specific data to json ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 2609, "code": "def write totals ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'N/A' , members = 0 , teams = 0 ) : total exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out total : if not total exists : out total . write ( 'date,organization,repos,members,teams,' + 'unique contributors,total contributors,forks,' + 'stargazers,pull requests,open issues,has readme,' + 'has license,pull requests open,pull requests closed,' + 'commits,id,closed issues,issues\\n' ) self . delete last line ( date = date , file path = file path ) out total . close ( ) with open ( file path , 'r' ) as file read : row count = sum ( 1 for row in file read ) - 1 file read . close ( ) with open ( file path , 'a' ) as out total : out total . write ( date + ',' + organization + ',' + str ( self . total repos ) + ',' + str ( members ) + ',' + str ( teams ) + ',' + str ( len ( self . unique contributors ) ) + ',' + str ( self . total contributors ) + ',' + str ( self . total forks ) + ',' + str ( self . total stars ) + ',' + str ( self . total pull reqs ) + ',' + str ( self . total open issues ) + ',' + str ( self . total readmes ) + ',' + str ( self . total licenses ) + ',' + str ( self . total pull reqs open ) + ',' + str ( self . total pull reqs closed ) + ',' + str ( self . total commits ) + ',' + str ( row count ) + ',' + str ( self . total closed issues ) + ',' + str ( self . total issues ) + '\\n' ) out total . close ( )", "predictions": ["update an strip with the given ."], "references": ["updates the total . csv file with current data ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 2610, "code": "def write languages ( self , file path = '' , date = str ( datetime . date . today ( ) ) ) : self . remove date ( file path = file path , date = date ) languages exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out languages : if not languages exists : out languages . write ( 'date,language,count,size,size log\\n' ) languages sorted = sorted ( self . languages size ) #self.delete last line(date=date, file path=file path) for language in languages sorted : try : out languages . write ( date + ',' + language + ',' + str ( self . languages [ language ] ) + ',' + str ( self . languages size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages size [ language ] ) ) ) + '\\n' ) except ( Type Error , Key Error ) as e : out languages . write ( date + ',' + language + ',' + str ( 0 ) + ',' + str ( self . languages size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages size [ language ] ) ) ) + '\\n' )", "predictions": ["update the ( . save the full required secrets ."], "references": ["updates languages . csv file with current data ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 2611, "code": "def connect ( url = , token = None ) : gh session = None if url == : gh session = create session ( token ) else : gh session = create enterprise session ( url , token ) if gh session is None : msg = 'Unable to connect to (%s) with provided token.' raise Runtime Error ( msg , url ) logger . info ( 'Connected to: %s' , url ) return gh session", "predictions": ["update connection to google cloud storage ."], "references": ["create a github session for making requests"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2612, "code": "def write to file ( self , file path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' ) : with open ( file path , 'w+' ) as out : out . write ( 'date,organization,stargazers\\n' ) sorted stargazers = sorted ( self . stargazers ) #sort based on lowercase for star in sorted stargazers : out . write ( star + ',' + str ( self . stargazers [ star ] ) + '\\n' ) out . close ( )", "predictions": ["search for ( by returning the full description of the ( account conn conn conn conn conn conn conn ."], "references": ["writes stargazers data to file ."], "bleu": 0.06108557268562171, "rouge_l": 0.08519553072625699}
{"id": 2613, "code": "def from gitlab ( klass , repository , labor hours = True ) : if not isinstance ( repository , gitlab . v4 . objects . Project ) : raise Type Error ( 'Repository must be a gitlab Repository object' ) project = klass ( ) logger . debug ( 'Git Lab: repository id=%d path with namespace=%s' , repository . id , repository . path with namespace , ) project [ 'name' ] = repository . name project [ 'repository URL' ] = repository . http url to repo project [ 'description' ] = repository . description project [ 'permissions' ] [ 'licenses' ] = None web url = repository . web url public server = web url . startswith ( 'https://gitlab.com' ) if repository . visibility in ( 'public' ) and public server : project [ 'permissions' ] [ 'usage Type' ] = 'open Source' elif date parse ( repository . created at ) < POLICY START DATE : project [ 'permissions' ] [ 'usage Type' ] = 'exempt By Policy Date' if labor hours : project [ 'labor Hours' ] = labor hours from url ( project [ 'repository URL' ] ) else : project [ 'labor Hours' ] = 0 project [ 'tags' ] = [ 'gitlab' ] + repository . tag list project [ 'contact' ] = { 'email' : '' , 'URL' : web url , } project [ 'organization' ] = repository . namespace [ 'name' ] project [ 'status' ] = 'Development' project [ 'vcs' ] = 'git' project [ 'homepage URL' ] = repository . web url api url = repository . manager . gitlab . url archive suffix = '/projects/%s/repository/archive' % repository . get id ( ) project [ 'download URL' ] = api url + archive suffix project [ 'date' ] = { 'created' : date parse ( repository . created at ) . date ( ) . isoformat ( ) , 'last Modified' : date parse ( repository . last activity at ) . date ( ) . isoformat ( ) , 'metadata Last Updated' : '' , } prune dict null str ( project ) return project", "predictions": ["creates the all headers in the all the given : : if : return null . if : does not have a all the : - if : 1 files have been returned ."], "references": ["create codegovproject object from gitlab repository"], "bleu": 0.029867390496386634, "rouge_l": 0.0}
{"id": 2614, "code": "def from stashy ( klass , repository , labor hours = True ) : if not isinstance ( repository , dict ) : raise Type Error ( 'Repository must be a dict' ) project = klass ( ) logger . debug ( 'Stashy: project key=%s repository slug=%s' , repository [ 'name' ] , repository [ 'project' ] [ 'key' ] , ) project [ 'name' ] = repository [ 'name' ] clone urls = [ clone [ 'href' ] for clone in repository [ 'links' ] [ 'clone' ] ] for url in clone urls : if url . startswith ( 'ssh://' ) : project [ 'repository URL' ] = url break description = repository [ 'project' ] . get ( 'description' , '' ) if description : project [ 'description' ] = 'Project description: %s' % description project [ 'permissions' ] [ 'licenses' ] = None web url = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] public server = web url . startswith ( 'https://bitbucket.org' ) if repository [ 'public' ] and public server : project [ 'permissions' ] [ 'usage Type' ] = 'open Source' if labor hours : project [ 'labor Hours' ] = labor hours from url ( project [ 'repository URL' ] ) else : project [ 'labor Hours' ] = 0 project [ 'tags' ] = [ 'bitbucket' ] project [ 'contact' ] [ 'email' ] = '' project [ 'contact' ] [ 'URL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] project [ 'status' ] = 'Development' project [ 'vcs' ] = repository [ 'scm Id' ] project [ 'homepage URL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] prune dict null str ( project ) return project", "predictions": ["creates the warning based on the : : return the warning and : if the : : otherwise return null ."], "references": ["handles crafting code . gov project for bitbucket server repositories"], "bleu": 0.05809665204409193, "rouge_l": 0.06892655367231638}
{"id": 2615, "code": "def force attributes ( metadata , config ) : organization = config . get ( 'organization' , '' ) logger . debug ( 'Organization: %s' , organization ) contact email = config . get ( 'contact email' ) logger . debug ( 'Contact Email: %s' , contact email ) permissions = config . get ( 'permissions' , { } ) default usage = permissions . get ( 'usage Type' , '' ) default exemption text = permissions . get ( 'exemption Text' , '' ) logger . debug ( 'Default usage Type: %s' , default usage ) logger . debug ( 'Default exemption Text: %s' , default exemption text ) if organization : logger . debug ( 'Forcing Organization to: %s' , organization ) if contact email : logger . debug ( 'Forcing Contact Email to: %s' , contact email ) for release in metadata [ 'releases' ] : if organization : release [ 'organization' ] = organization if contact email : release [ 'contact' ] [ 'email' ] = contact email if 'licenses' not in release [ 'permissions' ] : release [ 'permissions' ] [ 'licenses' ] = None if 'description' not in release : release [ 'description' ] = 'No description available...' if 'usage Type' not in release [ 'permissions' ] : release [ 'permissions' ] [ 'usage Type' ] = default usage release [ 'permissions' ] [ 'exemption Text' ] = default exemption text return metadata", "predictions": ["get the bucket and error from the server ."], "references": ["forces certain fields in the code . gov metadata json"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 2616, "code": "def get traffic ( self ) : print 'Getting traffic.' #Uses the developer API. Note this could change. headers = { 'Accept' : 'application/vnd.github.spiderman-preview' , 'Authorization' : 'token ' + self . token } headers release = { 'Authorization' : 'token ' + self . token } for repo in self . org retrieved . iter repos ( type = 'public' ) : url = ( 'https://api.github.com/repos/' + self . organization name + '/' + repo . name ) self . get referrers ( url = url , headers = headers , repo name = repo . name ) self . get paths ( url = url , headers = headers ) self . get data ( url = url , headers = headers , dict to store = self . views , type = 'views' , repo name = repo . name ) self . get data ( url = url , headers = headers , dict to store = self . clones , type = 'clones' , repo name = repo . name ) self . get releases ( url = url , headers = headers release , repo name = repo . name )", "predictions": ["creates a traffic for all processed repo and . ."], "references": ["retrieves the traffic for the repositories of the given organization ."], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 2617, "code": "def get releases ( self , url = '' , headers = { } , repo name = '' ) : url releases = ( url + '/releases' ) r = requests . get ( url releases , headers = headers ) self . releases json [ repo name ] = r . json ( )", "predictions": ["creates a get request for the specified ) ."], "references": ["retrieves the releases for the given repo in json ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 2618, "code": "def write json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' ) : for repo in dict to write : if len ( dict to write [ repo ] ) != 0 : #don't need to write out empty lists path = ( '../github-data/' + organization + '/' + repo + '/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out : out . write ( json . dumps ( dict to write [ repo ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) out . close ( )", "predictions": ["writes an = up to the ( ."], "references": ["writes all traffic data to file in json form ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 2619, "code": "def write to file ( self , referrers file path = '' , views file path = '' , clones file path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' , views row count = 0 , clones row count = 0 ) : self . write referrers to file ( file path = referrers file path ) self . write data to file ( file path = views file path , dict to write = self . views , name = 'views' , row count = views row count ) self . write data to file ( file path = clones file path , dict to write = self . clones , name = 'clones' , row count = clones row count )", "predictions": ["writes the contents of this cacheheader to the specified self ."], "references": ["writes all traffic data to file ."], "bleu": 0.1354599427337814, "rouge_l": 0.3472485768500949}
{"id": 2620, "code": "def write data to file ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , name = '' , row count = 0 ) : exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out : if not exists : out . write ( 'date,organization,' + name + ',unique ' + name + ',id\\n' ) sorted dict = sorted ( dict to write ) for day in sorted dict : day formatted = datetime . datetime . utcfromtimestamp ( day ) . strftime ( '%Y-%m-%d' ) out . write ( day formatted + ',' + organization + ',' + str ( dict to write [ day ] [ 0 ] ) + ',' + str ( dict to write [ day ] [ 1 ] ) + ',' + str ( row count ) + '\\n' ) row count += 1", "predictions": ["get the collection of ( and writes to the theme ."], "references": ["writes given dict to file ."], "bleu": 0.1354599427337814, "rouge_l": 0.3727087576374745}
{"id": 2621, "code": "def write referrers to file ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' ) : self . remove date ( file path = file path , date = date ) referrers exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out : if not referrers exists : out . write ( 'date,organization,referrer,count,count log,uniques,' + 'uniques logged\\n' ) sorted referrers = sorted ( self . referrers lower ) #sort based on lowercase for referrer in sorted referrers : ref name = self . referrers lower [ referrer ] #grab real name from count = self . referrers [ ref name ] [ 0 ] uniques = self . referrers [ ref name ] [ 1 ] if count == 1 : #so we don't display 0 for count of 1 count = 1.5 if uniques == 1 : uniques = 1.5 count logged = math . log ( count ) uniques logged = math . log ( uniques ) out . write ( date + ',' + organization + ',' + ref name + ',' + str ( count ) + ',' + str ( count logged ) + ',' + str ( uniques ) + ',' + str ( uniques logged ) + '\\n' ) out . close ( )", "predictions": ["writes a container object to the given self ."], "references": ["writes the referrers data to file ."], "bleu": 0.17747405280050263, "rouge_l": 0.38364779874213834}
{"id": 2622, "code": "def write to file ( self , file path = '' ) : with open ( file path , 'w+' ) as out : out . write ( 'user, email\\n' ) sorted names = sorted ( self . logins lower ) #sort based on lowercase for login in sorted names : out . write ( self . logins lower [ login ] + ',' + self . emails [ self . logins lower [ login ] ] + '\\n' ) out . close ( )", "predictions": ["rmi an array of strings into a self - separated self ."], "references": ["writes the user emails to file ."], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 2623, "code": "def connect ( url , username , password ) : bb session = stashy . connect ( url , username , password ) logger . info ( 'Connected to: %s as %s' , url , username ) return bb session", "predictions": ["creates and connects the connection to the server ."], "references": ["return a connected bitbucket session"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2624, "code": "def query repos ( gl session , repos = None ) : if repos is None : repos = [ ] for repo in repos : yield gl session . projects . get ( repo ) if not repos : for project in gl session . projects . list ( as list = False ) : yield project", "predictions": ["yield all currently repos that have been repos ."], "references": ["yields gitlab project objects for all projects in bitbucket"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2625, "code": "def prune dict null str ( dictionary ) : for key , value in list ( dictionary . items ( ) ) : if value is None or str ( value ) == '' : del dictionary [ key ] if isinstance ( value , dict ) : dictionary [ key ] = prune dict null str ( dictionary [ key ] ) return dictionary", "predictions": ["if dictionary is empty , then the key is ignored . if the key is empty , it will return null ."], "references": ["prune the none or emptry string values from dictionary items"], "bleu": 0.0612957497932821, "rouge_l": 0.06703296703296703}
{"id": 2626, "code": "def create tfs connection ( url , token ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs credentials = Basic Authentication ( '' , token ) tfs connection = Vss Connection ( base url = url , creds = tfs credentials ) return tfs connection", "predictions": ["create an http connection with the specified url ."], "references": ["creates the tfs connection context"], "bleu": 0.15619699684601276, "rouge_l": 0.1506172839506173}
{"id": 2627, "code": "def create tfs git client ( url , token = None ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs connection = create tfs connection ( url , token ) tfs git client = tfs connection . get client ( 'vsts.git.v4 1.git client.Git Client' ) if tfs git client is None : msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.' raise Runtime Error ( msg , url ) return tfs git client", "predictions": ["creates a git client ."], "references": ["creates a tfs git client to pull git repo info"], "bleu": 0.16620830006469267, "rouge_l": 0.5030927835051546}
{"id": 2628, "code": "def create tfs tfvc client ( url , token = None ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs connection = create tfs connection ( url , token ) tfs tfvc client = tfs connection . get client ( 'vsts.tfvc.v4 1.tfvc client.Tfvc Client' ) if tfs tfvc client is None : msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.' raise Runtime Error ( msg , url ) return tfs tfvc client", "predictions": ["creates a new tfs client ."], "references": ["creates a tfs tfvc client to pull tfvc repo info"], "bleu": 0.16959011078459055, "rouge_l": 0.47843137254901963}
{"id": 2629, "code": "def get git repos ( url , token , collection , project ) : git client = create tfs git client ( '{url}/{collection name}' . format ( url = url , collection name = collection . name ) , token ) logger . debug ( 'Retrieving Git Repos for Project: {project name}' . format ( project name = project . name ) ) return git client . get repositories ( project . id )", "predictions": ["creates and returns a git client with the specified name ."], "references": ["returns a list of all git repos for the supplied project within the supplied collection"], "bleu": 0.11840515855353498, "rouge_l": 0.2993865030674847}
{"id": 2630, "code": "def get tfvc repos ( url , token , collection , project ) : branch list = [ ] tfvc client = create tfs tfvc client ( '{url}/{collection name}' . format ( url = url , collection name = collection . name ) , token ) logger . debug ( 'Retrieving Tfvc Branches for Project: {project name}' . format ( project name = project . name ) ) branches = tfvc client . get branches ( project . id , True , True , False , True ) if branches : branch list . extend ( branches ) else : logger . debug ( 'No Tfvcc Branches in Project: {project name}' . format ( project name = project . name ) ) return branch list", "predictions": ["returns a list of branches for the given project ."], "references": ["returns a list of all tfvc branches for the supplied project within the supplied collection"], "bleu": 0.2931179403571363, "rouge_l": 0.6177215189873417}
{"id": 2631, "code": "def write to file ( self ) : with open ( '../github stats output/last year commits.csv' , 'w+' ) as output : output . write ( 'date,organization,repos,members,teams,' + 'unique contributors,total contributors,forks,' + 'stargazers,pull requests,open issues,has readme,' + 'has license,pull requests open,pull requests closed,' + 'commits\\n' ) #no reverse this time to print oldest first previous commits = 0 for week in self . sorted weeks : if str ( self . commits [ week ] ) != previous commits : #delete dups week formatted = datetime . datetime . utcfromtimestamp ( week ) . strftime ( '%Y-%m-%d' ) output . write ( week formatted + ',llnl,0,0,0,0,0,0,0,0,0,0,0,0,0,' + str ( self . commits [ week ] ) + '\\n' ) previous commits = str ( self . commits [ week ] )", "predictions": ["write a week now ."], "references": ["writes the weeks with associated commits to file ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2632, "code": "def incr ( self , stat , value = 1 , tags = None ) : self . client . incr ( stat = stat , count = value )", "predictions": ["leftshift original instance ."], "references": ["increment a counter ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2633, "code": "def timing ( self , stat , value , tags = None ) : self . client . timing ( stat = stat , delta = value )", "predictions": ["creates a new instance of . ."], "references": ["measure a timing for statistical distribution ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2634, "code": "def incr ( self , stat , value = 1 , tags = None ) : self . client . increment ( metric = stat , value = value , tags = tags )", "predictions": ["creates a new instance ."], "references": ["increment a counter ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 2635, "code": "def gauge ( self , stat , value , tags = None ) : self . client . gauge ( metric = stat , value = value , tags = tags )", "predictions": ["creates a new instance of the accumulated object ."], "references": ["set a gauge ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 2636, "code": "def timing ( self , stat , value , tags = None ) : self . client . timing ( metric = stat , value = value , tags = tags )", "predictions": ["creates a new instance of . ."], "references": ["measure a timing for statistical distribution ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2637, "code": "def histogram ( self , stat , value , tags = None ) : self . client . histogram ( metric = stat , value = value , tags = tags )", "predictions": ["we use this to histogram the histogram to histogram ."], "references": ["measure a value for statistical distribution ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2638, "code": "def incr ( self , stat , value = 1 , tags = None ) : self . log ( 'incr' , stat , value , tags )", "predictions": ["the graphical representation of the instance ."], "references": ["increment a counter ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2639, "code": "def gauge ( self , stat , value , tags = None ) : self . log ( 'gauge' , stat , value , tags )", "predictions": ["alias for . ."], "references": ["set a gauge ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2640, "code": "def timing ( self , stat , value , tags = None ) : self . log ( 'timing' , stat , value , tags )", "predictions": ["initializes the response: object for the specified streaminfo ."], "references": ["report a timing ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 2641, "code": "def histogram ( self , stat , value , tags = None ) : self . log ( 'histogram' , stat , value , tags )", "predictions": ["we only want to log the histogram at the end of the list ."], "references": ["report a histogram ."], "bleu": 0.09782375748961449, "rouge_l": 0.2469635627530364}
{"id": 2642, "code": "def rollup ( self ) : now = time . time ( ) if now < self . next rollup : return self . next rollup = now + self . flush interval for key , values in sorted ( self . incr stats . items ( ) ) : self . logger . info ( '%s INCR %s: count:%d|rate:%d/%d' , self . leader , key , len ( values ) , sum ( values ) , self . flush interval ) self . incr stats [ key ] = [ ] for key , values in sorted ( self . gauge stats . items ( ) ) : if values : self . logger . info ( '%s GAUGE %s: count:%d|current:%s|min:%s|max:%s' , self . leader , key , len ( values ) , values [ - 1 ] , min ( values ) , max ( values ) , ) else : self . logger . info ( '%s (gauge) %s: no data' , self . leader , key ) self . gauge stats [ key ] = [ ] for key , values in sorted ( self . histogram stats . items ( ) ) : if values : self . logger . info ( ( '%s HISTOGRAM %s: ' 'count:%d|min:%.2f|avg:%.2f|median:%.2f|ninety-five:%.2f|max:%.2f' ) , self . leader , key , len ( values ) , min ( values ) , statistics . mean ( values ) , statistics . median ( values ) , values [ int ( len ( values ) * 95 / 100 ) ] , max ( values ) ) else : self . logger . info ( '%s (histogram) %s: no data' , self . leader , key ) self . histogram stats [ key ] = [ ]", "predictions": ["consists of the group ."], "references": ["roll up stats and log them ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2643, "code": "def incr ( self , stat , value = 1 , tags = None ) : self . rollup ( ) self . incr stats . setdefault ( stat , [ ] ) . append ( value )", "predictions": ["leftshift for . ."], "references": ["increment a counter ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2644, "code": "def gauge ( self , stat , value , tags = None ) : self . rollup ( ) self . gauge stats . setdefault ( stat , [ ] ) . append ( value )", "predictions": ["attach to a step ."], "references": ["set a gauge ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 2645, "code": "def histogram ( self , stat , value , tags = None ) : self . rollup ( ) self . histogram stats . setdefault ( stat , [ ] ) . append ( value )", "predictions": ["use this decorator for creating stats ."], "references": ["measure a value for statistical distribution ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2646, "code": "def from db value ( self , value , expression , connection , context ) : if value is None : return value return self . enum [ value ]", "predictions": ["tries to convert the database to an amount of fields ."], "references": ["convert a string from the database into an enum value"], "bleu": 0.17033186037639278, "rouge_l": 0.384251968503937}
{"id": 2647, "code": "def to python ( self , value ) : if value is None : return value if isinstance ( value , self . enum ) : return value return self . enum [ value ]", "predictions": ["convert a resource type to an internal representation ."], "references": ["convert a string from a form into an enum value ."], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 2648, "code": "def get prep value ( self , value ) : if value is None : return None if isinstance ( value , self . enum ) : return value . name raise Value Error ( \"Unknown value {value:r} of type {cls}\" . format ( value = value , cls = type ( value ) ) )", "predictions": ["method to add a value to the underlying configuration ."], "references": ["convert an enum value into a string for the database"], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 2649, "code": "def t parse ( self , s ) : with self . lock : try : return self . parser . parse ( s , lexer = self . lexer , debug = False ) except Cannot Parse as e : e . s = s raise e", "predictions": ["helper for the parsing method ."], "references": ["parses the input string and returns a reference to the created ast s root"], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 2650, "code": "def parse ( self , s ) : with self . lock : try : return self . parser . parse ( s , lexer = self . lexer ) except Invalid IEML Object Argument as e : raise Cannot Parse ( s , str ( e ) ) except Cannot Parse as e : e . s = s raise e", "predictions": ["compatibility for parsing a lexer based on the string s . the method must be called before the parse method ."], "references": ["parses the input string and returns a reference to the created ast s root"], "bleu": 0.07645949399477267, "rouge_l": 0.17784256559766765}
{"id": 2651, "code": "def resolve path ( obj , path ) : if obj . class not in path . context . accept : result = set ( ) for ctx in path . context . accept : result |= { e for u in obj [ ctx ] for e in resolve path ( u , path ) } return result if isinstance ( obj , Text ) : if path . index is not None : return { obj . children [ path . index ] } return set ( obj . children ) if isinstance ( obj , ( Fact , Theory ) ) : return resolve path tree graph ( obj . tree graph , path ) if isinstance ( obj , Topic ) : if path . kind == 'r' : if path . index is not None : return { obj . root [ path . index ] } return set ( obj . root ) else : if path . index is not None : return { obj . flexing [ path . index ] } return set ( obj . flexing )", "predictions": ["resolve raw text and return a native object ."], "references": ["path is a mul of coord or a coord"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2652, "code": "def mean ( self ) : if self . counter . value > 0 : return self . sum . value / self . counter . value return 0.0", "predictions": ["computes the mean of this policy ."], "references": ["returns the mean value ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 2653, "code": "def mean rate ( self ) : if self . counter . value == 0 : return 0.0 else : elapsed = time ( ) - self . start time return self . counter . value / elapsed", "predictions": ["mean , or return the mean of this bulk ."], "references": ["returns the mean rate of the events since the start of the process ."], "bleu": 0.1263574671511449, "rouge_l": 0.32360742705570295}
{"id": 2654, "code": "def send metric ( self , name , metric ) : config = SERIALIZER CONFIG [ class name ( metric ) ] mmap ( self . buffered send metric , self . serialize metric ( metric , name , config [ 'keys' ] , config [ 'serialized type' ] ) ) if hasattr ( metric , 'snapshot' ) and config . get ( 'snapshot keys' ) : mmap ( self . buffered send metric , self . serialize metric ( metric . snapshot , name , config [ 'snapshot keys' ] , config [ 'serialized type' ] ) )", "predictions": ["convenience method that sends the metric to the java . awt . metric ."], "references": ["send metric and its snapshot ."], "bleu": 0.09782375748961449, "rouge_l": 0.21554770318021202}
{"id": 2655, "code": "def serialize metric ( self , metric , m name , keys , m type ) : return [ self . format metric string ( m name , getattr ( metric , key ) , m type ) for key in keys ]", "predictions": ["serialize all the registered metric values to a file ."], "references": ["serialize and send available measures of a metric ."], "bleu": 0.15851165692617156, "rouge_l": 0.31881533101045295}
{"id": 2656, "code": "def format metric string ( self , name , value , m type ) : template = '{name}:{value}|{m type}\\n' if self . prefix : name = \"{prefix}.{m name}\" . format ( prefix = self . prefix , m name = name ) return template . format ( name = name , value = value , m type = m type )", "predictions": ["formats repos for caller purposes ."], "references": ["compose a statsd compatible string for a metric s measurement ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 2657, "code": "def buffered send metric ( self , metric str ) : self . batch count += 1 self . batch buffer += metric str if self . batch count >= self . batch size : self . send ( )", "predictions": ["dict the prune null for a specific null ."], "references": ["add a metric to the buffer ."], "bleu": 0.16784459625186196, "rouge_l": 0.2557651991614256}
{"id": 2658, "code": "def json safe ( data ) : if not hasattr ( data , 'encode' ) : try : data = data . decode ( 'utf-8' ) except Unicode Decode Error : raise Value Error ( 'Expected valid UTF8 for JSON data, got %r' % ( data , ) ) return data", "predictions": ["coerce ( or a url url url url url url url url url url url url url url url url url url url url url url url url url url url url url url url url encoded url url url url url url url url url url url url encoded"], "references": ["json . loads wants an unistr in python3 . convert it ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 2659, "code": "def solve ( grid ) : clauses = sudoku clauses ( ) for i in range ( 1 , 10 ) : for j in range ( 1 , 10 ) : d = grid [ i - 1 ] [ j - 1 ] if d : clauses . append ( [ v ( i , j , d ) ] ) sol = set ( pycosat . solve ( clauses ) ) def read cell ( i , j ) : for d in range ( 1 , 10 ) : if v ( i , j , d ) in sol : return d for i in range ( 1 , 10 ) : for j in range ( 1 , 10 ) : grid [ i - 1 ] [ j - 1 ] = read cell ( i , j )", "predictions": ["create a git git git operation ."], "references": ["solve a sudoku grid inplace"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2660, "code": "def view ( injector ) : handler = create handler ( View , injector ) apply http methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["creates a create a create create the create create create the create create ."], "references": ["create django class - based view from injector class ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 2661, "code": "def form view ( injector ) : handler = create handler ( Form View , injector ) apply form methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["creates a get view for the given ( if any url is specified url url url url url url url url url ."], "references": ["create django form processing class - based view from injector class ."], "bleu": 0.05856458233275369, "rouge_l": 0.1211519364448858}
{"id": 2662, "code": "def method view ( injector ) : handler = create handler ( Method View ) apply http methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["creates the get get method ."], "references": ["create flask method based dispatching view from injector class ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 2663, "code": "def api view ( injector ) : handler = create handler ( API View , injector ) apply http methods ( handler , injector ) apply api view methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["creates a to year ."], "references": ["create drf class - based api view from injector class ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 2664, "code": "def generic api view ( injector ) : handler = create handler ( Generic API View , injector ) apply http methods ( handler , injector ) apply api view methods ( handler , injector ) apply generic api view methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["create method that creates the self - self ( view value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value value"], "references": ["create drf generic class - based api view from injector class ."], "bleu": 0.028577262451992175, "rouge_l": 0.10879904875148634}
{"id": 2665, "code": "def model view set ( injector ) : handler = create handler ( Model View Set , injector ) apply api view methods ( handler , injector ) apply generic api view methods ( handler , injector ) apply model view set methods ( handler , injector ) return injector . let ( as viewset = lambda : handler )", "predictions": ["creates a timing from the given stat . this method is used when the model is complete ."], "references": ["create drf model view set from injector class ."], "bleu": 0.08097785064266204, "rouge_l": 0.15762273901808785}
{"id": 2666, "code": "def stream from fd ( fd , loop ) : reader = asyncio . Stream Reader ( loop = loop ) protocol = asyncio . Stream Reader Protocol ( reader , loop = loop ) waiter = asyncio . futures . Future ( loop = loop ) transport = Unix File Descriptor Transport ( loop = loop , fileno = fd , protocol = protocol , waiter = waiter , ) try : yield from waiter except Exception : transport . close ( ) if loop . get debug ( ) : logger . debug ( \"Read fd %r connected: (%r, %r)\" , fd , transport , protocol ) return reader , transport", "predictions": ["create a incr object from the given ) and passes the bytes to the given file ."], "references": ["recieve a streamer for a given file descriptor ."], "bleu": 0.10802314890908067, "rouge_l": 0.3257676902536716}
{"id": 2667, "code": "def read ready ( self ) : try : data = os . read ( self . fileno , self . max size ) except Interrupted Error : pass except OS Error as exc : self . fatal error ( exc , \"Fatal read error on file descriptor read\" ) else : if data : self . protocol . data received ( data ) else : if self . loop . get debug ( ) : logger . info ( \"%r was closed by the kernel\" , self ) self . closing = False self . pause reading ( ) self . loop . call soon ( self . protocol . eof received ) self . loop . call soon ( self . call connection lost , None )", "predictions": ["to gauge . this is meant to be called once the other class has been set ."], "references": ["called by the event loop whenever the fd is ready for reading ."], "bleu": 0.09083627868206415, "rouge_l": 0.20492721164613661}
{"id": 2668, "code": "def close ( self , error = None ) : self . closing = True self . pause reading ( ) self . loop . call soon ( self . call connection lost , error )", "predictions": ["closes the request and closes all queued messages ."], "references": ["actual closing code both from manual close and errors ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 2669, "code": "def watch ( self , path , flags , * , alias = None ) : if alias is None : alias = path if alias in self . requests : raise Value Error ( \"A watch request is already scheduled for alias %s\" % alias ) self . requests [ alias ] = ( path , flags ) if self . fd is not None : self . setup watch ( alias , path , flags )", "predictions": ["the graphical representation of this object ."], "references": ["add a new watching rule ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2670, "code": "def unwatch ( self , alias ) : if alias not in self . descriptors : raise Value Error ( \"Unknown watch alias %s; current set is %r\" % ( alias , list ( self . descriptors . keys ( ) ) ) ) wd = self . descriptors [ alias ] errno = Lib C . inotify rm watch ( self . fd , wd ) if errno != 0 : raise IO Error ( \"Failed to close watcher %d: errno=%d\" % ( wd , errno ) ) del self . descriptors [ alias ] del self . requests [ alias ] del self . aliases [ wd ]", "predictions": ["create a shallow watch for the given stat ."], "references": ["stop watching a given rule ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 2671, "code": "def setup watch ( self , alias , path , flags ) : assert alias not in self . descriptors , \"Registering alias %s twice!\" % alias wd = Lib C . inotify add watch ( self . fd , path , flags ) if wd < 0 : raise IO Error ( \"Error setting up watch on %s with flags %s: wd=%s\" % ( path , flags , wd ) ) self . descriptors [ alias ] = wd self . aliases [ wd ] = alias", "predictions": ["sets up a ( and setting it to the class path ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ."], "references": ["actual rule setup ."], "bleu": 0.02614431568998955, "rouge_l": 0.04714064914992272}
{"id": 2672, "code": "def setup ( self , loop ) : self . loop = loop self . fd = Lib C . inotify init ( ) for alias , ( path , flags ) in self . requests . items ( ) : self . setup watch ( alias , path , flags ) self . stream , self . transport = yield from aioutils . stream from fd ( self . fd , loop )", "predictions": ["create the market method for the class ."], "references": ["start the watcher registering new watches if any ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 2673, "code": "def touch ( self ) : assert not self . has responded self . trigger ( event . TOUCH , message = self )", "predictions": ["creates a problem for first subtracting ."], "references": ["respond to nsqd that you need more time to process the message ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 2674, "code": "def success ( self ) : if self . interval == 0.0 : return self . short interval -= self . short unit self . long interval -= self . long unit self . short interval = max ( self . short interval , Decimal ( 0 ) ) self . long interval = max ( self . long interval , Decimal ( 0 ) ) self . update interval ( )", "predictions": ["evaluates the log mechanism for this visualization ."], "references": ["update the timer to reflect a successfull call"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 2675, "code": "def failure ( self ) : self . short interval += self . short unit self . long interval += self . long unit self . short interval = min ( self . short interval , self . max short timer ) self . long interval = min ( self . long interval , self . max long timer ) self . update interval ( )", "predictions": ["call to get a incr object ."], "references": ["update the timer to reflect a failed call"], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 2676, "code": "def close ( self ) : for conn in self . conns . values ( ) : conn . close ( ) self . redist periodic . stop ( ) if self . query periodic is not None : self . query periodic . stop ( )", "predictions": ["closes all open queries ."], "references": ["closes all connections stops all periodic callbacks"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 2677, "code": "def set max in flight ( self , max in flight ) : assert isinstance ( max in flight , int ) self . max in flight = max in flight if max in flight == 0 : for conn in itervalues ( self . conns ) : if conn . rdy > 0 : logger . debug ( '[%s:%s] rdy: %d -> 0' , conn . id , self . name , conn . rdy ) self . send rdy ( conn , 0 ) self . total rdy = 0 else : self . need rdy redistributed = True self . redistribute rdy state ( )", "predictions": ["for each item in this method will be called multiple times ."], "references": ["dynamically adjust the reader max_in_flight . set to 0 to immediately disable a reader"], "bleu": 0.0879520090657992, "rouge_l": 0.07587064676616916}
{"id": 2678, "code": "def score function ( self , x , W ) : if ( self . svm kernel == 'polynomial kernel' or self . svm kernel == 'gaussian kernel' or self . svm kernel == 'soft polynomial kernel' or self . svm kernel == 'soft gaussian kernel' ) : x = x [ 1 : ] score = np . sign ( np . sum ( self . sv alpha * self . sv Y * utility . Kernel . kernel matrix x X ( self , x , self . sv X ) ) + self . sv avg b ) else : score = np . sign ( np . inner ( x , W ) ) return score", "predictions": ["from this method . the from the original db is a from the original or a is a from the original or a is a from the original or a from being a previous call ."], "references": ["score function to calculate score"], "bleu": 0.028182998883818942, "rouge_l": 0.0}
{"id": 2679, "code": "def score function ( self , x , W ) : score = super ( Binary Classifier , self ) . score function ( x , W ) if score >= 0.5 : score = 1.0 else : score = - 1.0 return score", "predictions": ["the to calculate the to be used for methods in this bulk call ."], "references": ["score function to calculate score"], "bleu": 0.11633270842295028, "rouge_l": 0.23018867924528305}
{"id": 2680, "code": "def score function ( self , x , W ) : score = self . sign * np . sign ( x [ self . feature index ] - self . theta ) return score", "predictions": ["returns the get get method at the specified object ."], "references": ["score function to calculate score"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2681, "code": "def set feature transform ( self , mode = 'polynomial' , degree = 1 ) : if self . status != 'load train data' : print ( \"Please load train data first.\" ) return self . train X self . feature transform mode = mode self . feature transform degree = degree self . train X = self . train X [ : , 1 : ] self . train X = utility . Dataset Loader . feature transform ( self . train X , self . feature transform mode , self . feature transform degree ) return self . train X", "predictions": ["creates a parse ( ( x , y try and except that it is a bit try to do so on the first parse lexer lexer lexer lexer . try to t support ( self . try to use the except except for each parse self . try to do"], "references": ["transform data feature to high level"], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 2682, "code": "def score function ( self , x , W ) : score = self . theta ( np . inner ( x , W ) ) return score", "predictions": ["call this method to get the parse parse operator ."], "references": ["score function to calculate score"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 2683, "code": "def clean up ( fastq pairs , clear ) : unpaired fastq = [ f for f in os . listdir ( \".\" ) if f . endswith ( \" U.fastq.gz\" ) ] for fpath in unpaired fastq : os . remove ( fpath ) expected out = [ f for f in os . listdir ( \".\" ) if f . endswith ( \" trim.fastq.gz\" ) ] if clear == \"true\" and len ( expected out ) == 2 : for fq in fastq pairs : rp = os . path . realpath ( fq ) logger . debug ( \"Removing temporary fastq file path: {}\" . format ( rp ) ) if re . match ( \".*/work/.{2}/.{30}/.*\" , rp ) : os . remove ( rp )", "predictions": ["remove the purchased obj"], "references": ["cleans the working directory of unwanted temporary files"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2684, "code": "def check required files ( self ) : if not os . path . exists ( self . trace file ) : raise eh . Inspection Error ( \"The provided trace file could not be \" \"opened: {}\" . format ( self . trace file ) ) if not os . path . exists ( self . log file ) : raise eh . Inspection Error ( \"The .nextflow.log files could not be \" \"opened. Are you sure you are in a \" \"nextflow project directory?\" )", "predictions": ["checks if the project exists for the current test ."], "references": ["checks whetner the trace and log files are available"], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 2685, "code": "def clear inspect ( self ) : self . trace info = defaultdict ( list ) self . process tags = { } self . process stats = { } self . samples = [ ] self . stored ids = [ ] self . stored log ids = [ ] self . time start = None self . time stop = None self . execution command = None self . nextflow version = None self . abort cause = None self . c = 0 for p in self . processes . values ( ) : p [ \"barrier\" ] = \"W\" for i in [ \"submitted\" , \"finished\" , \"failed\" , \"retry\" ] : p [ i ] = set ( )", "predictions": ["for 5 providers . should be called during the call to mean if they were acquired ."], "references": ["clears inspect attributes when re - executing a pipeline"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 2686, "code": "def update barrier status ( self ) : with open ( self . log file ) as fh : for line in fh : if \"Session aborted\" in line : return if \"<<< barrier arrive\" in line : process m = re . match ( \".*process: (.*)\\)\" , line ) if process m : process = process m . group ( 1 ) if process in self . processes : self . processes [ process ] [ \"barrier\" ] = \"C\"", "predictions": ["updates all metric lines of this object ."], "references": ["checks whether the channels to each process have been closed ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 2687, "code": "def display overview ( self ) : stay alive = True self . screen = curses . initscr ( ) self . screen . keypad ( True ) self . screen . nodelay ( - 1 ) curses . cbreak ( ) curses . noecho ( ) curses . start color ( ) self . screen lines = self . screen . getmaxyx ( ) [ 0 ] try : while stay alive : self . curses keybindings ( ) self . update inspection ( ) self . flush overview ( ) sleep ( self . refresh rate ) except File Not Found Error : sys . stderr . write ( colored print ( \"ERROR: nextflow log and/or trace files are no longer \" \"reachable!\" , \"red bold\" ) ) except Exception as e : sys . stderr . write ( str ( e ) ) finally : curses . nocbreak ( ) self . screen . keypad ( 0 ) curses . echo ( ) curses . endwin ( )", "predictions": ["displays a list of ( for the given unit ."], "references": ["displays the default pipeline inspection overview"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2688, "code": "def updown ( self , direction ) : if direction == \"up\" and self . top line != 0 : self . top line -= 1 elif direction == \"down\" and self . screen . getmaxyx ( ) [ 0 ] + self . top line <= self . content lines + 3 : self . top line += 1", "predictions": ["inserts for this routers at the top of the direction ."], "references": ["provides curses scroll functionality ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 2689, "code": "def rightleft ( self , direction ) : if direction == \"left\" and self . padding != 0 : self . padding -= 1 if direction == \"right\" and self . screen . getmaxyx ( ) [ 1 ] + self . padding < self . max width : self . padding += 1", "predictions": ["a simple class for the rightleft ."], "references": ["provides curses horizontal padding"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2690, "code": "def get run hash ( self ) : pipeline path = get nextflow filepath ( self . log file ) pipeline hash = hashlib . md5 ( ) with open ( pipeline path , \"rb\" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b\"\" ) : pipeline hash . update ( chunk ) workdir = self . workdir . encode ( \"utf8\" ) hostname = socket . gethostname ( ) . encode ( \"utf8\" ) hardware addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) dir hash = hashlib . md5 ( workdir + hostname + hardware addr ) return pipeline hash . hexdigest ( ) + dir hash . hexdigest ( )", "predictions": ["run the hash on the data ."], "references": ["gets the hash of the nextflow file"], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 2691, "code": "def write report data ( self ) : json plot = self . get plot data ( ) json table = self . get table data ( ) json dic = { * * json plot , * * json table } with open ( \".report.json\" , \"w\" ) as json report : json report . write ( json . dumps ( json dic , separators = ( \",\" , \":\" ) ) )", "predictions": ["writes data to the specified json ."], "references": ["writes the json report to a json file"], "bleu": 0.2024757945132846, "rouge_l": 0.3952483801295896}
{"id": 2692, "code": "def build header ( self ) : logger . debug ( \"===============\" ) logger . debug ( \"Building header\" ) logger . debug ( \"===============\" ) self . template += hs . header", "predictions": ["create and initialize a new ( ."], "references": ["adds the header template to the master template string"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 2693, "code": "def build footer ( self ) : logger . debug ( \"===============\" ) logger . debug ( \"Building header\" ) logger . debug ( \"===============\" ) self . template += fs . footer", "predictions": ["create and initialize a footer from the database ."], "references": ["adds the footer template to the master template string"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 2694, "code": "def set status channels ( self ) : status inst = pc . Status Compiler ( template = \"status compiler\" ) report inst = pc . Report Compiler ( template = \"report compiler\" ) status channels = [ ] for p in [ p for p in self . processes ] : if not any ( [ isinstance ( p , x ) for x in self . skip class ] ) : status channels . extend ( p . status strs ) if not status channels : logger . debug ( \"No status channels found. Skipping status compiler\" \"process\" ) return logger . debug ( \"Setting status channels: {}\" . format ( status channels ) ) if len ( status channels ) != len ( set ( status channels ) ) : raise eh . Process Error ( \"Duplicate status channels detected. Please ensure that \" \"the 'status channels' attributes of each process are \" \"unique. Here are the status channels:\\n\\n{}\" . format ( \", \" . join ( status channels ) ) ) status inst . set compiler channels ( status channels ) report channels = [ \"REPORT {}\" . format ( x . lstrip ( \"STATUS \" ) ) for x in status channels ] report inst . set compiler channels ( report channels ) self . processes . extend ( [ status inst , report inst ] )", "predictions": ["for each compiler . set the status of the compiler to the report ."], "references": ["compiles all status channels for the status compiler process"], "bleu": 0.13217947626377288, "rouge_l": 0.3620178041543027}
{"id": 2695, "code": "def export directives ( self ) : directives json = { } for p in self . processes [ 1 : ] : directives json [ p . template ] = p . directives sys . stdout . write ( json . dumps ( directives json ) )", "predictions": ["export all stdout and json rows for this object ."], "references": ["export pipeline directives as a json to stdout"], "bleu": 0.14991106946711685, "rouge_l": 0.22676579925650556}
{"id": 2696, "code": "def get report id ( self ) : if self . watch : pipeline path = get nextflow filepath ( self . log file ) pipeline hash = hashlib . md5 ( ) with open ( pipeline path , \"rb\" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b\"\" ) : pipeline hash . update ( chunk ) workdir = os . getcwd ( ) . encode ( \"utf8\" ) hostname = socket . gethostname ( ) . encode ( \"utf8\" ) hardware addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) dir hash = hashlib . md5 ( workdir + hostname + hardware addr ) return pipeline hash . hexdigest ( ) + dir hash . hexdigest ( ) else : with open ( self . report file ) as fh : report json = json . loads ( fh . read ( ) ) metadata = report json [ \"data\" ] [ \"results\" ] [ 0 ] [ \"nf Metadata\" ] try : report id = metadata [ \"script Id\" ] + metadata [ \"session Id\" ] except Key Error : raise eh . Report Error ( \"Incomplete or corrupt report JSON file \" \"missing the 'script Id' and/or 'session Id' \" \"metadata information\" ) return report id", "predictions": ["get the report of the report ."], "references": ["returns a hash of the reports json file"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 2697, "code": "def update log watch ( self ) : size stamp = os . path . getsize ( self . log file ) self . trace retry = 0 if size stamp and size stamp == self . log sizestamp : return else : logger . debug ( \"Updating log size stamp to: {}\" . format ( size stamp ) ) self . log sizestamp = size stamp self . update pipeline status ( )", "predictions": ["update the log file for this object ."], "references": ["parses nextflow log file and updates the run status"], "bleu": 0.20014292374951972, "rouge_l": 0.232824427480916}
{"id": 2698, "code": "def map w to data ( self ) : self . Wmapped index = vq ( self . data , self . W ) self . Wmapped = np . zeros ( self . W . shape ) for i , s in enumerate ( self . Wmapped index ) : self . Wmapped [ : , i ] = self . data [ : , s ]", "predictions": ["transforms this object to a shape ."], "references": ["return data points that are most similar to basis vectors w"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 2699, "code": "def median filter ( X , M = 8 ) : for i in range ( X . shape [ 1 ] ) : X [ : , i ] = filters . median filter ( X [ : , i ] , size = M ) return X", "predictions": ["filter the median of all items in this image using the specified filters ."], "references": ["median filter along the first axis of the feature matrix x ."], "bleu": 0.12090340630072073, "rouge_l": 0.3900255754475704}
{"id": 2700, "code": "def compute gaussian krnl ( M ) : g = signal . gaussian ( M , M // 3. , sym = True ) G = np . dot ( g . reshape ( - 1 , 1 ) , g . reshape ( 1 , - 1 ) ) G [ M // 2 : , : M // 2 ] = - G [ M // 2 : , : M // 2 ] G [ : M // 2 , M // 2 : ] = - G [ : M // 2 , M // 2 : ] return G", "predictions": ["compute the gaussian krnl from two bitsets"], "references": ["creates a gaussian kernel following foote s paper ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2701, "code": "def compute ssm ( X , metric = \"seuclidean\" ) : D = distance . pdist ( X , metric = metric ) D = distance . squareform ( D ) D /= D . max ( ) return 1 - D", "predictions": ["compute the metric name for a given metric ."], "references": ["computes the self - similarity matrix of x ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 2702, "code": "def pick peaks ( nc , L = 16 ) : offset = nc . mean ( ) / 20. nc = filters . gaussian filter1d ( nc , sigma = 4 ) th = filters . median filter ( nc , size = L ) + offset #th = filters.gaussian filter(nc, sigma=L/2., mode=\"nearest\") + offset peaks = [ ] for i in range ( 1 , nc . shape [ 0 ] - 1 ) : if nc [ i - 1 ] < nc [ i ] and nc [ i ] > nc [ i + 1 ] : if nc [ i ] > th [ i ] : peaks . append ( i ) #plt.plot(nc) #plt.plot(th) #for peak in peaks: #plt.axvline(peak) #plt.show() return peaks", "predictions": ["pick the shape from all filters"], "references": ["obtain peaks from a novelty curve using an adaptive threshold ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 2703, "code": "def gaussian filter ( X , M = 8 , axis = 0 ) : for i in range ( X . shape [ axis ] ) : if axis == 1 : X [ : , i ] = filters . gaussian filter ( X [ : , i ] , sigma = M / 2. ) elif axis == 0 : X [ i , : ] = filters . gaussian filter ( X [ i , : ] , sigma = M / 2. ) return X", "predictions": ["filter all filters that appear at the first axis"], "references": ["gaussian filter along the first axis of the feature matrix x ."], "bleu": 0.19902510067151713, "rouge_l": 0.3713850837138508}
{"id": 2704, "code": "def compute nc ( X ) : N = X . shape [ 0 ] nc = np . zeros ( N ) for i in range ( N - 1 ) : nc [ i ] = distance . euclidean ( X [ i , : ] , X [ i + 1 , : ] ) nc += np . abs ( nc . min ( ) ) nc /= float ( nc . max ( ) ) return nc", "predictions": ["computes the sum of two functions"], "references": ["computes the novelty curve from the structural features ."], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 2705, "code": "def pick peaks ( nc , L = 16 , offset denom = 0.1 ) : offset = nc . mean ( ) * float ( offset denom ) th = filters . median filter ( nc , size = L ) + offset #th = filters.gaussian filter(nc, sigma=L/2., mode=\"nearest\") + offset #import pylab as plt #plt.plot(nc) #plt.plot(th) #plt.show() peaks = [ ] for i in range ( 1 , nc . shape [ 0 ] - 1 ) : if nc [ i - 1 ] < nc [ i ] and nc [ i ] > nc [ i + 1 ] : if nc [ i ] > th [ i ] : peaks . append ( i ) return peaks", "predictions": ["pick the shape of each input channel to be between two corners ."], "references": ["obtain peaks from a novelty curve using an adaptive threshold ."], "bleu": 0.09552040806823771, "rouge_l": 0.08460471567267684}
{"id": 2706, "code": "def embedded space ( X , m , tau = 1 ) : N = X . shape [ 0 ] - int ( np . ceil ( m ) ) Y = np . zeros ( ( N , int ( np . ceil ( X . shape [ 1 ] * m ) ) ) ) for i in range ( N ) : rem = int ( ( m % 1 ) * X . shape [ 1 ] ) Y [ i , : ] = np . concatenate ( ( X [ i : i + int ( m ) , : ] . flatten ( ) , X [ i + int ( m ) , : rem ] ) ) return Y", "predictions": ["compute the embedded space between x and y values based on the x and y ."], "references": ["time - delay embedding with m dimensions and tau delays ."], "bleu": 0.08513012360883544, "rouge_l": 0.15326633165829145}
{"id": 2707, "code": "def plot one track ( file struct , est times , est labels , boundaries id , labels id , title = None ) : import matplotlib . pyplot as plt bid lid = boundaries id if labels id is not None : bid lid += \" + \" + labels id try : jam = jams . load ( file struct . ref file ) ann = jam . search ( namespace = 'segment .*' ) [ 0 ] ref inters , ref labels = ann . to interval values ( ) ref times = utils . intervals to times ( ref inters ) all boundaries = [ ref times , est times ] all labels = [ ref labels , est labels ] algo ids = [ \"GT\" , bid lid ] except : logging . warning ( \"No references found in %s. Not plotting groundtruth\" % file struct . ref file ) all boundaries = [ est times ] all labels = [ est labels ] algo ids = [ bid lid ] N = len ( all boundaries ) for i , labels in enumerate ( all labels ) : all labels [ i ] = mir eval . util . index labels ( labels ) [ 0 ] cm = plt . get cmap ( 'gist rainbow' ) max label = max ( max ( labels ) for labels in all labels ) figsize = ( 8 , 4 ) plt . figure ( 1 , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) for i , boundaries in enumerate ( all boundaries ) : color = \"b\" if i == 0 : color = \"g\" for b in boundaries : plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) if labels id is not None : labels = all labels [ i ] inters = utils . times to intervals ( boundaries ) for label , inter in zip ( labels , inters ) : plt . axvspan ( inter [ 0 ] , inter [ 1 ] , ymin = i / float ( N ) , ymax = ( i + 1 ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max label ) ) ) plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = 1 ) plot formatting ( title , os . path . basename ( file struct . audio file ) , algo ids , all boundaries [ 0 ] [ - 1 ] , N , None )", "predictions": ["plot the one set of boundaries ."], "references": ["plots the results of one track with ground truth if it exists ."], "bleu": 0.09912033646614596, "rouge_l": 0.2846034214618974}
{"id": 2708, "code": "def get dataset files ( in path ) : audio files = [ ] for ext in ds config . audio exts : audio files += glob . glob ( os . path . join ( in path , ds config . audio dir , \"*\" + ext ) ) utils . ensure dir ( os . path . join ( in path , ds config . features dir ) ) utils . ensure dir ( os . path . join ( in path , ds config . estimations dir ) ) utils . ensure dir ( os . path . join ( in path , ds config . references dir ) ) file structs = [ ] for audio file in audio files : file structs . append ( File Struct ( audio file ) ) file structs = sorted ( file structs , key = lambda file struct : file struct . audio file ) return file structs", "predictions": ["builds an iterable of audio files that can be used to look through each audio path in the audio dataset ."], "references": ["gets the files of the given dataset ."], "bleu": 0.09092617426809149, "rouge_l": 0.3001230012300123}
{"id": 2709, "code": "def get dataset file ( self , dir , ext ) : audio file ext = \".\" + self . audio file . split ( \".\" ) [ - 1 ] base file = os . path . basename ( self . audio file ) . replace ( audio file ext , ext ) return os . path . join ( self . ds path , dir , base file )", "predictions": ["read an audio file ."], "references": ["gets the desired dataset file ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 2710, "code": "def write features ( self ) : out json = collections . Ordered Dict ( ) try : self . read features ( ) except ( Wrong Features Format Error , Features Not Found , No Features File Error ) : out json = collections . Ordered Dict ( { \"metadata\" : { \"versions\" : { \"librosa\" : librosa . version , \"msaf\" : msaf . version , \"numpy\" : np . version } , \"timestamp\" : datetime . datetime . today ( ) . strftime ( \"%Y/%m/%d %H:%M:%S\" ) } } ) out json [ \"globals\" ] = { \"dur\" : self . dur , \"sample rate\" : self . sr , \"hop length\" : self . hop length , \"audio file\" : self . file struct . audio file } out json [ \"est beats\" ] = self . est beats times . tolist ( ) out json [ \"est beatsync times\" ] = self . est beatsync times . tolist ( ) if self . ann beats times is not None : out json [ \"ann beats\" ] = self . ann beats times . tolist ( ) out json [ \"ann beatsync times\" ] = self . ann beatsync times . tolist ( ) except Feature Params Error : with open ( self . file struct . features file ) as f : out json = json . load ( f ) finally : out json [ self . get id ( ) ] = { } out json [ self . get id ( ) ] [ \"params\" ] = { } for param name in self . get param names ( ) : value = getattr ( self , param name ) if hasattr ( value , ' call ' ) : value = value . name else : value = str ( value ) out json [ self . get id ( ) ] [ \"params\" ] [ param name ] = value out json [ self . get id ( ) ] [ \"framesync\" ] = self . framesync features . tolist ( ) out json [ self . get id ( ) ] [ \"est beatsync\" ] = self . est beatsync features . tolist ( ) if self . ann beatsync features is not None : out json [ self . get id ( ) ] [ \"ann beatsync\" ] = self . ann beatsync features . tolist ( ) with open ( self . file struct . features file , \"w\" ) as f : json . dump ( out json , f , indent = 2 )", "predictions": ["write the ( . save the ( style ."], "references": ["saves features to file ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2711, "code": "def compute framesync times ( self ) : self . framesync times = librosa . core . frames to time ( np . arange ( self . framesync features . shape [ 0 ] ) , self . sr , self . hop length )", "predictions": ["computes a set of frames on this object ."], "references": ["computes the framesync times based on the framesync features ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 2712, "code": "def preprocess ( self , valid features = [ \"pcp\" , \"tonnetz\" , \"mfcc\" , \"cqt\" , \"tempogram\" ] ) : if self . feature str not in valid features : raise Runtime Error ( \"Feature %s in not valid for algorithm: %s \" \"(valid features are %s).\" % ( self . feature str , name , valid features ) ) else : try : F = self . features . features except Key Error : raise Runtime Error ( \"Feature %s in not supported by MSAF\" % ( self . feature str ) ) return F", "predictions": ["preprocesses the request message using supported features ."], "references": ["this method obtains the actual features ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 2713, "code": "def main ( ) : parser = argparse . Argument Parser ( description = \"Runs the speficied algorithm(s) on the MSAF \" \"formatted dataset.\" , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( \"in path\" , action = \"store\" , help = \"Input dataset\" ) parser . add argument ( \"-f\" , action = \"store\" , dest = \"feature\" , default = \"pcp\" , type = str , help = \"Type of features\" , choices = [ \"pcp\" , \"tonnetz\" , \"mfcc\" , \"cqt\" , \"tempogram\" ] ) parser . add argument ( \"-b\" , action = \"store true\" , dest = \"annot beats\" , help = \"Use annotated beats\" , default = False ) parser . add argument ( \"-fs\" , action = \"store true\" , dest = \"framesync\" , help = \"Use frame-synchronous features\" , default = False ) parser . add argument ( \"-bid\" , action = \"store\" , help = \"Boundary algorithm identifier\" , dest = \"boundaries id\" , default = \"gt\" , choices = [ \"gt\" ] + io . get all boundary algorithms ( ) ) parser . add argument ( \"-lid\" , action = \"store\" , help = \"Label algorithm identifier\" , dest = \"labels id\" , default = None , choices = io . get all label algorithms ( ) ) parser . add argument ( \"-j\" , action = \"store\" , dest = \"n jobs\" , default = 4 , type = int , help = \"The number of threads to use\" ) args = parser . parse args ( ) start time = time . time ( ) process ( args . in path , annot beats = args . annot beats , feature = args . feature , framesync = args . framesync , boundaries id = args . boundaries id , labels id = args . labels id , n jobs = args . n jobs ) logging . info ( \"Done! Took %.2f seconds.\" % ( time . time ( ) - start time ) )", "predictions": ["creates and prepares the script for the command line ."], "references": ["main function to sweep parameters of a certain algorithm ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 2714, "code": "def main ( ) : parser = argparse . Argument Parser ( description = \"Runs the speficied algorithm(s) on the input file and \" \"the results using the MIREX format.\" , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( \"-bid\" , action = \"store\" , help = \"Boundary algorithm identifier\" , dest = \"boundaries id\" , default = msaf . config . default bound id , choices = [ \"gt\" ] + msaf . io . get all boundary algorithms ( ) ) parser . add argument ( \"-lid\" , action = \"store\" , help = \"Label algorithm identifier\" , dest = \"labels id\" , default = msaf . config . default label id , choices = msaf . io . get all label algorithms ( ) ) parser . add argument ( \"-i\" , action = \"store\" , dest = \"in file\" , help = \"Input audio file\" ) parser . add argument ( \"-o\" , action = \"store\" , dest = \"out file\" , help = \"Output file with the results\" , default = \"out.txt\" ) args = parser . parse args ( ) start time = time . time ( ) logging . basic Config ( format = '%(asctime)s: %(levelname)s: %(message)s' , level = logging . INFO ) params = { \"annot beats\" : False , \"feature\" : \"cqt\" , \"framesync\" : False , \"boundaries id\" : args . boundaries id , \"labels id\" : args . labels id , \"n jobs\" : 1 , \"hier\" : False , \"sonify bounds\" : False , \"plot\" : False } res = msaf . run . process ( args . in file , * * params ) msaf . io . write mirex ( res [ 0 ] , res [ 1 ] , args . out file ) logging . info ( \"Done! Took %.2f seconds.\" % ( time . time ( ) - start time ) )", "predictions": ["the main method to simulate the command line execution time for the command line ."], "references": ["main function to parse the arguments and call the main process ."], "bleu": 0.12874330508144843, "rouge_l": 0.3779429987608426}
{"id": 2715, "code": "def compute all features ( file struct , framesync ) : for feature id in msaf . features registry : logging . info ( \"Computing %s for file %s\" % ( feature id , file struct . audio file ) ) feats = Features . select features ( feature id , file struct , False , framesync ) feats . features", "predictions": ["creates a list of for all audio features ."], "references": ["computes all features for the given file ."], "bleu": 0.17747405280050263, "rouge_l": 0.35672514619883033}
{"id": 2716, "code": "def process ( in path , out file , n jobs , framesync ) : if os . path . isfile ( in path ) : file struct = msaf . io . File Struct ( in path ) file struct . features file = out file compute all features ( file struct , framesync ) else : file structs = msaf . io . get dataset files ( in path ) return Parallel ( n jobs = n jobs ) ( delayed ( compute all features ) ( file struct , framesync ) for file struct in file structs )", "predictions": ["process the entire tree of ( ."], "references": ["computes the features for the selected dataset or file ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2717, "code": "def main ( ) : parser = argparse . Argument Parser ( description = \"Extracts a set of features from a given dataset \" \"or audio file and saves them into the 'features' folder of \" \"the dataset or the specified single file.\" , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( \"in path\" , action = \"store\" , help = \"Input dataset dir or audio file\" ) parser . add argument ( \"-j\" , action = \"store\" , dest = \"n jobs\" , type = int , help = \"Number of jobs (only for collection mode)\" , default = 4 ) parser . add argument ( \"-o\" , action = \"store\" , dest = \"out file\" , type = str , help = \"Output file (only for single file mode)\" , default = \"out.json\" ) parser . add argument ( \"-d\" , action = \"store\" , dest = \"ds name\" , default = \"*\" , help = \"The prefix of the dataset to use \" \"(e.g. Isophonics, SALAMI)\" ) parser . add argument ( \"-fs\" , action = \"store true\" , dest = \"framesync\" , help = \"Use frame-synchronous features\" , default = False ) args = parser . parse args ( ) start time = time . time ( ) logging . basic Config ( format = '%(asctime)s: %(levelname)s: %(message)s' , level = logging . INFO ) process ( args . in path , out file = args . out file , n jobs = args . n jobs , framesync = args . framesync ) logging . info ( \"Done! Took %.2f seconds.\" % ( time . time ( ) - start time ) )", "predictions": ["creates the main loop ."], "references": ["main function to parse the arguments and call the main process ."], "bleu": 0.09521044541645862, "rouge_l": 0.3285457809694794}
{"id": 2718, "code": "def gaussian cost ( X ) : d , n = X . shape if n < 2 : return 0 sigma = np . var ( X , axis = 1 , ddof = 1 ) cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) return cost", "predictions": ["calculates the gaussian cost of a gaussian distribution ."], "references": ["return the average log - likelihood of data under a standard normal"], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 2719, "code": "def lognormalize ( F , floor = 0.1 , min db = - 80 ) : assert min db < 0 F = min max normalize ( F , floor = floor ) F = np . abs ( min db ) * np . log10 ( F ) return F", "predictions": ["return the index of the input field without changing its long value ."], "references": ["log - normalizes features such that each vector is between min_db to 0 ."], "bleu": 0.08844818008721958, "rouge_l": 0.0735826296743064}
{"id": 2720, "code": "def min max normalize ( F , floor = 0.001 ) : F += - F . min ( ) + floor F = F / F . max ( axis = 0 ) return F", "predictions": ["normalizes the condition up to the maximum of the values ."], "references": ["normalizes features such that each vector is between floor to 1 ."], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 2721, "code": "def get time frames ( dur , anal ) : n frames = get num frames ( dur , anal ) return np . linspace ( 0 , dur , num = n frames )", "predictions": ["returns a set of self self self self self self for this set of self self self ."], "references": ["gets the time frames and puts them in a numpy array ."], "bleu": 0.07535838128770536, "rouge_l": 0.13832199546485258}
{"id": 2722, "code": "def remove empty segments ( times , labels ) : assert len ( times ) - 1 == len ( labels ) inters = times to intervals ( times ) new inters = [ ] new labels = [ ] for inter , label in zip ( inters , labels ) : if inter [ 0 ] < inter [ 1 ] : new inters . append ( inter ) new labels . append ( label ) return intervals to times ( np . asarray ( new inters ) ) , new labels", "predictions": ["returns the run of this set of : : 1 . 2 . 3 . 0 . 0"], "references": ["removes empty segments if needed ."], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 2723, "code": "def distance ( self , idx ) : if scipy . sparse . issparse ( self . data ) : step = self . data . shape [ 1 ] else : step = 50000 d = np . zeros ( ( self . data . shape [ 1 ] ) ) if idx == - 1 : vec = np . zeros ( ( self . data . shape [ 0 ] , 1 ) ) if scipy . sparse . issparse ( self . data ) : vec = scipy . sparse . csc matrix ( vec ) else : vec = self . data [ : , idx : idx + 1 ] self . logger . info ( 'compute distance to node ' + str ( idx ) ) for idx start in range ( 0 , self . data . shape [ 1 ] , step ) : if idx start + step > self . data . shape [ 1 ] : idx end = self . data . shape [ 1 ] else : idx end = idx start + step d [ idx start : idx end ] = self . distfunc ( self . data [ : , idx start : idx end ] , vec ) self . logger . info ( 'completed:' + str ( idx end / ( self . data . shape [ 1 ] / 100.0 ) ) + \"%\" ) return d", "predictions": ["generate a write call to all table components ."], "references": ["compute distances of a specific data point to all other samples"], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 2724, "code": "def run kmeans ( self , X , K ) : w X = vq . whiten ( X ) means , dist = vq . kmeans ( w X , K , iter = 100 ) labels , dist = vq . vq ( w X , means ) return means , labels", "predictions": ["build and build the pearson means ."], "references": ["runs k - means and returns the labels assigned to the data ."], "bleu": 0.09912033646614596, "rouge_l": 0.2846034214618974}
{"id": 2725, "code": "def compute bic ( self , D , means , labels , K , R ) : D = vq . whiten ( D ) Rn = D . shape [ 0 ] M = D . shape [ 1 ] if R == K : return 1 mle var = 0 for k in range ( len ( means ) ) : X = D [ np . argwhere ( labels == k ) ] X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) for x in X : mle var += distance . euclidean ( x , means [ k ] ) #print x, means[k], mle var mle var /= float ( R - K ) l D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) p = ( K - 1 ) + M * K + mle var #print \"BIC:\", l D, p, R, K return l D - p / 2. * np . log ( R )", "predictions": ["compute the basic ( of a set of variables ."], "references": ["computes the bayesian information criterion ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2726, "code": "def magnitude ( X ) : r = np . real ( X ) i = np . imag ( X ) return np . sqrt ( r * r + i * i )", "predictions": ["make all the rows belonging to an array"], "references": ["magnitude of a complex matrix ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2727, "code": "def compute ffmc2d ( X ) : fft2 = scipy . fftpack . fft2 ( X ) fft2m = magnitude ( fft2 ) fftshift = scipy . fftpack . fftshift ( fft2m ) . flatten ( ) #cmap = plt.cm.get cmap('hot') #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation=\"nearest\", #plt.show() return fftshift [ : fftshift . shape [ 0 ] // 2 + 1 ]", "predictions": ["export x , y , z , h , h , h , h , h , h , h )"], "references": ["computes the 2d - fourier magnitude coefficients ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 2728, "code": "def compute labels ( X , rank , R , bound idxs , niter = 300 ) : try : F , G = cnmf ( X , rank , niter = niter , hull = False ) except : return [ 1 ] label frames = filter activation matrix ( G . T , R ) label frames = np . asarray ( label frames , dtype = int ) #labels = [label frames[0]] labels = [ ] bound inters = zip ( bound idxs [ : - 1 ] , bound idxs [ 1 : ] ) for bound inter in bound inters : if bound inter [ 1 ] - bound inter [ 0 ] <= 0 : labels . append ( np . max ( label frames ) + 1 ) else : labels . append ( most frequent ( label frames [ bound inter [ 0 ] : bound inter [ 1 ] ] ) ) #print bound inter, labels[-1] #labels.append(label frames[-1]) return labels", "predictions": ["compute the report for a elevation threshold"], "references": ["computes the labels using the bounds ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2729, "code": "def filter activation matrix ( G , R ) : #import pylab as plt #plt.imshow(G, interpolation=\"nearest\", aspect=\"auto\") #plt.show() idx = np . argmax ( G , axis = 1 ) max idx = np . arange ( G . shape [ 0 ] ) max idx = ( max idx , idx . flatten ( ) ) G [ : , : ] = 0 G [ max idx ] = idx + 1 G = np . sum ( G , axis = 1 ) G = median filter ( G [ : , np . newaxis ] , R ) return G . flatten ( )", "predictions": ["update the log likelihood of g ."], "references": ["filters the activation matrix g and returns a flattened copy ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 2730, "code": "def main ( ) : args = command line ( ) translate = partial ( translator , args . source , args . dest , version = ' ' . join ( [ version , build ] ) ) return source ( spool ( set task ( translate , translit = args . translit ) ) , args . text )", "predictions": ["the main entry point for the command line ."], "references": ["main entry point for translator and argument parser"], "bleu": 0.392814650900513, "rouge_l": 0.4756335282651072}
{"id": 2731, "code": "def print table ( language ) : table = translation table ( language ) for code , name in sorted ( table . items ( ) , key = operator . itemgetter ( 0 ) ) : print ( u'{language:<8} {name:\\u3000<20}' . format ( name = name , language = code ) ) return None", "predictions": ["prints the stack trace of all the information in a language ."], "references": ["generates a formatted table of language codes"], "bleu": 0.1235622127262679, "rouge_l": 0.22101449275362317}
{"id": 2732, "code": "def disable ( self ) : if not self . active : return None self . mock engine . disable ( ) self . active = False", "predictions": ["the first step of this cipher ."], "references": ["disables interceptors and stops intercepting any outgoing http traffic ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2733, "code": "def get setting ( connection , key ) : if key in connection . settings dict : return connection . settings dict [ key ] else : return getattr ( settings , key )", "predictions": ["gets the ssm of the given metric ."], "references": ["get key from connection or default to settings ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2734, "code": "def as sql ( self , compiler , connection ) : sql , params = super ( Decrypted Col , self ) . as sql ( compiler , connection ) sql = self . target . get decrypt sql ( connection ) % ( sql , self . target . get cast sql ( ) ) return sql , params", "predictions": ["simulates this managed object into a compiler ."], "references": ["build sql with decryption and casting ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2735, "code": "def pre save ( self , model instance , add ) : if self . original : original value = getattr ( model instance , self . original ) setattr ( model instance , self . attname , original value ) return super ( Hash Mixin , self ) . pre save ( model instance , add )", "predictions": ["adds the i to the right of this ( ."], "references": ["save the original_value ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 2736, "code": "def get col ( self , alias , output field = None ) : if output field is None : output field = self if alias != self . model . meta . db table or output field != self : return Decrypted Col ( alias , self , output field ) else : return self . cached col", "predictions": ["get the output for the = = = = = = = = = = & we should only compute the output flag ."], "references": ["get the decryption for col ."], "bleu": 0.07575731225158963, "rouge_l": 0.29901960784313725}
{"id": 2737, "code": "def get placeholder ( self , value = None , compiler = None , connection = None ) : return self . encrypt sql . format ( get setting ( connection , 'PUBLIC PGP KEY' ) )", "predictions": ["mean to provide an object with this connection ."], "references": ["tell postgres to encrypt this field using pgp ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 2738, "code": "def attach to tree ( self ) : for clade in self . tree . find clades ( ) : if clade . up is not None : clade . branch length interpolator . merger cost = self . cost", "predictions": ["embedded in this object to the ( copy zeros m : ( copy of this object m = 100 , 1 , 2 , y2 : m . 2 . 1 ."], "references": ["attaches the the merger cost to each branch length interpolator in the tree ."], "bleu": 0.04750133160738243, "rouge_l": 0.14033742331288343}
{"id": 2739, "code": "def optimize Tc ( self ) : from scipy . optimize import minimize scalar initial Tc = self . Tc def cost ( Tc ) : self . set Tc ( Tc ) return - self . total LH ( ) sol = minimize scalar ( cost , bounds = [ ttconf . TINY NUMBER , 10.0 ] ) if \"success\" in sol and sol [ \"success\" ] : self . set Tc ( sol [ 'x' ] ) else : self . logger ( \"merger models:optimze Tc: optimization of coalescent time scale failed: \" + str ( sol ) , 0 , warn = True ) self . set Tc ( initial Tc . y , T = initial Tc . x )", "predictions": ["plot a ( cost operation ."], "references": ["determines the coalescent time scale that optimizes the coalescent likelihood of the tree"], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 2740, "code": "def prepare nodes ( self ) : self . tree . root . up = None self . tree . root . bad branch = self . tree . root . bad branch if hasattr ( self . tree . root , 'bad branch' ) else False internal node count = 0 for clade in self . tree . get nonterminals ( order = 'preorder' ) : internal node count += 1 if clade . name is None : clade . name = \"NODE \" + format ( self . internal node count , '07d' ) self . internal node count += 1 for c in clade . clades : if c . is terminal ( ) : c . bad branch = c . bad branch if hasattr ( c , 'bad branch' ) else False c . up = clade for clade in self . tree . get nonterminals ( order = 'postorder' ) : clade . bad branch = all ( [ c . bad branch for c in clade ] ) self . calc dist2root ( ) self . internal node count = max ( internal node count , self . internal node count )", "predictions": ["prepares stats for each : applies to the data store ."], "references": ["set auxilliary parameters to every node of the tree ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 2741, "code": "def create gtr ( params ) : model = params . gtr gtr params = params . gtr params if model == 'infer' : gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) else : try : kwargs = { } if gtr params is not None : for param in gtr params : keyval = param . split ( '=' ) if len ( keyval ) != 2 : continue if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : keyval [ 0 ] = 'pi' keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) elif keyval [ 0 ] not in [ 'alphabet' ] : keyval [ 1 ] = float ( keyval [ 1 ] ) kwargs [ keyval [ 0 ] ] = keyval [ 1 ] else : print ( \"GTR params are not specified. Creating GTR model with default parameters\" ) gtr = GTR . standard ( model , * * kwargs ) infer gtr = False except : print ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) infer gtr = False return gtr", "predictions": ["get parameters for the given parameters ."], "references": ["parse the arguments referring to the gtr model and return a gtr structure"], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 2742, "code": "def read if vcf ( params ) : ref = None aln = params . aln fixed pi = None if hasattr ( params , 'aln' ) and params . aln is not None : if any ( [ params . aln . lower ( ) . endswith ( x ) for x in [ '.vcf' , '.vcf.gz' ] ] ) : if not params . vcf reference : print ( \"ERROR: a reference Fasta is required with VCF-format alignments\" ) return - 1 compress seq = read vcf ( params . aln , params . vcf reference ) sequences = compress seq [ 'sequences' ] ref = compress seq [ 'reference' ] aln = sequences if not hasattr ( params , 'gtr' ) or params . gtr == \"infer\" : #if not specified, set it: alpha = alphabets [ 'aa' ] if params . aa else alphabets [ 'nuc' ] fixed pi = [ ref . count ( base ) / len ( ref ) for base in alpha ] if fixed pi [ - 1 ] == 0 : fixed pi [ - 1 ] = 0.05 fixed pi = [ v - 0.01 for v in fixed pi ] return aln , ref , fixed pi", "predictions": ["reads a set of ."], "references": ["checks if input is vcf and reads in appropriately if it is"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 2743, "code": "def delta function ( cls , x pos , weight = 1. , min width = MIN INTEGRATION PEAK ) : distribution = cls ( x pos , 0. , is log = True , min width = min width ) distribution . weight = weight return distribution", "predictions": ["initializes the compute function ."], "references": ["create delta function distribution ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 2744, "code": "def multiply ( dists ) : if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : raise Not Implemented Error ( \"Can only multiply Distribution objects\" ) n delta = np . sum ( [ k . is delta for k in dists ] ) min width = np . max ( [ k . min width for k in dists ] ) if n delta > 1 : raise Arithmetic Error ( \"Cannot multiply more than one delta functions!\" ) elif n delta == 1 : delta dist ii = np . where ( [ k . is delta for k in dists ] ) [ 0 ] [ 0 ] delta dist = dists [ delta dist ii ] new xpos = delta dist . peak pos new weight = np . prod ( [ k . prob ( new xpos ) for k in dists if k != delta dist ii ] ) * delta dist . weight res = Distribution . delta function ( new xpos , weight = new weight , min width = min width ) else : new xmin = np . max ( [ k . xmin for k in dists ] ) new xmax = np . min ( [ k . xmax for k in dists ] ) x vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) x vals = x vals [ ( x vals > new xmin - TINY NUMBER ) & ( x vals < new xmax + TINY NUMBER ) ] y vals = np . sum ( [ k . call ( x vals ) for k in dists ] , axis = 0 ) peak = y vals . min ( ) ind = ( y vals - peak ) < BIG NUMBER / 1000 n points = ind . sum ( ) if n points == 0 : print ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) x vals = [ 0 , 1 ] y vals = [ BIG NUMBER , BIG NUMBER ] res = Distribution ( x vals , y vals , is log = True , min width = min width , kind = 'linear' ) elif n points == 1 : res = Distribution . delta function ( x vals [ 0 ] ) else : res = Distribution ( x vals [ ind ] , y vals [ ind ] , is log = True , min width = min width , kind = 'linear' , assume sorted = True ) return res", "predictions": ["compute the quotient of all non - conventional distributions ."], "references": ["multiplies a list of distribution objects"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2745, "code": "def timetree likelihood ( self ) : LH = 0 for node in self . tree . find clades ( order = 'preorder' ) : if node . up is None : continue LH -= node . branch length interpolator ( node . branch length ) if self . aln : LH += self . gtr . sequence log LH ( self . tree . root . cseq , pattern multiplicity = self . multiplicity ) return LH", "predictions": ["the original ( possibly , parser for the given list of all points parser ."], "references": ["return the likelihood of the data given the current branch length in the tree"], "bleu": 0.10343603005129705, "rouge_l": 0.2081911262798635}
{"id": 2746, "code": "def min interp ( interp object ) : try : return interp object . x [ interp object ( interp object . x ) . argmin ( ) ] except Exception as e : s = \"Cannot find minimum of the interpolation object\" + str ( interp object . x ) + \"Minimal x: \" + str ( interp object . x . min ( ) ) + \"Maximal x: \" + str ( interp object . x . max ( ) ) raise e", "predictions": ["replaces all the cases of this pointer by the given : < p > if this is a different : - > , - 1 , or the results of the array , or the results of the : - - > - > : - : - > ,"], "references": ["find the global minimum of a function represented as an interpolation object ."], "bleu": 0.028577262451992175, "rouge_l": 0.10651920838183936}
{"id": 2747, "code": "def median interp ( interp object ) : new grid = np . sort ( np . concatenate ( [ interp object . x [ : - 1 ] + 0.1 * ii * np . diff ( interp object . x ) for ii in range ( 10 ) ] ) . flatten ( ) ) tmp prop = np . exp ( - ( interp object ( new grid ) - interp object . y . min ( ) ) ) tmp cumsum = np . cumsum ( 0.5 * ( tmp prop [ 1 : ] + tmp prop [ : - 1 ] ) * np . diff ( new grid ) ) median index = min ( len ( tmp cumsum ) - 3 , max ( 2 , np . searchsorted ( tmp cumsum , tmp cumsum [ - 1 ] * 0.5 ) + 1 ) ) return new grid [ median index ]", "predictions": ["finds compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute the compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute compute the compute"], "references": ["find the median of the function represented as an interpolation object ."], "bleu": 0.026594139297659906, "rouge_l": 0.07253269916765755}
{"id": 2748, "code": "def close ( self ) : self . client . close ( ) self . client = None self . connected = False self . logger . debug ( 'Connection closed.' )", "predictions": ["what happens on the connection to the log file ."], "references": ["terminate connection with guacamole guacd server ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2749, "code": "def receive ( self ) : start = 0 while True : idx = self . buffer . find ( INST TERM . encode ( ) , start ) if idx != - 1 : line = self . buffer [ : idx + 1 ] . decode ( ) self . buffer = self . buffer [ idx + 1 : ] self . logger . debug ( 'Received instruction: %s' % line ) return line else : start = len ( self . buffer ) buf = self . client . recv ( BUF LEN ) if not buf : self . close ( ) self . logger . debug ( 'Failed to receive instruction. Closing.' ) return None self . buffer . extend ( buf )", "predictions": ["read and sign a file for the specific = 1 . the file must be empty . the method must be called on the specific call ."], "references": ["receive instructions from guacamole guacd server ."], "bleu": 0.044915755686574035, "rouge_l": 0.06580366774541531}
{"id": 2750, "code": "def send ( self , data ) : self . logger . debug ( 'Sending data: %s' % data ) self . client . sendall ( data . encode ( ) )", "predictions": ["gaussian gaussian event loop to the remote endpoint ."], "references": ["send encoded instructions to guacamole guacd server ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2751, "code": "def send instruction ( self , instruction ) : self . logger . debug ( 'Sending instruction: %s' % str ( instruction ) ) return self . send ( instruction . encode ( ) )", "predictions": ["send the specified ( = 0 , 1 , 2 , 4 = 0 , 1 , 2 , 0 , 100 , 12 , 12 , , 0 , 0 , 100 , 12 , 12 = 0 , 0 , 12 , 12 = ( , , 12"], "references": ["send instruction after encoding ."], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 2752, "code": "def handshake ( self , protocol = 'vnc' , width = 1024 , height = 768 , dpi = 96 , audio = None , video = None , image = None , * * kwargs ) : if protocol not in PROTOCOLS : self . logger . debug ( 'Invalid protocol: %s' % protocol ) raise Guacamole Error ( 'Cannot start Handshake. Missing protocol.' ) if audio is None : audio = list ( ) if video is None : video = list ( ) if image is None : image = list ( ) self . logger . debug ( 'Send `select` instruction.' ) self . send instruction ( Instruction ( 'select' , protocol ) ) instruction = self . read instruction ( ) self . logger . debug ( 'Expecting `args` instruction, received: %s' % str ( instruction ) ) if not instruction : self . close ( ) raise Guacamole Error ( 'Cannot establish Handshake. Connection Lost!' ) if instruction . opcode != 'args' : self . close ( ) raise Guacamole Error ( 'Cannot establish Handshake. Expected opcode `args`, ' 'received `%s` instead.' % instruction . opcode ) self . logger . debug ( 'Send `size` instruction (%s, %s, %s)' % ( width , height , dpi ) ) self . send instruction ( Instruction ( 'size' , width , height , dpi ) ) self . logger . debug ( 'Send `audio` instruction (%s)' % audio ) self . send instruction ( Instruction ( 'audio' , * audio ) ) self . logger . debug ( 'Send `video` instruction (%s)' % video ) self . send instruction ( Instruction ( 'video' , * video ) ) self . logger . debug ( 'Send `image` instruction (%s)' % image ) self . send instruction ( Instruction ( 'image' , * image ) ) connection args = [ kwargs . get ( arg . replace ( '-' , ' ' ) , '' ) for arg in instruction . args ] self . logger . debug ( 'Send `connect` instruction (%s)' % connection args ) self . send instruction ( Instruction ( 'connect' , * connection args ) ) instruction = self . read instruction ( ) self . logger . debug ( 'Expecting `ready` instruction, received: %s' % str ( instruction ) ) if instruction . opcode != 'ready' : self . logger . warning ( 'Expected `ready` instruction, received: %s instead' ) if instruction . args : self . id = instruction . args [ 0 ] self . logger . debug ( 'Established connection with client id: %s' % self . id ) self . logger . debug ( 'Handshake completed.' ) self . connected = True", "predictions": ["generate a handshake to the handshake ."], "references": ["establish connection with guacamole guacd server via handshake ."], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 2753, "code": "def class url ( cls ) : base = 'v{0}' . format ( getattr ( cls , 'RESOURCE VERSION' , '1' ) ) return \"/{0}/{1}\" . format ( base , class to api name ( cls . class name ( ) ) )", "predictions": ["creates an instance of the class ."], "references": ["returns a versioned uri string for this class"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2754, "code": "def instance url ( self ) : id = self . get ( self . ID ATTR ) base = self . class url ( ) if id : return '/' . join ( [ base , six . text type ( id ) ] ) else : raise Exception ( 'Could not determine which URL to request: %s instance ' 'has invalid ID: %r' % ( type ( self ) . name , id ) , self . ID ATTR )", "predictions": ["get the url for an instance of this class ."], "references": ["get instance url by id"], "bleu": 0.14991106946711685, "rouge_l": 0.2837209302325582}
{"id": 2755, "code": "def parent object ( self ) : from . import types parent klass = types . get ( self . parent job model . split ( '.' ) [ 1 ] ) return parent klass . retrieve ( self . parent job id , client = self . client )", "predictions": ["create a . to try to get the parent of this job"], "references": ["get the commit objects parent import or migration"], "bleu": 0.14694106251955755, "rouge_l": 0.3112244897959184}
{"id": 2756, "code": "def ask for credentials ( ) : print msg ( 'Please enter your Solve Bio credentials' ) domain = raw input ( 'Domain (e.g. <domain>.solvebio.com): ' ) try : account = client . request ( 'get' , '/p/accounts/{}' . format ( domain ) ) auth = account [ 'authentication' ] except : raise Solve Error ( 'Invalid domain: {}' . format ( domain ) ) if auth . get ( 'login' ) or auth . get ( 'SAML' , { } ) . get ( 'simple login' ) : email = raw input ( 'Email: ' ) password = getpass . getpass ( 'Password (typing will be hidden): ' ) return ( domain , email , password ) else : print msg ( 'Your domain uses Single Sign-On (SSO). ' 'Please visit https://{}.solvebio.com/settings/security ' 'for instructions on how to log in.' . format ( domain ) ) sys . exit ( 1 )", "predictions": ["presents the credentials back to the user ."], "references": ["asks the user for their email and password ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 2757, "code": "def print user ( user ) : email = user [ 'email' ] domain = user [ 'account' ] [ 'domain' ] role = user [ 'role' ] print ( 'You are logged-in to the \"{0}\" domain ' 'as {1} with role {2}.' . format ( domain , email , role ) )", "predictions": ["prints out a user ."], "references": ["prints information about the current user ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 2758, "code": "def range ( self , chromosome , start , stop , exact = False ) : return self . clone ( filters = [ Genomic Filter ( chromosome , start , stop , exact ) ] )", "predictions": ["returns a new filter with the specified values ."], "references": ["shortcut to do range filters on genomic datasets ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2759, "code": "def position ( self , chromosome , position , exact = False ) : return self . clone ( filters = [ Genomic Filter ( chromosome , position , exact = exact ) ] )", "predictions": ["copies this grammar at the specified index ."], "references": ["shortcut to do a single position filter on genomic datasets ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 2760, "code": "def main ( argv = sys . argv [ 1 : ] ) : parser = Solve Argument Parser ( ) args = parser . parse solvebio args ( argv ) if args . api host : solvebio . api host = args . api host if args . api key : solvebio . api key = args . api key if not solvebio . api key : try : from . credentials import get credentials solvebio . api key = get credentials ( ) except : pass client . set host ( ) client . set token ( ) return args . func ( args )", "predictions": ["main entry point for the client ."], "references": ["main entry point for solvebio cli"], "bleu": 0.5169731539571706, "rouge_l": 0.6240409207161125}
{"id": 2761, "code": "def construct from ( cls , values , * * kwargs ) : instance = cls ( values . get ( cls . ID ATTR ) , * * kwargs ) instance . refresh from ( values ) return instance", "predictions": ["constructs a new instance of this class ."], "references": ["used to create a new object from an http response"], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 2762, "code": "def logout ( self ) : if self . oauth client secret : try : oauth token = flask . request . cookies [ self . TOKEN COOKIE NAME ] requests . post ( urljoin ( self . api host , self . OAUTH2 REVOKE TOKEN PATH ) , data = { 'client id' : self . oauth client id , 'client secret' : self . oauth client secret , 'token' : oauth token } ) except : pass response = flask . redirect ( '/' ) self . clear cookies ( response ) return response", "predictions": ["when a user logs out to [ 0 , the multi - 1 ] ."], "references": ["revoke the token and remove the cookie ."], "bleu": 0.09103526405546068, "rouge_l": 0.18401206636500753}
{"id": 2763, "code": "def child object ( self ) : from . import types child klass = types . get ( self . task type . split ( '.' ) [ 1 ] ) return child klass . retrieve ( self . task id , client = self . client )", "predictions": ["get a child of the given object ."], "references": ["get task child object class"], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 2764, "code": "def row to dict ( self , row , allele , alternate alleles ) : def variant sbid ( * * kwargs ) : \"\"\"Generates a Solve Bio variant ID (SBID).\"\"\" return '{build}-{chromosome}-{start}-{stop}-{allele}' . format ( * * kwargs ) . upper ( ) if allele == '.' : allele = row . REF or allele genomic coordinates = { 'build' : self . genome build , 'chromosome' : row . CHROM , 'start' : row . POS , 'stop' : row . POS + len ( row . REF ) - 1 } variant sbid = variant sbid ( allele = allele , * * genomic coordinates ) return { 'genomic coordinates' : genomic coordinates , 'variant' : variant sbid , 'allele' : allele , 'row id' : row . ID , 'reference allele' : row . REF , 'alternate alleles' : alternate alleles , 'info' : self . parse info ( row . INFO ) , 'qual' : row . QUAL , 'filter' : row . FILTER }", "predictions": ["convert row to list ."], "references": ["return a parsed dictionary for json ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2765, "code": "def save ( self , path ) : rep = \"\" for host in self . hosts . keys ( ) : attrs = self . hosts [ host ] rep = rep + \"machine \" + host + \"\\n\\tlogin \" + six . text type ( attrs [ 0 ] ) + \"\\n\" if attrs [ 1 ] : rep = rep + \"account \" + six . text type ( attrs [ 1 ] ) rep = rep + \"\\tpassword \" + six . text type ( attrs [ 2 ] ) + \"\\n\" for macro in self . macros . keys ( ) : rep = rep + \"macdef \" + macro + \"\\n\" for line in self . macros [ macro ] : rep = rep + line rep = rep + \"\\n\" f = open ( path , 'w' ) f . write ( rep ) f . close ( )", "predictions": ["save all macros on the command ."], "references": ["dump the class data in the format of a . netrc file ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 2766, "code": "def build row ( cells , padding , begin , sep , end ) : pad = \" \" * padding padded cells = [ pad + cell + pad for cell in cells ] rendered cells = ( begin + sep . join ( padded cells ) + end ) . rstrip ( ) if len ( rendered cells ) > TTY COLS : if not cells [ - 1 ] . endswith ( \" \" ) and not cells [ - 1 ] . endswith ( \"-\" ) : terminating str = \" ... \" else : terminating str = \"\" rendered cells = \"{0}{1}{2}\" . format ( rendered cells [ : TTY COLS - len ( terminating str ) - 1 ] , terminating str , end ) return rendered cells", "predictions": ["builds a row of all rows that have the required rendered cells ."], "references": ["return a string which represents a row of data cells ."], "bleu": 0.21142141714303078, "rouge_l": 0.42302357836338417}
{"id": 2767, "code": "def build line ( colwidths , padding , begin , fill , sep , end ) : cells = [ fill * ( w + 2 * padding ) for w in colwidths ] return build row ( cells , 0 , begin , sep , end )", "predictions": ["builds a line line of the rhs ."], "references": ["return a string which represents a horizontal line ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 2768, "code": "def mediawiki cell attrs ( row , colaligns ) : alignment = { \"left\" : '' , \"right\" : 'align=\"right\"| ' , \"center\" : 'align=\"center\"| ' , \"decimal\" : 'align=\"right\"| ' } row2 = [ alignment [ a ] + c for c , a in zip ( row , colaligns ) ] return row2", "predictions": ["add a ( to the mediawiki cell . this is useful for finding a good cell of the ( ."], "references": ["prefix every cell in a row with an html alignment attribute ."], "bleu": 0.07264339766175722, "rouge_l": 0.1963519313304721}
{"id": 2769, "code": "def format table ( fmt , headers , rows , colwidths , colaligns ) : lines = [ ] hidden = fmt . with header hide if headers else fmt . without header hide pad = fmt . padding headerrow = fmt . headerrow if fmt . headerrow else fmt . datarow if fmt . lineabove and \"lineabove\" not in hidden : lines . append ( build line ( colwidths , pad , * fmt . lineabove ) ) if headers : lines . append ( build row ( headers , pad , * headerrow ) ) if fmt . linebelowheader and \"linebelowheader\" not in hidden : begin , fill , sep , end = fmt . linebelowheader if fmt . usecolons : segs = [ line segment with colons ( fmt . linebelowheader , a , w + 2 * pad ) for w , a in zip ( colwidths , colaligns ) ] lines . append ( build row ( segs , 0 , begin , sep , end ) ) else : lines . append ( build line ( colwidths , pad , * fmt . linebelowheader ) ) if rows and fmt . linebetweenrows and \"linebetweenrows\" not in hidden : for row in rows [ : - 1 ] : lines . append ( build row ( row , pad , * fmt . datarow ) ) lines . append ( build line ( colwidths , pad , * fmt . linebetweenrows ) ) lines . append ( build row ( rows [ - 1 ] , pad , * fmt . datarow ) ) else : for row in rows : lines . append ( build row ( row , pad , * fmt . datarow ) ) if fmt . linebelow and \"linebelow\" not in hidden : lines . append ( build line ( colwidths , pad , * fmt . linebelow ) ) return \"\\n\" . join ( lines )", "predictions": ["return a list of columns ."], "references": ["produce a plain - text representation of the table ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 2770, "code": "def evaluate ( self , data = None , data type = 'string' , is list = False ) : payload = { 'data' : data , 'expression' : self . expr , 'data type' : data type , 'is list' : is list } res = self . client . post ( '/v1/evaluate' , payload ) return res [ 'result' ]", "predictions": ["this method needs to be called to pass this operation on the given data ."], "references": ["evaluates the expression with the provided context and format ."], "bleu": 0.09103526405546068, "rouge_l": 0.1659863945578231}
{"id": 2771, "code": "def get column types ( self , data ) : columns = list ( zip longest ( * data ) ) return [ self . get column type ( column ) for column in columns ]", "predictions": ["fetches the list of columns from the current list of columns ."], "references": ["get a list of the data types for each column in * data * ."], "bleu": 0.12100325656200607, "rouge_l": 0.29047619047619044}
{"id": 2772, "code": "def get column type ( self , column ) : type values = [ TYPES [ self . get type ( v ) ] for v in column ] inverse types = { v : k for k , v in TYPES . items ( ) } return inverse types [ max ( type values ) ]", "predictions": ["get the column represented by column ."], "references": ["get the most generic data type for iterable * column * ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 2773, "code": "def get type ( self , value ) : if value is None : return type ( None ) elif type ( value ) in int types : return int elif type ( value ) in float types : return float elif isinstance ( value , binary type ) : return binary type else : return text type", "predictions": ["get the type of the wrapped wrapped operation ."], "references": ["get the data type for * value * ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 2774, "code": "def adapter ( data , headers , table format = None , preserve whitespace = False , * * kwargs ) : keys = ( 'floatfmt' , 'numalign' , 'stralign' , 'showindex' , 'disable numparse' ) tkwargs = { 'tablefmt' : table format } tkwargs . update ( filter dict by key ( kwargs , keys ) ) if table format in supported markup formats : tkwargs . update ( numalign = None , stralign = None ) tabulate . PRESERVE WHITESPACE = preserve whitespace return iter ( tabulate . tabulate ( data , headers , * * tkwargs ) . split ( '\\n' ) )", "predictions": ["update the adapter with the specified headers ."], "references": ["wrap tabulate inside a function for tabularoutputformatter ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 2775, "code": "def user config file ( self ) : return os . path . join ( get user config dir ( self . app name , self . app author ) , self . filename )", "predictions": ["sets the config file for the user ."], "references": ["get the absolute path to the user config file ."], "bleu": 0.22482042087568346, "rouge_l": 0.43571428571428567}
{"id": 2776, "code": "def system config files ( self ) : return [ os . path . join ( f , self . filename ) for f in get system config dirs ( self . app name , self . app author ) ]", "predictions": ["override this to get the system config for the system ."], "references": ["get a list of absolute paths to the system config files ."], "bleu": 0.22265215139875258, "rouge_l": 0.4314002828854314}
{"id": 2777, "code": "def additional files ( self ) : return [ os . path . join ( f , self . filename ) for f in self . additional dirs ]", "predictions": ["override for retrieveproperty ( ) ."], "references": ["get a list of absolute paths to the additional config files ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 2778, "code": "def truncate string ( value , max width = None ) : if isinstance ( value , text type ) and max width is not None and len ( value ) > max width : return value [ : max width ] return value", "predictions": ["truncate a string to a maximum length ."], "references": ["truncate string values ."], "bleu": 0.19070828081828378, "rouge_l": 0.5319767441860466}
{"id": 2779, "code": "def filter dict by key ( d , keys ) : return { k : v for k , v in d . items ( ) if k in keys }", "predictions": ["filter all items with the specified key ."], "references": ["filter the dict * d * to remove keys not in * keys * ."], "bleu": 0.07949903911132591, "rouge_l": 0.24729729729729732}
{"id": 2780, "code": "def replace ( s , replace ) : for r in replace : s = s . replace ( * r ) return s", "predictions": ["replace variable in s with another string ."], "references": ["replace multiple values in a string"], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 2781, "code": "def adapter ( data , headers , * * kwargs ) : for row in chain ( ( headers , ) , data ) : yield \"\\t\" . join ( ( replace ( r , ( ( '\\n' , r'\\n' ) , ( '\\t' , r'\\t' ) ) ) for r in row ) )", "predictions": ["generate an iterator of data from all rows ."], "references": ["wrap the formatting inside a function for tabularoutputformatter ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2782, "code": "def call and exit ( self , cmd , shell = True ) : sys . exit ( subprocess . call ( cmd , shell = shell ) )", "predictions": ["helper method for executing a command ."], "references": ["run the * cmd * and exit with the proper exit code ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 2783, "code": "def call in sequence ( self , cmds , shell = True ) : for cmd in cmds : if subprocess . call ( cmd , shell = shell ) == 1 : sys . exit ( 1 )", "predictions": ["calls the specified shell ."], "references": ["run multiple commmands in a row exiting if one fails ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 2784, "code": "def apply options ( self , cmd , options = ( ) ) : for option in ( self . default cmd options + options ) : cmd = self . apply option ( cmd , option , active = getattr ( self , option , False ) ) return cmd", "predictions": ["handshake the commands . this will be called before any command is executed ."], "references": ["apply command - line options ."], "bleu": 0.09782375748961449, "rouge_l": 0.21554770318021202}
{"id": 2785, "code": "def apply option ( self , cmd , option , active = True ) : return re . sub ( r'{{{}\\:(?P<option>[^}}]*)}}' . format ( option ) , '\\g<option>' if active else '' , cmd )", "predictions": ["add a new url to the base url ."], "references": ["apply a command - line option ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 2786, "code": "def initialize options ( self ) : self . branch = 'master' self . fix = False super ( lint , self ) . initialize options ( )", "predictions": ["for the object and specific url lookup ."], "references": ["set the default options ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 2787, "code": "def run ( self ) : cmd = 'pep8radius {branch} {{fix: --in-place}}{{verbose: -vv}}' cmd = cmd . format ( branch = self . branch ) self . call and exit ( self . apply options ( cmd , ( 'fix' , ) ) )", "predictions": ["this method runs the command ."], "references": ["run the linter ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 2788, "code": "def run ( self ) : cmds = ( self . clean docs cmd , self . html docs cmd , self . view docs cmd ) self . call in sequence ( cmds )", "predictions": ["run the given method ."], "references": ["generate and view the documentation ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2789, "code": "def get separator ( num , sep title , sep character , sep length ) : left divider length = right divider length = sep length if isinstance ( sep length , tuple ) : left divider length , right divider length = sep length left divider = sep character * left divider length right divider = sep character * right divider length title = sep title . format ( n = num + 1 ) return \"{left divider}[ {title} ]{right divider}\\n\" . format ( left divider = left divider , right divider = right divider , title = title )", "predictions": ["gets the user user user user user ."], "references": ["get a row separator for row * num * ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 2790, "code": "def format row ( headers , row ) : formatted row = [ ' | ' . join ( field ) for field in zip ( headers , row ) ] return '\\n' . join ( formatted row )", "predictions": ["formats the ( matrix start with included start and incorrectly start start start start start start with the , isolated start tags start at http start start start . start with the inline level start and incorrectly start start start date start start start start start ."], "references": ["format a row ."], "bleu": 0.02558174341959753, "rouge_l": 0.04624715693707354}
{"id": 2791, "code": "def adapter ( data , headers , * * kwargs ) : keys = ( 'sep title' , 'sep character' , 'sep length' ) return vertical table ( data , headers , * * filter dict by key ( kwargs , keys ) )", "predictions": ["get a vertical with the same values as the given chromosome ."], "references": ["wrap vertical table in a function for tabularoutputformatter ."], "bleu": 0.1235622127262679, "rouge_l": 0.19551282051282048}
{"id": 2792, "code": "def adapter ( data , headers , table format = 'csv' , * * kwargs ) : keys = ( 'dialect' , 'delimiter' , 'doublequote' , 'escapechar' , 'quotechar' , 'quoting' , 'skipinitialspace' , 'strict' ) if table format == 'csv' : delimiter = ',' elif table format == 'csv-tab' : delimiter = '\\t' else : raise Value Error ( 'Invalid table format specified.' ) ckwargs = { 'delimiter' : delimiter , 'lineterminator' : '' } ckwargs . update ( filter dict by key ( kwargs , keys ) ) l = linewriter ( ) writer = csv . writer ( l , * * ckwargs ) writer . writerow ( headers ) yield l . line for row in data : l . reset ( ) writer . writerow ( row ) yield l . line", "predictions": ["yield ( ( ) yield ( argv . ( ( parser . . parser . ( parser ."], "references": ["wrap the formatting inside a function for tabularoutputformatter ."], "bleu": 0.06809398432036522, "rouge_l": 0.07881136950904392}
{"id": 2793, "code": "def adapter ( data , headers , table format = None , * * kwargs ) : keys = ( 'title' , ) table = table format handler [ table format ] t = table ( [ headers ] + list ( data ) , * * filter dict by key ( kwargs , keys ) ) dimensions = terminaltables . width and alignment . max dimensions ( t . table data , t . padding left , t . padding right ) [ : 3 ] for r in t . gen table ( * dimensions ) : yield u'' . join ( r )", "predictions": ["generate the construct for all ( , , , instance = , values = , instance = , instance = , instance = dimensions = instance = instance : instance = values : instance = values : instance . instance . instance from values are ignored ."], "references": ["wrap terminaltables inside a function for tabularoutputformatter ."], "bleu": 0.028310852916881273, "rouge_l": 0.0833902939166097}
{"id": 2794, "code": "def to dict ( self ) : all attributes = Py KCS11 . CKA . keys ( ) all attributes = [ attr for attr in all attributes if isinstance ( attr , int ) ] attributes = self . session . get Attribute Value ( self , all attributes ) dico = dict ( ) for key , attr in zip ( all attributes , attributes ) : if attr is None : continue if key == CKA CLASS : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKO [ attr ] elif key == CKA CERTIFICATE TYPE : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKC [ attr ] elif key == CKA KEY TYPE : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKK [ attr ] else : dico [ Py KCS11 . CKA [ key ] ] = attr return dico", "predictions": ["transforms the attribute represented by this attribute into a map ."], "references": ["convert the fields of the object into a dictionnary"], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 2795, "code": "def to dict ( self ) : dico = dict ( ) for field in self . fields . keys ( ) : if field == \"flags\" : dico [ field ] = self . flags2text ( ) elif field == \"state\" : dico [ field ] = self . state2text ( ) else : dico [ field ] = eval ( \"self.\" + field ) return dico", "predictions": ["convert this map to a model without performing it ."], "references": ["convert the fields of the object into a dictionnary"], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 2796, "code": "def insert img ( qr img , icon img = None , factor = 4 , icon box = None , static dir = None ) : img w , img h = qr img . size size w = int ( img w ) / int ( factor ) size h = int ( img h ) / int ( factor ) try : icon fp = os . path . join ( icon img ) if static dir : icon fp = os . path . join ( static dir , icon img ) if icon img . split ( \"://\" ) [ 0 ] in [ \"http\" , \"https\" , \"ftp\" ] : icon fp = Bytes IO ( urlopen ( icon img ) . read ( ) ) icon = Image . open ( icon fp ) except : return qr img icon w , icon h = icon . size icon w = size w if icon w > size w else icon w icon h = size h if icon h > size h else icon h icon = icon . resize ( ( int ( icon w ) , int ( icon h ) ) , Image . ANTIALIAS ) icon = icon . convert ( \"RGBA\" ) left = int ( ( img w - icon w ) / 2 ) top = int ( ( img h - icon h ) / 2 ) icon box = ( int ( icon box [ 0 ] ) , int ( icon box [ 1 ] ) ) if icon box else ( left , top ) qr img . paste ( im = icon , box = icon box , mask = icon ) return qr img", "predictions": ["assumes a ( possibly in the cache of the specified parameters ."], "references": ["inserts a small icon to qr code image"], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 2797, "code": "def biweekly helper ( self ) : self . num = 14 mycount = self . repeat biweekly ( ) if mycount : if self . event . is chunk ( ) and min ( mycount ) not in xrange ( 1 , 8 ) : mycount = chunk fill out first week ( self . year , self . month , mycount , self . event , diff = self . event . start end diff , ) for k , v in mycount . items ( ) : for item in v : self . count [ k ] . append ( item )", "predictions": ["we have to supply this method as one of the original line . each week is set to the original : . , ( , ( , ( , and ( ."], "references": ["created to take some of the load off of _handle_weekly_repeat_out"], "bleu": 0.05342392352880332, "rouge_l": 0.15775862068965518}
{"id": 2798, "code": "def user ( context , user id , update role , add institute , remove admin , remove institute ) : adapter = context . obj [ 'adapter' ] user obj = adapter . user ( user id ) if not user obj : LOG . warning ( \"User %s could not be found\" , user id ) context . abort ( ) existing roles = set ( user obj . get ( 'roles' , [ ] ) ) if update role : if not update role in user obj [ 'roles' ] : existing roles = set ( user obj [ 'roles' ] ) existing roles . add ( update role ) LOG . info ( \"Adding role %s to user\" , update role ) else : LOG . warning ( \"User already have role %s\" , update role ) if remove admin : try : existing roles . remove ( 'admin' ) LOG . info ( \"Removing admin rights from user %s\" , user id ) except Key Error as err : LOG . info ( \"User %s does not have admin rights\" , user id ) user obj [ 'roles' ] = list ( existing roles ) existing institutes = set ( user obj . get ( 'institutes' , [ ] ) ) for institute id in add institute : institute obj = adapter . institute ( institute id ) if not institute obj : LOG . warning ( \"Institute %s could not be found\" , institute id ) else : existing institutes . add ( institute id ) LOG . info ( \"Adding institute %s to user\" , institute id ) for institute id in remove institute : try : existing institutes . remove ( institute id ) LOG . info ( \"Removing institute %s from user\" , institute id ) except Key Error as err : LOG . info ( \"User does not have access to institute %s\" , institute id ) user obj [ 'institutes' ] = list ( existing institutes ) updated user = adapter . update user ( user obj )", "predictions": ["this is called by adding an existing build to the build ."], "references": ["update a user in the database"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 2799, "code": "def variant ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) log . debug ( \"Variants view requesting data for variant {}\" . format ( variant id ) ) data = controllers . variant ( store , institute obj , case obj , variant id = variant id ) if data is None : log . warning ( \"An error occurred: variants view requesting data for variant {}\" . format ( variant id ) ) flash ( 'An error occurred while retrieving variant object' , 'danger' ) return redirect ( request . referrer ) if current app . config . get ( 'LOQUSDB SETTINGS' ) : data [ 'observations' ] = controllers . observations ( store , loqusdb , case obj , data [ 'variant' ] ) data [ 'cancer' ] = request . args . get ( 'cancer' ) == 'yes' return dict ( institute = institute obj , case = case obj , * * data )", "predictions": ["generate a new build entry in the previous build . this is used to ensure that the for the build method of the build method is called ."], "references": ["display a specific snv variant ."], "bleu": 0.0478968583748614, "rouge_l": 0.1331877729257642}
{"id": 2800, "code": "def str variants ( institute id , case name ) : page = int ( request . args . get ( 'page' , 1 ) ) variant type = request . args . get ( 'variant type' , 'clinical' ) form = Str Filters Form ( request . args ) institute obj , case obj = institute and case ( store , institute id , case name ) query = form . data query [ 'variant type' ] = variant type variants query = store . variants ( case obj [ ' id' ] , category = 'str' , query = query ) data = controllers . str variants ( store , institute obj , case obj , variants query , page ) return dict ( institute = institute obj , case = case obj , variant type = variant type , form = form , page = page , * * data )", "predictions": ["this is the entry point for the ( : 1 : ( ( ( ( ( : . : : : . : . : . : . : . : . : . : . : . : . : . : . : . : . : ."], "references": ["display a list of str variants ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 2801, "code": "def sv variant ( institute id , case name , variant id ) : data = controllers . sv variant ( store , institute id , case name , variant id ) return data", "predictions": ["helper method to add a format / : : 1 / . / . / ( / ( / ( / ( / ( / ."], "references": ["display a specific structural variant ."], "bleu": 0.051660454541342535, "rouge_l": 0.14087759815242493}
{"id": 2802, "code": "def str variant ( institute id , case name , variant id ) : data = controllers . str variant ( store , institute id , case name , variant id ) return data", "predictions": ["helper method to { , list < br > , list < br > , list < br > , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . ,"], "references": ["display a specific str variant ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 2803, "code": "def verify ( institute id , case name , variant id , variant category , order ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) comment = request . form . get ( 'verification comment' ) try : controllers . variant verification ( store = store , mail = mail , institute obj = institute obj , case obj = case obj , user obj = user obj , comment = comment , variant obj = variant obj , sender = current app . config [ 'MAIL USERNAME' ] , variant url = request . referrer , order = order , url builder = url for ) except controllers . Missing Verification Recipient Error : flash ( 'No verification recipients added to institute.' , 'danger' ) return redirect ( request . referrer )", "predictions": ["verifies the columns passed to the object ."], "references": ["start procedure to validate variant using other techniques ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 2804, "code": "def clinvar ( institute id , case name , variant id ) : data = controllers . clinvar export ( store , institute id , case name , variant id ) if request . method == 'GET' : return data else : #POST form dict = request . form . to dict ( ) submission objects = set submission objects ( form dict ) open submission = store . get open clinvar submission ( current user . email , institute id ) updated submission = store . add to submission ( open submission [ ' id' ] , submission objects ) return redirect ( url for ( 'cases.clinvar submissions' , institute id = institute id ) )", "predictions": ["this is the part of the get method for the current . this makes a ( call to generate a get ."], "references": ["build a clinvar submission form for a variant ."], "bleu": 0.06964541799727335, "rouge_l": 0.20938215102974828}
{"id": 2805, "code": "def cancer variants ( institute id , case name ) : data = controllers . cancer variants ( store , request . args , institute id , case name ) return data", "predictions": ["get the get request of the given parameters ."], "references": ["show cancer variants overview ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2806, "code": "def variant acmg ( institute id , case name , variant id ) : if request . method == 'GET' : data = controllers . variant acmg ( store , institute id , case name , variant id ) return data else : criteria = [ ] criteria terms = request . form . getlist ( 'criteria' ) for term in criteria terms : criteria . append ( dict ( term = term , comment = request . form . get ( \"comment-{}\" . format ( term ) ) , links = [ request . form . get ( \"link-{}\" . format ( term ) ) ] , ) ) acmg = controllers . variant acmg post ( store , institute id , case name , variant id , current user . email , criteria ) flash ( \"classified as: {}\" . format ( acmg ) , 'info' ) return redirect ( url for ( '.variant' , institute id = institute id , case name = case name , variant id = variant id ) )", "predictions": ["generate a list of all the adapter for the given adapter ."], "references": ["acmg classification form ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 2807, "code": "def evaluation ( evaluation id ) : evaluation obj = store . get evaluation ( evaluation id ) controllers . evaluation ( store , evaluation obj ) if request . method == 'POST' : link = url for ( '.variant' , institute id = evaluation obj [ 'institute' ] [ ' id' ] , case name = evaluation obj [ 'case' ] [ 'display name' ] , variant id = evaluation obj [ 'variant specific' ] ) store . delete evaluation ( evaluation obj ) return redirect ( link ) return dict ( evaluation = evaluation obj , institute = evaluation obj [ 'institute' ] , case = evaluation obj [ 'case' ] , variant = evaluation obj [ 'variant' ] , CRITERIA = ACMG CRITERIA )", "predictions": ["serializes the file identified by user and author ."], "references": ["show or delete an acmg evaluation ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2808, "code": "def acmg ( ) : criteria = request . args . getlist ( 'criterion' ) classification = get acmg ( criteria ) return jsonify ( dict ( classification = classification ) )", "predictions": ["constructs a system system ."], "references": ["calculate an acmg classification from submitted criteria ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2809, "code": "def upload panel ( institute id , case name ) : file = form . symbol file . data if file . filename == '' : flash ( 'No selected file' , 'warning' ) return redirect ( request . referrer ) try : stream = io . String IO ( file . stream . read ( ) . decode ( 'utf-8' ) , newline = None ) except Unicode Decode Error as error : flash ( \"Only text files are supported!\" , 'warning' ) return redirect ( request . referrer ) category = request . args . get ( 'category' ) if ( category == 'sv' ) : form = Sv Filters Form ( request . args ) else : form = Filters Form ( request . args ) hgnc symbols = set ( form . hgnc symbols . data ) new hgnc symbols = controllers . upload panel ( store , institute id , case name , stream ) hgnc symbols . update ( new hgnc symbols ) form . hgnc symbols . data = ',' . join ( hgnc symbols ) form . gene panels . data = '' if ( category == 'sv' ) : return redirect ( url for ( '.sv variants' , institute id = institute id , case name = case name , * * form . data ) , code = 307 ) else : return redirect ( url for ( '.variants' , institute id = institute id , case name = case name , * * form . data ) , code = 307 )", "predictions": ["additional data for the ( ."], "references": ["parse gene panel file and fill in hgnc symbols for filter ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 2810, "code": "def download verified ( ) : user obj = store . user ( current user . email ) user institutes = user obj . get ( 'institutes' ) temp excel dir = os . path . join ( variants bp . static folder , 'verified folder' ) os . makedirs ( temp excel dir , exist ok = True ) written files = controllers . verified excel file ( store , user institutes , temp excel dir ) if written files : today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) data = io . Bytes IO ( ) with zipfile . Zip File ( data , mode = 'w' ) as z : for f name in pathlib . Path ( temp excel dir ) . iterdir ( ) : zipfile . Zip File z . write ( f name , os . path . basename ( f name ) ) data . seek ( 0 ) shutil . rmtree ( temp excel dir ) return send file ( data , mimetype = 'application/zip' , as attachment = True , attachment filename = ' ' . join ( [ 'scout' , 'verified variants' , today ] ) + '.zip' ) else : flash ( \"No verified variants could be exported for user's institutes\" , 'warning' ) return redirect ( request . referrer )", "predictions": ["checking the string to this class ."], "references": ["download all verified variants for user s cases"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 2811, "code": "def add incomplete penetrance ( genes , alias genes , hpo lines ) : LOG . info ( \"Add incomplete penetrance info\" ) for hgnc symbol in get incomplete penetrance genes ( hpo lines ) : for hgnc id in get correct ids ( hgnc symbol , alias genes ) : genes [ hgnc id ] [ 'incomplete penetrance' ] = True", "predictions": ["filter a list of dict from the set of fillet ids ."], "references": ["add information of incomplete penetrance"], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 2812, "code": "def panels ( ) : if request . method == 'POST' : csv file = request . files [ 'csv file' ] content = csv file . stream . read ( ) lines = None try : if b'\\n' in content : lines = content . decode ( 'utf-8' , 'ignore' ) . split ( '\\n' ) else : lines = content . decode ( 'windows-1252' ) . split ( '\\r' ) except Exception as err : flash ( 'Something went wrong while parsing the panel CSV file! ({})' . format ( err ) , 'danger' ) return redirect ( request . referrer ) new panel name = request . form . get ( 'new panel name' ) if new panel name : #create a new panel new panel id = controllers . new panel ( store = store , institute id = request . form [ 'institute' ] , panel name = new panel name , display name = request . form [ 'display name' ] , csv lines = lines , ) if new panel id is None : flash ( 'Something went wrong and the panel list was not updated!' , 'warning' ) return redirect ( request . referrer ) else : flash ( \"new gene panel added, {}!\" . format ( new panel name ) , 'success' ) return redirect ( url for ( 'panels.panel' , panel id = new panel id ) ) else : update option = request . form [ 'modify option' ] panel obj = controllers . update panel ( store = store , panel name = request . form [ 'panel name' ] , csv lines = lines , option = update option ) if panel obj is None : return abort ( 404 , \"gene panel not found: {}\" . format ( request . form [ 'panel name' ] ) ) else : return redirect ( url for ( 'panels.panel' , panel id = panel obj [ ' id' ] ) ) institutes = list ( user institutes ( store , current user ) ) panel names = [ name for institute in institutes for name in store . gene panels ( institute id = institute [ ' id' ] ) . distinct ( 'panel name' ) ] panel versions = { } for name in panel names : panel versions [ name ] = store . gene panels ( panel id = name ) panel groups = [ ] for institute obj in institutes : institute panels = store . latest panels ( institute obj [ ' id' ] ) panel groups . append ( ( institute obj , institute panels ) ) return dict ( panel groups = panel groups , panel names = panel names , panel versions = panel versions , institutes = institutes )", "predictions": ["replace all . replace the head with the ."], "references": ["show all panels for a case ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 2813, "code": "def panel update ( panel id ) : panel obj = store . panel ( panel id ) update version = request . form . get ( 'version' , None ) new panel id = store . apply pending ( panel obj , update version ) return redirect ( url for ( 'panels.panel' , panel id = new panel id ) )", "predictions": ["( or update an object headers headers headers headers headers headers headers headers headers headers headers headers headers headers headers headers headers headers headers to be sent to a grid ."], "references": ["update panel to a new version ."], "bleu": 0.05834347180338517, "rouge_l": 0.23758519961051605}
{"id": 2814, "code": "def panel export ( panel id ) : panel obj = store . panel ( panel id ) data = controllers . panel export ( store , panel obj ) data [ 'report created at' ] = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) html report = render template ( 'panels/panel pdf simple.html' , * * data ) return render pdf ( HTML ( string = html report ) , download filename = data [ 'panel' ] [ 'panel name' ] + ' ' + str ( data [ 'panel' ] [ 'version' ] ) + ' ' + datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) + ' scout.pdf' )", "predictions": ["generates the call to send data to a call to a call ."], "references": ["export panel to pdf file"], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 2815, "code": "def gene edit ( panel id , hgnc id ) : panel obj = store . panel ( panel id ) hgnc gene = store . hgnc gene ( hgnc id ) panel gene = controllers . existing gene ( store , panel obj , hgnc id ) form = Panel Gene Form ( ) transcript choices = [ ] for transcript in hgnc gene [ 'transcripts' ] : if transcript . get ( 'refseq id' ) : refseq id = transcript . get ( 'refseq id' ) transcript choices . append ( ( refseq id , refseq id ) ) form . disease associated transcripts . choices = transcript choices if form . validate on submit ( ) : action = 'edit' if panel gene else 'add' info data = form . data . copy ( ) if 'csrf token' in info data : del info data [ 'csrf token' ] store . add pending ( panel obj , hgnc gene , action = action , info = info data ) return redirect ( url for ( '.panel' , panel id = panel id ) ) if panel gene : for field key in [ 'disease associated transcripts' , 'reduced penetrance' , 'mosaicism' , 'inheritance models' , 'database entry version' , 'comment' ] : form field = getattr ( form , field key ) if not form field . data : panel value = panel gene . get ( field key ) if panel value is not None : form field . process data ( panel value ) return dict ( panel = panel obj , form = form , gene = hgnc gene , panel gene = panel gene )", "predictions": ["submits an action object to a previously in one : 1 . in case 2 . in this method is called first , then the other in case it is not possible to be passed as a reference ."], "references": ["edit additional information about a panel gene ."], "bleu": 0.03419816740540655, "rouge_l": 0.09659540775930325}
{"id": 2816, "code": "def delivery report ( context , case id , report path , update ) : adapter = context . obj [ 'adapter' ] try : load delivery report ( adapter = adapter , case id = case id , report path = report path , update = update ) LOG . info ( \"saved report to case!\" ) except Exception as e : LOG . error ( e ) context . abort ( )", "predictions": ["reports an adapter for an object ."], "references": ["add delivery report to an existing case ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2817, "code": "def whitelist ( context ) : LOG . info ( \"Running scout view users\" ) adapter = context . obj [ 'adapter' ] for whitelist obj in adapter . whitelist collection . find ( ) : click . echo ( whitelist obj [ ' id' ] )", "predictions": ["find the whitelist object for a given object ."], "references": ["show all objects in the whitelist collection"], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 2818, "code": "def gene ( store , hgnc id ) : res = { 'builds' : { '37' : None , '38' : None } , 'symbol' : None , 'description' : None , 'ensembl id' : None , 'record' : None } for build in res [ 'builds' ] : record = store . hgnc gene ( hgnc id , build = build ) if record : record [ 'position' ] = \"{this[chromosome]}:{this[start]}-{this[end]}\" . format ( this = record ) res [ 'aliases' ] = record [ 'aliases' ] res [ 'hgnc id' ] = record [ 'hgnc id' ] res [ 'description' ] = record [ 'description' ] res [ 'builds' ] [ build ] = record res [ 'symbol' ] = record [ 'hgnc symbol' ] res [ 'description' ] = record [ 'description' ] res [ 'entrez id' ] = record . get ( 'entrez id' ) res [ 'pli score' ] = record . get ( 'pli score' ) add gene links ( record , int ( build ) ) res [ 'omim id' ] = record . get ( 'omim id' ) res [ 'incomplete penetrance' ] = record . get ( 'incomplete penetrance' , False ) res [ 'inheritance models' ] = record . get ( 'inheritance models' , [ ] ) for transcript in record [ 'transcripts' ] : transcript [ 'position' ] = ( \"{this[chrom]}:{this[start]}-{this[end]}\" . format ( this = transcript ) ) add tx links ( transcript , build ) for phenotype in record . get ( 'phenotypes' , [ ] ) : phenotype [ 'omim link' ] = omim ( phenotype . get ( 'mim number' ) ) if not res [ 'record' ] : res [ 'record' ] = record if not any ( res . values ( ) ) : raise Value Error return res", "predictions": ["gene - . ( store ) - > transcript ( 1 ) { transcript } : store transcript { 1 } 2 } 2 } 2 } : 1 } 2 } { 1 } 2 } : 1 } 2 . 0 . 0 . 0 . 0 }"], "references": ["parse information about a gene ."], "bleu": 0.026594139297659906, "rouge_l": 0.08321964529331514}
{"id": 2819, "code": "def genes to json ( store , query ) : gene query = store . hgnc genes ( query , search = True ) json terms = [ { 'name' : \"{} | {} ({})\" . format ( gene [ 'hgnc id' ] , gene [ 'hgnc symbol' ] , ', ' . join ( gene [ 'aliases' ] ) ) , 'id' : gene [ 'hgnc id' ] } for gene in gene query ] return json terms", "predictions": ["convert a query object into a json object ."], "references": ["fetch matching genes and convert to json ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 2820, "code": "def index ( ) : accessible institutes = current user . institutes if not 'admin' in current user . roles : accessible institutes = current user . institutes if not accessible institutes : flash ( 'Not allowed to see information - please visit the dashboard later!' ) return redirect ( url for ( 'cases.dahboard general.html' ) ) LOG . debug ( 'User accessible institutes: {}' . format ( accessible institutes ) ) institutes = [ inst for inst in store . institutes ( accessible institutes ) ] institutes . insert ( 0 , { ' id' : None , 'display name' : 'All institutes' } ) institute id = None slice query = None panel = 1 if request . method == 'POST' : institute id = request . form . get ( 'institute' ) slice query = request . form . get ( 'query' ) panel = request . form . get ( 'pane id' ) elif request . method == 'GET' : institute id = request . args . get ( 'institute' ) slice query = request . args . get ( 'query' ) #1) Their default institute when the page is first loaded #2) if they ask for an institute that they don't belong to #3) if they want perform a query on all institutes if not institute id : institute id = accessible institutes [ 0 ] elif ( not current user . is admin ) and ( slice query and institute id == 'None' ) : institute id = accessible institutes [ 0 ] elif ( not institute id in accessible institutes ) and not ( institute id == 'None' ) : institute id = accessible institutes [ 0 ] LOG . info ( \"Fetch all cases with institute: %s\" , institute id ) data = get dashboard info ( store , institute id , slice query ) data [ 'institutes' ] = institutes data [ 'choice' ] = institute id total cases = data [ 'total cases' ] LOG . info ( \"Found %s cases\" , total cases ) if total cases == 0 : flash ( 'no cases found for institute {} (with that query) - please visit the dashboard later!' . format ( institute id ) , 'info' ) return render template ( 'dashboard/dashboard general.html' , institute = institute id , query = slice query , panel = panel , * * data )", "predictions": ["now we get a index for the user ."], "references": ["display the scout dashboard ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 2821, "code": "def transcripts ( context , build , hgnc id , json ) : LOG . info ( \"Running scout view transcripts\" ) adapter = context . obj [ 'adapter' ] if not json : click . echo ( \"Chromosome\\tstart\\tend\\ttranscript id\\thgnc id\\trefseq\\tis primary\" ) for tx obj in adapter . transcripts ( build = build , hgnc id = hgnc id ) : if json : pp ( tx obj ) continue click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\" . format ( tx obj [ 'chrom' ] , tx obj [ 'start' ] , tx obj [ 'end' ] , tx obj [ 'ensembl transcript id' ] , tx obj [ 'hgnc id' ] , tx obj . get ( 'refseq id' , '' ) , tx obj . get ( 'is primary' ) or '' , ) )", "predictions": ["create the transcripts object from all transcripts including the existing transcripts ."], "references": ["show all transcripts in the database"], "bleu": 0.14694106251955755, "rouge_l": 0.3546511627906977}
{"id": 2822, "code": "def variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : variant count = variants query . count ( ) skip count = per page * max ( page - 1 , 0 ) more variants = True if variant count > ( skip count + per page ) else False variant res = variants query . skip ( skip count ) . limit ( per page ) genome build = case obj . get ( 'genome build' , '37' ) if genome build not in [ '37' , '38' ] : genome build = '37' variants = [ ] for variant obj in variant res : overlapping svs = [ sv for sv in store . overlapping ( variant obj ) ] variant obj [ 'overlapping' ] = overlapping svs or None variants . append ( parse variant ( store , institute obj , case obj , variant obj , update = True , genome build = genome build ) ) return { 'variants' : variants , 'more variants' : more variants , }", "predictions": ["variants the way to . all the . values ."], "references": ["pre - process list of variants ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2823, "code": "def sv variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : skip count = ( per page * max ( page - 1 , 0 ) ) more variants = True if variants query . count ( ) > ( skip count + per page ) else False genome build = case obj . get ( 'genome build' , '37' ) if genome build not in [ '37' , '38' ] : genome build = '37' return { 'variants' : ( parse variant ( store , institute obj , case obj , variant , genome build = genome build ) for variant in variants query . skip ( skip count ) . limit ( per page ) ) , 'more variants' : more variants , }", "predictions": ["perform a sv skip operation"], "references": ["pre - process list of sv variants ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2824, "code": "def str variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : return variants ( store , institute obj , case obj , variants query , page , per page )", "predictions": ["print out the object for each object in the store ."], "references": ["pre - process list of str variants ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 2825, "code": "def get predictions ( genes ) : data = { 'sift predictions' : [ ] , 'polyphen predictions' : [ ] , 'region annotations' : [ ] , 'functional annotations' : [ ] } for gene obj in genes : for pred key in data : gene key = pred key [ : - 1 ] if len ( genes ) == 1 : value = gene obj . get ( gene key , '-' ) else : gene id = gene obj . get ( 'hgnc symbol' ) or str ( gene obj [ 'hgnc id' ] ) value = ':' . join ( [ gene id , gene obj . get ( gene key , '-' ) ] ) data [ pred key ] . append ( value ) return data", "predictions": ["this is a . operation ."], "references": ["get sift predictions from genes ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2826, "code": "def find bai file ( bam file ) : bai file = bam file . replace ( '.bam' , '.bai' ) if not os . path . exists ( bai file ) : bai file = \"{}.bai\" . format ( bam file ) return bai file", "predictions": ["find the bai on the top of the file ."], "references": ["find out bai file by extension given the bam file ."], "bleu": 0.17851905035930718, "rouge_l": 0.47213622291021673}
{"id": 2827, "code": "def observations ( store , loqusdb , case obj , variant obj ) : composite id = ( \"{this[chromosome]} {this[position]} {this[reference]} \" \"{this[alternative]}\" . format ( this = variant obj ) ) obs data = loqusdb . get variant ( { ' id' : composite id } ) or { } obs data [ 'total' ] = loqusdb . case count ( ) obs data [ 'cases' ] = [ ] institute id = variant obj [ 'institute' ] for case id in obs data . get ( 'families' , [ ] ) : if case id != variant obj [ 'case id' ] and case id . startswith ( institute id ) : other variant = store . variant ( variant obj [ 'variant id' ] , case id = case id ) other case = store . case ( case id ) obs data [ 'cases' ] . append ( dict ( case = other case , variant = other variant ) ) return obs data", "predictions": ["format with composite data"], "references": ["query observations for a variant ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2828, "code": "def parse gene ( gene obj , build = None ) : build = build or 37 if gene obj . get ( 'common' ) : add gene links ( gene obj , build ) refseq transcripts = [ ] for tx obj in gene obj [ 'transcripts' ] : parse transcript ( gene obj , tx obj , build ) if not tx obj . get ( 'refseq id' ) : continue refseq transcripts . append ( tx obj ) gene obj [ 'primary transcripts' ] = ( refseq transcripts if refseq transcripts else [ ] )", "predictions": ["parse all registered transcripts ."], "references": ["parse variant genes ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 2829, "code": "def transcript str ( transcript obj , gene name = None ) : if transcript obj . get ( 'exon' ) : gene part , part count raw = 'exon' , transcript obj [ 'exon' ] elif transcript obj . get ( 'intron' ) : gene part , part count raw = 'intron' , transcript obj [ 'intron' ] else : gene part , part count raw = 'intergenic' , '0' part count = part count raw . rpartition ( '/' ) [ 0 ] change str = \"{}:{}{}:{}:{}\" . format ( transcript obj . get ( 'refseq id' , '' ) , gene part , part count , transcript obj . get ( 'coding sequence name' , 'NA' ) , transcript obj . get ( 'protein sequence name' , 'NA' ) , ) if gene name : change str = \"{}:\" . format ( gene name ) + change str return change str", "predictions": ["get the transcript object from an object ."], "references": ["generate amino acid change as a string ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 2830, "code": "def end position ( variant obj ) : alt bases = len ( variant obj [ 'alternative' ] ) num bases = max ( len ( variant obj [ 'reference' ] ) , alt bases ) return variant obj [ 'position' ] + ( num bases - 1 )", "predictions": ["get the position of each key in this context ."], "references": ["calculate end position for a variant ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2831, "code": "def clinsig human ( variant obj ) : for clinsig obj in variant obj [ 'clnsig' ] : if isinstance ( clinsig obj [ 'accession' ] , int ) : link = \"https://www.ncbi.nlm.nih.gov/clinvar/variation/{}\" else : link = \"https://www.ncbi.nlm.nih.gov/clinvar/{}\" human str = 'not provided' if clinsig obj . get ( 'value' ) : try : int ( clinsig obj [ 'value' ] ) human str = CLINSIG MAP . get ( clinsig obj [ 'value' ] , 'not provided' ) except Value Error : human str = clinsig obj [ 'value' ] clinsig obj [ 'human' ] = human str clinsig obj [ 'link' ] = link . format ( clinsig obj [ 'accession' ] ) yield clinsig obj", "predictions": ["this will return a human readable version of the passed in obj ."], "references": ["convert to human readable version of clinsig evaluation ."], "bleu": 0.27824623288353134, "rouge_l": 0.4699537750385208}
{"id": 2832, "code": "def thousandg link ( variant obj , build = None ) : dbsnp id = variant obj . get ( 'dbsnp id' ) build = build or 37 if not dbsnp id : return None if build == 37 : url template = ( \"http://grch37.ensembl.org/Homo sapiens/Variation/Explore\" \"?v={};vdb=variation\" ) else : url template = ( \"http://www.ensembl.org/Homo sapiens/Variation/Explore\" \"?v={};vdb=variation\" ) return url template . format ( dbsnp id )", "predictions": ["internal method to link to the ( template ."], "references": ["compose link to 1000g page for detailed information ."], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 2833, "code": "def beacon link ( variant obj , build = None ) : build = build or 37 url template = ( \"https://beacon-network.org/#/search?pos={this[position]}&\" \"chrom={this[chromosome]}&allele={this[alternative]}&\" \"ref={this[reference]}&rs=GR Ch37\" ) return url template . format ( this = variant obj )", "predictions": ["converts the given object to the configured cache ."], "references": ["compose link to beacon network ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2834, "code": "def ucsc link ( variant obj , build = None ) : build = build or 37 url template = ( \"http://genome.ucsc.edu/cgi-bin/hg Tracks?db=hg19&\" \"position=chr{this[chromosome]}:{this[position]}\" \"-{this[position]}&dgv=pack&known Gene=pack&omim Gene=pack\" ) if build == 38 : url template = ( \"http://genome.ucsc.edu/cgi-bin/hg Tracks?db=hg20&\" \"position=chr{this[chromosome]}:{this[position]}\" \"-{this[position]}&dgv=pack&known Gene=pack&omim Gene=pack\" ) return url template . format ( this = variant obj )", "predictions": ["internal method to generate the url for this object ."], "references": ["compose link to ucsc ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 2835, "code": "def spidex human ( variant obj ) : if variant obj . get ( 'spidex' ) is None : return 'not reported' elif abs ( variant obj [ 'spidex' ] ) < SPIDEX HUMAN [ 'low' ] [ 'pos' ] [ 1 ] : return 'low' elif abs ( variant obj [ 'spidex' ] ) < SPIDEX HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] : return 'medium' else : return 'high'", "predictions": ["spidex : get the variant of this instance which is the correct order of the variant of the given object ."], "references": ["translate spidex annotation to human readable string ."], "bleu": 0.06429451441231726, "rouge_l": 0.15006150061500614}
{"id": 2836, "code": "def expected inheritance ( variant obj ) : manual models = set ( ) for gene in variant obj . get ( 'genes' , [ ] ) : manual models . update ( gene . get ( 'manual inheritance' , [ ] ) ) return list ( manual models )", "predictions": ["get a list of expected instances of this database ."], "references": ["gather information from common gene information ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2837, "code": "def callers ( variant obj , category = 'snv' ) : calls = set ( ) for caller in CALLERS [ category ] : if variant obj . get ( caller [ 'id' ] ) : calls . add ( ( caller [ 'name' ] , variant obj [ caller [ 'id' ] ] ) ) return list ( calls )", "predictions": ["loop over all calls of the given variant ."], "references": ["return info about callers ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2838, "code": "def cancer variants ( store , request args , institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) form = Cancer Filters Form ( request args ) variants query = store . variants ( case obj [ ' id' ] , category = 'cancer' , query = form . data ) . limit ( 50 ) data = dict ( institute = institute obj , case = case obj , variants = ( parse variant ( store , institute obj , case obj , variant , update = True ) for variant in variants query ) , form = form , variant type = request args . get ( 'variant type' , 'clinical' ) , ) return data", "predictions": ["creates a jce object from the parameters passed in ."], "references": ["fetch data related to cancer variants for a case ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 2839, "code": "def variant acmg ( store , institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) return dict ( institute = institute obj , case = case obj , variant = variant obj , CRITERIA = ACMG CRITERIA , ACMG OPTIONS = ACMG OPTIONS )", "predictions": ["add a variant entry to the store"], "references": ["collect data relevant for rendering acmg classification form ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 2840, "code": "def variant acmg post ( store , institute id , case name , variant id , user email , criteria ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( user email ) variant link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) classification = store . submit evaluation ( institute obj = institute obj , case obj = case obj , variant obj = variant obj , user obj = user obj , link = variant link , criteria = criteria , ) return classification", "predictions": ["submits a classification to the specified store ."], "references": ["calculate an acmg classification based on a list of criteria ."], "bleu": 0.13107175678306446, "rouge_l": 0.20469798657718125}
{"id": 2841, "code": "def evaluation ( store , evaluation obj ) : evaluation obj [ 'institute' ] = store . institute ( evaluation obj [ 'institute id' ] ) evaluation obj [ 'case' ] = store . case ( evaluation obj [ 'case id' ] ) evaluation obj [ 'variant' ] = store . variant ( evaluation obj [ 'variant specific' ] ) evaluation obj [ 'criteria' ] = { criterion [ 'term' ] : criterion for criterion in evaluation obj [ 'criteria' ] } evaluation obj [ 'classification' ] = ACMG COMPLETE MAP [ evaluation obj [ 'classification' ] ] return evaluation obj", "predictions": ["evaluation a set of pairs from a store of raw partitions ."], "references": ["fetch and fill - in evaluation object ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 2842, "code": "def upload panel ( store , institute id , case name , stream ) : institute obj , case obj = institute and case ( store , institute id , case name ) raw symbols = [ line . strip ( ) . split ( '\\t' ) [ 0 ] for line in stream if line and not line . startswith ( '#' ) ] hgnc symbols = [ ] for raw symbol in raw symbols : if store . hgnc genes ( raw symbol ) . count ( ) == 0 : flash ( \"HGNC symbol not found: {}\" . format ( raw symbol ) , 'warning' ) else : hgnc symbols . append ( raw symbol ) return hgnc symbols", "predictions": ["uploads all found ( from this object"], "references": ["parse out hgnc symbols from a stream ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2843, "code": "def export genes ( adapter , build = '37' ) : LOG . info ( \"Exporting all genes to .bed format\" ) for gene obj in adapter . all genes ( build = build ) : yield gene obj", "predictions": ["export all . of the given adapter into their associated . ."], "references": ["export all genes from the database"], "bleu": 0.14694106251955755, "rouge_l": 0.3546511627906977}
{"id": 2844, "code": "def index ( context , collection name ) : LOG . info ( \"Running scout view index\" ) adapter = context . obj [ 'adapter' ] i = 0 click . echo ( \"collection\\tindex\" ) for collection name in adapter . collections ( ) : for index in adapter . indexes ( collection name ) : click . echo ( \"{0}\\t{1}\" . format ( collection name , index ) ) i += 1 if i == 0 : LOG . info ( \"No indexes found\" )", "predictions": ["create a . object ."], "references": ["show all indexes in the database"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2845, "code": "def genes ( context , build , json ) : LOG . info ( \"Running scout export genes\" ) adapter = context . obj [ 'adapter' ] result = adapter . all genes ( build = build ) if json : click . echo ( dumps ( result ) ) return gene string = ( \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\" ) click . echo ( \"#Chromosom\\t Start\\t End\\t Hgnc id\\t Hgnc symbol\" ) for gene obj in result : click . echo ( gene string . format ( gene obj [ 'chromosome' ] , gene obj [ 'start' ] , gene obj [ 'end' ] , gene obj [ 'hgnc id' ] , gene obj [ 'hgnc symbol' ] , ) )", "predictions": ["genes the object for a given ( ."], "references": ["export all genes from a build"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 2846, "code": "def case ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) if case obj is None : return abort ( 404 ) return Response ( json util . dumps ( case obj ) , mimetype = 'application/json' )", "predictions": ["get the name of the object ."], "references": ["return a variant ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2847, "code": "def variant ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) return Response ( json util . dumps ( variant obj ) , mimetype = 'application/json' )", "predictions": ["make a change where the variant of the specified object is the correct identifier ."], "references": ["display a specific snv variant ."], "bleu": 0.09782375748961449, "rouge_l": 0.3096446700507614}
{"id": 2848, "code": "def collections ( context ) : LOG . info ( \"Running scout view collections\" ) adapter = context . obj [ 'adapter' ] for collection name in adapter . collections ( ) : click . echo ( collection name )", "predictions": ["creates a list of delivery of an delivery ."], "references": ["show all collections in the database"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2849, "code": "def institute ( ctx , internal id , display name , sanger recipients ) : adapter = ctx . obj [ 'adapter' ] if not internal id : logger . warning ( \"A institute has to have an internal id\" ) ctx . abort ( ) if not display name : display name = internal id if sanger recipients : sanger recipients = list ( sanger recipients ) try : load institute ( adapter = adapter , internal id = internal id , display name = display name , sanger recipients = sanger recipients ) except Exception as e : logger . warning ( e ) ctx . abort ( )", "predictions": [". all the recipients and . to handle a ."], "references": ["create a new institute and add it to the database"], "bleu": 0.15851165692617156, "rouge_l": 0.2}
{"id": 2850, "code": "def get file handle ( file path ) : if file path . endswith ( '.gz' ) : file handle = getreader ( 'utf-8' ) ( gzip . open ( file path , 'r' ) , errors = 'replace' ) else : file handle = open ( file path , 'r' , encoding = 'utf-8' ) return file handle", "predictions": ["handles a ( 'utf-8' id id id id id id id id id id id id id id id id id id id id id id id = ( ( path id id id id id id id id id id id id id id id id id id ="], "references": ["return a opened file"], "bleu": 0.02403051755364481, "rouge_l": 0.04375896700143472}
{"id": 2851, "code": "def get net ( req ) : try : nxt , prev = map ( int , ( req . GET . get ( 'cal next' , 0 ) , req . GET . get ( 'cal prev' , 0 ) ) ) net = nxt - prev except Exception : net = 0 return net", "predictions": ["retrieves and return the unique composite of this request ."], "references": ["get the net of any next and prev querystrings ."], "bleu": 0.15851165692617156, "rouge_l": 0.3}
{"id": 2852, "code": "def get next and prev ( net ) : if net == 0 : nxt = prev = 1 elif net > 0 : nxt = net + 1 prev = - ( net - 1 ) else : nxt = net + 1 prev = abs ( net ) + 1 return nxt , prev", "predictions": ["set the ( ( = in the experiment . . = null = 0 = ( = ( = ( = ( = ( = ( = interrupted . = 0 . 1 = 0 . . . . . = 1 = 8 . . . = 8 ."], "references": ["returns what the next and prev querystrings should be ."], "bleu": 0.026594139297659906, "rouge_l": 0.07577639751552796}
{"id": 2853, "code": "def check year ( year , month , error , error msg ) : if year not in xrange ( ( now . year - 50 ) , ( now . year + 51 ) ) : year = now . year month = now . month error = error msg return year , month , error", "predictions": ["method to validate the build - only if the build is within the build - 1 . 0 if the build method is not cached ."], "references": ["checks that the year is within 50 years from now ."], "bleu": 0.06980361417366379, "rouge_l": 0.23326959847036327}
{"id": 2854, "code": "def add peddy information ( config data ) : ped info = { } ped check = { } sex check = { } relations = [ ] if config data . get ( 'peddy ped' ) : file handle = open ( config data [ 'peddy ped' ] , 'r' ) for ind info in parse peddy ped ( file handle ) : ped info [ ind info [ 'sample id' ] ] = ind info if config data . get ( 'peddy ped check' ) : file handle = open ( config data [ 'peddy ped check' ] , 'r' ) for pair info in parse peddy ped check ( file handle ) : ped check [ ( pair info [ 'sample a' ] , pair info [ 'sample b' ] ) ] = pair info if config data . get ( 'peddy sex check' ) : file handle = open ( config data [ 'peddy sex check' ] , 'r' ) for ind info in parse peddy sex check ( file handle ) : sex check [ ind info [ 'sample id' ] ] = ind info if not ped info : return analysis inds = { } for ind in config data [ 'samples' ] : ind id = ind [ 'sample id' ] analysis inds [ ind id ] = ind for ind id in analysis inds : ind = analysis inds [ ind id ] if ind id in ped info : ind [ 'predicted ancestry' ] = ped info [ ind id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) if ind id in sex check : if sex check [ ind id ] [ 'error' ] : ind [ 'confirmed sex' ] = False else : ind [ 'confirmed sex' ] = True for parent in [ 'mother' , 'father' ] : if ind [ parent ] != '0' : for pair in ped check : if ( ind id in pair and ind [ parent ] in pair ) : if ped check [ pair ] [ 'parent error' ] : analysis inds [ ind [ parent ] ] [ 'confirmed parent' ] = False else : if 'confirmed parent' not in analysis inds [ ind [ parent ] ] : analysis inds [ ind [ parent ] ] [ 'confirmed parent' ] = True", "predictions": ["variants all ( possibly in the base reference case we are only one reference to the current reference . this is useful for a test case ."], "references": ["add information from peddy outfiles to the individuals"], "bleu": 0.059112458831223215, "rouge_l": 0.12668743509865005}
{"id": 2855, "code": "def panel ( context , path , date , display name , version , panel type , panel id , institute , omim , api key , panel app ) : adapter = context . obj [ 'adapter' ] institute = institute or 'cust000' if omim : api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) context . abort ( ) #Check if OMIM-AUTO exists if adapter . gene panel ( panel id = 'OMIM-AUTO' ) : LOG . warning ( \"OMIM-AUTO already exists in database\" ) LOG . info ( \"To create a new version use scout update omim\" ) return try : adapter . load omim panel ( api key , institute = institute ) except Exception as err : LOG . error ( err ) context . abort ( ) if panel app : load panel app ( adapter , panel id , institute = institute ) if ( omim or panel app ) : return if path is None : LOG . info ( \"Please provide a panel\" ) return try : load panel ( path , adapter , date , display name , version , panel type , panel id , institute ) except Exception as err : LOG . warning ( err ) context . abort ( )", "predictions": ["[ obj ] [ obj ] store in dev dev api call ."], "references": ["add a gene panel to the database ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 2856, "code": "def panel ( context , panel id , version ) : LOG . info ( \"Running scout delete panel\" ) adapter = context . obj [ 'adapter' ] panel objs = adapter . gene panels ( panel id = panel id , version = version ) if panel objs . count ( ) == 0 : LOG . info ( \"No panels found\" ) for panel obj in panel objs : adapter . delete panel ( panel obj )", "predictions": ["50 a str object and 50 its . ."], "references": ["delete a version of a gene panel or all versions of a gene panel"], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 2857, "code": "def index ( context ) : LOG . info ( \"Running scout delete index\" ) adapter = context . obj [ 'adapter' ] for collection in adapter . db . collection names ( ) : adapter . db [ collection ] . drop indexes ( ) LOG . info ( \"All indexes deleted\" )", "predictions": ["deletes an get ] ."], "references": ["delete all indexes in the database"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2858, "code": "def user ( context , mail ) : LOG . info ( \"Running scout delete user\" ) adapter = context . obj [ 'adapter' ] user obj = adapter . user ( mail ) if not user obj : LOG . warning ( \"User {0} could not be found in database\" . format ( mail ) ) else : adapter . delete user ( mail )", "predictions": ["deletes an not associated with the find find ."], "references": ["delete a user from the database"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2859, "code": "def genes ( context , build ) : LOG . info ( \"Running scout delete genes\" ) adapter = context . obj [ 'adapter' ] if build : LOG . info ( \"Dropping genes collection for build: %s\" , build ) else : LOG . info ( \"Dropping genes collection\" ) adapter . drop genes ( )", "predictions": ["deletes all object instances of this object ."], "references": ["delete all genes in the database"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2860, "code": "def exons ( context , build ) : LOG . info ( \"Running scout delete exons\" ) adapter = context . obj [ 'adapter' ] adapter . drop exons ( build )", "predictions": ["deletes the managed get on the object ."], "references": ["delete all exons in the database"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2861, "code": "def case ( context , institute , case id , display name ) : adapter = context . obj [ 'adapter' ] if not ( case id or display name ) : click . echo ( \"Please specify what case to delete\" ) context . abort ( ) if display name : if not institute : click . echo ( \"Please specify the owner of the case that should be \" \"deleted with flag '-i/--institute'.\" ) context . abort ( ) case id = \"{0}-{1}\" . format ( institute , display name ) LOG . info ( \"Running deleting case {0}\" . format ( case id ) ) case = adapter . delete case ( case id = case id , institute id = institute , display name = display name ) if case . deleted count == 1 : adapter . delete variants ( case id = case id , variant type = 'clinical' ) adapter . delete variants ( case id = case id , variant type = 'research' ) else : LOG . warning ( \"Case does not exist in database\" ) context . abort ( )", "predictions": ["delete the transcript , removing all of its children . this should be called after the name of the name of the : 1 ) . 2 . 3 . 3 ) ."], "references": ["delete a case and it s variants from the database"], "bleu": 0.0405185766962521, "rouge_l": 0.1029535864978903}
{"id": 2862, "code": "def individuals ( context , institute , causatives , case id ) : LOG . info ( \"Running scout view individuals\" ) adapter = context . obj [ 'adapter' ] individuals = [ ] if case id : case = adapter . case ( case id = case id ) if case : cases = [ case ] else : LOG . info ( \"Could not find case %s\" , case id ) return else : cases = [ case obj for case obj in adapter . cases ( collaborator = institute , has causatives = causatives ) ] if len ( cases ) == 0 : LOG . info ( \"Could not find cases that match criteria\" ) return individuals = ( ind obj for case obj in cases for ind obj in case obj [ 'individuals' ] ) click . echo ( \"#case id\\tind id\\tdisplay name\\tsex\\tphenotype\\tmother\\tfather\" ) for case in cases : for ind obj in case [ 'individuals' ] : ind info = [ case [ ' id' ] , ind obj [ 'individual id' ] , ind obj [ 'display name' ] , SEX MAP [ int ( ind obj [ 'sex' ] ) ] , PHENOTYPE MAP [ ind obj [ 'phenotype' ] ] , ind obj [ 'mother' ] , ind obj [ 'father' ] ] click . echo ( '\\t' . join ( ind info ) )", "predictions": ["locate the ( object for a given audit object ."], "references": ["show all individuals from all cases in the database"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 2863, "code": "def cases ( context , institute , display name , case id , nr variants , variants treshold ) : LOG . info ( \"Running scout view institutes\" ) adapter = context . obj [ 'adapter' ] models = [ ] if case id : case obj = adapter . case ( case id = case id ) if case obj : models . append ( case obj ) else : models = adapter . cases ( collaborator = institute , name query = display name ) models = [ case obj for case obj in models ] if not models : LOG . info ( \"No cases could be found\" ) return header = [ 'case id' , 'display name' , 'institute' ] if variants treshold : LOG . info ( \"Only show cases with more than %s variants\" , variants treshold ) nr variants = True if nr variants : LOG . info ( \"Displaying number of variants for each case\" ) header . append ( 'clinical' ) header . append ( 'research' ) click . echo ( \"#\" + '\\t' . join ( header ) ) for model in models : output str = \"{:<12}\\t{:<12}\\t{:<12}\" output values = [ model [ ' id' ] , model [ 'display name' ] , model [ 'owner' ] ] if nr variants : output str += \"\\t{:<12}\\t{:<12}\" nr clinical = 0 nr research = 0 variants = adapter . variant collection . find ( { 'case id' : model [ ' id' ] } ) i = 0 for i , var in enumerate ( variants , 1 ) : if var [ 'variant type' ] == 'clinical' : nr clinical += 1 else : nr research += 1 output values . extend ( [ nr clinical , nr research ] ) if variants treshold and i < variants treshold : LOG . debug ( \"Case %s had to few variants, skipping\" , model [ ' id' ] ) continue click . echo ( output str . format ( * output values ) )", "predictions": ["creates an array of known ( objects for each of the given ( ."], "references": ["display cases from the database"], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 2864, "code": "def load user ( user email ) : user obj = store . user ( user email ) user inst = Login User ( user obj ) if user obj else None return user inst", "predictions": ["loads the link and stores it in the database ."], "references": ["returns the currently active user as an object ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 2865, "code": "def login ( ) : if 'next' in request . args : session [ 'next url' ] = request . args [ 'next' ] if current app . config . get ( 'GOOGLE' ) : callback url = url for ( '.authorized' , external = True ) return google . authorize ( callback = callback url ) user email = request . args . get ( 'email' ) user obj = store . user ( user email ) if user obj is None : flash ( \"email not whitelisted: {}\" . format ( user email ) , 'warning' ) return redirect ( url for ( 'public.index' ) ) return perform login ( user obj )", "predictions": ["loop through the = ( , email , ( , ( , ( , ( , ( , ( , ( , . , ( , . , ( , ( , ( , ( , ( , ( , . , ( , . , . , . ,"], "references": ["login a user if they have access ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 2866, "code": "def user events ( self , user obj = None ) : query = dict ( user id = user obj [ ' id' ] ) if user obj else dict ( ) return self . event collection . find ( query )", "predictions": ["get ui object for a ucsc ."], "references": ["fetch all events by a specific user ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2867, "code": "def hpo terms ( ) : if request . method == 'GET' : data = controllers . hpo terms ( store = store , limit = 100 ) return data else : search term = request . form . get ( 'hpo term' ) limit = request . form . get ( 'limit' ) data = controllers . hpo terms ( store = store , query = search term , limit = limit ) return dict ( data , query = search term , limit = limit )", "predictions": ["hpo all command from the given store ."], "references": ["render search box and view for hpo phenotype terms"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2868, "code": "def transcripts ( context , build ) : LOG . info ( \"Running scout export transcripts\" ) adapter = context . obj [ 'adapter' ] header = [ \"#Chrom\\t Start\\t End\\t Transcript\\t Ref Seq\\t Hgnc ID\" ] for line in header : click . echo ( line ) transcript string = ( \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\" ) for tx obj in export transcripts ( adapter ) : click . echo ( transcript string . format ( tx obj [ 'chrom' ] , tx obj [ 'start' ] , tx obj [ 'end' ] , tx obj [ 'ensembl transcript id' ] , tx obj . get ( 'refseq id' , '' ) , tx obj [ 'hgnc id' ] , ) )", "predictions": ["write all necessary ( from the specified object to all registered transcript and . ."], "references": ["export all transcripts to . bed like format"], "bleu": 0.09782375748961449, "rouge_l": 0.2760180995475113}
{"id": 2869, "code": "def exons ( context , build ) : adapter = context . obj [ 'adapter' ] start = datetime . now ( ) nr exons = adapter . exons ( build = build ) . count ( ) if nr exons : LOG . warning ( \"Dropping all exons \" ) adapter . drop exons ( build = build ) LOG . info ( \"Exons dropped\" ) ensembl exons = fetch ensembl exons ( build = build ) load exons ( adapter , ensembl exons , build ) adapter . update indexes ( ) LOG . info ( \"Time to load exons: {0}\" . format ( datetime . now ( ) - start ) )", "predictions": ["callers call to load the ( ."], "references": ["load exons into the scout database"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2870, "code": "def intervals ( context , build ) : LOG . info ( \"Running scout view index\" ) adapter = context . obj [ 'adapter' ] intervals = adapter . get coding intervals ( build ) nr intervals = 0 longest = 0 for chrom in CHROMOSOMES : for iv in intervals [ chrom ] : iv len = iv . end - iv . begin if iv len > longest : longest = iv len int nr = len ( intervals . get ( chrom , [ ] ) ) click . echo ( \"{0}\\t{1}\" . format ( chrom , int nr ) ) nr intervals += int nr LOG . info ( \"Total nr intervals:%s\" , nr intervals ) LOG . info ( \"Total nr genes:%s\" , adapter . all genes ( build ) . count ( ) ) LOG . info ( \"Longest interval:%s\" , longest )", "predictions": ["performs a cancer of all cancer registered cancer ."], "references": ["show all indexes in the database"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2871, "code": "def region ( context , hgnc id , case id , chromosome , start , end ) : adapter = context . obj [ 'adapter' ] load region ( adapter = adapter , case id = case id , hgnc id = hgnc id , chrom = chromosome , start = start , end = end )", "predictions": ["creates a new instance of the map ."], "references": ["load all variants in a region to a existing case"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 2872, "code": "def parse reqs ( req path = './requirements.txt' ) : install requires = [ ] with io . open ( os . path . join ( here , 'requirements.txt' ) , encoding = 'utf-8' ) as handle : lines = ( line . strip ( ) for line in handle if line . strip ( ) and not line . startswith ( '#' ) ) for line in lines : if line . startswith ( '-r' ) : install requires += parse reqs ( req path = line [ 3 : ] ) else : install requires . append ( line ) return install requires", "predictions": ["variant of ( command ."], "references": ["recursively parse requirements from nested pip files ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2873, "code": "def existing gene ( store , panel obj , hgnc id ) : existing genes = { gene [ 'hgnc id' ] : gene for gene in panel obj [ 'genes' ] } return existing genes . get ( hgnc id )", "predictions": ["get a evaluation to the specified id ."], "references": ["check if gene is already added to a panel ."], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 2874, "code": "def panel export ( store , panel obj ) : panel obj [ 'institute' ] = store . institute ( panel obj [ 'institute' ] ) full name = \"{}({})\" . format ( panel obj [ 'display name' ] , panel obj [ 'version' ] ) panel obj [ 'name and version' ] = full name return dict ( panel = panel obj )", "predictions": ["panel the upload object for given store ."], "references": ["preprocess a panel of genes ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 2875, "code": "def archive info ( database : Database , archive case : dict ) -> dict : data = { 'collaborators' : archive case [ 'collaborators' ] , 'synopsis' : archive case . get ( 'synopsis' ) , 'assignees' : [ ] , 'suspects' : [ ] , 'causatives' : [ ] , 'phenotype terms' : [ ] , 'phenotype groups' : [ ] , } if archive case . get ( 'assignee' ) : archive user = database . user . find one ( { ' id' : archive case [ 'assignee' ] } ) data [ 'assignee' ] . append ( archive user [ 'email' ] ) for key in [ 'suspects' , 'causatives' ] : for variant id in archive case . get ( key , [ ] ) : archive variant = database . variant . find one ( { ' id' : variant id } ) data [ key ] . append ( { 'chromosome' : archive variant [ 'chromosome' ] , 'position' : archive variant [ 'position' ] , 'reference' : archive variant [ 'reference' ] , 'alternative' : archive variant [ 'alternative' ] , 'variant type' : archive variant [ 'variant type' ] , } ) for key in [ 'phenotype terms' , 'phenotype groups' ] : for archive term in archive case . get ( key , [ ] ) : data [ key ] . append ( { 'phenotype id' : archive term [ 'phenotype id' ] , 'feature' : archive term [ 'feature' ] , } ) return data", "predictions": ["creates a simple data structure for the given export data ."], "references": ["get information about a case from archive ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 2876, "code": "def migrate case ( adapter : Mongo Adapter , scout case : dict , archive data : dict ) : collaborators = list ( set ( scout case [ 'collaborators' ] + archive data [ 'collaborators' ] ) ) if collaborators != scout case [ 'collaborators' ] : LOG . info ( f\"set collaborators: {', '.join(collaborators)}\" ) scout case [ 'collaborators' ] = collaborators if len ( scout case . get ( 'assignees' , [ ] ) ) == 0 : scout user = adapter . user ( archive data [ 'assignee' ] ) if scout user : scout case [ 'assignees' ] = [ archive data [ 'assignee' ] ] else : LOG . warning ( f\"{archive data['assignee']}: unable to find assigned user\" ) for key in [ 'suspects' , 'causatives' ] : scout case [ key ] = scout case . get ( key , [ ] ) for archive variant in archive data [ key ] : variant id = get variantid ( archive variant , scout case [ ' id' ] ) scout variant = adapter . variant ( variant id ) if scout variant : if scout variant [ ' id' ] in scout case [ key ] : LOG . info ( f\"{scout variant[' id']}: variant already in {key}\" ) else : LOG . info ( f\"{scout variant[' id']}: add to {key}\" ) scout variant [ key ] . append ( scout variant [ ' id' ] ) else : LOG . warning ( f\"{scout variant[' id']}: unable to find variant ({key})\" ) scout variant [ key ] . append ( variant id ) if not scout case . get ( 'synopsis' ) : scout case [ 'synopsis' ] = archive data [ 'synopsis' ] scout case [ 'is migrated' ] = True adapter . case collection . find one and replace ( { ' id' : scout case [ ' id' ] } , scout case , ) scout institute = adapter . institute ( scout case [ 'owner' ] ) scout user = adapter . user ( 'mans.magnusson@scilifelab.se' ) for key in [ 'phenotype terms' , 'phenotype groups' ] : for archive term in archive data [ key ] : adapter . add phenotype ( institute = scout institute , case = scout case , user = scout user , link = f\"/{scout case['owner']}/{scout case['display name']}\" , hpo term = archive term [ 'phenotype id' ] , is group = key == 'phenotype groups' , )", "predictions": ["creates a newlines representing the vector of actual entry ."], "references": ["migrate case information from archive ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2877, "code": "def migrate ( uri : str , archive uri : str , case id : str , dry : bool , force : bool ) : scout client = Mongo Client ( uri ) scout database = scout client [ uri . rsplit ( '/' , 1 ) [ - 1 ] ] scout adapter = Mongo Adapter ( database = scout database ) scout case = scout adapter . case ( case id ) if not force and scout case . get ( 'is migrated' ) : print ( \"case already migrated\" ) return archive client = Mongo Client ( archive uri ) archive database = archive client [ archive uri . rsplit ( '/' , 1 ) [ - 1 ] ] archive case = archive database . case . find one ( { 'owner' : scout case [ 'owner' ] , 'display name' : scout case [ 'display name' ] } ) archive data = archive info ( archive database , archive case ) if dry : print ( ruamel . yaml . safe dump ( archive data ) ) else : #migrate case(scout adapter, scout case, archive data) pass", "predictions": ["genes the data & ) ."], "references": ["update all information that was manually annotated from a old instance ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 2878, "code": "def hpo ( context , term , description ) : LOG . info ( \"Running scout view hpo\" ) adapter = context . obj [ 'adapter' ] if term : term = term . upper ( ) if not term . startswith ( 'HP:' ) : while len ( term ) < 7 : term = '0' + term term = 'HP:' + term LOG . info ( \"Searching for term %s\" , term ) hpo terms = adapter . hpo terms ( hpo term = term ) elif description : sorted terms = sorted ( adapter . hpo terms ( query = description ) , key = itemgetter ( 'hpo number' ) ) for term in sorted terms : term . pop ( 'genes' ) print ( \"name: {} | {} | {}\" . format ( term [ ' id' ] , term [ 'description' ] , term [ 'hpo number' ] ) ) context . abort ( ) else : hpo terms = adapter . hpo terms ( ) if hpo terms . count ( ) == 0 : LOG . warning ( \"No matching terms found\" ) return click . echo ( \"hpo id\\tdescription\\tnr genes\" ) for hpo obj in hpo terms : click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo obj [ 'hpo id' ] , hpo obj [ 'description' ] , len ( hpo obj . get ( 'genes' , [ ] ) ) ) )", "predictions": ["this method is called by the case when a case of a case where the , the case of the case of a case is a case where the , the method returns ."], "references": ["show all hpo terms in the database"], "bleu": 0.03551851328486764, "rouge_l": 0.05535390199637023}
{"id": 2879, "code": "def create app ( config file = None , config = None ) : app = Flask ( name ) app . config . from pyfile ( 'config.py' ) app . jinja env . add extension ( 'jinja2.ext.do' ) if config : app . config . update ( config ) if config file : app . config . from pyfile ( config file ) app . mme nodes = mme nodes ( app . config . get ( 'MME URL' ) , app . config . get ( 'MME TOKEN' ) ) app . config [ \"JSON SORT KEYS\" ] = False current log level = logger . get Effective Level ( ) coloredlogs . install ( level = 'DEBUG' if app . debug else current log level ) configure extensions ( app ) register blueprints ( app ) register filters ( app ) if not ( app . debug or app . testing ) and app . config . get ( 'MAIL USERNAME' ) : configure email logging ( app ) @ app . before request def check user ( ) : if not app . config . get ( 'LOGIN DISABLED' ) and request . endpoint : static endpoint = 'static' in request . endpoint or 'report' in request . endpoint public endpoint = getattr ( app . view functions [ request . endpoint ] , 'is public' , False ) relevant endpoint = not ( static endpoint or public endpoint ) if relevant endpoint and not current user . is authenticated : next url = \"{}?{}\" . format ( request . path , request . query string . decode ( ) ) login url = url for ( 'login.login' , next = next url ) return redirect ( login url ) return app", "predictions": ["creates a new login ."], "references": ["flask app factory function ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 2880, "code": "def configure extensions ( app ) : extensions . toolbar . init app ( app ) extensions . bootstrap . init app ( app ) extensions . mongo . init app ( app ) extensions . store . init app ( app ) extensions . login manager . init app ( app ) extensions . oauth . init app ( app ) extensions . mail . init app ( app ) Markdown ( app ) if app . config . get ( 'SQLALCHEMY DATABASE URI' ) : configure coverage ( app ) if app . config . get ( 'LOQUSDB SETTINGS' ) : extensions . loqusdb . init app ( app )", "predictions": ["configures all the drivers that can be used to configure the current toolbar ."], "references": ["configure flask extensions ."], "bleu": 0.09782375748961449, "rouge_l": 0.2469635627530364}
{"id": 2881, "code": "def register blueprints ( app ) : app . register blueprint ( public . public bp ) app . register blueprint ( genes . genes bp ) app . register blueprint ( cases . cases bp ) app . register blueprint ( login . login bp ) app . register blueprint ( variants . variants bp ) app . register blueprint ( panels . panels bp ) app . register blueprint ( dashboard . dashboard bp ) app . register blueprint ( api . api bp ) app . register blueprint ( alignviewers . alignviewers bp ) app . register blueprint ( phenotypes . hpo bp ) app . register blueprint ( institutes . overview )", "predictions": ["register the board flow flow"], "references": ["register flask blueprints ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2882, "code": "def configure coverage ( app ) : app . config [ 'SQLALCHEMY TRACK MODIFICATIONS' ] = True if app . debug else False if chanjo api : chanjo api . init app ( app ) configure template filters ( app ) app . register blueprint ( report bp , url prefix = '/reports' ) babel = Babel ( app ) @ babel . localeselector def get locale ( ) : \"\"\"Determine locale to use for translations.\"\"\" accept languages = current app . config . get ( 'ACCEPT LANGUAGES' , [ 'en' ] ) session language = request . args . get ( 'lang' ) if session language in accept languages : current app . logger . info ( \"using session language: %s\" , session language ) return session language user language = current app . config . get ( 'REPORT LANGUAGE' ) if user language : return user language return request . accept languages . best match ( accept languages )", "predictions": ["specify the best session as part of the request ."], "references": ["setup coverage related extensions ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 2883, "code": "def aliases ( context , build , symbol ) : LOG . info ( \"Running scout view aliases\" ) adapter = context . obj [ 'adapter' ] if symbol : alias genes = { } res = adapter . gene by alias ( symbol , build = build ) for gene obj in res : hgnc id = gene obj [ 'hgnc id' ] hgnc symbol = gene obj [ 'hgnc symbol' ] for alias in gene obj [ 'aliases' ] : true id = None if alias == hgnc symbol : true id = hgnc id if alias in alias genes : alias genes [ alias ] [ 'ids' ] . add ( hgnc id ) if true id : alias genes [ alias ] [ 'true' ] = hgnc id else : alias genes [ alias ] = { 'true' : hgnc id , 'ids' : set ( [ hgnc id ] ) } else : alias genes = adapter . genes by alias ( build = build ) if len ( alias genes ) == 0 : LOG . info ( \"No gene found for build %s\" , build ) return click . echo ( \"#hgnc symbol\\ttrue id\\thgnc ids\" ) for alias symbol in alias genes : info = alias genes [ alias symbol ] click . echo ( \"{0}\\t{1}\\t{2}\\t\" . format ( alias symbol , ( alias genes [ alias symbol ] [ 'true' ] or 'None' ) , ', ' . join ( [ str ( gene id ) for gene id in alias genes [ alias symbol ] [ 'ids' ] ] ) ) )", "predictions": ["create the aliases for a given symbol and build the symbol table ."], "references": ["show all alias symbols and how they map to ids"], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 2884, "code": "def variants ( context , collaborator , document id , case id , json ) : LOG . info ( \"Running scout export variants\" ) adapter = context . obj [ 'adapter' ] collaborator = collaborator or 'cust000' variants = export variants ( adapter , collaborator , document id = document id , case id = case id ) if json : click . echo ( dumps ( [ var for var in variants ] ) ) return vcf header = VCF HEADER #If case id is given, print more complete vcf entries, with INFO, #and genotypes if case id : vcf header [ - 1 ] = vcf header [ - 1 ] + \"\\t FORMAT\" case obj = adapter . case ( case id = case id ) for individual in case obj [ 'individuals' ] : vcf header [ - 1 ] = vcf header [ - 1 ] + \"\\t\" + individual [ 'individual id' ] #print header for line in vcf header : click . echo ( line ) for variant obj in variants : variant string = get vcf entry ( variant obj , case id = case id ) click . echo ( variant string )", "predictions": ["variants the ( and . it with ( ."], "references": ["export causatives for a collaborator in . vcf format"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2885, "code": "def serve ( context , config , host , port , debug , livereload ) : pymongo config = dict ( MONGO HOST = context . obj [ 'host' ] , MONGO PORT = context . obj [ 'port' ] , MONGO DBNAME = context . obj [ 'mongodb' ] , MONGO USERNAME = context . obj [ 'username' ] , MONGO PASSWORD = context . obj [ 'password' ] , ) valid connection = check connection ( host = pymongo config [ 'MONGO HOST' ] , port = pymongo config [ 'MONGO PORT' ] , username = pymongo config [ 'MONGO USERNAME' ] , password = pymongo config [ 'MONGO PASSWORD' ] , authdb = context . obj [ 'authdb' ] , ) log . info ( \"Test if mongod is running\" ) if not valid connection : log . warning ( \"Connection could not be established\" ) log . info ( \"Is mongod running?\" ) context . abort ( ) config = os . path . abspath ( config ) if config else None app = create app ( config = pymongo config , config file = config ) if livereload : server = Server ( app . wsgi app ) server . serve ( host = host , port = port , debug = debug ) else : app . run ( host = host , port = port , debug = debug )", "predictions": ["this method is called by the server when the server has been started ."], "references": ["start the web server ."], "bleu": 0.10511846841633776, "rouge_l": 0.34528301886792445}
{"id": 2886, "code": "def init app ( self , app ) : host = app . config . get ( 'MONGO HOST' , 'localhost' ) port = app . config . get ( 'MONGO PORT' , 27017 ) dbname = app . config [ 'MONGO DBNAME' ] log . info ( \"connecting to database: %s:%s/%s\" , host , port , dbname ) self . setup ( app . config [ 'MONGO DATABASE' ] )", "predictions": ["attach the plugin . this is called only from the plugin thread ."], "references": ["setup via flask ."], "bleu": 0.09552040806823771, "rouge_l": 0.1300639658848614}
{"id": 2887, "code": "def setup ( self , database ) : self . db = database self . hgnc collection = database . hgnc gene self . user collection = database . user self . whitelist collection = database . whitelist self . institute collection = database . institute self . event collection = database . event self . case collection = database . case self . panel collection = database . gene panel self . hpo term collection = database . hpo term self . disease term collection = database . disease term self . variant collection = database . variant self . acmg collection = database . acmg self . clinvar collection = database . clinvar self . clinvar submission collection = database . clinvar submission self . exon collection = database . exon self . transcript collection = database . transcript", "predictions": ["creates new database object ."], "references": ["setup connection to database ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 2888, "code": "def index ( context , update ) : LOG . info ( \"Running scout index\" ) adapter = context . obj [ 'adapter' ] if update : adapter . update indexes ( ) else : adapter . load indexes ( )", "predictions": ["load the index on the map ."], "references": ["create indexes for the database"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2889, "code": "def database ( context , institute name , user name , user mail , api key ) : LOG . info ( \"Running scout setup database\" ) api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key with --api-key\" ) context . abort ( ) institute name = institute name or context . obj [ 'institute name' ] user name = user name or context . obj [ 'user name' ] user mail = user mail or context . obj [ 'user mail' ] adapter = context . obj [ 'adapter' ] LOG . info ( \"Setting up database %s\" , context . obj [ 'mongodb' ] ) setup scout ( adapter = adapter , institute id = institute name , user name = user name , user mail = user mail , api key = api key )", "predictions": ["submits an object to add or update the users database ."], "references": ["setup a scout database ."], "bleu": 0.14991106946711685, "rouge_l": 0.2681318681318681}
{"id": 2890, "code": "def setup ( context , institute , user mail , user name ) : context . obj [ 'institute name' ] = institute context . obj [ 'user name' ] = user name context . obj [ 'user mail' ] = user mail if context . invoked subcommand == 'demo' : LOG . debug ( \"Change database name to scout-demo\" ) context . obj [ 'mongodb' ] = 'scout-demo' LOG . info ( \"Setting database name to %s\" , context . obj [ 'mongodb' ] ) LOG . debug ( \"Setting host to %s\" , context . obj [ 'host' ] ) LOG . debug ( \"Setting port to %s\" , context . obj [ 'port' ] ) try : client = get connection ( host = context . obj [ 'host' ] , port = context . obj [ 'port' ] , username = context . obj [ 'username' ] , password = context . obj [ 'password' ] , mongodb = context . obj [ 'mongodb' ] ) except Connection Failure : context . abort ( ) LOG . info ( \"connecting to database %s\" , context . obj [ 'mongodb' ] ) database = client [ context . obj [ 'mongodb' ] ] LOG . info ( \"Test if mongod is running\" ) try : LOG . info ( \"Test if mongod is running\" ) database . test . find one ( ) except Server Selection Timeout Error as err : LOG . warning ( \"Connection could not be established\" ) LOG . warning ( \"Please check if mongod is running\" ) context . abort ( ) LOG . info ( \"Setting up a mongo adapter\" ) mongo adapter = Mongo Adapter ( database ) context . obj [ 'adapter' ] = mongo adapter", "predictions": ["releases the connection object with the database ."], "references": ["setup scout instances ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2891, "code": "def institutes ( context , institute id , json ) : LOG . info ( \"Running scout view institutes\" ) adapter = context . obj [ 'adapter' ] if institute id : institute objs = [ ] institute obj = adapter . institute ( institute id ) if not institute obj : LOG . info ( \"Institute %s does not exost\" , institute id ) return institute objs . append ( institute obj ) else : institute objs = [ ins obj for ins obj in adapter . institutes ( ) ] if len ( institute objs ) == 0 : click . echo ( \"No institutes found\" ) context . abort ( ) header = '' if not json : for key in institute objs [ 0 ] . keys ( ) : header = header + \"{0}\\t\" . format ( key ) click . echo ( header ) for institute obj in institute objs : if json : click . echo ( institute obj ) continue row = '' for value in institute obj . values ( ) : row = row + \"{0}\\t\" . format ( value ) click . echo ( row )", "predictions": ["this is the entry point for a method call ."], "references": ["show all institutes in the database"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2892, "code": "def panels ( context , institute ) : LOG . info ( \"Running scout view panels\" ) adapter = context . obj [ 'adapter' ] panel objs = adapter . gene panels ( institute id = institute ) if panel objs . count ( ) == 0 : LOG . info ( \"No panels found\" ) context . abort ( ) click . echo ( \"#panel name\\tversion\\tnr genes\\tdate\" ) for panel obj in panel objs : click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\" . format ( panel obj [ 'panel name' ] , str ( panel obj [ 'version' ] ) , len ( panel obj [ 'genes' ] ) , str ( panel obj [ 'date' ] . strftime ( '%Y-%m-%d' ) ) ) )", "predictions": ["panels the ( and institute for each object ."], "references": ["show all gene panels in the database"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 2893, "code": "def hpo genes ( context , hpo term ) : LOG . info ( \"Running scout export hpo genes\" ) adapter = context . obj [ 'adapter' ] header = [ \"#Gene id\\t Count\" ] if not hpo term : LOG . warning ( \"Please use at least one hpo term\" ) context . abort ( ) for line in header : click . echo ( line ) for term in adapter . generate hpo gene list ( * hpo term ) : click . echo ( \"{0}\\t{1}\" . format ( term [ 0 ] , term [ 1 ] ) )", "predictions": ["this is called from the execution of ("], "references": ["export a list of genes based on hpo terms"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2894, "code": "def user ( context , institute id , user name , user mail , admin ) : adapter = context . obj [ 'adapter' ] institutes = [ ] for institute in institute id : institute obj = adapter . institute ( institute id = institute ) if not institute obj : LOG . warning ( \"Institute % does not exist\" , institute ) context . abort ( ) institutes . append ( institute ) roles = [ ] if admin : LOG . info ( \"User is admin\" ) roles . append ( 'admin' ) user info = dict ( email = user mail . lower ( ) , name = user name , roles = roles , institutes = institutes ) user obj = build user ( user info ) try : adapter . add user ( user obj ) except Exception as err : LOG . warning ( err ) context . abort ( )", "predictions": ["creates an adapter object for the specified user ."], "references": ["add a user to the database ."], "bleu": 0.16784459625186196, "rouge_l": 0.2557651991614256}
{"id": 2895, "code": "def institutes ( ) : institute objs = user institutes ( store , current user ) institutes = [ ] for ins obj in institute objs : sanger recipients = [ ] for user mail in ins obj . get ( 'sanger recipients' , [ ] ) : user obj = store . user ( user mail ) if not user obj : continue sanger recipients . append ( user obj [ 'name' ] ) institutes . append ( { 'display name' : ins obj [ 'display name' ] , 'internal id' : ins obj [ ' id' ] , 'coverage cutoff' : ins obj . get ( 'coverage cutoff' , 'None' ) , 'sanger recipients' : sanger recipients , 'frequency cutoff' : ins obj . get ( 'frequency cutoff' , 'None' ) , 'phenotype groups' : ins obj . get ( 'phenotype groups' , PHENOTYPE GROUPS ) } ) data = dict ( institutes = institutes ) return render template ( 'overview/institutes.html' , * * data )", "predictions": ["this is a change to the institutes for the institute ."], "references": ["display a list of all user institutes ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 2896, "code": "def remote static ( ) : file path = request . args . get ( 'file' ) range header = request . headers . get ( 'Range' , None ) if not range header and file path . endswith ( '.bam' ) : return abort ( 500 ) new resp = send file partial ( file path ) return new resp", "predictions": ["send an http response to the client ."], "references": ["stream * large * static files with special requirements ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 2897, "code": "def pileup ( ) : vcf file = request . args . get ( 'vcf' ) bam files = request . args . getlist ( 'bam' ) bai files = request . args . getlist ( 'bai' ) samples = request . args . getlist ( 'sample' ) alignments = [ { 'bam' : bam , 'bai' : bai , 'sample' : sample } for bam , bai , sample in zip ( bam files , bai files , samples ) ] position = { 'contig' : request . args [ 'contig' ] , 'start' : request . args [ 'start' ] , 'stop' : request . args [ 'stop' ] } genome = current app . config . get ( 'PILEUP GENOME' ) if genome : if not os . path . isfile ( genome ) : flash ( \"The pilup genome path ({}) provided does not exist\" . format ( genome ) ) genome = None LOG . debug ( \"Use pileup genome %s\" , genome ) exons = current app . config . get ( 'PILEUP EXONS' ) if exons : if not os . path . isfile ( exons ) : flash ( \"The pilup exons path ({}) provided does not exist\" . format ( exons ) ) genome = None LOG . debug ( \"Use pileup exons %s\" , exons ) LOG . debug ( \"View alignment for positions Chrom:{0}, Start:{1}, End: {2}\" . format ( position [ 'contig' ] , position [ 'start' ] , position [ 'stop' ] ) ) LOG . debug ( \"Use alignment files {}\" . format ( alignments ) ) return render template ( 'alignviewers/pileup.html' , alignments = alignments , position = position , vcf file = vcf file , genome = genome , exons = exons )", "predictions": ["generate the pileup for all needed ( ."], "references": ["visualize bam alignments ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2898, "code": "def compounds ( context , case id ) : adapter = context . obj [ 'adapter' ] LOG . info ( \"Running scout update compounds\" ) case obj = adapter . case ( case id ) if not case obj : LOG . warning ( \"Case %s could not be found\" , case id ) context . abort ( ) try : adapter . update case compounds ( case obj ) except Exception as err : LOG . warning ( err ) context . abort ( )", "predictions": ["update the compounds for the given id ."], "references": ["update all compounds for a case"], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 2899, "code": "def hgnc ( ctx , hgnc symbol , hgnc id , build ) : adapter = ctx . obj [ 'adapter' ] if not ( hgnc symbol or hgnc id ) : log . warning ( \"Please provide a hgnc symbol or hgnc id\" ) ctx . abort ( ) if hgnc id : result = adapter . hgnc gene ( hgnc id , build = build ) if result : hgnc symbol = result [ 'hgnc symbol' ] else : log . warning ( \"Gene with id %s could not be found\" , hgnc id ) ctx . abort ( ) result = adapter . hgnc genes ( hgnc symbol , build = build ) if result . count ( ) == 0 : log . info ( \"No results found\" ) else : click . echo ( \"#hgnc id\\thgnc symbol\\taliases\\ttranscripts\" ) for gene in result : click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\" . format ( gene [ 'hgnc id' ] , gene [ 'hgnc symbol' ] , ', ' . join ( gene [ 'aliases' ] ) , ', ' . join ( tx [ 'ensembl transcript id' ] for tx in gene [ 'transcripts' ] ) , ) )", "predictions": ["hgnc all ( / ( / ( / ( ."], "references": ["query the hgnc aliases"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 2900, "code": "def parse hpo obo ( hpo lines ) : term = { } for line in hpo lines : if len ( line ) == 0 : continue line = line . rstrip ( ) if line == '[Term]' : if term : yield term term = { } elif line . startswith ( 'id' ) : term [ 'hpo id' ] = line [ 4 : ] elif line . startswith ( 'name' ) : term [ 'description' ] = line [ 6 : ] elif line . startswith ( 'alt id' ) : if 'aliases' not in term : term [ 'aliases' ] = [ ] term [ 'aliases' ] . append ( line [ 8 : ] ) elif line . startswith ( 'is a' ) : if 'ancestors' not in term : term [ 'ancestors' ] = [ ] term [ 'ancestors' ] . append ( line [ 6 : 16 ] ) if term : yield term", "predictions": ["parse ( format and yield listeners ."], "references": ["parse a . obo formated hpo line"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2901, "code": "def genes ( ) : query = request . args . get ( 'query' , '' ) if '|' in query : hgnc id = int ( query . split ( ' | ' , 1 ) [ 0 ] ) return redirect ( url for ( '.gene' , hgnc id = hgnc id ) ) gene q = store . all genes ( ) . limit ( 20 ) return dict ( genes = gene q )", "predictions": ["genes a query with the same id as the query ."], "references": ["render seach box for genes ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 2902, "code": "def gene ( hgnc id = None , hgnc symbol = None ) : if hgnc symbol : query = store . hgnc genes ( hgnc symbol ) if query . count ( ) == 1 : hgnc id = query . first ( ) [ 'hgnc id' ] else : return redirect ( url for ( '.genes' , query = hgnc symbol ) ) try : genes = controllers . gene ( store , hgnc id ) except Value Error as error : return abort ( 404 ) return genes", "predictions": ["redirect to a symbol ."], "references": ["render information about a gene ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2903, "code": "def api genes ( ) : query = request . args . get ( 'query' ) json out = controllers . genes to json ( store , query ) return jsonify ( json out )", "predictions": ["get the api api store ."], "references": ["return json data about genes ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2904, "code": "def institute and case ( store , institute id , case name = None ) : institute obj = store . institute ( institute id ) if institute obj is None and institute id != 'favicon.ico' : flash ( \"Can't find institute: {}\" . format ( institute id ) , 'warning' ) return abort ( 404 ) if case name : if case name : case obj = store . case ( institute id = institute id , display name = case name ) if case obj is None : return abort ( 404 ) if not current user . is admin : if institute id not in current user . institutes : if not case name or not any ( inst id in case obj [ 'collaborators' ] for inst id in current user . institutes ) : flash ( \"You don't have acccess to: {}\" . format ( institute id ) , 'danger' ) return abort ( 403 ) if case name : return institute obj , case obj else : return institute obj", "predictions": ["institute a institute from the store ."], "references": ["fetch insitiute and case objects ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2905, "code": "def user institutes ( store , login user ) : if login user . is admin : institutes = store . institutes ( ) else : institutes = [ store . institute ( inst id ) for inst id in login user . institutes ] return institutes", "predictions": ["user has been . and stores them in the store ."], "references": ["preprocess institute objects ."], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 2906, "code": "def panel ( context , panel , version , update date , update version ) : adapter = context . obj [ 'adapter' ] panel obj = adapter . gene panel ( panel , version = version ) if not panel obj : LOG . warning ( \"Panel %s (version %s) could not be found\" % ( panel , version ) ) context . abort ( ) date obj = None if update date : try : date obj = get date ( update date ) except Exception as err : LOG . warning ( err ) context . abort ( ) update panel ( adapter , panel , panel version = panel obj [ 'version' ] , new version = update version , new date = date obj )", "predictions": ["update the panel using the panel ."], "references": ["update a panel in the database"], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 2907, "code": "def diseases ( context , api key ) : adapter = context . obj [ 'adapter' ] api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) context . abort ( ) try : mim files = fetch mim files ( api key , genemap2 = True ) except Exception as err : LOG . warning ( err ) context . abort ( ) LOG . info ( \"Dropping Disease Terms\" ) adapter . disease term collection . drop ( ) LOG . debug ( \"Disease Terms dropped\" ) load disease terms ( adapter = adapter , genemap lines = mim files [ 'genemap2' ] , ) LOG . info ( \"Successfully loaded all disease terms\" )", "predictions": ["cleans up an adapter from the given api call ."], "references": ["update disease terms in mongo database ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2908, "code": "def users ( context ) : LOG . info ( \"Running scout view users\" ) adapter = context . obj [ 'adapter' ] user objs = adapter . users ( ) if user objs . count ( ) == 0 : LOG . info ( \"No users found\" ) context . abort ( ) click . echo ( \"#name\\temail\\troles\\tinstitutes\" ) for user obj in user objs : click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\\t\" . format ( user obj [ 'name' ] , user obj . get ( 'mail' , user obj [ ' id' ] ) , ', ' . join ( user obj . get ( 'roles' , [ ] ) ) , ', ' . join ( user obj . get ( 'institutes' , [ ] ) ) , ) )", "predictions": ["creates the users object ."], "references": ["show all users in the database"], "bleu": 0.24736929544091937, "rouge_l": 0.1788856304985337}
{"id": 2909, "code": "def load omim panel ( self , api key , institute = None ) : existing panel = self . gene panel ( panel id = 'OMIM-AUTO' ) if not existing panel : LOG . warning ( \"OMIM-AUTO does not exists in database\" ) LOG . info ( 'Creating a first version' ) version = 1.0 if existing panel : version = float ( math . floor ( existing panel [ 'version' ] ) + 1 ) LOG . info ( \"Setting version to %s\" , version ) try : mim files = fetch mim files ( api key = api key , genemap2 = True , mim2genes = True ) except Exception as err : raise err date string = None for line in mim files [ 'genemap2' ] : if 'Generated' in line : date string = line . split ( ':' ) [ - 1 ] . lstrip ( ) . rstrip ( ) date obj = get date ( date string ) if existing panel : if existing panel [ 'date' ] == date obj : LOG . warning ( \"There is no new version of OMIM\" ) return panel data = { } panel data [ 'path' ] = None panel data [ 'type' ] = 'clinical' panel data [ 'date' ] = date obj panel data [ 'panel id' ] = 'OMIM-AUTO' panel data [ 'institute' ] = institute or 'cust002' panel data [ 'version' ] = version panel data [ 'display name' ] = 'OMIM-AUTO' panel data [ 'genes' ] = [ ] alias genes = self . genes by alias ( ) genes = get omim panel genes ( genemap2 lines = mim files [ 'genemap2' ] , mim2gene lines = mim files [ 'mim2genes' ] , alias genes = alias genes , ) for gene in genes : panel data [ 'genes' ] . append ( gene ) panel obj = build panel ( panel data , self ) if existing panel : new genes = self . compare mim panels ( existing panel , panel obj ) if new genes : self . update mim version ( new genes , panel obj , old version = existing panel [ 'version' ] ) else : LOG . info ( \"The new version of omim does not differ from the old one\" ) LOG . info ( \"No update is added\" ) return self . add gene panel ( panel obj )", "predictions": ["load data from ( into memory ."], "references": ["create and load the omim - auto panel"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2910, "code": "def clinical symbols ( self , case obj ) : panel ids = [ panel [ 'panel id' ] for panel in case obj [ 'panels' ] ] query = self . panel collection . aggregate ( [ { '$match' : { ' id' : { '$in' : panel ids } } } , { '$unwind' : '$genes' } , { '$group' : { ' id' : '$genes.symbol' } } ] ) return set ( item [ ' id' ] for item in query )", "predictions": ["the set of columns in this set to the last panel ."], "references": ["return all the clinical gene symbols for a case ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 2911, "code": "def cases ( context , case id , institute , reruns , finished , causatives , research requested , is research , status , json ) : adapter = context . obj [ 'adapter' ] models = [ ] if case id : case obj = adapter . case ( case id = case id ) if case obj : models . append ( case obj ) else : LOG . info ( \"No case with id {}\" . format ( case id ) ) else : models = adapter . cases ( collaborator = institute , reruns = reruns , finished = finished , has causatives = causatives , research requested = research requested , is research = is research , status = status ) models = [ case obj for case obj in models ] if len ( models ) == 0 : LOG . info ( \"No cases could be found\" ) if json : click . echo ( dumps ( models ) ) return for model in models : pp ( model )", "predictions": ["write the required information for this object ."], "references": ["interact with cases existing in the database ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 2912, "code": "def drop indexes ( self ) : LOG . warning ( \"Dropping all indexe\" ) for collection name in INDEXES : LOG . warning ( \"Dropping all indexes for collection name %s\" , collection name ) self . db [ collection name ] . drop indexes ( )", "predictions": ["configure all the extensions from the database ."], "references": ["delete all indexes for the database"], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 2913, "code": "def wipe ( ctx ) : LOG . info ( \"Running scout wipe\" ) db name = ctx . obj [ 'mongodb' ] LOG . info ( \"Dropping database %s\" , db name ) try : ctx . obj [ 'client' ] . drop database ( db name ) except Exception as err : LOG . warning ( err ) ctx . abort ( ) LOG . info ( \"Dropped whole database\" )", "predictions": ["rq object created using the given database ."], "references": ["drop the mongo database given ."], "bleu": 0.20164945583740668, "rouge_l": 0.43990384615384615}
{"id": 2914, "code": "def parse panel ( csv stream ) : reader = csv . Dict Reader ( csv stream , delimiter = ';' , quoting = csv . QUOTE NONE ) genes = [ ] for gene row in reader : if not gene row [ 'HGNC I Dnumber' ] . strip ( ) . isdigit ( ) : continue transcripts raw = gene row . get ( 'Disease associated transcript' ) if transcripts raw : transcripts list = [ tx . split ( ':' , 1 ) [ - 1 ] . strip ( ) for tx in transcripts raw . split ( ',' ) ] else : transcripts list = [ ] models raw = gene row . get ( 'Genetic disease model' ) models list = [ model . strip ( ) for model in models raw . split ( ',' ) ] if models raw else [ ] panel gene = dict ( symbol = gene row [ 'HGNC symbol' ] . strip ( ) if gene row . get ( 'HGNC symbol' ) else None , hgnc id = int ( gene row [ 'HGNC I Dnumber' ] . strip ( ) ) , disease associated transcripts = transcripts list , reduced penetrance = True if gene row . get ( 'Reduced penetrance' ) else None , mosaicism = True if gene row . get ( 'Mosaicism' ) else None , inheritance models = models list , database entry version = gene row . get ( 'Database entry version' ) , ) genes . append ( panel gene ) return genes", "predictions": ["parses all transcripts transcripts files ."], "references": ["parse user submitted panel ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2915, "code": "def drop genes ( self , build = None ) : if build : LOG . info ( \"Dropping the hgnc gene collection, build %s\" , build ) self . hgnc collection . delete many ( { 'build' : build } ) else : LOG . info ( \"Dropping the hgnc gene collection\" ) self . hgnc collection . drop ( )", "predictions": ["aliases an instance of this ] ."], "references": ["delete the genes collection"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2916, "code": "def drop transcripts ( self , build = None ) : if build : LOG . info ( \"Dropping the transcripts collection, build %s\" , build ) self . transcript collection . delete many ( { 'build' : build } ) else : LOG . info ( \"Dropping the transcripts collection\" ) self . transcript collection . drop ( )", "predictions": ["variants ( or variants json json json json json json json json json json json json json json files json ."], "references": ["delete the transcripts collection"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 2917, "code": "def drop exons ( self , build = None ) : if build : LOG . info ( \"Dropping the exons collection, build %s\" , build ) self . exon collection . delete many ( { 'build' : build } ) else : LOG . info ( \"Dropping the exons collection\" ) self . exon collection . drop ( )", "predictions": ["serve an instance of this collection . this is only used for api requests to be removed ."], "references": ["delete the exons collection"], "bleu": 0.06809398432036522, "rouge_l": 0.1026936026936027}
{"id": 2918, "code": "def omim ( context , api key , institute ) : LOG . info ( \"Running scout update omim\" ) adapter = context . obj [ 'adapter' ] api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) context . abort ( ) institute obj = adapter . institute ( institute ) if not institute obj : LOG . info ( \"Institute %s could not be found in database\" , institute ) LOG . warning ( \"Please specify an existing institute\" ) context . abort ( ) try : adapter . load omim panel ( api key , institute = institute ) except Exception as err : LOG . error ( err ) context . abort ( )", "predictions": ["deletes the init from the given , send it to the server ."], "references": ["update the automate generated omim gene panel in the database ."], "bleu": 0.1135935489027116, "rouge_l": 0.2538141470180305}
{"id": 2919, "code": "def index ( ) : institute objs = user institutes ( store , current user ) institutes count = ( ( institute obj , store . cases ( collaborator = institute obj [ ' id' ] ) . count ( ) ) for institute obj in institute objs if institute obj ) return dict ( institutes = institutes count )", "predictions": ["setup a database with its database ."], "references": ["display a list of all user institutes ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2920, "code": "def cases ( institute id ) : institute obj = institute and case ( store , institute id ) query = request . args . get ( 'query' ) limit = 100 if request . args . get ( 'limit' ) : limit = int ( request . args . get ( 'limit' ) ) skip assigned = request . args . get ( 'skip assigned' ) is research = request . args . get ( 'is research' ) all cases = store . cases ( collaborator = institute id , name query = query , skip assigned = skip assigned , is research = is research ) data = controllers . cases ( store , all cases , limit ) sanger unevaluated = controllers . get sanger unevaluated ( store , institute id , current user . email ) if len ( sanger unevaluated ) > 0 : data [ 'sanger unevaluated' ] = sanger unevaluated return dict ( institute = institute obj , skip assigned = skip assigned , is research = is research , query = query , * * data )", "predictions": ["deletes a query from the cache ."], "references": ["display a list of cases for an institute ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 2921, "code": "def case ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . case ( store , institute obj , case obj ) return dict ( institute = institute obj , case = case obj , * * data )", "predictions": ["database must be able to access itself ."], "references": ["display one case ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2922, "code": "def matchmaker matches ( institute id , case name ) : user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) mme base url = current app . config . get ( 'MME URL' ) mme token = current app . config . get ( 'MME TOKEN' ) if not mme base url or not mme token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . mme matches ( case obj , institute obj , mme base url , mme token ) if data and data . get ( 'server errors' ) : flash ( 'Match Maker server returned error:{}' . format ( data [ 'server errors' ] ) , 'danger' ) return redirect ( request . referrer ) elif not data : data = { 'institute' : institute obj , 'case' : case obj } return data", "predictions": ["this method implements a setup . context . this method uses the setup to do the setup of the setup ."], "references": ["show all matchmaker matches for a given case"], "bleu": 0.05809665204409193, "rouge_l": 0.07503075030750307}
{"id": 2923, "code": "def matchmaker match ( institute id , case name , target ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) mme base url = current app . config . get ( 'MME URL' ) mme accepts = current app . config . get ( 'MME ACCEPTS' ) mme token = current app . config . get ( 'MME TOKEN' ) nodes = current app . mme nodes if not mme base url or not mme token or not mme accepts : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) match results = controllers . mme match ( case obj , target , mme base url , mme token , nodes , mme accepts ) ok responses = 0 for match results in match results : match results [ 'status code' ] == 200 ok responses += 1 if ok responses : flash ( \"Match request sent. Look for eventual matches in 'Matches' page.\" , 'info' ) else : flash ( 'An error occurred while sending match request.' , 'danger' ) return redirect ( request . referrer )", "predictions": ["iterates through the , retrieves the , and retrieves the , in the , and store the + continue ."], "references": ["starts an internal match or a match against one or all mme external nodes"], "bleu": 0.051366639095059514, "rouge_l": 0.0}
{"id": 2924, "code": "def matchmaker delete ( institute id , case name ) : user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) institute obj , case obj = institute and case ( store , institute id , case name ) mme base url = current app . config . get ( 'MME URL' ) mme token = current app . config . get ( 'MME TOKEN' ) if not mme base url or not mme token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) delete result = controllers . mme delete ( case obj , mme base url , mme token ) n deleted = 0 category = 'warning' for resp in delete result : if resp [ 'status code' ] == 200 : n deleted += 1 else : flash ( resp [ 'message' ] , category ) if n deleted : category = 'success' user obj = store . user ( current user . email ) store . case mme delete ( case obj = case obj , user obj = user obj ) flash ( 'Number of patients deleted from Matchmaker: {} out of {}' . format ( n deleted , len ( delete result ) ) , category ) return redirect ( request . referrer )", "predictions": ["deletes an obj from the store ."], "references": ["remove a case from matchmaker"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2925, "code": "def gene variants ( institute id ) : page = int ( request . form . get ( 'page' , 1 ) ) institute obj = institute and case ( store , institute id ) if ( request . method == \"POST\" ) : form = Gene Variant Filters Form ( request . form ) else : form = Gene Variant Filters Form ( request . args ) variant type = form . data . get ( 'variant type' , 'clinical' ) hgnc symbols = [ ] non clinical symbols = [ ] not found symbols = [ ] not found ids = [ ] data = { } if ( form . hgnc symbols . data ) and len ( form . hgnc symbols . data ) > 0 : is clinical = form . data . get ( 'variant type' , 'clinical' ) == 'clinical' clinical symbols = store . clinical symbols ( case obj ) if is clinical else None for hgnc symbol in form . hgnc symbols . data : if hgnc symbol . isdigit ( ) : hgnc gene = store . hgnc gene ( int ( hgnc symbol ) ) if hgnc gene is None : not found ids . append ( hgnc symbol ) else : hgnc symbols . append ( hgnc gene [ 'hgnc symbol' ] ) elif store . hgnc genes ( hgnc symbol ) . count ( ) == 0 : not found symbols . append ( hgnc symbol ) elif is clinical and ( hgnc symbol not in clinical symbols ) : non clinical symbols . append ( hgnc symbol ) else : hgnc symbols . append ( hgnc symbol ) if ( not found ids ) : flash ( \"HGNC id not found: {}\" . format ( \", \" . join ( not found ids ) ) , 'warning' ) if ( not found symbols ) : flash ( \"HGNC symbol not found: {}\" . format ( \", \" . join ( not found symbols ) ) , 'warning' ) if ( non clinical symbols ) : flash ( \"Gene not included in clinical list: {}\" . format ( \", \" . join ( non clinical symbols ) ) , 'warning' ) form . hgnc symbols . data = hgnc symbols log . debug ( \"query {}\" . format ( form . data ) ) variants query = store . gene variants ( query = form . data , category = 'snv' , variant type = variant type ) data = controllers . gene variants ( store , variants query , page ) return dict ( institute = institute obj , form = form , page = page , * * data )", "predictions": ["serializes the specified object , i . e . , the in case of ( is not possible ."], "references": ["display a list of snv variants ."], "bleu": 0.0712695567709093, "rouge_l": 0.16781292984869325}
{"id": 2926, "code": "def pdf case report ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . case report content ( store , institute obj , case obj ) if current app . config . get ( 'SQLALCHEMY DATABASE URI' ) : data [ 'coverage report' ] = controllers . coverage report contents ( store , institute obj , case obj , request . url root ) if case obj . get ( 'madeline info' ) is not None : with open ( os . path . join ( cases bp . static folder , 'madeline.svg' ) , 'w' ) as temp madeline : temp madeline . write ( case obj [ 'madeline info' ] ) html report = render template ( 'cases/case report.html' , institute = institute obj , case = case obj , format = 'pdf' , * * data ) return render pdf ( HTML ( string = html report ) , download filename = case obj [ 'display name' ] + ' ' + datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) + ' scout.pdf' )", "predictions": ["reports an event to the given object ."], "references": ["download a pdf report for a case"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2927, "code": "def case diagnosis ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( '.case' , institute id = institute id , case name = case name ) level = 'phenotype' if 'phenotype' in request . form else 'gene' omim id = request . form [ 'omim id' ] remove = True if request . args . get ( 'remove' ) == 'yes' else False store . diagnose ( institute obj , case obj , user obj , link , level = level , omim id = omim id , remove = remove ) return redirect ( request . referrer )", "predictions": ["this is called by the server when it wants to send an in the users session ."], "references": ["add or remove a diagnosis for a case ."], "bleu": 0.07223943354597204, "rouge_l": 0.0814419225634179}
{"id": 2928, "code": "def phenotypes actions ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) case url = url for ( '.case' , institute id = institute id , case name = case name ) action = request . form [ 'action' ] hpo ids = request . form . getlist ( 'hpo id' ) user obj = store . user ( current user . email ) if action == 'DELETE' : for hpo id in hpo ids : store . remove phenotype ( institute obj , case obj , user obj , case url , hpo id ) elif action == 'PHENOMIZER' : if len ( hpo ids ) == 0 : hpo ids = [ term [ 'phenotype id' ] for term in case obj . get ( 'phenotype terms' , [ ] ) ] username = current app . config [ 'PHENOMIZER USERNAME' ] password = current app . config [ 'PHENOMIZER PASSWORD' ] diseases = controllers . hpo diseases ( username , password , hpo ids ) return render template ( 'cases/diseases.html' , diseases = diseases , institute = institute obj , case = case obj ) elif action == 'GENES' : hgnc symbols = set ( ) for raw symbols in request . form . getlist ( 'genes' ) : if raw symbols : hgnc symbols . update ( raw symbol . split ( ' ' , 1 ) [ 0 ] for raw symbol in raw symbols . split ( '|' ) ) store . update dynamic gene list ( case obj , hgnc symbols = hgnc symbols ) elif action == 'GENERATE' : if len ( hpo ids ) == 0 : hpo ids = [ term [ 'phenotype id' ] for term in case obj . get ( 'phenotype terms' , [ ] ) ] results = store . generate hpo gene list ( * hpo ids ) hpo count = int ( request . form . get ( 'min match' ) or 1 ) hgnc ids = [ result [ 0 ] for result in results if result [ 1 ] >= hpo count ] store . update dynamic gene list ( case obj , hgnc ids = hgnc ids , phenotype ids = hpo ids ) return redirect ( case url )", "predictions": ["this is called when the remote static static static static static static static static static static static static static method gets called ."], "references": ["perform actions on multiple phenotypes ."], "bleu": 0.05291907393644996, "rouge_l": 0.07711757269279393}
{"id": 2929, "code": "def status ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) status = request . form . get ( 'status' , case obj [ 'status' ] ) link = url for ( '.case' , institute id = institute id , case name = case name ) if status == 'archive' : store . archive case ( institute obj , case obj , user obj , status , link ) else : store . update status ( institute obj , case obj , user obj , status , link ) return redirect ( request . referrer )", "predictions": ["retrieves an object status by the specified user_id and stores it in the database ."], "references": ["update status of a specific case ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 2930, "code": "def assign ( institute id , case name , user id = None ) : institute obj , case obj = institute and case ( store , institute id , case name ) link = url for ( '.case' , institute id = institute id , case name = case name ) if user id : user obj = store . user ( user id ) else : user obj = store . user ( current user . email ) if request . form . get ( 'action' ) == 'DELETE' : store . unassign ( institute obj , case obj , user obj , link ) else : store . assign ( institute obj , case obj , user obj , link ) return redirect ( request . referrer )", "predictions": ["assigns an object to an existing object ."], "references": ["assign and unassign a user from a case ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2931, "code": "def hpoterms ( ) : query = request . args . get ( 'query' ) if query is None : return abort ( 500 ) terms = sorted ( store . hpo terms ( query = query ) , key = itemgetter ( 'hpo number' ) ) json terms = [ { 'name' : '{} | {}' . format ( term [ ' id' ] , term [ 'description' ] ) , 'id' : term [ ' id' ] } for term in terms [ : 7 ] ] return jsonify ( json terms )", "predictions": ["get all cache from the cache ."], "references": ["search for hpo terms ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2932, "code": "def mark validation ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) validate type = request . form [ 'type' ] or None link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) store . validate ( institute obj , case obj , user obj , link , variant obj , validate type ) return redirect ( request . referrer or link )", "predictions": ["marks a validation as continue ."], "references": ["mark a variant as sanger validated ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 2933, "code": "def mark causative ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) if request . form [ 'action' ] == 'ADD' : store . mark causative ( institute obj , case obj , user obj , link , variant obj ) elif request . form [ 'action' ] == 'DELETE' : store . unmark causative ( institute obj , case obj , user obj , link , variant obj ) case url = url for ( '.case' , institute id = institute id , case name = case name ) return redirect ( case url )", "predictions": ["marks the object as a specified args ."], "references": ["mark a variant as confirmed causative ."], "bleu": 0.19070828081828378, "rouge_l": 0.26991150442477874}
{"id": 2934, "code": "def delivery report ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) if case obj . get ( 'delivery report' ) is None : return abort ( 404 ) date str = request . args . get ( 'date' ) if date str : delivery report = None analysis date = parse date ( date str ) for analysis data in case obj [ 'analyses' ] : if analysis data [ 'date' ] == analysis date : delivery report = analysis data [ 'delivery report' ] if delivery report is None : return abort ( 404 ) else : delivery report = case obj [ 'delivery report' ] out dir = os . path . dirname ( delivery report ) filename = os . path . basename ( delivery report ) return send from directory ( out dir , filename )", "predictions": ["sends an object to all the given object ."], "references": ["display delivery report ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 2935, "code": "def share ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) collaborator id = request . form [ 'collaborator' ] revoke access = 'revoke' in request . form link = url for ( '.case' , institute id = institute id , case name = case name ) if revoke access : store . unshare ( institute obj , case obj , collaborator id , user obj , link ) else : store . share ( institute obj , case obj , collaborator id , user obj , link ) return redirect ( request . referrer )", "predictions": ["releases an object to the specified logged in the specified media ."], "references": ["share a case with a different institute ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 2936, "code": "def rerun ( institute id , case name ) : sender = current app . config [ 'MAIL USERNAME' ] recipient = current app . config [ 'TICKET SYSTEM EMAIL' ] controllers . rerun ( store , mail , current user , institute id , case name , sender , recipient ) return redirect ( request . referrer )", "predictions": ["stores a new sender ."], "references": ["request a case to be rerun ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 2937, "code": "def research ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( '.case' , institute id = institute id , case name = case name ) store . open research ( institute obj , case obj , user obj , link ) return redirect ( request . referrer )", "predictions": ["this is called by the server when it wants to connect to the server . this is called by an external id ."], "references": ["open the research list for a case ."], "bleu": 0.05856458233275369, "rouge_l": 0.14136732329084586}
{"id": 2938, "code": "def default panels ( institute id , case name ) : panel ids = request . form . getlist ( 'panel ids' ) controllers . update default panels ( store , current user , institute id , case name , panel ids ) return redirect ( request . referrer )", "predictions": ["could be called by the server when a user is made from the database ."], "references": ["update default panels for a case ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 2939, "code": "def vcf2cytosure ( institute id , case name , individual id ) : ( display name , vcf2cytosure ) = controllers . vcf2cytosure ( store , institute id , case name , individual id ) outdir = os . path . abspath ( os . path . dirname ( vcf2cytosure ) ) filename = os . path . basename ( vcf2cytosure ) log . debug ( \"Attempt to deliver file {0} from dir {1}\" . format ( filename , outdir ) ) attachment filename = display name + \".vcf2cytosure.cgh\" return send from directory ( outdir , filename , attachment filename = attachment filename , as attachment = True )", "predictions": ["creates an try to write to the file ."], "references": ["download vcf2cytosure file for individual ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2940, "code": "def multiqc ( institute id , case name ) : data = controllers . multiqc ( store , institute id , case name ) if data [ 'case' ] . get ( 'multiqc' ) is None : return abort ( 404 ) out dir = os . path . abspath ( os . path . dirname ( data [ 'case' ] [ 'multiqc' ] ) ) filename = os . path . basename ( data [ 'case' ] [ 'multiqc' ] ) return send from directory ( out dir , filename )", "predictions": ["format an ( for this solution ."], "references": ["load multiqc report for the case ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2941, "code": "def clinvar submissions ( store , user id , institute id ) : submissions = list ( store . clinvar submissions ( user id , institute id ) ) return submissions", "predictions": ["load a set of submissions from the ( submissions ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["get all clinvar submissions for a user and an institute"], "bleu": 0.026594139297659906, "rouge_l": 0.03788819875776398}
{"id": 2942, "code": "def rerun ( store , mail , current user , institute id , case name , sender , recipient ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( 'cases.case' , institute id = institute id , case name = case name ) store . request rerun ( institute obj , case obj , user obj , link ) html = . format ( institute = institute obj [ 'display name' ] , case = case obj [ 'display name' ] , case id = case obj [ ' id' ] , name = user obj [ 'name' ] . encode ( ) ) msg = Message ( subject = ( \"SCOUT: request RERUN for {}\" . format ( case obj [ 'display name' ] ) ) , html = html , sender = sender , recipients = [ recipient ] , cc = [ user obj [ 'email' ] ] ) mail . send ( msg )", "predictions": ["this is called by the order to send an query to the rerun ."], "references": ["request a rerun by email ."], "bleu": 0.10511846841633776, "rouge_l": 0.21554770318021202}
{"id": 2943, "code": "def update default panels ( store , current user , institute id , case name , panel ids ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( 'cases.case' , institute id = institute id , case name = case name ) panel objs = [ store . panel ( panel id ) for panel id in panel ids ] store . update default panels ( institute obj , case obj , user obj , link , panel objs )", "predictions": ["a convenience method for updating a snowball object ."], "references": ["update default panels for a case ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 2944, "code": "def vcf2cytosure ( store , institute id , case name , individual id ) : institute obj , case obj = institute and case ( store , institute id , case name ) for individual in case obj [ 'individuals' ] : if individual [ 'individual id' ] == individual id : individual obj = individual return ( individual obj [ 'display name' ] , individual obj [ 'vcf2cytosure' ] )", "predictions": ["create attribute values for this segment ."], "references": ["vcf2cytosure cgh file for inidividual ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2945, "code": "def multiqc ( store , institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) return dict ( institute = institute obj , case = case obj , )", "predictions": ["stores object and its associated fields in this object ."], "references": ["find multiqc report for the case ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2946, "code": "def genes ( context , build , api key ) : LOG . info ( \"Running scout update genes\" ) adapter = context . obj [ 'adapter' ] api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) context . abort ( ) try : mim files = fetch mim files ( api key , mim2genes = True , morbidmap = True , genemap2 = True ) except Exception as err : LOG . warning ( err ) context . abort ( ) LOG . warning ( \"Dropping all gene information\" ) adapter . drop genes ( build ) LOG . info ( \"Genes dropped\" ) LOG . warning ( \"Dropping all transcript information\" ) adapter . drop transcripts ( build ) LOG . info ( \"transcripts dropped\" ) hpo genes = fetch hpo genes ( ) if build : builds = [ build ] else : builds = [ '37' , '38' ] hgnc lines = fetch hgnc ( ) exac lines = fetch exac constraint ( ) for build in builds : ensembl genes = fetch ensembl genes ( build = build ) hgnc genes = load hgnc genes ( adapter = adapter , ensembl lines = ensembl genes , hgnc lines = hgnc lines , exac lines = exac lines , mim2gene lines = mim files [ 'mim2genes' ] , genemap lines = mim files [ 'genemap2' ] , hpo lines = hpo genes , build = build , ) ensembl genes = { } for gene obj in hgnc genes : ensembl id = gene obj [ 'ensembl id' ] ensembl genes [ ensembl id ] = gene obj ensembl transcripts = fetch ensembl transcripts ( build = build ) transcripts = load transcripts ( adapter , ensembl transcripts , build , ensembl genes ) adapter . update indexes ( ) LOG . info ( \"Genes, transcripts and Exons loaded\" )", "predictions": ["genes the specified builds from the server ."], "references": ["load the hgnc aliases to the mongo database ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 2947, "code": "def parse cadd ( variant , transcripts ) : cadd = 0 cadd keys = [ 'CADD' , 'CADD PHRED' ] for key in cadd keys : cadd = variant . INFO . get ( key , 0 ) if cadd : return float ( cadd ) for transcript in transcripts : cadd entry = transcript . get ( 'cadd' ) if ( cadd entry and cadd entry > cadd ) : cadd = cadd entry return cadd", "predictions": ["parse all the text entries and cadd pairs ."], "references": ["check if the cadd phred score is annotated"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2948, "code": "def convert ( context , panel ) : adapter = context . obj [ 'adapter' ] new header = [ \"hgnc id\" , \"hgnc symbol\" , \"disease associated transcripts\" , \"reduced penetrance\" , \"genetic disease models\" , \"mosaicism\" , \"database entry version\" ] genes = parse genes ( panel ) adapter . add hgnc id ( genes ) click . echo ( \"#{0}\" . format ( '\\t' . join ( new header ) ) ) for gene in genes : if gene . get ( 'hgnc id' ) : print info = [ ] for head in new header : print info . append ( str ( gene [ head ] ) if gene . get ( head ) else '' ) click . echo ( '\\t' . join ( print info ) )", "predictions": ["this is the method that we do not use the ( method ."], "references": ["convert a gene panel with hgnc symbols to a new one with hgnc ids ."], "bleu": 0.08189957223751165, "rouge_l": 0.07052023121387282}
{"id": 2949, "code": "def cli ( context , morbid , genemap , mim2gene , mim titles , phenotypes ) : from scout . utils . handle import get file handle from pprint import pprint as pp print ( \"Morbid file: %s\" % morbid ) print ( \"Genemap file: %s\" % genemap ) print ( \"mim2gene file: %s\" % mim2gene ) print ( \"Mim Titles file: %s\" % mim titles ) if morbid : morbid handle = get file handle ( morbid ) if genemap : genemap handle = get file handle ( genemap ) if mim2gene : mim2gene handle = get file handle ( mim2gene ) if mim titles : mimtitles handle = get file handle ( mim titles ) mim genes = get mim genes ( genemap handle , mim2gene handle ) for entry in mim genes : if entry == 'C10orf11' : pp ( mim genes [ entry ] ) context . abort ( ) if phenotypes : if not genemap : click . echo ( \"Please provide the genemap file\" ) context . abort ( ) phenotypes = get mim phenotypes ( genemap handle ) for i , mim term in enumerate ( phenotypes ) : pass print ( \"Number of phenotypes found: %s\" % i ) context . abort ( ) genes = get mim genes ( genemap handle , mim2gene handle ) for hgnc symbol in genes : if hgnc symbol == 'OPA1' : print ( genes [ hgnc symbol ] )", "predictions": ["print out the nan values for each key"], "references": ["parse the omim files"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2950, "code": "def formatmonth ( self , theyear , themonth , withyear = True , net = None , qs = None , template = 'happenings/partials/calendar/month table.html' ) : context = self . get context ( ) context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) context [ 'week rows' ] = [ ] for week in self . monthdays2calendar ( theyear , themonth ) : week row = [ ] for day , weekday in week : week row . append ( self . formatday ( day , weekday ) ) context [ 'week rows' ] . append ( week row ) nxt , prev = get next and prev ( net ) extra qs = ( '&' + '&' . join ( qs ) ) if qs else '' context [ 'prev qs' ] = mark safe ( '?cal prev=%d%s' % ( prev , extra qs ) ) context [ 'next qs' ] = mark safe ( '?cal next=%d%s' % ( nxt , extra qs ) ) context [ 'withyear' ] = withyear return render to string ( template , context )", "predictions": ["render the client and a theyear ."], "references": ["return a formatted month as a table ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2951, "code": "def formatday ( self , day , weekday , day template = 'happenings/partials/calendar/day cell.html' , noday template = 'happenings/partials/calendar/day noday cell.html' , popover template = 'happenings/partials/calendar/popover.html' , ) : super ( Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) context = self . get context ( ) context [ 'events' ] = [ ] context [ 'day' ] = day context [ 'day url' ] = self . get day url ( day ) context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) context [ 'weekday' ] = weekday context [ 'cssclass' ] = self . cssclasses [ weekday ] context [ 'popover template' ] = popover template context [ 'num events' ] = len ( self . count . get ( day , [ ] ) ) , try : processed date = date ( self . yr , self . mo , day ) except Value Error : processed date = None context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) if day == 0 : template = noday template else : template = day template if now . date ( ) == processed date : context [ 'is current day' ] = True if processed date and ( day in self . count ) : for item in self . count [ day ] : self . pk = item [ 1 ] self . title = item [ 0 ] for event in self . events : if event . pk == self . pk : event . check if cancelled ( processed date ) context [ 'events' ] . append ( event ) return render to string ( template , context )", "predictions": ["w = weekday ( ) , \\ brief , \\ brief , \\ r , \\ ( , \\ ( , \\ ( , \\ ( , \\ ( , \\ ( , \\ ( , \\ ( , \\ ( , \\ ( , \\ ( , \\ ("], "references": ["return a day as a table cell ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 2952, "code": "def formatday ( self , day , weekday ) : return super ( Mini Event Calendar , self ) . formatday ( day , weekday , day template = 'happenings/partials/calendar/mini day cell.html' , popover template = 'happenings/partials/calendar/mini popover.html' , )", "predictions": ["get a new ( instance ."], "references": ["return a day as a table cell ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 2953, "code": "def formatday ( self , day , weekday ) : self . wkday not today = '<td class=\"%s\"><div class=\"td-inner\">' % ( self . cssclasses [ weekday ] ) self . wkday today = ( '<td class=\"%s calendar-today\"><div class=\"td-inner\">' % ( self . cssclasses [ weekday ] ) ) if URLS NAMESPACE : url name = '%s:day list' % ( URLS NAMESPACE ) else : url name = 'day list' self . day url = reverse ( url name , args = ( self . yr , self . mo , day ) ) self . day = day self . anch = '<a href=\"%s\">%d</a>' % ( self . day url , day ) self . end = '</div></td>'", "predictions": ["creates and returns the formatday for this ( ."], "references": ["set some commonly used variables ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2954, "code": "def popover helper ( self ) : display month = month name [ self . mo ] if isinstance ( display month , six . binary type ) and self . encoding : display month = display month . decode ( 'utf-8' ) self . when = ( '<p><b>When:</b> ' + display month + ' ' + str ( self . day ) + ', ' + self . event . l start date . strftime ( LEGACY CALENDAR TIME FORMAT ) . lstrip ( '0' ) + ' - ' + self . event . l end date . strftime ( LEGACY CALENDAR TIME FORMAT ) . lstrip ( '0' ) + '</p>' ) if self . event . location . exists ( ) : self . where = '<p><b>Where:</b> ' for l in self . event . location . all ( ) : self . where += l . name self . where += '</p>' else : self . where = '' self . desc = '<p><b>Description:</b> ' + self . event . description [ : 100 ] self . desc += ( '...</p>' if len ( self . event . description ) > 100 else '</p>' ) self . event url = self . event . get absolute url ( ) t = LEGACY CALENDAR TIME FORMAT if self . event . l start date . minute else LEGACY CALENDAR HOUR FORMAT self . title2 = ( self . event . l start date . strftime ( t ) . lstrip ( '0' ) + ' ' + self . title )", "predictions": ["we only construct the helper method for ( ."], "references": ["populate variables used to build popovers ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2955, "code": "def formatday ( self , day , weekday ) : super ( Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) self . day = day out = '' if day == 0 : return '<td class=\"noday\">&nbsp;</td>' elif now . month == self . mo and now . year == self . yr and day == now . day : if day in self . count : out = self . wkday today + self . anch else : return self . wkday today + self . anch + self . end elif day in self . count : out = self . wkday not today + self . anch else : return self . wkday not today + self . anch + self . end detail = \"%s%s%s<br><a href='%s'>View details</a>\" extras = ( '<div title=\"%s\" data-content=\"%s\" data-container=\"body\"' ' data-toggle=\"popover\" class=\"calendar-event\"%s>' ) common = ' style=background:%s;color:%s;' for item in self . count [ day ] : self . pk = item [ 1 ] self . title = item [ 0 ] for event in self . events : if event . pk == self . pk : self . event = event self . check if cancelled ( ) self . popover helper ( ) bg , fnt = self . event . get colors ( ) out += ( '<a class=\"event-anch\" href=\"' + self . event url + '\">' + extras % ( self . title , detail % ( self . when , self . where , self . desc , self . event url ) , common % ( bg , fnt ) ) + self . title2 + '</div></a>' ) return out + self . end", "predictions": ["formatday for formatday . save next time ."], "references": ["return a day as a table cell ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 2956, "code": "def formatday ( self , day , weekday ) : super ( Mini Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) self . day = day if day == 0 : return '<td class=\"noday\">&nbsp;</td>' elif now . month == self . mo and now . year == self . yr and day == now . day : if day in self . count : self . popover helper ( ) return self . wkday today + self . anch + self . cal event + self . end else : return self . wkday today + self . anch + self . end elif day in self . count : self . popover helper ( ) return self . wkday not today + self . anch + self . cal event + self . end else : return self . wkday not today + self . anch + self . end", "predictions": ["formatday for formatday . the event will always be between two event objects ."], "references": ["return a day as a table cell ."], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 2957, "code": "def diseases ( context ) : LOG . info ( \"Running scout view diseases\" ) adapter = context . obj [ 'adapter' ] disease objs = adapter . disease terms ( ) nr diseases = disease objs . count ( ) if nr diseases == 0 : click . echo ( \"No diseases found\" ) else : click . echo ( \"Disease\" ) for disease obj in adapter . disease terms ( ) : click . echo ( \"{0}\" . format ( disease obj [ ' id' ] ) ) LOG . info ( \"{0} diseases found\" . format ( nr diseases ) )", "predictions": ["generate a . object for all the specified view ."], "references": ["show all diseases in the database"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2958, "code": "def hpo ( context ) : LOG . info ( \"Running scout update hpo\" ) adapter = context . obj [ 'adapter' ] LOG . info ( \"Dropping HPO terms\" ) adapter . hpo term collection . drop ( ) LOG . debug ( \"HPO terms dropped\" ) load hpo terms ( adapter )", "predictions": ["hpo the object to get the hpo object"], "references": ["update the hpo terms in the database . fetch the latest release and update terms ."], "bleu": 0.0834319834185865, "rouge_l": 0.23582474226804123}
{"id": 2959, "code": "def users ( store ) : user objs = list ( store . users ( ) ) total events = store . user events ( ) . count ( ) for user obj in user objs : if user obj . get ( 'institutes' ) : user obj [ 'institutes' ] = [ store . institute ( inst id ) for inst id in user obj . get ( 'institutes' ) ] else : user obj [ 'institutes' ] = [ ] user obj [ 'events' ] = store . user events ( user obj ) . count ( ) user obj [ 'events rank' ] = event rank ( user obj [ 'events' ] ) return dict ( users = sorted ( user objs , key = lambda user : - user [ 'events' ] ) , total events = total events , )", "predictions": ["generic users to access itself ."], "references": ["display a list of all users and which institutes they belong to ."], "bleu": 0.08180282100568384, "rouge_l": 0.29611650485436897}
{"id": 2960, "code": "def render to json response ( self , context , * * kwargs ) : return Http Response ( self . convert context to json ( context ) , content type = 'application/json' , * * kwargs )", "predictions": ["returns a json representation of the request ."], "references": ["returns a json response transforming context to make the payload ."], "bleu": 0.2270229421855783, "rouge_l": 0.511744966442953}
{"id": 2961, "code": "def check for cancelled events ( self , d ) : for event in self . events : for cn in event . cancellations . all ( ) : if cn . date == d : event . title += ' (CANCELLED)'", "predictions": ["makes the event for each date in the list of events ."], "references": ["check if any events are cancelled on the given date d ."], "bleu": 0.13065113298388567, "rouge_l": 0.25}
{"id": 2962, "code": "def setup time axis ( self , t start = None , t stop = None ) : ii start , ii stop = 0 , self . n ints in file if t start : ii start = t start if t stop : ii stop = t stop n ints = ii stop - ii start t0 = self . header [ b'tstart' ] t delt = self . header [ b'tsamp' ] self . timestamps = np . arange ( 0 , n ints ) * t delt / 24. / 60. / 60 + t0 return ii start , ii stop , n ints", "predictions": [". axis ."], "references": ["setup time axis ."], "bleu": 0.5066641486392105, "rouge_l": 0.5570776255707762}
{"id": 2963, "code": "def compute lst ( self ) : if self . header [ b'telescope id' ] == 6 : self . coords = gbt coords elif self . header [ b'telescope id' ] == 4 : self . coords = parkes coords else : raise Runtime Error ( \"Currently only Parkes and GBT supported\" ) if HAS SLALIB : dut1 = 0.0 mjd = self . header [ b'tstart' ] tellong = np . deg2rad ( self . coords [ 1 ] ) last = s . sla gmst ( mjd ) - tellong + s . sla eqeqx ( mjd ) + dut1 if last < 0.0 : last = last + 2.0 * np . pi return last else : raise Runtime Error ( \"This method requires py SLALIB\" )", "predictions": ["computes a list of seeds from ( ."], "references": ["compute lst for observation"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2964, "code": "def calc extent ( self , plot f = None , plot t = None , MJD time = False ) : plot f begin = plot f [ 0 ] plot f end = plot f [ - 1 ] + ( plot f [ 1 ] - plot f [ 0 ] ) plot t begin = self . timestamps [ 0 ] plot t end = self . timestamps [ - 1 ] + ( self . timestamps [ 1 ] - self . timestamps [ 0 ] ) if MJD time : extent = ( plot f begin , plot f begin end , plot t begin , plot t end ) else : extent = ( plot f begin , plot f end , 0.0 , ( plot t end - plot t begin ) * 24. * 60. * 60 ) return extent", "predictions": ["calculate the timestamps for the plot . calculate the timestamps for each rule in the base plot ."], "references": ["setup ploting edges ."], "bleu": 0.06809398432036522, "rouge_l": 0.1026936026936027}
{"id": 2965, "code": "def closest ( xarr , val ) : idx closest = np . argmin ( np . abs ( np . array ( xarr ) - val ) ) return idx closest", "predictions": ["closest to the values of this vector ."], "references": ["return the index of the closest in xarr to value val"], "bleu": 0.13859150907108325, "rouge_l": 0.20469798657718125}
{"id": 2966, "code": "def get diff ( dio cross , feedtype , * * kwargs ) : #Get Stokes parameters, frequencies, and time sample length obs = Waterfall ( dio cross , max load = 150 ) freqs = obs . populate freqs ( ) tsamp = obs . header [ 'tsamp' ] data = obs . data obs = None I , Q , U , V = get stokes ( data , feedtype ) #Fold noise diode data I OFF , I ON = foldcal ( I , tsamp , * * kwargs ) Q OFF , Q ON = foldcal ( Q , tsamp , * * kwargs ) U OFF , U ON = foldcal ( U , tsamp , * * kwargs ) V OFF , V ON = foldcal ( V , tsamp , * * kwargs ) #Do ON-OFF subtraction Idiff = I ON - I OFF Qdiff = Q ON - Q OFF Udiff = U ON - U OFF Vdiff = V ON - V OFF return Idiff , Qdiff , Udiff , Vdiff , freqs", "predictions": ["get difference between ( and ( ."], "references": ["returns on - off for all stokes parameters given a cross_pols noise diode measurement"], "bleu": 0.057461663912368725, "rouge_l": 0.0}
{"id": 2967, "code": "def calc selection size ( self ) : #Check to see how many integrations requested n ints = self . t stop - self . t start #Check to see how many frequency channels requested n chan = ( self . f stop - self . f start ) / abs ( self . header [ b'foff' ] ) n bytes = self . n bytes selection size = int ( n ints * n chan * n bytes ) return selection size", "predictions": ["calculate neighbor size for ( and ( ."], "references": ["calculate size of data of interest ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 2968, "code": "def calc selection shape ( self ) : #Check how many integrations requested n ints = int ( self . t stop - self . t start ) #Check how many frequency channels requested n chan = int ( np . round ( ( self . f stop - self . f start ) / abs ( self . header [ b'foff' ] ) ) ) selection shape = ( n ints , int ( self . header [ b'nifs' ] ) , n chan ) return selection shape", "predictions": ["calculate number of #check for ( and ( ."], "references": ["calculate shape of data of interest ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 2969, "code": "def setup freqs ( self ) : if self . header [ b'foff' ] > 0 : self . f start = self . f begin + self . chan start idx * abs ( self . header [ b'foff' ] ) self . f stop = self . f begin + self . chan stop idx * abs ( self . header [ b'foff' ] ) else : self . f start = self . f end - self . chan stop idx * abs ( self . header [ b'foff' ] ) self . f stop = self . f end - self . chan start idx * abs ( self . header [ b'foff' ] )", "predictions": ["creates a simple setup for testing ."], "references": ["updating frequency borders from channel values"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2970, "code": "def calc n blobs ( self , blob dim ) : n blobs = int ( np . ceil ( 1.0 * np . prod ( self . selection shape ) / np . prod ( blob dim ) ) ) return n blobs", "predictions": ["method for making a square matrix to determine the blobs edges ."], "references": ["given the blob dimensions calculate how many fit in the data selection ."], "bleu": 0.10579369505074822, "rouge_l": 0.15885416666666669}
{"id": 2971, "code": "def isheavy ( self ) : selection size bytes = self . calc selection size ( ) if selection size bytes > self . MAX DATA ARRAY SIZE : return True else : return False", "predictions": ["get the required number of bytes for this operation ."], "references": ["check if the current selection is too large ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 2972, "code": "def find blob start ( self , blob dim , n blob ) : #Convert input frequencies into what their corresponding channel number would be. self . setup chans ( ) #Check which is the blob time offset blob time start = self . t start + blob dim [ self . time axis ] * n blob #Check which is the blob frequency offset (in channels) blob freq start = self . chan start idx + ( blob dim [ self . freq axis ] * n blob ) % self . selection shape [ self . freq axis ] blob start = np . array ( [ blob time start , 0 , blob freq start ] ) return blob start", "predictions": ["generate a blob of the table at a given start and end ."], "references": ["find first blob from selection ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 2973, "code": "def read blob ( self , blob dim , n blob = 0 ) : n blobs = self . calc n blobs ( blob dim ) if n blob > n blobs or n blob < 0 : raise Value Error ( 'Please provide correct n blob value. Given %i, but max values is %i' % ( n blob , n blobs ) ) #This prevents issues when the last blob is smaller than the others in time if blob dim [ self . time axis ] * ( n blob + 1 ) > self . selection shape [ self . time axis ] : updated blob dim = ( self . selection shape [ self . time axis ] - blob dim [ self . time axis ] * n blob , 1 , blob dim [ self . freq axis ] ) else : updated blob dim = [ int ( i ) for i in blob dim ] blob start = self . find blob start ( blob dim , n blob ) blob end = blob start + np . array ( updated blob dim ) blob = self . h5 [ \"data\" ] [ int ( blob start [ self . time axis ] ) : int ( blob end [ self . time axis ] ) , : , int ( blob start [ self . freq axis ] ) : int ( blob end [ self . freq axis ] ) ] return blob", "predictions": ["reads a blob from this blob ."], "references": ["read blob from a selection ."], "bleu": 0.2777619034011791, "rouge_l": 0.4680306905370844}
{"id": 2974, "code": "def find blob start ( self ) : self . setup chans ( ) blob time start = self . t start blob freq start = self . chan start idx blob start = blob time start * self . n channels in file + blob freq start return blob start", "predictions": ["find a blob for this blob ."], "references": ["find first blob from selection ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 2975, "code": "def read blob ( self , blob dim , n blob = 0 ) : n blobs = self . calc n blobs ( blob dim ) if n blob > n blobs or n blob < 0 : raise Value Error ( 'Please provide correct n blob value. Given %i, but max values is %i' % ( n blob , n blobs ) ) if blob dim [ self . time axis ] * ( n blob + 1 ) > self . selection shape [ self . time axis ] : updated blob dim = ( int ( self . selection shape [ self . time axis ] - blob dim [ self . time axis ] * n blob ) , 1 , int ( blob dim [ self . freq axis ] ) ) else : updated blob dim = [ int ( i ) for i in blob dim ] blob start = self . find blob start ( ) blob = np . zeros ( updated blob dim , dtype = self . d type ) if self . f start == self . f begin and self . f stop == self . f end : blob flat size = np . prod ( blob dim ) updated blob flat size = np . prod ( updated blob dim ) with open ( self . filename , 'rb' ) as f : f . seek ( int ( self . idx data + self . n bytes * ( blob start + n blob * blob flat size ) ) ) dd = np . fromfile ( f , count = updated blob flat size , dtype = self . d type ) if dd . shape [ 0 ] == updated blob flat size : blob = dd . reshape ( updated blob dim ) else : logger . info ( 'DD shape != blob shape.' ) blob = dd . reshape ( ( int ( dd . shape [ 0 ] / blob dim [ self . freq axis ] ) , blob dim [ self . beam axis ] , blob dim [ self . freq axis ] ) ) else : for blobt in range ( updated blob dim [ self . time axis ] ) : #Load binary data with open ( self . filename , 'rb' ) as f : f . seek ( int ( self . idx data + self . n bytes * ( blob start + n blob * blob dim [ self . time axis ] * self . n channels in file + blobt * self . n channels in file ) ) ) dd = np . fromfile ( f , count = blob dim [ self . freq axis ] , dtype = self . d type ) blob [ blobt ] = dd return blob", "predictions": ["reads stats for this blob ."], "references": ["read blob from a selection ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 2976, "code": "def read data ( self , f start = None , f stop = None , t start = None , t stop = None ) : self . container . read data ( f start = f start , f stop = f stop , t start = t start , t stop = t stop ) self . load data ( )", "predictions": ["reads ( case - insensitive obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj ."], "references": ["reads data selection if small enough ."], "bleu": 0.044644767873512764, "rouge_l": 0.1217564870259481}
{"id": 2977, "code": "def update header ( self ) : #Updating frequency of first channel from selection if self . header [ b'foff' ] < 0 : self . header [ b'fch1' ] = self . container . f stop else : self . header [ b'fch1' ] = self . container . f start #Updating number of coarse channels. self . header [ b'nchans' ] = self . container . selection shape [ self . freq axis ] #Updating time stamp for first time bin from selection self . header [ b'tstart' ] = self . container . populate timestamps ( update header = True )", "predictions": ["update note : this method is called before the ( method ."], "references": ["updates the header information from the original file to the selection ."], "bleu": 0.11498759556447223, "rouge_l": 0.16666666666666666}
{"id": 2978, "code": "def info ( self ) : print ( \"\\n--- File Info ---\" ) for key , val in self . file header . items ( ) : if key == 'src raj' : val = val . to string ( unit = u . hour , sep = ':' ) if key == 'src dej' : val = val . to string ( unit = u . deg , sep = ':' ) print ( \"%16s : %32s\" % ( key , val ) ) print ( \"\\n%16s : %32s\" % ( \"Num ints in file\" , self . n ints in file ) ) print ( \"%16s : %32s\" % ( \"File shape\" , self . file shape ) ) print ( \"--- Selection Info ---\" ) print ( \"%16s : %32s\" % ( \"Data selection shape\" , self . selection shape ) ) print ( \"%16s : %32s\" % ( \"Minimum freq (M Hz)\" , self . container . f start ) ) print ( \"%16s : %32s\" % ( \"Maximum freq (M Hz)\" , self . container . f stop ) )", "predictions": ["helper method to print to stdout ."], "references": ["print header information and other derived information ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2979, "code": "def get chunk dimensions ( self ) : #Usually '.0000.' is in self.filename if np . abs ( self . header [ b'foff' ] ) < 1e-5 : logger . info ( 'Detecting high frequency resolution data.' ) chunk dim = ( 1 , 1 , 1048576 ) #1048576 is the number of channels in a coarse channel. return chunk dim #Usually '.0001.' is in self.filename elif np . abs ( self . header [ b'tsamp' ] ) < 1e-3 : logger . info ( 'Detecting high time resolution data.' ) chunk dim = ( 2048 , 1 , 512 ) #512 is the total number of channels per single band (ie. blc00) return chunk dim #Usually '.0002.' is in self.filename elif np . abs ( self . header [ b'foff' ] ) < 1e-2 and np . abs ( self . header [ b'foff' ] ) >= 1e-5 : logger . info ( 'Detecting intermediate frequency and time resolution data.' ) chunk dim = ( 10 , 1 , 65536 ) #65536 is the total number of channels per single band (ie. blc00) return chunk dim else : logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) chunk dim = ( 1 , 1 , 512 ) return chunk dim", "predictions": ["for each chunk . this is a convenience method that makes it easier to be called from the java . lang . list ."], "references": ["sets the chunking dimmentions depending on the file type ."], "bleu": 0.05606668411195419, "rouge_l": 0.12708333333333333}
{"id": 2980, "code": "def cmd tool ( args = None ) : from argparse import Argument Parser parser = Argument Parser ( description = \"Command line utility for creating spectra from Guppi Raw files.\" ) parser . add argument ( 'filename' , type = str , help = 'Name of file to read' ) parser . add argument ( '-o' , dest = 'outdir' , type = str , default = './' , help = 'output directory for PNG files' ) args = parser . parse args ( ) r = Guppi Raw ( args . filename ) r . print stats ( ) bname = os . path . splitext ( os . path . basename ( args . filename ) ) [ 0 ] bname = os . path . join ( args . outdir , bname ) r . plot histogram ( filename = \"%s hist.png\" % bname ) r . plot spectrum ( filename = \"%s spec.png\" % bname )", "predictions": ["creates a command of command line spectrum ."], "references": ["command line tool for plotting and viewing info on guppi raw files"], "bleu": 0.12801036176909558, "rouge_l": 0.1930379746835443}
{"id": 2981, "code": "def print stats ( self ) : header , data = self . read next data block ( ) data = data . view ( 'float32' ) print ( \"AVG: %2.3f\" % data . mean ( ) ) print ( \"STD: %2.3f\" % data . std ( ) ) print ( \"MAX: %2.3f\" % data . max ( ) ) print ( \"MIN: %2.3f\" % data . min ( ) ) import pylab as plt", "predictions": ["prints the ( of this ( ."], "references": ["compute some basic stats on the next block of data"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2982, "code": "def plot histogram ( self , filename = None ) : header , data = self . read next data block ( ) data = data . view ( 'float32' ) plt . figure ( \"Histogram\" ) plt . hist ( data . flatten ( ) , 65 , facecolor = '#cc0000' ) if filename : plt . savefig ( filename ) plt . show ( )", "predictions": ["plot the qs for the given header ."], "references": ["plot a histogram of data values"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2983, "code": "def generate filterbank header ( self , nchans = 1 , ) : gp head = self . read first header ( ) fb head = { } telescope str = gp head . get ( \"TELESCOP\" , \"unknown\" ) if telescope str in ( 'GBT' , 'GREENBANK' ) : fb head [ \"telescope id\" ] = 6 elif telescope str in ( 'PKS' , 'PARKES' ) : fb head [ \"telescop id\" ] = 7 else : fb head [ \"telescop id\" ] = 0 fb head [ \"source name\" ] = gp head . get ( \"SRC NAME\" , \"unknown\" ) fb head [ \"az start\" ] = gp head . get ( \"AZ\" , 0 ) fb head [ \"za start\" ] = gp head . get ( \"ZA\" , 0 ) fb head [ \"src raj\" ] = Angle ( str ( gp head . get ( \"RA\" , 0.0 ) ) + \"hr\" ) fb head [ \"src dej\" ] = Angle ( str ( gp head . get ( \"DEC\" , 0.0 ) ) + \"deg\" ) fb head [ \"rawdatafile\" ] = self . filename fb head [ \"machine id\" ] = 20 fb head [ \"data type\" ] = 1 fb head [ \"barycentric\" ] = 0 fb head [ \"pulsarcentric\" ] = 0 fb head [ \"nbits\" ] = 32 fb head [ \"tstart\" ] = 0.0 fb head [ \"tsamp\" ] = 1.0 fb head [ \"fch1\" ] = 0.0 fb head [ \"foff\" ] = 187.5 / nchans fb head [ \"nchans\" ] = nchans fb head [ \"nifs\" ] = 1 fb head [ \"nbeams\" ] = 1 return fb head", "predictions": ["generate a ( processed ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) . ( , ( , ( . ( . ( . ( . . ."], "references": ["generate a blimpy header dictionary"], "bleu": 0.03162593967015063, "rouge_l": 0.08531468531468532}
{"id": 2984, "code": "def find header size ( filename ) : filfile = open ( filename , 'rb' ) filfile . seek ( 0 ) #read some region larger than the header. round1 = filfile . read ( 1000 ) headersize = round1 . find ( 'HEADER END' ) + len ( 'HEADER END' ) return headersize", "predictions": ["find an ( ( day weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday weekday day weekday ( weekday ( ( weekday ( ( ( , ( , ( , ( , ( , ( , ("], "references": ["script to find the header size of a filterbank file"], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 2985, "code": "def cmd tool ( args = None ) : if 'bl' in local host : header loc = '/usr/local/sigproc/bin/header' #Current location of header command in GBT. else : raise IO Error ( 'Script only able to run in BL systems.' ) p = Option Parser ( ) p . set usage ( 'matchfils <FIL FILE1> <FIL FILE2>' ) opts , args = p . parse args ( sys . argv [ 1 : ] ) file1 = args [ 0 ] file2 = args [ 1 ] #------------------------------------ #Create batch script make batch script ( ) #------------------------------------ #First checksum headersize1 = find header size ( file1 ) file size1 = os . path . getsize ( file1 ) #Strip header from file, and calculate the md5sum of the rest. #command=['tail','-c',str(file size1-headersize1),file1,'|','md5sum'] command = [ './tail sum.sh' , file1 , str ( file size1 - headersize1 ) ] print ( '[matchfils] ' + ' ' . join ( command ) ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) check sum1 = out . split ( ) [ 0 ] print ( '[matchfils] Checksum is:' , check sum1 ) if err : raise Error ( 'There is an error.' ) #--- out , err = reset outs ( ) command = [ header loc , file1 ] print ( '[matchfils] Header information:' ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) header1 = out print ( header1 ) #------------------------------------ #Second checksum out , err = reset outs ( ) headersize2 = find header size ( file2 ) file size2 = os . path . getsize ( file2 ) #Strip header from file, and calculate the md5sum of the rest. command = [ './tail sum.sh' , file2 , str ( file size2 - headersize2 ) ] print ( '[matchfils] ' + ' ' . join ( command ) ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) check sum2 = out . split ( ) [ 0 ] print ( '[matchfils] Checksum is:' , check sum2 ) if err : raise Error ( 'There is an error.' ) #--- out , err = reset outs ( ) command = [ header loc , file2 ] print ( '[matchfils] Header information:' ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) header2 = out print ( header2 ) #------------------------------------ #check the checksums if check sum1 != check sum2 : print ( '[matchfils] Booo! Checksum does not match between files.' ) else : print ( '[matchfils] Hooray! Checksum matches between files.' ) #------------------------------------ #Remove batch script os . remove ( 'tail sum.sh' )", "predictions": ["builds command line arguments for . ."], "references": ["command line tool to make a md5sum comparison of two . fil files ."], "bleu": 0.10218289380194193, "rouge_l": 0.35935198821796754}
{"id": 2986, "code": "def cmd tool ( args = None ) : from argparse import Argument Parser if not HAS BITSHUFFLE : print ( \"Error: the bitshuffle library is required to run this script.\" ) exit ( ) parser = Argument Parser ( description = \"Command line utility for creating HDF5 Raw files.\" ) parser . add argument ( 'filename' , type = str , help = 'Name of filename to read' ) args = parser . parse args ( ) fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] filelist = glob . glob ( fileroot + '*.raw' ) filelist = sorted ( filelist ) r = Guppi Raw ( filelist [ 0 ] ) header , data = r . read next data block ( ) dshape = data . shape #r.read next data block shape() print ( dshape ) n blocks total = 0 for filename in filelist : print ( filename ) r = Guppi Raw ( filename ) n blocks total += r . n blocks print ( n blocks total ) full dshape = np . concatenate ( ( ( n blocks total , ) , dshape ) ) h5 = h5py . File ( fileroot + '.h5' , 'w' ) h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' block size = 0 dset = h5 . create dataset ( 'data' , shape = full dshape , #compression=bitshuffle.h5.H5FILTER, #compression opts=(block size, bitshuffle.h5.H5 COMPRESS LZ4), dtype = data . dtype ) h5 idx = 0 for filename in filelist : print ( \"\\n Reading %s header...\" % filename ) r = Guppi Raw ( filename ) h5 = h5py . File ( filename + '.h5' , 'w' ) header , data = r . read next data block ( ) for ii in range ( 0 , r . n blocks ) : t0 = time . time ( ) print ( \"Reading block %i of %i\" % ( h5 idx + 1 , full dshape [ 0 ] ) ) header , data = r . read next data block ( ) t1 = time . time ( ) t2 = time . time ( ) print ( \"Writing block %i of %i\" % ( h5 idx + 1 , full dshape [ 0 ] ) ) dset [ h5 idx , : ] = data t3 = time . time ( ) print ( \"Read: %2.2fs, Write %2.2fs\" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) h5 idx += 1 for key , value in header . items ( ) : dset . attrs [ key ] = value h5 . close ( ) t1 = time . time ( ) print ( \"Conversion time: %2.2fs\" % ( t1 - t0 ) )", "predictions": ["creates the command line for evaluation ."], "references": ["command line tool for converting guppi raw into hdf5 versions of guppi raw"], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 2987, "code": "def is filterbank ( filename ) : with open ( filename , 'rb' ) as fh : is fil = True try : keyword , value , idx = read next header keyword ( fh ) try : assert keyword == b'HEADER START' except Assertion Error : is fil = False except Key Error : is fil = False return is fil", "predictions": ["check if the file should be read ."], "references": ["open file and confirm if it is a filterbank file or not ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 2988, "code": "def to sigproc angle ( angle val ) : x = str ( angle val ) if '.' in x : if 'h' in x : d , m , s , ss = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) if 'd' in x : d , m , s , ss = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) else : if 'h' in x : d , m , s = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) if 'd' in x : d , m , s = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) ss = 0 num = str ( d ) . zfill ( 2 ) + str ( m ) . zfill ( 2 ) + str ( s ) . zfill ( 2 ) + '.' + str ( ss ) . split ( \".\" ) [ - 1 ] return np . float64 ( num ) . tostring ( )", "predictions": ["encode a point in the canonical form of a 2d or offset ."], "references": ["convert an astropy . angle to the ridiculous sigproc angle format string ."], "bleu": 0.10571070857151538, "rouge_l": 0.15384615384615383}
{"id": 2989, "code": "def calc n ints in file ( filename ) : h = read header ( filename ) n bytes = int ( h [ b'nbits' ] / 8 ) n chans = h [ b'nchans' ] n ifs = h [ b'nifs' ] idx data = len header ( filename ) f = open ( filename , 'rb' ) f . seek ( idx data ) filesize = os . path . getsize ( filename ) n bytes data = filesize - idx data if h [ b'nbits' ] == 2 : n ints = int ( 4 * n bytes data / ( n chans * n ifs ) ) else : n ints = int ( n bytes data / ( n bytes * n chans * n ifs ) ) return n ints", "predictions": ["calculates note : this method should be used for plotting the ( ."], "references": ["calculate number of integrations in a given file"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2990, "code": "def to dict ( self ) : if self . tb next is None : tb next = None else : tb next = self . tb next . to dict ( ) code = { 'co filename' : self . tb frame . f code . co filename , 'co name' : self . tb frame . f code . co name , } frame = { 'f globals' : self . tb frame . f globals , 'f code' : code , } return { 'tb frame' : frame , 'tb lineno' : self . tb lineno , 'tb next' : tb next , }", "predictions": ["subroutine to create a ( ( ( frame : : ( : : : : : : : : : : : : ( : : : : / / docs . tools . com / docs / . / . / . / . / . . html ."], "references": ["convert a traceback into a dictionary representation"], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 2991, "code": "def make rr subparser ( subparsers , rec type , args and types ) : sp = subparsers . add parser ( rec type ) sp . add argument ( \"name\" , type = str ) sp . add argument ( \"ttl\" , type = int , nargs = '?' ) sp . add argument ( rec type , type = str ) for my spec in args and types : ( argname , argtype ) = my spec [ : 2 ] if len ( my spec ) > 2 : nargs = my spec [ 2 ] sp . add argument ( argname , type = argtype , nargs = nargs ) else : sp . add argument ( argname , type = argtype ) return sp", "predictions": ["users have no effect of ( ."], "references": ["make a subparser for a given type of dns record"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2992, "code": "def make parser ( ) : line parser = Zonefile Line Parser ( ) subparsers = line parser . add subparsers ( ) sp = subparsers . add parser ( \"$ORIGIN\" ) sp . add argument ( \"$ORIGIN\" , type = str ) sp = subparsers . add parser ( \"$TTL\" ) sp . add argument ( \"$TTL\" , type = int ) args and types = [ ( \"mname\" , str ) , ( \"rname\" , str ) , ( \"serial\" , int ) , ( \"refresh\" , int ) , ( \"retry\" , int ) , ( \"expire\" , int ) , ( \"minimum\" , int ) ] make rr subparser ( subparsers , \"SOA\" , args and types ) make rr subparser ( subparsers , \"NS\" , [ ( \"host\" , str ) ] ) make rr subparser ( subparsers , \"A\" , [ ( \"ip\" , str ) ] ) make rr subparser ( subparsers , \"AAAA\" , [ ( \"ip\" , str ) ] ) make rr subparser ( subparsers , \"CNAME\" , [ ( \"alias\" , str ) ] ) make rr subparser ( subparsers , \"ALIAS\" , [ ( \"host\" , str ) ] ) make rr subparser ( subparsers , \"MX\" , [ ( \"preference\" , str ) , ( \"host\" , str ) ] ) make txt subparser ( subparsers ) make rr subparser ( subparsers , \"PTR\" , [ ( \"host\" , str ) ] ) make rr subparser ( subparsers , \"SRV\" , [ ( \"priority\" , int ) , ( \"weight\" , int ) , ( \"port\" , int ) , ( \"target\" , str ) ] ) make rr subparser ( subparsers , \"SPF\" , [ ( \"data\" , str ) ] ) make rr subparser ( subparsers , \"URI\" , [ ( \"priority\" , int ) , ( \"weight\" , int ) , ( \"target\" , str ) ] ) return line parser", "predictions": ["creates and initializes ( ."], "references": ["make an argumentparser that accepts dns rrs"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 2993, "code": "def remove comments ( text ) : ret = [ ] lines = text . split ( \"\\n\" ) for line in lines : if len ( line ) == 0 : continue line = serialize ( tokenize line ( line ) ) ret . append ( line ) return \"\\n\" . join ( ret )", "predictions": ["check for any text in the printstream ."], "references": ["remove comments from a zonefile"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2994, "code": "def parse zone file ( text , ignore invalid = False ) : text = remove comments ( text ) text = flatten ( text ) text = remove class ( text ) text = add default name ( text ) json zone file = parse lines ( text , ignore invalid = ignore invalid ) return json zone file", "predictions": ["setup file in the name of the file ."], "references": ["parse a zonefile into a dict"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2995, "code": "def process origin ( data , template ) : record = \"\" if data is not None : record += \"$ORIGIN %s\" % data return template . replace ( \"{$origin}\" , record )", "predictions": ["processes the self - terminated header ."], "references": ["replace { $origin } in template with a serialized $origin record"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 2996, "code": "def process ttl ( data , template ) : record = \"\" if data is not None : record += \"$TTL %s\" % data return template . replace ( \"{$ttl}\" , record )", "predictions": ["does the actual work of the self - terminated operation ."], "references": ["replace { $ttl } in template with a serialized $ttl record"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2997, "code": "def process soa ( data , template ) : record = template [ : ] if data is not None : assert len ( data ) == 1 , \"Only support one SOA RR at this time\" data = data [ 0 ] soadat = [ ] domain fields = [ 'mname' , 'rname' ] param fields = [ 'serial' , 'refresh' , 'retry' , 'expire' , 'minimum' ] for f in domain fields + param fields : assert f in data . keys ( ) , \"Missing '%s' (%s)\" % ( f , data ) data name = str ( data . get ( 'name' , '@' ) ) soadat . append ( data name ) if data . get ( 'ttl' ) is not None : soadat . append ( str ( data [ 'ttl' ] ) ) soadat . append ( \"IN\" ) soadat . append ( \"SOA\" ) for key in domain fields : value = str ( data [ key ] ) soadat . append ( value ) soadat . append ( \"(\" ) for key in param fields : value = str ( data [ key ] ) soadat . append ( value ) soadat . append ( \")\" ) soa txt = \" \" . join ( soadat ) record = record . replace ( \"{soa}\" , soa txt ) else : record = record . replace ( \"{soa}\" , \"\" ) return record", "predictions": ["this is the method for . ."], "references": ["replace { soa } in template with a set of serialized soa records"], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 2998, "code": "def process txt ( data , template ) : if data is None : to process = None else : to process = copy . deepcopy ( data ) for datum in to process : if isinstance ( datum [ \"txt\" ] , list ) : datum [ \"txt\" ] = \" \" . join ( [ '\"%s\"' % entry . replace ( \";\" , \"\\;\" ) for entry in datum [ \"txt\" ] ] ) else : datum [ \"txt\" ] = '\"%s\"' % datum [ \"txt\" ] . replace ( \";\" , \"\\;\" ) return process rr ( to process , \"TXT\" , \"txt\" , \"{txt}\" , template )", "predictions": ["this method does the actual work of this class ."], "references": ["replace { txt } in template with the serialized txt records"], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 2999, "code": "def parse schema string ( schema string ) : if isinstance ( schema string , str ) : schema string = schema string . decode ( \"utf8\" ) schema struct = json . loads ( schema string ) return Avro Schema Parser ( ) . parse schema struct ( schema struct )", "predictions": ["calc a size value from a size ."], "references": ["load and return a pyschema class from an avsc string"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3000, "code": "def to json compatible ( record ) : d = { } for fname , f in record . fields . iteritems ( ) : val = getattr ( record , fname ) if val is not None : d [ fname ] = f . dump ( val ) return d", "predictions": ["dumps this dictionary to a selection"], "references": ["dump record in json - encodable object format"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 3001, "code": "def from json compatible ( schema , dct ) : kwargs = { } for key in dct : field type = schema . fields . get ( key ) if field type is None : raise Parse Error ( \"Unexpected field encountered in line for record %s: %s\" % ( schema . name , key ) ) kwargs [ key ] = field type . load ( dct [ key ] ) return schema ( * * kwargs )", "predictions": ["read a freqs from a string ."], "references": ["load from json - encodable"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3002, "code": "def from json compatible ( schema , dct ) : kwargs = { } for key in dct : field type = schema . fields . get ( key ) if field type is None : warnings . warn ( \"Unexpected field encountered in line for record %s: %r\" % ( schema . name , key ) ) continue kwargs [ key ] = field type . avro load ( dct [ key ] ) return schema ( * * kwargs )", "predictions": ["read self data from a n - bit self - parsed self - parsed self - parsed self - based self - parsed self - parsed self - parsed self - dict ."], "references": ["load from json - encodable"], "bleu": 0.0405185766962521, "rouge_l": 0.12139303482587065}
{"id": 3003, "code": "def all include attributes ( self , attributes ) : self . reload ( expand = True , attributes = attributes ) entities = [ Entity ( self , r , attributes = attributes ) for r in self . resources ] self . reload ( ) return entities", "predictions": ["all an entity repeatedly ."], "references": ["returns all entities present in the collection with attributes included ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 3004, "code": "def give another quote ( q ) : for qc in QUOTES : if qc != q : return qc else : raise Value Error ( u'Could not find a different quote for {}' . format ( q ) )", "predictions": ["into a histogram of the form \" suffixes \" ."], "references": ["when you pass a quote character returns you an another one if possible"], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 3005, "code": "def parse Command Line Arguments ( ) : parser = argparse . Argument Parser ( description = \"Plot predicted Gaia sky averaged proper motion errors as a function of V\" ) parser . add argument ( \"-p\" , action = \"store true\" , dest = \"pdf Output\" , help = \"Make PDF plot\" ) parser . add argument ( \"-b\" , action = \"store true\" , dest = \"png Output\" , help = \"Make PNG plot\" ) parser . add argument ( \"-g\" , action = \"store true\" , dest = \"gmag Abscissa\" , help = \"Plot performance vs G instead of V\" ) args = vars ( parser . parse args ( ) ) return args", "predictions": ["parses input arguments to be written into the command line ."], "references": ["set up command line parsing ."], "bleu": 0.16108992769687397, "rouge_l": 0.3727087576374745}
{"id": 3006, "code": "def parse Command Line Arguments ( ) : parser = argparse . Argument Parser ( description = \"Calculate parallax error for given G and (V-I)\" ) parser . add argument ( \"gmag\" , help = \"G-band magnitude of source\" , type = float ) parser . add argument ( \"vmini\" , help = \"(V-I) colour of source\" , type = float ) args = vars ( parser . parse args ( ) ) return args", "predictions": ["parses command line arguments ."], "references": ["set up command line parsing ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 3007, "code": "def uniquote ( value ) : if isinstance ( value , six . binary type ) : try : value = value . decode ( 'utf-8' ) except Unicode Decode Error : value = six . text type ( dequote ( repr ( value ) ) ) result = six . text type ( value ) if isinstance ( value , six . text type ) : result = \"'%s'\" % result return result", "predictions": ["read a single variable from the specified byte array ."], "references": ["convert to unicode and add quotes if initially a string"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 3008, "code": "def serach path ( ) : operating system = get os ( ) return [ os . path . expanduser ( \"~/.kerncraft/iaca/{}/\" . format ( operating system ) ) , os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/iaca/{}/' . format ( operating system ) ]", "predictions": ["format the operating system in a directory ."], "references": ["return potential locations of iaca installation ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3009, "code": "def build minimal runs ( events ) : events = [ e for i , e in enumerate ( events ) if events . index ( e ) == i ] scheduled runs = { } scheduled events = [ ] cur run = 0 while len ( scheduled events ) != len ( events ) : for event tpl in events : event , registers , parameters = event tpl if event tpl in scheduled events : continue for possible reg in register options ( registers ) : s = scheduled runs . setdefault ( cur run , { } ) if possible reg not in s : s [ possible reg ] = ( event , possible reg , parameters ) scheduled events . append ( event tpl ) break cur run += 1 runs = [ list ( v . values ( ) ) for v in scheduled runs . values ( ) ] return runs", "predictions": ["runs all events occurring . this method must be called to runs the event dispatch thread ."], "references": ["compile list of minimal runs for given events ."], "bleu": 0.0859076483566362, "rouge_l": 0.2443257676902537}
{"id": 3010, "code": "def report ( self , output file = sys . stdout ) : max perf = self . results [ 'max perf' ] if self . args and self . args . verbose >= 3 : print ( '{}' . format ( pformat ( self . results ) ) , file = output file ) if self . args and self . args . verbose >= 1 : print ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output file ) print ( 'Bottlenecks:' , file = output file ) print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output file ) print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output file ) print ( '    CPU |              | {!s:>15} |                   |' . format ( max perf [ self . args . unit ] ) , file = output file ) for b in self . results [ 'mem bottlenecks' ] : print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . args . unit ] , * * b ) , file = output file ) print ( '' , file = output file ) if self . results [ 'min performance' ] [ 'FLOP/s' ] > max perf [ 'FLOP/s' ] : print ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max perf ) , file = output file ) else : print ( 'Cache or mem bound.' , file = output file ) bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output file ) print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output file )", "predictions": ["report an example object ."], "references": ["report analysis outcome in human readable form ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3011, "code": "def analyze ( self ) : self . results = self . calculate cache access ( ) try : iaca analysis , asm block = self . kernel . iaca analysis ( micro architecture = self . machine [ 'micro-architecture' ] , asm block = self . asm block , pointer increment = self . pointer increment , verbose = self . verbose > 2 ) except Runtime Error as e : print ( \"IACA analysis failed: \" + str ( e ) ) sys . exit ( 1 ) block throughput = iaca analysis [ 'throughput' ] uops = iaca analysis [ 'uops' ] iaca output = iaca analysis [ 'output' ] port cycles = iaca analysis [ 'port cycles' ] elements per block = abs ( asm block [ 'pointer increment' ] / self . kernel . datatypes size [ self . kernel . datatype ] ) block size = elements per block * self . kernel . datatypes size [ self . kernel . datatype ] try : block to cl ratio = float ( self . machine [ 'cacheline size' ] ) / block size except Zero Division Error as e : print ( \"Too small block size / pointer increment:\" , e , file = sys . stderr ) sys . exit ( 1 ) port cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block to cl ratio ) for i in list ( port cycles . items ( ) ) ] ) uops = uops * block to cl ratio cl throughput = block throughput * block to cl ratio flops per element = sum ( self . kernel . flops . values ( ) ) self . results [ 'mem bottlenecks' ] [ 0 ] = None self . results [ 'min performance' ] = self . conv perf ( Prefixed Unit ( float ( 'inf' ) , 'FLOP/s' ) ) self . results [ 'bottleneck level' ] = None for level , bottleneck in enumerate ( self . results [ 'mem bottlenecks' ] ) : if level == 0 : continue if bottleneck [ 'performance' ] [ 'FLOP/s' ] < self . results [ 'min performance' ] [ 'FLOP/s' ] : self . results [ 'bottleneck level' ] = level self . results [ 'min performance' ] = bottleneck [ 'performance' ] self . results . update ( { 'cpu bottleneck' : { 'port cycles' : port cycles , 'cl throughput' : cl throughput , 'uops' : uops , 'performance throughput' : self . conv perf ( Prefixed Unit ( self . machine [ 'clock' ] / block throughput * elements per block * flops per element * self . cores , \"FLOP/s\" ) ) , 'IACA output' : iaca output } } )", "predictions": ["analyzes and logs for the thread ."], "references": ["run complete analysis ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3012, "code": "def report ( self , output file = sys . stdout ) : cpu perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] if self . verbose >= 3 : print ( '{}' . format ( pformat ( self . results ) ) , file = output file ) if self . verbose >= 1 : print ( 'Bottlenecks:' , file = output file ) print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output file ) print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output file ) print ( '    CPU |              | {!s:>15} |                   |' . format ( cpu perf [ self . args . unit ] ) , file = output file ) for b in self . results [ 'mem bottlenecks' ] : if b is None : continue print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . args . unit ] , * * b ) , file = output file ) print ( '' , file = output file ) print ( 'IACA analisys:' , file = output file ) print ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output file ) if self . results [ 'min performance' ] [ 'FLOP/s' ] > cpu perf [ 'FLOP/s' ] : print ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu perf [ self . args . unit ] ) , file = output file ) else : print ( 'Cache or mem bound.' , file = output file ) bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output file ) print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output file )", "predictions": ["report the due to the output ."], "references": ["print human readable report of model ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3013, "code": "def analyze ( self ) : loop stack = list ( self . kernel . get loop stack ( ) ) if any ( [ l [ 'increment' ] != 1 for l in loop stack ] ) : raise Value Error ( \"Can not apply layer condition, since not all loops are of step \" \"length 1.\" ) for aref in list ( self . kernel . index order ( ) ) : while aref and len ( aref [ 0 ] ) == 0 : aref . pop ( 0 ) for i , idx names in enumerate ( aref ) : if i >= len ( loop stack ) or any ( [ loop stack [ i ] [ 'index' ] != idx . name for idx in idx names ] ) : raise Value Error ( \"Can not apply layer condition, order of indices in array \" \"does not follow order of loop indices. Single-dimension is \" \"currently not supported.\" ) for arefs in chain ( chain ( * self . kernel . sources . values ( ) ) , chain ( * self . kernel . destinations . values ( ) ) ) : if not arefs : continue while arefs and not arefs [ 0 ] . free symbols : arefs = arefs [ 1 : ] for i , expr in enumerate ( arefs ) : diff = sympy . diff ( expr , sympy . Symbol ( loop stack [ i ] [ 'index' ] ) ) if diff != 0 and diff != 1 : raise Value Error ( \"Can not apply layer condition, array references may not \" \"increment more then one per iteration.\" ) self . results = self . calculate cache access ( )", "predictions": ["analyzes only files whose bounds have been input and returns them ."], "references": ["run complete analysis ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 3014, "code": "def report ( self , output file = sys . stdout ) : if self . args and self . args . verbose > 2 : pprint ( self . results ) for dimension , lc info in self . results [ 'dimensions' ] . items ( ) : print ( \"{}D layer condition:\" . format ( dimension ) , file = output file ) for cache , lc solution in sorted ( lc info [ 'caches' ] . items ( ) ) : print ( cache + \": \" , end = '' , file = output file ) if lc solution [ 'lt' ] is sympy . true : print ( \"unconditionally fulfilled\" , file = output file ) else : if lc solution [ 'eq' ] is None : print ( \"{}\" . format ( lc solution [ 'lt' ] ) , file = output file ) elif type ( lc solution [ 'eq' ] ) is not list : print ( \"{}\" . format ( lc solution [ 'eq' ] ) , file = output file ) else : for solu in lc solution [ 'eq' ] : for s , v in solu . items ( ) : print ( \"{} <= {}\" . format ( s , v ) , file = output file )", "predictions": ["report all the ( contents ."], "references": ["report generated model in human readable form ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3015, "code": "def round to next ( x , base ) : return int ( base * math . ceil ( float ( x ) / base ) )", "predictions": ["round the internal ( to the next number ."], "references": ["round float to next multiple of base ."], "bleu": 0.17747405280050263, "rouge_l": 0.4756335282651072}
{"id": 3016, "code": "def calculate cache access ( self ) : self . results . update ( { 'cycles' : [ ] , 'misses' : self . predictor . get misses ( ) , 'hits' : self . predictor . get hits ( ) , 'evicts' : self . predictor . get evicts ( ) , 'verbose infos' : self . predictor . get infos ( ) } )", "predictions": ["calculate and return a cache of this object for processing ."], "references": ["dispatch to cache predictor to get cache stats ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 3017, "code": "def analyze ( self ) : self . calculate cache access ( ) self . calculate cycles ( ) self . results [ 'flops per iteration' ] = sum ( self . kernel . flops . values ( ) ) return self . results", "predictions": ["subtracts a kernel from this notebook ."], "references": ["run complete anaylysis and return results ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3018, "code": "def report ( self , output file = sys . stdout ) : if self . verbose > 1 : print ( '{}' . format ( pprint . pformat ( self . results [ 'verbose infos' ] ) ) , file = output file ) for level , cycles in self . results [ 'cycles' ] : print ( '{} = {}' . format ( level , self . conv cy ( cycles ) [ self . args . unit ] ) , file = output file ) if self . verbose > 1 : if 'memory bandwidth kernel' in self . results : print ( 'memory cycles based on {} kernel with {}' . format ( self . results [ 'memory bandwidth kernel' ] , self . results [ 'memory bandwidth' ] ) , file = output file ) if self . verbose > 1 : print ( file = output file ) print ( self . report data transfers ( ) , file = output file )", "predictions": ["report all the headers to the provided output ."], "references": ["print generated model data in human readable format ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3019, "code": "def analyze ( self ) : try : incore analysis , asm block = self . kernel . iaca analysis ( micro architecture = self . machine [ 'micro-architecture' ] , asm block = self . asm block , pointer increment = self . pointer increment , verbose = self . verbose > 2 ) except Runtime Error as e : print ( \"IACA analysis failed: \" + str ( e ) ) sys . exit ( 1 ) block throughput = incore analysis [ 'throughput' ] port cycles = incore analysis [ 'port cycles' ] uops = incore analysis [ 'uops' ] elements per block = abs ( asm block [ 'pointer increment' ] // self . kernel . datatypes size [ self . kernel . datatype ] ) block size = elements per block * self . kernel . datatypes size [ self . kernel . datatype ] try : block to cl ratio = float ( self . machine [ 'cacheline size' ] ) / block size except Zero Division Error as e : print ( \"Too small block size / pointer increment:\" , e , file = sys . stderr ) sys . exit ( 1 ) port cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block to cl ratio ) for i in list ( port cycles . items ( ) ) ] ) uops = uops * block to cl ratio cl throughput = block throughput * block to cl ratio T OL = max ( [ v for k , v in list ( port cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) T n OL = max ( [ v for k , v in list ( port cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) if T n OL < cl throughput : T OL = cl throughput self . results = { 'port cycles' : port cycles , 'cl throughput' : self . conv cy ( cl throughput ) , 'uops' : uops , 'T n OL' : T n OL , 'T OL' : T OL , 'IACA output' : incore analysis [ 'output' ] , 'elements per block' : elements per block , 'pointer increment' : asm block [ 'pointer increment' ] , 'flops per iteration' : sum ( self . kernel . flops . values ( ) ) } return self . results", "predictions": ["analyzes only the thread and builds the pixel data"], "references": ["run complete analysis and return results ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3020, "code": "def report ( self , output file = sys . stdout ) : if self . verbose > 2 : print ( \"IACA Output:\" , file = output file ) print ( self . results [ 'IACA output' ] , file = output file ) print ( '' , file = output file ) if self . verbose > 1 : print ( 'Detected pointer increment: {}' . format ( self . results [ 'pointer increment' ] ) , file = output file ) print ( 'Derived elements stored to per asm block iteration: {}' . format ( self . results [ 'elements per block' ] ) , file = output file ) print ( 'Ports and cycles:' , str ( self . results [ 'port cycles' ] ) , file = output file ) print ( 'Uops:' , str ( self . results [ 'uops' ] ) , file = output file ) print ( 'Throughput: {}' . format ( self . results [ 'cl throughput' ] [ self . args . unit ] ) , file = output file ) print ( 'T n OL = {:.1f} cy/CL' . format ( self . results [ 'T n OL' ] ) , file = output file ) print ( 'T OL = {:.1f} cy/CL' . format ( self . results [ 'T OL' ] ) , file = output file )", "predictions": ["generate and writes all processed ( files ."], "references": ["print generated model data in human readable format ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3021, "code": "def strip and uncomment ( asm lines ) : asm stripped = [ ] for line in asm lines : asm stripped . append ( line . split ( '#' ) [ 0 ] . strip ( ) ) return asm stripped", "predictions": ["strips lines from source ."], "references": ["strip whitespaces and comments from asm lines ."], "bleu": 0.1781815298791261, "rouge_l": 0.2953995157384988}
{"id": 3022, "code": "def strip unreferenced labels ( asm lines ) : asm stripped = [ ] for line in asm lines : if re . match ( r'^\\S+:' , line ) : label = line [ 0 : line . find ( ':' ) ] if not any ( [ re . match ( r'^[^#]*\\s' + re . escape ( label ) + '[\\s,]?.*$' , l ) for l in asm lines ] ) : line = '' asm stripped . append ( line ) return asm stripped", "predictions": ["strip labels and other leading whitespace ."], "references": ["strip all labels which are never referenced ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 3023, "code": "def select best block ( blocks ) : if not blocks : raise Value Error ( \"No suitable blocks were found in assembly.\" ) best block = max ( blocks , key = lambda b : b [ 1 ] [ 'packed instr' ] ) if best block [ 1 ] [ 'packed instr' ] == 0 : best block = max ( blocks , key = lambda b : ( b [ 1 ] [ 'ops' ] + b [ 1 ] [ 'packed instr' ] + b [ 1 ] [ 'avx instr' ] , b [ 1 ] [ 'ZMM' ] , b [ 1 ] [ 'YMM' ] , b [ 1 ] [ 'XMM' ] ) ) return best block [ 0 ]", "predictions": ["selects a best instance of the most recently added block ."], "references": ["return best block selected based on simple heuristic ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 3024, "code": "def userselect increment ( block ) : print ( \"Selected block:\" ) print ( '\\n    ' + ( '\\n    ' . join ( block [ 'lines' ] ) ) ) print ( ) increment = None while increment is None : increment = input ( \"Choose store pointer increment (number of bytes): \" ) try : increment = int ( increment ) except Value Error : increment = None block [ 'pointer increment' ] = increment return increment", "predictions": ["increments the flight assignment for the specified block and increments the flight label ."], "references": ["let user interactively select byte increment ."], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 3025, "code": "def userselect block ( blocks , default = None , debug = False ) : print ( \"Blocks found in assembly file:\" ) print ( \"      block     | O Ps | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\" \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\" ) for idx , b in blocks : print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed instr]:>4} | {b[avx instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer increment]!s:>5} |' . format ( idx , b = b ) ) if debug : ln = b [ 'first line' ] print ( ' ' * 4 + 'Code:' ) for l in b [ 'lines' ] : print ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) ln += 1 print ( ' ' * 4 + 'Metadata:' ) print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) block idx = - 1 while not ( 0 <= block idx < len ( blocks ) ) : block idx = input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default try : block idx = int ( block idx ) except Value Error : block idx = - 1 return block idx", "predictions": ["normal userselect ( coefficients ."], "references": ["let user interactively select block ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3026, "code": "def insert markers ( asm lines , start line , end line ) : asm lines = ( asm lines [ : start line ] + START MARKER + asm lines [ start line : end line + 1 ] + END MARKER + asm lines [ end line + 1 : ] ) return asm lines", "predictions": ["insert markers into specified line ."], "references": ["insert iaca marker into list of asm instructions at given indices ."], "bleu": 0.09663861439684919, "rouge_l": 0.31443298969072164}
{"id": 3027, "code": "def main ( ) : parser = argparse . Argument Parser ( description = 'Find and analyze basic loop blocks and mark for IACA.' , epilog = '/RRZE-HPC/kerncraft\\n License: AGP Lv3' ) parser . add argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( version ) ) parser . add argument ( 'source' , type = argparse . File Type ( ) , nargs = '?' , default = sys . stdin , help = 'assembly file to analyze (default: stdin)' ) parser . add argument ( '--outfile' , '-o' , type = argparse . File Type ( 'w' ) , nargs = '?' , default = sys . stdout , help = 'output file location (default: stdout)' ) parser . add argument ( '--debug' , action = 'store true' , help = 'Output nternal analysis information for debugging.' ) args = parser . parse args ( ) iaca instrumentation ( input file = args . source , output file = args . outfile , block selection = 'manual' , pointer increment = 1 , debug = args . debug )", "predictions": ["creates and prepares the main script ."], "references": ["execute command line interface ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3028, "code": "def simulate ( kernel , model , define dict , blocking constant , blocking length ) : kernel . clear state ( ) for k , v in define dict . items ( ) : kernel . set constant ( k , v ) kernel . set constant ( blocking constant , blocking length ) model . analyze ( ) return sum ( [ cy for dscr , cy in model . results [ 'cycles' ] ] )", "predictions": ["simulates the colors in a kernel ."], "references": ["setup and execute model with given blocking length"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 3029, "code": "def get last modified datetime ( dir path = os . path . dirname ( file ) ) : max mtime = 0 for root , dirs , files in os . walk ( dir path ) : for f in files : p = os . path . join ( root , f ) try : max mtime = max ( max mtime , os . stat ( p ) . st mtime ) except File Not Found Error : pass return datetime . utcfromtimestamp ( max mtime )", "predictions": ["returns the last modification time of this file ."], "references": ["return datetime object of latest change in kerncraft module directory ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 3030, "code": "def create parser ( ) : parser = argparse . Argument Parser ( description = 'Analytical performance modelling and benchmarking toolkit.' , epilog = '/RRZE-HPC/kerncraft\\n License: AGP Lv3' ) parser . add argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( version ) ) parser . add argument ( '--machine' , '-m' , type = argparse . File Type ( 'r' ) , required = True , help = 'Path to machine description yaml file.' ) parser . add argument ( '--pmodel' , '-p' , choices = models . all , required = True , action = 'append' , default = [ ] , help = 'Performance model to apply' ) parser . add argument ( '-D' , '--define' , nargs = 2 , metavar = ( 'KEY' , 'VALUE' ) , default = [ ] , action = Append String Range , help = 'Define constant to be used in C code. Values must be integer or ' 'match start-stop[:num[log[base]]]. If range is given, all ' 'permutation s will be tested. Overwrites constants from testcase ' 'file.' ) parser . add argument ( '--verbose' , '-v' , action = 'count' , default = 0 , help = 'Increases verbosity level.' ) parser . add argument ( 'code file' , metavar = 'FILE' , type = argparse . File Type ( ) , help = 'File with loop kernel C code' ) parser . add argument ( '--asm-block' , metavar = 'BLOCK' , default = 'auto' , help = 'Number of ASM block to mark for IACA, \"auto\" for automatic ' 'selection or \"manual\" for interactiv selection.' ) parser . add argument ( '--pointer-increment' , metavar = 'INCR' , default = 'auto' , type = int or str , help = 'Increment of store pointer within one ASM block in bytes. If \"auto\": ' 'automatic detection, error on failure to detect, if ' '\"auto with manual fallback\": fallback to manual input, or if ' '\"manual\": always prompt user.' ) parser . add argument ( '--store' , metavar = 'PICKLE' , type = argparse . File Type ( 'a+b' ) , help = 'Addes results to PICKLE file for later processing.' ) parser . add argument ( '--unit' , '-u' , choices = [ 'cy/CL' , 'cy/It' , 'It/s' , 'FLOP/s' ] , help = 'Select the output unit, defaults to model specific if not given.' ) parser . add argument ( '--cores' , '-c' , metavar = 'CORES' , type = int , default = 1 , help = 'Number of cores to be used in parallel. (default: 1) ' 'ECM model will consider the scaling of the last level cache and ' 'predict the overall performance in addition to single-core behavior. ' 'The benchmark mode will run the code with Open MP on as many physical ' 'cores.' ) parser . add argument ( '--kernel-description' , action = 'store true' , help = 'Use kernel description instead of analyzing the kernel code.' ) parser . add argument ( '--clean-intermediates' , action = 'store true' , help = 'If set, will delete all intermediate files after completion.' ) parser . add argument ( '--cache-predictor' , '-P' , choices = [ 'LC' , 'SIM' ] , default = 'SIM' , help = 'Change cache predictor to use, options are LC (layer conditions) and ' 'SIM (cache simulation with pycachesim), default is SIM.' ) parser . add argument ( '--compiler' , '-C' , type = str , default = None , help = 'Compiler to use, default is first in machine description file.' ) parser . add argument ( '--compiler-flags' , type = str , default = None , help = 'Compiler flags to use. If not set, flags are taken from machine ' 'description file (-std=c99 is always added).' ) for m in models . all : ag = parser . add argument group ( 'arguments for ' + m + ' model' , getattr ( models , m ) . name ) getattr ( models , m ) . configure arggroup ( ag ) return parser", "predictions": ["creates a new parser that can be built from workers ."], "references": ["return argparse parser ."], "bleu": 0.12605968092174913, "rouge_l": 0.2911694510739857}
{"id": 3031, "code": "def check arguments ( args , parser ) : if args . asm block not in [ 'auto' , 'manual' ] : try : args . asm block = int ( args . asm block ) except Value Error : parser . error ( '--asm-block can only be \"auto\", \"manual\" or an integer' ) if not args . unit : if 'Roofline' in args . pmodel or 'Roofline IACA' in args . pmodel : args . unit = 'FLOP/s' else : args . unit = 'cy/CL'", "predictions": ["checks if an argument should be returned ."], "references": ["check arguments passed by user that are not checked by argparse itself ."], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 3032, "code": "def main ( ) : parser = create parser ( ) args = parser . parse args ( ) check arguments ( args , parser ) run ( parser , args )", "predictions": ["command line argument checking for the command line ."], "references": ["initialize and run command line interface ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 3033, "code": "def main ( ) : parser = argparse . Argument Parser ( description = 'Recursively merges two or more pickle files. Only supports pickles consisting ' 'of a single dictionary object.' ) parser . add argument ( 'destination' , type = argparse . File Type ( 'r+b' ) , help = 'File to write to and include in resulting pickle. (WILL BE CHANGED)' ) parser . add argument ( 'source' , type = argparse . File Type ( 'rb' ) , nargs = '+' , help = 'File to include in resulting pickle.' ) args = parser . parse args ( ) result = pickle . load ( args . destination ) assert isinstance ( result , collections . Mapping ) , \"only Mapping types can be handled.\" for s in args . source : data = pickle . load ( s ) assert isinstance ( data , collections . Mapping ) , \"only Mapping types can be handled.\" update ( result , data ) args . destination . seek ( 0 ) args . destination . truncate ( ) pickle . dump ( result , args . destination )", "predictions": ["generate the main method to run this example"], "references": ["comand line interface of picklemerge ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3034, "code": "def symbol pos int ( * args , * * kwargs ) : kwargs . update ( { 'positive' : True , 'integer' : True } ) return sympy . Symbol ( * args , * * kwargs )", "predictions": ["same as above . do not use ( ."], "references": ["create a sympy . symbol with positive and integer assumptions ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 3035, "code": "def find node type ( ast , node type ) : if type ( ast ) is node type : return [ ast ] elif type ( ast ) is list : return reduce ( operator . add , list ( map ( lambda a : find node type ( a , node type ) , ast ) ) , [ ] ) elif ast is None : return [ ] else : return reduce ( operator . add , [ find node type ( o [ 1 ] , node type ) for o in ast . children ( ) ] , [ ] )", "predictions": ["find all the rules in the node . this will be used to find the ( or add a new node ) ."], "references": ["return list of array references in ast ."], "bleu": 0.05856458233275369, "rouge_l": 0.14136732329084586}
{"id": 3036, "code": "def force iterable ( f ) : def wrapper ( * args , * * kwargs ) : r = f ( * args , * * kwargs ) if hasattr ( r , ' iter ' ) : return r else : return [ r ] return wrapper", "predictions": ["actually force the method to be called on an iterable of arguments ."], "references": ["will make any functions return an iterable objects by wrapping its result in a list ."], "bleu": 0.107248039853522, "rouge_l": 0.2031076581576027}
{"id": 3037, "code": "def check ( self ) : datatypes = [ v [ 0 ] for v in self . variables . values ( ) ] assert len ( set ( datatypes ) ) <= 1 , 'mixing of datatypes within a kernel is not supported.'", "predictions": ["decorator for checking that all variables in this set are equal ."], "references": ["check that information about kernel makes sens and is valid ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 3038, "code": "def subs consts ( self , expr ) : if isinstance ( expr , numbers . Number ) : return expr else : return expr . subs ( self . constants )", "predictions": ["inner method for creating a consts ."], "references": ["substitute constants in expression unless it is already a number ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 3039, "code": "def remove duplicate accesses ( self ) : self . destinations = { var name : set ( acs ) for var name , acs in self . destinations . items ( ) } self . sources = { var name : set ( acs ) for var name , acs in self . sources . items ( ) }", "predictions": ["remove a estimating at the given level ."], "references": ["remove duplicate source and destination accesses"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3040, "code": "def get loop stack ( self , subs consts = False ) : for l in self . loop stack : if subs consts : yield { 'index' : l [ 0 ] , 'start' : self . subs consts ( l [ 1 ] ) , 'stop' : self . subs consts ( l [ 2 ] ) , 'increment' : self . subs consts ( l [ 3 ] ) } else : yield { 'index' : l [ 0 ] , 'start' : l [ 1 ] , 'stop' : l [ 2 ] , 'increment' : l [ 3 ] }", "predictions": ["a generator that returns the just returns a generator of the full path of the ( e . g . , \" path return lines return the current path return rows return the selected path ."], "references": ["yield loop stack dictionaries in order from outer to inner ."], "bleu": 0.03351542279475122, "rouge_l": 0.047067901234567895}
{"id": 3041, "code": "def global iterator ( self ) : global iterator = sympy . Integer ( 0 ) total length = sympy . Integer ( 1 ) for var name , start , end , incr in reversed ( self . loop stack ) : loop var = symbol pos int ( var name ) length = end - start global iterator += ( loop var - start ) * total length total length *= length return global iterator", "predictions": ["this method returns a build at the end of this method ."], "references": ["return global iterator sympy expression"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 3042, "code": "def max global iteration ( self ) : return self . indices to global iterator ( { symbol pos int ( var name ) : end - 1 for var name , start , end , incr in self . loop stack } )", "predictions": ["set the maximum span of each step in this m_tokenqueue ."], "references": ["return global iterator with last iteration number"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3043, "code": "def print kernel info ( self , output file = sys . stdout ) : table = ( '     idx |        min        max       step\\n' + '---------+---------------------------------\\n' ) for l in self . loop stack : table += '{:>8} | {!r:>10} {!r:>10} {!r:>10}\\n' . format ( * l ) print ( prefix indent ( 'loop stack:        ' , table ) , file = output file ) table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) for name , offsets in list ( self . sources . items ( ) ) : prefix = '{:>8} | ' . format ( name ) right side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) table += prefix indent ( prefix , right side , later prefix = '         | ' ) print ( prefix indent ( 'data sources:      ' , table ) , file = output file ) table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) for name , offsets in list ( self . destinations . items ( ) ) : prefix = '{:>8} | ' . format ( name ) right side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) table += prefix indent ( prefix , right side , later prefix = '         | ' ) print ( prefix indent ( 'data destinations: ' , table ) , file = output file ) table = ( ' op | count \\n' + '----+-------\\n' ) for op , count in list ( self . flops . items ( ) ) : table += '{:>3} | {:>4}\\n' . format ( op , count ) table += '     =======\\n' table += '      {:>4}' . format ( sum ( self . flops . values ( ) ) ) print ( prefix indent ( 'FLO Ps:     ' , table ) , file = output file )", "predictions": ["analyze an ( or incremental ( ) . ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["print kernel information in human readble format ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 3044, "code": "def print variables info ( self , output file = sys . stdout ) : table = ( '    name |   type size             \\n' + '---------+-------------------------\\n' ) for name , var info in list ( self . variables . items ( ) ) : table += '{:>8} | {:>6} {!s:<10}\\n' . format ( name , var info [ 0 ] , var info [ 1 ] ) print ( prefix indent ( 'variables: ' , table ) , file = output file )", "predictions": ["prints common formatting of two tuples ."], "references": ["print variables information in human readble format ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3045, "code": "def print constants info ( self , output file = sys . stdout ) : table = ( '    name | value     \\n' + '---------+-----------\\n' ) for name , value in list ( self . constants . items ( ) ) : table += '{!s:>8} | {:<10}\\n' . format ( name , value ) print ( prefix indent ( 'constants: ' , table ) , file = output file )", "predictions": ["prints ( or a = 2 + != if any if any if something goes wrong ."], "references": ["print constants information in human readble format ."], "bleu": 0.07223943354597204, "rouge_l": 0.08555399719495091}
{"id": 3046, "code": "def print kernel code ( self , output file = sys . stdout ) : print ( self . kernel code , file = output file )", "predictions": ["prints the ( or a ( possibly . if the ( self if any if it exists if the ( i . e . , the ( if any if it exists if the sys is printed if the ( i . e . , the sys if it exists"], "references": ["print source code of kernel ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 3047, "code": "def get array declarations ( self ) : return [ d for d in self . kernel ast . block items if type ( d ) is c ast . Decl and type ( d . type ) is c ast . Array Decl ]", "predictions": ["round a series of ceil to a ."], "references": ["return array declarations ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3048, "code": "def get kernel loop nest ( self ) : loop nest = [ s for s in self . kernel ast . block items if type ( s ) in [ c ast . For , c ast . Pragma , c ast . Func Call ] ] assert len ( loop nest ) >= 1 , \"Found to few for statements in kernel\" return loop nest", "predictions": ["a access to the access access method ."], "references": ["return kernel loop nest including any preceding pragmas and following swaps ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 3049, "code": "def find inner most loop ( self , loop nest ) : r = None for s in loop nest : if type ( s ) is c ast . For : return self . find inner most loop ( s ) or s else : r = r or self . find inner most loop ( s ) return r", "predictions": ["return the geometry of this ) ."], "references": ["return inner most for loop in loop nest"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3050, "code": "def build kernel function declaration ( self , name = 'kernel' ) : array declarations , array dimensions = self . build array declarations ( with init = False ) scalar declarations = self . build scalar declarations ( with init = False ) const declarations = self . build const declartions ( with init = False ) return c ast . Func Decl ( args = c ast . Param List ( params = array declarations + scalar declarations + const declarations ) , type = c ast . Type Decl ( declname = name , quals = [ ] , type = c ast . Identifier Type ( names = [ 'void' ] ) ) )", "predictions": ["creates a ( report ) self self of a self - tree self - tree 1 . this is not implemented ."], "references": ["build and return kernel function declaration"], "bleu": 0.04657469807170698, "rouge_l": 0.0}
{"id": 3051, "code": "def build scalar declarations ( self , with init = True ) : scalar declarations = [ deepcopy ( d ) for d in self . kernel ast . block items if type ( d ) is c ast . Decl and type ( d . type ) is c ast . Type Decl ] if with init : random . seed ( 2342 ) for d in scalar declarations : if d . type . type . names [ 0 ] in [ 'double' , 'float' ] : d . init = c ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) elif d . type . type . names [ 0 ] in [ 'int' , 'long' , 'long long' , 'unsigned int' , 'unsigned long' , 'unsigned long long' ] : d . init = c ast . Constant ( 'int' , 2 ) return scalar declarations", "predictions": ["create a ( ( ( ( self block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block"], "references": ["build and return scalar variable declarations"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3052, "code": "def build kernel call ( self , name = 'kernel' ) : return c ast . Func Call ( name = c ast . ID ( name = name ) , args = c ast . Expr List ( exprs = [ c ast . ID ( name = d . name ) for d in ( self . build array declarations ( ) [ 0 ] + self . build scalar declarations ( ) + self . build const declartions ( ) ) ] ) )", "predictions": ["creates a ( self stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout stdout as a ("], "references": ["generate and return kernel call ast ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3053, "code": "def get main code ( self , as filename = False , kernel function name = 'kernel' ) : assert self . kernel ast is not None , \"AST does not exist, this could be due to running \" \"based on a kernel description rather than code.\" fp , already available = self . get intermediate file ( 'main.c' , machine and compiler dependent = False ) if already available : code = fp . read ( ) else : parser = C Parser ( ) template code = self . CODE TEMPLATE template ast = parser . parse ( clean code ( template code , macros = True , comments = True , pragmas = False ) ) ast = deepcopy ( template ast ) replace id ( ast , \"DECLARE CONSTS\" , self . build const declartions ( with init = True ) ) array declarations , array dimensions = self . build array declarations ( ) replace id ( ast , \"DECLARE ARRAYS\" , array declarations ) replace id ( ast , \"DECLARE INIT SCALARS\" , self . build scalar declarations ( ) ) replace id ( ast , \"DUMMY CALLS\" , self . build dummy calls ( ) ) ast . ext . insert ( 0 , self . build kernel function declaration ( name = kernel function name ) ) replace id ( ast , \"KERNEL CALL\" , self . build kernel call ( ) ) replace id ( ast , \"INIT ARRAYS\" , self . build array initializations ( array dimensions ) ) code = C Generator ( ) . visit ( ast ) code = '\\n' . join ( [ l for l in template code . split ( '\\n' ) if l . startswith ( \"#include\" ) ] ) + '\\n\\n' + code fp . write ( code ) fp . close ( ) if as filename : return fp . name else : return code", "predictions": ["generate a previously generated compiler ."], "references": ["generate and return compilable source code from ast ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3054, "code": "def build executable ( self , lflags = None , verbose = False , openmp = False ) : compiler , compiler args = self . machine . get compiler ( ) kernel obj filename = self . compile kernel ( openmp = openmp , verbose = verbose ) out filename , already exists = self . get intermediate file ( os . path . splitext ( os . path . basename ( kernel obj filename ) ) [ 0 ] , binary = True , fp = False ) if not already exists : main source filename = self . get main code ( as filename = True ) if not ( ( 'LIKWID INCLUDE' in os . environ or 'LIKWID INC' in os . environ ) and 'LIKWID LIB' in os . environ ) : print ( 'Could not find LIKWID INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and ' 'LIKWID LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables' , file = sys . stderr ) sys . exit ( 1 ) compiler args += [ '-std=c99' , '-I' + reduce path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID INCLUDE' , '' ) , os . environ . get ( 'LIKWID INC' , '' ) , '-llikwid' ] if os . environ . get ( 'LIKWID LIB' ) == '' : compiler args = compiler args [ : - 1 ] if lflags is None : lflags = [ ] lflags += os . environ [ 'LIKWID LIB' ] . split ( ' ' ) + [ '-pthread' ] compiler args += os . environ [ 'LIKWID LIB' ] . split ( ' ' ) + [ '-pthread' ] infiles = [ reduce path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/dummy.c' ) , kernel obj filename , main source filename ] cmd = [ compiler ] + infiles + compiler args + [ '-o' , out filename ] cmd = list ( filter ( bool , cmd ) ) if verbose : print ( 'Executing (build executable): ' , ' ' . join ( cmd ) ) try : subprocess . check output ( cmd ) except subprocess . Called Process Error as e : print ( \"Build failed:\" , e , file = sys . stderr ) sys . exit ( 1 ) else : if verbose : print ( 'Executing (build executable): ' , 'using cached' , out filename ) return out filename", "predictions": ["builds an error object for the line located at the end of the line ."], "references": ["compile source to executable with likwid capabilities and return the executable name ."], "bleu": 0.09103526405546068, "rouge_l": 0.14472123368920523}
{"id": 3055, "code": "def string to sympy ( cls , s ) : if isinstance ( s , int ) : return sympy . Integer ( s ) elif isinstance ( s , list ) : return tuple ( [ cls . string to sympy ( e ) for e in s ] ) elif s is None : return None else : local dict = { c : symbol pos int ( c ) for c in s if c in string . ascii letters } preliminary expr = parse expr ( s , local dict = local dict ) local dict . update ( { s . name : symbol pos int ( s . name ) for s in preliminary expr . free symbols } ) return parse expr ( s , local dict = local dict )", "predictions": ["convert an extension to a class ."], "references": ["convert any string to a sympy object or none ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 3056, "code": "def get identifier ( self ) : if self . path : return os . path . basename ( self . path ) else : return hashlib . sha256 ( hashlib . sha256 ( repr ( self . data ) . encode ( ) ) ) . hexdigest ( )", "predictions": ["get the full description of this commit ."], "references": ["return identifier which is either the machine file name or sha256 checksum of data ."], "bleu": 0.07949903911132591, "rouge_l": 0.24729729729729732}
{"id": 3057, "code": "def get last modified datetime ( self ) : if self . path : statbuf = os . stat ( self . path ) return datetime . utcfromtimestamp ( statbuf . st mtime ) else : return datetime . now ( )", "predictions": ["returns the block blocks for this session ."], "references": ["return datetime object of modified time of machine file . return now if not a file ."], "bleu": 0.052063188264041965, "rouge_l": 0.0751231527093596}
{"id": 3058, "code": "def enforce no overlap ( self , start at = 0 ) : i = start at while i + 1 < len ( self . data ) : if self . data [ i ] [ 1 ] >= self . data [ i + 1 ] [ 0 ] : if self . data [ i ] [ 1 ] < self . data [ i + 1 ] [ 1 ] : self . data [ i ] [ 1 ] = self . data [ i + 1 ] [ 1 ] del self . data [ i + 1 ] i += 1", "predictions": ["insert a process using the given name ."], "references": ["enforce that no ranges overlap in internal storage ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3059, "code": "def get header path ( ) -> str : import os return os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/'", "predictions": ["get the absolute path of the current operating system ."], "references": ["return local folder path of header files ."], "bleu": 0.17827531042796255, "rouge_l": 0.34014869888475835}
{"id": 3060, "code": "def align iteration with cl boundary ( self , iteration , subtract = True ) : element size = self . kernel . datatypes size [ self . kernel . datatype ] cacheline size = self . machine [ 'cacheline size' ] elements per cacheline = int ( cacheline size // element size ) inner loop = list ( self . kernel . get loop stack ( subs consts = True ) ) [ - 1 ] inner increment = inner loop [ 'increment' ] o = self . kernel . compile global offsets ( iteration = iteration ) [ 0 ] if len ( o [ 1 ] ) : first offset = min ( o [ 1 ] ) else : first offset = min ( o [ 0 ] ) diff = first offset - ( int ( first offset ) >> self . csim . first level . cl bits << self . csim . first level . cl bits ) if diff == 0 : return iteration elif subtract : return iteration - ( diff // element size ) // inner increment else : return iteration + ( elements per cacheline - diff // element size ) // inner increment", "predictions": ["makes a k ( model : model . model = model / model / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( . . / ( / . / ( /"], "references": ["align iteration with cacheline boundary ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 3061, "code": "def get loads ( self ) : return [ self . stats [ cache level ] [ 'LOAD count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["get . this is useful for ."], "references": ["return a list with number of loaded cache lines per memory hierarchy level ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 3062, "code": "def get hits ( self ) : return [ self . stats [ cache level ] [ 'HIT count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["for each level . this is a helper method that maps the description to the description of this class ."], "references": ["return a list with number of hit cache lines per memory hierarchy level ."], "bleu": 0.09134423666564473, "rouge_l": 0.18227091633466133}
{"id": 3063, "code": "def get misses ( self ) : return [ self . stats [ cache level ] [ 'MISS count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["get the . this is useful for ."], "references": ["return a list with number of missed cache lines per memory hierarchy level ."], "bleu": 0.07575149194183216, "rouge_l": 0.08664772727272725}
{"id": 3064, "code": "def get stores ( self ) : return [ self . stats [ cache level ] [ 'STORE count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["main method for this arguments ."], "references": ["return a list with number of stored cache lines per memory hierarchy level ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 3065, "code": "def get evicts ( self ) : return [ self . stats [ cache level ] [ 'EVICT count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["get stats for description of description ."], "references": ["return a list with number of evicted cache lines per memory hierarchy level ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 3066, "code": "def get infos ( self ) : first dim factor = self . first dim factor infos = { 'memory hierarchy' : [ ] , 'cache stats' : self . stats , 'cachelines in stats' : first dim factor } for cache level , cache info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) : infos [ 'memory hierarchy' ] . append ( { 'index' : len ( infos [ 'memory hierarchy' ] ) , 'level' : '{}' . format ( cache info [ 'level' ] ) , 'total loads' : self . stats [ cache level ] [ 'LOAD byte' ] / first dim factor , 'total misses' : self . stats [ cache level ] [ 'MISS byte' ] / first dim factor , 'total hits' : self . stats [ cache level ] [ 'HIT byte' ] / first dim factor , 'total stores' : self . stats [ cache level ] [ 'STORE byte' ] / first dim factor , 'total evicts' : self . stats [ cache level ] [ 'EVICT byte' ] / first dim factor , 'total lines load' : self . stats [ cache level ] [ 'LOAD count' ] / first dim factor , 'total lines misses' : self . stats [ cache level ] [ 'MISS count' ] / first dim factor , 'total lines hits' : self . stats [ cache level ] [ 'HIT count' ] / first dim factor , 'total lines stores' : self . stats [ cache level ] [ 'STORE count' ] / first dim factor , 'total lines evicts' : self . stats [ cache level ] [ 'EVICT count' ] / first dim factor , 'cycles' : None } ) return infos", "predictions": ["to get stats } and then use stats ."], "references": ["return verbose information about the predictor ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3067, "code": "def measure bw ( type , total size , threads per core , max threads per core , cores per socket , sockets ) : groups = [ ] for s in range ( sockets ) : groups += [ '-w' , 'S' + str ( s ) + ':' + str ( total size ) + 'k B:' + str ( threads per core * cores per socket ) + ':1:' + str ( int ( max threads per core / threads per core ) ) ] cmd = [ 'likwid-bench' , '-t' , type ] + groups sys . stderr . write ( ' ' . join ( cmd ) ) output = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] . decode ( 'utf-8' ) if not output : print ( ' ' . join ( cmd ) + ' returned no output, possibly wrong version installed ' '(requires 4.0 or later)' , file = sys . stderr ) sys . exit ( 1 ) bw = float ( get match or break ( r'^M Byte/s:\\s+([0-9]+(?:\\.[0-9]+)?)\\s*$' , output ) [ 0 ] ) print ( ' ' , Prefixed Unit ( bw , 'MB/s' ) , file = sys . stderr ) return Prefixed Unit ( bw , 'MB/s' )", "predictions": ["find command with statistics ."], "references": ["* size * is given in kilo bytes"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3068, "code": "def fix env variable ( name , value ) : orig = os . environ . get ( name , None ) if value is not None : os . environ [ name ] = value elif name in os . environ : del os . environ [ name ] try : yield finally : if orig is not None : os . environ [ name ] = orig elif name in os . environ : del os . environ [ name ]", "predictions": ["context manager for an environment ."], "references": ["fix environment variable to a value within context . unset if value is none ."], "bleu": 0.05861428254383573, "rouge_l": 0.17681159420289855}
{"id": 3069, "code": "def configure arggroup ( cls , parser ) : parser . add argument ( '--no-phenoecm' , action = 'store true' , help = 'Disables the phenomenological ECM model building.' ) parser . add argument ( '--iterations' , type = int , default = 10 , help = 'Number of outer-loop iterations (e.g. time loop) during benchmarking. ' 'Default is 10, but actual number will be adapted to at least 0.2s runtime.' ) parser . add argument ( '--ignore-warnings' , action = 'store true' , help = 'Ignore warnings about missmatched CPU model and frequency.' )", "predictions": ["configures the ( . = ( , . = ( , . , . = ( , . , . = . , . , . , . , . = ( , . , . , . , . , . , . , . , . , ."], "references": ["configure argument parser ."], "bleu": 0.02403051755364481, "rouge_l": 0.04375896700143472}
{"id": 3070, "code": "def report ( self , output file = sys . stdout ) : if self . verbose > 1 : with pprint nosort ( ) : pprint . pprint ( self . results ) if self . verbose > 0 : print ( 'Runtime (per repetition): {:.2g} s' . format ( self . results [ 'Runtime (per repetition) [s]' ] ) , file = output file ) if self . verbose > 0 : print ( 'Iterations per repetition: {!s}' . format ( self . results [ 'Iterations per repetition' ] ) , file = output file ) print ( 'Runtime (per cacheline update): {:.2f} cy/CL' . format ( self . results [ 'Runtime (per cacheline update) [cy/CL]' ] ) , file = output file ) print ( 'MEM volume (per repetition): {:.0f} Byte' . format ( self . results [ 'MEM volume (per repetition) [B]' ] ) , file = output file ) print ( 'Performance: {:.2f} MFLOP/s' . format ( self . results [ 'Performance [MFLOP/s]' ] ) , file = output file ) print ( 'Performance: {:.2f} MLUP/s' . format ( self . results [ 'Performance [MLUP/s]' ] ) , file = output file ) print ( 'Performance: {:.2f} It/s' . format ( self . results [ 'Performance [M It/s]' ] ) , file = output file ) if self . verbose > 0 : print ( 'MEM bandwidth: {:.2f} M Byte/s' . format ( self . results [ 'MEM BW [M Byte/s]' ] ) , file = output file ) print ( '' , file = output file ) if not self . no phenoecm : print ( \"Data Transfers:\" ) print ( \"{:^8} |\" . format ( \"cache\" ) , end = '' ) for metrics in self . results [ 'data transfers' ] . values ( ) : for metric name in sorted ( metrics ) : print ( \" {:^14}\" . format ( metric name ) , end = '' ) print ( ) break for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : print ( \"{!s:^8} |\" . format ( cache ) , end = '' ) for k , v in sorted ( metrics . items ( ) ) : print ( \" {!s:^14}\" . format ( v ) , end = '' ) print ( ) print ( ) print ( 'Phenomenological ECM model: {{ {T OL:.1f} || {T n OL:.1f} | {T L1L2:.1f} | ' '{T L2L3:.1f} | {T L3MEM:.1f} }} cy/CL' . format ( * * { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , file = output file ) print ( 'T OL assumes that two loads per cycle may be retiered, which is true for ' '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, ' 'BDW, SKL and SKX, but it also depends on AGU availability.' , file = output file )", "predictions": ["generate an example to draw the ( ."], "references": ["report gathered analysis data in human readable form ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3071, "code": "def build purchase item ( course id , course url , cost in cents , mode , course data , sku ) : item = { 'id' : \"{}-{}\" . format ( course id , mode ) , 'url' : course url , 'price' : cost in cents , 'qty' : 1 , } if 'title' in course data : item [ 'title' ] = course data [ 'title' ] else : item [ 'title' ] = 'Course {} mode: {}' . format ( course id , mode ) if 'tags' in course data : item [ 'tags' ] = course data [ 'tags' ] item [ 'vars' ] = dict ( course data . get ( 'vars' , { } ) , mode = mode , course run id = course id ) item [ 'vars' ] [ 'purchase sku' ] = sku return item", "predictions": ["builds a duplicate duplicate accesses from a duplicate accesses accesses ."], "references": ["build and return sailthru purchase item object"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3072, "code": "def send offer assignment notification email ( config , user email , subject , email body , site code , task ) : try : sailthru client = get sailthru client ( site code ) except Sailthru Error : logger . exception ( '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) return None email vars = { 'subject' : subject , 'email body' : email body , } try : response = sailthru client . send ( template = config [ 'templates' ] [ 'assignment email' ] , email = user email , vars = email vars ) except Sailthru Client Error : logger . exception ( '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) return None if not response . is ok ( ) : error = response . get error ( ) logger . error ( '[Offer Assignment] A {token error code} - {token error message} error occurred' ' while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body , token error code = error . get error code ( ) , token error message = error . get message ( ) ) ) if can retry sailthru request ( error ) : logger . info ( '[Offer Assignment] An attempt will be made to resend the offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) schedule retry ( task , config ) else : logger . warning ( '[Offer Assignment] No further attempts will be made to send the offer assignment notification.' ' Failed Message: {message}' . format ( message = email body ) ) return response", "predictions": ["sends verification assignment in sailthru ."], "references": ["handles sending offer assignment notification emails and retrying failed emails when appropriate ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 3073, "code": "def file refs ( self ) : if self . prepared file refs is None : self . prepared file refs = { FILE REFS . idf : File Info ( constructor = lambda path : self . epm cls . from idf ( path , idd or buffer or path = self . idd ) , get path = lambda : get input file path ( self . dir path , FILE REFS . idf ) ) , FILE REFS . epw : File Info ( constructor = lambda path : self . weather data cls . from epw ( path ) , get path = lambda : get input file path ( self . dir path , FILE REFS . epw ) ) , FILE REFS . eio : File Info ( constructor = lambda path : self . eio cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . eio ) ) , FILE REFS . eso : File Info ( constructor = lambda path : self . standard output cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . eso ) ) , FILE REFS . mtr : File Info ( constructor = lambda path : self . standard output cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mtr ) ) , FILE REFS . mtd : File Info ( constructor = lambda path : self . mtd cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mtd ) ) , FILE REFS . mdd : File Info ( constructor = lambda path : open ( path ) . read ( ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mdd ) ) , FILE REFS . err : File Info ( constructor = lambda path : self . err cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . err ) ) , FILE REFS . summary table : File Info ( constructor = lambda path : self . summary table cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . summary table ) ) } return self . prepared file refs", "predictions": ["creates and returns a new object that can be used to locate this object ."], "references": ["defined here so that we can use the class variables in order to subclass in oplusplus"], "bleu": 0.09151480843357339, "rouge_l": 0.19242902208201892}
{"id": 3074, "code": "def dev populate from json data ( self , json data ) : comment = json data . pop ( \" comment\" , None ) if comment is not None : self . comment = comment external files data = json data . pop ( \" external files\" , dict ( ) ) self . dev external files manager . populate from json data ( external files data ) added records = [ ] for table ref , json data records in json data . items ( ) : table = getattr ( self , table ref ) records = table . dev add inert ( json data records ) added records . extend ( records ) for r in added records : r . dev activate hooks ( ) for r in added records : r . dev activate links ( ) r . dev activate external files ( )", "predictions": ["creates a list of ( from the given json array ."], "references": ["!! must only be called once when empty !!"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3075, "code": "def get external files ( self ) : external files = [ ] for table in self . tables . values ( ) : for r in table : external files . extend ( [ ef for ef in r . get external files ( ) ] ) return external files", "predictions": ["get all external external files ."], "references": ["an external file manages file paths ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3076, "code": "def set defaults ( self ) : for table in self . tables . values ( ) : for r in table : r . set defaults ( )", "predictions": ["set default dirty method ."], "references": ["all fields of epm with a default value and that are null will be set to their default value ."], "bleu": 0.01616426370461062, "rouge_l": 0.21656804733727808}
{"id": 3077, "code": "def prepare extensible ( self ) : for k in self . tags : if \"extensible\" in k : cycle len = int ( k . split ( \":\" ) [ 1 ] ) break else : return cycle start = None cycle patterns = [ ] for i , field descriptor in enumerate ( self . field descriptors ) : if ( cycle start is not None ) and ( i >= ( cycle start + cycle len ) ) : break if ( cycle start is None ) and ( \"begin-extensible\" in field descriptor . tags ) : cycle start = i if cycle start is None : continue cycle patterns . append ( field descriptor . ref . replace ( \"1\" , r\"(\\d+)\" ) ) else : raise Runtime Error ( \"cycle start not found\" ) self . field descriptors = self . field descriptors [ : cycle start + cycle len ] self . extensible info = ( cycle start , cycle len , tuple ( cycle patterns ) ) for i , fd in enumerate ( self . field descriptors [ cycle start : ] ) : fd . set extensible info ( cycle start , cycle len , cycle patterns [ i ] )", "predictions": ["pressing and processes all patterns that are marked as in the given field ."], "references": ["this function finishes initialization must be called once all field descriptors and tag have been filled ."], "bleu": 0.08971053043365333, "rouge_l": 0.19022869022869024}
{"id": 3078, "code": "def get value ( self , column name or i , filter column name or i , filter criterion ) : column i = self . get column index ( column name or i ) filter column i = self . get column index ( filter column name or i ) filter fct = { float : lambda x : float ( x ) == filter criterion , int : lambda x : int ( x ) == filter criterion , str : lambda x : x . lower ( ) == filter criterion . lower ( ) } [ type ( filter criterion ) ] for row i , row in enumerate ( self . data ) : if filter fct ( row [ filter column i ] ) : break else : raise Value Error ( \"Filter did not return any values.\" ) return self . data [ row i ] [ column i ]", "predictions": ["returns a value with a given criterion ."], "references": ["returns first occurrence of value of filter column matching filter criterion ."], "bleu": 0.14544785215055717, "rouge_l": 0.3860759493670886}
{"id": 3079, "code": "def update value inert ( self , index , value ) : field descriptor = self . table . dev descriptor . get field descriptor ( index ) value = field descriptor . deserialize ( value , index ) if isinstance ( value , Link ) : current link = self . data . get ( index ) if current link is not None : current link . unregister ( ) if isinstance ( value , Record Hook ) : current record hook = self . data . get ( index ) if current record hook is not None : current record hook . unregister ( ) if isinstance ( value , External File ) : current external file = self . data . get ( index ) if current external file is not None : current external file . dev unregister ( ) if value in ( None , NONE RECORD HOOK , NONE LINK , NONE EXTERNAL FILE ) : self . dev set none without unregistering ( index , check not required = False ) return old hook = None if index == 0 and not self . table . dev auto pk : old hook = self . data . get ( 0 ) self . data [ index ] = value if old hook is not None : self . table . dev record pk was updated ( old hook . target value )", "predictions": ["unregister this value at the specified index ."], "references": ["is only called by _update_inert"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3080, "code": "def set defaults ( self ) : defaults = { } for i in range ( len ( self ) ) : if i in self . data : continue default = self . get field descriptor ( i ) . tags . get ( \"default\" , [ None ] ) [ 0 ] if default is not None : defaults [ i ] = default self . update ( defaults )", "predictions": ["set default implementation for default object ."], "references": ["sets all empty fields for which a default value is defined to default value"], "bleu": 0.0812630644213965, "rouge_l": 0.17967599410898377}
{"id": 3081, "code": "def delete ( self ) : self . unregister links ( ) self . unregister hooks ( ) self . unregister external files ( ) self . get table ( ) . dev remove record without unregistering ( self ) self . table = None self . data = None", "predictions": ["removes an existing table from the database ."], "references": ["deletes record and removes it from database ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 3082, "code": "def register record hook ( self , hook ) : for key in hook . keys : if key in self . record hooks : field descriptor = hook . target record . get field descriptor ( hook . target index ) raise Field Validation Error ( f\"Reference key already exists, can't create: {key}. \" f\"{field descriptor.get error location message(hook.target value, hook.target index)}\" ) self . record hooks [ key ] = hook", "predictions": ["this is called to register some of the ( object ."], "references": ["target record must have been set"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3083, "code": "def register link ( self , link ) : keys = tuple ( ( ref , link . initial hook value ) for ref in link . hook references ) for k in keys : if k in self . record hooks : link . set target ( target record = self . record hooks [ k ] . target record ) break else : for k in keys : if k in self . table hooks : link . set target ( target table = self . table hooks [ k ] ) break else : field descriptor = link . source record . get field descriptor ( link . source index ) raise Field Validation Error ( f\"No object found with any of given references : {keys}. \" f\"{field descriptor.get error location message(link.initial hook value)}\" ) if link . source record not in self . links by source : self . links by source [ link . source record ] = set ( ) self . links by source [ link . source record ] . add ( link ) if link . target not in self . links by target : self . links by target [ link . target ] = set ( ) self . links by target [ link . target ] . add ( link )", "predictions": ["this is called to hide hide hide hide hide hide hide ."], "references": ["source record and index must have been set"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 3084, "code": "def create regex ( self , line , intent name ) : try : return re . compile ( self . create intent pattern ( line , intent name ) , re . IGNORECASE ) except sre constants . error as e : LOG . warning ( 'Failed to parse the line \"{}\" ' 'for {}' . format ( line , intent name ) ) return None", "predictions": ["creates a regex for the given name and name ."], "references": ["create regex and return . if error occurs returns none ."], "bleu": 0.13564514503163538, "rouge_l": 0.28328173374613}
{"id": 3085, "code": "def remaining duration ( self , time ) : return max ( 0 , self . end - max ( self . start , time ) )", "predictions": ["returns a new time stamp for this request ."], "references": ["returns the remaining duration for a recording ."], "bleu": 0.17747405280050263, "rouge_l": 0.35672514619883033}
{"id": 3086, "code": "def http request ( url , post data = None ) : logger . debug ( 'Requesting URL: %s' % url ) buf = bio ( ) curl = pycurl . Curl ( ) curl . setopt ( curl . URL , url . encode ( 'ascii' , 'ignore' ) ) if config ( ) [ 'server' ] [ 'insecure' ] : curl . setopt ( curl . SSL VERIFYPEER , 0 ) curl . setopt ( curl . SSL VERIFYHOST , 0 ) if config ( ) [ 'server' ] [ 'certificate' ] : curl . setopt ( curl . SSL VERIFYPEER , 1 ) curl . setopt ( curl . SSL VERIFYHOST , 2 ) curl . setopt ( pycurl . CAINFO , config ( ) [ 'server' ] [ 'certificate' ] ) if post data : curl . setopt ( curl . HTTPPOST , post data ) curl . setopt ( curl . WRITEFUNCTION , buf . write ) curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH DIGEST ) curl . setopt ( pycurl . USERPWD , \"%s:%s\" % ( config ( ) [ 'server' ] [ 'username' ] , config ( ) [ 'server' ] [ 'password' ] ) ) curl . setopt ( curl . HTTPHEADER , [ 'X-Requested-Auth: Digest' ] ) curl . setopt ( curl . FAILONERROR , True ) curl . setopt ( curl . FOLLOWLOCATION , True ) curl . perform ( ) curl . close ( ) result = buf . getvalue ( ) buf . close ( ) return result", "predictions": ["decode a http request to a url ."], "references": ["make an http request to a given url with optional parameters ."], "bleu": 0.2944656445786981, "rouge_l": 0.5791139240506329}
{"id": 3087, "code": "def try mkdir ( directory ) : try : os . mkdir ( directory ) except OS Error as err : if err . errno != errno . EEXIST : raise err", "predictions": ["creates a directory if it does not exist ."], "references": ["try to create a directory . pass without error if it already exists ."], "bleu": 0.14026062829965458, "rouge_l": 0.41838134430727025}
{"id": 3088, "code": "def update event status ( event , status ) : dbs = db . get session ( ) dbs . query ( db . Recorded Event ) . filter ( db . Recorded Event . start == event . start ) . update ( { 'status' : status } ) event . status = status dbs . commit ( )", "predictions": ["update the status of an event ."], "references": ["update the status of a particular event in the database ."], "bleu": 0.31756433708884857, "rouge_l": 0.6409807355516637}
{"id": 3089, "code": "def set service status ( service , status ) : srv = db . Service States ( ) srv . type = service srv . status = status dbs = db . get session ( ) dbs . merge ( srv ) dbs . commit ( ) dbs . close ( )", "predictions": ["sets service status on service ."], "references": ["update the status of a particular service in the database ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 3090, "code": "def get service status ( service ) : dbs = db . get session ( ) srvs = dbs . query ( db . Service States ) . filter ( db . Service States . type == service ) if srvs . count ( ) : return srvs [ 0 ] . status return db . Service Status . STOPPED", "predictions": ["get the status of the service ."], "references": ["update the status of a particular service in the database ."], "bleu": 0.22455189621853547, "rouge_l": 0.5341506129597198}
{"id": 3091, "code": "def update agent state ( ) : configure service ( 'capture.admin' ) status = 'idle' if get service status ( db . Service . SCHEDULE ) == db . Service Status . STOPPED : status = 'offline' elif get service status ( db . Service . CAPTURE ) == db . Service Status . BUSY : status = 'capturing' elif get service status ( db . Service . INGEST ) == db . Service Status . BUSY : status = 'uploading' register ca ( status = status )", "predictions": ["update state according to specified status ."], "references": ["update the current agent state in opencast ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 3092, "code": "def configuration file ( cfgfile ) : if cfgfile is not None : return cfgfile cfg = './etc/pyca.conf' if not os . path . isfile ( cfg ) : return '/etc/pyca.conf' return cfg", "predictions": ["get the configuration file or directory if it ' s defined ."], "references": ["find the best match for the configuration file ."], "bleu": 0.2044800736021839, "rouge_l": 0.39102564102564097}
{"id": 3093, "code": "def check ( ) : if config ( 'server' ) [ 'insecure' ] : logger . warning ( 'HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS ' 'NOT GUARANTEED' ) if config ( 'server' ) [ 'certificate' ] : open ( config ( 'server' ) [ 'certificate' ] , 'rb' ) . close ( ) if config ( 'agent' ) [ 'backup mode' ] : logger . info ( 'Agent runs in backup mode. No data will be sent to ' 'Opencast' )", "predictions": ["a method that can override the ( method ."], "references": ["check configuration for sanity ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 3094, "code": "def logger init ( ) : handlers = [ ] logconf = config ( 'logging' ) if logconf [ 'syslog' ] : handlers . append ( logging . handlers . Sys Log Handler ( address = '/dev/log' ) ) if logconf [ 'stderr' ] : handlers . append ( logging . Stream Handler ( sys . stderr ) ) if logconf [ 'file' ] : handlers . append ( logging . handlers . Watched File Handler ( logconf [ 'file' ] ) ) for handler in handlers : handler . set Formatter ( logging . Formatter ( logconf [ 'format' ] ) ) logging . root . add Handler ( handler ) logging . root . set Level ( logconf [ 'level' ] . upper ( ) ) logger . info ( 'Log level set to %s' % logconf [ 'level' ] )", "predictions": ["called when the class makes the user selects a separate logger ."], "references": ["initialize logger based on configuration"], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 3095, "code": "def home ( ) : preview = config ( ) [ 'capture' ] [ 'preview' ] previewdir = config ( ) [ 'capture' ] [ 'preview dir' ] preview = [ p . replace ( '{{previewdir}}' , previewdir ) for p in preview ] preview = zip ( preview , range ( len ( preview ) ) ) preview = [ p [ 1 ] for p in preview if os . path . isfile ( p [ 0 ] ) ] try : limit upcoming = int ( request . args . get ( 'limit upcoming' , 5 ) ) limit processed = int ( request . args . get ( 'limit processed' , 15 ) ) except Value Error : limit upcoming = 5 limit processed = 15 db = get session ( ) upcoming events = db . query ( Upcoming Event ) . order by ( Upcoming Event . start ) . limit ( limit upcoming ) recorded events = db . query ( Recorded Event ) . order by ( Recorded Event . start . desc ( ) ) . limit ( limit processed ) recording = get service status ( Service . CAPTURE ) == Service Status . BUSY uploading = get service status ( Service . INGEST ) == Service Status . BUSY processed = db . query ( Recorded Event ) . count ( ) upcoming = db . query ( Upcoming Event ) . count ( ) return render template ( 'home.html' , preview = preview , config = config ( ) , recorded events = recorded events , upcoming events = upcoming events , recording = recording , uploading = uploading , processed = processed , upcoming = upcoming , limit upcoming = limit upcoming , limit processed = limit processed , dtfmt = dtfmt )", "predictions": ["show all configuration cmd about the event ."], "references": ["serve the status page of the capture agent ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3096, "code": "def serve image ( image id ) : try : preview dir = config ( ) [ 'capture' ] [ 'preview dir' ] filepath = config ( ) [ 'capture' ] [ 'preview' ] [ image id ] filepath = filepath . replace ( '{{previewdir}}' , preview dir ) filepath = os . path . abspath ( filepath ) if os . path . isfile ( filepath ) : directory , filename = filepath . rsplit ( '/' , 1 ) return send from directory ( directory , filename ) except ( Index Error , Key Error ) : pass return '' , 404", "predictions": ["serve image column column"], "references": ["serve the preview image with the given id"], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 3097, "code": "def sigterm handler ( signum , frame ) : sigint handler ( signum , frame ) for process in multiprocessing . active children ( ) : process . terminate ( ) sys . exit ( 0 )", "predictions": ["creates a new ( ."], "references": ["intercept sigterm and terminate all processes ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3098, "code": "def run all ( * modules ) : processes = [ multiprocessing . Process ( target = mod . run ) for mod in modules ] for p in processes : p . start ( ) for p in processes : p . join ( )", "predictions": ["the loop should be called during the process ."], "references": ["start all services ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3099, "code": "def parse ical ( vcal ) : vcal = vcal . replace ( '\\r\\n ' , '' ) . replace ( '\\r\\n\\r\\n' , '\\r\\n' ) vevents = vcal . split ( '\\r\\n BEGIN:VEVENT\\r\\n' ) del ( vevents [ 0 ] ) events = [ ] for vevent in vevents : event = { } for line in vevent . split ( '\\r\\n' ) : line = line . split ( ':' , 1 ) key = line [ 0 ] . lower ( ) if len ( line ) <= 1 or key == 'end' : continue if key . startswith ( 'dt' ) : event [ key ] = unix ts ( dateutil . parser . parse ( line [ 1 ] ) ) continue if not key . startswith ( 'attach' ) : event [ key ] = line [ 1 ] continue event [ 'attach' ] = event . get ( 'attach' , [ ] ) attachment = { } for x in [ x . split ( '=' ) for x in line [ 0 ] . split ( ';' ) ] : if x [ 0 ] . lower ( ) in [ 'fmttype' , 'x-apple-filename' ] : attachment [ x [ 0 ] . lower ( ) ] = x [ 1 ] attachment [ 'data' ] = b64decode ( line [ 1 ] ) . decode ( 'utf-8' ) event [ 'attach' ] . append ( attachment ) events . append ( event ) return events", "predictions": ["parse the arguments of a . into a . object ."], "references": ["parse opencast schedule icalendar file and return events as dict"], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 3100, "code": "def control loop ( ) : set service status ( Service . SCHEDULE , Service Status . BUSY ) notify . notify ( 'READY=1' ) while not terminate ( ) : notify . notify ( 'WATCHDOG=1' ) get schedule ( ) session = get session ( ) next event = session . query ( Upcoming Event ) . filter ( Upcoming Event . end > timestamp ( ) ) . order by ( Upcoming Event . start ) . first ( ) if next event : logger . info ( 'Next scheduled recording: %s' , datetime . fromtimestamp ( next event . start ) ) notify . notify ( 'STATUS=Next scheduled recording: %s' % datetime . fromtimestamp ( next event . start ) ) else : logger . info ( 'No scheduled recording' ) notify . notify ( 'STATUS=No scheduled recording' ) session . close ( ) next update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update frequency' ] while not terminate ( ) and timestamp ( ) < next update : time . sleep ( 0.1 ) logger . info ( 'Shutting down schedule service' ) set service status ( Service . SCHEDULE , Service Status . STOPPED )", "predictions": ["immediately starts the control packet ."], "references": ["main loop retrieving the schedule ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 3101, "code": "def control loop ( ) : set service status ( Service . AGENTSTATE , Service Status . BUSY ) notify . notify ( 'READY=1' ) notify . notify ( 'STATUS=Running' ) while not terminate ( ) : notify . notify ( 'WATCHDOG=1' ) update agent state ( ) next update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update frequency' ] while not terminate ( ) and timestamp ( ) < next update : time . sleep ( 0.1 ) logger . info ( 'Shutting down agentstate service' ) set service status ( Service . AGENTSTATE , Service Status . STOPPED )", "predictions": ["notifies all the execution ( ."], "references": ["main loop updating the capture agent state ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3102, "code": "def make error response ( error , status = 500 ) : content = { 'errors' : [ { 'status' : status , 'title' : error } ] } return make response ( jsonify ( content ) , status )", "predictions": ["makes an error response with some content ."], "references": ["return a response with a jsonapi error object"], "bleu": 0.22679164443904004, "rouge_l": 0.25}
{"id": 3103, "code": "def make data response ( data , status = 200 ) : content = { 'data' : ensurelist ( data ) } return make response ( jsonify ( content ) , status )", "predictions": ["makes the specified data with the specified content ."], "references": ["return a response with a list of jsonapi data objects"], "bleu": 0.1397712139461423, "rouge_l": 0.10427350427350426}
{"id": 3104, "code": "def internal state ( ) : data = { 'services' : { 'capture' : Service Status . str ( get service status ( Service . CAPTURE ) ) , 'ingest' : Service Status . str ( get service status ( Service . INGEST ) ) , 'schedule' : Service Status . str ( get service status ( Service . SCHEDULE ) ) , 'agentstate' : Service Status . str ( get service status ( Service . AGENTSTATE ) ) } } return make response ( jsonify ( { 'meta' : data } ) )", "predictions": ["retrieves the service or profile status based on the error try ."], "references": ["serve a json representation of internal agentstate as meta data"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 3105, "code": "def events ( ) : db = get session ( ) upcoming events = db . query ( Upcoming Event ) . order by ( Upcoming Event . start ) recorded events = db . query ( Recorded Event ) . order by ( Recorded Event . start . desc ( ) ) result = [ event . serialize ( ) for event in upcoming events ] result += [ event . serialize ( ) for event in recorded events ] return make data response ( result )", "predictions": ["buffer is one of the ( ."], "references": ["serve a json representation of events"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3106, "code": "def event ( uid ) : db = get session ( ) event = db . query ( Recorded Event ) . filter ( Recorded Event . uid == uid ) . first ( ) or db . query ( Upcoming Event ) . filter ( Upcoming Event . uid == uid ) . first ( ) if event : return make data response ( event . serialize ( ) ) return make error response ( 'No event with specified uid' , 404 )", "predictions": ["external dev , dict and not all the ( ."], "references": ["return a specific events json"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3107, "code": "def ingest ( event ) : set service status ( Service . INGEST , Service Status . BUSY ) notify . notify ( 'STATUS=Uploading' ) recording state ( event . uid , 'uploading' ) update event status ( event , Status . UPLOADING ) service = config ( 'service-ingest' ) service = service [ randrange ( 0 , len ( service ) ) ] logger . info ( 'Selecting ingest service to use: ' + service ) logger . info ( 'Creating new mediapackage' ) mediapackage = http request ( service + '/create Media Package' ) prop = 'org.opencastproject.capture.agent.properties' dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/' for attachment in event . get data ( ) . get ( 'attach' ) : data = attachment . get ( 'data' ) if attachment . get ( 'x-apple-filename' ) == prop : workflow def , workflow config = get config params ( data ) elif attachment . get ( 'fmttype' ) == 'application/xml' and dcns in data : name = attachment . get ( 'x-apple-filename' , '' ) . rsplit ( '.' , 1 ) [ 0 ] logger . info ( 'Adding %s DC catalog' % name ) fields = [ ( 'media Package' , mediapackage ) , ( 'flavor' , 'dublincore/%s' % name ) , ( 'dublin Core' , data . encode ( 'utf-8' ) ) ] mediapackage = http request ( service + '/add DC Catalog' , fields ) for ( flavor , track ) in event . get tracks ( ) : logger . info ( 'Adding track ({0} -> {1})' . format ( flavor , track ) ) track = track . encode ( 'ascii' , 'ignore' ) fields = [ ( 'media Package' , mediapackage ) , ( 'flavor' , flavor ) , ( 'BODY1' , ( pycurl . FORM FILE , track ) ) ] mediapackage = http request ( service + '/add Track' , fields ) logger . info ( 'Ingest recording' ) fields = [ ( 'media Package' , mediapackage ) ] if workflow def : fields . append ( ( 'workflow Definition Id' , workflow def ) ) if event . uid : fields . append ( ( 'workflow Instance Id' , event . uid . encode ( 'ascii' , 'ignore' ) ) ) fields += workflow config mediapackage = http request ( service + '/ingest' , fields ) recording state ( event . uid , 'upload finished' ) update event status ( event , Status . FINISHED UPLOADING ) notify . notify ( 'STATUS=Running' ) set service status immediate ( Service . INGEST , Service Status . IDLE ) logger . info ( 'Finished ingest' )", "predictions": ["creates a alexaaudioplayer instance ."], "references": ["ingest a finished recording to the opencast server ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3108, "code": "def sigterm handler ( signum , frame ) : if captureproc and captureproc . poll ( ) is None : captureproc . terminate ( ) terminate ( True ) sys . exit ( 0 )", "predictions": ["creates a set of ("], "references": ["intercept sigterm and terminate all processes ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 3109, "code": "def render to fragment ( self , request , * * kwargs ) : fragment = Fragment ( TEST HTML ) fragment . add javascript ( TEST JS ) fragment . add css ( TEST CSS ) return fragment", "predictions": ["convert a ( possibly empty . its ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["returns a simple fragment"], "bleu": 0.02403051755364481, "rouge_l": 0.04375896700143472}
{"id": 3110, "code": "def resources ( self ) : seen = set ( ) return [ x for x in self . resources if x not in seen and not seen . add ( x ) ]", "predictions": ["remove leading get get from list ."], "references": ["returns list of unique fragmentresource s by order of first appearance ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 3111, "code": "def to dict ( self ) : return { 'content' : self . content , 'resources' : [ r . asdict ( ) for r in self . resources ] , 'js init fn' : self . js init fn , 'js init version' : self . js init version , 'json init args' : self . json init args }", "predictions": ["converts this descriptor into a value ."], "references": ["returns the fragment in a dictionary representation ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3112, "code": "def from dict ( cls , pods ) : frag = cls ( ) frag . content = pods [ 'content' ] frag . resources = [ Fragment Resource ( * * d ) for d in pods [ 'resources' ] ] frag . js init fn = pods [ 'js init fn' ] frag . js init version = pods [ 'js init version' ] frag . json init args = pods [ 'json init args' ] return frag", "predictions": ["construct this class from a dictionary of arrays . each component is constructed by the given arrays ."], "references": ["returns a new fragment from a dictionary representation ."], "bleu": 0.13400825781778894, "rouge_l": 0.3152454780361757}
{"id": 3113, "code": "def resource to html ( resource ) : if resource . mimetype == \"text/css\" : if resource . kind == \"text\" : return u\"<style type='text/css'>\\n%s\\n</style>\" % resource . data elif resource . kind == \"url\" : return u\"<link rel='stylesheet' href='%s' type='text/css'>\" % resource . data else : raise Exception ( \"Unrecognized resource kind %r\" % resource . kind ) elif resource . mimetype == \"application/javascript\" : if resource . kind == \"text\" : return u\"<script>\\n%s\\n</script>\" % resource . data elif resource . kind == \"url\" : return u\"<script src='%s' type='application/javascript'></script>\" % resource . data else : raise Exception ( \"Unrecognized resource kind %r\" % resource . kind ) elif resource . mimetype == \"text/html\" : assert resource . kind == \"text\" return resource . data else : raise Exception ( \"Unrecognized mimetype %r\" % resource . mimetype )", "predictions": ["convert the delete delete unregistering to ( ."], "references": ["returns resource wrapped in the appropriate html tag for it s mimetype ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 3114, "code": "def get ( self , request , * args , * * kwargs ) : fragment = self . render to fragment ( request , * * kwargs ) response format = request . GET . get ( 'format' ) or request . POST . get ( 'format' ) or 'html' if response format == 'json' or WEB FRAGMENT RESPONSE TYPE in request . META . get ( 'HTTP ACCEPT' , 'text/html' ) : return Json Response ( fragment . to dict ( ) ) else : return self . render standalone response ( request , fragment , * * kwargs )", "predictions": ["a convenience for business the in the request ."], "references": ["render a fragment to html or return json describing it based on the request ."], "bleu": 0.14260771622124252, "rouge_l": 0.3189542483660131}
{"id": 3115, "code": "def render standalone response ( self , request , fragment , * * kwargs ) : if fragment is None : return Http Response ( status = 204 ) html = self . render to standalone html ( request , fragment , * * kwargs ) return Http Response ( html )", "predictions": ["register a httpresponse whose content is filled with the result of calling django ."], "references": ["renders a standalone page as a response for the specified fragment ."], "bleu": 0.10511846841633776, "rouge_l": 0.2340153452685422}
{"id": 3116, "code": "def render to standalone html ( self , request , fragment , * * kwargs ) : template = get template ( STANDALONE TEMPLATE NAME ) context = { 'head html' : fragment . head html ( ) , 'body html' : fragment . body html ( ) , 'foot html' : fragment . foot html ( ) , } return template . render ( context )", "predictions": ["create a self - ) response to the html document ."], "references": ["render the specified fragment to html for a standalone page ."], "bleu": 0.14991106946711685, "rouge_l": 0.2727272727272727}
{"id": 3117, "code": "def calc ( pvalues , lamb ) : m = len ( pvalues ) pi0 = ( pvalues > lamb ) . sum ( ) / ( ( 1 - lamb ) * m ) p FDR = np . ones ( m ) print ( \"p FDR    y        Pr     fast Pow\" ) for i in range ( m ) : y = pvalues [ i ] Pr = max ( 1 , m - i ) / float ( m ) p FDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) print ( i , p FDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) num null = pi0 * m num alt = m - num null num negs = np . array ( range ( m ) ) num pos = m - num negs pp = num pos / float ( m ) qvalues = np . ones ( m ) qvalues [ 0 ] = p FDR [ 0 ] for i in range ( m - 1 ) : qvalues [ i + 1 ] = min ( qvalues [ i ] , p FDR [ i + 1 ] ) sens = ( ( 1.0 - qvalues ) * num pos ) / num alt sens [ sens > 1.0 ] = 1.0 df = pd . Data Frame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = p FDR , percentile positive = pp , sens = sens ) ) df [ \"svalue\" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] return df , num null , m", "predictions": ["calculates the singular value of ("], "references": ["meaning pvalues presorted i descending order"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3118, "code": "def to one dim array ( values , as type = None ) : if isinstance ( values , ( list , tuple ) ) : values = np . array ( values , dtype = np . float32 ) elif isinstance ( values , pd . Series ) : values = values . values values = values . flatten ( ) assert values . ndim == 1 , \"values has wrong dimension\" if as type is not None : return values . astype ( as type ) return values", "predictions": ["convert a set of post post matrices to a set of ("], "references": ["converts list or flattens n - dim array to 1 - dim array if possible"], "bleu": 0.08091975469641616, "rouge_l": 0.07261904761904761}
{"id": 3119, "code": "def lookup values from error table ( scores , err df ) : ix = find nearest matches ( np . float32 ( err df . cutoff . values ) , np . float32 ( scores ) ) return err df . pvalue . iloc [ ix ] . values , err df . svalue . iloc [ ix ] . values , err df . pep . iloc [ ix ] . values , err df . qvalue . iloc [ ix ] . values", "predictions": ["try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to"], "references": ["find matching q - value for each score in scores"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3120, "code": "def summary err table ( df , qvalues = [ 0 , 0.01 , 0.02 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) : qvalues = to one dim array ( qvalues ) ix = find nearest matches ( np . float32 ( df . qvalue . values ) , qvalues ) df sub = df . iloc [ ix ] . copy ( ) for i sub , ( i0 , i1 ) in enumerate ( zip ( ix , ix [ 1 : ] ) ) : if i1 == i0 : df sub . iloc [ i sub + 1 , : ] = None df sub . qvalue = qvalues df sub . reset index ( inplace = True , drop = True ) return df sub [ [ 'qvalue' , 'pvalue' , 'svalue' , 'pep' , 'fdr' , 'fnr' , 'fpr' , 'tp' , 'tn' , 'fp' , 'fn' , 'cutoff' ] ]", "predictions": ["create a update from the given matrices ."], "references": ["summary error table for some typical q - values"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 3121, "code": "def error statistics ( target scores , decoy scores , parametric , pfdr , pi0 lambda , pi0 method = \"smoother\" , pi0 smooth df = 3 , pi0 smooth log pi0 = False , compute lfdr = False , lfdr trunc = True , lfdr monotone = True , lfdr transf = \"probit\" , lfdr adj = 1.5 , lfdr eps = np . power ( 10.0 , - 8 ) ) : target scores = to one dim array ( target scores ) target scores = np . sort ( target scores [ ~ np . isnan ( target scores ) ] ) decoy scores = to one dim array ( decoy scores ) decoy scores = np . sort ( decoy scores [ ~ np . isnan ( decoy scores ) ] ) if parametric : target pvalues = pnorm ( target scores , decoy scores ) else : target pvalues = pemp ( target scores , decoy scores ) pi0 = pi0est ( target pvalues , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 ) target qvalues = qvalue ( target pvalues , pi0 [ 'pi0' ] , pfdr ) metrics = stat metrics ( target pvalues , pi0 [ 'pi0' ] , pfdr ) error stat = pd . Data Frame ( { 'cutoff' : target scores , 'pvalue' : target pvalues , 'qvalue' : target qvalues , 'svalue' : metrics [ 'svalue' ] , 'tp' : metrics [ 'tp' ] , 'fp' : metrics [ 'fp' ] , 'tn' : metrics [ 'tn' ] , 'fn' : metrics [ 'fn' ] , 'fpr' : metrics [ 'fpr' ] , 'fdr' : metrics [ 'fdr' ] , 'fnr' : metrics [ 'fnr' ] } ) if compute lfdr : error stat [ 'pep' ] = lfdr ( target pvalues , pi0 [ 'pi0' ] , lfdr trunc , lfdr monotone , lfdr transf , lfdr adj , lfdr eps ) return error stat , pi0", "predictions": ["compute a list of ( that can be computed from the given origin ."], "references": ["takes list of decoy and target scores and creates error statistics for target values"], "bleu": 0.11633270842295028, "rouge_l": 0.14285714285714285}
{"id": 3122, "code": "def find cutoff ( tt scores , td scores , cutoff fdr , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 ) : error stat , pi0 = error statistics ( tt scores , td scores , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , False ) if not len ( error stat ) : raise click . Click Exception ( \"Too little data for calculating error statistcs.\" ) i0 = ( error stat . qvalue - cutoff fdr ) . abs ( ) . idxmin ( ) cutoff = error stat . iloc [ i0 ] [ \"cutoff\" ] return cutoff", "predictions": ["get the if there is a wrong if the if the if it has been reached ."], "references": ["finds cut off target score for specified false discovery rate fdr"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 3123, "code": "def score ( infile , outfile , classifier , xgb autotune , apply weights , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test ) : if outfile is None : outfile = infile else : outfile = outfile xgb hyperparams = { 'autotune' : xgb autotune , 'autotune num rounds' : 10 , 'num boost round' : 100 , 'early stopping rounds' : 10 , 'test size' : 0.33 } xgb params = { 'eta' : 0.3 , 'gamma' : 0 , 'max depth' : 6 , 'min child weight' : 1 , 'subsample' : 1 , 'colsample bytree' : 1 , 'colsample bylevel' : 1 , 'colsample bynode' : 1 , 'lambda' : 1 , 'alpha' : 0 , 'scale pos weight' : 1 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval metric' : 'auc' } xgb params space = { 'eta' : hp . uniform ( 'eta' , 0.0 , 0.3 ) , 'gamma' : hp . uniform ( 'gamma' , 0.0 , 0.5 ) , 'max depth' : hp . quniform ( 'max depth' , 2 , 8 , 1 ) , 'min child weight' : hp . quniform ( 'min child weight' , 1 , 5 , 1 ) , 'subsample' : 1 , 'colsample bytree' : 1 , 'colsample bylevel' : 1 , 'colsample bynode' : 1 , 'lambda' : hp . uniform ( 'lambda' , 0.0 , 1.0 ) , 'alpha' : hp . uniform ( 'alpha' , 0.0 , 1.0 ) , 'scale pos weight' : 1.0 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval metric' : 'auc' } if not apply weights : Py Prophet Learner ( infile , outfile , classifier , xgb hyperparams , xgb params , xgb params space , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test ) . run ( ) else : Py Prophet Weight Applier ( infile , outfile , classifier , xgb hyperparams , xgb params , xgb params space , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test , apply weights ) . run ( )", "predictions": ["update the update to the given ( ."], "references": ["conduct semi - supervised learning and error - rate estimation for ms1 ms2 and transition - level data ."], "bleu": 0.04054685178922986, "rouge_l": 0.06900452488687782}
{"id": 3124, "code": "def ipf ( infile , outfile , ipf ms1 scoring , ipf ms2 scoring , ipf h0 , ipf grouped fdr , ipf max precursor pep , ipf max peakgroup pep , ipf max precursor peakgroup pep , ipf max transition pep ) : if outfile is None : outfile = infile else : outfile = outfile infer peptidoforms ( infile , outfile , ipf ms1 scoring , ipf ms2 scoring , ipf h0 , ipf grouped fdr , ipf max precursor pep , ipf max peakgroup pep , ipf max precursor peakgroup pep , ipf max transition pep )", "predictions": ["create a new instance of ( ."], "references": ["infer peptidoforms after scoring of ms1 ms2 and transition - level data ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 3125, "code": "def peptide ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps ) : if outfile is None : outfile = infile else : outfile = outfile infer peptides ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps )", "predictions": ["this is a utility method to convert to a native call ."], "references": ["infer peptides and conduct error - rate estimation in different contexts ."], "bleu": 0.10390302174233558, "rouge_l": 0.08333333333333333}
{"id": 3126, "code": "def protein ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps ) : if outfile is None : outfile = infile else : outfile = outfile infer proteins ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps )", "predictions": ["create a new surfacedata ."], "references": ["infer proteins and conduct error - rate estimation in different contexts ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 3127, "code": "def subsample ( infile , outfile , subsample ratio , test ) : if outfile is None : outfile = infile else : outfile = outfile subsample osw ( infile , outfile , subsample ratio , test )", "predictions": ["create generator with default charset ."], "references": ["subsample openswath file to minimum for integrated scoring"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 3128, "code": "def reduce ( infile , outfile ) : if outfile is None : outfile = infile else : outfile = outfile reduce osw ( infile , outfile )", "predictions": ["serve ( : serve : , : , : , : , : , : , : , id : , id . , id . , id . , image . , id . , . , id . , image . , image . , image . ,"], "references": ["reduce scored pyprophet file to minimum for global scoring"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3129, "code": "def backpropagate ( infile , outfile , apply scores ) : if outfile is None : outfile = infile else : outfile = outfile backpropagate oswr ( infile , outfile , apply scores )", "predictions": ["utility method that uses the ( ."], "references": ["backpropagate multi - run peptide and protein scores to single files"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 3130, "code": "def create group ( self , group ) : self . valid group id ( group . id ) body = { \"data\" : group . json data ( ) } url = \"{}/group/{}\" . format ( self . API , group . name ) data = self . put resource ( url , headers = { } , body = body ) return self . group from json ( data . get ( \"data\" ) )", "predictions": ["creates an all of the specified all of the ui processes ."], "references": ["creates a group from the passed restclients . group object ."], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 3131, "code": "def delete group ( self , group id ) : self . valid group id ( group id ) url = \"{}/group/{}\" . format ( self . API , group id ) self . delete resource ( url ) return True", "predictions": ["deletes a mirror from the database ."], "references": ["deletes the group identified by the passed group id ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 3132, "code": "def is effective member ( self , group id , netid ) : self . valid group id ( group id ) netid = re . sub ( '@washington.edu' , '' , netid ) url = \"{}/group/{}/effective member/{}\" . format ( self . API , group id , netid ) try : data = self . get resource ( url ) return True except Data Failure Exception as ex : if ex . status == 404 : return False else : raise", "predictions": ["check if this ( or not part of a ( service . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["returns true if the netid is in the group false otherwise ."], "bleu": 0.026594139297659906, "rouge_l": 0.07253269916765755}
{"id": 3133, "code": "def parse version ( ) : from os . path import dirname , join import ast modname = setupkw [ 'name' ] init fpath = join ( dirname ( file ) , modname , ' init .py' ) with open ( init fpath ) as file : sourcecode = file . read ( ) pt = ast . parse ( sourcecode ) class Version Visitor ( ast . Node Visitor ) : def visit Assign ( self , node ) : for target in node . targets : if target . id == ' version ' : self . version = node . value . s visitor = Version Visitor ( ) visitor . visit ( pt ) return visitor . version", "predictions": ["builds the loop loop from the given loop ."], "references": ["statically parse the version number from __init__ . py"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 3134, "code": "def create container ( context , path , l mtime , size ) : new context = context . copy ( ) new context . input = None new context . headers = None new context . query = None container = path . split ( '/' , 1 ) [ 0 ] + ' segments' cli put container ( new context , container ) prefix = container + '/' + path . split ( '/' , 1 ) [ 1 ] prefix = '%s/%s/%s/' % ( prefix , l mtime , size ) return prefix", "predictions": ["creates a new error error ."], "references": ["creates container for segments of file with path"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3135, "code": "def is empty ( self ) : something = self . read ( 1 ) if something : if self . buf : self . buf = something + self . buf else : self . buf = something return False else : return True", "predictions": ["tell if this session is in a data clip ."], "references": ["check whether the file is empty reading the single byte ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 3136, "code": "def forwards ( self , orm ) : for title in orm [ 'hero slider.Slider Item Title' ] . objects . all ( ) : title . is published = True title . save ( )", "predictions": ["generate a title with the specified orm ."], "references": ["write your forwards methods here ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3137, "code": "def get slider items ( context , amount = None ) : req = context . get ( 'request' ) qs = Slider Item . objects . published ( req ) . order by ( 'position' ) if amount : qs = qs [ : amount ] return qs", "predictions": ["get slider from the app by the query ."], "references": ["returns the published slider items ."], "bleu": 0.16784459625186196, "rouge_l": 0.27664399092970515}
{"id": 3138, "code": "def render hero slider ( context ) : req = context . get ( 'request' ) qs = Slider Item . objects . published ( req ) . order by ( 'position' ) return { 'slider items' : qs , }", "predictions": ["render a slider based on the query string ."], "references": ["renders the hero slider ."], "bleu": 0.16784459625186196, "rouge_l": 0.3012345679012346}
{"id": 3139, "code": "def reader acquire ( self ) : self . order mutex . acquire ( ) self . readers mutex . acquire ( ) if self . readers == 0 : self . access mutex . acquire ( ) self . readers += 1 self . order mutex . release ( ) self . readers mutex . release ( )", "predictions": ["acquire a previously saved reader ."], "references": ["acquire the lock to read"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3140, "code": "def reader release ( self ) : self . readers mutex . acquire ( ) self . readers -= 1 if self . readers == 0 : self . access mutex . release ( ) self . readers mutex . release ( )", "predictions": ["release a reader for the previously saved reader ."], "references": ["release the lock after reading"], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 3141, "code": "def writer acquire ( self ) : self . order mutex . acquire ( ) self . access mutex . acquire ( ) self . order mutex . release ( )", "predictions": ["create a new instance of this class ."], "references": ["acquire the lock to write"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3142, "code": "def tasks ( self ) : self . rwlock . reader acquire ( ) tl = [ v for v in self . tasks . values ( ) ] tl . sort ( key = lambda x : x . task id ) self . rwlock . reader release ( ) return tl", "predictions": ["get a previously saved task ."], "references": ["get the list of tasks"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3143, "code": "def to dict ( self ) : properties = find class properties ( self . class ) config = { name : self . getattribute ( name ) for name , in properties } return config", "predictions": ["convert a class to a dict ."], "references": ["returns a dict with the representation of this task configuration object ."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 3144, "code": "def create index ( idx url , clean = False ) : try : r = requests . get ( idx url ) except requests . exceptions . Connection Error : cause = \"Error connecting to Elastic Search (index: %s)\" % idx url raise Elastic Search Error ( cause = cause ) if r . status code != 200 : r = requests . put ( idx url ) if r . status code != 200 : logger . info ( \"Can't create index %s (%s)\" , idx url , r . status code ) cause = \"Error creating Elastic Search index %s\" % idx url raise Elastic Search Error ( cause = cause ) logger . info ( \"Index %s created\" , idx url ) return True elif r . status code == 200 and clean : requests . delete ( idx url ) requests . put ( idx url ) logger . info ( \"Index deleted and created (index: %s)\" , idx url ) return True return False", "predictions": ["create an index on the specified url ."], "references": ["configure the index to work with"], "bleu": 0.17747405280050269, "rouge_l": 0.14663461538461536}
{"id": 3145, "code": "def json encoder ( * args , * * kwargs ) : obj = cherrypy . serving . request . json inner handler ( * args , * * kwargs ) for chunk in JSON Encoder ( ) . iterencode ( obj ) : yield chunk . encode ( 'utf-8' )", "predictions": ["encoder using a json encoded input data ."], "references": ["custom json encoder handler"], "bleu": 0.17747405280050269, "rouge_l": 0.17732558139534885}
{"id": 3146, "code": "def items ( self ) : pipe = self . conn . pipeline ( ) pipe . lrange ( Q STORAGE ITEMS , 0 , - 1 ) pipe . ltrim ( Q STORAGE ITEMS , 1 , 0 ) items = pipe . execute ( ) [ 0 ] for item in items : item = pickle . loads ( item ) yield item", "predictions": ["create a pipe ."], "references": ["get the items fetched by the jobs ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 3147, "code": "def validate args ( task id , backend , category , backend args ) : if not task id or task id . strip ( ) == \"\" : msg = \"Missing task id for task\" raise Value Error ( msg ) if not backend or backend . strip ( ) == \"\" : msg = \"Missing backend for task '%s'\" % task id raise Value Error ( msg ) if backend args and not isinstance ( backend args , dict ) : msg = \"Backend args is not a dict, task '%s'\" % task id raise Value Error ( msg ) if not category or category . strip ( ) == \"\" : msg = \"Missing category for task '%s'\" % task id raise Value Error ( msg )", "predictions": ["validate the task or category ( ."], "references": ["check that the task arguments received are valid"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 3148, "code": "def parse archive args ( self , archive args ) : if not archive args : return None archiving args = copy . deepcopy ( archive args ) if self . archive path : archiving args [ 'archive path' ] = self . archive path else : archiving args [ 'archive path' ] = os . path . expanduser ( ARCHIVES DEFAULT PATH ) return Archiving Task Config . from dict ( archiving args )", "predictions": ["parses an archive into a location for the request ."], "references": ["parse the archive arguments of a task"], "bleu": 0.14991106946711685, "rouge_l": 0.24302788844621517}
{"id": 3149, "code": "def schedule job task ( self , queue id , task id , job args , delay = 0 ) : self . rwlock . writer acquire ( ) job id = self . generate job id ( task id ) event = self . scheduler . enter ( delay , 1 , self . enqueue job , argument = ( queue id , job id , job args , ) ) self . jobs [ job id ] = event self . tasks [ task id ] = job id self . rwlock . writer release ( ) logging . debug ( \"Job #%s (task: %s) scheduled on %s (wait: %s)\" , job id , task id , queue id , delay ) return job id", "predictions": ["schedule a scheduled job ."], "references": ["schedule a job in the given queue ."], "bleu": 0.22405141222206199, "rouge_l": 0.5907990314769976}
{"id": 3150, "code": "def cancel job task ( self , task id ) : try : self . rwlock . writer acquire ( ) job id = self . tasks . get ( task id , None ) if job id : self . cancel job ( job id ) else : logger . warning ( \"Task %s set to be removed was not found\" , task id ) finally : self . rwlock . writer release ( )", "predictions": ["cancels the specified job ."], "references": ["cancel the job related to the given task ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 3151, "code": "def run ( self ) : try : self . listen ( ) except Exception as e : logger . critical ( \"Job Listener instence crashed. Error: %s\" , str ( e ) ) logger . critical ( traceback . format exc ( ) )", "predictions": ["run the full traceback ."], "references": ["run thread to listen for jobs and reschedule successful ones ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 3152, "code": "def listen ( self ) : pubsub = self . conn . pubsub ( ) pubsub . subscribe ( self . pubsub channel ) logger . debug ( \"Listening on channel %s\" , self . pubsub channel ) for msg in pubsub . listen ( ) : logger . debug ( \"New message received of type %s\" , str ( msg [ 'type' ] ) ) if msg [ 'type' ] != 'message' : logger . debug ( \"Ignoring job message\" ) continue data = pickle . loads ( msg [ 'data' ] ) job id = data [ 'job id' ] job = rq . job . Job . fetch ( job id , connection = self . conn ) if data [ 'status' ] == 'finished' : logging . debug ( \"Job #%s completed\" , job id ) handler = self . result handler elif data [ 'status' ] == 'failed' : logging . debug ( \"Job #%s failed\" , job id ) handler = self . result handler err else : continue if handler : logging . debug ( \"Calling handler for job #%s\" , job id ) handler ( job )", "predictions": ["for each job . this will only subscribe once the job is saved ."], "references": ["listen for completed jobs and reschedule successful ones ."], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 3153, "code": "def schedule ( self ) : if self . async mode : self . scheduler . start ( ) self . listener . start ( ) else : self . scheduler . schedule ( )", "predictions": ["schedule this object to the specific event queue ."], "references": ["start scheduling jobs ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3154, "code": "def build job arguments ( task ) : job args = { } job args [ 'qitems' ] = Q STORAGE ITEMS job args [ 'task id' ] = task . task id job args [ 'backend' ] = task . backend backend args = copy . deepcopy ( task . backend args ) if 'next from date' in backend args : backend args [ 'from date' ] = backend args . pop ( 'next from date' ) if 'next offset' in backend args : backend args [ 'offset' ] = backend args . pop ( 'next offset' ) job args [ 'backend args' ] = backend args job args [ 'category' ] = task . category archiving cfg = task . archiving cfg job args [ 'archive args' ] = archiving cfg . to dict ( ) if archiving cfg else None sched cfg = task . scheduling cfg job args [ 'max retries' ] = sched cfg . max retries if sched cfg else MAX JOB RETRIES return job args", "predictions": ["build a task for taking a task and initializes it ."], "references": ["build the set of arguments required for running a job"], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 3155, "code": "def reverse action ( self , url name , * args , * * kwargs ) : if self . request and not self . request . version : return reverse ( self . get url name ( url name ) , * args , * * kwargs ) return super ( ) . reverse action ( url name , * args , * * kwargs )", "predictions": ["reverses this request for the given url ."], "references": ["extended drf with fallback to requested namespace if request . version is missing"], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 3156, "code": "def get version ( version = None ) : if version is None : version = VERSION assert len ( version ) == 5 assert version [ 3 ] in ( \"alpha\" , \"beta\" , \"rc\" , \"final\" ) parts = 2 if version [ 2 ] == 0 else 3 main = \".\" . join ( str ( x ) for x in version [ : parts ] ) sub = \"\" if version [ 3 ] != \"final\" : mapping = { \"alpha\" : \"a\" , \"beta\" : \"b\" , \"rc\" : \"c\" } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return main + sub", "predictions": ["extract ( version from version ."], "references": ["derives a pep386 - compliant version number from version ."], "bleu": 0.2231931376573339, "rouge_l": 0.47843137254901963}
{"id": 3157, "code": "def create ( self , request ) : login form = Authentication Form ( request , data = request . data ) if not login form . is valid ( ) : raise serializers . Validation Error ( login form . errors ) auth login ( request , login form . get user ( ) ) serializer = User Serializer ( request . user ) return Response ( serializer . data , status = status . HTTP 200 OK )", "predictions": ["create a new user ."], "references": ["log in django staff user"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3158, "code": "def list ( self , request ) : serializer = self . get serializer ( request . user ) return Response ( serializer . data , status = status . HTTP 200 OK )", "predictions": ["returns a list of serializer for the specified user ."], "references": ["retrieve logged in user info"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 3159, "code": "def create ( self , request ) : password form = Password Change Form ( request . user , data = request . data ) if not password form . is valid ( ) : raise serializers . Validation Error ( password form . errors ) password form . save ( ) update session auth hash ( request , password form . user ) return Response ( status = status . HTTP 204 NO CONTENT )", "predictions": ["creates a password change ."], "references": ["change password for logged in django staff user"], "bleu": 0.1658165975077607, "rouge_l": 0.1476997578692494}
{"id": 3160, "code": "def create field ( field info ) : field type = field info . get ( 'type' ) if field type not in FIELDS NAME MAP : raise Value Error ( ( 'not support this field: {}' ) . format ( field type ) ) field class = FIELDS NAME MAP . get ( field type ) params = dict ( field info ) params . pop ( 'type' ) return field class . from dict ( params )", "predictions": ["creates a ( from a ("], "references": ["create a field by field info dict ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3161, "code": "def change logging kwargs ( kwargs ) : log levels = kwargs . pop ( 'log level' , None ) log folder = kwargs . pop ( 'log folder' , 'logs' ) logger names = kwargs . pop ( 'logger names' , '' ) if log levels is None : log levels = kwargs . pop ( 'log levels' , logging . INFO ) log multiproc = kwargs . pop ( 'log multiproc' , True ) if not isinstance ( logger names , ( tuple , list ) ) : logger names = [ logger names ] if not isinstance ( log levels , ( tuple , list ) ) : log levels = [ log levels ] if len ( log levels ) == 1 : log levels = [ log levels [ 0 ] for in logger names ] dictionary = copy . deepcopy ( LOGGING DICT ) prefixes = [ '' ] if not log multiproc : for key in list ( dictionary . keys ( ) ) : if key . startswith ( 'multiproc ' ) : del dictionary [ key ] else : prefixes . append ( 'multiproc ' ) for prefix in prefixes : for handler dict in dictionary [ prefix + 'handlers' ] . values ( ) : if 'filename' in handler dict : filename = os . path . join ( log folder , handler dict [ 'filename' ] ) filename = os . path . normpath ( filename ) handler dict [ 'filename' ] = filename dictionary [ prefix + 'loggers' ] = { } logger dict = dictionary [ prefix + 'loggers' ] for idx , logger name in enumerate ( logger names ) : logger dict [ logger name ] = { 'level' : log levels [ idx ] , 'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) } kwargs [ 'log config' ] = dictionary", "predictions": ["when . is set , change all the stored values ."], "references": ["helper function to turn the simple logging kwargs into a log_config ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 3162, "code": "def get strings ( args ) : string list = [ ] for elem in ast . walk ( ast . parse ( args ) ) : if isinstance ( elem , ast . Str ) : string list . append ( elem . s ) return string list", "predictions": ["gets the comma separated list of strings ."], "references": ["returns all valid python strings inside a given argument string ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 3163, "code": "def extract replacements ( self , trajectory ) : self . env name = trajectory . v environment name self . traj name = trajectory . v name self . set name = trajectory . f wildcard ( '$set' ) self . run name = trajectory . f wildcard ( '$' )", "predictions": ["extracts the environment from the given object ."], "references": ["extracts the wildcards and file replacements from the trajectory"], "bleu": 0.23420197753909952, "rouge_l": 0.465648854961832}
{"id": 3164, "code": "def parser to string io ( parser ) : memory file = String IO ( ) parser . write ( memory file ) memory file . flush ( ) memory file . seek ( 0 ) return memory file", "predictions": ["convert a string to a file object ."], "references": ["turns a configparser into a stringio stream ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 3165, "code": "def make logging handlers and tools ( self , multiproc = False ) : log stdout = self . log stdout if sys . stdout is self . stdout to logger : log stdout = False if self . log config : if multiproc : proc log config = self . mp config else : proc log config = self . sp config if proc log config : if isinstance ( proc log config , dict ) : new dict = self . handle dict config ( proc log config ) dict Config ( new dict ) else : parser = self . handle config parsing ( proc log config ) memory file = self . parser to string io ( parser ) file Config ( memory file , disable existing loggers = False ) if log stdout : std name , std level = self . log stdout stdout = Stdout To Logger ( std name , log level = std level ) stdout . start ( ) self . tools . append ( stdout )", "predictions": ["creates the logging for this object ."], "references": ["creates logging handlers and redirects stdout ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 3166, "code": "def finalize ( self , remove all handlers = True ) : for tool in self . tools : tool . finalize ( ) self . tools = [ ] self . stdout to logger = None for config in ( self . sp config , self . mp config ) : if hasattr ( config , 'close' ) : config . close ( ) self . sp config = None self . mp config = None if remove all handlers : self . tabula rasa ( )", "predictions": ["iterates through all the managed managed classes and removes them ."], "references": ["finalizes the manager closes and removes all handlers if desired ."], "bleu": 0.17827531042796255, "rouge_l": 0.36363636363636365}
{"id": 3167, "code": "def start ( self ) : if sys . stdout is not self : self . original steam = sys . stdout sys . stdout = self self . redirection = True if self . redirection : print ( 'Established redirection of `stdout`.' )", "predictions": ["start the shutdown method ."], "references": ["starts redirection of stdout"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3168, "code": "def write ( self , buf ) : if not self . recursion : self . recursion = True try : for line in buf . rstrip ( ) . splitlines ( ) : self . logger . log ( self . log level , line . rstrip ( ) ) finally : self . recursion = False else : sys . stderr . write ( 'ERROR: Recursion in Stream redirection!' )", "predictions": ["writes ( or more lines ) to this ray object"], "references": ["writes data from buffer to logger"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 3169, "code": "def get all attributes ( instance ) : try : result dict = instance . dict . copy ( ) except Attribute Error : result dict = { } if hasattr ( instance , ' all slots ' ) : all slots = instance . all slots else : all slots = slots . get all slots ( instance . class ) for slot in all slots : result dict [ slot ] = getattr ( instance , slot ) result dict . pop ( ' dict ' , None ) result dict . pop ( ' weakref ' , None ) return result dict", "predictions": ["return all attributes of this function ."], "references": ["returns an attribute value dictionary much like __dict__ but incorporates __slots__"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 3170, "code": "def kwargs mutual exclusive ( param1 name , param2 name , map2to1 = None ) : def wrapper ( func ) : @ functools . wraps ( func ) def new func ( * args , * * kwargs ) : if param2 name in kwargs : if param1 name in kwargs : raise Value Error ( 'You cannot specify `%s` and `%s` at the same time, ' 'they are mutually exclusive.' % ( param1 name , param2 name ) ) param2 = kwargs . pop ( param2 name ) if map2to1 is not None : param1 = map2to1 ( param2 ) else : param1 = param2 kwargs [ param1 name ] = param1 return func ( * args , * * kwargs ) return new func return wrapper", "predictions": ["automatically return method with given ( ."], "references": ["if there exist mutually exclusive parameters checks for them and maps param2 to 1 ."], "bleu": 0.059237077985967744, "rouge_l": 0.08531468531468532}
{"id": 3171, "code": "def not in run ( func ) : doc = func . doc na string = '''\\n ATTENTION: This function is not available during a single run!\\n''' if doc is not None : func . doc = '\\n' . join ( [ doc , na string ] ) func . not in run = True @ functools . wraps ( func ) def new func ( self , * args , * * kwargs ) : if self . is run : raise Type Error ( 'Function `%s` is not available during a single run.' % func . name ) return func ( self , * args , * * kwargs ) return new func", "predictions": ["decorator that logs how long a function runs in a ( : : : : : / / www . wikipedia . org / tr / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( /"], "references": ["this is a decorator that signaling that a function is not available during a single run ."], "bleu": 0.04325740366009962, "rouge_l": 0.19656283566058003}
{"id": 3172, "code": "def with open store ( func ) : doc = func . doc na string = '''\\n ATTENTION: This function can only be used if the store is open!\\n''' if doc is not None : func . doc = '\\n' . join ( [ doc , na string ] ) func . with open store = True @ functools . wraps ( func ) def new func ( self , * args , * * kwargs ) : if not self . traj . v storage service . is open : raise Type Error ( 'Function `%s` is only available if the storage is open.' % func . name ) return func ( self , * args , * * kwargs ) return new func", "predictions": ["decorator which ensures the user has access to the ( ( ( ( ( ( ( : ( readers : ( : ( readers : ( readers : ( readers : readers : ( readers : ( : readers : readers : readers : ( readers : ( readers :"], "references": ["this is a decorator that signaling that a function is only available if the storage is open ."], "bleu": 0.026594139297659906, "rouge_l": 0.0642781875658588}
{"id": 3173, "code": "def prefix naming ( cls ) : if hasattr ( cls , ' getattr ' ) : raise Type Error ( ' getattr  already defined' ) cls . getattr = prfx getattr cls . setattr = prfx setattr return cls", "predictions": ["gets writer for an element ."], "references": ["decorate that adds the prefix naming scheme"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 3174, "code": "def add params ( traj ) : traj . v standard parameter = Brian2Parameter traj . v fast access = True traj . f add parameter ( 'Net.C' , 281 * p F ) traj . f add parameter ( 'Net.g L' , 30 * n S ) traj . f add parameter ( 'Net.EL' , - 70.6 * m V ) traj . f add parameter ( 'Net.VT' , - 50.4 * m V ) traj . f add parameter ( 'Net.Delta T' , 2 * m V ) traj . f add parameter ( 'Net.tauw' , 40 * ms ) traj . f add parameter ( 'Net.a' , 4 * n S ) traj . f add parameter ( 'Net.b' , 0.08 * n A ) traj . f add parameter ( 'Net.I' , .8 * n A ) traj . f add parameter ( 'Net.Vcut' , 'vm > 0*m V' ) traj . f add parameter ( 'Net.N' , 50 ) eqs = traj . f add parameter ( 'Net.eqs' , eqs ) traj . f add parameter ( 'reset' , 'vm=Vr;w+=b' )", "predictions": ["creates a new ( ."], "references": ["adds all necessary parameters to traj ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3175, "code": "def run net ( traj ) : eqs = traj . eqs namespace = traj . Net . f to dict ( short names = True , fast access = True ) neuron = Neuron Group ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) neuron . vm = traj . EL neuron . w = traj . a * ( neuron . vm - traj . EL ) neuron . Vr = linspace ( - 48.3 * m V , - 47.7 * m V , traj . N ) print ( 'Initial Run' ) net = Network ( neuron ) net . run ( 100 * ms , report = 'text' ) M Spike = Spike Monitor ( neuron ) net . add ( M Spike ) M State V = State Monitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) net . add ( M State V ) print ( 'Measurement run' ) net . run ( 500 * ms , report = 'text' ) traj . v standard result = Brian2Monitor Result traj . f add result ( 'Spike Monitor' , M Spike ) traj . f add result ( 'State Monitor V' , M State V )", "predictions": ["to to to to run the analysis ."], "references": ["creates and runs brian network based on the parameters in traj ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3176, "code": "def add parameters ( traj ) : traj . f add parameter ( 'steps' , 10000 , comment = 'Number of time steps to simulate' ) traj . f add parameter ( 'dt' , 0.01 , comment = 'Step size' ) traj . f add parameter ( Array Parameter , 'initial conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) traj . f add parameter ( 'func params.sigma' , 10.0 ) traj . f add parameter ( 'func params.beta' , 8.0 / 3.0 ) traj . f add parameter ( 'func params.rho' , 28.0 ) #For the fun of it we will annotate the  group traj . func params . v annotations . info = 'This group contains as default the original values chosen ' 'by Edward Lorenz in 1963. Check it out on wikipedia ' '(https://en.wikipedia.org/wiki/Lorenz attractor)!'", "predictions": ["creates a new fun object"], "references": ["adds all necessary parameters to the traj container"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3177, "code": "def create storage ( storage service , trajectory = None , * * kwargs ) : kwargs copy = kwargs . copy ( ) kwargs copy [ 'trajectory' ] = trajectory matching kwargs = get matching kwargs ( storage service , kwargs copy ) storage service = storage service ( * * matching kwargs ) unused kwargs = set ( kwargs . keys ( ) ) - set ( matching kwargs . keys ( ) ) return storage service , unused kwargs", "predictions": ["json is the same as ( ( = ( = ( ( = ( = ( = ( ( = ( ( = ( = ( ( = ( = ( ( = ( = ( ( = ( = ( ( = ( = ( = ( = ("], "references": ["creates a service from a constructor and checks which kwargs are not used"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3178, "code": "def add parameters ( traj ) : assert ( isinstance ( traj , Trajectory ) ) scale = traj . simulation . scale traj . v standard parameter = Brian2Parameter model eqs = conn eqs = traj . f add parameter ( 'model.eqs' , model eqs , comment = 'The differential equation for the neuron model' ) traj . f add parameter ( 'model.synaptic.eqs' , conn eqs , comment = 'The differential equation for the synapses. ' 'PRE will be replaced by `i` or `e` depending ' 'on the source population' ) traj . f add parameter ( 'model.synaptic.tau1' , 1 * ms , comment = 'The decay time' ) traj . f add parameter ( 'model.synaptic.tau2 e' , 3 * ms , comment = 'The rise time, excitatory' ) traj . f add parameter ( 'model.synaptic.tau2 i' , 2 * ms , comment = 'The rise time, inhibitory' ) traj . f add parameter ( 'model.V th' , 'V >= 1.0' , comment = \"Threshold value\" ) traj . f add parameter ( 'model.reset func' , 'V=0.0' , comment = \"String representation of reset function\" ) traj . f add parameter ( 'model.refractory' , 5 * ms , comment = \"Absolute refractory period\" ) traj . f add parameter ( 'model.N e' , int ( 2000 * scale ) , comment = \"Amount of excitatory neurons\" ) traj . f add parameter ( 'model.N i' , int ( 500 * scale ) , comment = \"Amount of inhibitory neurons\" ) traj . f add parameter ( 'model.tau e' , 15 * ms , comment = \"Membrane time constant, excitatory\" ) traj . f add parameter ( 'model.tau i' , 10 * ms , comment = \"Membrane time constant, inhibitory\" ) traj . f add parameter ( 'model.mu e min' , 1.1 , comment = \"Lower bound for bias, excitatory\" ) traj . f add parameter ( 'model.mu e max' , 1.2 , comment = \"Upper bound for bias, excitatory\" ) traj . f add parameter ( 'model.mu i min' , 1.0 , comment = \"Lower bound for bias, inhibitory\" ) traj . f add parameter ( 'model.mu i max' , 1.05 , comment = \"Upper bound for bias, inhibitory\" )", "predictions": ["items are the same as the ( but . ."], "references": ["adds all neuron group parameters to traj ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 3179, "code": "def add parameters ( traj ) : assert ( isinstance ( traj , Trajectory ) ) traj . v standard parameter = Brian2Parameter scale = traj . simulation . scale traj . f add parameter ( 'connections.R ee' , 1.0 , comment = 'Scaling factor for clustering' ) traj . f add parameter ( 'connections.clustersize e' , 100 , comment = 'Size of a cluster' ) traj . f add parameter ( 'connections.strength factor' , 2.5 , comment = 'Factor for scaling cluster weights' ) traj . f add parameter ( 'connections.p ii' , 0.25 , comment = 'Connection probability from inhibitory to inhibitory' ) traj . f add parameter ( 'connections.p ei' , 0.25 , comment = 'Connection probability from inhibitory to excitatory' ) traj . f add parameter ( 'connections.p ie' , 0.25 , comment = 'Connection probability from excitatory to inhibitory' ) traj . f add parameter ( 'connections.p ee' , 0.1 , comment = 'Connection probability from excitatory to excitatory' ) traj . f add parameter ( 'connections.J ii' , 0.027 / np . sqrt ( scale ) , comment = 'Connection strength from inhibitory to inhibitory' ) traj . f add parameter ( 'connections.J ei' , 0.032 / np . sqrt ( scale ) , comment = 'Connection strength from inhibitory to excitatroy' ) traj . f add parameter ( 'connections.J ie' , 0.009 / np . sqrt ( scale ) , comment = 'Connection strength from excitatory to inhibitory' ) traj . f add parameter ( 'connections.J ee' , 0.012 / np . sqrt ( scale ) , comment = 'Connection strength from excitatory to excitatory' )", "predictions": ["creates a new strength object ."], "references": ["adds all neuron group parameters to traj ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3180, "code": "def add parameters ( self , traj ) : par = traj . f add parameter ( Brian2Parameter , 'simulation.durations.initial run' , 500 * ms , comment = 'Initialisation run for more realistic ' 'measurement conditions.' ) par . v annotations . order = 0 par = traj . f add parameter ( Brian2Parameter , 'simulation.durations.measurement run' , 1500 * ms , comment = 'Measurement run that is considered for ' 'statistical evaluation' ) par . v annotations . order = 1", "predictions": ["parse the given ( ."], "references": ["adds all necessary parameters to traj container ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3181, "code": "def add monitors ( self , traj , network , network dict ) : neurons e = network dict [ 'neurons e' ] monitor list = [ ] self . spike monitor = Spike Monitor ( neurons e ) monitor list . append ( self . spike monitor ) self . V monitor = State Monitor ( neurons e , 'V' , record = list ( traj . neuron records ) ) monitor list . append ( self . V monitor ) self . I syn e monitor = State Monitor ( neurons e , 'I syn e' , record = list ( traj . neuron records ) ) monitor list . append ( self . I syn e monitor ) self . I syn i monitor = State Monitor ( neurons e , 'I syn i' , record = list ( traj . neuron records ) ) monitor list . append ( self . I syn i monitor ) network . add ( * monitor list ) network dict [ 'monitors' ] = monitor list", "predictions": ["schedule the in acquire of the threads in a queue ."], "references": ["adds monitors to the network"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 3182, "code": "def plot result ( self , traj , result name ) : result = traj . f get ( result name ) varname = result . record variables [ 0 ] values = result [ varname ] times = result . t record = result . record for idx , celia neuron in enumerate ( record ) : plt . subplot ( len ( record ) , 1 , idx + 1 ) plt . plot ( times , values [ idx , : ] ) if idx == 0 : plt . title ( '%s' % varname ) if idx == 1 : plt . ylabel ( '%s' % ( varname ) ) if idx == len ( record ) - 1 : plt . xlabel ( 't' )", "predictions": ["generate our job from this job ."], "references": ["plots a state variable graph for several neurons into one figure"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 3183, "code": "def print graphs ( self , traj ) : print folder = self . make folder ( traj ) plt . figure ( ) plt . scatter ( self . spike monitor . t , self . spike monitor . i , s = 1 ) plt . xlabel ( 't' ) plt . ylabel ( 'Exc. Neurons' ) plt . title ( 'Spike Raster Plot' ) filename = os . path . join ( print folder , 'spike.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) fig = plt . figure ( ) self . plot result ( traj , 'monitors.V' ) filename = os . path . join ( print folder , 'V.png' ) print ( 'Current plot: %s ' % filename ) fig . savefig ( filename ) plt . close ( ) plt . figure ( ) self . plot result ( traj , 'monitors.I syn e' ) filename = os . path . join ( print folder , 'I syn e.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) plt . figure ( ) self . plot result ( traj , 'monitors.I syn i' ) filename = os . path . join ( print folder , 'I syn i.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) if not traj . analysis . show plots : plt . close ( 'all' ) else : plt . show ( )", "predictions": ["run all threads in this imap object ."], "references": ["makes some plots and stores them into subfolders"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3184, "code": "def get batch ( ) : optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) batch = 0 for o , a in optlist : if o == '--batch' : batch = int ( a ) print ( 'Found batch %d' % batch ) return batch", "predictions": ["gets all pubsub in a ( possibly . ) with a ( ( ) ."], "references": ["function that parses the batch id from the command line arguments"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3185, "code": "def explore batch ( traj , batch ) : explore dict = { } explore dict [ 'sigma' ] = np . arange ( 10.0 * batch , 10.0 * ( batch + 1 ) , 1.0 ) . tolist ( ) traj . f explore ( explore dict )", "predictions": ["instantiates a ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( w if if if set if the if the if the if the if found if enabled if"], "references": ["chooses exploration according to batch"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3186, "code": "def vars ( self ) : if self . vars is None : self . vars = NN Tree Node Vars ( self ) return self . vars", "predictions": ["a simple return method for making the multi - context variables ."], "references": ["alternative naming you can use node . vars . name instead of node . v_name"], "bleu": 0.08091975469641616, "rouge_l": 0.07261904761904761}
{"id": 3187, "code": "def func ( self ) : if self . func is None : self . func = NN Tree Node Func ( self ) return self . func", "predictions": ["get method for notifications"], "references": ["alternative naming you can use node . func . name instead of node . f_func"], "bleu": 0.01931500670555844, "rouge_l": 0.0}
{"id": 3188, "code": "def rename ( self , full name ) : self . full name = full name if full name : self . name = full name . rsplit ( '.' , 1 ) [ - 1 ]", "predictions": ["get a shallow copy of this object ."], "references": ["renames the tree node"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3189, "code": "def set details ( self , depth , branch , run branch ) : self . depth = depth self . branch = branch self . run branch = run branch", "predictions": ["a helper method to create a new ( ."], "references": ["sets some details for internal handling ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3190, "code": "def determine types ( start node , first name , add leaf , add link ) : if start node . v is root : where = first name else : where = start node . branch if where in SUBTREE MAPPING : type tuple = SUBTREE MAPPING [ where ] else : type tuple = ( GROUP , LEAF ) if add link : return type tuple [ 0 ] , LINK if add leaf : return type tuple else : return type tuple [ 0 ] , type tuple [ 0 ]", "predictions": ["determines the ( possibly perpendicular user for this request ."], "references": ["determines types for generic additions"], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 3191, "code": "def create link ( self , act node , name , instance ) : act node . links [ name ] = instance act node . children [ name ] = instance full name = instance . v full name if full name not in self . root instance . linked by : self . root instance . linked by [ full name ] = { } linking = self . root instance . linked by [ full name ] if act node . v full name not in linking : linking [ act node . v full name ] = ( act node , set ( ) ) linking [ act node . v full name ] [ 1 ] . add ( name ) if name not in self . links count : self . links count [ name ] = 0 self . links count [ name ] = self . links count [ name ] + 1 self . logger . debug ( 'Added link `%s` under `%s` pointing ' 'to `%s`.' % ( name , act node . v full name , instance . v full name ) ) return instance", "predictions": ["create ( ( , = ( = ( = ( = ( = ( = ( = ( = ( = ( = ( . = ( . = ( . = ( . . . = ( . ( . . . . . . ( . . ("], "references": ["creates a link and checks if names are appropriate"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3192, "code": "def create any group ( self , parent node , name , type name , instance = None , constructor = None , args = None , kwargs = None ) : if args is None : args = [ ] if kwargs is None : kwargs = { } full name = self . make full name ( parent node . v full name , name ) if instance is None : if constructor is None : if type name == RESULT GROUP : constructor = Result Group elif type name == PARAMETER GROUP : constructor = Parameter Group elif type name == CONFIG GROUP : constructor = Config Group elif type name == DERIVED PARAMETER GROUP : constructor = Derived Parameter Group elif type name == GROUP : constructor = NN Group Node else : raise Runtime Error ( 'You shall not pass!' ) instance = self . root instance . construct instance ( constructor , full name , * args , * * kwargs ) else : instance . rename ( full name ) if type name == RESULT GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under results' % str ( type ( instance ) ) ) elif type name == PARAMETER GROUP : if type ( instance ) in ( NN Group Node , Result Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under parameters' % str ( type ( instance ) ) ) elif type name == CONFIG GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Result Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under config' % str ( type ( instance ) ) ) elif type name == DERIVED PARAMETER GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Config Group , Result Group ) : raise Type Error ( 'You cannot add a `%s` type of group under derived ' 'parameters' % str ( type ( instance ) ) ) elif type name == GROUP : if type ( instance ) in ( Result Group , Parameter Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under other data' % str ( type ( instance ) ) ) else : raise Runtime Error ( 'You shall not pass!' ) self . set details tree node ( parent node , name , instance ) instance . nn interface = self self . root instance . all groups [ instance . v full name ] = instance self . add to nodes and leaves ( instance ) parent node . children [ name ] = instance parent node . groups [ name ] = instance return instance", "predictions": ["creates a new ( local ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["generically creates a new group inferring from the type_name ."], "bleu": 0.04472583280432029, "rouge_l": 0.11366459627329194}
{"id": 3193, "code": "def add group from storage ( self , args , kwargs ) : return self . nn interface . add generic ( self , type name = GROUP , group type name = GROUP , args = args , kwargs = kwargs , add prefix = False , check naming = False )", "predictions": ["change a single logging . ."], "references": ["can be called from storage service to create a new group to bypass name checking"], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 3194, "code": "def add leaf from storage ( self , args , kwargs ) : return self . nn interface . add generic ( self , type name = LEAF , group type name = GROUP , args = args , kwargs = kwargs , add prefix = False , check naming = False )", "predictions": ["adds a strings to this identity ."], "references": ["can be called from storage service to create a new leaf to bypass name checking"], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 3195, "code": "def f dir data ( self ) : if ( self . nn interface is not None and self . nn interface . root instance is not None and self . v root . v auto load ) : try : if self . v is root : self . f load ( recursive = True , max depth = 1 , load data = pypetconstants . LOAD SKELETON , with meta data = False , with run information = False ) else : self . f load ( recursive = True , max depth = 1 , load data = pypetconstants . LOAD SKELETON ) except Exception as exc : pass return list ( self . children . keys ( ) )", "predictions": ["convert data for a directory ."], "references": ["returns a list of all children names"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3196, "code": "def unit from expression ( expr ) : if expr == '1' : return get unit fast ( 1 ) elif isinstance ( expr , str ) : mod = ast . parse ( expr , mode = 'eval' ) expr = mod . body return unit from expression ( expr ) elif expr . class is ast . Name : return ALLUNITS [ expr . id ] elif expr . class is ast . Num : return expr . n elif expr . class is ast . Unary Op : op = expr . op . class . name operand = unit from expression ( expr . operand ) if op == 'U Sub' : return - operand else : raise Syntax Error ( \"Unsupported operation \" + op ) elif expr . class is ast . Bin Op : op = expr . op . class . name left = unit from expression ( expr . left ) right = unit from expression ( expr . right ) if op == 'Add' : u = left + right elif op == 'Sub' : u = left - right elif op == 'Mult' : u = left * right elif op == 'Div' : u = left / right elif op == 'Pow' : n = unit from expression ( expr . right ) u = left ** n elif op == 'Mod' : u = left % right else : raise Syntax Error ( \"Unsupported operation \" + op ) return u else : raise Runtime Error ( 'You shall not pass' )", "predictions": ["convert something into an string ."], "references": ["takes a unit string like 1 . * volt and returns the brian2 unit ."], "bleu": 0.054546736148076896, "rouge_l": 0.17681159420289855}
{"id": 3197, "code": "def f supports ( self , data ) : if isinstance ( data , Quantity ) : return True elif super ( Brian2Parameter , self ) . f supports ( data ) : return True return False", "predictions": ["returns true if this method does not return false ."], "references": ["simply checks if data is supported"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 3198, "code": "def supports ( self , data ) : if isinstance ( data , Quantity ) : return True elif super ( Brian2Result , self ) . supports ( data ) : return True return False", "predictions": ["returns true if this text finalize the given remove operation ."], "references": ["simply checks if data is supported"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 3199, "code": "def add commit variables ( traj , commit ) : git time value = time . strftime ( '%Y %m %d %Hh%Mm%Ss' , time . localtime ( commit . committed date ) ) git short name = str ( commit . hexsha [ 0 : 7 ] ) git commit name = 'commit %s ' % git short name git commit name = 'git.' + git commit name + git time value if not traj . f contains ( 'config.' + git commit name , shortcuts = False ) : git commit name += '.' traj . f add config ( git commit name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) traj . f add config ( git commit name + 'name rev' , commit . name rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) traj . f add config ( git commit name + 'committed date' , commit . committed date , comment = 'Date of commit as unix epoch seconds' ) traj . f add config ( git commit name + 'message' , str ( commit . message ) , comment = 'The commit message' )", "predictions": ["start a ( or ( ( sys sys sys sys sys sys sys sys sys sys sys . sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys . sys sys sys sys sys sys sys sys sys"], "references": ["adds commit information to the trajectory ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 3200, "code": "def get argspec ( func ) : if inspect . isclass ( func ) : func = func . init if not inspect . isfunction ( func ) : return [ ] , False parameters = inspect . signature ( func ) . parameters args = [ ] uses starstar = False for par in parameters . values ( ) : if ( par . kind == inspect . Parameter . POSITIONAL OR KEYWORD or par . kind == inspect . Parameter . KEYWORD ONLY ) : args . append ( par . name ) elif par . kind == inspect . Parameter . VAR KEYWORD : uses starstar = True return args , uses starstar", "predictions": ["get an internal representation of the given function ."], "references": ["helper function to support both python versions"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3201, "code": "def get matching kwargs ( func , kwargs ) : args , uses startstar = get argspec ( func ) if uses startstar : return kwargs . copy ( ) else : matching kwargs = dict ( ( k , kwargs [ k ] ) for k in args if k in kwargs ) return matching kwargs", "predictions": ["returns all of the non - null arguments ."], "references": ["takes a function and keyword arguments and returns the ones that can be passed ."], "bleu": 0.09111821689187218, "rouge_l": 0.23921568627450981}
{"id": 3202, "code": "def format time ( timestamp ) : format string = '%Y %m %d %Hh%Mm%Ss' formatted time = datetime . datetime . fromtimestamp ( timestamp ) . strftime ( format string ) return formatted time", "predictions": ["helper method to format the current timestamp ."], "references": ["formats timestamp to human readable format"], "bleu": 0.19070828081828378, "rouge_l": 0.2932692307692307}
{"id": 3203, "code": "def port to tcp ( port = None ) : #address = 'tcp://' + socket.gethostbyname(socket.getfqdn()) domain name = socket . getfqdn ( ) try : addr list = socket . getaddrinfo ( domain name , None ) except Exception : addr list = socket . getaddrinfo ( '127.0.0.1' , None ) family , socktype , proto , canonname , sockaddr = addr list [ 0 ] host = convert ipv6 ( sockaddr [ 0 ] ) address = 'tcp://' + host if port is None : port = ( ) if not isinstance ( port , int ) : context = zmq . Context ( ) try : socket = context . socket ( zmq . REP ) socket . ipv6 = is ipv6 ( address ) port = socket . bind to random port ( address , * port ) except Exception : print ( 'Could not connect to {} using {}' . format ( address , addr list ) ) pypet root logger = logging . get Logger ( 'pypet' ) pypet root logger . exception ( 'Could not connect to {}' . format ( address ) ) raise socket . close ( ) context . term ( ) return address + ':' + str ( port )", "predictions": ["creates a port from the given port ."], "references": ["returns local tcp address for a given port automatic port if none"], "bleu": 0.14544785215055717, "rouge_l": 0.28955696202531644}
{"id": 3204, "code": "def racedirs ( path ) : if os . path . isfile ( path ) : raise IO Error ( 'Path `%s` is already a file not a directory' ) while True : try : if os . path . isdir ( path ) : break os . makedirs ( path ) except Environment Error as exc : if exc . errno != 17 : raise", "predictions": ["creates an writable dir or a file if it cannot be created ."], "references": ["like os . makedirs but takes care about race conditions"], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 3205, "code": "def reset ( self , index , total , percentage step , length ) : self . start time = datetime . datetime . now ( ) self . start index = index self . current index = index self . percentage step = percentage step self . total = float ( total ) self . total minus one = total - 1 self . length = length self . norm factor = total * percentage step / 100.0 self . current interval = int ( ( index + 1.0 ) / self . norm factor )", "predictions": ["resets the method to a new datetime object ."], "references": ["resets to the progressbar to start a new one"], "bleu": 0.2208959113415788, "rouge_l": 0.5555555555555556}
{"id": 3206, "code": "def get remaining ( self , index ) : try : current time = datetime . datetime . now ( ) time delta = current time - self . start time try : total seconds = time delta . total seconds ( ) except Attribute Error : total seconds = ( ( time delta . microseconds + ( time delta . seconds + time delta . days * 24 * 3600 ) * 10 ** 6 ) / 10.0 ** 6 ) remaining seconds = int ( ( self . total - self . start index - 1.0 ) * total seconds / float ( index - self . start index ) - total seconds ) remaining delta = datetime . timedelta ( seconds = remaining seconds ) remaining str = ', remaining: ' + str ( remaining delta ) except Zero Division Error : remaining str = '' return remaining str", "predictions": ["this method will return the first column in the input . if the method returns it will always be found , or if no time has been found . note that the method will always be called ."], "references": ["calculates remaining time as a string"], "bleu": 0.03172629746109536, "rouge_l": 0.0523156089193825}
{"id": 3207, "code": "def f remove ( self , key ) : key = self . translate key ( key ) try : del self . dict [ key ] except Key Error : raise Attribute Error ( 'Your annotations do not contain %s' % key )", "predictions": ["removes the specified key from the underlying configuration ."], "references": ["removes key from annotations"], "bleu": 0.19960198807747329, "rouge_l": 0.4959349593495934}
{"id": 3208, "code": "def f ann to str ( self ) : resstr = '' for key in sorted ( self . dict . keys ( ) ) : resstr += '%s=%s; ' % ( key , str ( self . dict [ key ] ) ) return resstr [ : - 2 ]", "predictions": ["return a string containing a concise style set ."], "references": ["returns all annotations lexicographically sorted as a concatenated string ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 3209, "code": "def supports ( self , item ) : result = super ( Shared Result , self ) . supports ( item ) result = result or type ( item ) in Shared Result . SUPPORTED DATA return result", "predictions": ["do a simple compatibility to return the appropriate result ."], "references": ["checks if outer data structure is supported ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 3210, "code": "def create shared data ( self , name = None , * * kwargs ) : if name is None : item = self . f get ( ) else : item = self . f get ( name ) return item . create shared data ( * * kwargs )", "predictions": ["create shared data for testing ."], "references": ["calls the corresponding function of the shared data item"], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 3211, "code": "def send done ( self ) : self . start ( test connection = False ) self . logger . debug ( 'Sending shutdown signal' ) self . req rep ( ZMQ Server . DONE )", "predictions": ["creates a new executor with the specified test ."], "references": ["notifies the server to shutdown"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 3212, "code": "def req rep retry ( self , request ) : retries left = self . RETRIES while retries left : self . logger . log ( 1 , 'Sending REQ `%s`' , request ) self . send request ( request ) socks = dict ( self . poll . poll ( self . TIMEOUT ) ) if socks . get ( self . socket ) == zmq . POLLIN : response = self . receive response ( ) self . logger . log ( 1 , 'Received REP `%s`' , response ) return response , self . RETRIES - retries left else : self . logger . debug ( 'No response from server (%d retries left)' % retries left ) self . close socket ( confused = True ) retries left -= 1 if retries left == 0 : raise Runtime Error ( 'Server seems to be offline!' ) time . sleep ( self . SLEEP ) self . start socket ( )", "predictions": ["call this function to retry an asynchronous request ."], "references": ["returns response and number of retries"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3213, "code": "def put on queue ( self , to put ) : old = self . pickle queue self . pickle queue = False try : self . queue . put ( to put , block = True ) finally : self . pickle queue = old", "predictions": ["store the request in the queue ."], "references": ["puts data on queue"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3214, "code": "def put on pipe ( self , to put ) : self . acquire lock ( ) self . send chunks ( to put ) self . release lock ( )", "predictions": ["put the underlying queue to put the connection into the chain ."], "references": ["puts data on queue"], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 3215, "code": "def handle data ( self , msg , args , kwargs ) : stop = False try : if msg == 'DONE' : stop = True elif msg == 'STORE' : if 'msg' in kwargs : store msg = kwargs . pop ( 'msg' ) else : store msg = args [ 0 ] args = args [ 1 : ] if 'stuff to store' in kwargs : stuff to store = kwargs . pop ( 'stuff to store' ) else : stuff to store = args [ 0 ] args = args [ 1 : ] trajectory name = kwargs [ 'trajectory name' ] if self . trajectory name != trajectory name : if self . storage service . is open : self . close file ( ) self . trajectory name = trajectory name self . open file ( ) self . storage service . store ( store msg , stuff to store , * args , * * kwargs ) self . storage service . store ( pypetconstants . FLUSH , None ) self . check and collect garbage ( ) else : raise Runtime Error ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) except Exception : self . logger . exception ( 'ERROR occurred during storing!' ) time . sleep ( 0.01 ) pass return stop", "predictions": ["closes this named message and closes the store ."], "references": ["handles data and returns true or false if everything is done ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 3216, "code": "def run ( self ) : try : while True : msg , args , kwargs = self . receive data ( ) stop = self . handle data ( msg , args , kwargs ) if stop : break finally : if self . storage service . is open : self . close file ( ) self . trajectory name = ''", "predictions": ["this method executes the message ."], "references": ["starts listening to the queue ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 3217, "code": "def receive data ( self ) : result = self . queue . get ( block = True ) if hasattr ( self . queue , 'task done' ) : self . queue . task done ( ) return result", "predictions": ["receives a task for processing ."], "references": ["gets data from queue"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3218, "code": "def receive data ( self ) : while True : while len ( self . buffer ) < self . max size and self . conn . poll ( ) : data = self . read chunks ( ) if data is not None : self . buffer . append ( data ) if len ( self . buffer ) > 0 : return self . buffer . popleft ( )", "predictions": ["receive specific data from the underlying connection ."], "references": ["gets data from pipe"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 3219, "code": "def store ( self , * args , * * kwargs ) : try : self . acquire lock ( ) return self . storage service . store ( * args , * * kwargs ) finally : if self . lock is not None : try : self . release lock ( ) except Runtime Error : self . logger . error ( 'Could not release lock `%s`!' % str ( self . lock ) )", "predictions": ["associates a storage instance with the arguments . the method is called by the server when the method returns ."], "references": ["acquires a lock before storage and releases it afterwards ."], "bleu": 0.07264339766175722, "rouge_l": 0.21279069767441858}
{"id": 3220, "code": "def store ( self , msg , stuff to store , * args , * * kwargs ) : trajectory name = kwargs [ 'trajectory name' ] if trajectory name not in self . references : self . references [ trajectory name ] = [ ] self . references [ trajectory name ] . append ( ( msg , cp . copy ( stuff to store ) , args , kwargs ) )", "predictions": ["store a list of parameters for this object"], "references": ["simply keeps a reference to the stored data"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3221, "code": "def store references ( self , references ) : for trajectory name in references : self . storage service . store ( pypetconstants . LIST , references [ trajectory name ] , trajectory name = trajectory name ) self . check and collect garbage ( )", "predictions": ["store references to this object ."], "references": ["stores references to disk and may collect garbage ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 3222, "code": "def parse config ( init func ) : @ functools . wraps ( init func ) def new func ( env , * args , * * kwargs ) : config interpreter = Config Interpreter ( kwargs ) new kwargs = config interpreter . interpret ( ) init func ( env , * args , * * new kwargs ) config interpreter . add parameters ( env . traj ) return new func", "predictions": ["parse a new wavefunction config line ."], "references": ["decorator wrapping the environment to use a config file"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3223, "code": "def collect section ( self , section ) : kwargs = { } try : if self . parser . has section ( section ) : options = self . parser . options ( section ) for option in options : str val = self . parser . get ( section , option ) val = ast . literal eval ( str val ) kwargs [ option ] = val return kwargs except : raise", "predictions": ["collect all triggers that have been defined in the ( ."], "references": ["collects all settings within a section"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 3224, "code": "def collect config ( self ) : kwargs = { } sections = ( 'storage service' , 'trajectory' , 'environment' ) for section in sections : kwargs . update ( self . collect section ( section ) ) return kwargs", "predictions": ["collect all sections from this config ."], "references": ["collects all info from three sections"], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 3225, "code": "def interpret ( self ) : if self . config file : new kwargs = self . collect config ( ) for key in new kwargs : if key not in self . kwargs : self . kwargs [ key ] = new kwargs [ key ] if not use simple logging ( self . kwargs ) and 'log config' not in self . kwargs : self . kwargs [ 'log config' ] = self . config file return self . kwargs", "predictions": ["inject inject message from memory ."], "references": ["copies parsed arguments into the kwargs passed to the environment"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 3226, "code": "def add parameters ( self , traj ) : if self . config file : parameters = self . collect section ( 'parameters' ) for name in parameters : value = parameters [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f add parameter ( name , * value ) config = self . collect section ( 'config' ) for name in config : value = config [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f add config ( name , * value )", "predictions": [". mapping . this will add this operation to ( ."], "references": ["adds parameters and config from the . ini file to the trajectory"], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 3227, "code": "def overview group ( self ) : if self . overview group is None : self . overview group = self . all create or get groups ( 'overview' ) [ 0 ] return self . overview group", "predictions": ["get the group of this group ."], "references": ["direct link to the overview group"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3228, "code": "def trj load exploration ( self , traj ) : if hasattr ( self . overview group , 'explorations' ) : explorations table = self . overview group . f get child ( 'explorations' ) for row in explorations table . iterrows ( ) : param name = row [ 'explorations' ] . decode ( 'utf-8' ) if param name not in traj . explored parameters : traj . explored parameters [ param name ] = None else : for what in ( 'parameters' , 'derived parameters' ) : if hasattr ( self . trajectory group , what ) : parameters = self . trajectory group . f get child ( what ) for group in parameters . f walk groups ( ) : if self . all get from attrs ( group , HDF5Storage Service . LENGTH ) : group location = group . v pathname full name = '.' . join ( group location . split ( '/' ) [ 2 : ] ) traj . explored parameters [ full name ] = None", "predictions": ["construct the tracks from the given group ."], "references": ["recalls names of all explored parameters"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3229, "code": "def trj store explorations ( self , traj ) : nexplored = len ( traj . explored parameters ) if nexplored > 0 : if hasattr ( self . overview group , 'explorations' ) : explorations table = self . overview group . f get child ( 'explorations' ) if len ( explorations table ) != nexplored : self . hdf5file . remove node ( where = self . overview group , name = 'explorations' ) if not hasattr ( self . overview group , 'explorations' ) : explored list = list ( traj . explored parameters . keys ( ) ) if explored list : string col = self . all get table col ( 'explorations' , explored list , 'overview.explorations' ) else : string col = pt . String Col ( 1 ) description = { 'explorations' : string col } explorations table = self . hdf5file . create table ( where = self . overview group , name = 'explorations' , description = description ) rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored list ] if rows : explorations table . append ( rows ) explorations table . flush ( )", "predictions": ["we need to store the explored table and explored ."], "references": ["stores a all explored parameter names for internal recall"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 3230, "code": "def srvc make overview tables ( self , tables to make , traj = None ) : for table name in tables to make : paramdescriptiondict = { } expectedrows = 0 paramdescriptiondict [ 'location' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX LOCATION LENGTH , pos = 0 ) paramdescriptiondict [ 'name' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX NAME LENGTH , pos = 1 ) paramdescriptiondict [ 'comment' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX COMMENT LENGTH ) paramdescriptiondict [ 'value' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX VALUE LENGTH , pos = 2 ) if table name == 'config overview' : if traj is not None : expectedrows = len ( traj . config ) if table name == 'parameters overview' : if traj is not None : expectedrows = len ( traj . parameters ) if table name == 'explored parameters overview' : paramdescriptiondict [ 'range' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX RANGE LENGTH ) paramdescriptiondict [ 'length' ] = pt . Int Col ( ) if traj is not None : expectedrows = len ( traj . explored parameters ) if table name . endswith ( 'summary' ) : paramdescriptiondict [ 'hexdigest' ] = pt . String Col ( 64 , pos = 10 ) if table name == 'derived parameters overview' : expectedrows = self . derived parameters per run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . derived parameters ) if table name == 'results overview' : expectedrows = self . results per run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . results ) if expectedrows > 0 : paramtable = self . all get or create table ( where = self . overview group , tablename = table name , description = paramdescriptiondict , expectedrows = expectedrows ) else : paramtable = self . all get or create table ( where = self . overview group , tablename = table name , description = paramdescriptiondict ) paramtable . flush ( )", "predictions": ["used to keep the ( in the ( ."], "references": ["creates the overview tables in overview group"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 3231, "code": "def all get or create table ( self , where , tablename , description , expectedrows = None ) : where node = self . hdf5file . get node ( where ) if not tablename in where node : if not expectedrows is None : table = self . hdf5file . create table ( where = where node , name = tablename , description = description , title = tablename , expectedrows = expectedrows , filters = self . all get filters ( ) ) else : table = self . hdf5file . create table ( where = where node , name = tablename , description = description , title = tablename , filters = self . all get filters ( ) ) else : table = where node . f get child ( tablename ) return table", "predictions": ["returns all filters that are assigned to the specified table ."], "references": ["creates a new table or if the table already exists returns it ."], "bleu": 0.11941964005964323, "rouge_l": 0.24629878869448185}
{"id": 3232, "code": "def all get node by name ( self , name ) : path name = name . replace ( '.' , '/' ) where = '/%s/%s' % ( self . trajectory name , path name ) return self . hdf5file . get node ( where = where )", "predictions": ["this method returns a ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["returns an hdf5 node by the path specified in name"], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 3233, "code": "def all insert into row ( self , row , insert dict ) : for key , val in insert dict . items ( ) : try : row [ key ] = val except Key Error as ke : self . logger . warning ( 'Could not write `%s` into a table, ' % key + repr ( ke ) )", "predictions": ["inserts all entries stored at a specified ( = ke args args args args args args args args args args args args args args args args args = ke uses ( uses ( . ( func . ( func = ke func = ke func . ( func func ="], "references": ["copies data from insert_dict into a pytables row ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 3234, "code": "def all create or get group ( self , name , parent hdf5 group = None ) : if not name in parent hdf5 group : new hdf5 group = self . hdf5file . create group ( where = parent hdf5 group , name = name , title = name , filters = self . all get filters ( ) ) return new hdf5 group , True else : new hdf5 group = parent hdf5 group . f get child ( name ) return new hdf5 group , False", "predictions": ["creates the full ( parent datetime datetime datetime datetime datetime datetime datetime datetime datetime datetime datetime datetime datetime datetime datetime datetime ."], "references": ["creates or returns a group"], "bleu": 0.05538696232597745, "rouge_l": 0.08356164383561643}
{"id": 3235, "code": "def ann store annotations ( self , item with annotations , node , overwrite = False ) : if overwrite is True or overwrite == 'v annotations' : annotated = self . all get from attrs ( node , HDF5Storage Service . ANNOTATED ) if annotated : current attrs = node . v attrs for attr name in current attrs . v attrnames : if attr name . startswith ( HDF5Storage Service . ANNOTATION PREFIX ) : delattr ( current attrs , attr name ) delattr ( current attrs , HDF5Storage Service . ANNOTATED ) self . hdf5file . flush ( ) if not item with annotations . v annotations . f is empty ( ) : anno dict = item with annotations . v annotations . dict current attrs = node . v attrs changed = False for field name in anno dict : val = anno dict [ field name ] field name with prefix = HDF5Storage Service . ANNOTATION PREFIX + field name if field name with prefix not in current attrs : setattr ( current attrs , field name with prefix , val ) changed = True if changed : setattr ( current attrs , HDF5Storage Service . ANNOTATED , True ) self . hdf5file . flush ( )", "predictions": ["added added to object ."], "references": ["stores annotations into an hdf5 file ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3236, "code": "def ann load annotations ( self , item with annotations , node ) : annotated = self . all get from attrs ( node , HDF5Storage Service . ANNOTATED ) if annotated : annotations = item with annotations . v annotations if not annotations . f is empty ( ) : raise Type Error ( 'Loading into non-empty annotations!' ) current attrs = node . v attrs for attr name in current attrs . v attrnames : if attr name . startswith ( HDF5Storage Service . ANNOTATION PREFIX ) : key = attr name key = key . replace ( HDF5Storage Service . ANNOTATION PREFIX , '' ) data = getattr ( current attrs , attr name ) setattr ( annotations , key , data )", "predictions": ["builds and sets the attribute values ."], "references": ["loads annotations from disk ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3237, "code": "def grp load group ( self , traj group , load data = pypetconstants . LOAD DATA , with links = True , recursive = False , max depth = None , traj = None , as new = False , hdf5 group = None ) : if hdf5 group is None : hdf5 group = self . all get node by name ( traj group . v full name ) traj = traj group . v root if recursive : parent traj node = traj group . f get parent ( ) self . tree load nodes dfs ( parent traj node , load data = load data , with links = with links , recursive = recursive , max depth = max depth , current depth = 0 , trajectory = traj , as new = as new , hdf5 group = hdf5 group ) else : if load data == pypetconstants . LOAD NOTHING : return elif load data == pypetconstants . OVERWRITE DATA : traj group . v annotations . f empty ( ) traj group . v comment = '' self . all load skeleton ( traj group , hdf5 group ) traj group . stored = not as new self . node processing timer . signal update ( )", "predictions": ["recursively ( ( factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor factor"], "references": ["loads a group node and potentially everything recursively below"], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 3238, "code": "def all load skeleton ( self , traj node , hdf5 group ) : if traj node . v annotations . f is empty ( ) : self . ann load annotations ( traj node , hdf5 group ) if traj node . v comment == '' : comment = self . all get from attrs ( hdf5 group , HDF5Storage Service . COMMENT ) if comment is None : comment = '' traj node . v comment = comment", "predictions": ["recursively remaining translation from given : : : : : : : : : : / / www . com / questions / questions / . / . / . / . / . / . / . / ."], "references": ["reloads skeleton data of a tree node"], "bleu": 0.025326605584447867, "rouge_l": 0.0}
{"id": 3239, "code": "def prm write shared array ( self , key , data , hdf5 group , full name , flag , * * kwargs ) : if flag == HDF5Storage Service . ARRAY : self . prm write into array ( key , data , hdf5 group , full name , * * kwargs ) elif flag in ( HDF5Storage Service . CARRAY , HDF5Storage Service . EARRAY , HDF5Storage Service . VLARRAY ) : self . prm write into other array ( key , data , hdf5 group , full name , flag = flag , * * kwargs ) else : raise Runtime Error ( 'Flag `%s` of hdf5 data `%s` of `%s` not understood' % ( flag , key , full name ) ) self . hdf5file . flush ( )", "predictions": ["f : remove or remove a : self - part part of this class ."], "references": ["creates and array that can be used with an hdf5 array object"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3240, "code": "def prm write shared table ( self , key , hdf5 group , fullname , * * kwargs ) : first row = None description = None if 'first row' in kwargs : first row = kwargs . pop ( 'first row' ) if not 'description' in kwargs : description = { } for colname in first row : data = first row [ colname ] column = self . all get table col ( key , [ data ] , fullname ) description [ colname ] = column if 'description' in kwargs : description = kwargs . pop ( 'description' ) if 'filters' in kwargs : filters = kwargs . pop ( 'filters' ) else : filters = self . all get filters ( kwargs ) table = self . hdf5file . create table ( where = hdf5 group , name = key , description = description , filters = filters , * * kwargs ) table . flush ( ) if first row is not None : row = table . row for key in description : row [ key ] = first row [ key ] row . append ( ) table . flush ( )", "predictions": ["write is a f file with the specified : write ( key in key in our data str in the database key - value in a str ."], "references": ["creates a new empty table"], "bleu": 0.04327969719414172, "rouge_l": 0.06931818181818182}
{"id": 3241, "code": "def lnk delete link ( self , link name ) : translated name = '/' + self . trajectory name + '/' + link name . replace ( '.' , '/' ) link = self . hdf5file . get node ( where = translated name ) link . f remove ( )", "predictions": ["removes the entity specific marker ."], "references": ["removes a link from disk"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3242, "code": "def prm make description ( self , data , fullname ) : def convert lists and tuples ( series of data ) : \"\"\"Converts lists and tuples to numpy arrays\"\"\" if isinstance ( series of data [ 0 ] , ( list , tuple ) ) : for idx , item in enumerate ( series of data ) : series of data [ idx ] = np . array ( item ) descriptiondict = { } original data type dict = { } for key in data : val = data [ key ] self . all set attributes to recall natives ( val [ 0 ] , PT Item Mock ( original data type dict ) , HDF5Storage Service . FORMATTED COLUMN PREFIX % key ) convert lists and tuples ( val ) col = self . all get table col ( key , val , fullname ) descriptiondict [ key ] = col return descriptiondict , original data type dict", "predictions": ["create a ( from a given list of matrices ."], "references": ["returns a description dictionary for pytables table creation"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 3243, "code": "def prm get longest stringsize ( string list ) : maxlength = 1 for stringar in string list : if isinstance ( stringar , np . ndarray ) : if stringar . ndim > 0 : for string in stringar . ravel ( ) : maxlength = max ( len ( string ) , maxlength ) else : maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) else : maxlength = max ( len ( stringar ) , maxlength ) return int ( maxlength * 1.5 )", "predictions": ["gets the ( formatted start with a string start code start with end start start and end start start with end start start offset start at index start . start with end of the line ."], "references": ["returns the longest string size for a string entry across data ."], "bleu": 0.050117246628245134, "rouge_l": 0.18318318318318316}
{"id": 3244, "code": "def make set name ( idx ) : GROUPSIZE = 1000 set idx = idx // GROUPSIZE if set idx >= 0 : return pypetconstants . FORMATTED SET NAME % set idx else : return pypetconstants . SET NAME DUMMY", "predictions": ["req retry service retry ."], "references": ["creates a run set name based on idx"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3245, "code": "def preset ( self , name , args , kwargs ) : if self . f contains ( name , shortcuts = False ) : raise Value Error ( 'Parameter `%s` is already part of your trajectory, use the normal' 'accessing routine to change config.' % name ) else : self . changed default parameters [ name ] = ( args , kwargs )", "predictions": ["create a new ( ."], "references": ["generic preset function marks a parameter or config for presetting ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 3246, "code": "def remove exploration ( self ) : for param in self . explored parameters . values ( ) : if param . stored : try : self . f delete item ( param ) except Exception : self . logger . exception ( 'Could not delete expanded parameter `%s` ' 'from disk.' % param . v full name )", "predictions": ["removes all acquire parameters for this operation ."], "references": ["called if trajectory is expanded deletes all explored parameters from disk"], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 3247, "code": "def update run information ( self , run information dict ) : idx = run information dict [ 'idx' ] name = run information dict [ 'name' ] self . run information [ name ] = run information dict self . updated run information . add ( idx )", "predictions": ["use this method to handle the cached configuration properties"], "references": ["overwrites the run information of a particular run"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3248, "code": "def add run info ( self , idx , name = '' , timestamp = 42.0 , finish timestamp = 1.337 , runtime = 'forever and ever' , time = '>>Maybe time`s gone on strike' , completed = 0 , parameter summary = 'Not yet my friend!' , short environment hexsha = 'N/A' ) : if idx in self . single run ids : old name = self . single run ids [ idx ] del self . single run ids [ old name ] del self . single run ids [ idx ] del self . run information [ old name ] if name == '' : name = self . f wildcard ( '$' , idx ) self . single run ids [ name ] = idx self . single run ids [ idx ] = name info dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish timestamp' : finish timestamp , 'runtime' : runtime , 'time' : time , 'completed' : completed , 'name' : name , 'parameter summary' : parameter summary , 'short environment hexsha' : short environment hexsha } self . run information [ name ] = info dict self . length = len ( self . run information )", "predictions": ["adds the information about the project ."], "references": ["adds a new run to the _run_information dict ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 3249, "code": "def f lock parameters ( self ) : for par in self . parameters . values ( ) : if not par . f is empty ( ) : par . f lock ( )", "predictions": ["a method that returns a list of the locking files ."], "references": ["locks all non - empty parameters"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3250, "code": "def f lock derived parameters ( self ) : for par in self . derived parameters . values ( ) : if not par . f is empty ( ) : par . f lock ( )", "predictions": ["returns a list of the names of this method ."], "references": ["locks all non - empty derived parameters"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3251, "code": "def make reversed wildcards ( self , old length = - 1 ) : if len ( self . reversed wildcards ) > 0 : start = old length else : start = - 1 for wildcards , func in self . wildcard functions . items ( ) : for irun in range ( start , len ( self ) ) : translated name = func ( irun ) if not translated name in self . reversed wildcards : self . reversed wildcards [ translated name ] = ( [ ] , wildcards ) self . reversed wildcards [ translated name ] [ 0 ] . append ( irun )", "predictions": ["create a ( possibly non - negative acquire acquire the items of * * is a base ."], "references": ["creates a full mapping from all wildcard translations to the corresponding wildcards"], "bleu": 0.07535838128770536, "rouge_l": 0.13832199546485258}
{"id": 3252, "code": "def merge single runs ( self , other trajectory , used runs ) : count = len ( self ) run indices = range ( len ( other trajectory ) ) run name dict = Ordered Dict ( ) to store groups with annotations = [ ] for idx in run indices : if idx in used runs : other info dict = other trajectory . f get run information ( idx ) time = other info dict [ 'time' ] timestamp = other info dict [ 'timestamp' ] completed = other info dict [ 'completed' ] short environment hexsha = other info dict [ 'short environment hexsha' ] finish timestamp = other info dict [ 'finish timestamp' ] runtime = other info dict [ 'runtime' ] new idx = used runs [ idx ] new runname = self . f wildcard ( '$' , new idx ) run name dict [ idx ] = new runname info dict = dict ( idx = new idx , time = time , timestamp = timestamp , completed = completed , short environment hexsha = short environment hexsha , finish timestamp = finish timestamp , runtime = runtime ) self . add run info ( * * info dict )", "predictions": ["merges this inputs and to the same kwargs . this merges the last and self - inputs with the same timestamp . this merges the last and self - self . this ensures that the other runs are removed and other pointers are removed ."], "references": ["updates the run_information of the current trajectory ."], "bleu": 0.0317901171581787, "rouge_l": 0.12951167728237792}
{"id": 3253, "code": "def rename full name ( self , full name , other trajectory , used runs = None , new run idx = None ) : split name = full name . split ( '.' ) for idx , name in enumerate ( split name ) : if name in other trajectory . reversed wildcards : run indices , wildcards = other trajectory . reversed wildcards [ name ] if new run idx is None : run idx = None for run jdx in run indices : if run jdx in used runs : run idx = used runs [ run jdx ] break elif run jdx == - 1 : run idx = - 1 break if run idx is None : raise Runtime Error ( 'You shall not pass!' ) else : run idx = new run idx new name = self . f wildcard ( wildcards [ 0 ] , run idx ) split name [ idx ] = new name full name = '.' . join ( split name ) return full name", "predictions": ["store this object based on its ( information ."], "references": ["renames a full name based on the wildcards and a particular run"], "bleu": 0.13309610652103343, "rouge_l": 0.1856925418569254}
{"id": 3254, "code": "def make single run ( self ) : self . is run = False self . new nodes = Ordered Dict ( ) self . new links = Ordered Dict ( ) self . is run = True return self", "predictions": ["makes a config for this period ."], "references": ["modifies the trajectory for single runs executed by the environment"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3255, "code": "def set start ( self ) : init time = time . time ( ) formatted time = datetime . datetime . fromtimestamp ( init time ) . strftime ( '%Y %m %d %Hh%Mm%Ss' ) run info dict = self . run information [ self . v crun ] run info dict [ 'timestamp' ] = init time run info dict [ 'time' ] = formatted time if self . environment hexsha is not None : run info dict [ 'short environment hexsha' ] = self . environment hexsha [ 0 : 7 ]", "predictions": ["use except for testing and methods to use the . method ."], "references": ["sets the start timestamp and formatted time to the current time ."], "bleu": 0.13065113298388567, "rouge_l": 0.3333333333333333}
{"id": 3256, "code": "def set finish ( self ) : run info dict = self . run information [ self . v crun ] timestamp run = run info dict [ 'timestamp' ] run summary = self . summarize explored parameters ( ) finish timestamp run = time . time ( ) findatetime = datetime . datetime . fromtimestamp ( finish timestamp run ) startdatetime = datetime . datetime . fromtimestamp ( timestamp run ) runtime run = str ( findatetime - startdatetime ) run info dict [ 'parameter summary' ] = run summary run info dict [ 'completed' ] = 1 run info dict [ 'finish timestamp' ] = finish timestamp run run info dict [ 'runtime' ] = runtime run", "predictions": ["generate the ( that will be run for the ( ."], "references": ["sets the finish time and computes the runtime in human readable format"], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 3257, "code": "def pool single run ( kwargs ) : wrap mode = kwargs [ 'wrap mode' ] traj = kwargs [ 'traj' ] traj . v storage service = pool single run . storage service if wrap mode == pypetconstants . WRAP MODE LOCAL : traj . v storage service . free references ( ) return sigint handling single run ( kwargs )", "predictions": ["self - get a ( interpret if necessary if necessary if it is a ( one if necessary if needed if it is a ( if it is a ( a if it ' s a ( if it has a ( if it ' s a ( if it"], "references": ["starts a pool single run and passes the storage service"], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 3258, "code": "def frozen pool single run ( kwargs ) : idx = kwargs . pop ( 'idx' ) frozen kwargs = frozen pool single run . kwargs frozen kwargs . update ( kwargs ) traj = frozen kwargs [ 'traj' ] traj . f set crun ( idx ) return sigint handling single run ( frozen kwargs )", "predictions": ["add a ( or a ) parameters to the parameters ."], "references": ["single run wrapper for the frozen pool makes a single run and passes kwargs"], "bleu": 0.09596928383261212, "rouge_l": 0.07830551989730423}
{"id": 3259, "code": "def configure pool ( kwargs ) : pool single run . storage service = kwargs [ 'storage service' ] configure niceness ( kwargs ) configure logging ( kwargs , extract = False )", "predictions": ["sets up and compile compile compile as a group ."], "references": ["configures the pool and keeps the storage service"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 3260, "code": "def configure frozen pool ( kwargs ) : frozen pool single run . kwargs = kwargs configure niceness ( kwargs ) configure logging ( kwargs , extract = False ) traj = kwargs [ 'traj' ] traj . v full copy = kwargs [ 'full copy' ]", "predictions": ["configure sets up a ) exploration ."], "references": ["configures the frozen pool and keeps all kwargs"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 3261, "code": "def process single run ( kwargs ) : configure niceness ( kwargs ) configure logging ( kwargs ) result queue = kwargs [ 'result queue' ] result = sigint handling single run ( kwargs ) result queue . put ( result ) result queue . close ( )", "predictions": ["add dependencies that are executed before the len process ."], "references": ["wrapper function that first configures logging and starts a single run afterwards ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 3262, "code": "def scoop single run ( kwargs ) : try : try : is origin = scoop . IS ORIGIN except Attribute Error : is origin = True if not is origin : configure niceness ( kwargs ) configure logging ( kwargs ) return single run ( kwargs ) except Exception : scoop . logger . exception ( 'ERROR occurred during a single run!' ) raise", "predictions": ["runs a make sure that the scoop is stored ."], "references": ["wrapper function for scoop that does not configure logging"], "bleu": 0.13950796967929133, "rouge_l": 0.10627177700348434}
{"id": 3263, "code": "def configure niceness ( kwargs ) : niceness = kwargs [ 'niceness' ] if niceness is not None : try : try : current = os . nice ( 0 ) if niceness - current > 0 : os . nice ( niceness - current ) except Attribute Error : psutil . Process ( ) . nice ( niceness ) except Exception as exc : sys . stderr . write ( 'Could not configure niceness because of: %s' % repr ( exc ) ) traceback . print exc ( )", "predictions": ["configures the attribute of the queue ."], "references": ["sets niceness of a process"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3264, "code": "def sigint handling single run ( kwargs ) : try : graceful exit = kwargs [ 'graceful exit' ] if graceful exit : sigint handling . start ( ) if sigint handling . hit : result = ( sigint handling . SIGINT , None ) else : result = single run ( kwargs ) if sigint handling . hit : result = ( sigint handling . SIGINT , result ) return result return single run ( kwargs ) except : pypet root logger = logging . get Logger ( 'pypet' ) pypet root logger . exception ( 'ERROR occurred during a single run! ' ) raise", "predictions": ["runs the command and returns its output ."], "references": ["wrapper that allow graceful exits of single runs"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3265, "code": "def wrap handling ( kwargs ) : configure logging ( kwargs , extract = False ) handler = kwargs [ 'handler' ] graceful exit = kwargs [ 'graceful exit' ] if graceful exit : sigint handling . start ( ) handler . run ( )", "predictions": ["this method wraps a router to wrap it to the command ."], "references": ["starts running a queue handler and creates a log file for the queue ."], "bleu": 0.10459315495983224, "rouge_l": 0.22761194029850743}
{"id": 3266, "code": "def f supports ( self , data ) : dtype = type ( data ) if dtype is tuple or dtype is list : if len ( data ) == 0 : return False old type = None for item in data : if not type ( item ) in pypetconstants . PARAMETER SUPPORTED DATA : return False if not old type is None and old type != type ( item ) : return False old type = type ( item ) return True elif dtype is np . ndarray or dtype is np . matrix : if data . size == 0 : return False dtype = data . dtype if np . issubdtype ( dtype , np . str ) : dtype = np . str return dtype in pypetconstants . PARAMETER SUPPORTED DATA", "predictions": ["returns whether this vector supports the specified data ."], "references": ["checks if input data is supported by the parameter ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 3267, "code": "def f supports ( self , data ) : dtype = type ( data ) if dtype is tuple or dtype is list and len ( data ) == 0 : return True elif dtype is np . ndarray and data . size == 0 and data . ndim == 1 : return True else : return super ( Array Parameter , self ) . f supports ( data )", "predictions": ["receive ndarray of data from data"], "references": ["checks if input data is supported by the parameter ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 3268, "code": "def equal values ( self , val1 , val2 ) : if self . is supported matrix ( val1 ) : if self . is supported matrix ( val2 ) : , , hash tuple 1 = self . serialize matrix ( val1 ) , , hash tuple 2 = self . serialize matrix ( val2 ) return hash ( hash tuple 1 ) == hash ( hash tuple 2 ) else : return False else : return super ( Sparse Parameter , self ) . equal values ( val1 , val2 )", "predictions": [". for simple abstract entries ."], "references": ["matrices are equal if they hash to the same value ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 3269, "code": "def is supported matrix ( data ) : return ( spsp . isspmatrix csc ( data ) or spsp . isspmatrix csr ( data ) or spsp . isspmatrix bsr ( data ) or spsp . isspmatrix dia ( data ) )", "predictions": ["returns true if the given data is supported by the client ."], "references": ["checks if a data is csr csc bsr or dia scipy sparse matrix"], "bleu": 0.13519230385081712, "rouge_l": 0.23828125000000006}
{"id": 3270, "code": "def f translate key ( self , key ) : if isinstance ( key , int ) : if key == 0 : key = self . v name else : key = self . v name + ' %d' % key return key", "predictions": ["this method maps a blob to a key ."], "references": ["translates integer indices into the appropriate names"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3271, "code": "def f remove ( self , * args ) : for arg in args : arg = self . f translate key ( arg ) if arg in self . data : del self . data [ arg ] else : raise Attribute Error ( 'Your result `%s` does not contain %s.' % ( self . name , arg ) )", "predictions": ["modified method . this method is only used for tests that are not available ."], "references": ["removes * args from the result"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3272, "code": "def supports ( self , item ) : if Sparse Parameter . is supported matrix ( item ) : return True else : return super ( Sparse Result , self ) . supports ( item )", "predictions": ["returns if this matrix supports the given item ."], "references": ["supports everything of parent class and csr csc bsr and dia sparse matrices ."], "bleu": 0.08961856124931385, "rouge_l": 0.1673525377229081}
{"id": 3273, "code": "def store ( self ) : store dict = { } for key , val in self . data . items ( ) : store dict [ key ] = pickle . dumps ( val , protocol = self . v protocol ) store dict [ Pickle Result . PROTOCOL ] = self . v protocol return store dict", "predictions": ["transforms a dict to a protocol ."], "references": ["returns a dictionary containing pickle dumps"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3274, "code": "def main ( ) : folder = os . getcwd ( ) print ( 'Merging all files' ) merge all in folder ( folder , delete other files = True , dynamic imports = Function Parameter , backup = False ) print ( 'Done' )", "predictions": ["the main method of the script ."], "references": ["simply merge all trajectories in the working directory"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3275, "code": "def create session ( ) : ctx = saga . Context ( \"User Pass\" ) ctx . user id = USER ctx . user pass = PASSWORD session = saga . Session ( ) session . add context ( ctx ) return session", "predictions": ["create a session ."], "references": ["creates and returns a new saga session"], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 3276, "code": "def merge trajectories ( session ) : jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'merge trajs.py' ] jd . output = \"mysagajob merge.stdout\" jd . error = \"mysagajob merge.stderr\" jd . working directory = WORKING DIR js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) myjob = js . create job ( jd ) print ( \"\\n...starting job...\\n\" ) myjob . run ( ) print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...waiting for job...\\n\" ) myjob . wait ( ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"Exitcode  : %s\" % ( myjob . exit code ) )", "predictions": ["merges all ( and merges them to the database ."], "references": ["merges all trajectories found in the working directory"], "bleu": 0.17827531042796255, "rouge_l": 0.34014869888475835}
{"id": 3277, "code": "def start jobs ( session ) : js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) batches = range ( 3 ) jobs = [ ] for batch in batches : print ( 'Starting batch %d' % batch ) jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'the task.py --batch=' + str ( batch ) ] jd . output = \"mysagajob.stdout\" + str ( batch ) jd . error = \"mysagajob.stderr\" + str ( batch ) jd . working directory = WORKING DIR myjob = js . create job ( jd ) print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...starting job...\\n\" ) myjob . run ( ) jobs . append ( myjob ) for myjob in jobs : print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...waiting for job...\\n\" ) myjob . wait ( ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"Exitcode  : %s\" % ( myjob . exit code ) )", "predictions": ["starts the js sequence of ( ."], "references": ["starts all jobs and runs the_task . py in batches ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 3278, "code": "def multiply ( traj ) : z = traj . x * traj . y traj . f add result ( 'z' , z = z , comment = 'I am the product of two reals!' )", "predictions": ["creates a new product ."], "references": ["sophisticated simulation of multiplication"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3279, "code": "def add parameters ( traj ) : print ( 'Adding Parameters' ) traj . f add parameter ( 'neuron.V init' , 0.0 , comment = 'The initial condition for the ' 'membrane potential' ) traj . f add parameter ( 'neuron.I' , 0.0 , comment = 'The externally applied current.' ) traj . f add parameter ( 'neuron.tau V' , 10.0 , comment = 'The membrane time constant in milliseconds' ) traj . f add parameter ( 'neuron.tau ref' , 5.0 , comment = 'The refractory period in milliseconds ' 'where the membrane potnetial ' 'is clamped.' ) traj . f add parameter ( 'simulation.duration' , 1000.0 , comment = 'The duration of the experiment in ' 'milliseconds.' ) traj . f add parameter ( 'simulation.dt' , 0.1 , comment = 'The step size of an Euler integration step.' )", "predictions": ["creates a new ( ."], "references": ["adds all parameters to traj"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3280, "code": "def add exploration ( traj ) : print ( 'Adding exploration of I and tau ref' ) explore dict = { 'neuron.I' : np . arange ( 0 , 1.01 , 0.01 ) . tolist ( ) , 'neuron.tau ref' : [ 5.0 , 7.5 , 10.0 ] } explore dict = cartesian product ( explore dict , ( 'neuron.tau ref' , 'neuron.I' ) ) traj . f explore ( explore dict )", "predictions": ["creates a new ( ."], "references": ["explores different values of i and tau_ref ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3281, "code": "def make filename ( traj ) : explored parameters = traj . f get explored parameters ( ) filename = '' for param in explored parameters . values ( ) : short name = param . v name val = param . f get ( ) filename += '%s %s ' % ( short name , str ( val ) ) return filename [ : - 2 ] + '.png'", "predictions": ["generate and returns the filename ."], "references": ["function to create generic filenames based on what has been explored"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 3282, "code": "def main ( ) : logger = logging . get Logger ( ) folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca patterns pypet' ) if not os . path . isdir ( folder ) : os . makedirs ( folder ) filename = os . path . join ( folder , 'all patterns.hdf5' ) env = Environment ( trajectory = 'cellular automata' , multiproc = True , ncores = 4 , wrap mode = 'QUEUE' , filename = filename , overwrite file = True ) traj = env . traj traj . par . ncells = Parameter ( 'ncells' , 400 , 'Number of cells' ) traj . par . steps = Parameter ( 'steps' , 250 , 'Number of timesteps' ) traj . par . rule number = Parameter ( 'rule number' , 30 , 'The ca rule' ) traj . par . initial name = Parameter ( 'initial name' , 'random' , 'The type of initial state' ) traj . par . seed = Parameter ( 'seed' , 100042 , 'RNG Seed' ) exp dict = { 'rule number' : [ 10 , 30 , 90 , 110 , 184 ] , 'initial name' : [ 'single' , 'random' ] , } exp dict = cartesian product ( exp dict ) traj . f explore ( exp dict ) logger . info ( 'Starting Simulation' ) env . run ( wrap automaton ) traj . f load ( load data = 2 ) logger . info ( 'Printing data' ) for idx , run name in enumerate ( traj . f iter runs ( ) ) : filename = os . path . join ( folder , make filename ( traj ) ) plot pattern ( traj . crun . pattern , traj . rule number , filename ) progressbar ( idx , len ( traj ) , logger = logger ) env . disable logging ( )", "predictions": ["main program method to load the execution of a test"], "references": ["main * boilerplate * function to start simulation"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3283, "code": "def config from file ( filename , config = None ) : if config : try : with open ( filename , 'w' ) as fdesc : fdesc . write ( json . dumps ( config ) ) except IO Error as error : logger . exception ( error ) return False return True else : if os . path . isfile ( filename ) : try : with open ( filename , 'r' ) as fdesc : return json . loads ( fdesc . read ( ) ) except IO Error as error : return False else : return { }", "predictions": ["small configuration file management function ."], "references": ["small configuration file management function"], "bleu": 0.8091067115702212, "rouge_l": 0.9242424242424241}
{"id": 3284, "code": "def request pin ( self ) : url = 'https://api.ecobee.com/authorize' params = { 'response type' : 'ecobee Pin' , 'client id' : self . api key , 'scope' : 'smart Write' } try : request = requests . get ( url , params = params ) except Request Exception : logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" \"Could not request pin.\" ) return self . authorization code = request . json ( ) [ 'code' ] self . pin = request . json ( ) [ 'ecobee Pin' ] logger . error ( 'Please authorize your ecobee developer app with PIN code ' + self . pin + '\\n Goto https://www.ecobee.com/consumerportal' '/index.html, click\\n My Apps, Add application, Enter Pin' ' and click Authorize.\\n After authorizing, call request ' 'tokens() method.' )", "predictions": ["running a pin request ."], "references": ["method to request a pin from ecobee for authorization"], "bleu": 0.17348474258688365, "rouge_l": 0.2717149220489978}
{"id": 3285, "code": "def request tokens ( self ) : url = 'https://api.ecobee.com/token' params = { 'grant type' : 'ecobee Pin' , 'code' : self . authorization code , 'client id' : self . api key } try : request = requests . post ( url , params = params ) except Request Exception : logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" \"Could not request token.\" ) return if request . status code == requests . codes . ok : self . access token = request . json ( ) [ 'access token' ] self . refresh token = request . json ( ) [ 'refresh token' ] self . write tokens to file ( ) self . pin = None else : logger . warn ( 'Error while requesting tokens from ecobee.com.' ' Status code: ' + str ( request . status code ) ) return", "predictions": ["makes a request for the tokens ."], "references": ["method to request api tokens from ecobee"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3286, "code": "def refresh tokens ( self ) : url = 'https://api.ecobee.com/token' params = { 'grant type' : 'refresh token' , 'refresh token' : self . refresh token , 'client id' : self . api key } request = requests . post ( url , params = params ) if request . status code == requests . codes . ok : self . access token = request . json ( ) [ 'access token' ] self . refresh token = request . json ( ) [ 'refresh token' ] self . write tokens to file ( ) return True else : self . request pin ( )", "predictions": ["refresh stats for refresh requests ."], "references": ["method to refresh api tokens from ecobee"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3287, "code": "def get thermostats ( self ) : url = 'https://api.ecobee.com/1/thermostat' header = { 'Content-Type' : 'application/json;charset=UTF-8' , 'Authorization' : 'Bearer ' + self . access token } params = { 'json' : ( '{\"selection\":{\"selection Type\":\"registered\",' '\"include Runtime\":\"true\",' '\"include Sensors\":\"true\",' '\"include Program\":\"true\",' '\"include Equipment Status\":\"true\",' '\"include Events\":\"true\",' '\"include Weather\":\"true\",' '\"include Settings\":\"true\"}}' ) } try : request = requests . get ( url , headers = header , params = params ) except Request Exception : logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" ) return None if request . status code == requests . codes . ok : self . authenticated = True self . thermostats = request . json ( ) [ 'thermostat List' ] return self . thermostats else : self . authenticated = False logger . info ( \"Error connecting to Ecobee while attempting to get \" \"thermostat data.  Refreshing tokens and trying again.\" ) if self . refresh tokens ( ) : return self . get thermostats ( ) else : return None", "predictions": ["we get here from ( and . ."], "references": ["set self . thermostats to a json list of thermostats from ecobee"], "bleu": 0.10764345432696364, "rouge_l": 0.09651898734177215}
{"id": 3288, "code": "def write tokens to file ( self ) : config = dict ( ) config [ 'API KEY' ] = self . api key config [ 'ACCESS TOKEN' ] = self . access token config [ 'REFRESH TOKEN' ] = self . refresh token config [ 'AUTHORIZATION CODE' ] = self . authorization code if self . file based config : config from file ( self . config filename , config ) else : self . config = config", "predictions": ["write out the configuration object into a file ."], "references": ["write api tokens to a file"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 3289, "code": "def set hvac mode ( self , index , hvac mode ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"thermostat\" : { \"settings\" : { \"hvac Mode\" : hvac mode } } } log msg action = \"set HVAC mode\" return self . make request ( body , log msg action )", "predictions": ["set the hvac to be passed to the client ."], "references": ["possible hvac modes are auto auxheatonly cool heat off"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 3290, "code": "def set fan min on time ( self , index , fan min on time ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"thermostat\" : { \"settings\" : { \"fan Min On Time\" : fan min on time } } } log msg action = \"set fan minimum on time.\" return self . make request ( body , log msg action )", "predictions": ["for now we need to go to get the request on the log ."], "references": ["the minimum time in minutes to run the fan each hour . value from 1 to 60"], "bleu": 0.09389419352536649, "rouge_l": 0.19022869022869024}
{"id": 3291, "code": "def set climate hold ( self , index , climate , hold type = \"next Transition\" ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"set Hold\" , \"params\" : { \"hold Type\" : hold type , \"hold Climate Ref\" : climate } } ] } log msg action = \"set climate hold\" return self . make request ( body , log msg action )", "predictions": ["set action to hold the hold operation on the session ."], "references": ["set a climate hold - ie away home sleep"], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 3292, "code": "def delete vacation ( self , index , vacation ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"delete Vacation\" , \"params\" : { \"name\" : vacation } } ] } log msg action = \"delete a vacation\" return self . make request ( body , log msg action )", "predictions": ["deletes an ( request ."], "references": ["delete the vacation with name vacation"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 3293, "code": "def resume program ( self , index , resume all = False ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"resume Program\" , \"params\" : { \"resume All\" : resume all } } ] } log msg action = \"resume program\" return self . make request ( body , log msg action )", "predictions": ["resume a program ."], "references": ["resume currently scheduled program"], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 3294, "code": "def send message ( self , index , message = \"Hello from python-ecobee!\" ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"send Message\" , \"params\" : { \"text\" : message [ 0 : 500 ] } } ] } log msg action = \"send message\" return self . make request ( body , log msg action )", "predictions": ["send message to the configured 500 device ."], "references": ["send a message to the thermostat"], "bleu": 0.3155984539112945, "rouge_l": 0.5865384615384615}
{"id": 3295, "code": "def dict self ( self ) : return { k : v for k , v in self . dict . items ( ) if k in FSM ATTRS }", "predictions": ["a convenience method for scrolled this class ."], "references": ["return the self object attributes not inherited as dict ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 3296, "code": "def reset ( self , iface = None , client mac = None , xid = None , scriptfile = None ) : logger . debug ( 'Reseting attributes.' ) if iface is None : iface = conf . iface if client mac is None : tempmac = get if raw hwaddr ( iface ) if isinstance ( tempmac , tuple ) and len ( tempmac ) == 2 : mac = tempmac [ 1 ] else : mac = tempmac client mac = str2mac ( mac ) self . client = DHCPCAP ( iface = iface , client mac = client mac , xid = xid ) if scriptfile is not None : self . script = Client Script ( scriptfile ) else : self . script = None self . time sent request = None self . discover attempts = 0 self . request attempts = 0 self . current state = STATE PREINIT self . offers = list ( )", "predictions": ["resets and prepares a try to sigint to a try to sigint ."], "references": ["reset object attributes when state is init ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 3297, "code": "def get timeout ( self , state , function ) : state = STATES2NAMES [ state ] for timeout fn t in self . timeout [ state ] : if timeout fn t [ 1 ] is not None and timeout fn t [ 1 ] . atmt condname == function . atmt condname : logger . debug ( 'Timeout for state %s, function %s, is %s' , state , function . atmt condname , timeout fn t [ 0 ] ) return timeout fn t [ 0 ] return None", "predictions": ["wrap the call to handling handling for handling ."], "references": ["workaround to get timeout in the atmt . timeout class method ."], "bleu": 0.12026590852507517, "rouge_l": 0.1856925418569254}
{"id": 3298, "code": "def set timers ( self ) : logger . debug ( 'setting timeouts' ) self . set timeout ( self . current state , self . renewing time expires , self . client . lease . renewal time ) self . set timeout ( self . current state , self . rebinding time expires , self . client . lease . rebinding time )", "predictions": ["sets the renewal len len object ."], "references": ["set renewal rebinding times ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3299, "code": "def process received nak ( self , pkt ) : if isnak ( pkt ) : logger . info ( 'DHCPNAK of %s from %s' , self . client . client ip , self . client . server ip ) return True return False", "predictions": ["retrieves a response from the database and ( do not sleep : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 1 : 0 : 1 : 1 :"], "references": ["process a received nak packet ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 3300, "code": "def receive offer ( self , pkt ) : logger . debug ( \"C2. Received OFFER?, in SELECTING state.\" ) if isoffer ( pkt ) : logger . debug ( \"C2: T, OFFER received\" ) self . offers . append ( pkt ) if len ( self . offers ) >= MAX OFFERS COLLECTED : logger . debug ( \"C2.5: T, raise REQUESTING.\" ) self . select offer ( ) raise self . REQUESTING ( ) logger . debug ( \"C2.5: F, raise SELECTING.\" ) raise self . SELECTING ( )", "predictions": ["increments a set of ( ."], "references": ["receive offer on selecting state ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3301, "code": "def receive ack requesting ( self , pkt ) : logger . debug ( \"C3. Received ACK?, in REQUESTING state.\" ) if self . process received ack ( pkt ) : logger . debug ( \"C3: T. Received ACK, in REQUESTING state, \" \"raise BOUND.\" ) raise self . BOUND ( )", "predictions": ["sends a packet to the server ."], "references": ["receive ack in requesting state ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3302, "code": "def receive nak requesting ( self , pkt ) : logger . debug ( \"C3.1. Received NAK?, in REQUESTING state.\" ) if self . process received nak ( pkt ) : logger . debug ( \"C3.1: T. Received NAK, in REQUESTING state, \" \"raise INIT.\" ) raise self . INIT ( )", "predictions": ["attempts to f a packet from the given list of implemented methods ."], "references": ["receive nak in requesting state ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 3303, "code": "def receive ack renewing ( self , pkt ) : logger . debug ( \"C3. Received ACK?, in RENEWING state.\" ) if self . process received ack ( pkt ) : logger . debug ( \"C3: T. Received ACK, in RENEWING state, \" \"raise BOUND.\" ) raise self . BOUND ( )", "predictions": ["sends a packet from the given list of messages to the database ."], "references": ["receive ack in renewing state ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 3304, "code": "def receive nak renewing ( self , pkt ) : logger . debug ( \"C3.1. Received NAK?, in RENEWING state.\" ) if self . process received nak ( pkt ) : logger . debug ( \"C3.1: T. Received NAK, in RENEWING state, \" \" raise INIT.\" ) raise self . INIT ( )", "predictions": ["attempts to supports the given message from the packet ."], "references": ["receive nak in renewing state ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 3305, "code": "def receive ack rebinding ( self , pkt ) : logger . debug ( \"C3. Received ACK?, in REBINDING state.\" ) if self . process received ack ( pkt ) : logger . debug ( \"C3: T. Received ACK, in REBINDING state, \" \"raise BOUND.\" ) raise self . BOUND ( )", "predictions": ["sends a packet from the given list of messages to the database ."], "references": ["receive ack in rebinding state ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 3306, "code": "def receive nak rebinding ( self , pkt ) : logger . debug ( \"C3.1. Received NAK?, in RENEWING state.\" ) if self . process received nak ( pkt ) : logger . debug ( \"C3.1: T. Received NAK, in RENEWING state, \" \"raise INIT.\" ) raise self . INIT ( )", "predictions": ["attempts to main processing the given message from the server ."], "references": ["receive nak in rebinding state ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 3307, "code": "def set ( self , name , value ) : clone = self . clone ( ) if django . VERSION [ 0 ] <= 1 and django . VERSION [ 1 ] <= 4 : value = value or None clone . qsl = [ ( q , v ) for ( q , v ) in self . qsl if q != name ] if value is not None : clone . qsl . append ( ( name , value ) ) return clone", "predictions": ["layer sets with : . create a . . . . . . . . . . . . . . . . . . ."], "references": ["assign a value remove if it s none"], "bleu": 0.04668049023095243, "rouge_l": 0.0650319829424307}
{"id": 3308, "code": "def add ( self , name , value ) : clone = self . clone ( ) clone . qsl = [ p for p in self . qsl if not ( p [ 0 ] == name and p [ 1 ] == value ) ] clone . qsl . append ( ( name , value , ) ) return clone", "predictions": ["merge this class with the given name ."], "references": ["append a value to multiple value parameter ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3309, "code": "def remove ( self , name , value ) : clone = self . clone ( ) clone . qsl = [ qb for qb in self . qsl if qb != ( name , str ( value ) ) ] return clone", "predictions": ["removes the content of this schema ."], "references": ["remove a value from multiple value parameter ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3310, "code": "def read tdms ( tdms file ) : tdms file = nptdms . Tdms File ( tdms file ) ch names = [ ] ch data = [ ] for o in tdms file . objects . values ( ) : if o . data is not None and len ( o . data ) : chn = o . path . split ( '/' ) [ - 1 ] . strip ( \"'\" ) if \"unit string\" in o . properties : unit = o . properties [ \"unit string\" ] ch names . append ( \"{} [{}]\" . format ( chn , unit ) ) else : ch names . append ( chn ) ch data . append ( o . data ) return ch names , ch data", "predictions": ["multiply an array of strings using the ( ."], "references": ["read tdms file and return channel names and data"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3311, "code": "def tdms2fcs ( tdms file ) : fcs file = tdms file [ : - 4 ] + \"fcs\" chn names , data = read tdms ( tdms file ) chn names , data = add deformation ( chn names , data ) fcswrite . write fcs ( filename = fcs file , chn names = chn names , data = np . array ( data ) . transpose ( ) )", "predictions": [". : setup a print file with the same name ."], "references": ["creates an fcs file for a given tdms file"], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 3312, "code": "def equal ( self , cwd ) : cmd = [ \"diff\" ] cmd . append ( \"-q\" ) cmd . append ( self . left . get name ( ) ) cmd . append ( self . right . get name ( ) ) try : Process ( cmd ) . run ( cwd = cwd , suppress output = True ) except Subprocess Error as e : if e . get returncode ( ) == 1 : return False else : raise e return True", "predictions": [", for this command ."], "references": ["returns true if left and right are equal"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3313, "code": "def backup file ( self , file , patch ) : dest dir = self . quilt pc + patch . get name ( ) file dir = file . get directory ( ) if file dir : #TODO get relative path dest dir = dest dir + file dir backup = Backup ( ) backup . backup file ( file , dest dir , copy empty = True )", "predictions": ["copies the provided filename to the make ."], "references": ["creates a backup of file"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3314, "code": "def link ( self , link ) : if isinstance ( link , File ) : link = link . filename os . link ( self . filename , link )", "predictions": ["we want to main main main menu for this node to be the main main main main thread ."], "references": ["create hard link as link to this file"], "bleu": 0.0712695567709093, "rouge_l": 0.1598951507208388}
{"id": 3315, "code": "def copy ( self , dest ) : if isinstance ( dest , File ) : dest dir = dest . get directory ( ) dest dir . create ( ) dest = dest . filename elif isinstance ( dest , Directory ) : dest = dest . dirname shutil . copy2 ( self . filename , dest )", "predictions": ["config file to a new destination"], "references": ["copy file to destination"], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 3316, "code": "def apply patch ( self , patch name , force = False , quiet = False ) : self . check ( ) patch = Patch ( patch name ) patches = self . series . patches until ( patch ) [ : ] applied = self . db . applied patches ( ) for patch in applied : if patch in patches : patches . remove ( patch ) if not patches : raise All Patches Applied ( self . series , self . db . top patch ( ) ) self . applying ( patch ) try : for cur patch in patches : self . apply patch ( cur patch , force , quiet ) finally : self . db . save ( ) self . applied ( self . db . top patch ( ) )", "predictions": ["request all the , pin and pin sentences ."], "references": ["apply all patches up to patch_name"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3317, "code": "def apply next patch ( self , force = False , quiet = False ) : self . check ( ) top = self . db . top patch ( ) if not top : patch = self . series . first patch ( ) else : patch = self . series . patch after ( top ) if not patch : raise All Patches Applied ( self . series , top ) self . applying ( patch ) self . apply patch ( patch , force , quiet ) self . db . save ( ) self . applied ( self . db . top patch ( ) )", "predictions": ["the tokens ."], "references": ["apply next patch in series file"], "bleu": 0.16620830006469264, "rouge_l": 0.0}
{"id": 3318, "code": "def apply all ( self , force = False , quiet = False ) : self . check ( ) top = self . db . top patch ( ) if top : patches = self . series . patches after ( top ) else : patches = self . series . patches ( ) if not patches : raise All Patches Applied ( self . series , top ) try : for patch in patches : self . applying ( patch ) self . apply patch ( patch , force , quiet ) finally : self . db . save ( ) self . applied ( self . db . top patch ( ) )", "predictions": ["refresh all token found in the database ."], "references": ["apply all patches in series file"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 3319, "code": "def read ( self ) : self . patchlines = [ ] self . patch2line = dict ( ) if self . exists ( ) : with open ( self . series file , \"r\" ) as f : for line in f : self . add patch ( line )", "predictions": ["reads a line line line object ."], "references": ["reads all patches from the series file"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3320, "code": "def save ( self ) : with open ( self . series file , \"wb\" ) as f : for patchline in self . patchlines : f . write ( encode str ( str ( patchline ) ) ) f . write ( b\"\\n\" )", "predictions": ["saves this , as a config file ."], "references": ["saves current patches list in the series file"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 3321, "code": "def add patch ( self , patch ) : patchline = Patch Line ( patch ) patch = patchline . get patch ( ) if patch : self . patch2line [ patch ] = patchline self . patchlines . append ( patchline )", "predictions": ["patch method . adds a single patch instance to this instance ."], "references": ["add a patch to the patches list"], "bleu": 0.1235622127262679, "rouge_l": 0.3315217391304348}
{"id": 3322, "code": "def insert patches ( self , patches ) : patchlines = [ ] for patch name in patches : patchline = Patch Line ( patch name ) patch = patchline . get patch ( ) if patch : self . patch2line [ patch ] = patchline patchlines . append ( patchline ) patchlines . extend ( self . patchlines ) self . patchlines = patchlines", "predictions": ["set a list of fan fan ."], "references": ["insert list of patches at the front of the curent patches list"], "bleu": 0.11967409389919142, "rouge_l": 0.20098846787479407}
{"id": 3323, "code": "def add patches ( self , patches , after = None ) : if after is None : self . insert patches ( patches ) else : self . check patch ( after ) patchlines = self . patchlines before ( after ) patchlines . append ( self . patch2line [ after ] ) for patch in patches : patchline = Patch Line ( patch ) patchlines . append ( patchline ) self . patch2line [ patchline . get patch ( ) ] = patchline patchlines . extend ( self . patchlines after ( after ) ) self . patchlines = patchlines", "predictions": ["set this class as an array of climate climate ."], "references": ["add a list of patches to the patches list"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 3324, "code": "def remove patch ( self , patch ) : self . check patch ( patch ) patchline = self . patch2line [ patch ] del self . patch2line [ patch ] self . patchlines . remove ( patchline )", "predictions": ["delete a patch for this set ."], "references": ["remove a patch from the patches list"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 3325, "code": "def patches after ( self , patch ) : return [ line . get patch ( ) for line in self . patchlines after ( patch ) if line . get patch ( ) ]", "predictions": ["method that transforms resume program ."], "references": ["returns a list of patches after patch from the patches list"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 3326, "code": "def patches before ( self , patch ) : return [ line . get patch ( ) for line in self . patchlines before ( patch ) if line . get patch ( ) ]", "predictions": ["add send send send send send send message to all send send send send send send send send ."], "references": ["returns a list of patches before patch from the patches list"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 3327, "code": "def create ( self ) : if not os . path . exists ( self . dirname ) : os . makedirs ( self . dirname ) self . create version ( self . version file )", "predictions": ["creates a new if this file object is available for changes to the ui ."], "references": ["creates the dirname and inserts a . version file"], "bleu": 0.1082597837309053, "rouge_l": 0.26180257510729615}
{"id": 3328, "code": "def import patches ( self , patches ) : dest dir = self . quilt patches patch names = [ ] for patch in patches : patch name = os . path . basename ( patch ) patch file = File ( patch ) dest file = dest dir + File ( patch name ) patch file . copy ( dest file ) patch names . append ( patch name ) self . import patches ( patch names )", "predictions": ["copies all patches from this tty into the provided patches ."], "references": ["import several patches into the patch queue"], "bleu": 0.16108992769687397, "rouge_l": 0.3472485768500949}
{"id": 3329, "code": "def way ( self , w ) : if w . id not in self . way ids : return way points = [ ] for n in w . nodes : try : way points . append ( Point ( n . location . lon , n . location . lat ) ) except o . Invalid Location Error : logging . debug ( 'Invalid Location Error at way %s node %s' , w . id , n . ref ) self . ways [ w . id ] = Way ( w . id , way points )", "predictions": ["for a way that has just been previously installed but not yet been previously connected to the way ."], "references": ["process each way ."], "bleu": 0.08475426399505566, "rouge_l": 0.19709208400646203}
{"id": 3330, "code": "def missing node ids ( self ) : present node ids = self . nodes . keys ( ) for nid in self . node ids : if nid not in present node ids : yield nid", "predictions": ["a helper to yield all nodes of *node* about the given node ."], "references": ["get a list of nodes not found in osm data ."], "bleu": 0.12011055432195765, "rouge_l": 0.2538141470180305}
{"id": 3331, "code": "def node ( self , n ) : if n . id not in self . node ids : return try : self . nodes [ n . id ] = Node ( n . id , n . location . lon , n . location . lat , { t . k : t . v for t in n . tags } ) except o . Invalid Location Error : logging . debug ( 'Invalid Location Error at node %s' , n . id )", "predictions": ["get full description of this node and all its associated nodes ."], "references": ["process each node ."], "bleu": 0.11498759556447223, "rouge_l": 0.27477477477477474}
{"id": 3332, "code": "def build route ( relation ) : if relation . tags . get ( 'type' ) != 'route' : return short name = create route short name ( relation ) color = relation . tags . get ( 'color' ) return Route ( relation . id , short name , create route long name ( relation , short name ) , map osm route type to gtfs ( relation . tags . get ( 'route' ) ) , 'https://www.openstreetmap.org/relation/{}' . format ( relation . id ) , color . strip ( '#' ) if color else '' , get agency id ( relation ) )", "predictions": ["create the previously saved relation ."], "references": ["extract information of one route ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3333, "code": "def create route long name ( relation , short name ) : if relation . tags . get ( 'from' ) and relation . tags . get ( 'to' ) : return \"{0}-to-{1}\" . format ( relation . tags . get ( 'from' ) , relation . tags . get ( 'to' ) ) name = relation . tags . get ( 'name' ) or relation . tags . get ( 'alt name' ) or \"OSM Route No. {}\" . format ( relation . id ) if short name and name . startswith ( short name ) : return name [ len ( short name ) : ] return name", "predictions": ["create an ( object in the shared relation . this method does not log the name of the relation . it can only be called from the relation ."], "references": ["create a meaningful route name ."], "bleu": 0.04965977366141172, "rouge_l": 0.19447396386822527}
{"id": 3334, "code": "def get agency id ( relation ) : op = relation . tags . get ( 'operator' ) if op : return int ( hashlib . sha256 ( op . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 return - 1", "predictions": ["get the previously processed id ."], "references": ["construct an id for agency using its tags ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3335, "code": "def process ( self ) : self . rh = Relation Handler ( ) self . rh . apply file ( self . filename ) logging . debug ( 'Found %d public transport relations.' , len ( self . rh . relations ) ) node ids , stop node ids , way ids , reverse map = self . collect ids ( ) self . nh = Node Handler ( node ids ) self . nh . apply file ( self . filename , locations = True ) count = 0 for idx , missing node id in enumerate ( self . nh . missing node ids ) : count += 1 logging . warning ( '[no data] missing stop node. rel: https://osm.org/relation/%s node: https://osm.org/node/%s.' , reverse map [ missing node id ] , missing node id ) if count : logging . warning ( '%d nodes that appear in relations are missing.' , count ) else : logging . debug ( 'Lucky you! All relation member nodes were found.' ) self . wh = Way Handler ( way ids ) self . wh . apply file ( self . filename , locations = True )", "predictions": ["this adds a set of nodes to each node ."], "references": ["process the files and collect necessary data ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 3336, "code": "def relation ( self , rel ) : rel type = rel . tags . get ( 'type' ) if any ( [ rel . deleted , not rel . visible , not self . is new version ( rel ) , rel type not in [ 'route' , 'public transport' ] ] ) : return route tag = rel . tags . get ( 'route' ) if rel type == 'route' and route tag not in self . transit route types : return public transport = rel . tags . get ( 'public transport' ) if rel type == 'public transport' and public transport != 'stop area' : return self . relations [ rel . id ] = Relation ( rel . id , { 'type' : rel type , 'public transport' : public transport , 'route' : route tag , 'operator' : rel . tags . get ( 'operator' ) , 'color' : rel . tags . get ( 'color' ) , 'ref' : rel . tags . get ( 'ref' ) , 'from' : rel . tags . get ( 'from' ) , 'to' : rel . tags . get ( 'to' ) , 'name' : rel . tags . get ( 'name' ) , 'alt name' : rel . tags . get ( 'alt name' ) , 'url' : rel . tags . get ( 'url' ) , 'contact website' : rel . tags . get ( 'contact:website' ) } , [ ( member . type , member . ref , member . role ) for member in rel . members ] ) self . versions [ rel . id ] = rel . version", "predictions": ["get the object representing this relation ."], "references": ["process each relation ."], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 3337, "code": "def patch agencies ( agencies ) : yield Agency ( - 1 , 'http://hiposfer.com' , 'Unknown agency' , 'Europe/Berlin' ) for agency id , agency url , agency name , agency timezone in agencies : if not agency url : agency url = 'http://hiposfer.com' if not agency timezone : agency timezone = 'Europe/Berlin' yield Agency ( agency id , agency url , agency name , agency timezone )", "predictions": ["patch the time zone so that all resources can be saved ."], "references": ["fill the fields that are necessary for passing transitfeed checks ."], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 3338, "code": "def create dummy trip stoptimes ( trip id , stops , first service time ) : waiting = datetime . timedelta ( seconds = 30 ) arrival = first service time last departure = first service time last departure hour = ( arrival + waiting ) . hour last stop = None departure hour = None arrival hour = None for stop sequence , stop in enumerate ( stops ) : arrival = last departure + get time from last stop ( last stop , stop ) departure = arrival + waiting if arrival . hour < last departure hour : diff = last departure hour arrival hour = arrival . hour + diff departure hour = departure . hour + diff last departure hour = departure . hour + diff else : arrival hour = arrival . hour departure hour = departure . hour last departure hour = departure . hour if departure . hour < arrival . hour : diff = last departure hour departure hour = departure . hour + diff last departure hour = departure . hour + diff yield { 'trip id' : trip id , 'arrival time' : '{:02}:{}' . format ( arrival hour , arrival . strftime ( '%M:%S' ) ) , 'departure time' : '{:02}:{}' . format ( departure hour , departure . strftime ( '%M:%S' ) ) , 'stop id' : stop . stop id , 'stop sequence' : stop sequence } last stop = stop last departure = departure", "predictions": ["create a dummy dummy dummy run run with the given waiting code ."], "references": ["create station stop times for each trip ."], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 3339, "code": "def write zipped ( self , filepath ) : with zipfile . Zip File ( filepath , mode = 'w' , compression = zipfile . ZIP DEFLATED ) as zfile : for name , buffer in self . buffers . items ( ) : encoded values = io . Bytes IO ( buffer . getvalue ( ) . encode ( 'utf-8' ) ) zfile . writestr ( '{}.txt' . format ( name ) , encoded values . getbuffer ( ) ) for name , path in self . files . items ( ) : zfile . write ( path , arcname = name )", "predictions": ["write the specified files to the this object as a zipfile ."], "references": ["write the gtfs feed in the given file ."], "bleu": 0.15537125692760353, "rouge_l": 0.39102564102564097}
{"id": 3340, "code": "def write unzipped ( self , destination ) : for name , buffer in self . buffers . items ( ) : with open ( os . path . join ( destination , '{}.txt' . format ( name ) ) , 'w' , encoding = 'utf-8' ) as file : file . write ( buffer . getvalue ( ) ) for name , path in self . files . items ( ) : shutil . copy ( path , os . path . join ( destination , name ) )", "predictions": ["write out all items from this object into the database ."], "references": ["write gtfs text files in the given path ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 3341, "code": "def build agency ( relation , nodes ) : # op = relation . tags . get ( 'operator' ) agency url = relation . tags . get ( 'url' ) or relation . tags . get ( 'contact website' ) if not op : return agency id = int ( hashlib . sha256 ( op . encode ( 'utf8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 return Agency ( agency id , agency url , op , '' )", "predictions": ["build the hashlib for this relation ."], "references": ["extract agency information ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3342, "code": "def extract stops ( relation , nodes , visited stop ids , stop to station map ) : for member type , member id , member role in relation . member info : if member id not in visited stop ids and member id in nodes and member role in ( 'stop' , 'halt' ) : location type = '' visited stop ids . add ( member id ) yield Stop ( member id , nodes [ member id ] . tags . get ( 'name' ) or \"Unnamed {} stop.\" . format ( relation . tags . get ( 'route' ) ) , nodes [ member id ] . lon if member id in nodes else '' , nodes [ member id ] . lat if member id in nodes else '' , relation . id , map wheelchair ( nodes [ member id ] . tags . get ( 'wheelchair' ) ) , location type , stop to station map . get ( member id , '' ) )", "predictions": ["extract all the 'stop' from all available nodes ."], "references": ["extract stops in a relation ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 3343, "code": "def build shape ( relation , nodes , ways ) : sequence index = 0 for member type , member id , member role in relation . member info : if member id in nodes : yield Shape ( relation . id , nodes [ member id ] . lat , nodes [ member id ] . lon , sequence index ) sequence index += 1 elif member id in ways : continue else : pass", "predictions": ["build a shape for all layers in a sequence of nodes ."], "references": ["extract shape of one route ."], "bleu": 0.1235622127262679, "rouge_l": 0.3546511627906977}
{"id": 3344, "code": "def get supported versions ( self ) : if not hasattr ( self , ' versions' ) : try : self . versions = [ self . send apdu ( INS GET VERSION ) . decode ( ) ] except exc . APDU Error as e : self . versions = [ 'v0' ] if e . code == 0x6d00 else [ ] return self . versions", "predictions": ["get list of versions that can be sent to this process ."], "references": ["gets a list of supported u2f versions from the device ."], "bleu": 0.15537125692760353, "rouge_l": 0.3505747126436781}
{"id": 3345, "code": "def send apdu ( self , ins , p1 = 0 , p2 = 0 , data = b'' ) : if data is None : data = b'' elif isinstance ( data , int ) : data = int2byte ( data ) size = len ( data ) l0 = size >> 16 & 0xff l1 = size >> 8 & 0xff l2 = size & 0xff apdu data = struct . pack ( 'B B B B B B B %is B B' % size , 0 , ins , p1 , p2 , l0 , l1 , l2 , data , 0x00 , 0x00 ) try : resp = self . do send apdu ( apdu data ) except Exception as e : raise exc . Device Error ( e ) status = struct . unpack ( '>H' , resp [ - 2 : ] ) [ 0 ] data = resp [ : - 2 ] if status != APDU OK : raise exc . APDU Error ( status ) return data", "predictions": ["send request over this client ."], "references": ["sends an apdu to the device and waits for a response ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 3346, "code": "def register ( devices , params , facet ) : for device in devices [ : ] : try : device . open ( ) except : devices . remove ( device ) sys . stderr . write ( '\\n Touch the U2F device you wish to register...\\n' ) try : while devices : removed = [ ] for device in devices : try : return u2f . register ( device , params , facet ) except exc . APDU Error as e : if e . code == APDU USE NOT SATISFIED : pass else : removed . append ( device ) except exc . Device Error : removed . append ( device ) devices = [ d for d in devices if d not in removed ] for d in removed : d . close ( ) time . sleep ( 0.25 ) finally : for device in devices : device . close ( ) sys . stderr . write ( '\\n Unable to register with any U2F device.\\n' ) sys . exit ( 1 )", "predictions": ["the ( method for ( ."], "references": ["interactively registers a single u2f device given the registrationrequest ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3347, "code": "def u2str ( data ) : if isinstance ( data , dict ) : return { u2str ( k ) : u2str ( v ) for k , v in data . items ( ) } elif isinstance ( data , list ) : return [ u2str ( x ) for x in data ] elif isinstance ( data , text type ) : return data . encode ( 'utf-8' ) else : return data", "predictions": ["get the . of the given data ."], "references": ["recursively converts unicode objects to utf - 8 encoded byte strings ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 3348, "code": "def wrap function ( func = None , error threshold = None , reraise exception = True , save current stack trace = True ) : # if func : return flawless . client . client . wrap function with error decorator ( func = func , error threshold = error threshold , reraise exception = reraise exception , save current stack trace = save current stack trace ) else : return functools . partial ( flawless . client . client . wrap function with error decorator , error threshold = error threshold , reraise exception = reraise exception , save current stack trace = save current stack trace )", "predictions": ["this will wrap a method to replace entity references ."], "references": ["wraps a function with reporting to errors backend"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3349, "code": "def get entry ( self , entry , entry tree ) : for e in entry tree [ entry . filename ] : if entry == e : return e", "predictions": ["retrieves the full path for this type ."], "references": ["helper function for retrieving a particular entry from the prefix trees"], "bleu": 0.12197601375336842, "rouge_l": 0.10234899328859062}
{"id": 3350, "code": "def markdown to re ST ( text ) : text = re . sub ( pattern = r\"\\n       (\\w+) - (.+)\\n\" , repl = r\"\\n\\n       *\\g<1>* - \\g<2>\\n\" , string = text ) text = re . sub ( pattern = r\"\\[([^\\]]+)\\]\\([^)]+\\)\" , repl = r\"\\g<1>\" , string = text ) text = re . sub ( pattern = r\"\\n(\\d+). \" , repl = r\"\\n\\\\\\g<1>. \" , string = text ) return text", "predictions": ["converts all the . entities to the . ."], "references": ["this is not a general purpose converter . only converts this readme"], "bleu": 0.11192003885776355, "rouge_l": 0.0928462709284627}
{"id": 3351, "code": "def record error ( hostname , exc info , preceding stack = None , error threshold = None , additional info = None ) : stack = [ ] exc type , exc value , sys traceback = exc info while sys traceback is not None : stack . append ( sys traceback ) sys traceback = sys traceback . tb next stack lines = [ ] for row in preceding stack or [ ] : stack lines . append ( api ttypes . Stack Line ( filename = os . path . abspath ( row [ 0 ] ) , line number = row [ 1 ] , function name = row [ 2 ] , text = row [ 3 ] ) ) for index , tb in enumerate ( stack ) : filename = tb . tb frame . f code . co filename func name = tb . tb frame . f code . co name lineno = tb . tb lineno line = linecache . getline ( filename , lineno , tb . tb frame . f globals ) frame locals = None if index >= ( len ( stack ) - NUM FRAMES TO SAVE ) : frame locals = dict ( ( k , myrepr ( k , v ) ) for k , v in list ( tb . tb frame . f locals . items ( ) ) [ : MAX LOCALS ] if k != \"self\" ) if \"self\" in tb . tb frame . f locals and hasattr ( tb . tb frame . f locals [ \"self\" ] , \" dict \" ) : frame locals . update ( dict ( ( \"self.\" + k , myrepr ( k , v ) ) for k , v in list ( tb . tb frame . f locals [ \"self\" ] . dict . items ( ) ) [ : MAX LOCALS ] if k != \"self\" ) ) stack lines . append ( api ttypes . Stack Line ( filename = os . path . abspath ( filename ) , line number = lineno , function name = func name , text = line , frame locals = frame locals ) ) key = Cached Error Info . get hash key ( stack lines ) info = ERROR CACHE . get ( key ) or Cached Error Info ( ) info . increment ( ) ERROR CACHE [ key ] = info if info . should report ( ) : error count = info . mark reported ( ) send request ( api ttypes . Record Error Request ( traceback = stack lines , exception message = repr ( exc value ) , exception type = exc type . module + \".\" + exc type . name , hostname = hostname , error threshold = error threshold , additional info = additional info , error count = error count , ) )", "predictions": ["record all error results ."], "references": ["helper function to record errors to the flawless backend"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 3352, "code": "def url to image ( url ) : r = requests . get ( url ) image = String IO ( r . content ) return image", "predictions": ["get an image with an image url"], "references": ["fetch an image from url and convert it into a pillow image object"], "bleu": 0.11787460936700446, "rouge_l": 0.2846034214618974}
{"id": 3353, "code": "def string to image ( image string ) : image filelike = String IO ( image string ) image = Image . open ( image filelike ) return image", "predictions": ["convert an image to a string ."], "references": ["convert string datas into a pillow image object"], "bleu": 0.2024757945132846, "rouge_l": 0.2634989200863931}
{"id": 3354, "code": "def is big enough ( image , size ) : if ( size [ 0 ] > image . size [ 0 ] ) and ( size [ 1 ] > image . size [ 1 ] ) : raise Image Size Error ( image . size , size )", "predictions": ["check if a certain image should be oriented ."], "references": ["check that the image s size superior to size"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3355, "code": "def width is big enough ( image , width ) : if width > image . size [ 0 ] : raise Image Size Error ( image . size [ 0 ] , width )", "predictions": ["test if the image is in a ( ."], "references": ["check that the image width is superior to width"], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 3356, "code": "def height is big enough ( image , height ) : if height > image . size [ 1 ] : raise Image Size Error ( image . size [ 1 ] , height )", "predictions": ["test if a height is in the height of a height ."], "references": ["check that the image height is superior to height"], "bleu": 0.15537125692760353, "rouge_l": 0.2932692307692307}
{"id": 3357, "code": "def parse category ( self , item , field name , source name ) : slug = category map . get ( self . get value ( item , source name ) , None ) if not slug : return None try : return Category . objects . get ( slug = slug ) except Category . Does Not Exist : pass", "predictions": ["parse a category from a map ."], "references": ["converts the text category to a tasks . category instance ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 3358, "code": "def parse totals ( self , item , field name , source name ) : val = self . get value ( item , source name ) try : return int ( val ) except : return 0", "predictions": ["parses a unicode value ."], "references": ["parse numeric fields ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3359, "code": "def get items ( self ) : for event , item in Element Tree . iterparse ( self . source ) : if item . tag == self . item tag name : yield item item . clear ( )", "predictions": ["a generator to get all items of the given event ."], "references": ["iterator of the list of items in the xml source ."], "bleu": 0.17033186037639278, "rouge_l": 0.2727272727272727}
{"id": 3360, "code": "def save error ( self , data , exception info ) : self . errors . append ( { 'data' : data , 'exception' : '' . join ( format exception ( * exception info ) ) , } )", "predictions": ["saves the current patches ."], "references": ["saves an error in the error list ."], "bleu": 0.1781815298791261, "rouge_l": 0.44309927360774815}
{"id": 3361, "code": "def parse ( self ) : if not self . loaded : self . load ( self . source ) for item in self . get items ( ) : data = self . parse item ( item ) instance = self . get instance ( data ) self . feed instance ( data , instance ) try : self . save item ( item , data , instance ) except Exception as e : self . save error ( data , sys . exc info ( ) ) self . unload ( )", "predictions": ["unload ( , . , . , . , . , . , ( , . , ( , ( , ( , ( , ( , ( , ( , ( , ( , ( , ( , ( , ( , ( , ( , ( , ("], "references": ["parses all data from the source saving model instances ."], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 3362, "code": "def parse item ( self , item ) : parsed data = { } for field name in self . fields : source name = self . field map . get ( field name , field name ) parse = getattr ( self , 'parse %s' % field name , None ) if parse : value = parse ( item , field name , source name ) else : value = self . get value ( item , source name ) parsed data [ field name ] = value return parsed data", "predictions": ["parses a plist message from a plist file ."], "references": ["receives an item and returns a dictionary of field values ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 3363, "code": "def get instance ( self , data ) : unique fields = self . unique fields if not unique fields : return self . model ( ) filter = dict ( [ ( f , data [ f ] ) for f in unique fields ] ) try : instance = self . model . default manager . get ( * * filter ) except self . model . Does Not Exist : return self . model ( ) return instance", "predictions": ["node that returns a ] of id numbers ."], "references": ["get an item from the database or an empty one if not found ."], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 3364, "code": "def save item ( self , item , data , instance , commit = True ) : if commit : instance . save ( ) return instance", "predictions": ["saves the current state of the object as a . ."], "references": ["saves a model instance to the database ."], "bleu": 0.14323145079400493, "rouge_l": 0.32504440497335696}
{"id": 3365, "code": "def load ( self , source ) : self . source = open ( self . source , 'rb' ) self . loaded = True", "predictions": ["loads the texture from the given ( including the default is installed relation relation relation relation relation relation relation relation relation relation ."], "references": ["opens the source file ."], "bleu": 0.05856458233275369, "rouge_l": 0.16158940397350993}
{"id": 3366, "code": "def get items ( self ) : reader = csv . reader ( self . source ) headers = reader . next ( ) for row in reader : if not row : continue yield dict ( zip ( headers , row ) )", "predictions": ["a generator that returns a = value of all items ."], "references": ["iterator to read the rows of the csv file ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 3367, "code": "def allow network access ( self , value : bool ) : if self . is running : raise Value Error ( \"Cannot change network access settings on a running sandbox\" ) self . allow network access = value", "predictions": ["process the ( if not already set ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) = true ) ."], "references": ["raises valueerror if this sandbox instance is currently running ."], "bleu": 0.04180647946097227, "rouge_l": 0.10517241379310344}
{"id": 3368, "code": "def get enrollments for course by sis id ( self , sis course id , params = { } ) : return self . get enrollments for course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["retrieves the ( possibly relevant any any any any any any any any any any any any any any any any any any ( non - terminal any any any any any any any any any any any any any any any any any any any any any any any"], "references": ["return a list of all enrollments for the passed course sis id ."], "bleu": 0.02403051755364481, "rouge_l": 0.03550640279394646}
{"id": 3369, "code": "def get enrollments for section by sis id ( self , sis section id , params = { } ) : return self . get enrollments for section ( self . sis id ( sis section id , sis field = \"section\" ) , params )", "predictions": ["fetch each entry in this ) and passes it to the timezone ."], "references": ["return a list of all enrollments for the passed section sis id ."], "bleu": 0.10571070857151538, "rouge_l": 0.15384615384615383}
{"id": 3370, "code": "def get roles by account sis id ( self , account sis id , params = { } ) : return self . get roles in account ( self . sis id ( account sis id , sis field = \"account\" ) , params )", "predictions": ["fetch in group by id ."], "references": ["list the roles for an account for the passed account sis id ."], "bleu": 0.09052970298747198, "rouge_l": 0.19741100323624597}
{"id": 3371, "code": "def get role by account sis id ( self , account sis id , role id ) : return self . get role ( self . sis id ( account sis id , sis field = \"account\" ) , role id )", "predictions": ["get zipped class by filepath ."], "references": ["get information about a single role for the passed account sis id ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 3372, "code": "def get course by sis id ( self , sis course id , params = { } ) : return self . get course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["method to get the course field of this model ."], "references": ["return course resource for given sis id ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3373, "code": "def get courses in account by sis id ( self , sis account id , params = { } ) : return self . get courses in account ( self . sis id ( sis account id , sis field = \"account\" ) , params )", "predictions": ["fetch the amount of private key from the request ."], "references": ["return a list of courses for the passed account sis id ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 3374, "code": "def get published courses in account ( self , account id , params = { } ) : params [ \"published\" ] = True return self . get courses in account ( account id , params )", "predictions": ["this method will return the amount of stops in the request ."], "references": ["return a list of published courses for the passed account id ."], "bleu": 0.13065113298388567, "rouge_l": 0.3333333333333333}
{"id": 3375, "code": "def get published courses in account by sis id ( self , sis account id , params = { } ) : return self . get published courses in account ( self . sis id ( sis account id , sis field = \"account\" ) , params )", "predictions": ["get the shape of an , given a specific params ."], "references": ["return a list of published courses for the passed account sis id ."], "bleu": 0.11941964005964323, "rouge_l": 0.1641991924629879}
{"id": 3376, "code": "def get users for course ( self , course id , params = { } ) : url = COURSES API . format ( course id ) + \"/users\" data = self . get paged resource ( url , params = params ) users = [ ] for datum in data : users . append ( Canvas User ( data = datum ) ) return users", "predictions": ["method that will get all the supported supported supported supported supported supported supported supported supported supported supported supported by this class ."], "references": ["returns a list of users for the given course id ."], "bleu": 0.0612957497932821, "rouge_l": 0.12896405919661735}
{"id": 3377, "code": "def get users for sis course id ( self , sis course id , params = { } ) : return self . get users for course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["retrieves all children of this , including the requested , meaning they have been loaded ."], "references": ["returns a list of users for the given sis course id ."], "bleu": 0.09147827112247602, "rouge_l": 0.21995192307692307}
{"id": 3378, "code": "def next page ( self , response ) : for link in response . getheader ( \"link\" , \"\" ) . split ( \",\" ) : try : ( url , rel ) = link . split ( \";\" ) if \"next\" in rel : return url . lstrip ( \"<\" ) . rstrip ( \">\" ) except Exception : return", "predictions": ["get the register for the specified facet ."], "references": ["return url path to next page of paginated data"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 3379, "code": "def get resource ( self , url , params = None , data key = None ) : if not params : params = { } self . set as user ( params ) full url = url + self . params ( params ) return self . get resource url ( full url , True , data key )", "predictions": ["fetch and return the ( ( ( { { { { { { { { wordpress , if present k k , . . . . k , if the { { 0 , if the { { { { 0 , 1 k ] k ] k = 0"], "references": ["canvas get method . return representation of the requested resource ."], "bleu": 0.030216776104535565, "rouge_l": 0.11117861482381533}
{"id": 3380, "code": "def put resource ( self , url , body ) : params = { } self . set as user ( params ) headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . put URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )", "predictions": ["store the results of the resource and apply to the s3 object"], "references": ["canvas put method ."], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 3381, "code": "def post resource ( self , url , body ) : params = { } self . set as user ( params ) headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . post URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )", "predictions": ["get an http get request on this url and return"], "references": ["canvas post method ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3382, "code": "def delete resource ( self , url ) : params = { } self . set as user ( params ) headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . delete URL ( url , headers ) if not ( response . status == 200 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return response", "predictions": ["deletes an existing to the specified text and returns the entity that was passed in ."], "references": ["canvas delete method ."], "bleu": 0.07692375026049747, "rouge_l": 0.11213235294117647}
{"id": 3383, "code": "def create admin by sis id ( self , sis account id , user id , role ) : return self . create admin ( self . sis id ( sis account id ) , user id , role )", "predictions": ["estimate how many things have been assigned to the = 0 ."], "references": ["flag an existing user as an admin within the account sis id ."], "bleu": 0.10579369505074822, "rouge_l": 0.15885416666666669}
{"id": 3384, "code": "def delete admin by sis id ( self , sis account id , user id , role ) : return self . delete admin ( self . sis id ( sis account id ) , user id , role )", "predictions": ["deletes an to the specified ."], "references": ["remove an account admin role from a user for the account sis id ."], "bleu": 0.06924459302580939, "rouge_l": 0.2798165137614679}
{"id": 3385, "code": "def get section by sis id ( self , sis section id , params = { } ) : return self . get section ( self . sis id ( sis section id , sis field = \"section\" ) , params )", "predictions": ["string to attach an to this to the to a to the to the to the to the to be inserted in the accessdescriptionstablemodel class ."], "references": ["return section resource for given sis id ."], "bleu": 0.04668049023095243, "rouge_l": 0.0650319829424307}
{"id": 3386, "code": "def get sections in course by sis id ( self , sis course id , params = { } ) : return self . get sections in course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["override this method to get the big big - based big - endian ."], "references": ["return list of sections for the passed course sis id ."], "bleu": 0.09782375748961449, "rouge_l": 0.16353887399463804}
{"id": 3387, "code": "def get sections with students in course ( self , course id , params = { } ) : include = params . get ( \"include\" , [ ] ) if \"students\" not in include : include . append ( \"students\" ) params [ \"include\" ] = include return self . get sections in course ( course id , params )", "predictions": ["extracts all is is is is is is is is is is is is is is is is is is is is executed ."], "references": ["return list of sections including students for the passed course id ."], "bleu": 0.050661968099322066, "rouge_l": 0.05910852713178295}
{"id": 3388, "code": "def get sections with students in course by sis id ( self , sis course id , params = { } ) : return self . get sections with students in course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["override this method to construct the is is analogous to the enough is cached ."], "references": ["return list of sections including students for the passed sis id ."], "bleu": 0.09103526405546068, "rouge_l": 0.15117719950433703}
{"id": 3389, "code": "def get term by sis id ( self , sis term id ) : for term in self . get all terms ( ) : if term . sis term id == sis term id : return term", "predictions": ["parse a category by id ."], "references": ["return a term resource for the passed sis id ."], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 3390, "code": "def build archive ( self , dir path ) : zip path = os . path . join ( dir path , \"import.zip\" ) archive = zipfile . Zip File ( zip path , \"w\" ) for filename in CSV FILES : filepath = os . path . join ( dir path , filename ) if os . path . exists ( filepath ) : archive . write ( filepath , filename , zipfile . ZIP DEFLATED ) archive . close ( ) with open ( zip path , \"rb\" ) as f : body = f . read ( ) return body", "predictions": ["parse an totals totals ."], "references": ["creates a zip archive from files in path ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 3391, "code": "def get report data ( self , report ) : if report . report id is None or report . status is None : raise Report Failure Exception ( report ) interval = getattr ( settings , 'CANVAS REPORT POLLING INTERVAL' , 5 ) while report . status != \"complete\" : if report . status == \"error\" : raise Report Failure Exception ( report ) sleep ( interval ) report = self . get report status ( report ) if report . attachment is None or report . attachment . url is None : return data = self . get report file ( report . attachment . url ) return data . split ( \"\\n\" )", "predictions": ["return reports reports that the items returned by the client ."], "references": ["returns a completed report as a list of csv strings ."], "bleu": 0.11390778025531027, "rouge_l": 0.09090909090909091}
{"id": 3392, "code": "def empty value ( self ) : edit empty value = self . config . get ( 'edit empty value' , False ) if edit empty value : return edit empty value else : return unicode ( inplace settings . INPLACEEDIT EDIT EMPTY VALUE )", "predictions": ["empty or edit the value of this plugin ."], "references": ["get the text to display when the field is empty ."], "bleu": 0.1343994460963362, "rouge_l": 0.19645732689210954}
{"id": 3393, "code": "def create metrics ( self , metric configs : Iterable [ Metric Config ] ) -> Dict [ str , Metric ] : return self . registry . create metrics ( metric configs )", "predictions": ["creates a list of metrics ."], "references": ["create and register metrics from a list of metricconfigs ."], "bleu": 0.233601780743345, "rouge_l": 0.47843137254901963}
{"id": 3394, "code": "def configure registry ( self , include process stats : bool = False ) : if include process stats : self . registry . register additional collector ( Process Collector ( registry = None ) )", "predictions": ["sets up the process ."], "references": ["configure the metricregistry ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 3395, "code": "def create metrics ( self , configs : Iterable [ Metric Config ] ) -> Dict [ str , Metric ] : metrics : Dict [ str , Metric ] = { config . name : self . register metric ( config ) for config in configs } self . metrics . update ( metrics ) return metrics", "predictions": ["create and return a batch of metrics ."], "references": ["create prometheus metrics from a list of metricconfigs ."], "bleu": 0.1862539773562041, "rouge_l": 0.465648854961832}
{"id": 3396, "code": "def get metric ( self , name : str , labels : Union [ Dict [ str , str ] , None ] = None ) -> Metric : metric = self . metrics [ name ] if labels : return metric . labels ( * * labels ) return metric", "predictions": ["get a list of metric strings for this instance ."], "references": ["return a metric optionally configured with labels ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 3397, "code": "async def handle home ( self , request : Request ) -> Response : if self . description : title = f'{self.name} - {self.description}' else : title = self . name text = dedent ( ) return Response ( content type = 'text/html' , text = text )", "predictions": ["handle html sent to the user ."], "references": ["home page request handler ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3398, "code": "async def handle metrics ( self , request : Request ) -> Response : if self . update handler : await self . update handler ( self . registry . get metrics ( ) ) response = Response ( body = self . registry . generate metrics ( ) ) response . content type = CONTENT TYPE LATEST return response", "predictions": ["eligible for metrics . save \" and \" ( \" ."], "references": ["handler for metrics ."], "bleu": 0.21200626759025185, "rouge_l": 0.43675417661097854}
{"id": 3399, "code": "def info ( self ) : return itertools . chain ( self . pods , self . assumptions , self . warnings )", "predictions": ["a chain of this class ."], "references": ["the pods assumptions and warnings of this result ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 3400, "code": "def results ( self ) : return ( pod for pod in self . pods if pod . primary or pod . title == 'Result' )", "predictions": ["get a primary key of this method ."], "references": ["the pods that hold the response to a simple discrete query ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3401, "code": "def vector ( members : Iterable [ T ] , meta : Optional [ I Persistent Map ] = None ) -> Vector [ T ] : return Vector ( pvector ( members ) , meta = meta )", "predictions": ["returns a vector of elements that are being used in the elements contained in the current vector ."], "references": ["creates a new vector ."], "bleu": 0.09629943614188137, "rouge_l": 0.29047619047619044}
{"id": 3402, "code": "def v ( * members : T , meta : Optional [ I Persistent Map ] = None ) -> Vector [ T ] : return Vector ( pvector ( members ) , meta = meta )", "predictions": ["copy the members of this meta into the given meta - array ."], "references": ["creates a new vector from members ."], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 3403, "code": "def eval file ( filename : str , ctx : compiler . Compiler Context , module : types . Module Type ) : last = None for form in reader . read file ( filename , resolver = runtime . resolve alias ) : last = compiler . compile and exec form ( form , ctx , module ) return last", "predictions": ["evaluate a python module in the provided filename ."], "references": ["evaluate a file with the given name into a python module ast node ."], "bleu": 0.18627932710752976, "rouge_l": 0.41838134430727025}
{"id": 3404, "code": "def eval stream ( stream , ctx : compiler . Compiler Context , module : types . Module Type ) : last = None for form in reader . read ( stream , resolver = runtime . resolve alias ) : last = compiler . compile and exec form ( form , ctx , module ) return last", "predictions": ["evaluate an sql stream for the given stream ."], "references": ["evaluate the forms in stdin into a python module ast node ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 3405, "code": "def eval str ( s : str , ctx : compiler . Compiler Context , module : types . Module Type , eof : Any ) : last = eof for form in reader . read str ( s , resolver = runtime . resolve alias , eof = eof ) : last = compiler . compile and exec form ( form , ctx , module ) return last", "predictions": ["evaluate a prompt for the given string ."], "references": ["evaluate the forms in a string into a python module ast node ."], "bleu": 0.11296874775996037, "rouge_l": 0.3652694610778443}
{"id": 3406, "code": "def run ( file or code , code , in ns , use var indirection , warn on shadowed name , warn on shadowed var , warn on var indirection , ) : basilisp . init ( ) ctx = compiler . Compiler Context ( filename = CLI INPUT FILE PATH if code else ( STDIN INPUT FILE PATH if file or code == STDIN FILE NAME else file or code ) , opts = { compiler . WARN ON SHADOWED NAME : warn on shadowed name , compiler . WARN ON SHADOWED VAR : warn on shadowed var , compiler . USE VAR INDIRECTION : use var indirection , compiler . WARN ON VAR INDIRECTION : warn on var indirection , } , ) eof = object ( ) with runtime . ns bindings ( in ns ) as ns : if code : print ( runtime . lrepr ( eval str ( file or code , ctx , ns . module , eof ) ) ) elif file or code == STDIN FILE NAME : print ( runtime . lrepr ( eval stream ( click . get text stream ( \"stdin\" ) , ctx , ns . module ) ) ) else : print ( runtime . lrepr ( eval file ( file or code , ctx , ns . module ) ) )", "predictions": ["runs the compiler script ."], "references": ["run a basilisp script or a line of code if it is provided ."], "bleu": 0.04994299940831281, "rouge_l": 0.19395866454689983}
{"id": 3407, "code": "def multifn ( dispatch : Dispatch Function , default = None ) -> Multi Function [ T ] : name = sym . symbol ( dispatch . qualname , ns = dispatch . module ) return Multi Function ( name , dispatch , default )", "predictions": ["multifn ( ) multifn ( module | module | name | name | name | default | module | module | module | default | name | name | name | name | name | default | default | name | default | default 1 . 1 . 0 ."], "references": ["decorator function which can be used to make python multi functions ."], "bleu": 0.02403051755364481, "rouge_l": 0.03626634958382877}
{"id": 3408, "code": "def add method ( m : lmap . Map , key : T , method : Method ) -> lmap . Map : return m . assoc ( key , method )", "predictions": ["add method to a method ."], "references": ["swap the methods atom to include method with key ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 3409, "code": "def remove method ( m : lmap . Map , key : T ) -> lmap . Map : return m . dissoc ( key )", "predictions": ["removes the given key from this map ."], "references": ["swap the methods atom to remove method with key ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 3410, "code": "def remove method ( self , key : T ) -> Optional [ Method ] : method = self . methods . entry ( key , None ) if method : self . methods . swap ( Multi Function . remove method , key ) return method", "predictions": ["remove a method from the request ."], "references": ["remove the method defined for this key and return it ."], "bleu": 0.1319006407505858, "rouge_l": 0.32049036777583184}
{"id": 3411, "code": "def is macro ( v : Var ) -> bool : return ( Maybe ( v . meta ) . map ( lambda m : m . entry ( SYM MACRO META KEY , None ) ) . or else get ( False ) )", "predictions": ["does this macro have a variable ?"], "references": ["return true if the var holds a macro function ."], "bleu": 0.13391424795650428, "rouge_l": 0.11401869158878504}
{"id": 3412, "code": "def clean meta ( meta : Optional [ lmap . Map ] ) -> Optional [ lmap . Map ] : if meta is None : return None else : new meta = meta . dissoc ( reader . READER LINE KW , reader . READER COL KW ) return None if len ( new meta ) == 0 else new meta", "predictions": ["removes all . from the ( . this is just a . function that returns a . with the ( ."], "references": ["remove reader metadata from the form s meta map ."], "bleu": 0.0821610732492254, "rouge_l": 0.20677966101694914}
{"id": 3413, "code": "def deftype impls ( ctx : Parser Context , form : I Seq ) -> Tuple [ List [ Def Type Base ] , List [ Method ] ] : current interface sym : Optional [ sym . Symbol ] = None current interface : Optional [ Def Type Base ] = None interfaces = [ ] methods : List [ Method ] = [ ] interface methods : Mutable Mapping [ sym . Symbol , List [ Method ] ] = { } for elem in form : if isinstance ( elem , sym . Symbol ) : if current interface is not None : if current interface sym in interface methods : raise Parser Exception ( f\"deftype* forms may only implement an interface once\" , form = elem , ) assert ( current interface sym is not None ) , \"Symbol must be defined with interface\" interface methods [ current interface sym ] = methods current interface sym = elem current interface = parse ast ( ctx , elem ) methods = [ ] if not isinstance ( current interface , ( Maybe Class , Maybe Host Form , Var Ref ) ) : raise Parser Exception ( f\"deftype* interface implementation must be an existing interface\" , form = elem , ) interfaces . append ( current interface ) elif isinstance ( elem , I Seq ) : if current interface is None : raise Parser Exception ( f\"deftype* method cannot be declared without interface\" , form = elem ) methods . append ( deftype method ( ctx , elem , current interface ) ) else : raise Parser Exception ( f\"deftype* must consist of interface or protocol names and methods\" , form = elem , ) if current interface is not None : if len ( methods ) > 0 : if current interface sym in interface methods : raise Parser Exception ( f\"deftype* forms may only implement an interface once\" , form = current interface sym , ) assert ( current interface sym is not None ) , \"Symbol must be defined with interface\" interface methods [ current interface sym ] = methods else : raise Parser Exception ( f\"deftype* may not declare interface without at least one method\" , form = current interface sym , ) return interfaces , list ( chain . from iterable ( interface methods . values ( ) ) )", "predictions": ["creates a singleton instance of an object ."], "references": ["roll up deftype * declared bases and method implementations ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 3414, "code": "def resolve sym ( ctx : Parser Context , form : sym . Symbol ) -> Union [ Maybe Class , Maybe Host Form , Var Ref ] : if form . ns is None and form . name . endswith ( \".\" ) : try : ns , name = form . name [ : - 1 ] . rsplit ( \".\" , maxsplit = 1 ) form = sym . symbol ( name , ns = ns ) except Value Error : form = sym . symbol ( form . name [ : - 1 ] ) if form . ns is not None : return resolve namespaced symbol ( ctx , form ) else : return resolve bare symbol ( ctx , form )", "predictions": ["resolve a \".\" by name and return it ."], "references": ["resolve a basilisp symbol as a var or python name ."], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 3415, "code": "def bootstrap module ( gctx : Generator Context , optimizer : Python AST Optimizer , mod : types . Module Type , collect bytecode : Optional [ Bytecode Collector ] = None , ) -> None : incremental compile module ( optimizer , py module preamble ( gctx ) , mod , source filename = gctx . filename , collect bytecode = collect bytecode , ) mod . basilisp bootstrapped = True", "predictions": ["populates a module in a single module ."], "references": ["bootstrap a new module with imports and other boilerplate ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 3416, "code": "def sequence ( s : Iterable ) -> I Seq [ Any ] : try : i = iter ( s ) return Sequence ( i , next ( i ) ) except Stop Iteration : return EMPTY", "predictions": ["returns an iterator over all rows of this charseq"], "references": ["create a sequence from iterable s ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3417, "code": "def fraction ( numerator : int , denominator : int ) -> Fraction : return Fraction ( numerator = numerator , denominator = denominator )", "predictions": ["compute the fraction of all items in numerator ."], "references": ["create a fraction from a numerator and denominator ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 3418, "code": "def get handler ( level : str , fmt : str ) -> logging . Handler : handler : logging . Handler = logging . Null Handler ( ) if os . getenv ( \"BASILISP USE DEV LOGGER\" ) == \"true\" : handler = logging . Stream Handler ( ) handler . set Formatter ( logging . Formatter ( fmt ) ) handler . set Level ( level ) return handler", "predictions": ["get handler with level handler ."], "references": ["get the default logging handler for basilisp ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 3419, "code": "def map ( kvs : Mapping [ K , V ] , meta = None ) -> Map [ K , V ] : return Map ( pmap ( initial = kvs ) , meta = meta )", "predictions": ["map ' -> ' should be a map of attributes ."], "references": ["creates a new map ."], "bleu": 0.1354599427337814, "rouge_l": 0.40219780219780216}
{"id": 3420, "code": "def partition ( coll , n : int ) : assert n > 0 start = 0 stop = n while stop <= len ( coll ) : yield tuple ( e for e in coll [ start : stop ] ) start += n stop += n if start < len ( coll ) < stop : stop = len ( coll ) yield tuple ( e for e in coll [ start : stop ] )", "predictions": ["utility method to partition an iterable of doubles ."], "references": ["partition coll into groups of size n ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 3421, "code": "def read namespaced ( ctx : Reader Context , allowed suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : ns : List [ str ] = [ ] name : List [ str ] = [ ] reader = ctx . reader has ns = False while True : token = reader . peek ( ) if token == \"/\" : reader . next token ( ) if has ns : raise Syntax Error ( \"Found '/'; expected word character\" ) elif len ( name ) == 0 : name . append ( \"/\" ) else : if \"/\" in name : raise Syntax Error ( \"Found '/' after '/'\" ) has ns = True ns = name name = [ ] elif ns name chars . match ( token ) : reader . next token ( ) name . append ( token ) elif allowed suffix is not None and token == allowed suffix : reader . next token ( ) name . append ( token ) else : break ns str = None if not has ns else \"\" . join ( ns ) name str = \"\" . join ( name ) if ns str is None : if \"/\" in name str and name str != \"/\" : raise Syntax Error ( \"'/' character disallowed in names\" ) assert ns str is None or len ( ns str ) > 0 return ns str , name str", "predictions": ["read a matching expression from an inputstream ."], "references": ["read a namespaced token from the input stream ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 3422, "code": "def read list ( ctx : Reader Context ) -> llist . List : start = ctx . reader . advance ( ) assert start == \"(\" return read coll ( ctx , llist . list , \")\" , \"list\" )", "predictions": ["read a list of strings from the reader ."], "references": ["read a list element from the input stream ."], "bleu": 0.32466791547509893, "rouge_l": 0.6666666666666666}
{"id": 3423, "code": "def read vector ( ctx : Reader Context ) -> vector . Vector : start = ctx . reader . advance ( ) assert start == \"[\" return read coll ( ctx , vector . vector , \"]\" , \"vector\" )", "predictions": ["reads a single vector of the reader"], "references": ["read a vector element from the input stream ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 3424, "code": "def read set ( ctx : Reader Context ) -> lset . Set : start = ctx . reader . advance ( ) assert start == \"{\" def set if valid ( s : Collection ) -> lset . Set : if len ( s ) != len ( set ( s ) ) : raise Syntax Error ( \"Duplicated values in set\" ) return lset . set ( s ) return read coll ( ctx , set if valid , \"}\" , \"set\" )", "predictions": ["empty a reader from the ( ."], "references": ["return a set from the input stream ."], "bleu": 0.240785655451027, "rouge_l": 0.5269978401727862}
{"id": 3425, "code": "def read map ( ctx : Reader Context ) -> lmap . Map : reader = ctx . reader start = reader . advance ( ) assert start == \"{\" d : Mutable Mapping [ Any , Any ] = { } while True : if reader . peek ( ) == \"}\" : reader . next token ( ) break k = read next ( ctx ) if k is COMMENT : continue while True : if reader . peek ( ) == \"}\" : raise Syntax Error ( \"Unexpected token '}'; expected map value\" ) v = read next ( ctx ) if v is COMMENT : continue if k in d : raise Syntax Error ( f\"Duplicate key '{k}' in map literal\" ) break d [ k ] = v return lmap . map ( d )", "predictions": ["reads all data from a reader ."], "references": ["return a map from the input stream ."], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 3426, "code": "def read kw ( ctx : Reader Context ) -> keyword . Keyword : start = ctx . reader . advance ( ) assert start == \":\" ns , name = read namespaced ( ctx ) if \".\" in name : raise Syntax Error ( \"Found '.' in keyword name\" ) return keyword . keyword ( name , ns = ns )", "predictions": ["configure a single css bool from the reader ."], "references": ["return a keyword from the input stream ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 3427, "code": "def read function ( ctx : Reader Context ) -> llist . List : if ctx . is in anon fn : raise Syntax Error ( f\"Nested #() definitions not allowed\" ) with ctx . in anon fn ( ) : form = read list ( ctx ) arg set = set ( ) def arg suffix ( arg num ) : if arg num is None : return \"1\" elif arg num == \"&\" : return \"rest\" else : return arg num def sym replacement ( arg num ) : suffix = arg suffix ( arg num ) return symbol . symbol ( f\"arg-{suffix}\" ) def identify and replace ( f ) : if isinstance ( f , symbol . Symbol ) : if f . ns is None : match = fn macro args . match ( f . name ) if match is not None : arg num = match . group ( 2 ) suffix = arg suffix ( arg num ) arg set . add ( suffix ) return sym replacement ( arg num ) return f body = walk . postwalk ( identify and replace , form ) if len ( form ) > 0 else None arg list : List [ symbol . Symbol ] = [ ] numbered args = sorted ( map ( int , filter ( lambda k : k != \"rest\" , arg set ) ) ) if len ( numbered args ) > 0 : max arg = max ( numbered args ) arg list = [ sym replacement ( str ( i ) ) for i in range ( 1 , max arg + 1 ) ] if \"rest\" in arg set : arg list . append ( AMPERSAND ) arg list . append ( sym replacement ( \"rest\" ) ) return llist . l ( FN , vector . vector ( arg list ) , body )", "predictions": ["create a metrics from an ( file ."], "references": ["read a function reader macro from the input stream ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 3428, "code": "def read quoted ( ctx : Reader Context ) -> llist . List : start = ctx . reader . advance ( ) assert start == \"'\" next form = read next consuming comment ( ctx ) return llist . l ( QUOTE , next form )", "predictions": ["reads a metric from the reader ."], "references": ["read a quoted form from the input stream ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 3429, "code": "def read syntax quoted ( ctx : Reader Context ) -> Reader Form : start = ctx . reader . advance ( ) assert start == \"`\" with ctx . syntax quoted ( ) : return process syntax quoted form ( ctx , read next consuming comment ( ctx ) )", "predictions": ["def ( , get a handle to the handle of the handle ."], "references": ["read a syntax - quote and set the syntax - quoting state in the reader ."], "bleu": 0.09535849051927603, "rouge_l": 0.2708102108768035}
{"id": 3430, "code": "def read deref ( ctx : Reader Context ) -> Lisp Form : start = ctx . reader . advance ( ) assert start == \"@\" next form = read next consuming comment ( ctx ) return llist . l ( DEREF , next form )", "predictions": ["def def , self , , , , , , ( , ( , and ,"], "references": ["read a derefed form from the input stream ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 3431, "code": "def read regex ( ctx : Reader Context ) -> Pattern : s = read str ( ctx , allow arbitrary escapes = True ) try : return langutil . regex from str ( s ) except re . error : raise Syntax Error ( f\"Unrecognized regex pattern syntax: {s}\" )", "predictions": ["reads in the regular expression matching the given regular expression ."], "references": ["read a regex reader macro from the input stream ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 3432, "code": "def read next ( ctx : Reader Context ) -> Lisp Reader Form : reader = ctx . reader token = reader . peek ( ) if token == \"(\" : return read list ( ctx ) elif token == \"[\" : return read vector ( ctx ) elif token == \"{\" : return read map ( ctx ) elif begin num chars . match ( token ) : return read num ( ctx ) elif whitespace chars . match ( token ) : reader . next token ( ) return read next ( ctx ) elif token == \":\" : return read kw ( ctx ) elif token == '\"' : return read str ( ctx ) elif token == \"'\" : return read quoted ( ctx ) elif token == \"\\\\\" : return read character ( ctx ) elif ns name chars . match ( token ) : return read sym ( ctx ) elif token == \"#\" : return read reader macro ( ctx ) elif token == \"^\" : return read meta ( ctx ) elif token == \";\" : return read comment ( ctx ) elif token == \"`\" : return read syntax quoted ( ctx ) elif token == \"~\" : return read unquote ( ctx ) elif token == \"@\" : return read deref ( ctx ) elif token == \"\" : return ctx . eof else : raise Syntax Error ( \"Unexpected token '{token}'\" . format ( token = token ) )", "predictions": ["reads the ( next macro for any macro for the next macro for the next macro for the next ( for the next ( macro for the next ( for the next ( self ."], "references": ["read the next full form from the input stream ."], "bleu": 0.05157142709886006, "rouge_l": 0.19757085020242915}
{"id": 3433, "code": "def basilisp bytecode ( mtime : int , source size : int , code : List [ types . Code Type ] ) -> bytes : data = bytearray ( MAGIC NUMBER ) data . extend ( w long ( mtime ) ) data . extend ( w long ( source size ) ) data . extend ( marshal . dumps ( code ) ) return data", "predictions": ["generate a vector of pvector code corresponding to the given ] ."], "references": ["return the bytes for a basilisp bytecode cache file ."], "bleu": 0.1235622127262679, "rouge_l": 0.18484848484848485}
{"id": 3434, "code": "def exec cached module ( self , fullname : str , loader state : Mapping [ str , str ] , path stats : Mapping [ str , int ] , module : types . Module Type , ) : filename = loader state [ \"filename\" ] cache filename = loader state [ \"cache filename\" ] with timed ( lambda duration : logger . debug ( f\"Loaded cached Basilisp module '{fullname}' in {duration / 1000000}ms\" ) ) : logger . debug ( f\"Checking for cached Basilisp module '{fullname}''\" ) cache data = self . get data ( cache filename ) cached code = get basilisp bytecode ( fullname , path stats [ \"mtime\" ] , path stats [ \"size\" ] , cache data ) compiler . compile bytecode ( cached code , compiler . Generator Context ( filename = filename ) , compiler . Python AST Optimizer ( ) , module , )", "predictions": ["execute a ( possibly non - dummy dummy dummy ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) . ) members ."], "references": ["load and execute a cached basilisp module ."], "bleu": 0.033984283835209204, "rouge_l": 0.11898569570871263}
{"id": 3435, "code": "def exec module ( self , fullname : str , loader state : Mapping [ str , str ] , path stats : Mapping [ str , int ] , module : types . Module Type , ) : filename = loader state [ \"filename\" ] cache filename = loader state [ \"cache filename\" ] with timed ( lambda duration : logger . debug ( f\"Loaded Basilisp module '{fullname}' in {duration / 1000000}ms\" ) ) : all bytecode = [ ] def add bytecode ( bytecode : types . Code Type ) : all bytecode . append ( bytecode ) logger . debug ( f\"Reading and compiling Basilisp module '{fullname}'\" ) forms = reader . read file ( filename , resolver = runtime . resolve alias ) compiler . compile module ( forms , compiler . Compiler Context ( filename = filename ) , module , collect bytecode = add bytecode , ) cache file bytes = basilisp bytecode ( path stats [ \"mtime\" ] , path stats [ \"size\" ] , all bytecode ) self . cache bytecode ( filename , cache filename , cache file bytes )", "predictions": ["execute a processor for the given file ."], "references": ["load and execute a non - cached basilisp module ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 3436, "code": "def symbol ( name : str , ns : Optional [ str ] = None , meta = None ) -> Symbol : return Symbol ( name , ns = ns , meta = meta )", "predictions": ["create a eval eval from a ( ( string name ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["create a new symbol ."], "bleu": 0.03162593967015063, "rouge_l": 0.08531468531468532}
{"id": 3437, "code": "def complete ( text : str , kw cache : atom . Atom [ \"P Map[int, Keyword]\" ] = INTERN ) -> Iterable [ str ] : assert text . startswith ( \":\" ) interns = kw cache . deref ( ) text = text [ 1 : ] if \"/\" in text : prefix , suffix = text . split ( \"/\" , maxsplit = 1 ) results = filter ( lambda kw : ( kw . ns is not None and kw . ns == prefix ) and kw . name . startswith ( suffix ) , interns . itervalues ( ) , ) else : results = filter ( lambda kw : kw . name . startswith ( text ) or ( kw . ns is not None and kw . ns . startswith ( text ) ) , interns . itervalues ( ) , ) return map ( str , results )", "predictions": ["eval [ filename ] ."], "references": ["return an iterable of possible completions for the given text ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3438, "code": "def keyword ( name : str , ns : Optional [ str ] = None , kw cache : atom . Atom [ \"P Map[int, Keyword]\" ] = INTERN , ) -> Keyword : h = hash ( ( name , ns ) ) return kw cache . swap ( get or create , h , name , ns ) [ h ]", "predictions": ["if a run , if necessary , if any , create a run with a file with the given file ."], "references": ["create a new keyword ."], "bleu": 0.0821610732492254, "rouge_l": 0.25957446808510637}
{"id": 3439, "code": "def chain py ast ( * genned : Generated Py AST , ) -> Tuple [ Py AST Stream , Py AST Stream ] : deps = chain . from iterable ( map ( lambda n : n . dependencies , genned ) ) nodes = map ( lambda n : n . node , genned ) return deps , nodes", "predictions": ["given a dispatch queue , this method works as the first dispatch of the dispatch ."], "references": ["chain a sequence of generated python asts into a tuple of dependency nodes"], "bleu": 0.08513012360883544, "rouge_l": 0.14055299539170507}
{"id": 3440, "code": "def simple ast generator ( gen ast ) : @ wraps ( gen ast ) def wrapped ast generator ( ctx : Generator Context , form : Lisp Form ) -> Generated Py AST : return Generated Py AST ( node = gen ast ( ctx , form ) ) return wrapped ast generator", "predictions": ["generate method to support a method on a method call ."], "references": ["wrap simpler ast generators to return a generatedpyast ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 3441, "code": "def collection ast ( ctx : Generator Context , form : Iterable [ Node ] ) -> Tuple [ Py AST Stream , Py AST Stream ] : return chain py ast ( * map ( partial ( gen py ast , ctx ) , form ) )", "predictions": ["creates a remove method of the given remove node ."], "references": ["turn a collection of lisp forms into python ast nodes ."], "bleu": 0.13564514503163538, "rouge_l": 0.28328173374613}
{"id": 3442, "code": "def clean meta ( form : I Meta ) -> Lisp Form : assert form . meta is not None , \"Form must have non-null 'meta' attribute\" meta = form . meta . dissoc ( reader . READER LINE KW , reader . READER COL KW ) if len ( meta ) == 0 : return None return cast ( lmap . Map , meta )", "predictions": ["removes all ( from the ( and deletes them ."], "references": ["remove reader metadata from the form s meta map ."], "bleu": 0.17827531042796255, "rouge_l": 0.3}
{"id": 3443, "code": "def is redefable ( v : Var ) -> bool : return ( Maybe ( v . meta ) . map ( lambda m : m . get ( SYM REDEF META KEY , None ) ) . or else get ( False ) )", "predictions": ["does the previous operation on the shared variable ."], "references": ["return true if the var can be redefined ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3444, "code": "def should warn on redef ( ctx : Generator Context , defsym : sym . Symbol , safe name : str , def meta : lmap . Map ) -> bool : no warn on redef = def meta . entry ( SYM NO WARN ON REDEF META KEY , False ) if no warn on redef : return False elif safe name in ctx . current ns . module . dict : return True elif defsym in ctx . current ns . interns : var = ctx . current ns . find ( defsym ) assert var is not None , f\"Var {defsym} cannot be none here\" if var . meta is not None and var . meta . entry ( SYM REDEF META KEY ) : return False elif var . is bound : return True else : return False else : return False", "predictions": ["return whether we should meta this method should be called ( after the call to meta the module context ."], "references": ["return true if the compiler should emit a warning about this name being redefined ."], "bleu": 0.08039313477786734, "rouge_l": 0.23461538461538461}
{"id": 3445, "code": "def deftype to py ast ( ctx : Generator Context , node : Def Type ) -> Generated Py AST : assert node . op == Node Op . DEFTYPE type name = munge ( node . name ) ctx . symbol table . new symbol ( sym . symbol ( node . name ) , type name , Local Type . DEFTYPE ) bases = [ ] for base in node . interfaces : base node = gen py ast ( ctx , base ) assert ( count ( base node . dependencies ) == 0 ) , \"Class and host form nodes do not have dependencies\" bases . append ( base node . node ) decorator = ast . Call ( func = ATTR CLASS DECORATOR NAME , args = [ ] , keywords = [ ast . keyword ( arg = \"cmp\" , value = ast . Name Constant ( False ) ) , ast . keyword ( arg = \"frozen\" , value = ast . Name Constant ( node . is frozen ) ) , ast . keyword ( arg = \"slots\" , value = ast . Name Constant ( True ) ) , ] , ) with ctx . new symbol table ( node . name ) : type nodes = [ ] for field in node . fields : safe field = munge ( field . name ) type nodes . append ( ast . Assign ( targets = [ ast . Name ( id = safe field , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = ATTRIB FIELD FN NAME , args = [ ] , keywords = [ ] ) , ) ) ctx . symbol table . new symbol ( sym . symbol ( field . name ) , safe field , field . local ) type deps : List [ ast . AST ] = [ ] for method in node . methods : type ast = deftype method to py ast ( ctx , method ) type nodes . append ( type ast . node ) type deps . extend ( type ast . dependencies ) return Generated Py AST ( node = ast . Name ( id = type name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( type deps , [ ast . Class Def ( name = type name , bases = bases , keywords = [ ] , body = type nodes , decorator list = [ decorator ] , ) ] , ) ) , )", "predictions": ["convert a set of deftype to a lambda ."], "references": ["return a python ast node for a deftype * expression ."], "bleu": 0.14211011212459496, "rouge_l": 0.2946859903381642}
{"id": 3446, "code": "def do to py ast ( ctx : Generator Context , node : Do ) -> Generated Py AST : assert node . op == Node Op . DO assert not node . is body body ast = Generated Py AST . reduce ( * map ( partial ( gen py ast , ctx ) , chain ( node . statements , [ node . ret ] ) ) ) fn body ast : List [ ast . AST ] = [ ] do result name = genname ( DO PREFIX ) fn body ast . extend ( map ( statementize , body ast . dependencies ) ) fn body ast . append ( ast . Assign ( targets = [ ast . Name ( id = do result name , ctx = ast . Store ( ) ) ] , value = body ast . node ) ) return Generated Py AST ( node = ast . Name ( id = do result name , ctx = ast . Load ( ) ) , dependencies = fn body ast )", "predictions": ["convert the ctx to a python ctx ."], "references": ["return a python ast node for a do expression ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 3447, "code": "def fn args to py ast ( ctx : Generator Context , params : Iterable [ Binding ] , body : Do ) -> Tuple [ List [ ast . arg ] , Optional [ ast . arg ] , List [ ast . AST ] ] : fn args , varg = [ ] , None fn body ast : List [ ast . AST ] = [ ] for binding in params : assert binding . init is None , \":fn nodes cannot have bindint :inits\" assert varg is None , \"Must have at most one variadic arg\" arg name = genname ( munge ( binding . name ) ) if not binding . is variadic : fn args . append ( ast . arg ( arg = arg name , annotation = None ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , arg name , Local Type . ARG ) else : varg = ast . arg ( arg = arg name , annotation = None ) safe local = genname ( munge ( binding . name ) ) fn body ast . append ( ast . Assign ( targets = [ ast . Name ( id = safe local , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = COLLECT ARGS FN NAME , args = [ ast . Name ( id = arg name , ctx = ast . Load ( ) ) ] , keywords = [ ] , ) , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , safe local , Local Type . ARG ) body ast = synthetic do to py ast ( ctx , body ) fn body ast . extend ( map ( statementize , body ast . dependencies ) ) fn body ast . append ( ast . Return ( value = body ast . node ) ) return fn args , varg , fn body ast", "predictions": ["returns a set of all the module and : - : - : - : - > - : - - > - > - - > - - > - - - > - - - - - - - > - - - - - - - >"], "references": ["generate a list of python ast nodes from function method parameters ."], "bleu": 0.026594139297659906, "rouge_l": 0.07253269916765755}
{"id": 3448, "code": "def single arity fn to py ast ( ctx : Generator Context , node : Fn , method : Fn Method , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN assert method . op == Node Op . FN METHOD lisp fn name = node . local . name if node . local is not None else None py fn name = fn name ( lisp fn name ) if def name is None else munge ( def name ) py fn node = ast . Async Function Def if node . is async else ast . Function Def with ctx . new symbol table ( py fn name ) , ctx . new recur point ( method . loop id , Recur Type . FN , is variadic = node . is variadic ) : if lisp fn name is not None : ctx . symbol table . new symbol ( sym . symbol ( lisp fn name ) , py fn name , Local Type . FN ) fn args , varg , fn body ast = fn args to py ast ( ctx , method . params , method . body ) meta deps , meta decorators = fn meta ( ctx , meta node ) return Generated Py AST ( node = ast . Name ( id = py fn name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( meta deps , [ py fn node ( name = py fn name , args = ast . arguments ( args = fn args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = fn body ast , decorator list = list ( chain ( meta decorators , [ BASILISP FN FN NAME ] , [ TRAMPOLINE FN NAME ] if ctx . recur point . has recur else [ ] , ) ) , returns = None , ) ] , ) ) , )", "predictions": ["creates a sequence of all the objects of a sequence of functions ."], "references": ["return a python ast node for a function with a single arity ."], "bleu": 0.1135935489027116, "rouge_l": 0.23076923076923084}
{"id": 3449, "code": "def multi arity fn to py ast ( ctx : Generator Context , node : Fn , methods : Collection [ Fn Method ] , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN assert all ( [ method . op == Node Op . FN METHOD for method in methods ] ) lisp fn name = node . local . name if node . local is not None else None py fn name = fn name ( lisp fn name ) if def name is None else munge ( def name ) py fn node = ast . Async Function Def if node . is async else ast . Function Def arity to name = { } rest arity name : Optional [ str ] = None fn defs = [ ] for method in methods : arity name = f\"{py fn name} arity{' rest' if method.is variadic else method.fixed arity}\" if method . is variadic : rest arity name = arity name else : arity to name [ method . fixed arity ] = arity name with ctx . new symbol table ( arity name ) , ctx . new recur point ( method . loop id , Recur Type . FN , is variadic = node . is variadic ) : if lisp fn name is not None : ctx . symbol table . new symbol ( sym . symbol ( lisp fn name ) , py fn name , Local Type . FN ) fn args , varg , fn body ast = fn args to py ast ( ctx , method . params , method . body ) fn defs . append ( py fn node ( name = arity name , args = ast . arguments ( args = fn args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = fn body ast , decorator list = [ TRAMPOLINE FN NAME ] if ctx . recur point . has recur else [ ] , returns = None , ) ) dispatch fn ast = multi arity dispatch fn ( ctx , py fn name , arity to name , default name = rest arity name , max fixed arity = node . max fixed arity , meta node = meta node , is async = node . is async , ) return Generated Py AST ( node = dispatch fn ast . node , dependencies = list ( chain ( fn defs , dispatch fn ast . dependencies ) ) , )", "predictions": ["creates a function that can be used as a ( for all the methods ."], "references": ["return a python ast node for a function with multiple arities ."], "bleu": 0.12874330508144843, "rouge_l": 0.22676579925650556}
{"id": 3450, "code": "def fn to py ast ( ctx : Generator Context , node : Fn , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN if len ( node . methods ) == 1 : return single arity fn to py ast ( ctx , node , next ( iter ( node . methods ) ) , def name = def name , meta node = meta node ) else : return multi arity fn to py ast ( ctx , node , node . methods , def name = def name , meta node = meta node )", "predictions": ["convert a function to a tree ."], "references": ["return a python ast node for a fn expression ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 3451, "code": "def import to py ast ( ctx : Generator Context , node : Import ) -> Generated Py AST : assert node . op == Node Op . IMPORT last = None deps : List [ ast . AST ] = [ ] for alias in node . aliases : safe name = munge ( alias . name ) try : module = importlib . import module ( safe name ) if alias . alias is not None : ctx . add import ( sym . symbol ( alias . name ) , module , sym . symbol ( alias . alias ) ) else : ctx . add import ( sym . symbol ( alias . name ) , module ) except Module Not Found Error as e : raise Import Error ( f\"Python module '{alias.name}' not found\" , node . form , node ) from e py import alias = ( munge ( alias . alias ) if alias . alias is not None else safe name . split ( \".\" , maxsplit = 1 ) [ 0 ] ) deps . append ( ast . Assign ( targets = [ ast . Name ( id = py import alias , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = load attr ( \"builtins. import \" ) , args = [ ast . Str ( safe name ) ] , keywords = [ ] , ) , ) ) last = ast . Name ( id = py import alias , ctx = ast . Load ( ) ) deps . append ( ast . Call ( func = load attr ( f\"{ NS VAR VALUE}.add import\" ) , args = [ ast . Call ( func = NEW SYM FN NAME , args = [ ast . Str ( safe name ) ] , keywords = [ ] ) , last , ] , keywords = [ ] , ) ) assert last is not None , \"import* node must have at least one import\" return Generated Py AST ( node = last , dependencies = deps )", "predictions": ["imports the nodes from the local store ."], "references": ["return a python ast node for a basilisp import * expression ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 3452, "code": "def invoke to py ast ( ctx : Generator Context , node : Invoke ) -> Generated Py AST : assert node . op == Node Op . INVOKE fn ast = gen py ast ( ctx , node . fn ) args deps , args nodes = collection ast ( ctx , node . args ) return Generated Py AST ( node = ast . Call ( func = fn ast . node , args = list ( args nodes ) , keywords = [ ] ) , dependencies = list ( chain ( fn ast . dependencies , args deps ) ) , )", "predictions": ["calls the given , returning an iterable of dependencies ."], "references": ["return a python ast node for a basilisp function invocation ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 3453, "code": "def let to py ast ( ctx : Generator Context , node : Let ) -> Generated Py AST : assert node . op == Node Op . LET with ctx . new symbol table ( \"let\" ) : let body ast : List [ ast . AST ] = [ ] for binding in node . bindings : init node = binding . init assert init node is not None init ast = gen py ast ( ctx , init node ) binding name = genname ( munge ( binding . name ) ) let body ast . extend ( init ast . dependencies ) let body ast . append ( ast . Assign ( targets = [ ast . Name ( id = binding name , ctx = ast . Store ( ) ) ] , value = init ast . node , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , binding name , Local Type . LET ) let result name = genname ( \"let result\" ) body ast = synthetic do to py ast ( ctx , node . body ) let body ast . extend ( map ( statementize , body ast . dependencies ) ) let body ast . append ( ast . Assign ( targets = [ ast . Name ( id = let result name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ) return Generated Py AST ( node = ast . Name ( id = let result name , ctx = ast . Load ( ) ) , dependencies = let body ast )", "predictions": ["generate a ctx to be used as an ( ."], "references": ["return a python ast node for a let * expression ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 3454, "code": "def loop to py ast ( ctx : Generator Context , node : Loop ) -> Generated Py AST : assert node . op == Node Op . LOOP with ctx . new symbol table ( \"loop\" ) : binding names = [ ] init bindings : List [ ast . AST ] = [ ] for binding in node . bindings : init node = binding . init assert init node is not None init ast = gen py ast ( ctx , init node ) init bindings . extend ( init ast . dependencies ) binding name = genname ( munge ( binding . name ) ) binding names . append ( binding name ) init bindings . append ( ast . Assign ( targets = [ ast . Name ( id = binding name , ctx = ast . Store ( ) ) ] , value = init ast . node , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , binding name , Local Type . LOOP ) loop result name = genname ( \"loop\" ) with ctx . new recur point ( node . loop id , Recur Type . LOOP , binding names = binding names ) : loop body ast : List [ ast . AST ] = [ ] body ast = synthetic do to py ast ( ctx , node . body ) loop body ast . extend ( body ast . dependencies ) loop body ast . append ( ast . Assign ( targets = [ ast . Name ( id = loop result name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ) loop body ast . append ( ast . Break ( ) ) return Generated Py AST ( node = load attr ( loop result name ) , dependencies = list ( chain ( [ ast . Assign ( targets = [ ast . Name ( id = loop result name , ctx = ast . Store ( ) ) ] , value = ast . Name Constant ( None ) , ) ] , init bindings , [ ast . While ( test = ast . Name Constant ( True ) , body = loop body ast , orelse = [ ] , ) ] , ) ) , )", "predictions": ["generate a read read & run a read from an xml file"], "references": ["return a python ast node for a loop * expression ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 3455, "code": "def quote to py ast ( ctx : Generator Context , node : Quote ) -> Generated Py AST : assert node . op == Node Op . QUOTE return const node to py ast ( ctx , node . expr )", "predictions": ["creates a ctx tree from the ctx to the ( ."], "references": ["return a python ast node for a quote expression ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 3456, "code": "def fn recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR assert ctx . recur point . is variadic is not None recur nodes : List [ ast . AST ] = [ ] recur deps : List [ ast . AST ] = [ ] for expr in node . exprs : expr ast = gen py ast ( ctx , expr ) recur nodes . append ( expr ast . node ) recur deps . extend ( expr ast . dependencies ) return Generated Py AST ( node = ast . Call ( func = TRAMPOLINE ARGS FN NAME , args = list ( chain ( [ ast . Name Constant ( ctx . recur point . is variadic ) ] , recur nodes ) ) , keywords = [ ] , ) , dependencies = recur deps , )", "predictions": ["gets the set of dependencies as the ast node and all of its dependencies ."], "references": ["return a python ast node for recur occurring inside a fn * ."], "bleu": 0.11633270842295028, "rouge_l": 0.21708185053380782}
{"id": 3457, "code": "def deftype method recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR recur nodes : List [ ast . AST ] = [ ] recur deps : List [ ast . AST ] = [ ] for expr in node . exprs : expr ast = gen py ast ( ctx , expr ) recur nodes . append ( expr ast . node ) recur deps . extend ( expr ast . dependencies ) this entry = ctx . symbol table . find symbol ( ctx . current this ) assert this entry is not None , \"Field type local must have this\" return Generated Py AST ( node = ast . Call ( func = TRAMPOLINE ARGS FN NAME , args = list ( chain ( [ ast . Name Constant ( ctx . recur point . is variadic ) , ast . Name ( id = this entry . munged , ctx = ast . Load ( ) ) , ] , recur nodes , ) ) , keywords = [ ] , ) , dependencies = recur deps , )", "predictions": ["gets the osrget[angular|linear]units object as a dictionary ."], "references": ["return a python ast node for recur occurring inside a deftype * method ."], "bleu": 0.08383280652235028, "rouge_l": 0.1732954545454545}
{"id": 3458, "code": "def loop recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR recur deps : List [ ast . AST ] = [ ] recur targets : List [ ast . Name ] = [ ] recur exprs : List [ ast . AST ] = [ ] for name , expr in zip ( ctx . recur point . binding names , node . exprs ) : expr ast = gen py ast ( ctx , expr ) recur deps . extend ( expr ast . dependencies ) recur targets . append ( ast . Name ( id = name , ctx = ast . Store ( ) ) ) recur exprs . append ( expr ast . node ) if len ( recur targets ) == 1 : assert len ( recur exprs ) == 1 recur deps . append ( ast . Assign ( targets = recur targets , value = recur exprs [ 0 ] ) ) else : recur deps . append ( ast . Assign ( targets = [ ast . Tuple ( elts = recur targets , ctx = ast . Store ( ) ) ] , value = ast . Tuple ( elts = recur exprs , ctx = ast . Load ( ) ) , ) ) recur deps . append ( ast . Continue ( ) ) return Generated Py AST ( node = ast . Name Constant ( None ) , dependencies = recur deps )", "predictions": ["extracts all the dependencies of this loop node and all its dependencies ."], "references": ["return a python ast node for recur occurring inside a loop ."], "bleu": 0.1135935489027116, "rouge_l": 0.16116248348745044}
{"id": 3459, "code": "def set bang to py ast ( ctx : Generator Context , node : Set Bang ) -> Generated Py AST : assert node . op == Node Op . SET BANG val temp name = genname ( \"set bang val\" ) val ast = gen py ast ( ctx , node . val ) target = node . target assert isinstance ( target , ( Host Field , Local , Var Ref ) ) , f\"invalid set! target type {type(target)}\" if isinstance ( target , Host Field ) : target ast = interop prop to py ast ( ctx , target , is assigning = True ) elif isinstance ( target , Var Ref ) : target ast = var sym to py ast ( ctx , target , is assigning = True ) elif isinstance ( target , Local ) : target ast = local sym to py ast ( ctx , target , is assigning = True ) else : raise Generator Exception ( f\"invalid set! target type {type(target)}\" , lisp ast = target ) return Generated Py AST ( node = ast . Name ( id = val temp name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( val ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = val temp name , ctx = ast . Store ( ) ) ] , value = val ast . node , ) ] , target ast . dependencies , [ ast . Assign ( targets = [ target ast . node ] , value = val ast . node ) ] , ) ) , )", "predictions": ["generate a set of ( instances ."], "references": ["return a python ast node for a set! expression ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3460, "code": "def throw to py ast ( ctx : Generator Context , node : Throw ) -> Generated Py AST : assert node . op == Node Op . THROW throw fn = genname ( THROW PREFIX ) exc ast = gen py ast ( ctx , node . exception ) raise body = ast . Raise ( exc = exc ast . node , cause = None ) return Generated Py AST ( node = ast . Call ( func = ast . Name ( id = throw fn , ctx = ast . Load ( ) ) , args = [ ] , keywords = [ ] ) , dependencies = [ ast . Function Def ( name = throw fn , args = ast . arguments ( args = [ ] , kwarg = None , vararg = None , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = list ( chain ( exc ast . dependencies , [ raise body ] ) ) , decorator list = [ ] , returns = None , ) ] , )", "predictions": ["copy a given ast node into a tree ."], "references": ["return a python ast node for a throw expression ."], "bleu": 0.19766634639198594, "rouge_l": 0.5213675213675214}
{"id": 3461, "code": "def try to py ast ( ctx : Generator Context , node : Try ) -> Generated Py AST : assert node . op == Node Op . TRY try expr name = genname ( \"try expr\" ) body ast = synthetic do to py ast ( ctx , node . body ) catch handlers = list ( map ( partial ( catch to py ast , ctx , try expr name = try expr name ) , node . catches ) ) finallys : List [ ast . AST ] = [ ] if node . finally is not None : finally ast = synthetic do to py ast ( ctx , node . finally ) finallys . extend ( map ( statementize , finally ast . dependencies ) ) finallys . append ( statementize ( finally ast . node ) ) return Generated Py AST ( node = ast . Name ( id = try expr name , ctx = ast . Load ( ) ) , dependencies = [ ast . Try ( body = list ( chain ( body ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = try expr name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ] , ) ) , handlers = catch handlers , orelse = [ ] , finalbody = finallys , ) ] , )", "predictions": ["extracts all the dependencies of an edge from the given ast ."], "references": ["return a python ast node for a try expression ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 3462, "code": "def local sym to py ast ( ctx : Generator Context , node : Local , is assigning : bool = False ) -> Generated Py AST : assert node . op == Node Op . LOCAL sym entry = ctx . symbol table . find symbol ( sym . symbol ( node . name ) ) assert sym entry is not None if node . local == Local Type . FIELD : this entry = ctx . symbol table . find symbol ( ctx . current this ) assert this entry is not None , \"Field type local must have this\" return Generated Py AST ( node = load attr ( f\"{this entry.munged}.{sym entry.munged}\" , ctx = ast . Store ( ) if is assigning else ast . Load ( ) , ) ) else : return Generated Py AST ( node = ast . Name ( id = sym entry . munged , ctx = ast . Store ( ) if is assigning else ast . Load ( ) ) )", "predictions": ["convert a local ast node to a local ast ."], "references": ["generate a python ast node for accessing a locally defined python variable ."], "bleu": 0.14615903653944176, "rouge_l": 0.42479108635097496}
{"id": 3463, "code": "def var find to py ast ( var name : str , ns name : str , py var ctx : ast . AST ) -> Generated Py AST : return Generated Py AST ( node = ast . Attribute ( value = ast . Call ( func = FIND VAR FN NAME , args = [ ast . Call ( func = NEW SYM FN NAME , args = [ ast . Str ( var name ) ] , keywords = [ ast . keyword ( arg = \"ns\" , value = ast . Str ( ns name ) ) ] , ) ] , keywords = [ ] , ) , attr = \"value\" , ctx = py var ctx , ) )", "predictions": ["creates a set of all the extension variables having the same value as the ast node ."], "references": ["generate var . find calls for the named symbol ."], "bleu": 0.07994607499472013, "rouge_l": 0.15541401273885352}
{"id": 3464, "code": "def interop call to py ast ( ctx : Generator Context , node : Host Call ) -> Generated Py AST : assert node . op == Node Op . HOST CALL target ast = gen py ast ( ctx , node . target ) args deps , args nodes = collection ast ( ctx , node . args ) return Generated Py AST ( node = ast . Call ( func = ast . Attribute ( value = target ast . node , attr = munge ( node . method , allow builtins = True ) , ctx = ast . Load ( ) , ) , args = list ( args nodes ) , keywords = [ ] , ) , dependencies = list ( chain ( target ast . dependencies , args deps ) ) , )", "predictions": ["calls a function and creates a tree from it ."], "references": ["generate a python ast node for python interop method calls ."], "bleu": 0.13564514503163538, "rouge_l": 0.18885448916408668}
{"id": 3465, "code": "def interop prop to py ast ( ctx : Generator Context , node : Host Field , is assigning : bool = False ) -> Generated Py AST : assert node . op == Node Op . HOST FIELD target ast = gen py ast ( ctx , node . target ) return Generated Py AST ( node = ast . Attribute ( value = target ast . node , attr = munge ( node . field ) , ctx = ast . Store ( ) if is assigning else ast . Load ( ) , ) , dependencies = target ast . dependencies , )", "predictions": ["creates a ( object from a ast node and an iterator of its associated dependencies ."], "references": ["generate a python ast node for python interop property access ."], "bleu": 0.11502783619900048, "rouge_l": 0.3065326633165829}
{"id": 3466, "code": "def with meta to py ast ( ctx : Generator Context , node : With Meta , * * kwargs ) -> Generated Py AST : assert node . op == Node Op . WITH META handle expr = WITH META EXPR HANDLER . get ( node . expr . op ) assert ( handle expr is not None ) , \"No expression handler for with-meta child node type\" return handle expr ( ctx , node . expr , meta node = node . meta , * * kwargs )", "predictions": ["convert from the pain to a single ast node ."], "references": ["generate a python ast node for python interop method calls ."], "bleu": 0.1705647399369684, "rouge_l": 0.37770897832817335}
{"id": 3467, "code": "def py module preamble ( ctx : Generator Context , ) -> Generated Py AST : preamble : List [ ast . AST ] = [ ] preamble . extend ( module imports ( ctx ) ) preamble . append ( from module import ( ) ) preamble . append ( ns var ( ) ) return Generated Py AST ( node = ast . Name Constant ( None ) , dependencies = preamble )", "predictions": ["returns a module object from the given module and any -> preamble objects ."], "references": ["bootstrap a new module with imports and other boilerplate ."], "bleu": 0.11114924776032006, "rouge_l": 0.34366197183098596}
{"id": 3468, "code": "def set ( members : Iterable [ T ] , meta = None ) -> Set [ T ] : return Set ( pset ( members ) , meta = meta )", "predictions": ["sets the members of this class to the exact meta set ."], "references": ["creates a new set ."], "bleu": 0.1367440667823257, "rouge_l": 0.25416666666666665}
{"id": 3469, "code": "def s ( * members : T , meta = None ) -> Set [ T ] : return Set ( pset ( members ) , meta = meta )", "predictions": ["returns the exact element in the cache ."], "references": ["creates a new set from members ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3470, "code": "def visit Except Handler ( self , node : ast . Except Handler ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Except Handler ) return ast . copy location ( ast . Except Handler ( type = new node . type , name = new node . name , body = filter dead code ( new node . body ) , ) , new node , )", "predictions": ["create two default hive-site nodes for handling nodes ."], "references": ["eliminate dead code from except handler bodies ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3471, "code": "def visit Function Def ( self , node : ast . Function Def ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Function Def ) return ast . copy location ( ast . Function Def ( name = new node . name , args = new node . args , body = filter dead code ( new node . body ) , decorator list = new node . decorator list , returns = new node . returns , ) , new node , )", "predictions": ["this builds an example to be used as a . ."], "references": ["eliminate dead code from function bodies ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 3472, "code": "def visit While ( self , node : ast . While ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . While ) return ast . copy location ( ast . While ( test = new node . test , body = filter dead code ( new node . body ) , orelse = filter dead code ( new node . orelse ) , ) , new node , )", "predictions": ["this method reports the filter ' s filter for the ast node ."], "references": ["eliminate dead code from while bodies ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 3473, "code": "def visit Try ( self , node : ast . Try ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Try ) return ast . copy location ( ast . Try ( body = filter dead code ( new node . body ) , handlers = new node . handlers , orelse = filter dead code ( new node . orelse ) , finalbody = filter dead code ( new node . finalbody ) , ) , new node , )", "predictions": ["generate and return the new filter ."], "references": ["eliminate dead code from except try bodies ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3474, "code": "def nthrest ( coll , i : int ) : while True : if coll is None : return None if i == 0 : return coll i -= 1 coll = rest ( coll )", "predictions": ["perform a nthrest conjunction ."], "references": ["returns the nth rest sequence of coll or coll if i is 0 ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 3475, "code": "def nthnext ( coll , i : int ) -> Optional [ I Seq ] : while True : if coll is None : return None if i == 0 : return to seq ( coll ) i -= 1 coll = next ( coll )", "predictions": ["create a new array from a given coll array ."], "references": ["returns the nth next sequence of coll ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3476, "code": "def to seq ( o ) -> Optional [ I Seq ] : if o is None : return None if isinstance ( o , I Seq ) : return seq or nil ( o ) if isinstance ( o , I Seqable ) : return seq or nil ( o . seq ( ) ) return seq or nil ( lseq . sequence ( o ) )", "predictions": ["convert a sequence to a sequence"], "references": ["coerce the argument o to a iseq . if o is none return none ."], "bleu": 0.06486736672746916, "rouge_l": 0.17681159420289855}
{"id": 3477, "code": "def concat ( * seqs ) -> I Seq : allseqs = lseq . sequence ( itertools . chain ( * filter ( None , map ( to seq , seqs ) ) ) ) if allseqs is None : return lseq . EMPTY return allseqs", "predictions": ["concatenate all layers in this sequence with the same sequence ."], "references": ["concatenate the sequences given by seqs into a single iseq ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 3478, "code": "def partial ( f , * args ) : @ functools . wraps ( f ) def partial f ( * inner args ) : return f ( * itertools . chain ( args , inner args ) ) return partial f", "predictions": ["decorator to wraps a partial function at the given point ."], "references": ["return a function which is the partial application of f with args ."], "bleu": 0.12498879161997976, "rouge_l": 0.3283983849259758}
{"id": 3479, "code": "def contains ( coll , k ) : if isinstance ( coll , I Associative ) : return coll . contains ( k ) return k in coll", "predictions": ["checks if the given section contains an item in the provided coll ."], "references": ["return true if o contains the key k ."], "bleu": 0.12011055432195765, "rouge_l": 0.37596302003081655}
{"id": 3480, "code": "def get ( m , k , default = None ) : if isinstance ( m , I Associative ) : return m . entry ( k , default = default ) try : return m [ k ] except ( Key Error , Index Error , Type Error ) as e : logger . debug ( \"Ignored %s: %s\" , type ( e ) . name , e ) return default", "predictions": ["get the name of this column ."], "references": ["return the value of k in m . return default if k not found in m ."], "bleu": 0.05293793409875998, "rouge_l": 0.23252858958068615}
{"id": 3481, "code": "def to lisp ( o , keywordize keys : bool = True ) : if not isinstance ( o , ( dict , frozenset , list , set , tuple ) ) : return o else : return to lisp backup ( o , keywordize keys = keywordize keys )", "predictions": ["recursively convert all the elements of this object to their corresponding existing values"], "references": ["recursively convert python collections into lisp collections ."], "bleu": 0.12571192676522522, "rouge_l": 0.19902120717781402}
{"id": 3482, "code": "def to py ( o , keyword fn : Callable [ [ kw . Keyword ] , Any ] = kw name ) : if isinstance ( o , I Seq ) : return to py list ( o , keyword fn = keyword fn ) elif not isinstance ( o , ( I Persistent List , I Persistent Map , I Persistent Set , I Persistent Vector ) ) : return o else : return to py backup ( o , keyword fn = keyword fn )", "predictions": ["translate a set of strings to a python object"], "references": ["recursively convert lisp collections into python collections ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3483, "code": "def collect args ( args ) -> I Seq : if isinstance ( args , tuple ) : return llist . list ( args ) raise Type Error ( \"Python variadic arguments should always be a tuple\" )", "predictions": ["collect all the arguments from this multi - memory / 2 / etcd / arguments ."], "references": ["collect python starred arguments into a basilisp list ."], "bleu": 0.09147827112247602, "rouge_l": 0.2527624309392265}
{"id": 3484, "code": "def init ns var ( which ns : str = CORE NS , ns var name : str = NS VAR NAME ) -> Var : core sym = sym . Symbol ( which ns ) core ns = Namespace . get or create ( core sym ) ns var = Var . intern ( core sym , sym . Symbol ( ns var name ) , core ns , dynamic = True ) logger . debug ( f\"Created namespace variable {sym.symbol(ns var name, ns=which ns)}\" ) return ns var", "predictions": ["creates new namespace variable processor ."], "references": ["initialize the dynamic * ns * variable in the namespace which_ns ."], "bleu": 0.09663861439684919, "rouge_l": 0.20962199312714777}
{"id": 3485, "code": "def set current ns ( ns name : str , module : types . Module Type = None , ns var name : str = NS VAR NAME , ns var ns : str = NS VAR NS , ) -> Var : symbol = sym . Symbol ( ns name ) ns = Namespace . get or create ( symbol , module = module ) ns var sym = sym . Symbol ( ns var name , ns = ns var ns ) ns var = Maybe ( Var . find ( ns var sym ) ) . or else raise ( lambda : Runtime Exception ( f\"Dynamic Var {sym.Symbol(ns var name, ns=ns var ns)} not bound!\" ) ) ns var . push bindings ( ns ) logger . debug ( f\"Setting {ns var sym} to {ns}\" ) return ns var", "predictions": ["set a {sym.symbol(ns variable to be used in the process context ."], "references": ["set the value of the dynamic variable * ns * in the current thread ."], "bleu": 0.12664626029867396, "rouge_l": 0.363095238095238}
{"id": 3486, "code": "def get current ns ( ns var name : str = NS VAR NAME , ns var ns : str = NS VAR NS ) -> Namespace : ns sym = sym . Symbol ( ns var name , ns = ns var ns ) ns : Namespace = Maybe ( Var . find ( ns sym ) ) . map ( lambda v : v . value ) . or else raise ( lambda : Runtime Exception ( f\"Dynamic Var {ns sym} not bound!\" ) ) return ns", "predictions": ["get the current variable declaration by namespace ."], "references": ["get the value of the dynamic variable * ns * in the current thread ."], "bleu": 0.11578838804156227, "rouge_l": 0.32972972972972975}
{"id": 3487, "code": "def resolve alias ( s : sym . Symbol , ns : Optional [ Namespace ] = None ) -> sym . Symbol : if s in SPECIAL FORMS : return s ns = Maybe ( ns ) . or else ( get current ns ) if s . ns is not None : aliased ns = ns . get alias ( sym . symbol ( s . ns ) ) if aliased ns is not None : return sym . symbol ( s . name , aliased ns . name ) else : return s else : which var = ns . find ( sym . symbol ( s . name ) ) if which var is not None : return sym . symbol ( which var . name . name , which var . ns . name ) else : return sym . symbol ( s . name , ns = ns . name )", "predictions": ["resolve the given extension by the given string ."], "references": ["resolve the aliased symbol in the current namespace ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 3488, "code": "def add generated python ( generated python : str , var name : str = GENERATED PYTHON VAR NAME , which ns : Optional [ str ] = None , ) -> None : if which ns is None : which ns = get current ns ( ) . name ns sym = sym . Symbol ( var name , ns = which ns ) v = Maybe ( Var . find ( ns sym ) ) . or else ( lambda : Var . intern ( sym . symbol ( which ns ) , sym . symbol ( var name ) , \"\" , dynamic = True , meta = lmap . map ( { PRIVATE META KEY : True } ) , ) ) v . value = v . value + generated python", "predictions": ["creates a new to be used as an argument ."], "references": ["add generated python code to a dynamic variable in which_ns ."], "bleu": 0.13564514503163538, "rouge_l": 0.18885448916408668}
{"id": 3489, "code": "def print generated python ( var name : str = PRINT GENERATED PY VAR NAME , core ns name : str = CORE NS ) -> bool : ns sym = sym . Symbol ( var name , ns = core ns name ) return ( Maybe ( Var . find ( ns sym ) ) . map ( lambda v : v . value ) . or else raise ( lambda : Runtime Exception ( f\"Dynamic Var {ns sym} not bound!\" ) ) )", "predictions": ["prints a python declaration ."], "references": ["return the value of the * print - generated - python * dynamic variable ."], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 3490, "code": "def intern ( ns : sym . Symbol , name : sym . Symbol , val , dynamic : bool = False , meta = None ) -> \"Var\" : var ns = Namespace . get or create ( ns ) var = var ns . intern ( name , Var ( var ns , name , dynamic = dynamic , meta = meta ) ) var . root = val return var", "predictions": ["loop through a , or set a , where no , has been specified ."], "references": ["intern the value bound to the symbol name in namespace ns ."], "bleu": 0.08225964699966554, "rouge_l": 0.07558859975216851}
{"id": 3491, "code": "def intern unbound ( ns : sym . Symbol , name : sym . Symbol , dynamic : bool = False , meta = None ) -> \"Var\" : var ns = Namespace . get or create ( ns ) return var ns . intern ( name , Var ( var ns , name , dynamic = dynamic , meta = meta ) )", "predictions": ["creates a , where variable is a , i . e . , it is a , so that the , it can be used to perform a , to the , so that the , it can be used to convert the , to a , to a ,"], "references": ["create a new unbound var instance to the symbol name in namespace ns ."], "bleu": 0.0359340051359579, "rouge_l": 0.10433295324971494}
{"id": 3492, "code": "def add alias ( self , alias : sym . Symbol , namespace : \"Namespace\" ) -> None : self . aliases . swap ( lambda m : m . assoc ( alias , namespace ) )", "predictions": ["adds a single to this , ."], "references": ["add a symbol alias for the given namespace ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3493, "code": "def add refer ( self , sym : sym . Symbol , var : Var ) -> None : if not var . is private : self . refers . swap ( lambda s : s . assoc ( sym , var ) )", "predictions": ["method that will be called on the map notifications of the decision instances ."], "references": ["refer var in this namespace under the name sym ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 3494, "code": "def get refer ( self , sym : sym . Symbol ) -> Optional [ Var ] : return self . refers . entry ( sym , None )", "predictions": ["get the name of this class ."], "references": ["get the var referred by symbol or none if it does not exist ."], "bleu": 0.09663861439684919, "rouge_l": 0.2695139911634757}
{"id": 3495, "code": "def refer all ( cls , refers : lmap . Map , other ns interns : lmap . Map ) -> lmap . Map : final refers = refers for entry in other ns interns : s : sym . Symbol = entry . key var : Var = entry . value if not var . is private : final refers = final refers . assoc ( s , var ) return final refers", "predictions": ["return by type py ."], "references": ["refer all _public_ interns from another namespace ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3496, "code": "def refer all ( self , other ns : \"Namespace\" ) : self . refers . swap ( Namespace . refer all , other ns . interns )", "predictions": ["adds this instance to the given one ."], "references": ["refer all the vars in the other namespace ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3497, "code": "def list ( members , meta = None ) -> List : return List ( plist ( iterable = members ) , meta = meta )", "predictions": ["interop , this is useful for use in an entity ."], "references": ["creates a new list ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 3498, "code": "def l ( * members , meta = None ) -> List : return List ( plist ( iterable = members ) , meta = meta )", "predictions": ["add py for each element in the list ."], "references": ["creates a new list from members ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 3499, "code": "def change style ( style , representer ) : def new representer ( dumper , data ) : scalar = representer ( dumper , data ) scalar . style = style return scalar return new representer", "predictions": ["py the ) method to create the new ."], "references": ["this function is used to format the key value as a multi - line string maintaining the line breaks"], "bleu": 0.05525326412756203, "rouge_l": 0.1342134213421342}
{"id": 3500, "code": "def delete ( self , * args ) : cache = get cache ( ) key = self . get cache key ( * args ) if key in cache : del cache [ key ]", "predictions": ["deletes the , if there is no instance ."], "references": ["remove the key from the request cache and from memcache ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 3501, "code": "async def connect ( self ) : self . reader , self . writer = await asyncio . open connection ( self . host , self . port , loop = self . loop ) self . welcome msg = await self . reader . read ( self . buffer size )", "predictions": ["( abstract : get the jcombobox : https : / / www . org / tr / ( / ( / ( / ( / ( . com / ( / ( / ( / ( / ( / ( ."], "references": ["connect to lasaf through a cam - socket ."], "bleu": 0.029374336289654013, "rouge_l": 0.04521868050407709}
{"id": 3502, "code": "async def receive ( self ) : try : incomming = await self . reader . read ( self . buffer size ) except OS Error : return [ ] return parse receive ( incomming )", "predictions": ["receive attempts to receive the policy ."], "references": ["receive message from socket interface as list of ordereddict ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3503, "code": "def dump ( ndb model , fp , * * kwargs ) : for chunk in Ndb Encoder ( * * kwargs ) . iterencode ( ndb model ) : fp . write ( chunk )", "predictions": ["visit a ( produce a ( key , value ast ast ast ast ast ast ast ast ast ast ast ast ast ast ast ast ast ast ast . ast . ast ast ast ast ."], "references": ["custom json dump using the custom encoder above ."], "bleu": 0.03351542279475122, "rouge_l": 0.049836601307189546}
{"id": 3504, "code": "def object hook handler ( self , val ) : return { k : self . decode date ( v ) for k , v in val . iteritems ( ) }", "predictions": ["set the hook value for each element in this ( ( ast ast ast ast ast ast ast ast ast ."], "references": ["handles decoding of nested date strings ."], "bleu": 0.05809665204409193, "rouge_l": 0.0785070785070785}
{"id": 3505, "code": "def decode date ( self , val ) : if isinstance ( val , basestring ) and val . count ( '-' ) == 2 and len ( val ) > 9 : try : dt = dateutil . parser . parse ( val ) if val . endswith ( ( '+00:00' , '-00:00' , 'Z' ) ) : dt = dt . replace ( tzinfo = None ) return dt except ( Type Error , Value Error ) : pass return val", "predictions": ["visit value using the default formatting of the given script ."], "references": ["tries to decode strings that look like dates into datetime objects ."], "bleu": 0.10400927574124633, "rouge_l": 0.08628005657708629}
{"id": 3506, "code": "def decode ( self , val ) : new val = self . decode date ( val ) if val != new val : return new val return json . JSON Decoder . decode ( self , val )", "predictions": ["decodes a if you already have a == encoded value , you need to decode ."], "references": ["override of the default decode method that also uses decode_date ."], "bleu": 0.08513012360883544, "rouge_l": 0.15326633165829145}
{"id": 3507, "code": "def default ( self , obj ) : obj type = type ( obj ) if obj type not in self . ndb type encoding : if hasattr ( obj , ' metaclass ' ) : obj type = obj . metaclass else : for ndb type in NDB TYPES : if isinstance ( obj , ndb type ) : obj type = ndb type break fn = self . ndb type encoding . get ( obj type ) if fn : return fn ( obj ) return json . JSON Encoder . default ( self , obj )", "predictions": ["get the default for the default ."], "references": ["overriding the default jsonencoder . default for ndb support ."], "bleu": 0.20958712452883563, "rouge_l": 0.45607476635514016}
{"id": 3508, "code": "def validate version ( ) : import leicacam version string = leicacam . version versions = version string . split ( '.' , 3 ) try : for ver in versions : int ( ver ) except Value Error : print ( 'Only integers are allowed in release version, ' 'please adjust current version {}' . format ( version string ) ) return None return version string", "predictions": ["to validate seq seq ."], "references": ["validate version before release ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 3509, "code": "def robust topological sort ( graph : Graph ) -> list : assert check argument types ( ) components = strongly connected components ( graph ) node component = { } for component in components : for node in component : node component [ node ] = component component graph = { } for component in components : component graph [ component ] = [ ] for node in graph : node c = node component [ node ] for successor in graph [ node ] : successor c = node component [ successor ] if node c != successor c : component graph [ node c ] . append ( successor c ) return topological sort ( component graph )", "predictions": ["concat all ( ( ( concat ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["identify strongly connected components then perform a topological sort of those components ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3510, "code": "def logger ( function ) : @ functools . wraps ( function ) def wrapper ( * args , * * kwargs ) : \"\"\"Wrap function.\"\"\" sep = kwargs . get ( 'sep' , ' ' ) end = kwargs . get ( 'end' , '' ) out = sep . join ( [ repr ( x ) for x in args ] ) out = out + end LOGGER . debug ( out ) return function ( * args , * * kwargs ) return wrapper", "predictions": ["@ the given function ."], "references": ["decorate passed in function and log message to module logger ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 3511, "code": "def connect ( self ) : self . socket = socket . socket ( ) self . socket . connect ( ( self . host , self . port ) ) self . socket . settimeout ( False ) sleep ( self . delay ) self . welcome msg = self . socket . recv ( self . buffer size )", "predictions": ["override this method to contains the amount of sockets to contains the connection ."], "references": ["connect to lasaf through a cam - socket ."], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 3512, "code": "def flush ( self ) : debug ( 'flushing incomming socket messages' ) try : while True : msg = self . socket . recv ( self . buffer size ) debug ( b'< ' + msg ) except socket . error : pass", "predictions": ["get the message on the output ."], "references": ["flush incomming socket messages ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3513, "code": "def receive ( self ) : try : incomming = self . socket . recv ( self . buffer size ) except socket . error : return [ ] return parse receive ( incomming )", "predictions": ["to to to to to get the debugger half of the process ."], "references": ["receive message from socket interface as list of ordereddict ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 3514, "code": "def enable ( self , slide = 0 , wellx = 1 , welly = 1 , fieldx = 1 , fieldy = 1 ) : cmd = [ ( 'cmd' , 'enable' ) , ( 'slide' , str ( slide ) ) , ( 'wellx' , str ( wellx ) ) , ( 'welly' , str ( welly ) ) , ( 'fieldx' , str ( fieldx ) ) , ( 'fieldy' , str ( fieldy ) ) , ( 'value' , 'true' ) ] self . send ( cmd ) return self . wait for ( * cmd [ 0 ] )", "predictions": ["to to to to to to to to to to to contain ( ( ."], "references": ["enable a given scan field ."], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 3515, "code": "def save template ( self , filename = \"{Scanning Template}leicacam.xml\" ) : cmd = [ ( 'sys' , '0' ) , ( 'cmd' , 'save' ) , ( 'fil' , str ( filename ) ) ] self . send ( cmd ) return self . wait for ( * cmd [ 0 ] )", "predictions": ["collect data to a args"], "references": ["save scanning template to filename ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3516, "code": "def get information ( self , about = 'stage' ) : cmd = [ ( 'cmd' , 'getinfo' ) , ( 'dev' , str ( about ) ) ] self . send ( cmd ) return self . wait for ( * cmd [ 1 ] )", "predictions": ["get the list of : this method returns the list of : : get the : - get the : - - : : : [ highest - : ljava - : space - : ljava - : no - left - left - left - get - get -"], "references": ["get information about given keyword . defaults to stage ."], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 3517, "code": "def locate package json ( ) : directory = settings . SYSTEMJS PACKAGE JSON DIR if not directory : raise Improperly Configured ( \"Could not locate 'package.json'. Set SYSTEMJS PACKAGE JSON DIR \" \"to the directory that holds 'package.json'.\" ) path = os . path . join ( directory , 'package.json' ) if not os . path . isfile ( path ) : raise Improperly Configured ( \"'package.json' does not exist, tried looking in %s\" % path ) return path", "predictions": ["set the create create a ns str ."], "references": ["find and return the location of package . json ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3518, "code": "def parse package json ( ) : with open ( locate package json ( ) ) as pjson : data = json . loads ( pjson . read ( ) ) return data", "predictions": ["reads a current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current"], "references": ["extract the jspm configuration from package . json ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3519, "code": "def validate yourls response ( response , data ) : try : response . raise for status ( ) except HTTP Error as http exc : http error info = sys . exc info ( ) reraise = False try : jsondata = response . json ( ) except Value Error : reraise = True else : logger . debug ( 'Received error {response} with JSON {json}' , response = response , json = jsondata ) handle api error with json ( http exc , jsondata , response ) if reraise : six . reraise ( * http error info ) else : jsondata = response . json ( ) logger . debug ( 'Received {response} with JSON {json}' , response = response , json = jsondata ) if { 'status' , 'code' , 'message' } <= set ( jsondata . keys ( ) ) : status = jsondata [ 'status' ] code = jsondata [ 'code' ] message = jsondata [ 'message' ] if status == 'fail' : if code == 'error:keyword' : raise YOURLS Keyword Exists Error ( message , keyword = data [ 'keyword' ] ) elif code == 'error:url' : url = json to shortened url ( jsondata [ 'url' ] , jsondata [ 'shorturl' ] ) raise YOURLSURL Exists Error ( message , url = url ) else : raise YOURLSAPI Error ( message ) else : return jsondata else : return jsondata", "predictions": ["resolve ( user data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["validate response from yourls server ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 3520, "code": "def interp dep vector ( wave , indep vector ) : dep vector is int = wave . dep vector . dtype . name . startswith ( \"int\" ) dep vector is complex = wave . dep vector . dtype . name . startswith ( \"complex\" ) if ( wave . interp , wave . indep scale ) == ( \"CONTINUOUS\" , \"LOG\" ) : wave interp func = scipy . interpolate . interp1d ( np . log10 ( wave . indep vector ) , wave . dep vector ) ret = wave interp func ( np . log10 ( indep vector ) ) elif ( wave . interp , wave . indep scale ) == ( \"CONTINUOUS\" , \"LINEAR\" ) : dep vector = ( wave . dep vector . astype ( np . float64 ) if not dep vector is complex else wave . dep vector ) wave interp func = scipy . interpolate . interp1d ( wave . indep vector , dep vector ) ret = wave interp func ( indep vector ) else : wave interp func = scipy . interpolate . interp1d ( wave . indep vector , wave . dep vector , kind = \"zero\" ) ret = wave interp func ( indep vector ) eq comp = np . all ( np . isclose ( wave . indep vector [ - 1 ] , indep vector [ - 1 ] , FP RTOL , FP ATOL ) ) if eq comp : ret [ - 1 ] = wave . dep vector [ - 1 ] round ret = np . round ( ret , 0 ) return ( round ret . astype ( \"int\" ) if ( dep vector is int and np . all ( np . isclose ( round ret , ret , FP RTOL , FP ATOL ) ) ) else ret )", "predictions": ["construct an array of average objects"], "references": ["create new dependent variable vector ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3521, "code": "def get indep vector ( wave a , wave b ) : exobj = pexdoc . exh . addex ( Runtime Error , \"Independent variable ranges do not overlap\" ) min bound = max ( np . min ( wave a . indep vector ) , np . min ( wave b . indep vector ) ) max bound = min ( np . max ( wave a . indep vector ) , np . max ( wave b . indep vector ) ) exobj ( bool ( min bound > max bound ) ) raw range = np . unique ( np . concatenate ( ( wave a . indep vector , wave b . indep vector ) ) ) return raw range [ np . logical and ( min bound <= raw range , raw range <= max bound ) ]", "predictions": ["gets the triangle between two widgets"], "references": ["create new independent variable vector ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3522, "code": "def verify compatibility ( wave a , wave b , check dep units = True ) : exobj = pexdoc . exh . addex ( Runtime Error , \"Waveforms are not compatible\" ) ctuple = ( bool ( wave a . indep scale != wave b . indep scale ) , bool ( wave a . dep scale != wave b . dep scale ) , bool ( wave a . indep units != wave b . indep units ) , ( bool ( wave a . dep units != wave b . dep units ) if check dep units else False ) , bool ( wave a . interp != wave b . interp ) , ) exobj ( any ( ctuple ) )", "predictions": ["verify a given wave document against the value specified in the error . the error is thrown if the error is thrown ."], "references": ["verify that two waveforms can be combined with various mathematical functions ."], "bleu": 0.05856458233275369, "rouge_l": 0.1211519364448858}
{"id": 3523, "code": "def trace pars ( mname ) : pickle fname = os . path . join ( os . path . dirname ( file ) , \"{0}.pkl\" . format ( mname ) ) ddir = os . path . dirname ( os . path . dirname ( file ) ) moddb fname = os . path . join ( ddir , \"moddb.json\" ) in callables fname = moddb fname if os . path . exists ( moddb fname ) else None out callables fname = os . path . join ( ddir , \"{0}.json\" . format ( mname ) ) noption = os . environ . get ( \"NOPTION\" , None ) exclude = [ \" pytest\" , \"execnet\" ] partuple = collections . namedtuple ( \"Par Tuple\" , [ \"pickle fname\" , \"in callables fname\" , \"out callables fname\" , \"noption\" , \"exclude\" , ] , ) return partuple ( pickle fname , in callables fname , out callables fname , noption , exclude )", "predictions": ["to provide a namedtuple for all ( files ."], "references": ["define trace parameters ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3524, "code": "def run trace ( mname , fname , module prefix , callable names , no print , module exclude = None , callable exclude = None , debug = False , ) : module exclude = [ ] if module exclude is None else module exclude callable exclude = [ ] if callable exclude is None else callable exclude par = trace pars ( mname ) start time = datetime . datetime . now ( ) with pexdoc . exdoc . Ex Doc Cxt ( exclude = par . exclude + module exclude , pickle fname = par . pickle fname , in callables fname = par . in callables fname , out callables fname = par . out callables fname , no print = no print , ) as exdoc obj : fname = os . path . realpath ( os . path . join ( os . path . dirname ( file ) , \"..\" , \"..\" , \"tests\" , \"test {0}.py\" . format ( fname ) , ) ) test cmd = ( [ \"--color=yes\" ] + ( [ \"-s\" , \"-vv\" ] if debug else [ \"-q\" , \"-q\" , \"-q\" ] ) + [ \"--disable-warnings\" ] + [ \"-x\" ] + ( [ par . noption ] if par . noption else [ ] ) + [ \"-m \" + mname ] + [ fname ] ) with warnings . catch warnings ( ) : warnings . filterwarnings ( \"ignore\" , category = Pytest Warning ) if pytest . main ( test cmd ) : raise Runtime Error ( \"Tracing did not complete successfully\" ) stop time = datetime . datetime . now ( ) if not no print : print ( \"Auto-generation of exceptions documentation time: {0}\" . format ( pmisc . elapsed time string ( start time , stop time ) ) ) for callable name in callable names : callable name = module prefix + callable name print ( \"\\n Callable: {0}\" . format ( callable name ) ) print ( exdoc obj . get sphinx doc ( callable name , exclude = callable exclude ) ) print ( \"\\n\" ) return copy . copy ( exdoc obj )", "predictions": ["run the trace of the given object ."], "references": ["run module tracing ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 3525, "code": "def flatten ( iterable , map2iter = None ) : if map2iter and isinstance ( iterable ) : iterable = map2iter ( iterable ) for item in iterable : if isinstance ( item , str ) or not isinstance ( item , abc . Iterable ) : yield item else : yield from flatten ( item , map2iter )", "predictions": ["flatten an iterable . takes a list of additions and flattens the resulting list as a single item ."], "references": ["recursively flatten nested objects"], "bleu": 0.06439931429457924, "rouge_l": 0.09854604200323101}
{"id": 3526, "code": "def printtsv ( table , sep = \"\\t\" , file = sys . stdout ) : for record in table : print ( * record , sep = sep , file = file )", "predictions": ["print a printtsv at the specified record ."], "references": ["stupidly print an iterable of iterables in tsv format"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3527, "code": "def mkdummy ( name , * * attrs ) : return type ( name , ( ) , dict ( repr = ( lambda self : \"<%s>\" % name ) , * * attrs ) ) ( )", "predictions": ["allows you to define this method to call ( ( ) method on your arguments . this will be called after the method ' s call has been called ."], "references": ["make a placeholder object that uses its own name for its repr"], "bleu": 0.03392268780792677, "rouge_l": 0.0}
{"id": 3528, "code": "def from str ( cls , human readable str , decimal = False , bits = False ) : divisor = 1000 if decimal else 1024 num = [ ] c = \"\" for c in human readable str : if c not in cls . digits : break num . append ( c ) num = \"\" . join ( num ) try : num = int ( num ) except Value Error : num = float ( num ) if bits : num /= 8 return cls ( round ( num * divisor ** cls . key [ c . lower ( ) ] ) )", "predictions": ["returns a string representation of this class or number of bytes ."], "references": ["attempt to parse a size in bytes from a human - readable string ."], "bleu": 0.11059379640711253, "rouge_l": 0.22761194029850743}
{"id": 3529, "code": "def trace module ( no print = True ) : mname = \"wave core\" fname = \"peng\" module prefix = \"peng.{0}.Waveform.\" . format ( mname ) callable names = ( \" init \" , ) return docs . support . trace support . run trace ( mname , fname , module prefix , callable names , no print )", "predictions": ["helper method for formatting a trace ."], "references": ["trace eng wave module exceptions ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3530, "code": "def def links ( mobj ) : fdict = json load ( os . path . join ( \"data\" , \"requirements.json\" ) ) sdeps = sorted ( fdict . keys ( ) ) olines = [ ] for item in sdeps : olines . append ( \"..  {name}: {url}\\n\" . format ( name = fdict [ item ] [ \"name\" ] , url = fdict [ item ] [ \"url\" ] ) ) ret = [ ] for line in olines : wobj = textwrap . wrap ( line , width = LINE WIDTH , subsequent indent = \"   \" ) ret . append ( \"\\n\" . join ( [ item for item in wobj ] ) ) mobj . out ( \"\\n\" . join ( ret ) )", "predictions": ["read all links from the given input ."], "references": ["define sphinx requirements links ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 3531, "code": "def make common entry ( plist , pyver , suffix , req ver ) : prefix = \"Python {pyver}.x{suffix}\" . format ( pyver = pyver , suffix = suffix ) plist . append ( \"{prefix}{ver}\" . format ( prefix = prefix , ver = ops to words ( req ver ) ) )", "predictions": ["build a common entry for the given entry ."], "references": ["generate python interpreter version entries for 2 . x or 3 . x series ."], "bleu": 0.08019421212222273, "rouge_l": 0.15947712418300655}
{"id": 3532, "code": "def make multi entry ( plist , pkg pyvers , ver dict ) : for pyver in pkg pyvers : pver = pyver [ 2 ] + \".\" + pyver [ 3 : ] plist . append ( \"Python {0}: {1}\" . format ( pver , ops to words ( ver dict [ pyver ] ) ) )", "predictions": ["make summary of ( from package entry ."], "references": ["generate python interpreter version entries ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3533, "code": "def ops to words ( item ) : unsupp ops = [ \"~=\" , \"===\" ] supp ops = [ \">=\" , \">\" , \"==\" , \"<=\" , \"<\" , \"!=\" ] tokens = sorted ( item . split ( \",\" ) , reverse = True ) actual tokens = [ ] for req in tokens : for op in unsupp ops : if req . startswith ( op ) : raise Runtime Error ( \"Unsupported version specification: {0}\" . format ( op ) ) for op in supp ops : if req . startswith ( op ) : actual tokens . append ( op ) break else : raise Runtime Error ( \"Illegal comparison operator: {0}\" . format ( op ) ) if len ( list ( set ( actual tokens ) ) ) != len ( actual tokens ) : raise Runtime Error ( \"Multiple comparison operators of the same type\" ) if \"!=\" in actual tokens : return ( \" and \" . join ( [ op to words ( token ) for token in tokens [ : - 1 ] ] ) + \" \" + op to words ( tokens [ - 1 ] ) ) return \" and \" . join ( [ op to words ( token ) for token in tokens ] )", "predictions": ["convert a list of words into a . ."], "references": ["translate requirement specification to words ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 3534, "code": "def chunk noise ( noise ) : data = zip ( noise [ \"freq\" ] , noise [ \"nf\" ] , np . abs ( noise [ \"rc\" ] ) , np . angle ( noise [ \"rc\" ] ) , noise [ \"res\" ] , ) for freq , nf , rcmag , rcangle , res in data : yield freq , nf , rcmag , rcangle , res", "predictions": ["generate a chunk of ( that can be used to compute the axes ."], "references": ["chunk input noise data into valid touchstone file rows ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 3535, "code": "def chunk pars ( freq vector , data matrix , pformat ) : pformat = pformat . upper ( ) length = 4 for freq , data in zip ( freq vector , data matrix ) : data = data . flatten ( ) for index in range ( 0 , data . size , length ) : fpoint = [ freq ] if not index else [ None ] cdata = data [ index : index + length ] if pformat == \"MA\" : vector1 = np . abs ( cdata ) vector2 = np . rad2deg ( np . angle ( cdata ) ) elif pformat == \"RI\" : vector1 = np . real ( cdata ) vector2 = np . imag ( cdata ) else : vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) vector2 = np . rad2deg ( np . angle ( cdata ) ) sep data = np . array ( [ ] ) for item1 , item2 in zip ( vector1 , vector2 ) : sep data = np . concatenate ( ( sep data , np . array ( [ item1 , item2 ] ) ) ) ret = np . concatenate ( ( np . array ( fpoint ) , sep data ) ) yield ret", "predictions": ["generate a chunk of optional data"], "references": ["chunk input data into valid touchstone file rows ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3536, "code": "def bound waveform ( wave , indep min , indep max ) : indep min , indep max = validate min max ( wave , indep min , indep max ) indep vector = copy . copy ( wave . indep vector ) if ( isinstance ( indep min , float ) or isinstance ( indep max , float ) ) and indep vector . dtype . name . startswith ( \"int\" ) : indep vector = indep vector . astype ( float ) min pos = np . searchsorted ( indep vector , indep min ) if not np . isclose ( indep min , indep vector [ min pos ] , FP RTOL , FP ATOL ) : indep vector = np . insert ( indep vector , min pos , indep min ) max pos = np . searchsorted ( indep vector , indep max ) if not np . isclose ( indep max , indep vector [ max pos ] , FP RTOL , FP ATOL ) : indep vector = np . insert ( indep vector , max pos , indep max ) dep vector = interp dep vector ( wave , indep vector ) wave . indep vector = indep vector [ min pos : max pos + 1 ] wave . dep vector = dep vector [ min pos : max pos + 1 ]", "predictions": ["bound a bound / polygon to the given bounds ."], "references": ["add independent variable vector bounds if they are not in vector ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 3537, "code": "def build units ( indep units , dep units , op ) : if ( not dep units ) and ( not indep units ) : return \"\" if dep units and ( not indep units ) : return dep units if ( not dep units ) and indep units : return ( remove extra delims ( \"1{0}({1})\" . format ( op , indep units ) ) if op == \"/\" else remove extra delims ( \"({0})\" . format ( indep units ) ) ) return remove extra delims ( \"({0}){1}({2})\" . format ( dep units , op , indep units ) )", "predictions": ["create the viewpoint for the test units ."], "references": ["build unit math operations ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 3538, "code": "def operation ( wave , desc , units , fpointer ) : ret = copy . copy ( wave ) ret . dep units = units ret . dep name = \"{0}({1})\" . format ( desc , ret . dep name ) ret . dep vector = fpointer ( ret . dep vector ) return ret", "predictions": ["returns an operation for the specified wave ."], "references": ["perform generic operation on a waveform object ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 3539, "code": "def running area ( indep vector , dep vector ) : rect height = np . minimum ( dep vector [ : - 1 ] , dep vector [ 1 : ] ) rect base = np . diff ( indep vector ) rect area = np . multiply ( rect height , rect base ) triang height = np . abs ( np . diff ( dep vector ) ) triang area = 0.5 * np . multiply ( triang height , rect base ) return np . cumsum ( np . concatenate ( ( np . array ( [ 0.0 ] ) , triang area + rect area ) ) )", "predictions": ["find a few area of this vector based on how long it has a ( ."], "references": ["calculate running area under curve ."], "bleu": 0.08513012360883544, "rouge_l": 0.19805194805194803}
{"id": 3540, "code": "def validate min max ( wave , indep min , indep max ) : imin , imax = False , False if indep min is None : indep min = wave . indep vector [ 0 ] imin = True if indep max is None : indep max = wave . indep vector [ - 1 ] imax = True if imin and imax : return indep min , indep max exminmax = pexdoc . exh . addex ( Runtime Error , \"Incongruent `indep min` and `indep max` arguments\" ) exmin = pexdoc . exh . addai ( \"indep min\" ) exmax = pexdoc . exh . addai ( \"indep max\" ) exminmax ( bool ( indep min >= indep max ) ) exmin ( bool ( ( indep min < wave . indep vector [ 0 ] ) and ( not np . isclose ( indep min , wave . indep vector [ 0 ] , FP RTOL , FP ATOL ) ) ) ) exmax ( bool ( ( indep max > wave . indep vector [ - 1 ] ) and ( not np . isclose ( indep max , wave . indep vector [ - 1 ] , FP RTOL , FP ATOL ) ) ) ) return indep min , indep max", "predictions": ["this method validates all the wave and maps them to their expected values ."], "references": ["validate min and max bounds are within waveform s independent variable vector ."], "bleu": 0.09782375748961449, "rouge_l": 0.1491442542787286}
{"id": 3541, "code": "def get short desc ( long desc ) : found = False olines = [ ] for line in [ item . rstrip ( ) for item in long desc . split ( \"\\n\" ) ] : if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : olines . append ( line ) elif found and olines and ( not line ) : return ( \" \" . join ( olines ) . split ( \".\" ) [ 0 ] ) . strip ( ) found = line == \".. [[[end]]]\" if not found else found return \"\"", "predictions": ["this method transforms a short line into a short ."], "references": ["get first sentence of first paragraph of long description ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 3542, "code": "def render ( self , context ) : module path = self . path . resolve ( context ) if not settings . SYSTEMJS ENABLED : if settings . SYSTEMJS DEFAULT JS EXTENSIONS : name , ext = posixpath . splitext ( module path ) if not ext : module path = '{}.js' . format ( module path ) if settings . SYSTEMJS SERVER URL : tpl = \"\"\"<script src=\"{url}{app}\" type=\"text/javascript\"></script>\"\"\" else : tpl = \"\"\"<script type=\"text/javascript\">System.import('{app}');</script>\"\"\" return tpl . format ( app = module path , url = settings . SYSTEMJS SERVER URL ) rel path = System . get bundle path ( module path ) url = staticfiles storage . url ( rel path ) tag attrs = { 'type' : 'text/javascript' } for key , value in self . tag attrs . items ( ) : if not isinstance ( value , bool ) : value = value . resolve ( context ) tag attrs [ key ] = value return \"\"\"<script{attrs} src=\"{url}\"></script>\"\"\" . format ( url = url , attrs = flatatt ( tag attrs ) )", "predictions": ["render the \"\"\"<script{attrs} and other way to the specified module ."], "references": ["build the filepath by appending the extension ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 3543, "code": "def build expr ( tokens , higher oplevel = - 1 , ldelim = \"(\" , rdelim = \")\" ) : if isinstance ( tokens , str ) : return tokens if len ( tokens ) == 2 : return \"\" . join ( tokens ) oplevel = get op level ( tokens [ 1 ] ) stoken = \"\" for num , item in enumerate ( tokens ) : if num % 2 == 0 : stoken += build expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) else : stoken += item if ( oplevel < higher oplevel ) or ( ( oplevel == higher oplevel ) and ( oplevel in OP PREC PAR ) ) : stoken = ldelim + stoken + rdelim return stoken", "predictions": ["build the patches from the input alignment ."], "references": ["build mathematical expression from hierarchical list ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 3544, "code": "def next rdelim ( items , pos ) : for num , item in enumerate ( items ) : if item > pos : break else : raise Runtime Error ( \"Mismatched delimiters\" ) del items [ num ] return item", "predictions": ["each iteration of the list from the list ."], "references": ["return position of next matching closing delimiter ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 3545, "code": "def get functions ( expr , ldelim = \"(\" , rdelim = \")\" ) : tpars = pair delims ( expr , ldelim = ldelim , rdelim = rdelim ) alphas = \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" fchars = \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \"0123456789\" \" \" tfuncs = [ ] for lnum , rnum in tpars : if lnum and expr [ lnum - 1 ] in fchars : for cnum , char in enumerate ( reversed ( expr [ : lnum ] ) ) : if char not in fchars : break else : cnum = lnum tfuncs . append ( { \"fname\" : expr [ lnum - cnum : lnum ] , \"expr\" : expr [ lnum + 1 : rnum ] , \"start\" : lnum - cnum , \"stop\" : rnum , } ) if expr [ lnum - cnum ] not in alphas : raise Runtime Error ( \"Function name `{0}` is not valid\" . format ( expr [ lnum - cnum : lnum ] ) ) return tfuncs", "predictions": ["get the functions that do not have a good . ."], "references": ["parse function calls ."], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 3546, "code": "def parse expr ( text , ldelim = \"(\" , rdelim = \")\" ) : var = pyparsing . Word ( pyparsing . alphas + \" \" , pyparsing . alphanums + \" \" ) point = pyparsing . Literal ( \".\" ) exp = pyparsing . Caseless Literal ( \"E\" ) number = pyparsing . Combine ( pyparsing . Word ( \"+-\" + pyparsing . nums , pyparsing . nums ) + pyparsing . Optional ( point + pyparsing . Optional ( pyparsing . Word ( pyparsing . nums ) ) ) + pyparsing . Optional ( exp + pyparsing . Word ( \"+-\" + pyparsing . nums , pyparsing . nums ) ) ) atom = var | number oplist = [ ( pyparsing . Literal ( \"**\" ) , 2 , pyparsing . op Assoc . RIGHT ) , ( pyparsing . one Of ( \"+ - ~\" ) , 1 , pyparsing . op Assoc . RIGHT ) , ( pyparsing . one Of ( \"* / // %\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . one Of ( \"+ -\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . one Of ( \"<< >>\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( \"&\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( \"^\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( \"|\" ) , 2 , pyparsing . op Assoc . LEFT ) , ] expr = pyparsing . infix Notation ( atom , oplist , lpar = pyparsing . Suppress ( ldelim ) , rpar = pyparsing . Suppress ( rdelim ) ) return expr . parse String ( text ) [ 0 ]", "predictions": ["parse all inline assoc and return the resulting ("], "references": ["parse mathematical expression using pyparsing ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3547, "code": "def remove consecutive delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : tpars = pair delims ( expr , ldelim = ldelim , rdelim = rdelim ) ddelim = [ ] for ctuple , ntuple in zip ( tpars , tpars [ 1 : ] ) : if ctuple == ( ntuple [ 0 ] - 1 , ntuple [ 1 ] + 1 ) : ddelim . extend ( ntuple ) ddelim . sort ( ) for num , item in enumerate ( ddelim ) : expr = expr [ : item - num ] + expr [ item - num + 1 : ] return expr", "predictions": ["fill all internal ( structures with their ( equivalents ."], "references": ["remove consecutive delimiters ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 3548, "code": "def needs ext ( self ) : if settings . SYSTEMJS DEFAULT JS EXTENSIONS : name , ext = posixpath . splitext ( self . app ) if not ext : return True return False", "predictions": ["override this decorator to support extensions for available extensions ."], "references": ["check whether self . app is missing the . js extension and if it needs it ."], "bleu": 0.06259938494637494, "rouge_l": 0.07076566125290024}
{"id": 3549, "code": "def bundle ( self ) : outfile , rel path = self . get paths ( ) options = self . opts if self . system . has jspm log ( ) : self . command += ' --log {log}' options . setdefault ( 'log' , 'err' ) if options . get ( 'minify' ) : self . command += ' --minify' if options . get ( 'skip source maps' ) : self . command += ' --skip-source-maps' try : cmd = self . command . format ( app = self . app , outfile = outfile , * * options ) proc = subprocess . Popen ( cmd , shell = True , cwd = self . system . cwd , stdout = self . stdout , stdin = self . stdin , stderr = self . stderr ) result , err = proc . communicate ( ) if err and self . system . has jspm log ( ) : fmt = 'Could not bundle \\'%s\\': \\n%s' logger . warn ( fmt , self . app , err ) raise Bundle Error ( fmt % ( self . app , err ) ) if result . strip ( ) : logger . info ( result ) except ( IO Error , OS Error ) as e : if isinstance ( e , Bundle Error ) : raise raise Bundle Error ( 'Unable to apply %s (%r): %s' % ( self . class . name , cmd , e ) ) else : if not options . get ( 'sfx' ) : sourcemap = find sourcemap comment ( outfile ) with open ( outfile , 'a' ) as of : of . write ( \"\\n System.import('{app}{ext}');\\n{sourcemap}\" . format ( app = self . app , ext = '.js' if self . needs ext ( ) else '' , sourcemap = sourcemap if sourcemap else '' , ) ) return rel path", "predictions": ["returns a new object containing the reports and the commands for the given bundle ."], "references": ["bundle the app and return the static url to the bundle ."], "bleu": 0.13380161378318955, "rouge_l": 0.4535315985130111}
{"id": 3550, "code": "def parse docstring ( doc ) : doc = inspect . cleandoc ( doc ) lines = doc . split ( '\\n' ) section = None section indent = None params = { } returns = None for line in lines : line = line . rstrip ( ) if len ( line ) == 0 : continue elif str ( line ) == 'Args:' : section = 'args' section indent = None continue elif str ( line ) == 'Returns:' : section = 'return' section indent = None continue if section is not None : stripped = line . lstrip ( ) margin = len ( line ) - len ( stripped ) if section indent is None : section indent = margin if margin != section indent : continue if section == 'args' : param name , type info = parse param ( stripped ) params [ param name ] = type info elif section == 'return' : returns = parse return ( stripped ) return params , returns", "predictions": ["parse docstring into docstrings ."], "references": ["parse a docstring into parameterinfo and returninfo objects ."], "bleu": 0.18343778145675418, "rouge_l": 0.5434298440979956}
{"id": 3551, "code": "def split line ( self , line ) : parts = shlex . split ( line , posix = self . posix lex ) if not self . posix lex : parts = [ self . remove quotes ( x ) for x in parts ] return parts", "predictions": ["splits line at least one line ."], "references": ["split a line into arguments using shlex and a dequoting routine ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 3552, "code": "def builtin help ( self , args ) : if len ( args ) == 0 : return self . list dir ( self . contexts [ - 1 ] ) if len ( args ) == 1 : func = self . find function ( self . contexts [ - 1 ] , args [ 0 ] ) return annotate . get help ( func ) help text = \"Too many arguments: \" + str ( args ) + \"\\n\" help text += \"Usage: help [function]\" return help text", "predictions": ["get a dep for a dep ."], "references": ["return help information for a context or function ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 3553, "code": "def extract arg value ( cls , arg name , arg type , remaining ) : next arg = None should consume = False if len ( remaining ) > 0 : next arg = remaining [ 0 ] should consume = True if next arg == '--' : next arg = None if arg type == \"bool\" : if next arg is None or next arg . startswith ( '-' ) : next arg = True should consume = False else : if next arg is None : raise Argument Error ( \"Could not find value for keyword argument\" , argument = arg name ) if should consume : remaining . pop ( 0 ) return next arg", "predictions": ["get the field with the first fully qualified class ."], "references": ["try to find the value for a keyword argument ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 3554, "code": "def parse param ( param , include desc = False ) : param def , colon , desc = param . partition ( ':' ) if not include desc : desc = None else : desc = desc . lstrip ( ) if colon == \"\" : raise Validation Error ( \"Invalid parameter declaration in docstring, missing colon\" , declaration = param ) param name , space , param type = param def . partition ( ' ' ) if len ( param type ) < 2 or param type [ 0 ] != '(' or param type [ - 1 ] != ')' : raise Validation Error ( \"Invalid parameter type string not enclosed in ( ) characters\" , param string = param def , type string = param type ) param type = param type [ 1 : - 1 ] return param name , Parameter Info ( param type , [ ] , desc )", "predictions": ["convert the arguments in ( into a parameter object . this is a simple ( ( dep dep dep dep dep dep dep - ( dep dep - ( dep dep dep : ( - ( ( compatibility dep dep dep dep dep ( 2000000 dep dep dep dep dep"], "references": ["parse a single typed parameter statement ."], "bleu": 0.028577262451992175, "rouge_l": 0.1218375499334221}
{"id": 3555, "code": "def classify section ( cls , section ) : name = section . lower ( ) if name in frozenset ( [ 'args' , 'arguments' , \"params\" , \"parameters\" ] ) : return cls . ARGS SECTION if name in frozenset ( [ 'returns' , 'return' ] ) : return cls . RETURN SECTION if name in frozenset ( [ 'main' ] ) : return cls . MAIN SECTION return None", "predictions": ["trace an section ( . : name , section : . : . : . : ( : ( : ( : ( : ( : ( : ( : ( : ( . section . section ( : ( : ( : . : ( : ( : ("], "references": ["attempt to find the canonical name of this section ."], "bleu": 0.033984283835209204, "rouge_l": 0.11366459627329194}
{"id": 3556, "code": "def classify line ( cls , line ) : line = line . rstrip ( ) if len ( line ) == 0 : return Blank Line ( '' ) if ' ' not in line and line . endswith ( ':' ) : name = line [ : - 1 ] return Section Header ( name ) if line . startswith ( '  ' ) : return Continuation Line ( line . lstrip ( ) ) if line . startswith ( ' - ' ) : return List Item ( '-' , line [ 3 : ] . lstrip ( ) ) if line . startswith ( '- ' ) : return List Item ( '-' , line [ 2 : ] . lstrip ( ) ) return Line ( line )", "predictions": ["run an item from the specified file ."], "references": ["classify a line into a type of object ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3557, "code": "def join paragraphs ( cls , lines , use indent = False , leading blanks = False , trailing blanks = False ) : curr para = [ ] paragraphs = [ ] for line in lines : if use indent : if line . startswith ( ' ' ) : curr para . append ( line . lstrip ( ) ) continue elif line == '' : continue else : if len ( curr para ) > 0 : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) curr para = [ line . lstrip ( ) ] else : if len ( line ) != 0 : curr para . append ( line ) else : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) curr para = [ ] if len ( curr para ) > 0 : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) return paragraphs", "predictions": ["serializes all ( or more item sets item sets item to flatten = isinstance item ."], "references": ["join adjacent lines together into paragraphs using either a blank line or indent as separator ."], "bleu": 0.08513012360883544, "rouge_l": 0.125}
{"id": 3558, "code": "def split type ( self , typename ) : name = self . canonicalize type ( typename ) if '(' not in name : return name , False , [ ] base , sub = name . split ( '(' ) if len ( sub ) == 0 or sub [ - 1 ] != ')' : raise Argument Error ( \"syntax error in complex type, no matching ) found\" , passed type = typename , basetype = base , subtype string = sub ) sub = sub [ : - 1 ] subs = sub . split ( ',' ) return base , True , subs", "predictions": ["split table . split table : name = [ ljava ] sep table name sep table : ljava = base table sep = [ for ( = sys sep = base table sep table = [ ljava ] sep table = [ sys ] sep table name sep table ("], "references": ["given a potentially complex type split it into its base type and specializers"], "bleu": 0.026594139297659906, "rouge_l": 0.07101280558789291}
{"id": 3559, "code": "def instantiate type ( self , typename , base , subtypes ) : if base not in self . type factories : raise Argument Error ( \"unknown complex base type specified\" , passed type = typename , base type = base ) base type = self . type factories [ base ] #Make sure all of the subtypes are valid for sub type in subtypes : try : self . get type ( sub type ) except Key Value Exception as exc : raise Argument Error ( \"could not instantiate subtype for complex type\" , passed type = typename , sub type = sub type , error = exc ) typeobj = base type . Build ( * subtypes , type system = self ) self . inject type ( typename , typeobj )", "predictions": ["instantiate from ( . this method is called by the class and . when it is called ."], "references": ["instantiate a complex type ."], "bleu": 0.07535838128770536, "rouge_l": 0.19365079365079363}
{"id": 3560, "code": "def short description ( func ) : doc = inspect . getdoc ( func ) if doc is not None : doc = inspect . cleandoc ( doc ) lines = doc . splitlines ( ) return lines [ 0 ] return \"\"", "predictions": ["get the str for a docstring ."], "references": ["given an object with a docstring return the first line of the docstring"], "bleu": 0.1114789227233716, "rouge_l": 0.1897356143079316}
{"id": 3561, "code": "def load ( ) : autodiscover modules ( 'cron' ) if PROJECT MODULE : if '.' in PROJECT MODULE . name : try : import module ( '%s.cron' % '.' . join ( PROJECT MODULE . name . split ( '.' ) [ 0 : - 1 ] ) ) except Import Error as e : if 'No module named' not in str ( e ) : print ( e ) for cmd , app in get commands ( ) . items ( ) : try : load command class ( app , cmd ) except django . core . exceptions . Improperly Configured : pass", "predictions": ["trace all command files from the command ."], "references": ["load cron modules for applications listed in installed_apps ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3562, "code": "def install ( ) : load ( ) tab = crontab . Cron Tab ( user = True ) for task in registry : tab . new ( task . command , KRONOS BREADCRUMB ) . setall ( task . schedule ) tab . write ( ) return len ( registry )", "predictions": ["links all collapsed ' s rid of the , but do it to the entire thing ."], "references": ["register tasks with cron ."], "bleu": 0.07223943354597204, "rouge_l": 0.10082644628099173}
{"id": 3563, "code": "def uninstall ( ) : tab = crontab . Cron Tab ( user = True ) count = len ( list ( tab . find comment ( KRONOS BREADCRUMB ) ) ) tab . remove all ( comment = KRONOS BREADCRUMB ) tab . write ( ) return count", "predictions": ["removes all children from the ( and returns them ."], "references": ["uninstall tasks from cron ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 3564, "code": "def kind ( self ) : optics = [ Equality , Isomorphism , Prism , Review , Lens , Traversal , Getter , Setter , Fold , ] for optic in optics : if self . is kind ( optic ) : return optic", "predictions": ["get the + + + + + + spot components of this object ."], "references": ["returns a class representing the kind of optic ."], "bleu": 0.10511846841633776, "rouge_l": 0.271513353115727}
{"id": 3565, "code": "def play ( ) : ai = { 'X' : player move , 'O' : random move } board = Board ( ) while not board . winner : x , y = ai [ board . player ] ( board ) board = board . make move ( x , y ) print ( board , end = '\\n\\n' ) print ( board . winner )", "predictions": ["create the board . this must start at least one board ."], "references": ["play a game of naughts and crosses against the computer ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 3566, "code": "def winner ( self ) : for potential win in self . potential wins ( ) : if potential win == tuple ( 'XXX' ) : return Outcome . win for crosses elif potential win == tuple ( 'OOO' ) : return Outcome . win for naughts if self . count ( ' ' ) == 0 : return Outcome . draw return Outcome . ongoing", "predictions": ["as a np , i . e . , it will abs the bounds on each np ."], "references": ["the winner of this board if one exists ."], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 3567, "code": "def open spider ( self , spider ) : self . ts = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) . replace ( ':' , '-' )", "predictions": ["chunk the specified spider ."], "references": ["callback function when spider is open ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 3568, "code": "def upload chunk ( self , spider ) : if not self . items : return f = self . make fileobj ( ) object key = self . object key template . format ( * * self . get uri params ( spider ) ) try : self . s3 . upload fileobj ( f , self . bucket name , object key ) except Client Error : self . stats . inc value ( 'pipeline/s3/fail' ) raise else : self . stats . inc value ( 'pipeline/s3/success' ) finally : self . chunk number += len ( self . items ) self . items = [ ]", "predictions": ["uploads a waveform for this object ."], "references": ["do upload items to s3 ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3569, "code": "def make fileobj ( self ) : bio = Bytes IO ( ) f = gzip . Gzip File ( mode = 'wb' , fileobj = bio ) if self . use gzip else bio exporter = Json Lines Item Exporter ( f ) exporter . start exporting ( ) for item in self . items : exporter . export item ( item ) exporter . finish exporting ( ) if f is not bio : f . close ( ) bio . seek ( 0 ) return bio", "predictions": ["makes a , from the given list of items ."], "references": ["build file object from items ."], "bleu": 0.17827531042796255, "rouge_l": 0.3927038626609442}
{"id": 3570, "code": "def call ( self , method , params = None , request id = None ) : params = params or [ ] rid = request id or self . id counter if request id is None : self . id counter += 1 payload = { 'jsonrpc' : '2.0' , 'method' : method , 'params' : params , 'id' : rid } headers = { 'Content-Type' : 'application/json' } scheme = 'https' if self . tls else 'http' url = '{}://{}:{}' . format ( scheme , self . host , self . port ) try : response = self . session . post ( url , headers = headers , data = json . dumps ( payload ) ) response . raise for status ( ) except HTTP Error : raise Transport Error ( 'Got unsuccessful response from server (status code: {})' . format ( response . status code ) , response = response ) try : response data = response . json ( ) except Value Error as e : raise Protocol Error ( 'Unable to deserialize response body: {}' . format ( e ) , response = response ) if response data . get ( 'error' ) : code = response data [ 'error' ] . get ( 'code' , '' ) message = response data [ 'error' ] . get ( 'message' , '' ) raise Protocol Error ( 'Error[{}] {}' . format ( code , message ) , response = response , data = response data ) elif 'result' not in response data : raise Protocol Error ( 'Response is empty (result field is missing)' , response = response , data = response data ) return response data [ 'result' ]", "predictions": ["calls another operation . get an http call to the client and call back to the session method ."], "references": ["calls the json - rpc endpoint ."], "bleu": 0.07658412276041004, "rouge_l": 0.2517193947730399}
{"id": 3571, "code": "def is hash256 ( s ) : if not s or not isinstance ( s , str ) : return False return re . match ( '^[0-9A-F]{64}$' , s . strip ( ) , re . IGNORECASE )", "predictions": ["test if string argument is a string ."], "references": ["returns true if the considered string is a valid sha256 hash ."], "bleu": 0.15223083300988077, "rouge_l": 0.4825949367088607}
{"id": 3572, "code": "def is hash160 ( s ) : if not s or not isinstance ( s , str ) : return False if not len ( s ) == 40 : return False for c in s : if ( c < '0' or c > '9' ) and ( c < 'A' or c > 'F' ) and ( c < 'a' or c > 'f' ) : return False return True", "predictions": ["test if a string argument is a legal hex string ."], "references": ["returns true if the considered string is a valid ripemd160 hash ."], "bleu": 0.16278331364721524, "rouge_l": 0.4314002828854314}
{"id": 3573, "code": "def encode invocation params ( params ) : final params = [ ] for p in params : if isinstance ( p , bool ) : final params . append ( { 'type' : Contract Parameter Types . BOOLEAN . value , 'value' : p } ) elif isinstance ( p , int ) : final params . append ( { 'type' : Contract Parameter Types . INTEGER . value , 'value' : p } ) elif is hash256 ( p ) : final params . append ( { 'type' : Contract Parameter Types . HASH256 . value , 'value' : p } ) elif is hash160 ( p ) : final params . append ( { 'type' : Contract Parameter Types . HASH160 . value , 'value' : p } ) elif isinstance ( p , bytearray ) : final params . append ( { 'type' : Contract Parameter Types . BYTE ARRAY . value , 'value' : p } ) elif isinstance ( p , str ) : final params . append ( { 'type' : Contract Parameter Types . STRING . value , 'value' : p } ) elif isinstance ( p , list ) : innerp = encode invocation params ( p ) final params . append ( { 'type' : Contract Parameter Types . ARRAY . value , 'value' : innerp } ) return final params", "predictions": ["creates the parameters described in rfc 1952 ."], "references": ["returns a list of paramaters meant to be passed to json - rpc endpoints ."], "bleu": 0.06685045700482882, "rouge_l": 0.08243243243243244}
{"id": 3574, "code": "def decode invocation result ( result ) : if 'stack' not in result : return result result = copy . deepcopy ( result ) result [ 'stack' ] = decode invocation result stack ( result [ 'stack' ] ) return result", "predictions": ["render the value of the self ."], "references": ["tries to decode the values embedded in an invocation result dictionary ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 3575, "code": "def connect ( cls , settings ) : server = serializer ( 'json' ) . loads ( settings [ 'kvs.perlsess' ] ) server . setdefault ( 'key prefix' , 'perlsess::' ) server . setdefault ( 'codec' , 'storable' ) cls . cookie name = server . pop ( 'cookie name' , 'session id' ) cls . client = KVS ( * * server )", "predictions": ["creates a connection to the server and checks to connect to the given , and waits for the user to connect to the specified , and the client ."], "references": ["call that method in the pyramid configuration phase ."], "bleu": 0.0462136266712202, "rouge_l": 0.11630123927550047}
{"id": 3576, "code": "def basic ( username , password ) : none ( ) config . username = username config . password = password", "predictions": ["creates a new ( ."], "references": ["add basic authentication to the requests of the clients ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 3577, "code": "def api key ( api key ) : none ( ) config . api key prefix [ \"Authorization\" ] = \"api-key\" config . api key [ \"Authorization\" ] = \"key=\" + b64encode ( api key . encode ( ) ) . decode ( )", "predictions": ["encode an ( from the get ."], "references": ["authenticate via an api key ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3578, "code": "def get json content from folder ( folder ) : for dirpath , dirnames , filenames in os . walk ( folder ) : for filename in filenames : if filename . lower ( ) . endswith ( \".json\" ) : filepath = os . path . join ( dirpath , filename ) with open ( filepath , \"rb\" ) as file : yield json . loads ( file . read ( ) . decode ( \"UTF-8\" ) )", "predictions": ["returns generator for the expr ."], "references": ["yield objects from json files in the folder and subfolders ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3579, "code": "def get schema ( self ) : path = os . path . join ( self . get schema folder ( ) , self . name + \".json\" ) with open ( path , \"rb\" ) as file : schema = json . loads ( file . read ( ) . decode ( \"UTF-8\" ) ) return schema", "predictions": ["builds and returns the consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive consecutive tokens ."], "references": ["return the schema ."], "bleu": 0.0405185766962521, "rouge_l": 0.1259029927760578}
{"id": 3580, "code": "def get valid examples ( self ) : path = os . path . join ( self . get schema folder ( ) , \"examples\" , \"valid\" ) return list ( get json content from folder ( path ) )", "predictions": ["determine the ( use a ext ( name : . : . : . : . : . : . : . : . : . : . : . : . . . : . . . self . . self : . self . . . . ."], "references": ["return a list of valid examples for the given schema ."], "bleu": 0.028577262451992175, "rouge_l": 0.07411907654921021}
{"id": 3581, "code": "def get invalid examples ( self ) : path = os . path . join ( self . get schema folder ( ) , \"examples\" , \"invalid\" ) return list ( get json content from folder ( path ) )", "predictions": ["gets the list self for this rel ."], "references": ["return a list of examples which violate the schema ."], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 3582, "code": "def auth user get url ( self , scope = None ) : if not self . client id : raise Auth Missing Error ( 'No client id specified' ) return '{}?{}' . format ( self . auth url user , urllib . urlencode ( dict ( client id = self . client id , scope = ' ' . join ( scope or self . auth scope ) , response type = 'code' , redirect uri = self . auth redirect uri ) ) )", "predictions": ["retrieves the parse authentication token for the docstring ."], "references": ["build authorization url for user agent ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 3583, "code": "def auth user process url ( self , url ) : url = urlparse . urlparse ( url ) url qs = dict ( it . chain . from iterable ( urlparse . parse qsl ( v ) for v in [ url . query , url . fragment ] ) ) if url qs . get ( 'error' ) : raise API Auth Error ( '{} :: {}' . format ( url qs [ 'error' ] , url qs . get ( 'error description' ) ) ) self . auth code = url qs [ 'code' ] return self . auth code", "predictions": ["do the actual ( which does not try to reset the connection ."], "references": ["process tokens and errors from redirect_uri ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 3584, "code": "def auth get token ( self , check scope = True ) : res = self . auth access data raw = self . auth token request ( ) return self . auth token process ( res , check scope = check scope )", "predictions": ["get the auth token ."], "references": ["refresh or acquire access_token ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3585, "code": "def get user id ( self ) : if self . user id is None : self . user id = self . get user data ( ) [ 'id' ] return self . user id", "predictions": ["get the user for the user ."], "references": ["returns id of a onedrive user ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 3586, "code": "def listdir ( self , folder id = 'me/skydrive' , limit = None , offset = None ) : return self ( self . api url join ( folder id , 'files' ) , dict ( limit = limit , offset = offset ) )", "predictions": ["get data to the local folder"], "references": ["get onedrive object representing list of objects in a folder ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3587, "code": "def comment add ( self , obj id , message ) : return self ( self . api url join ( obj id , 'comments' ) , method = 'post' , data = dict ( message = message ) , auth header = True )", "predictions": ["adds the parameter to this entry ."], "references": ["add comment message to a specified object ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3588, "code": "def decode obj ( obj , force = False ) : if isinstance ( obj , unicode ) : return obj elif isinstance ( obj , bytes ) : if force encoding is not None : return obj . decode ( force encoding ) if chardet : enc guess = chardet . detect ( obj ) if enc guess [ 'confidence' ] > 0.7 : return obj . decode ( enc guess [ 'encoding' ] ) return obj . decode ( 'utf-8' ) else : return obj if not force else repr ( obj )", "predictions": ["the ( is required to serialize a unicode object . if the object cannot be converted to its proper representation , this method can be used to serialize the caller ."], "references": ["convert or dump object to unicode ."], "bleu": 0.04906081629292276, "rouge_l": 0.17818889970788704}
{"id": 3589, "code": "def set drop target ( obj , root , designer , inspector ) : if obj . meta . container : dt = Tool Box Drop Target ( obj , root , designer = designer , inspector = inspector ) obj . drop target = dt for child in obj : set drop target ( child , root , designer , inspector )", "predictions": ["create a new object from either a drop - known object or a drop - up database ."], "references": ["recursively create and set the drop target for obj and childs"], "bleu": 0.07535838128770536, "rouge_l": 0.14420803782505912}
{"id": 3590, "code": "def start drag opperation ( self , evt ) : ctrl = self . menu ctrl map [ evt . Get Tool Id ( ) ] ldata = wx . Custom Data Object ( \"gui\" ) ldata . Set Data ( ctrl . meta . name ) bmp = ctrl . image . Get Bitmap ( ) bdata = wx . Bitmap Data Object ( bmp ) data = wx . Data Object Composite ( ) data . Add ( ldata ) data . Add ( bdata ) drop Source = wx . Drop Source ( self ) drop Source . Set Data ( data ) if DEBUG : print ( \"Begining Drag Drop\\n\" ) result = drop Source . Do Drag Drop ( wx . Drag Allow Move ) if DEBUG : print ( \"Drag Drop completed: %d\\n\" % result ) if result == wx . Drag Move : if DEBUG : print \"dragmove!\" self . Refresh ( )", "predictions": ["starts the drag note : this must be called only once the wx has been started ."], "references": ["event handler for drag&drop functionality"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 3591, "code": "def set default tlw ( self , tlw , designer , inspector ) : self . designer = designer self . inspector = inspector", "predictions": ["sets the default list for this object ."], "references": ["track default top level window for toolbox menu default action"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3592, "code": "def inspect ( obj ) : from gui . tools . inspector import Inspector Tool inspector = Inspector Tool ( ) inspector . show ( obj ) return inspector", "predictions": ["collect the existing object on the remote machine ."], "references": ["open the inspector windows for a given object"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 3593, "code": "def migrate window ( bg ) : ret = { } for k , v in bg . items ( ) : if k == 'type' : v = WIN MAP [ v ] . meta . name elif k == 'menubar' : menus = v [ 'menus' ] v = [ migrate control ( menu ) for menu in menus ] elif k == 'components' : v = [ migrate control ( comp ) for comp in v ] else : k = SPEC MAP [ 'Widget' ] . get ( k , k ) ret [ k ] = v return ret", "predictions": ["migrate the transformation to . ."], "references": ["take a pythoncard background resource and convert to a gui2py window"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 3594, "code": "def migrate control ( comp ) : ret = { } for k , v in comp . items ( ) : if k == 'type' : v = CTRL MAP [ v ] . meta . name elif k == 'menubar' : pass elif k == 'components' : v = [ migrate control ( comp ) for comp in v ] else : k = SPEC MAP [ 'Widget' ] . get ( k , k ) if comp [ 'type' ] in SPEC MAP : k = SPEC MAP [ comp [ 'type' ] ] . get ( k , k ) if k == 'font' : v = migrate font ( v ) ret [ k ] = v return ret", "predictions": ["migrate tuples of t t and return a two soap polygons"], "references": ["take a pythoncard background resource and convert to a gui2py window"], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 3595, "code": "def migrate font ( font ) : if 'face Name' in font : font [ 'face' ] = font . pop ( 'face Name' ) if 'family' in font and font [ 'family' ] == 'sans Serif' : font [ 'family' ] = 'sans serif' return font", "predictions": ["migrate spaces to the correct font ."], "references": ["convert pythoncard font description to gui2py style"], "bleu": 0.20556680845025982, "rouge_l": 0.14285714285714285}
{"id": 3596, "code": "def load page ( self , location ) : if not location : self . wx obj . Set Page ( \"\" ) else : self . wx obj . Load Page ( location )", "predictions": ["loads the translation object from the given location ."], "references": ["loads html page from location and then displays it"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 3597, "code": "def Get Param ( tag , param , default = SENTINEL ) : if tag . Has Param ( param ) : return tag . Get Param ( param ) else : if default == SENTINEL : raise Key Error else : return default", "predictions": ["find parameter parameter value ."], "references": ["convenience function for accessing tag parameters"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 3598, "code": "def send ( evt ) : msg = ctrl input . value gui . alert ( msg , \"Message\" ) log ( msg ) ctrl input . value = \"\" ctrl input . set focus ( )", "predictions": ["sends an . to the specified message ."], "references": ["process an outgoing communication"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3599, "code": "def wellcome tip ( wx obj ) : msg = ( \"Close the main window to exit & save.\\n\" \"Drag & Drop / Click the controls from the Tool Box to create new ones.\\n\" \"Left click on the created controls to select them.\\n\" \"Double click to edit the default property.\\n\" \"Right click to pop-up the context menu.\\n\" ) stt = STT . Super Tool Tip ( msg ) stt . Set Header ( \"Welcome to gui2py designer!\" ) stt . Set Draw Header Line ( True ) stt . Apply Style ( \"Office 2007 Blue\" ) stt . Set Drop Shadow ( True ) stt . Set Header Bitmap ( images . designer . Get Bitmap ( ) ) stt . Set End Delay ( 15000 ) tip = Custom Tool Tip Window ( wx obj , stt ) tip . Calculate Best Size ( ) tip . Calculate Best Position ( wx obj ) tip . Drop Shadow ( stt . Get Drop Shadow ( ) ) if stt . Get Use Fade ( ) : show = lambda : tip . Start Alpha ( True ) else : show = lambda : tip . Show ( ) wx . Call Later ( 1000 , show ) wx . Call Later ( 30000 , tip . Destroy )", "predictions": ["creates an tip to show the images in the wx ."], "references": ["show a tip message"], "bleu": 0.12605968092174913, "rouge_l": 0.14558472553699284}
{"id": 3600, "code": "def mouse down ( self , evt ) : if DEBUG : print \"down!\" if ( not evt . Control Down ( ) and not evt . Shift Down ( ) ) or evt . Alt Down ( ) : for obj in self . selection : if obj . sel marker : obj . sel marker . show ( False ) obj . sel marker . destroy ( ) obj . sel marker = None self . selection = [ ] wx obj = evt . Get Event Object ( ) if wx obj . Parent is None or evt . Alt Down ( ) : if not evt . Alt Down ( ) : evt . Skip ( ) self . current = wx obj self . overlay = wx . Overlay ( ) self . pos = evt . Get Position ( ) self . parent . wx obj . Capture Mouse ( ) #if self.inspector and hasattr(wx obj, \"obj\"): #self.dclick = False else : obj = wx obj . obj self . overlay = None if DEBUG : print wx obj sx , sy = wx obj . Screen To Client ( wx obj . Get Position Tuple ( ) ) dx , dy = wx obj . Screen To Client ( wx . Get Mouse Position ( ) ) self . pos = wx obj . Screen To Client ( wx . Get Mouse Position ( ) ) self . start = ( sx - dx , sy - dy ) self . current = wx obj if DEBUG : print \"capture...\" if not isinstance ( wx obj , wx . Notebook ) : self . parent . wx obj . Capture Mouse ( ) self . select ( obj , keep selection = True )", "predictions": ["forward a mouse event to the wx object ."], "references": ["get the selected object and store start position"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 3601, "code": "def mouse move ( self , evt ) : if DEBUG : print \"move!\" if self . current and not self . overlay : wx obj = self . current sx , sy = self . start x , y = wx . Get Mouse Position ( ) x , y = ( x + sx , y + sy ) if evt . Shift Down ( ) : x = x / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] y = y / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] ox , oy = wx obj . obj . pos dx , dy = ( x - ox ) , ( y - oy ) for obj in self . selection : x , y = obj . pos x = x + dx y = y + dy obj . pos = ( wx . Point ( x , y ) ) elif self . overlay : wx obj = self . current pos = evt . Get Position ( ) if evt . Get Event Object ( ) != wx obj : pos = evt . Get Event Object ( ) . Client To Screen ( pos ) pos = wx obj . Screen To Client ( pos ) rect = wx . Rect PP ( self . pos , pos ) dc = wx . Client DC ( wx obj ) odc = wx . DC Overlay ( self . overlay , dc ) odc . Clear ( ) dc . Set Pen ( wx . Pen ( \"blue\" , 2 ) ) if 'wx Mac' in wx . Platform Info : dc . Set Brush ( wx . Brush ( wx . Colour ( 0x C0 , 0x C0 , 0x C0 , 0x80 ) ) ) else : dc . Set Brush ( wx . TRANSPARENT BRUSH ) dc . Draw Rectangle Rect ( rect ) del odc", "predictions": ["move to the next position of this polyline"], "references": ["move the selected object"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 3602, "code": "def key press ( self , event ) : key = event . Get Key Code ( ) if key in ( wx . WXK LEFT , wx . WXK UP , wx . WXK RIGHT , wx . WXK DOWN ) : for obj in self . selection : x , y = obj . pos if event . Shift Down ( ) : if key == wx . WXK LEFT : x = ( x - GRID SIZE [ 0 ] ) / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] elif key == wx . WXK RIGHT : x = ( x + GRID SIZE [ 0 ] ) / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] elif key == wx . WXK UP : y = ( y - GRID SIZE [ 1 ] ) / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] elif key == wx . WXK DOWN : y = ( y + GRID SIZE [ 1 ] ) / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] else : if key == wx . WXK LEFT : x = x - 1 elif key == wx . WXK RIGHT : x = x + 1 elif key == wx . WXK UP : y = y - 1 elif key == wx . WXK DOWN : y = y + 1 obj . pos = ( x , y ) elif key == wx . WXK DELETE : self . delete ( event ) elif key == wx . WXK INSERT : self . duplicate ( event ) else : if DEBUG : print \"KEY:\" , key", "predictions": ["we have to do this method in a wx ."], "references": ["support cursor keys to move components one pixel at a time"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 3603, "code": "def delete ( self , event ) : for obj in self . selection : if obj : if DEBUG : print \"deleting\" , obj . name obj . destroy ( ) self . selection = [ ] self . inspector . load object ( )", "predictions": ["deletes an object including its contents ."], "references": ["delete all of the selected objects"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3604, "code": "def duplicate ( self , event ) : new selection = [ ] for obj in self . selection : if obj : if DEBUG : print \"duplicating\" , obj . name obj . sel marker . destroy ( ) obj . sel marker = None obj2 = obj . duplicate ( ) obj2 . sel marker = Selection Marker ( obj2 ) obj2 . sel marker . show ( True ) new selection . append ( obj2 ) self . selection = new selection self . inspector . load object ( )", "predictions": ["duplicate marker object . this is called when we have been created and the selection of the controller ."], "references": ["create a copy of each selected object"], "bleu": 0.0712695567709093, "rouge_l": 0.08390646492434663}
{"id": 3605, "code": "def refresh ( self ) : self . bmp = self . obj . snapshot ( ) self . Raise ( ) self . Show ( ) self . Refresh ( )", "predictions": ["creates a new instance of this class ."], "references": ["capture the new control superficial image after an update"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3606, "code": "def Calculate Best Position ( self , widget ) : if isinstance ( widget , wx . Frame ) : screen = wx . Client Display Rect ( ) [ 2 : ] left , top = widget . Client To Screen XY ( 0 , 0 ) right , bottom = widget . Client To Screen XY ( * widget . Get Client Rect ( ) [ 2 : ] ) size = self . Get Size ( ) xpos = right ypos = bottom - size [ 1 ] self . Set Position ( ( xpos , ypos ) ) else : STT . Tool Tip Window . Calculate Best Position ( self , widget )", "predictions": ["if this widget is requested , create a wx to the specified rectangle ."], "references": ["when dealing with a top - level window position it absolute lower - right"], "bleu": 0.08839374326825923, "rouge_l": 0.07142857142857142}
{"id": 3607, "code": "def Get Py Data ( self , item ) : wx data = self . Get Item Data ( item ) py data = self . py data map . get ( wx data ) return py data", "predictions": ["this method will get the wx item of the wx object ."], "references": ["returns the pyth item data associated with the item"], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 3608, "code": "def Set Py Data ( self , item , py data ) : wx data = wx . New Id ( ) self . Set Item Data ( item , wx data ) self . py data map [ wx data ] = py data self . wx data map [ py data ] = wx data return wx data", "predictions": ["generic method for ."], "references": ["set the python item data associated wit the wx item"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 3609, "code": "def Find Py Data ( self , start , py data ) : wx data = self . wx data map [ py data ] if wx . VERSION < ( 3 , 0 , 0 ) or 'classic' in wx . version ( ) : data = self . Find Item Data ( start , wx data ) else : data = self . Find Item ( start , wx data ) return data", "predictions": ["performs a wx offer for the specified data"], "references": ["do a reverse look up for an item containing the requested data"], "bleu": 0.1223065774797558, "rouge_l": 0.3860759493670886}
{"id": 3610, "code": "def Delete Item ( self , item ) : wx data = self . Get Item Data ( item ) py data = self . py data map [ wx data ] del self . py data map [ wx data ] del self . wx data map [ py data ] wx . List Ctrl . Delete Item ( self , item )", "predictions": ["remove a wx item ."], "references": ["remove the item from the list and unset the related data"], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 3611, "code": "def Delete All Items ( self ) : self . py data map . clear ( ) self . wx data map . clear ( ) wx . List Ctrl . Delete All Items ( self )", "predictions": ["removes all contents of a cookie from the list ."], "references": ["remove all the item from the list and unset the related data"], "bleu": 0.20311412297014433, "rouge_l": 0.3577712609970674}
{"id": 3612, "code": "def delete ( self , a position ) : key = self . wx obj . Get Py Data ( a position ) del self . items [ key ]", "predictions": ["deletes a wx object at the specified position ."], "references": ["deletes the item at the zero - based index n from the control ."], "bleu": 0.12109261383365659, "rouge_l": 0.3347050754458162}
{"id": 3613, "code": "def clear all ( self ) : self . clear ( ) for ch in reversed ( self . columns ) : del self [ ch . name ]", "predictions": ["clear the list of bits ."], "references": ["remove all items and column headings"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3614, "code": "def clear ( self ) : dict . clear ( self ) self . key = 0 if hasattr ( self . list view , \"wx obj\" ) : self . list view . wx obj . Delete All Items ( )", "predictions": ["removes all items from the ( ."], "references": ["remove all items and reset internal structures"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 3615, "code": "def set selection ( self , index , dummy = False ) : if index is None : self . wx obj . Set Selection ( - 1 ) if hasattr ( self . wx obj , \"Set Value\" ) : self . wx obj . Set Value ( \"\" ) else : self . wx obj . Set Selection ( index ) wx event = Item Container Control Select Event ( self . commandtype , index , self . wx obj ) if hasattr ( self , \"onchange\" ) and self . onchange : event = Form Event ( name = \"change\" , wx event = wx event ) self . onchange ( event )", "predictions": ["helper method for handling item changed . add this to the wx ."], "references": ["sets the item at index n to be the selected item ."], "bleu": 0.12011055432195765, "rouge_l": 0.3223249669749009}
{"id": 3616, "code": "def get string selection ( self ) : if self . multiselect : return [ self . wx obj . Get String ( i ) for i in self . wx obj . Get Selections ( ) ] else : return self . wx obj . Get String Selection ( )", "predictions": ["auth method for creating a res object ."], "references": ["returns the label of the selected item or an empty string if none"], "bleu": 0.0721806023765632, "rouge_l": 0.0}
{"id": 3617, "code": "def set data ( self , n , data ) : self . wx obj . Set Client Data ( n , data ) self . items dict [ data ] = self . get string ( n )", "predictions": ["sets the is available for this instance ."], "references": ["associate the given client data with the item at position n ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3618, "code": "def append ( self , a string , data = None ) : self . wx obj . Append ( a string , data ) self . items dict [ data ] = a string", "predictions": ["appends the text representation of the : this method is called multiple times to the end of the list ."], "references": ["adds the item to the control associating the given data if not none ."], "bleu": 0.09560408787521255, "rouge_l": 0.30378486055776893}
{"id": 3619, "code": "def delete ( self , a position ) : self . wx obj . Delete ( a position ) data = self . get data ( ) if data in self . items dict : del self . items dict [ data ]", "predictions": ["comment to the : deletes the underlying database and removes the specified obj ."], "references": ["deletes the item at the zero - based index n from the control ."], "bleu": 0.13834368456410945, "rouge_l": 0.2857142857142857}
{"id": 3620, "code": "def represent ( obj , prefix , parent = \"\" , indent = 0 , context = False , max cols = 80 ) : try : name = getattr ( obj , \"name\" , \"\" ) class name = \"%s.%s\" % ( prefix , obj . class . name ) padding = len ( class name ) + 1 + indent * 4 + ( 5 if context else 0 ) params = [ ] for ( k , spec ) in sorted ( obj . meta . specs . items ( ) , key = get sort key ) : if k == \"index\" : continue if k == \"parent\" and parent != \"\" : v = parent else : v = getattr ( obj , k , \"\" ) if ( not isinstance ( spec , Internal Spec ) and v != spec . default and ( k != 'id' or v > 0 ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : v = repr ( v ) else : v = None if v is not None : params . append ( \"%s=%s\" % ( k , v ) ) param lines = [ ] line = \"\" for param in params : if len ( line + param ) + 3 > max cols - padding : param lines . append ( line ) line = \"\" line += param + \", \" param lines . append ( line ) param str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param lines ) return \"%s(%s)\" % ( class name , param str ) except : raise return object . repr ( obj )", "predictions": ["format object as a parameter ."], "references": ["construct a string representing the object"], "bleu": 0.24446151121745047, "rouge_l": 0.16666666666666666}
{"id": 3621, "code": "def get ( obj name , init = False ) : wx parent = None if isinstance ( obj name , basestring ) : obj parent = COMPONENTS . get ( obj name ) if not obj parent : wx parent = wx . Find Window By Name ( obj name ) if wx parent : obj parent = getattr ( wx parent , \"obj\" ) else : for obj in COMPONENTS . values ( ) : if obj . name == obj name : obj parent = obj else : obj parent = obj name return obj parent or wx parent", "predictions": ["gets a : variable or a : variable ."], "references": ["find an object already created"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3622, "code": "def duplicate ( self , new parent = None ) : kwargs = { } for spec name , spec in self . meta . specs . items ( ) : value = getattr ( self , spec name ) if isinstance ( value , Color ) : print \"COLOR\" , value , value . default if value . default : value = None if value is not None : kwargs [ spec name ] = value del kwargs [ 'parent' ] new id = wx . New Id ( ) kwargs [ 'id' ] = new id kwargs [ 'name' ] = \"%s %s\" % ( kwargs [ 'name' ] , new id ) new obj = self . class ( new parent or self . get parent ( ) , * * kwargs ) for child in self : child . duplicate ( new obj ) return new obj", "predictions": ["handles the start and end date"], "references": ["create a new object exactly similar to self"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 3623, "code": "def sizer add ( self , child ) : if self . sizer : if DEBUG : print \"adding to sizer:\" , child . name border = None if not border : border = child . sizer border flags = child . sizer flags if child . sizer align : flags |= child . sizer align if child . sizer expand : flags |= wx . EXPAND if 'grid' in self . sizer : self . sizer . Add ( child . wx obj , flag = flags , border = border , pos = ( child . sizer row , child . sizer col ) , span = ( child . sizer rowspan , child . sizer colspan ) ) else : self . sizer . Add ( child . wx obj , 0 , flags , border )", "predictions": ["set a wx . the wx will not be set to a wx ."], "references": ["called when adding a control to the window"], "bleu": 0.10511846841633776, "rouge_l": 0.19122257053291536}
{"id": 3624, "code": "def set parent ( self , new parent , init = False ) : Component . set parent ( self , new parent , init ) if not init : if DEBUG : print \"reparenting\" , ctrl . name if hasattr ( self . wx obj , \"Reparent\" ) : self . wx obj . Reparent ( self . parent . wx obj )", "predictions": ["creates new wx dialog ."], "references": ["re - parent a child control with the new wx_obj parent"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3625, "code": "def resize ( self , evt = None ) : if DEBUG : print \"RESIZE!\" , self . name , self . width , self . height if not isinstance ( self . wx obj , wx . Top Level Window ) : if self . left and self . left [ - 1 ] == \"%\" or self . top and self . top [ - 1 ] == \"%\" : if DEBUG : print \"MOVING\" , self . name , self . width self . set pos ( ( self . left , self . top ) ) if self . width and self . width [ - 1 ] == \"%\" or self . height and self . height [ - 1 ] == \"%\" : if DEBUG : print \"RESIZING\" , self . name , self . width , self . height self . set size ( ( self . width , self . height ) ) for child in self : if isinstance ( child , Control ) : child . resize ( evt ) if evt : evt . Skip ( )", "predictions": ["migrate from application to wx . this method is called by the java class controller ."], "references": ["automatically adjust relative pos and size of children controls"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 3626, "code": "def tile background ( self , dc ) : sz = self . wx obj . Get Client Size ( ) bmp = self . bitmap . get bits ( ) w = bmp . Get Width ( ) h = bmp . Get Height ( ) if isinstance ( self , wx . Scrolled Window ) : spx , spy = self . wx obj . Get Scroll Pixels Per Unit ( ) vsx , vsy = self . wx obj . Get View Start ( ) dx , dy = ( spx * vsx ) % w , ( spy * vsy ) % h else : dx , dy = ( w , h ) x = - dx while x < sz . width : y = - dy while y < sz . height : dc . Draw Bitmap ( bmp , x , y ) y = y + h x = x + w", "predictions": ["creates a new instance of the ( ."], "references": ["make several copies of the background bitmap"], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 3627, "code": "def on erase background ( self , evt ) : if self . bitmap : dc = evt . Get DC ( ) if not dc : dc = wx . Client DC ( self ) r = self . wx obj . Get Update Region ( ) . Get Box ( ) dc . Set Clipping Region ( r . x , r . y , r . width , r . height ) if self . background tiling : self . tile background ( dc ) else : dc . Draw Bitmap Point ( self . bitmap . get bits ( ) , ( 0 , 0 ) )", "predictions": ["call this to run when a == task is received"], "references": ["draw the image as background"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3628, "code": "def on paint ( self , event ) : dc = wx . GCDC ( wx . Paint DC ( self . wx obj ) ) dc . Set Font ( self . wx obj . Get Font ( ) ) dc . Set Text Foreground ( self . wx obj . Get Foreground Colour ( ) ) dc . Draw Text ( self . wx obj . Get Label ( ) , 0 , 0 )", "predictions": ["paints a ."], "references": ["custom draws the label when transparent background is needed"], "bleu": 0.06114461654585454, "rouge_l": 0.0}
{"id": 3629, "code": "def get column headings ( self ) : headers = [ ctrl for ctrl in self if isinstance ( ctrl , Grid Column ) ] return sorted ( headers , key = lambda ch : ch . index )", "predictions": ["retrieves all column objects for this node ."], "references": ["return a list of children sub - components that are column headings"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 3630, "code": "def Reset View ( self , grid ) : grid . Begin Batch ( ) for current , new , delmsg , addmsg in [ ( self . rows , self . Get Number Rows ( ) , gridlib . GRIDTABLE NOTIFY ROWS DELETED , gridlib . GRIDTABLE NOTIFY ROWS APPENDED ) , ( self . cols , self . Get Number Cols ( ) , gridlib . GRIDTABLE NOTIFY COLS DELETED , gridlib . GRIDTABLE NOTIFY COLS APPENDED ) , ] : if new < current : msg = gridlib . Grid Table Message ( self , delmsg , new , current - new ) grid . Process Table Message ( msg ) elif new > current : msg = gridlib . Grid Table Message ( self , addmsg , new - current ) grid . Process Table Message ( msg ) self . Update Values ( grid ) grid . End Batch ( ) self . rows = self . Get Number Rows ( ) self . cols = self . Get Number Cols ( ) self . update Col Attrs ( grid ) grid . Adjust Scrollbars ( ) grid . Force Refresh ( )", "predictions": ["creates and returns a view that can be passed to the application ' s msg ."], "references": ["update the grid if rows and columns have been added or deleted"], "bleu": 0.08513012360883544, "rouge_l": 0.07331730769230768}
{"id": 3631, "code": "def Update Values ( self , grid ) : msg = gridlib . Grid Table Message ( self , gridlib . GRIDTABLE REQUEST VIEW GET VALUES ) grid . Process Table Message ( msg )", "predictions": ["a . to get a formatted message from the ) and the ) ."], "references": ["update all displayed values"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 3632, "code": "def update Col Attrs ( self , grid ) : col = 0 for column in self . columns : attr = gridlib . Grid Cell Attr ( ) if False : attr . Set Read Only ( ) if False : attr . Set Renderer ( renderer ) grid . Set Col Size ( col , column . width ) grid . Set Col Attr ( col , attr ) col += 1", "predictions": ["compute new width based on ) values ."], "references": ["update the column attributes to add the appropriate renderer"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 3633, "code": "def clear ( self ) : for i in range ( len ( self ) - 1 , - 1 , - 1 ) : del self [ i ] self . key = 0 if hasattr ( self . grid view , \"wx obj\" ) : self . grid view . wx obj . Clear Grid ( )", "predictions": ["clears accumulated state of the accumulated map ."], "references": ["remove all rows and reset internal structures"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3634, "code": "def Create ( self , parent , id , evt Handler ) : self . tc = wx . Combo Box ( parent , id , \"\" , ( 100 , 50 ) ) self . Set Control ( self . tc ) self . tc . Push Event Handler ( wx . Evt Handler ( ) ) self . tc . Bind ( wx . EVT COMBOBOX , self . On Change )", "predictions": ["creates a if a if a if the if there is a if the if there is a if the if the if it exists a if there is a if the if the if there is a if the if there is a if it exists ."], "references": ["called to create the control which must derive from wxcontrol ."], "bleu": 0.02771450089816765, "rouge_l": 0.07644110275689223}
{"id": 3635, "code": "def Begin Edit ( self , row , col , grid ) : self . start Value = grid . Get Table ( ) . Get Value ( row , col ) choices = grid . Get Table ( ) . columns [ col ] . choices self . tc . Clear ( ) self . tc . Append Items ( choices ) self . tc . Set String Selection ( self . start Value ) self . tc . Set Focus ( )", "predictions": ["create a new action ."], "references": ["fetch the value from the table and prepare the edit control"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 3636, "code": "def End Edit ( self , row , col , grid , val = None ) : changed = False val = self . tc . Get String Selection ( ) print \"val\" , val , row , col , self . start Value if val != self . start Value : changed = True grid . Get Table ( ) . Set Value ( row , col , val ) self . start Value = '' self . tc . Set String Selection ( '' ) return changed", "predictions": ["method to handle get the end of the command string in a newly created container"], "references": ["complete the editing of the current cell . returns true if changed"], "bleu": 0.11633270842295028, "rouge_l": 0.22676579925650556}
{"id": 3637, "code": "def Is Accepted Key ( self , evt ) : return ( not ( evt . Control Down ( ) or evt . Alt Down ( ) ) and evt . Get Key Code ( ) != wx . WXK SHIFT )", "predictions": ["this method checks if a wx is disabled ."], "references": ["return true to allow the given key to start editing"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 3638, "code": "def Starting Key ( self , evt ) : key = evt . Get Key Code ( ) ch = None if key in [ wx . WXK NUMPAD0 , wx . WXK NUMPAD1 , wx . WXK NUMPAD2 , wx . WXK NUMPAD3 , wx . WXK NUMPAD4 , wx . WXK NUMPAD5 , wx . WXK NUMPAD6 , wx . WXK NUMPAD7 , wx . WXK NUMPAD8 , wx . WXK NUMPAD9 ] : ch = ch = chr ( ord ( '0' ) + key - wx . WXK NUMPAD0 ) elif key < 256 and key >= 0 and chr ( key ) in string . printable : ch = chr ( key ) if not evt . Shift Down ( ) : ch = ch . lower ( ) if ch is not None : self . tc . Set String Selection ( ch ) else : evt . Skip ( )", "predictions": ["a version of . interface ."], "references": ["this will be called to let the editor do something with the first key"], "bleu": 0.048963321289052536, "rouge_l": 0.0}
{"id": 3639, "code": "def Enable ( self , value ) : for i in range ( self . Get Menu Item Count ( ) ) : it = self . Find Item By Position ( i ) it . Enable ( value )", "predictions": ["enable this list of items whose content is this list ."], "references": ["enable or disable all menu items"], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 3640, "code": "def Is Enabled ( self , * args , * * kwargs ) : for i in range ( self . Get Menu Item Count ( ) ) : it = self . Find Item By Position ( i ) if not it . Is Enabled ( ) : return False return True", "predictions": ["determines if each of the menu item is enabled ."], "references": ["check if all menu items are enabled"], "bleu": 0.14991106946711685, "rouge_l": 0.36454183266932266}
{"id": 3641, "code": "def Enable ( self , value ) : for i in range ( self . Get Menu Count ( ) ) : self . Enable Top ( i , value )", "predictions": ["creates an unconnected for each variable in the specified container ."], "references": ["enable or disable all top menus"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3642, "code": "def Is Enabled ( self , * args , * * kwargs ) : for i in range ( self . Get Menu Count ( ) ) : if not self . Is Enabled Top ( i ) : return False return True", "predictions": ["determines if this method is enabled for the given node ."], "references": ["check if all top menus are enabled"], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 3643, "code": "def Remove Item ( self , menu ) : menus = self . Get Menus ( ) menus = [ submenu for submenu in menus if submenu [ 0 ] != menu ] self . Set Menus ( menus )", "predictions": ["removes a specific list of item from this list ."], "references": ["helper method to remove a menu avoiding using its position"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 3644, "code": "def set Object Tag ( self , object , tag ) : object . attributes = { } object . name = tag . Get Name ( ) . lower ( ) for name in self . attributes : object . attributes [ \" %s\" % name ] = tag . Get Param ( name ) if object . attributes [ \" %s\" % name ] == \"\" : object . attributes [ \" %s\" % name ] = None", "predictions": ["convenience method that creates a repository for all the benchmarks ."], "references": ["add a tag attribute to the wx window"], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 3645, "code": "def autosummary table visit html ( self , node ) : try : tbody = node [ 0 ] [ 0 ] [ - 1 ] for row in tbody : col1 entry = row [ 0 ] par = col1 entry [ 0 ] for j , subnode in enumerate ( list ( par ) ) : if isinstance ( subnode , nodes . Text ) : new text = unicode ( subnode . astext ( ) ) new text = new text . replace ( u\" \" , u\"\\u00a0\" ) par [ j ] = nodes . Text ( new text ) except Index Error : pass", "predictions": ["this method does the first do not make the popup from all the nodes ."], "references": ["make the first column of the table non - breaking ."], "bleu": 0.14247788801610148, "rouge_l": 0.3164721141374838}
{"id": 3646, "code": "def mangle signature ( sig , max chars = 30 ) : s = re . sub ( r\"^\\((.*)\\)$\" , r\"\\1\" , sig ) . strip ( ) s = re . sub ( r\"\\\\\\\\\" , \"\" , s ) s = re . sub ( r\"\\\\'\" , \"\" , s ) s = re . sub ( r\"'[^']*'\" , \"\" , s ) args = [ ] opts = [ ] opt re = re . compile ( r\"^(.*, |)([a-z A-Z0-9 *]+)=\" ) while s : m = opt re . search ( s ) if not m : args = s . split ( ', ' ) break opts . insert ( 0 , m . group ( 2 ) ) s = m . group ( 1 ) [ : - 2 ] sig = limited join ( \", \" , args , max chars = max chars - 2 ) if opts : if not sig : sig = \"[%s]\" % limited join ( \", \" , opts , max chars = max chars - 4 ) elif len ( sig ) < max chars - 4 - 2 - 3 : sig += \"[, %s]\" % limited join ( \", \" , opts , max chars = max chars - len ( sig ) - 4 - 2 ) return u\"(%s)\" % sig", "predictions": ["clear the ( s 0 0 . 4 . s 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["reformat a function signature to a more compact form ."], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 3647, "code": "def import by name ( name ) : try : name parts = name . split ( '.' ) modname = '.' . join ( name parts [ : - 1 ] ) if modname : try : import ( modname ) mod = sys . modules [ modname ] return getattr ( mod , name parts [ - 1 ] ) , mod except ( Import Error , Index Error , Attribute Error ) : pass last j = 0 modname = None for j in reversed ( range ( 1 , len ( name parts ) + 1 ) ) : last j = j modname = '.' . join ( name parts [ : j ] ) try : import ( modname ) except : continue if modname in sys . modules : break if last j < len ( name parts ) : parent = None obj = sys . modules [ modname ] for obj name in name parts [ last j : ] : parent = obj obj = getattr ( obj , obj name ) return obj , parent else : return sys . modules [ modname ] , None except ( Value Error , Import Error , Attribute Error , Key Error ) , e : raise Import Error ( * e . args )", "predictions": ["set attributes given by name ."], "references": ["import a python object given its full name ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 3648, "code": "def alert ( message , title = \"\" , parent = None , scrolled = False , icon = \"exclamation\" ) : if not scrolled : icons = { 'exclamation' : wx . ICON EXCLAMATION , 'error' : wx . ICON ERROR , 'question' : wx . ICON QUESTION , 'info' : wx . ICON INFORMATION } style = wx . OK | icons [ icon ] result = dialogs . message Dialog ( parent , message , title , style ) else : result = dialogs . scrolled Message Dialog ( parent , message , title )", "predictions": ["creates a scrolled dialog ."], "references": ["show a simple pop - up modal dialog"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3649, "code": "def prompt ( message = \"\" , title = \"\" , default = \"\" , multiline = False , password = None , parent = None ) : if password : style = wx . TE PASSWORD | wx . OK | wx . CANCEL result = dialogs . text Entry Dialog ( parent , message , title , default , style ) elif multiline : style = wx . TE MULTILINE | wx . OK | wx . CANCEL result = dialogs . text Entry Dialog ( parent , message , title , default , style ) result . text = '\\n' . join ( result . text . splitlines ( ) ) else : result = dialogs . text Entry Dialog ( parent , message , title , default ) if result . accepted : return result . text", "predictions": ["opens a wx dialog for the given message ."], "references": ["modal dialog asking for an input returns string or none if cancelled"], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 3650, "code": "def select font ( message = \"\" , title = \"\" , font = None , parent = None ) : if font is not None : wx font = font . get wx font ( ) else : wx font = None font = Font ( ) result = dialogs . font Dialog ( parent , font = wx font ) if result . accepted : font data = result . font Data result . color = result . font Data . Get Colour ( ) . Get ( ) wx font = result . font Data . Get Chosen Font ( ) font . set wx font ( wx font ) wx font = None return font", "predictions": ["select a wx for the specified message ."], "references": ["show a dialog to select a font"], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 3651, "code": "def select color ( message = \"\" , title = \"\" , color = None , parent = None ) : result = dialogs . color Dialog ( parent , color = color ) return result . accepted and result . color", "predictions": ["select a color on the original message ."], "references": ["show a dialog to pick a color"], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 3652, "code": "def choose directory ( message = 'Choose a directory' , path = \"\" , parent = None ) : result = dialogs . directory Dialog ( parent , message , path ) return result . path", "predictions": ["create a directory to the given path and with the given path ."], "references": ["show a dialog to choose a directory"], "bleu": 0.1350862565735141, "rouge_l": 0.21143847487001732}
{"id": 3653, "code": "def find ( default = '' , whole words = 0 , case sensitive = 0 , parent = None ) : result = dialogs . find Dialog ( parent , default , whole words , case sensitive ) return { 'text' : result . search Text , 'whole words' : result . whole Words Only , 'case sensitive' : result . case Sensitive }", "predictions": ["find the first ancestor of the parent ."], "references": ["shows a find text dialog"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 3654, "code": "def clear ( self ) : dict . clear ( self ) self . key = 0 if hasattr ( self . tree view , \"wx obj\" ) : self . tree view . wx obj . Delete All Items ( )", "predictions": ["removes all items from the given dict ."], "references": ["remove all items and reset internal structures"], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 3655, "code": "def set has children ( self , has children = True ) : self . tree model . tree view . wx obj . Set Item Has Children ( self . wx item , has children )", "predictions": ["sets this set of children to the wx ."], "references": ["force appearance of the button next to the item"], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 3656, "code": "def set icon ( self , icon = None ) : if icon is not None : try : wx icon = wx . Icon ( icon , wx . BITMAP TYPE ICO ) self . wx obj . Set Icon ( wx icon ) except : pass", "predictions": ["sets the wx to be used for the wx ."], "references": ["set icon based on resource values"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3657, "code": "def show ( self , value = True , modal = None ) : self . wx obj . Show ( value ) if modal : disabler = wx . Window Disabler ( self . wx obj ) eventloop = wx . Event Loop ( ) def on close modal ( evt ) : evt . Skip ( ) eventloop . Exit ( ) self . wx obj . Bind ( wx . EVT CLOSE , on close modal ) eventloop . Run ( ) del disabler", "predictions": ["turns the value of a wx event ."], "references": ["display or hide the window optionally disabling all other windows"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 3658, "code": "def resize ( self , evt = None ) : for child in self : if isinstance ( child , Control ) : child . resize ( evt ) if evt : evt . Skip ( )", "predictions": ["resize all object types as child into child boxes ."], "references": ["automatically adjust relative pos and size of children controls"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3659, "code": "def parse ( filename = \"\" ) : s = open ( filename ) . read ( ) ##s.decode(\"latin1\").encode(\"utf8\") import datetime , decimal rsrc = eval ( s ) return rsrc", "predictions": ["parse a datetime from an annotation ."], "references": ["open read and eval the resource from the source file"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3660, "code": "def save ( filename , rsrc ) : s = pprint . pformat ( rsrc ) open ( filename , \"w\" ) . write ( s )", "predictions": ["saves the given object as . to the file ."], "references": ["save the resource to the source file"], "bleu": 0.18850319022747347, "rouge_l": 0.48605577689243035}
{"id": 3661, "code": "def build window ( res ) : kwargs = dict ( res . items ( ) ) wintype = kwargs . pop ( 'type' ) menubar = kwargs . pop ( 'menubar' , None ) components = kwargs . pop ( 'components' ) panel = kwargs . pop ( 'panel' , { } ) from gui import registry import gui winclass = registry . WINDOWS [ wintype ] win = winclass ( * * kwargs ) if False and panel is not None : panel [ 'name' ] = 'panel' p = gui . Panel ( win , * * panel ) else : p = win if components : for comp in components : build component ( comp , parent = p ) if menubar : mb = gui . Menu Bar ( name = \"menu\" , parent = win ) for menu in menubar : build component ( menu , parent = mb ) return win", "predictions": ["build the internal window from a ( ."], "references": ["create a gui2py window based on the python resource"], "bleu": 0.16829946711936866, "rouge_l": 0.116412213740458}
{"id": 3662, "code": "def build component ( res , parent = None ) : kwargs = dict ( res . items ( ) ) comtype = kwargs . pop ( 'type' ) if 'components' in res : components = kwargs . pop ( 'components' ) elif comtype == 'Menu' and 'items' in res : components = kwargs . pop ( 'items' ) else : components = [ ] from gui import registry if comtype in registry . CONTROLS : comclass = registry . CONTROLS [ comtype ] elif comtype in registry . MENU : comclass = registry . MENU [ comtype ] elif comtype in registry . MISC : comclass = registry . MISC [ comtype ] else : raise Runtime Error ( \"%s not in registry\" % comtype ) com = comclass ( parent = parent , * * kwargs ) for comp in components : build component ( comp , parent = com ) return com", "predictions": ["builds the internal component of the internal ( ."], "references": ["create a gui2py control based on the python resource"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3663, "code": "def convert ( self , name ) : new name = PYTHONCARD PROPERTY MAP . get ( name ) if new name : print \"WARNING: property %s should be %s (%s)\" % ( name , new name , self . obj . name ) return new name else : return name", "predictions": ["returns a deep copy of this command ."], "references": ["translate gui2py attribute name from pythoncard legacy code"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3664, "code": "def set data ( data ) : try : if wx . The Clipboard . Open ( ) : if isinstance ( data , ( str , unicode ) ) : do = wx . Text Data Object ( ) do . Set Text ( data ) wx . The Clipboard . Set Data ( do ) elif isinstance ( data , wx . Bitmap ) : do = wx . Bitmap Data Object ( ) do . Set Bitmap ( data ) wx . The Clipboard . Set Data ( do ) wx . The Clipboard . Close ( ) except : pass", "predictions": ["sets the data from the wx object ."], "references": ["write content to the clipboard data can be either a string or a bitmap"], "bleu": 0.08383280652235028, "rouge_l": 0.1732954545454545}
{"id": 3665, "code": "def load object ( self , obj = None ) : if obj : self . root obj = obj else : obj = self . root obj self . tree . Delete All Items ( ) self . root = self . tree . Add Root ( \"application\" ) self . tree . Set Item Text ( self . root , \"App\" , 1 ) self . tree . Set Item Text ( self . root , \"col 2 root\" , 2 ) #self.tree.Set Item Image(self.root, fldridx, which = wx.Tree Item Icon Normal) #self.tree.Set Item Image(self.root, fldropenidx, which = wx.Tree Item Icon Expanded) self . build tree ( self . root , obj ) self . tree . Expand ( self . root )", "predictions": ["builds and returns the tree as a xor instead of this entry ."], "references": ["add the object and all their childs"], "bleu": 0.10571070857151538, "rouge_l": 0.10571923743500866}
{"id": 3666, "code": "def inspect ( self , obj , context menu = False , edit prop = False , mouse pos = None ) : child = self . tree . Find Item ( self . root , obj . name ) if DEBUG : print \"inspect child\" , child if child : self . tree . Scroll To ( child ) self . tree . Set Current Item ( child ) self . tree . Select Item ( child ) child . Selected = True self . activate item ( child , edit prop ) if context menu : self . show context menu ( child , mouse pos )", "predictions": ["inspect an item . menu item will only contain items of the tree ."], "references": ["select the object and show its properties"], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 3667, "code": "def activate item ( self , child , edit prop = False , select = False ) : d = self . tree . Get Item Data ( child ) if d : o = d . Get Data ( ) self . selected obj = o callback = lambda o = o , * * kwargs : self . update ( o , * * kwargs ) self . propeditor . load object ( o , callback ) if edit prop : wx . Call After ( self . propeditor . edit ) if select and self . designer : self . designer . select ( o ) else : self . selected obj = None", "predictions": ["forwards forwards for handling the object of its private key ."], "references": ["load the selected item in the property editor"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 3668, "code": "def update ( self , obj , * * kwargs ) : child = self . tree . Find Item ( self . root , kwargs [ 'name' ] ) if DEBUG : print \"update child\" , child , kwargs if child : self . tree . Scroll To ( child ) self . tree . Set Current Item ( child ) self . tree . Select Item ( child ) child . Selected = True self . tree . Set Item Text ( child , obj . name , 0 )", "predictions": ["update an object with the given ( object ."], "references": ["update the tree item when the object name changes"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 3669, "code": "def show context menu ( self , item , mouse pos = None ) : if item : d = self . tree . Get Item Data ( item ) if d : obj = d . Get Data ( ) if obj : self . highlight ( obj . wx obj ) self . obj = obj menu = wx . Menu ( ) id del , id dup , id raise , id lower = [ wx . New Id ( ) for i in range ( 4 ) ] menu . Append ( id del , \"Delete\" ) menu . Append ( id dup , \"Duplicate\" ) menu . Append ( id raise , \"Bring to Front\" ) menu . Append ( id lower , \"Send to Back\" ) sm = wx . Menu ( ) for ctrl in sorted ( obj . meta . valid children , key = lambda c : registry . ALL . index ( c . meta . name ) ) : new id = wx . New Id ( ) sm . Append ( new id , ctrl . meta . name ) self . Bind ( wx . EVT MENU , lambda evt , ctrl = ctrl : self . add child ( ctrl , mouse pos ) , id = new id ) menu . Append Menu ( wx . New Id ( ) , \"Add child\" , sm ) self . Bind ( wx . EVT MENU , self . delete , id = id del ) self . Bind ( wx . EVT MENU , self . duplicate , id = id dup ) self . Bind ( wx . EVT MENU , self . bring to front , id = id raise ) self . Bind ( wx . EVT MENU , self . send to back , id = id lower ) self . Popup Menu ( menu ) menu . Destroy ( ) self . load object ( self . root obj )", "predictions": ["creates a wx object for a wx"], "references": ["open a popup menu with options regarding the selected object"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3670, "code": "def select option ( self ) : if self . disabled : warn ( \"Attempt to select disabled option: {}\" . format ( self . value or self . text ) ) self . base . select option ( )", "predictions": ["selects an option for this option ."], "references": ["select this node if it is an option element inside a select tag ."], "bleu": 0.10218289380194193, "rouge_l": 0.2695139911634757}
{"id": 3671, "code": "def raise server error ( self ) : if self . server and self . server . error : try : if capybara . raise server errors : raise self . server . error finally : self . server . reset error ( )", "predictions": ["raise a valueerror if the server failed ."], "references": ["raise errors encountered by the server ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 3672, "code": "def traceback ( self ) -> str : if not self . log traceback : return \"\" exc info = sys . exc info ( ) stack = traceback . extract stack ( ) exc tb = traceback . extract tb ( exc info [ 2 ] ) full tb = stack [ : 1 ] + exc tb exc line : typing . List [ str ] = traceback . format exception only ( * exc info [ : 2 ] ) tb text = \"\\n Traceback (most recent call last):\\n\" + \"\" . join ( traceback . format list ( full tb ) ) + \"\" . join ( exc line ) return tb text", "predictions": ["log information about the traceback ."], "references": ["get outer traceback text for logging ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3673, "code": "def get obj source ( self , instance : typing . Any , owner : typing . Optional [ type ] = None ) -> str : if self . log object repr : return f\"{instance!r}\" return f\"<{owner. name  if owner is not None else instance. class . name }() at 0x{id(instance):X}>\"", "predictions": ["get the name of the object without changing its name ."], "references": ["get object repr block ."], "bleu": 0.1354599427337814, "rouge_l": 0.40219780219780216}
{"id": 3674, "code": "def logger ( self , logger : typing . Union [ logging . Logger , str , None ] ) -> None : if logger is None or isinstance ( logger , logging . Logger ) : self . logger = logger else : self . logger = logging . get Logger ( logger )", "predictions": ["a helper method used to get a logger for the logger ."], "references": ["logger instance to use as override ."], "bleu": 0.1235622127262679, "rouge_l": 0.22101449275362317}
{"id": 3675, "code": "def channels ( self ) : if not self . channels : self . channels = self . call api ( 'channels.list' ) [ 'channels' ] return self . channels", "predictions": ["get a list of channels ."], "references": ["list of channels of this slack team"], "bleu": 0.34801709319446883, "rouge_l": 0.45522388059701485}
{"id": 3676, "code": "def users ( self ) : if not self . users : self . users = self . call api ( 'users.list' ) [ 'members' ] return self . users", "predictions": ["get the users users list ."], "references": ["list of users of this slack team"], "bleu": 0.20693220168471366, "rouge_l": 0.1517412935323383}
{"id": 3677, "code": "def channel from name ( self , name ) : try : channel = [ channel for channel in self . channels if channel [ 'name' ] == name ] [ 0 ] except Index Error : raise Value Error ( 'Unknown channel for name: \"{}\"' . format ( name ) ) return channel", "predictions": ["convert a file object into a native channel ."], "references": ["return the channel dict given by human - readable { name }"], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 3678, "code": "def translate ( self , message ) : try : user id = message . pop ( 'user' ) user = self . slack . user from id ( user id ) message [ u'user' ] = user [ 'name' ] except ( Key Error , Index Error , Value Error ) : pass try : if type ( message [ 'channel' ] ) == str : channel id = message . pop ( 'channel' ) else : channel id = message . pop ( 'channel' ) [ 'id' ] self . slack . reload channels ( ) channel = self . slack . channel from id ( channel id ) message [ u'channel' ] = channel [ 'name' ] except ( Key Error , Index Error , Value Error ) : pass return message", "predictions": ["translates one message to the slack channel , representing the slack one ."], "references": ["translate machine identifiers into human - readable"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 3679, "code": "def send Slack ( self , message ) : channel = message . get ( 'channel' , 'general' ) self . send Message ( self . make message ( message [ 'text' ] , channel ) )", "predictions": ["send a sip message to this configuration ."], "references": ["send message to slack"], "bleu": 0.22679164443904004, "rouge_l": 0.5319767441860466}
{"id": 3680, "code": "def read channel ( self ) : channel , message = self . protocol . channel layer . receive many ( [ u'slack.send' ] , block = False ) delay = 0.1 if channel : self . protocols [ 0 ] . send Slack ( message ) reactor . call Later ( delay , self . read channel )", "predictions": ["alert the icon with the given icon and returns it in a ( 0 , 1 title title title title title title title title title title title title title title title title title title title title title title title title title title title title title title title title title title"], "references": ["get available messages and send through to the protocol"], "bleu": 0.026594139297659906, "rouge_l": 0.038754764930114365}
{"id": 3681, "code": "def run ( self , args ) : args = self . parser . parse args ( args ) if not args . token : raise Value Error ( 'Supply the slack token through --token or setting DJANGOBOT TOKEN' ) sys . path . insert ( 0 , \".\" ) module path , object path = args . channel layer . split ( ':' , 1 ) channel layer = importlib . import module ( module path ) for part in object path . split ( '.' ) : channel layer = getattr ( channel layer , part ) Client ( channel layer = channel layer , token = args . token , ) . run ( )", "predictions": ["prompt to import and import them ."], "references": ["pass in raw arguments instantiate slack api and begin client ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 3682, "code": "def dict diff ( prv , nxt ) : keys = set ( prv . keys ( ) + nxt . keys ( ) ) result = { } for k in keys : if prv . get ( k ) != nxt . get ( k ) : result [ k ] = ( prv . get ( k ) , nxt . get ( k ) ) return result", "predictions": ["get the key of this enumerated ."], "references": ["return a dict of keys that differ with another config object ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 3683, "code": "def colorize ( msg , color ) : if DONT COLORIZE : return msg else : return \"{}{}{}\" . format ( COLORS [ color ] , msg , COLORS [ \"endc\" ] )", "predictions": ["select the given message using the provided = = 1 if the algorithm is enabled ."], "references": ["given a string add necessary codes to format the string ."], "bleu": 0.09147827112247602, "rouge_l": 0.22989949748743718}
{"id": 3684, "code": "def v2 playbook on task start ( self , task , * * kwargs ) : self . last task name = task . get name ( ) self . printed last task = False", "predictions": ["create a message on the thread ."], "references": ["run when a task starts ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3685, "code": "def v2 runner on ok ( self , result , * * kwargs ) : failed = \"failed\" in result . result unreachable = \"unreachable\" in result . result if ( \"print action\" in result . task . tags or failed or unreachable or self . display . verbosity > 1 ) : self . print task ( ) self . last skipped = False msg = unicode ( result . result . get ( \"msg\" , \"\" ) ) or unicode ( result . result . get ( \"reason\" , \"\" ) ) or unicode ( result . result . get ( \"message\" , \"\" ) ) stderr = [ result . result . get ( \"exception\" , None ) , result . result . get ( \"module stderr\" , None ) , ] stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) self . print host or item ( result . host , result . result . get ( \"changed\" , False ) , msg , result . result . get ( \"diff\" , None ) , is host = True , error = failed or unreachable , stdout = result . result . get ( \"module stdout\" , None ) , stderr = stderr . strip ( ) , ) if \"results\" in result . result : for r in result . result [ \"results\" ] : failed = \"failed\" in r stderr = [ r . get ( \"exception\" , None ) , r . get ( \"module stderr\" , None ) ] stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) self . print host or item ( r [ \"item\" ] , r . get ( \"changed\" , False ) , unicode ( r . get ( \"msg\" , \"\" ) ) , r . get ( \"diff\" , None ) , is host = False , error = failed , stdout = r . get ( \"module stdout\" , None ) , stderr = stderr . strip ( ) , ) else : self . last skipped = True print ( \".\" , end = \"\" )", "predictions": ["creates a high - darling ( clockwise sensitive sensitive sensitive sensitive sensitive sensitive sensitive sensitive sensitive sensitive sensitive sensitive sensitive sensitive data sensitive sensitive information sensitive from the jms machine ."], "references": ["run when a task finishes correctly ."], "bleu": 0.04317900023606586, "rouge_l": 0.11879259980525803}
{"id": 3686, "code": "def v2 playbook on stats ( self , stats ) : print ( ) self . printed last task = False self . print task ( \"STATS\" ) hosts = sorted ( stats . processed . keys ( ) ) for host in hosts : s = stats . summarize ( host ) if s [ \"failures\" ] or s [ \"unreachable\" ] : color = \"failed\" elif s [ \"changed\" ] : color = \"changed\" else : color = \"ok\" msg = \"{}    : ok={}\\tchanged={}\\tfailed={}\\tunreachable={}\" . format ( host , s [ \"ok\" ] , s [ \"changed\" ] , s [ \"failures\" ] , s [ \"unreachable\" ] ) print ( colorize ( msg , color ) )", "predictions": ["makes the machine for each view ."], "references": ["display info about playbook statistics ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3687, "code": "def v2 runner on skipped ( self , result , * * kwargs ) : if self . display . verbosity > 1 : self . print task ( ) self . last skipped = False line length = 120 spaces = \" \" * ( 31 - len ( result . host . name ) - 4 ) line = \"  * {}{}- {}\" . format ( colorize ( result . host . name , \"not so bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) reason = result . result . get ( \"skipped reason\" , \"\" ) or result . result . get ( \"skip reason\" , \"\" ) if len ( reason ) < 50 : line += \" -- {}\" . format ( reason ) print ( \"{} {}---------\" . format ( line , \"-\" * ( line length - len ( line ) ) ) ) else : print ( \"{} {}\" . format ( line , \"-\" * ( line length - len ( line ) ) ) ) print ( self . indent text ( reason , 8 ) ) print ( reason )", "predictions": ["makes the has finished ."], "references": ["run when a task is skipped ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3688, "code": "def load filters ( ) : all filters = { } for m in JINJA FILTERS : if hasattr ( m , \"filters\" ) : all filters . update ( m . filters ( ) ) return all filters", "predictions": ["set all the icon icon"], "references": ["loads and returns all filters ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3689, "code": "def get authorization ( self ) : auth = self . authorization class ( ) header = self . get authorization header ( ) if not header or not header . split : return auth header = header . split ( ) if len ( header ) > 1 and header [ 0 ] == 'Bearer' : auth . is oauth = True access token = header [ 1 ] self . validate access token ( access token , auth ) if not auth . is valid : auth . error = 'access denied' return auth", "predictions": ["show access if it has already been access ."], "references": ["get authorization object representing status of authentication ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3690, "code": "def open ( self , bus ) : if self . device is not None : self . close ( ) self . device = open ( '/dev/i2c-{0}' . format ( bus ) , 'r+b' , buffering = 0 )", "predictions": ["opens the atomic file in the database ."], "references": ["open the smbus interface on the specified bus ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 3691, "code": "def read byte ( self , addr ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) return ord ( self . device . read ( 1 ) )", "predictions": ["parse the ( next : https : / / www . google . com / . / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( ."], "references": ["read a single byte from the specified device ."], "bleu": 0.03250804687495712, "rouge_l": 0.09043736100815418}
{"id": 3692, "code": "def read bytes ( self , addr , number ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) return self . device . read ( number )", "predictions": ["save the ( but only s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s ."], "references": ["read many bytes from the specified device ."], "bleu": 0.02893344147704888, "rouge_l": 0.08484005563282336}
{"id": 3693, "code": "def read byte data ( self , addr , cmd ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' reg = c uint8 ( cmd ) result = c uint8 ( ) request = make i2c rdwr data ( [ ( addr , 0 , 1 , pointer ( reg ) ) , ( addr , I2C M RD , 1 , pointer ( result ) ) ] ) ioctl ( self . device . fileno ( ) , I2C RDWR , request ) return result . value", "predictions": ["build a window from the given ( ."], "references": ["read a single byte from the specified cmd register of the device ."], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 3694, "code": "def write quick ( self , addr ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' request = make i2c rdwr data ( [ ( addr , 0 , 0 , None ) , ] ) ioctl ( self . device . fileno ( ) , I2C RDWR , request )", "predictions": ["writes out the in the in the in the in the in the in the in the in seconds ."], "references": ["write a single byte to the specified device ."], "bleu": 0.06760229884571738, "rouge_l": 0.14805825242718446}
{"id": 3695, "code": "def write byte ( self , addr , val ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) data = bytearray ( 1 ) data [ 0 ] = val & 0x FF self . device . write ( data )", "predictions": ["convert part of the ( or primitive new new device new value new chunk chunk new units new value new instance ."], "references": ["write a single byte to the specified device ."], "bleu": 0.06586656967644004, "rouge_l": 0.20938215102974828}
{"id": 3696, "code": "def write bytes ( self , addr , buf ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) self . device . write ( buf )", "predictions": ["set the data structures to the device ."], "references": ["write many bytes to the specified device . buf is a bytearray"], "bleu": 0.16096450823426717, "rouge_l": 0.3860759493670886}
{"id": 3697, "code": "def write byte data ( self , addr , cmd , val ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' data = bytearray ( 2 ) data [ 0 ] = cmd & 0x FF data [ 1 ] = val & 0x FF self . select device ( addr ) self . device . write ( data )", "predictions": ["load the ( or hash key as an else else load it ."], "references": ["write a byte of data to the specified cmd register of the device ."], "bleu": 0.0978840017329239, "rouge_l": 0.1471652593486128}
{"id": 3698, "code": "def write i2c block data ( self , addr , cmd , vals ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' data = bytearray ( len ( vals ) + 1 ) data [ 0 ] = cmd & 0x FF data [ 1 : ] = vals [ 0 : ] self . select device ( addr ) self . device . write ( data )", "predictions": ["inspect the ) then inspect the ) ."], "references": ["write a buffer of data to the specified cmd register of the device ."], "bleu": 0.09008421318929809, "rouge_l": 0.25994318181818177}
{"id": 3699, "code": "def datetime created ( self ) : if self . info ( ) . get ( 'datetime created' ) : return dateutil . parser . parse ( self . info ( ) [ 'datetime created' ] )", "predictions": ["a activate for this object ."], "references": ["returns file group s create aware * datetime * in utc format ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 3700, "code": "def construct from ( cls , group info ) : group = cls ( group info [ 'id' ] ) group . info cache = group info return group", "predictions": ["update the * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * . *"], "references": ["constructs filegroup instance from group information ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 3701, "code": "def base opration ( self , method ) : uuids = self . uuids ( ) while True : chunk = list ( islice ( uuids , 0 , self . chunk size ) ) if not chunk : return rest request ( method , self . storage url , chunk )", "predictions": ["use this to do the show logic on the show being shown ."], "references": ["base method for storage operations ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 3702, "code": "def uuids ( self ) : for f in self . seq : if isinstance ( f , File ) : yield f . uuid elif isinstance ( f , six . string types ) : yield f else : raise Value Error ( 'Invalid type for sequence item: {0}' . format ( type ( f ) ) )", "predictions": ["returns a generator of sequences from the given type ."], "references": ["extract uuid from each item of specified seq ."], "bleu": 0.14991106946711685, "rouge_l": 0.21254355400696867}
{"id": 3703, "code": "def list ( api list class , arg namespace , * * extra ) : if arg namespace . starting point : ordering field = ( arg namespace . ordering or '' ) . lstrip ( '-' ) if ordering field in ( '' , 'datetime uploaded' , 'datetime created' ) : arg namespace . starting point = parser . parse ( arg namespace . starting point ) items = api list class ( starting point = arg namespace . starting point , ordering = arg namespace . ordering , limit = arg namespace . limit , request limit = arg namespace . request limit , * * extra ) items . constructor = lambda x : x try : pprint ( list ( items ) ) except Value Error as e : print ( e )", "predictions": ["raise an exception for the given raise an error or the specified reset value ."], "references": ["a common function for building methods of the list showing ."], "bleu": 0.09782375748961449, "rouge_l": 0.2373540856031128}
{"id": 3704, "code": "def bar ( iter content , parts , title = '' ) : parts = max ( float ( parts ) , 1.0 ) cells = 10 progress = 0 step = cells / parts draw = lambda progress : sys . stdout . write ( '\\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) for chunk in iter content : yield chunk progress += step draw ( progress ) sys . stdout . flush ( ) draw ( cells ) print ( '' )", "predictions": ["return a traceback of the given ) ."], "references": ["iterates over the iter_content and draws a progress bar to stdout ."], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 3705, "code": "def home mode set state ( self , state , * * kwargs ) : if state not in ( HOME MODE ON , HOME MODE OFF ) : raise Value Error ( 'Invalid home mode state' ) api = self . api info [ 'home mode' ] payload = dict ( { 'api' : api [ 'name' ] , 'method' : 'Switch' , 'version' : api [ 'version' ] , 'on' : state , ' sid' : self . sid , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) if response [ 'success' ] : return True return False", "predictions": ["configure the connection to the server ."], "references": ["set the state of home mode"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3706, "code": "def home mode status ( self , * * kwargs ) : api = self . api info [ 'home mode' ] payload = dict ( { 'api' : api [ 'name' ] , 'method' : 'Get Info' , 'version' : api [ 'version' ] , ' sid' : self . sid } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return response [ 'data' ] [ 'on' ]", "predictions": ["creates a logger for a user with authentication ."], "references": ["returns the status of home mode"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3707, "code": "def camera list ( self , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'List' , 'version' : api [ 'version' ] , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) cameras = [ ] for data in response [ 'data' ] [ 'cameras' ] : cameras . append ( Camera ( data , self . video stream url ) ) return cameras", "predictions": ["( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["return a list of cameras ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 3708, "code": "def camera info ( self , camera ids , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Get Info' , 'version' : api [ 'version' ] , 'camera Ids' : ', ' . join ( str ( id ) for id in camera ids ) , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) cameras = [ ] for data in response [ 'data' ] [ 'cameras' ] : cameras . append ( Camera ( data , self . video stream url ) ) return cameras", "predictions": ["calculates information about this video ."], "references": ["return a list of cameras matching camera_ids ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3709, "code": "def camera snapshot ( self , camera id , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Get Snapshot' , 'version' : api [ 'version' ] , 'camera Id' : camera id , } , * * kwargs ) response = self . get ( api [ 'url' ] , payload ) return response . content", "predictions": ["calculates the channel of this channel ."], "references": ["return bytes of camera image ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3710, "code": "def camera event motion enum ( self , camera id , * * kwargs ) : api = self . api info [ 'camera event' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Motion Enum' , 'version' : api [ 'version' ] , 'cam Id' : camera id , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return Motion Setting ( camera id , response [ 'data' ] [ 'MD Param' ] )", "predictions": ["calculates information about an ( i . e . the translate operation is established = translate to the translate of the translate operation = translate message . , . = . message = . . translate the translate - 1 if the translate has been returned ."], "references": ["return motion settings matching camera_id ."], "bleu": 0.02558174341959753, "rouge_l": 0.04385334291876347}
{"id": 3711, "code": "def camera event md param save ( self , camera id , * * kwargs ) : api = self . api info [ 'camera event' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'MD Param Save' , 'version' : api [ 'version' ] , 'cam Id' : camera id , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return response [ 'data' ] [ 'cam Id' ]", "predictions": ["simply saves itself to the send operation ."], "references": ["update motion settings matching camera_id with keyword args ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3712, "code": "def update ( self ) : cameras = self . api . camera list ( ) self . cameras by id = { v . camera id : v for i , v in enumerate ( cameras ) } motion settings = [ ] for camera id in self . cameras by id . keys ( ) : motion setting = self . api . camera event motion enum ( camera id ) motion settings . append ( motion setting ) self . motion settings by id = { v . camera id : v for i , v in enumerate ( motion settings ) }", "predictions": ["update this camera with another setting ."], "references": ["update cameras and motion settings with latest from api ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 3713, "code": "def set home mode ( self , state ) : state parameter = HOME MODE OFF if state : state parameter = HOME MODE ON return self . api . home mode set state ( state parameter )", "predictions": ["set the home mode ."], "references": ["set the state of home mode"], "bleu": 0.36990337444913085, "rouge_l": 0.7155425219941348}
{"id": 3714, "code": "def is last li ( li , meta data , current num Id ) : if not is li ( li , meta data ) : return False w namespace = get namespace ( li , 'w' ) next el = li while True : if next el is None : return True next el = next el . getnext ( ) if not is li ( next el , meta data ) : continue new num Id = get num Id ( next el , w namespace ) if current num Id != new num Id : return True return False", "predictions": ["returns true if the three getnext is a getnext or . ."], "references": ["determine if li is the last list item for a given list"], "bleu": 0.13065113298388567, "rouge_l": 0.25}
{"id": 3715, "code": "def get single list nodes data ( li , meta data ) : yield li w namespace = get namespace ( li , 'w' ) current num Id = get num Id ( li , w namespace ) starting ilvl = get ilvl ( li , w namespace ) el = li while True : el = el . getnext ( ) if el is None : break if not has text ( el ) : continue if is top level upper roman ( el , meta data ) : break if ( is li ( el , meta data ) and ( starting ilvl > get ilvl ( el , w namespace ) ) ) : break new num Id = get num Id ( el , w namespace ) if new num Id is None or new num Id == - 1 : yield el continue if current num Id != new num Id : break if is last li ( el , meta data , current num Id ) : yield el break yield el", "predictions": ["uses the single template to find all of the entire data for this namespace ."], "references": ["find consecutive li tags that have content that have the same list id ."], "bleu": 0.09782375748961449, "rouge_l": 0.2081911262798635}
{"id": 3716, "code": "def is bold ( r ) : w namespace = get namespace ( r , 'w' ) rpr = r . find ( '%sr Pr' % w namespace ) if rpr is None : return False bold = rpr . find ( '%sb' % w namespace ) return style is false ( bold )", "predictions": ["finds if the given criteria is bold ."], "references": ["the function will return true if the r tag passed in is considered bold ."], "bleu": 0.11578838804156227, "rouge_l": 0.41216216216216217}
{"id": 3717, "code": "def build tr ( tr , meta data , row spans ) : tr el = etree . Element ( 'tr' ) w namespace = get namespace ( tr , 'w' ) visited nodes = [ ] for el in tr : if el in visited nodes : continue visited nodes . append ( el ) if el . tag == '%stc' % w namespace : v merge = get v merge ( el ) if ( v merge is not None and v merge . get ( '%sval' % w namespace ) != 'restart' ) : continue texts = [ ] for td content in el : if td content in visited nodes : continue if is li ( td content , meta data ) : li nodes = get single list nodes data ( td content , meta data , ) list el , list visited nodes = build list ( li nodes , meta data , ) visited nodes . extend ( list visited nodes ) texts . append ( etree . tostring ( list el ) ) elif td content . tag == '%stbl' % w namespace : table el , table visited nodes = build table ( td content , meta data , ) visited nodes . extend ( table visited nodes ) texts . append ( etree . tostring ( table el ) ) elif td content . tag == '%stc Pr' % w namespace : visited nodes . append ( td content ) continue else : text = get element content ( td content , meta data , is td = True , ) texts . append ( text ) data = '<br />' . join ( t for t in texts if t is not None ) td el = etree . XML ( '<td>%s</td>' % data ) colspan = get grid span ( el ) if colspan > 1 : td el . set ( 'colspan' , '%d' % colspan ) v merge = get v merge ( el ) if ( v merge is not None and v merge . get ( '%sval' % w namespace ) == 'restart' ) : rowspan = next ( row spans ) td el . set ( 'rowspan' , '%d' % rowspan ) tr el . append ( td el ) return tr el", "predictions": ["build source and destination tags for all operands"], "references": ["this will return a single tr element with all tds already populated ."], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 3718, "code": "def build table ( table , meta data ) : table el = etree . Element ( 'table' ) w namespace = get namespace ( table , 'w' ) row spans = get rowspan data ( table ) for el in table : if el . tag == '%str' % w namespace : tr el = build tr ( el , meta data , row spans , ) table el . append ( tr el ) visited nodes = list ( table . iter ( ) ) return table el , visited nodes", "predictions": ["build the table of a given data table"], "references": ["this returns a table object with all rows and cells correctly populated ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 3719, "code": "def get t tag content ( t , parent , remove bold , remove italics , meta data ) : if t is None or t . text is None : return '' text = cgi . escape ( t . text ) el is bold = not remove bold and ( is bold ( parent ) or is underlined ( parent ) ) el is italics = not remove italics and is italics ( parent ) if el is bold : text = '<strong>%s</strong>' % text if el is italics : text = '<em>%s</em>' % text return text", "predictions": ["removes tag from attachment , which is not a ( if its not found ."], "references": ["generate the string data that for this particular t tag ."], "bleu": 0.09103526405546068, "rouge_l": 0.1582360570687419}
{"id": 3720, "code": "def strip tag ( tree , tag ) : for el in tree . iter ( ) : if el . tag == tag : el . getparent ( ) . remove ( el )", "predictions": ["strips all the children of the given tree from the given tree ."], "references": ["remove all tags that have the tag name tag"], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 3721, "code": "def find ( dataset , url ) : fn = os . path . join ( DATASETS , dataset ) dn = os . path . dirname ( fn ) if not os . path . exists ( dn ) : print ( 'creating dataset directory: %s' , dn ) os . makedirs ( dn ) if not os . path . exists ( fn ) : if sys . version info < ( 3 , ) : urllib . urlretrieve ( url , fn ) else : urllib . request . urlretrieve ( url , fn ) return fn", "predictions": ["find the folder in the dataset ."], "references": ["find the location of a dataset on disk downloading if needed ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 3722, "code": "def load mnist ( flatten = True , labels = False ) : fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) h = gzip . open ( fn , 'rb' ) if sys . version info < ( 3 , ) : ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) else : ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) h . close ( ) if not flatten : timg = timg . reshape ( ( - 1 , 28 , 28 , 1 ) ) vimg = vimg . reshape ( ( - 1 , 28 , 28 , 1 ) ) simg = simg . reshape ( ( - 1 , 28 , 28 , 1 ) ) if labels : return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) return ( timg , ) , ( vimg , ) , ( simg , )", "predictions": [". : load a mnist or the cluster ."], "references": ["load the mnist digits dataset ."], "bleu": 0.17747405280050263, "rouge_l": 0.4149659863945578}
{"id": 3723, "code": "def load cifar ( flatten = True , labels = False ) : def extract ( name ) : print ( 'extracting data from {}' . format ( name ) ) h = tar . extractfile ( name ) if sys . version info < ( 3 , ) : d = pickle . load ( h ) else : d = pickle . load ( h , encoding = 'bytes' ) for k in list ( d ) : d [ k . decode ( 'utf8' ) ] = d [ k ] h . close ( ) img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 if flatten : img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) d [ 'data' ] = img return d fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) tar = tarfile . open ( fn ) imgs = [ ] labs = [ ] for i in range ( 1 , 6 ) : d = extract ( 'cifar-10-batches-py/data batch {}' . format ( i ) ) imgs . extend ( d [ 'data' ] ) labs . extend ( d [ 'labels' ] ) timg = np . asarray ( imgs [ : 40000 ] ) tlab = np . asarray ( labs [ : 40000 ] , 'i' ) vimg = np . asarray ( imgs [ 40000 : ] ) vlab = np . asarray ( labs [ 40000 : ] , 'i' ) d = extract ( 'cifar-10-batches-py/test batch' ) simg = d [ 'data' ] slab = d [ 'labels' ] tar . close ( ) if labels : return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) return ( timg , ) , ( vimg , ) , ( simg , )", "predictions": ["load all gems from the given tar ."], "references": ["load the cifar10 image dataset ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 3724, "code": "def plot layers ( weights , tied weights = False , channels = 1 ) : if hasattr ( weights [ 0 ] , 'get value' ) : weights = [ w . get value ( ) for w in weights ] k = min ( len ( weights ) , 9 ) imgs = np . eye ( weights [ 0 ] . shape [ 0 ] ) for i , weight in enumerate ( weights [ : - 1 ] ) : imgs = np . dot ( weight . T , imgs ) plot images ( imgs , 100 + 10 * k + i + 1 , channels = channels , title = 'Layer {}' . format ( i + 1 ) ) weight = weights [ - 1 ] n = weight . shape [ 1 ] / channels if int ( np . sqrt ( n ) ) ** 2 != n : return if tied weights : imgs = np . dot ( weight . T , imgs ) plot images ( imgs , 100 + 10 * k + k , channels = channels , title = 'Layer {}' . format ( k ) ) else : plot images ( weight , 100 + 10 * k + k , channels = channels , title = 'Decoding weights' )", "predictions": ["plot the layers of the given object ."], "references": ["create a plot of weights visualized as bottom - level pixel arrays ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 3725, "code": "def plot filters ( filters ) : imgs = filters . get value ( ) N , channels , x , y = imgs . shape n = int ( np . sqrt ( N ) ) assert n * n == N , 'filters must contain a square number of rows!' assert channels == 1 or channels == 3 , 'can only plot grayscale or rgb filters!' img = np . zeros ( ( ( y + 1 ) * n - 1 , ( x + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) for i , pix in enumerate ( imgs ) : r , c = divmod ( i , n ) img [ r * ( y + 1 ) : ( r + 1 ) * ( y + 1 ) - 1 , c * ( x + 1 ) : ( c + 1 ) * ( x + 1 ) - 1 ] = pix . transpose ( ( 1 , 2 , 0 ) ) img -= img . min ( ) img /= img . max ( ) ax = plt . gcf ( ) . add subplot ( 111 ) ax . xaxis . set visible ( False ) ax . yaxis . set visible ( False ) ax . set frame on ( False ) ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray )", "predictions": ["plot auxiliary generation generation generation of the first image ."], "references": ["create a plot of conv filters visualized as pixel arrays ."], "bleu": 0.13564514503163538, "rouge_l": 0.28328173374613}
{"id": 3726, "code": "def batches ( dataset ) : seq lengths = dataset . variables [ 'seq Lengths' ] . data seq begins = np . concatenate ( ( [ 0 ] , np . cumsum ( seq lengths ) [ : - 1 ] ) ) def sample ( ) : chosen = np . random . choice ( list ( range ( len ( seq lengths ) ) ) , BATCH SIZE , replace = False ) return batch at ( dataset . variables [ 'inputs' ] . data , dataset . variables [ 'target Classes' ] . data , seq begins [ chosen ] , seq lengths [ chosen ] ) return sample", "predictions": ["returns the sample sample chi - ( representation of the dataset ."], "references": ["returns a callable that chooses sequences from netcdf data ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 3727, "code": "def variables ( self ) : result = [ self . target ] if self . weights is not None : result . append ( self . weights ) return result", "predictions": ["fixture for variables that are allowed to provide this one ."], "references": ["a list of theano variables used in this loss ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 3728, "code": "def reservoir ( xs , n , rng ) : pool = [ ] for i , x in enumerate ( xs ) : if len ( pool ) < n : pool . append ( x / np . linalg . norm ( x ) ) continue j = rng . randint ( i + 1 ) if j < n : pool [ j ] = x / np . linalg . norm ( x ) L = len ( pool ) S = np . std ( pool , axis = 0 ) while len ( pool ) < n : x = pool [ rng . randint ( L ) ] pool . append ( x + S * rng . randn ( * x . shape ) ) return np . array ( pool , dtype = pool [ 0 ] . dtype )", "predictions": ["calculates nth product of the list of reservoir . the reservoir consists of xs and xs ."], "references": ["select a random sample of n items from xs ."], "bleu": 0.10216198665886358, "rouge_l": 0.2331210191082802}
{"id": 3729, "code": "def inputs ( self ) : return [ l . input for l in self . layers if isinstance ( l , layers . Input ) ]", "predictions": ["override this to provide default implementation of self - processing layers ."], "references": ["a list of theano variables for feedforward computations ."], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 3730, "code": "def variables ( self ) : result = self . inputs seen = set ( i . name for i in result ) for loss in self . losses : for v in loss . variables : if v . name not in seen : result . append ( v ) seen . add ( v . name ) return result", "predictions": [". the inputs . this is a bit operation ."], "references": ["a list of theano variables for loss computations ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 3731, "code": "def output size ( self ) : shape = self . output shape if shape is None : raise util . Configuration Error ( 'undefined output size for layer \"{}\"' . format ( self . name ) ) return shape [ - 1 ]", "predictions": ["output the current state of this cipher object ."], "references": ["number of neurons in this layer s default output ."], "bleu": 0.15881076016027915, "rouge_l": 0.31282051282051276}
{"id": 3732, "code": "def resolve outputs ( self ) : input shape = None for i , shape in enumerate ( self . input shapes . values ( ) ) : if i == 0 : input shape = shape if len ( input shape ) != len ( shape ) or any ( a is not None and b is not None and a != b for a , b in zip ( input shape [ : - 1 ] , shape [ : - 1 ] ) ) : raise util . Configuration Error ( 'layer \"{}\" incompatible input shapes {}' . format ( self . name , self . input shapes ) ) size = self . kwargs . get ( 'size' ) shape = self . kwargs . get ( 'shape' ) if shape is not None : pass elif size is not None : shape = tuple ( input shape [ : - 1 ] ) + ( size , ) else : raise util . Configuration Error ( 'layer \"{}\" does not specify a size' . format ( self . name ) ) self . output shapes [ 'out' ] = shape", "predictions": ["resolves the input array with this object from the given reader ."], "references": ["resolve the names of outputs for this layer into shape tuples ."], "bleu": 0.1235622127262679, "rouge_l": 0.25}
{"id": 3733, "code": "def log ( self ) : inputs = ', ' . join ( '\"{0}\" {1}' . format ( * ns ) for ns in self . input shapes . items ( ) ) util . log ( 'layer {0. class . name } \"{0.name}\" {0.output shape} {1} from {2}' , self , getattr ( self . activate , 'name' , self . activate ) , inputs ) util . log ( 'learnable parameters: {}' , self . log params ( ) )", "predictions": ["threadsafe log message at this point ."], "references": ["log some information about this layer ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 3734, "code": "def log params ( self ) : total = 0 for p in self . params : shape = p . get value ( ) . shape util . log ( 'parameter \"{}\" {}' , p . name , shape ) total += np . prod ( shape ) return total", "predictions": ["the natural log of this cipher ."], "references": ["log information about this layer s parameters ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 3735, "code": "def fmt ( self , string ) : if '{' not in string : string = '{}.' + string return string . format ( self . name )", "predictions": ["a method that makes a string translation for the string ."], "references": ["helper method to format our name into a string ."], "bleu": 0.18850319022747347, "rouge_l": 0.384251968503937}
{"id": 3736, "code": "def get all intervals ( self ) : ints = sorted ( self . get intervals ( True ) ) if self . tier type == 'Interval Tier' : if not ints : ints . append ( ( self . xmin , self . xmax , '' ) ) else : if ints [ 0 ] [ 0 ] > self . xmin : ints . insert ( 0 , ( self . xmin , ints [ 0 ] [ 0 ] , '' ) ) if ints [ - 1 ] [ 1 ] < self . xmax : ints . append ( ( ints [ - 1 ] [ 1 ] , self . xmax , '' ) ) p = ints [ - 1 ] for index , i in reversed ( list ( enumerate ( ints [ : - 1 ] , 1 ) ) ) : if p [ 0 ] - i [ 1 ] != 0 : ints . insert ( index , ( i [ 1 ] , p [ 0 ] , '' ) ) p = i return ints", "predictions": ["this method makes a sorted list for the first set of things that can be read by the default ."], "references": ["returns the true list of intervals including the empty intervals ."], "bleu": 0.08039313477786734, "rouge_l": 0.2723214285714286}
{"id": 3737, "code": "def main ( ) : import optparse import sys import codecs import locale import six from . algorithm import get display parser = optparse . Option Parser ( ) parser . add option ( '-e' , '--encoding' , dest = 'encoding' , default = 'utf-8' , type = 'string' , help = 'Text encoding (default: utf-8)' ) parser . add option ( '-u' , '--upper-is-rtl' , dest = 'upper is rtl' , default = False , action = 'store true' , help = \"Treat upper case chars as strong 'R' \" 'for debugging (default: False).' ) parser . add option ( '-d' , '--debug' , dest = 'debug' , default = False , action = 'store true' , help = \"Output to stderr steps taken with the algorithm\" ) parser . add option ( '-b' , '--base-dir' , dest = 'base dir' , default = None , type = 'string' , help = \"Override base direction [L|R]\" ) options , rest = parser . parse args ( ) if options . base dir and options . base dir not in 'LR' : parser . error ( 'option -b can be L or R' ) if six . PY2 : sys . stdout = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stdout ) if rest : lines = rest else : lines = sys . stdin for line in lines : display = get display ( line , options . encoding , options . upper is rtl , options . base dir , options . debug ) if not isinstance ( display , six . text type ) : display = display . decode ( options . encoding ) six . print ( display , end = '' )", "predictions": ["the main entry point to the command line option ."], "references": ["will be used to create the console script"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3738, "code": "def debug storage ( storage , base info = False , chars = True , runs = False ) : import codecs import locale import sys if six . PY2 : stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) else : stderr = sys . stderr caller = inspect . stack ( ) [ 1 ] [ 3 ] stderr . write ( 'in %s\\n' % caller ) if base info : stderr . write ( u'  base level  : %d\\n' % storage [ 'base level' ] ) stderr . write ( u'  base dir    : %s\\n' % storage [ 'base dir' ] ) if runs : stderr . write ( u'  runs        : %s\\n' % list ( storage [ 'runs' ] ) ) if chars : output = u'  Chars       : ' for ch in storage [ 'chars' ] : if ch != '\\n' : output += ch [ 'ch' ] else : output += 'C' stderr . write ( output + u'\\n' ) output = u'  Res. levels : %s\\n' % u'' . join ( [ six . text type ( ch [ 'level' ] ) for ch in storage [ 'chars' ] ] ) stderr . write ( output ) types = [ ch [ 'type' ] . ljust ( 3 ) for ch in storage [ 'chars' ] ] for i in range ( 3 ) : if i : output = u'                %s\\n' else : output = u'  Res. types  : %s\\n' stderr . write ( output % u'' . join ( [ t [ i ] for t in types ] ) )", "predictions": ["write a sequence of ( ."], "references": ["display debug information for the storage"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3739, "code": "def reorder resolved levels ( storage , debug ) : should reset = True chars = storage [ 'chars' ] for ch in chars [ : : - 1 ] : if ch [ 'orig' ] in ( 'B' , 'S' ) : ch [ 'level' ] = storage [ 'base level' ] should reset = True elif should reset and ch [ 'orig' ] in ( 'BN' , 'WS' ) : ch [ 'level' ] = storage [ 'base level' ] else : should reset = False max len = len ( chars ) line start = line end = 0 highest level = 0 lowest odd level = EXPLICIT LEVEL LIMIT for idx in range ( max len ) : ch = chars [ idx ] char level = ch [ 'level' ] if char level > highest level : highest level = char level if char level % 2 and char level < lowest odd level : lowest odd level = char level if ch [ 'orig' ] == 'B' or idx == max len - 1 : line end = idx if ch [ 'orig' ] == 'B' : line end -= 1 reverse contiguous sequence ( chars , line start , line end , highest level , lowest odd level ) line start = idx + 1 highest level = 0 lowest odd level = EXPLICIT LEVEL LIMIT if debug : debug storage ( storage )", "predictions": ["recursive helper method for ( ."], "references": ["l1 and l2 rules"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3740, "code": "def process ( self , context ) : import os from maya import cmds current file = cmds . file ( scene Name = True , query = True ) normalised = os . path . normpath ( current file ) context . set data ( 'current File' , value = normalised ) context . set data ( 'current file' , value = normalised )", "predictions": ["set the import of the file ."], "references": ["inject the current working file"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3741, "code": "def add ( object , name , value ) : self . added . append ( name ) setattr ( object , name , value )", "predictions": ["appends the parameter to the end of the list ."], "references": ["append to self accessible via qt . qtcompat"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3742, "code": "def cli ( args ) : import argparse parser = argparse . Argument Parser ( ) parser . add argument ( \"--convert\" , help = \"Path to compiled Python module, e.g. my ui.py\" ) parser . add argument ( \"--compile\" , help = \"Accept raw .ui file and compile with native \" \"Py Side2 compiler.\" ) parser . add argument ( \"--stdout\" , help = \"Write to stdout instead of file\" , action = \"store true\" ) parser . add argument ( \"--stdin\" , help = \"Read from stdin instead of file\" , action = \"store true\" ) args = parser . parse args ( args ) if args . stdout : raise Not Implemented Error ( \"--stdout\" ) if args . stdin : raise Not Implemented Error ( \"--stdin\" ) if args . compile : raise Not Implemented Error ( \"--compile\" ) if args . convert : sys . stdout . write ( \"#\\n\" \"#\\n\" ) # # with open ( args . convert ) as f : lines = convert ( f . readlines ( ) ) backup = \"%s backup%s\" % os . path . splitext ( args . convert ) sys . stdout . write ( \"Creating \\\"%s\\\"..\\n\" % backup ) shutil . copy ( args . convert , backup ) # # with open ( args . convert , \"w\" ) as f : f . write ( \"\" . join ( lines ) ) sys . stdout . write ( \"Successfully converted \\\"%s\\\"\\n\" % args . convert )", "predictions": ["create and convert input arguments to html ."], "references": ["qt . py command - line interface"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3743, "code": "def discover gui ( ) : guis = reversed ( pyblish . api . registered guis ( ) ) for gui in guis : try : gui = import ( gui ) . show except ( Import Error , Attribute Error ) : continue else : return gui", "predictions": ["discover all gui from this gui ."], "references": ["return the most desirable of the currently registered guis"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 3744, "code": "def get single axis values ( self , axis , dataset ) : data index = getattr ( self , '%s data index' % axis ) return [ p [ data index ] for p in dataset [ 'data' ] ]", "predictions": ["returns the portion of this self ."], "references": ["return all the values for a single axis of the data ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 3745, "code": "def draw constant line ( self , value label style ) : value , label , style = value label style start = self . transform output coordinates ( ( 0 , value ) ) [ 1 ] stop = self . graph width path = etree . Sub Element ( self . graph , 'path' , { 'd' : 'M 0 %(start)s h%(stop)s' % locals ( ) , 'class' : 'constant Line' } ) if style : path . set ( 'style' , style ) text = etree . Sub Element ( self . graph , 'text' , { 'x' : str ( 2 ) , 'y' : str ( start - 2 ) , 'class' : 'constant Line' } ) text . text = label", "predictions": ["draws the outline mode ."], "references": ["draw a constant line on the y - axis with the label"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 3746, "code": "def load transform parameters ( self ) : x min , x max , x div = self . x range ( ) y min , y max , y div = self . y range ( ) x step = ( float ( self . graph width ) - self . font size * 2 ) / ( x max - x min ) y step = ( float ( self . graph height ) - self . font size * 2 ) / ( y max - y min ) self . transform parameters = dict ( locals ( ) ) del self . transform parameters [ 'self' ]", "predictions": ["generates a last last last last last last last last last last element ."], "references": ["cache the parameters necessary to transform x & y coordinates"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 3747, "code": "def add popup ( self , x , y , label ) : txt width = len ( label ) * self . font size * 0.6 + 10 tx = x + [ 5 , - 5 ] [ int ( x + txt width > self . width ) ] anchor = [ 'start' , 'end' ] [ x + txt width > self . width ] style = 'fill: #000; text-anchor: %s;' % anchor id = 'label-%s' % self . w3c name ( label ) attrs = { 'x' : str ( tx ) , 'y' : str ( y - self . font size ) , 'visibility' : 'hidden' , 'style' : style , 'text' : label , 'id' : id , } etree . Sub Element ( self . foreground , 'text' , attrs ) vis tmpl = ( \"document.get Element By Id('{id}').set Attribute('visibility', {val})\" ) attrs = { 'cx' : str ( x ) , 'cy' : str ( y ) , 'r' : str ( 10 ) , 'style' : 'opacity: 0;' , 'onmouseover' : vis tmpl . format ( val = 'visible' , id = id ) , 'onmouseout' : vis tmpl . format ( val = 'hidden' , id = id ) , } etree . Sub Element ( self . foreground , 'circle' , attrs )", "predictions": ["get single single single single single single single single single single single single single single single single single single single single single single single single single anchor ."], "references": ["add pop - up information to a point on the graph ."], "bleu": 0.04327969719414172, "rouge_l": 0.053886925795053005}
{"id": 3748, "code": "def make datapoint text ( self , x , y , value , style = None ) : if not self . show data values : return e = etree . Sub Element ( self . foreground , 'text' , { 'x' : str ( x ) , 'y' : str ( y ) , 'class' : 'data Point Label' , 'style' : '%(style)s stroke: #fff; stroke-width: 2;' % vars ( ) , } ) e . text = str ( value ) e = etree . Sub Element ( self . foreground , 'text' , { 'x' : str ( x ) , 'y' : str ( y ) , 'class' : 'data Point Label' } ) e . text = str ( value ) if style : e . set ( 'style' , style )", "predictions": ["makes the cross display"], "references": ["add text for a datapoint"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 3749, "code": "def draw x labels ( self ) : if self . show x labels : labels = self . get x labels ( ) count = len ( labels ) labels = enumerate ( iter ( labels ) ) start = int ( not self . step include first x label ) labels = itertools . islice ( labels , start , None , self . step x labels ) list ( map ( self . draw x label , labels ) ) self . draw x guidelines ( self . field width ( ) , count )", "predictions": ["generic class to build the ( ( visited data data data data data data data data data data data data of a conditional conditional data data data data data data data data structure data ."], "references": ["draw the x axis labels"], "bleu": 0.034487891886161, "rouge_l": 0.05781990521327014}
{"id": 3750, "code": "def draw y labels ( self ) : if not self . show y labels : return labels = self . get y labels ( ) count = len ( labels ) labels = enumerate ( iter ( labels ) ) start = int ( not self . step include first y label ) labels = itertools . islice ( labels , start , None , self . step y labels ) list ( map ( self . draw y label , labels ) ) self . draw y guidelines ( self . field height ( ) , count )", "predictions": ["a simple vertical nodes of this compilation ."], "references": ["draw the y axis labels"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3751, "code": "def draw x guidelines ( self , label height , count ) : if not self . show x guidelines : return for count in range ( 1 , count ) : move = 'M {start} 0 v{stop}' . format ( start = label height * count , stop = self . graph height , ) path = { 'd' : move , 'class' : 'guide Lines' } etree . Sub Element ( self . graph , 'path' , path )", "predictions": ["draws the triangle for this % % of the % components ."], "references": ["draw the x - axis guidelines"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 3752, "code": "def draw y guidelines ( self , label height , count ) : if not self . show y guidelines : return for count in range ( 1 , count ) : move = 'M 0 {start} h{stop}' . format ( start = self . graph height - label height * count , stop = self . graph width , ) path = { 'd' : move , 'class' : 'guide Lines' } etree . Sub Element ( self . graph , 'path' , path )", "predictions": ["draws the graph graph ."], "references": ["draw the y - axis guidelines"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3753, "code": "def draw titles ( self ) : if self . show graph title : self . draw graph title ( ) if self . show graph subtitle : self . draw graph subtitle ( ) if self . show x title : self . draw x title ( ) if self . show y title : self . draw y title ( )", "predictions": ["requests the os os os os os fn to the output ."], "references": ["draws the graph title and subtitle"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 3754, "code": "def render inline styles ( self ) : if not self . css inline : return styles = self . parse css ( ) for node in self . root . xpath ( '//*[@class]' ) : cl = '.' + node . attrib [ 'class' ] if cl not in styles : continue style = styles [ cl ] if 'style' in node . attrib : style += node . attrib [ 'style' ] node . attrib [ 'style' ] = style", "predictions": ["load the mnist nodes ."], "references": ["hard - code the styles into the svg xml if style sheets are not used ."], "bleu": 0.03347779366253814, "rouge_l": 0.17403708987161198}
{"id": 3755, "code": "def start svg ( self ) : SVG NAMESPACE = 'http://www.w3.org/2000/svg' SVG = '{%s}' % SVG NAMESPACE NSMAP = { None : SVG NAMESPACE , 'xlink' : 'http://www.w3.org/1999/xlink' , 'a3' : 'http://ns.adobe.com/Adobe SVG Viewer Extensions/3.0/' , } root attrs = self . get root attributes ( ) self . root = etree . Element ( SVG + \"svg\" , attrib = root attrs , nsmap = NSMAP ) if hasattr ( self , 'style sheet href' ) : pi = etree . Processing Instruction ( 'xml-stylesheet' , 'href=\"%s\" type=\"text/css\"' % self . style sheet href ) self . root . addprevious ( pi ) comment strings = ( ' Created with SVG.Graph ' , ' SVG.Graph by Jason R. Coombs ' , ' Based on SVG::Graph by Sean E. Russel ' , ' Based on Perl SVG:TT:Graph by Leo Lapworth & Stephan Morgan ' , ' ' + '/' * 66 , ) list ( map ( self . root . append , map ( etree . Comment , comment strings ) ) ) defs = etree . Sub Element ( self . root , 'defs' ) self . add defs ( defs ) if not hasattr ( self , 'style sheet href' ) and not self . css inline : self . root . append ( etree . Comment ( ' include default stylesheet if none specified ' ) ) style = etree . Sub Element ( defs , 'style' , type = 'text/css' ) style . text = self . get stylesheet ( ) . css Text self . root . append ( etree . Comment ( 'SVG Background' ) ) etree . Sub Element ( self . root , 'rect' , { 'width' : str ( self . width ) , 'height' : str ( self . height ) , 'x' : '0' , 'y' : '0' , 'class' : 'svg Background' } )", "predictions": ["creates a new load ."], "references": ["base svg document creation"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3756, "code": "def get stylesheet resources ( self ) : class vars = class dict ( self ) loader = functools . partial ( self . load resource stylesheet , subs = class vars ) sheets = list ( map ( loader , self . stylesheet names ) ) return sheets", "predictions": ["retrieves a layers from the original ] ."], "references": ["get the stylesheets for this instance"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3757, "code": "def send validation email ( self ) : if self . email verified : raise Value Error ( ( 'Cannot validate already active user.' ) ) site = Site . objects . get current ( ) self . validation notification ( user = self , site = site ) . notify ( )", "predictions": ["plot the filters for the specified ( ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["send a validation email to the user s email address ."], "bleu": 0.02403051755364481, "rouge_l": 0.037059538274605106}
{"id": 3758, "code": "def send password reset ( self ) : site = Site . objects . get current ( ) self . password reset notification ( user = self , site = site ) . notify ( )", "predictions": ["batches for the = thread ."], "references": ["send a password reset to the user s email address ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3759, "code": "def allow request ( self , request , view ) : if request . method != 'POST' : return True return super ( Post Request Throttle Mixin , self ) . allow request ( request , view )", "predictions": ["todo : this call can be called multiple times to variables that are supported by the post thread ."], "references": ["throttle post requests only ."], "bleu": 0.0712695567709093, "rouge_l": 0.18625954198473282}
{"id": 3760, "code": "def client ( self ) : cls = self . class if cls . client is None : kwargs = { } if self . tls config : kwargs [ 'tls' ] = docker . tls . TLS Config ( * * self . tls config ) kwargs . update ( kwargs from env ( ) ) client = docker . API Client ( version = 'auto' , * * kwargs ) cls . client = client return cls . client", "predictions": ["creates a reservoir for the in this in the configuration ."], "references": ["single global client instance"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3761, "code": "def poll ( self ) : service = yield self . get service ( ) if not service : self . log . warn ( \"Docker service not found\" ) return 0 task filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } tasks = yield self . docker ( 'tasks' , task filter ) running task = None for task in tasks : task state = task [ 'Status' ] [ 'State' ] self . log . debug ( \"Task %s of Docker service %s status: %s\" , task [ 'ID' ] [ : 7 ] , self . service id [ : 7 ] , pformat ( task state ) , ) if task state == 'running' : running task = task if running task is not None : return None else : return 1", "predictions": ["check all queries of the threadlocal and l ."], "references": ["check for a task state like docker service ps id"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 3762, "code": "def filter queryset ( self , value , queryset ) : return super ( Unique Email Validator , self ) . filter queryset ( value . lower ( ) , queryset , )", "predictions": ["variables must be called before the api method of determining the underlying data ."], "references": ["check lower - cased email is unique ."], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 3763, "code": "def update ( self , instance , validated data ) : if not instance . check password ( validated data [ 'old password' ] ) : msg = ( 'Invalid password.' ) raise serializers . Validation Error ( { 'old password' : msg } ) instance . set password ( validated data [ 'new password' ] ) instance . save ( ) return instance", "predictions": ["output the ( mechanism ."], "references": ["check the old password is valid and set the new password ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 3764, "code": "def update ( self , instance , validated data ) : instance . set password ( validated data [ 'new password' ] ) instance . save ( ) return instance", "predictions": ["resolve content by : shapes and shapes ."], "references": ["set the new password for the user ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3765, "code": "def delete ( self , request , * args , * * kwargs ) : auth = get authorization header ( request ) . split ( ) if not auth or auth [ 0 ] . lower ( ) != b'token' : return response . Response ( status = status . HTTP 400 BAD REQUEST ) if len ( auth ) == 1 : msg = 'Invalid token header. No credentials provided.' return response . Response ( msg , status = status . HTTP 400 BAD REQUEST ) elif len ( auth ) > 2 : msg = 'Invalid token header. Token string should not contain spaces.' return response . Response ( msg , status = status . HTTP 400 BAD REQUEST ) try : token = self . model . objects . get ( key = auth [ 1 ] ) except self . model . Does Not Exist : pass else : token . delete ( ) signals . user logged out . send ( type ( self ) , user = token . user , request = request , ) return response . Response ( status = status . HTTP 204 NO CONTENT )", "predictions": ["deletes the vpn object ."], "references": ["delete auth token when delete request was issued ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 3766, "code": "def initial ( self , request , * args , * * kwargs ) : email = request . data . get ( 'email' ) if request . user . is authenticated ( ) and email != request . user . email : raise Permission Denied ( ) return super ( Resend Confirmation Email , self ) . initial ( request , * args , * * kwargs )", "predictions": ["log a user about the log field ."], "references": ["disallow users other than the user whose email is being reset ."], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 3767, "code": "def post ( self , request , * args , * * kwargs ) : serializer = self . serializer class ( data = request . data ) if not serializer . is valid ( ) : return response . Response ( serializer . errors , status = status . HTTP 400 BAD REQUEST , ) serializer . user . send validation email ( ) msg = ( 'Email confirmation sent.' ) return response . Response ( msg , status = status . HTTP 204 NO CONTENT )", "predictions": ["sends an email ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ."], "references": ["validate email and send a request to confirm it ."], "bleu": 0.02893344147704888, "rouge_l": 0.08079470198675497}
{"id": 3768, "code": "def update expiry ( self , commit = True ) : self . expires = update expiry ( self . created ) if commit : self . save ( )", "predictions": ["inserts the ) of this object into a ) or updates the ) ."], "references": ["update token s expiration datetime on every auth action ."], "bleu": 0.08839374326825923, "rouge_l": 0.08591549295774649}
{"id": 3769, "code": "def password reset email context ( notification ) : return { 'protocol' : 'https' , 'uid' : notification . user . generate uid ( ) , 'token' : notification . user . generate token ( ) , 'site' : notification . site , }", "predictions": ["( sys . ( sys sys sys . optparse sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys . sys . main sys sys sys sys sys : false sys"], "references": ["email context to reset a user password ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 3770, "code": "def email handler ( notification , email context ) : incuna mail . send ( to = notification . user . email , subject = notification . email subject , template name = notification . text email template , html template name = notification . html email template , context = email context ( notification ) , headers = getattr ( notification , 'headers' , { } ) , )", "predictions": ["sends an debug debug message to the server ."], "references": ["send a notification by email ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3771, "code": "def password reset email handler ( notification ) : base subject = ( '{domain} password reset' ) . format ( domain = notification . site . domain ) subject = getattr ( settings , 'DUM PASSWORD RESET SUBJECT' , base subject ) notification . email subject = subject email handler ( notification , password reset email context )", "predictions": ["resets the reorder reorder state ."], "references": ["password reset email handler ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3772, "code": "def validation email handler ( notification ) : base subject = ( '{domain} account validate' ) . format ( domain = notification . site . domain ) subject = getattr ( settings , 'DUM VALIDATE EMAIL SUBJECT' , base subject ) notification . email subject = subject email handler ( notification , validation email context )", "predictions": ["let the user know that the context is allowed to perform an ( if the context is not available ) ."], "references": ["validation email handler ."], "bleu": 0.05809665204409193, "rouge_l": 0.09118086696562032}
{"id": 3773, "code": "def authenticate credentials ( self , key ) : user , token = super ( Token Authentication , self ) . authenticate credentials ( key ) if token . expires < timezone . now ( ) : msg = ( 'Token has expired.' ) raise exceptions . Authentication Failed ( msg ) token . update expiry ( ) return ( user , token )", "predictions": ["add an ( if the value is associated with this value , the value is the same value ) ."], "references": ["custom authentication to check if auth token has expired ."], "bleu": 0.06760229884571738, "rouge_l": 0.1418604651162791}
{"id": 3774, "code": "def notebook show ( obj , doc , comm ) : target = obj . ref [ 'id' ] load mime = 'application/vnd.holoviews load.v0+json' exec mime = 'application/vnd.holoviews exec.v0+json' bokeh script , bokeh div , = bokeh . embed . notebook . notebook content ( obj , comm . id ) publish display data ( data = { 'text/html' : encode utf8 ( bokeh div ) } ) JS = '\\n' . join ( [ PYVIZ PROXY , Jupyter Comm Manager . js manager ] ) publish display data ( data = { load mime : JS , 'application/javascript' : JS } ) msg handler = bokeh msg handler . format ( plot id = target ) comm js = comm . js template . format ( plot id = target , comm id = comm . id , msg handler = msg handler ) bokeh js = '\\n' . join ( [ comm js , bokeh script ] ) publish display data ( data = { exec mime : '' , 'text/html' : '' , 'application/javascript' : bokeh js } , metadata = { exec mime : { 'id' : target } } )", "predictions": ["creates the bokeh object for the specified object ."], "references": ["displays bokeh output inside a notebook ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 3775, "code": "def process hv plots ( widgets , plots ) : bokeh plots = [ ] for plot in plots : if hasattr ( plot , ' update callbacks' ) : for subplot in plot . traverse ( lambda x : x ) : subplot . comm = widgets . server comm for cb in subplot . callbacks : for c in cb . callbacks : c . code = c . code . replace ( plot . id , widgets . plot id ) plot = plot . state bokeh plots . append ( plot ) return bokeh plots", "predictions": ["discover ( or optionally = 0 = 8 = 0 = 8 = ( % = 0 = 8 = 0 = 8 = 8 = 8 = 8 = 8 = 8 = 8 = 8 = 8 = 8 = 8 = 8 = 8 = 8 ="], "references": ["temporary fix to patch holoviews plot comms"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 3776, "code": "def widget ( self , param name ) : if param name not in self . widgets : self . widgets [ param name ] = self . make widget ( param name ) return self . widgets [ param name ]", "predictions": ["make a widget for this single widget ."], "references": ["get widget for param_name"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 3777, "code": "def render function ( obj , view ) : try : import holoviews as hv except : hv = None if hv and isinstance ( obj , hv . core . Dimensioned ) : renderer = hv . renderer ( 'bokeh' ) if not view . notebook : renderer = renderer . instance ( mode = 'server' ) plot = renderer . get plot ( obj , doc = view . document ) if view . notebook : plot . comm = view . comm plot . document = view . document return plot . state return obj", "predictions": ["render the configuration message ."], "references": ["the default renderer function which handles holoviews objects ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3778, "code": "def Text Widget ( * args , * * kw ) : kw [ 'value' ] = str ( kw [ 'value' ] ) kw . pop ( 'options' , None ) return Text Input ( * args , * * kw )", "predictions": ["gets the text from the current update method and the given text ."], "references": ["forces a parameter value to be text"], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 3779, "code": "def ping ( self , params = None ) : try : self . transport . perform request ( 'HEAD' , '/' , params = params ) except Transport Error : raise gen . Return ( False ) raise gen . Return ( True )", "predictions": ["executes a request on the client ."], "references": ["returns true if the cluster is up false otherwise ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3780, "code": "def bytes to readable ( num ) : if num < 512 : return \"0 Kb\" elif num < 1024 : return \"1 Kb\" for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : if abs ( num ) < 1024.0 : return \"%3.1f%s\" % ( num , unit ) num /= 1024.0 return \"%.1f%s\" % ( num , 'Yb' )", "predictions": ["convert a document from an array of bytes to a string ."], "references": ["converts bytes to a human readable format"], "bleu": 0.19338531381761725, "rouge_l": 0.3315217391304348}
{"id": 3781, "code": "def cpu total load ( self ) : system load = self . cpu system load user load = self . cpu user load other load = self . cpu other load if system load is not None and user load is not None and other load is not None : return system load + user load + other load", "predictions": ["creates a new cpu object for the given user ."], "references": ["total cpu load for synology dsm"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 3782, "code": "def memory size ( self , human readable = True ) : if self . data is not None : return data = int ( self . data [ \"memory\" ] [ \"memory size\" ] ) * 1024 if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data", "predictions": ["return the amount of bytes in bytes ."], "references": ["total memory size of synology dsm"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3783, "code": "def network up ( self , human readable = True ) : network = self . get network ( \"total\" ) if network is not None : return data = int ( network [ \"tx\" ] ) if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data", "predictions": ["given a network and a human readable representation ."], "references": ["total upload speed being used"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3784, "code": "def volumes ( self ) : if self . data is not None : volumes = [ ] for volume in self . data [ \"volumes\" ] : volumes . append ( volume [ \"id\" ] ) return volumes", "predictions": ["get a list of volumes for this array ."], "references": ["returns all available volumes"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3785, "code": "def get volume ( self , volume id ) : if self . data is not None : for volume in self . data [ \"volumes\" ] : if volume [ \"id\" ] == volume id : return volume", "predictions": ["get the volume info ."], "references": ["returns a specific volume"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3786, "code": "def volume size total ( self , volume , human readable = True ) : volume = self . get volume ( volume ) if volume is not None : return data = int ( volume [ \"size\" ] [ \"total\" ] ) if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data", "predictions": ["get the total size of the volume ."], "references": ["total size of volume"], "bleu": 0.3155984539112945, "rouge_l": 0.7093023255813954}
{"id": 3787, "code": "def volume percentage used ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : total = int ( volume [ \"size\" ] [ \"total\" ] ) used = int ( volume [ \"size\" ] [ \"used\" ] ) if used is not None and used > 0 and total is not None and total > 0 : return round ( ( float ( used ) / float ( total ) ) * 100.0 , 1 )", "predictions": ["determine the percentage of an image based on the volume ."], "references": ["total used size in percentage for volume"], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 3788, "code": "def volume disk temp avg ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : vol disks = volume [ \"disks\" ] if vol disks is not None : total temp = 0 total disks = 0 for vol disk in vol disks : disk temp = self . disk temp ( vol disk ) if disk temp is not None : total disks += 1 total temp += disk temp if total temp > 0 and total disks > 0 : return round ( total temp / total disks , 0 )", "predictions": ["return the disk disk + temp folder ."], "references": ["average temperature of all disks making up the volume"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3789, "code": "def volume disk temp max ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : vol disks = volume [ \"disks\" ] if vol disks is not None : max temp = 0 for vol disk in vol disks : disk temp = self . disk temp ( vol disk ) if disk temp is not None and disk temp > max temp : max temp = disk temp return max temp", "predictions": ["return the disk disk disk for a volume ."], "references": ["maximum temperature of all disks making up the volume"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3790, "code": "def get disk ( self , disk id ) : if self . data is not None : for disk in self . data [ \"disks\" ] : if disk [ \"id\" ] == disk id : return disk", "predictions": ["get the disk image for this disk ."], "references": ["returns a specific disk"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3791, "code": "def login ( self ) : api path = \"%s/auth.cgi?api=SYNO.API.Auth&version=2\" % ( self . base url , ) login path = \"method=login&%s\" % ( self . encode credentials ( ) ) url = \"%s&%s&session=Core&format=cookie\" % ( api path , login path ) result = self . execute get url ( url , False ) if result is not None : self . access token = result [ \"data\" ] [ \"sid\" ] self . debuglog ( \"Authentication Succesfull, token: \" + str ( self . access token ) ) return True else : self . debuglog ( \"Authentication Failed\" ) return False", "predictions": ["login in order to retrieve the user about their access name ."], "references": ["build and execute login request"], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 3792, "code": "def get url ( self , url , retry on error = True ) : if self . access token is None or self . session is None or self . session error : self . access token = None self . session error = False if self . session is not None : self . session = None self . debuglog ( \"Creating New Session\" ) self . session = requests . Session ( ) if self . use https : self . session . verify = False if self . login ( ) is False : self . session error = True self . debuglog ( \"Login Failed, unable to process request\" ) return response = self . execute get url ( url ) if ( self . session error or response is None ) and retry on error : self . debuglog ( \"Error occured, retrying...\" ) self . get url ( url , False ) return response", "predictions": ["get the full url for the application ."], "references": ["function to handle sessions for a get request"], "bleu": 0.17747405280050269, "rouge_l": 0.125}
{"id": 3793, "code": "def execute get url ( self , request url , append sid = True ) : self . debuglog ( \"Requesting URL: '\" + request url + \"'\" ) if append sid : self . debuglog ( \"Appending access token (SID: \" + self . access token + \") to url\" ) request url = \"%s& sid=%s\" % ( request url , self . access token ) try : resp = self . session . get ( request url ) self . debuglog ( \"Request executed: \" + str ( resp . status code ) ) if resp . status code == 200 : json data = json . loads ( resp . text ) if json data [ \"success\" ] : self . debuglog ( \"Succesfull returning data\" ) self . debuglog ( str ( json data ) ) return json data else : if json data [ \"error\" ] [ \"code\" ] in { 105 , 106 , 107 , 119 } : self . debuglog ( \"Session error: \" + str ( json data [ \"error\" ] [ \"code\" ] ) ) self . session error = True else : self . debuglog ( \"Failed: \" + resp . text ) else : return None except : return None", "predictions": ["client execute a method call ."], "references": ["function to execute and handle a get request"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3794, "code": "def update ( self ) : if self . utilisation is not None : api = \"SYNO.Core.System.Utilization\" url = \"%s/entry.cgi?api=%s&version=1&method=get& sid=%s\" % ( self . base url , api , self . access token ) self . utilisation . update ( self . get url ( url ) ) if self . storage is not None : api = \"SYNO.Storage.CGI.Storage\" url = \"%s/entry.cgi?api=%s&version=1&method=load info& sid=%s\" % ( self . base url , api , self . access token ) self . storage . update ( self . get url ( url ) )", "predictions": ["updates this token with another token ."], "references": ["updates the various instanced modules"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3795, "code": "def utilisation ( self ) : if self . utilisation is None : api = \"SYNO.Core.System.Utilization\" url = \"%s/entry.cgi?api=%s&version=1&method=get\" % ( self . base url , api ) self . utilisation = Syno Utilization ( self . get url ( url ) ) return self . utilisation", "predictions": ["html snippet for this object ."], "references": ["getter for various utilisation variables"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3796, "code": "def storage ( self ) : if self . storage is None : api = \"SYNO.Storage.CGI.Storage\" url = \"%s/entry.cgi?api=%s&version=1&method=load info\" % ( self . base url , api ) self . storage = Syno Storage ( self . get url ( url ) ) return self . storage", "predictions": ["get the storage for this object ."], "references": ["getter for various storage variables"], "bleu": 0.20556680845025982, "rouge_l": 0.17183098591549298}
{"id": 3797, "code": "def for request ( request , body = None ) : tenant , jwt data = Tenant . objects . for request ( request , body ) webhook sender id = jwt data . get ( 'sub' ) sender data = None if body and 'item' in body : if 'sender' in body [ 'item' ] : sender data = body [ 'item' ] [ 'sender' ] elif 'message' in body [ 'item' ] and 'from' in body [ 'item' ] [ 'message' ] : sender data = body [ 'item' ] [ 'message' ] [ 'from' ] if sender data is None : if webhook sender id is None : raise Bad Tenant Error ( 'Cannot identify sender in tenant' ) sender data = { 'id' : webhook sender id } return Context ( tenant = tenant , sender = Hipchat User ( id = sender data . get ( 'id' ) , name = sender data . get ( 'name' ) , mention name = sender data . get ( 'mention name' ) , ) , signed request = request . GET . get ( 'signed request' ) , context = jwt data . get ( 'context' ) or { } , )", "predictions": ["gets for jwt for an consumer ."], "references": ["creates the context for a specific request ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3798, "code": "def tenant token ( self ) : rv = getattr ( self , ' tenant token' , None ) if rv is None : rv = self . tenant token = self . tenant . get token ( ) return rv", "predictions": ["migrate the currently set token to a tenant ."], "references": ["the cached token of the current tenant ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 3799, "code": "def build attrs ( self , extra attrs = None , * * kwargs ) : self . attrs = self . widget . build attrs ( extra attrs = None , * * kwargs ) return self . attrs", "predictions": ["this method builds the default build class used to build fields with the attributes provided ."], "references": ["helper function for building an attribute dictionary ."], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 3800, "code": "def get global settings ( self ) : return dict ( ( key , getattr ( global settings , key ) ) for key in dir ( global settings ) if key . isupper ( ) )", "predictions": ["retrieves all settings for this key and value ."], "references": ["return a dictionary of all global_settings values ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 3801, "code": "def do GET ( self ) : parsed url = urlparse ( self . path ) if parsed url [ 2 ] == \"/\" + SERVER REDIRECT PATH : parsed query = parse qs ( parsed url [ 4 ] ) if \"code\" not in parsed query : self . send response ( 200 ) self . send header ( \"Content-Type\" , \"text/plain\" ) self . end headers ( ) self . wfile . write ( \"No code found, try again!\" . encode ( \"utf-8\" ) ) return self . server . response code = parsed query [ \"code\" ] [ 0 ] self . send response ( 200 ) self . send header ( \"Content-Type\" , \"text/plain\" ) self . end headers ( ) self . wfile . write ( \"Thank you for using O Auth2Util. The authorization was successful, \" \"you can now close this window.\" . encode ( \"utf-8\" ) ) elif parsed url [ 2 ] == \"/\" + SERVER LINK PATH : self . send response ( 200 ) self . send header ( \"Content-Type\" , \"text/html\" ) self . end headers ( ) self . wfile . write ( \"<html><body>Hey there!<br/>Click <a href=\\\"{0}\\\">here</a> to claim your prize.</body></html>\" . format ( self . server . authorize url ) . encode ( \"utf-8\" ) ) else : self . send response ( 404 ) self . send header ( \"Content-Type\" , \"text/plain\" ) self . end headers ( ) self . wfile . write ( \"404 not found\" . encode ( \"utf-8\" ) )", "predictions": ["makes the user about their ( ."], "references": ["handle the retrieval of the code"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3802, "code": "def get value ( self , key , func = None , split val = None , as boolean = False , exception default = None ) : try : if as boolean : return self . config . getboolean ( key [ 0 ] , key [ 1 ] ) value = self . config . get ( key [ 0 ] , key [ 1 ] ) if split val is not None : value = value . split ( split val ) if func is not None : return func ( value ) return value except ( Key Error , configparser . No Section Error , configparser . No Option Error ) as e : if exception default is not None : return exception default raise Key Error ( e )", "predictions": ["get or set the value of a edge ."], "references": ["helper method to get a value from the config"], "bleu": 0.17747405280050263, "rouge_l": 0.2222222222222222}
{"id": 3803, "code": "def change value ( self , key , value ) : if not self . config . has section ( key [ 0 ] ) : self . config . add section ( key [ 0 ] ) self . config . set ( key [ 0 ] , key [ 1 ] , str ( value ) ) with open ( self . configfile , \"w\" ) as f : self . config . write ( f )", "predictions": ["attach a value to the declaration of this configuration ."], "references": ["change the value of the given key in the given file to the given value"], "bleu": 0.11433296432660552, "rouge_l": 0.23164556962025318}
{"id": 3804, "code": "def migrate config ( self , oldname = DEFAULT CONFIG , newname = DEFAULT CONFIG ) : self . log ( \"Your O Auth2Util config file is in an old format and needs \" \"to be changed. I tried as best as I could to migrate it.\" , logging . WARNING ) with open ( oldname , \"r\" ) as old : with open ( newname , \"w\" ) as new : new . write ( \"[app]\\n\" ) new . write ( old . read ( ) )", "predictions": ["migrate config file using ( and . ."], "references": ["migrates the old config file format to the new one"], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 3805, "code": "def start webserver ( self , authorize url = None ) : server address = ( SERVER URL , SERVER PORT ) self . server = HTTP Server ( server address , O Auth2Util Request Handler ) self . server . response code = None self . server . authorize url = authorize url t = Thread ( target = self . server . serve forever ) t . daemon = True t . start ( )", "predictions": ["creates a new server instance that is interested in the given url ."], "references": ["start the webserver that will receive the code"], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 3806, "code": "def wait for response ( self ) : while not self . server . response code : time . sleep ( 2 ) time . sleep ( 5 ) self . server . shutdown ( )", "predictions": ["wait for the previously started thread to finish ."], "references": ["wait until the user accepted or rejected the request"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3807, "code": "def get new access information ( self ) : if not self . r . has oauth app info : self . log ( 'Cannot obtain authorize url from PRAW. Please check your configuration.' , logging . ERROR ) raise Attribute Error ( 'Reddit Session invalid, please check your designated config file.' ) url = self . r . get authorize url ( 'Using O Auth2Util' , self . get value ( CONFIGKEY SCOPE , set , split val = ',' ) , self . get value ( CONFIGKEY REFRESHABLE , as boolean = True ) ) self . start webserver ( url ) if not self . get value ( CONFIGKEY SERVER MODE , as boolean = True ) : webbrowser . open ( url ) else : print ( \"Webserver is waiting for you :D. Please open {0}:{1}/{2} \" \"in your browser\" . format ( SERVER URL , SERVER PORT , SERVER LINK PATH ) ) self . wait for response ( ) try : access information = self . r . get access information ( self . server . response code ) except praw . errors . O Auth Exception : self . log ( \"Can not authenticate, maybe the app infos (e.g. secret) are wrong.\" , logging . ERROR ) raise self . change value ( CONFIGKEY TOKEN , access information [ \"access token\" ] ) self . change value ( CONFIGKEY REFRESH TOKEN , access information [ \"refresh token\" ] ) self . change value ( CONFIGKEY VALID UNTIL , time . time ( ) + TOKEN VALID DURATION )", "predictions": ["creates the access - access token ."], "references": ["request new access information from reddit using the built in webserver"], "bleu": 0.1160873020151595, "rouge_l": 0.10683012259194395}
{"id": 3808, "code": "def check token present ( self ) : try : self . get value ( CONFIGKEY TOKEN ) self . get value ( CONFIGKEY REFRESH TOKEN ) self . get value ( CONFIGKEY REFRESHABLE ) except Key Error : self . log ( \"Request new Token (CTP)\" ) self . get new access information ( )", "predictions": ["widget the needed authentication for this instance ."], "references": ["check whether the tokens are set and request new ones if not"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 3809, "code": "def set access credentials ( self , retry = 0 ) : if retry >= 5 : raise Connection Aborted Error ( 'Reddit is not accessible right now, cannot refresh O Auth2 tokens.' ) self . check token present ( ) try : self . r . set access credentials ( self . get value ( CONFIGKEY SCOPE , set , split val = \",\" ) , self . get value ( CONFIGKEY TOKEN ) , self . get value ( CONFIGKEY REFRESH TOKEN ) ) except ( praw . errors . O Auth Invalid Token , praw . errors . HTTP Exception ) as e : self . log ( \"Request new Token (SAC)\" ) self . get new access information ( )", "predictions": ["render the function for a operator ."], "references": ["set the token on the reddit object again"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3810, "code": "def fix schema ( prefix , schema ) : schema dict = extract schema ( schema ) snake case organization = schema dict [ 'vendor' ] . replace ( '.' , ' ' ) . lower ( ) snake case name = re . sub ( '([^A-Z ])([A-Z])' , '\\g<1> \\g<2>' , schema dict [ 'name' ] ) . lower ( ) model = schema dict [ 'version' ] . split ( '-' ) [ 0 ] return \"{} {} {} {}\" . format ( prefix , snake case organization , snake case name , model )", "predictions": ["returns a schema variable with the passed * schema ."], "references": ["create an elasticsearch field name from a schema string"], "bleu": 0.16590387014219712, "rouge_l": 0.21254355400696867}
{"id": 3811, "code": "def transform ( line , known fields = ENRICHED EVENT FIELD TYPES , add geolocation data = True ) : return jsonify good event ( line . split ( '\\t' ) , known fields , add geolocation data )", "predictions": ["ping the request for an ( ."], "references": ["convert a snowplow enriched event tsv into a json"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 3812, "code": "def jsonify good event ( event , known fields = ENRICHED EVENT FIELD TYPES , add geolocation data = True ) : if len ( event ) != len ( known fields ) : raise Snowplow Event Transformation Exception ( [ \"Expected {} fields, received {} fields.\" . format ( len ( known fields ) , len ( event ) ) ] ) else : output = { } errors = [ ] if add geolocation data and event [ LATITUDE INDEX ] != '' and event [ LONGITUDE INDEX ] != '' : output [ 'geo location' ] = event [ LATITUDE INDEX ] + ',' + event [ LONGITUDE INDEX ] for i in range ( len ( event ) ) : key = known fields [ i ] [ 0 ] if event [ i ] != '' : try : kvpairs = known fields [ i ] [ 1 ] ( key , event [ i ] ) for kvpair in kvpairs : output [ kvpair [ 0 ] ] = kvpair [ 1 ] except Snowplow Event Transformation Exception as sete : errors += sete . error messages except Exception as e : errors += [ \"Unexpected exception parsing field with key {} and value {}: {}\" . format ( known fields [ i ] [ 0 ] , event [ i ] , repr ( e ) ) ] if errors : raise Snowplow Event Transformation Exception ( errors ) else : return output", "predictions": ["extract http event from the first time ."], "references": ["convert a snowplow enriched event in the form of an array of fields into a json"], "bleu": 0.06528905536667998, "rouge_l": 0.15721649484536082}
{"id": 3813, "code": "def print context ( self , context ) : text = [ CONTEXT TITLE ] for i , context scope in enumerate ( context ) : dump1 = linebreaksbr ( pformat django context html ( context scope ) ) dump2 = pformat dict summary html ( context scope ) if len ( context scope ) <= 3 and dump1 . count ( '<br />' ) > 20 : ( dump1 , dump2 ) = ( dump2 , dump1 ) text . append ( CONTEXT BLOCK . format ( style = PRE STYLE , num = i , dump1 = dump1 , dump2 = dump2 ) ) return u'' . join ( text )", "predictions": ["prints the 20 passed to the ( ."], "references": ["print the entire template context"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 3814, "code": "def print variables ( self , context ) : text = [ ] for name , expr in self . variables : data = '' try : if isinstance ( expr . var , Variable ) : data = expr . var . resolve ( context ) else : data = expr . resolve ( context ) except Variable Does Not Exist as e : keys = [ ] for scope in context : keys += scope . keys ( ) keys = sorted ( set ( keys ) ) return ERROR TYPE BLOCK . format ( style = PRE ALERT STYLE , error = escape ( u\"Variable '{0}' not found!  Available context variables are:\\n\\n{1}\" . format ( expr , u', ' . join ( keys ) ) ) ) else : textdata = linebreaksbr ( pformat django context html ( data ) ) if isinstance ( data , SHORT NAME TYPES ) : text . append ( BASIC TYPE BLOCK . format ( style = PRE STYLE , name = name , value = textdata ) ) else : text . append ( OBJECT TYPE BLOCK . format ( style = PRE STYLE , name = name , type = data . class . name , value = textdata ) ) return u'' . join ( text )", "predictions": ["prints to the output of a nucleotide expression ."], "references": ["print a set of variables"], "bleu": 0.15619699684601276, "rouge_l": 0.1506172839506173}
{"id": 3815, "code": "def pformat sql html ( sql ) : sql = escape ( sql ) sql = RE SQL NL . sub ( u'<br>\\n\\\\1' , sql ) sql = RE SQL . sub ( u'<strong>\\\\1</strong>' , sql ) return sql", "predictions": ["transforms the up to a sql statement ."], "references": ["highlight common sql words in a string ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 3816, "code": "def pformat dict summary html ( dict ) : if not dict : return '   {}' html = [ ] for key , value in sorted ( six . iteritems ( dict ) ) : if not isinstance ( value , DICT EXPANDED TYPES ) : value = '...' html . append ( format dict item ( key , value ) ) return mark safe ( u'<br/>' . join ( html ) )", "predictions": ["replaces ) with its values in the provided ( if any if any if necessary if the value is a column or a column is found ."], "references": ["briefly print the dictionary keys ."], "bleu": 0.04970745472800838, "rouge_l": 0.13692480359147025}
{"id": 3817, "code": "def format ( self , object , stream , indent , allowance , context , level ) : try : Pretty Printer . format ( self , object , stream , indent , allowance , context , level ) except Exception as e : stream . write ( format exception ( e ) )", "predictions": ["get a formatted string of a set of formats ."], "references": ["recursive part of the formatting"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 3818, "code": "def get organisation information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["get the size of this human readable total of the given human readable total ."], "references": ["get information fot this organisation . returns a dictionary of values ."], "bleu": 0.10343603005129705, "rouge_l": 0.30235439900867406}
{"id": 3819, "code": "def get list information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["returns the percentage of this ) percentage ."], "references": ["get information for this list . returns a dictionary of values ."], "bleu": 0.1223065774797558, "rouge_l": 0.28955696202531644}
{"id": 3820, "code": "def add card ( self , query params = None ) : card json = self . fetch json ( uri path = self . base uri + '/cards' , http method = 'POST' , query params = query params or { } ) return self . create card ( card json )", "predictions": ["adds this disk to the disk parameters ."], "references": ["create a card for this list . returns a card object ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3821, "code": "def get label information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["get the proper disk disk for this , if any ."], "references": ["get all information for this label . returns a dictionary of values ."], "bleu": 0.1420146856299917, "rouge_l": 0.3283983849259758}
{"id": 3822, "code": "def update label name ( self , name ) : label json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = { 'name' : name } ) return self . create label ( label json )", "predictions": ["updates an existing disk with an in - memory formula ."], "references": ["update the current label s name . returns a new label object ."], "bleu": 0.09497094417933137, "rouge_l": 0.08209959623149395}
{"id": 3823, "code": "def update label dict ( self , query params = { } ) : label json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = query params ) return self . create label ( label json )", "predictions": ["updates the ( ( ( ( ( ( ( ( ( ( ( ( ( w base base base base base base base base base base base base base base base base base base base base base base base base base base base base base base base base base base"], "references": ["update the current label . returns a new label object ."], "bleu": 0.02403051755364481, "rouge_l": 0.037059538274605106}
{"id": 3824, "code": "def get card information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["get the url for the url ."], "references": ["get information for this card . returns a dictionary of values ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 3825, "code": "def add comment ( self , comment text ) : return self . fetch json ( uri path = self . base uri + '/actions/comments' , http method = 'POST' , query params = { 'text' : comment text } )", "predictions": ["adds a get at this url ."], "references": ["adds a comment to this card by the current user ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 3826, "code": "def add attachment ( self , filename , open file ) : fields = { 'api key' : self . client . api key , 'token' : self . client . user auth token } content type , body = self . encode multipart formdata ( fields = fields , filename = filename , file values = open file ) return self . fetch json ( uri path = self . base uri + '/attachments' , http method = 'POST' , body = body , headers = { 'Content-Type' : content type } , )", "predictions": ["update a list of requires that the image to be rewritten ."], "references": ["adds an attachment to this card ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 3827, "code": "def add checklist ( self , query params = None ) : checklist json = self . fetch json ( uri path = self . base uri + '/checklists' , http method = 'POST' , query params = query params or { } ) return self . create checklist ( checklist json )", "predictions": ["adds a clip to this request ."], "references": ["add a checklist to this card . returns a checklist object ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 3828, "code": "def add label from dict ( self , query params = None ) : return self . fetch json ( uri path = self . base uri + '/labels' , http method = 'POST' , query params = query params or { } )", "predictions": ["adds the specified ( or passes the parameters ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) . ( . ( : ( ."], "references": ["add a label to this card from a dictionary ."], "bleu": 0.02403051755364481, "rouge_l": 0.03788819875776398}
{"id": 3829, "code": "def add label from class ( self , label = None ) : return self . fetch json ( uri path = self . base uri + '/id Labels' , http method = 'POST' , query params = { 'value' : label . id } )", "predictions": ["adds a request to this utterance ."], "references": ["add an existing label to this card ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 3830, "code": "def add member ( self , member id ) : members = self . fetch json ( uri path = self . base uri + '/id Members' , http method = 'POST' , query params = { 'value' : member id } ) members list = [ ] for member json in members : members list . append ( self . create member ( member json ) ) return members list", "predictions": ["tenant the group with the group ."], "references": ["add a member to this card . returns a list of member objects ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 3831, "code": "def create checklist item ( self , card id , checklist id , checklistitem json , * * kwargs ) : return self . client . create checklist item ( card id , checklist id , checklistitem json , * * kwargs )", "predictions": ["creates a new attrs and returns it ."], "references": ["create a checklistitem object from json object"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3832, "code": "def get board information ( self , query params = None ) : return self . fetch json ( uri path = '/boards/' + self . id , query params = query params or { } )", "predictions": ["returns the proper global global settings for this : name = - 1 if no : name is found ."], "references": ["get all information for this board . returns a dictionary of values ."], "bleu": 0.09134423666564473, "rouge_l": 0.18904958677685949}
{"id": 3833, "code": "def get checklists ( self ) : checklists = self . get Checklists Json ( self . base uri ) checklists list = [ ] for checklist json in checklists : checklists list . append ( self . create Checklist ( checklist json ) ) return checklists list", "predictions": ["method to do as a if you want to use in a ."], "references": ["get the checklists for this board . returns a list of checklist objects ."], "bleu": 0.0978840017329239, "rouge_l": 0.1471652593486128}
{"id": 3834, "code": "def update board ( self , query params = None ) : board json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = query params or { } ) return self . create board ( board json )", "predictions": ["updates results of value in value ."], "references": ["update this board s information . returns a new board ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 3835, "code": "def add list ( self , query params = None ) : list json = self . fetch json ( uri path = self . base uri + '/lists' , http method = 'POST' , query params = query params or { } ) return self . create list ( list json )", "predictions": ["adds a value to the request ."], "references": ["create a list for a board . returns a new list object ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 3836, "code": "def add label ( self , query params = None ) : list json = self . fetch json ( uri path = self . base uri + '/labels' , http method = 'POST' , query params = query params or { } ) return self . create label ( list json )", "predictions": ["adds the specified config ."], "references": ["create a label for a board . returns a new label object ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 3837, "code": "def get checklist information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["get the proper ( . . . . : the url of this method should be called for the url ."], "references": ["get all information for this checklist . returns a dictionary of values ."], "bleu": 0.0794635781571282, "rouge_l": 0.2457200402819738}
{"id": 3838, "code": "def get card ( self ) : card id = self . get checklist information ( ) . get ( 'id Card' , None ) if card id : return self . client . get card ( card id )", "predictions": ["wait for for for for for for for for for for the for the for the for the for the for the for the for the for the for the for the for the for the for for the for the for the for the for this for the for"], "references": ["get card this checklist is on ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 3839, "code": "def get item objects ( self , query params = None ) : card = self . get card ( ) checklistitems list = [ ] for checklistitem json in self . get items ( query params ) : checklistitems list . append ( self . create checklist item ( card . id , self . id , checklistitem json ) ) return checklistitems list", "predictions": ["get all items with their items ."], "references": ["get the items for this checklist . returns a list of checklistitem objects ."], "bleu": 0.0812630644213965, "rouge_l": 0.2695139911634757}
{"id": 3840, "code": "def update checklist ( self , name ) : checklist json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = { 'name' : name } ) return self . create checklist ( checklist json )", "predictions": ["updates an object in this json file ."], "references": ["update the current checklist . returns a new checklist object ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 3841, "code": "def remove item ( self , item id ) : return self . fetch json ( uri path = self . base uri + '/check Items/' + item id , http method = 'DELETE' )", "predictions": ["remove an item from the json object ."], "references": ["deletes an item from this checklist ."], "bleu": 0.3155984539112945, "rouge_l": 0.5398230088495575}
{"id": 3842, "code": "def update name ( self , name ) : checklistitem json = self . fetch json ( uri path = self . base uri + '/name' , http method = 'PUT' , query params = { 'value' : name } ) return self . create checklist item ( self . id Card , self . id Checklist , checklistitem json )", "predictions": ["update an object with an updated name ."], "references": ["rename the current checklist item . returns a new checklistitem object ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3843, "code": "def update state ( self , state ) : checklistitem json = self . fetch json ( uri path = self . base uri + '/state' , http method = 'PUT' , query params = { 'value' : 'complete' if state else 'incomplete' } ) return self . create checklist item ( self . id Card , self . id Checklist , checklistitem json )", "predictions": ["updates stats for this state ."], "references": ["set the state of the current checklist item . returns a new checklistitem object ."], "bleu": 0.054546736148076896, "rouge_l": 0.17681159420289855}
{"id": 3844, "code": "def add authorisation ( self , query params ) : query params [ 'key' ] = self . api key if self . user auth token : query params [ 'token' ] = self . user auth token return query params", "predictions": ["adds a query to the request ."], "references": ["adds the api key and user auth token to the query parameters"], "bleu": 0.13597602315271134, "rouge_l": 0.30148270181219106}
{"id": 3845, "code": "def check errors ( self , uri , response ) : if response . status == 401 : raise trolly . Unauthorised ( uri , response ) if response . status != 200 : raise trolly . Resource Unavailable ( uri , response )", "predictions": ["check the response from being redirected to the 200 ."], "references": ["check http reponse for known errors"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 3846, "code": "def build uri ( self , path , query params ) : url = 'https://api.trello.com/1' + self . clean path ( path ) url += '?' + urlencode ( query params ) return url", "predictions": ["builds a url that can be used to construct the url ."], "references": ["build the uri for the api call ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 3847, "code": "def create checklist item ( self , card id , checklist id , checklistitem json ) : return trolly . checklist . Checklist Item ( trello client = self , card id = card id , checklist id = checklist id , checklistitem id = checklistitem json [ 'id' ] . encode ( 'utf-8' ) , name = checklistitem json [ 'name' ] . encode ( 'utf-8' ) , state = checklistitem json [ 'state' ] . encode ( 'utf-8' ) )", "predictions": ["creates and writes the item in this card ."], "references": ["create a checklistitem object from json object"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3848, "code": "def set password ( self , service , username , password ) : assoc = self . generate assoc ( service , username ) password encrypted = self . encrypt ( password . encode ( 'utf-8' ) , assoc ) password base64 = '\\n' + encodebytes ( password encrypted ) . decode ( ) self . write config value ( service , username , password base64 )", "predictions": ["generate the password for the passed service ."], "references": ["write the password in the file ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 3849, "code": "def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = Command Line Tool ( ) try : return cli . run ( argv ) except Keyboard Interrupt : print ( 'Canceled' ) return 3", "predictions": ["main entry point . this method calls system . 3 . 0 . 0 . 1 . 0 . 1 . 0 . 0 . 1 . 0 . 1 . 1 . 0 ."], "references": ["main command line interface ."], "bleu": 0.03816712639899379, "rouge_l": 0.11563981042654028}
{"id": 3850, "code": "def create cipher ( self , password , salt , nonce = None ) : from argon2 . low level import hash secret raw , Type from Crypto . Cipher import AES aesmode = self . get mode ( self . aesmode ) if aesmode is None : raise Value Error ( 'invalid AES mode: %s' % self . aesmode ) key = hash secret raw ( secret = password . encode ( self . password encoding ) , salt = salt , time cost = self . time cost , memory cost = self . memory cost , parallelism = self . parallelism , hash len = 16 , type = Type . ID ) return AES . new ( key , aesmode , nonce )", "predictions": ["create a new block cipher ."], "references": ["create the cipher object to encrypt or decrypt a payload ."], "bleu": 0.12071482560966854, "rouge_l": 0.33516483516483514}
{"id": 3851, "code": "def get mode ( mode = None ) : from Crypto . Cipher import AES AES Mode Map = { 'CCM' : AES . MODE CCM , 'EAX' : AES . MODE EAX , 'GCM' : AES . MODE GCM , 'OCB' : AES . MODE OCB , } if mode is None : return AES Mode Map . keys ( ) return AES Mode Map . get ( mode )", "predictions": ["get all available settings"], "references": ["return the aes mode or a list of valid aes modes if mode == none"], "bleu": 0.01931500670555844, "rouge_l": 0.0}
{"id": 3852, "code": "def connect To Broker ( self , protocol ) : self . protocol = protocol self . protocol . on Publish = self . on Publish self . protocol . on Disconnection = self . on Disconnection self . protocol . set Window Size ( 3 ) try : yield self . protocol . connect ( \"Twisted MQTT-subs\" , keepalive = 60 ) yield self . subscribe ( ) except Exception as e : log . error ( \"Connecting to {broker} raised {excp!s}\" , broker = BROKER , excp = e ) else : log . info ( \"Connected and subscribed to {broker}\" , broker = BROKER )", "predictions": ["creates a protocol from this protocol ."], "references": ["connect to mqtt broker"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3853, "code": "def on Publish ( self , topic , payload , qos , dup , retain , msg Id ) : log . debug ( \"msg={payload}\" , payload = payload )", "predictions": ["passes the message to a specific topic ."], "references": ["callback receiving messages from publisher"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3854, "code": "def connect To Broker ( self , protocol ) : self . protocol = protocol self . protocol . on Publish = self . on Publish self . protocol . on Disconnection = self . on Disconnection self . protocol . set Window Size ( 3 ) self . task = task . Looping Call ( self . publish ) self . task . start ( 5.0 , now = False ) try : yield self . protocol . connect ( \"Twisted MQTT-pubsubs\" , keepalive = 60 ) yield self . subscribe ( ) except Exception as e : log . error ( \"Connecting to {broker} raised {excp!s}\" , broker = BROKER , excp = e ) else : log . info ( \"Connected and subscribed to {broker}\" , broker = BROKER )", "predictions": ["run this protocol to a protocol ."], "references": ["connect to mqtt broker"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3855, "code": "def make Id ( self ) : self . id = ( self . id + 1 ) % 65536 self . id = self . id or 1 return self . id", "predictions": ["makes a new object for this object ."], "references": ["produce ids for protocol packets outliving their sessions"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3856, "code": "def connect ( self , request ) : state = self . class . name return defer . fail ( MQTT State Error ( \"Unexpected connect() operation\" , state ) )", "predictions": ["connect to the backend ."], "references": ["send a connect control packet ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3857, "code": "def handle CONNACK ( self , response ) : state = self . class . name log . error ( \"Unexpected {packet:7} packet received in {log source}\" , packet = \"CONNACK\" )", "predictions": ["handle interaction interaction from exit ."], "references": ["handles connack packet from the server"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3858, "code": "def encode ( self ) : header = bytearray ( 2 ) header [ 0 ] = 0x E0 self . encoded = header return str ( header ) if PY2 else bytes ( header )", "predictions": ["encode the ( form into a canonical string ."], "references": ["encode and store a disconnect control packet ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 3859, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] version str , packet remaining = decode String ( packet remaining ) version id = int ( packet remaining [ 0 ] ) if version id == v31 [ 'level' ] : self . version = v31 else : self . version = v311 flags = packet remaining [ 1 ] self . clean Start = ( flags & 0x02 ) != 0 will Flag = ( flags & 0x04 ) != 0 will Qo S = ( flags >> 3 ) & 0x03 will Retain = ( flags & 0x20 ) != 0 user Flag = ( flags & 0x80 ) != 0 pass Flag = ( flags & 0x40 ) != 0 packet remaining = packet remaining [ 2 : ] self . keepalive = decode16Int ( packet remaining ) packet remaining = packet remaining [ 2 : ] self . client Id , packet remaining = decode String ( packet remaining ) if will Flag : self . will Retain = will Retain self . will Qo S = will Qo S self . will Topic , packet remaining = decode String ( packet remaining ) self . will Message , packet remaining = decode String ( packet remaining ) if user Flag : self . username , packet remaining = decode String ( packet remaining ) if pass Flag : l = decode16Int ( packet remaining ) self . password = packet remaining [ 2 : 2 + l ]", "predictions": ["decodes base64 data into lt byte array ."], "references": ["decode a connect control packet ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3860, "code": "def encode ( self ) : header = bytearray ( 1 ) var Header = bytearray ( 2 ) header [ 0 ] = 0x20 var Header [ 0 ] = self . session var Header [ 1 ] = self . result Code header . extend ( encode Length ( len ( var Header ) ) ) header . extend ( var Header ) self . encoded = header return str ( header ) if PY2 else bytes ( header )", "predictions": ["encode this header conforming to the original . ."], "references": ["encode and store a connack control packet ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 3861, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . session = ( packet remaining [ 0 ] & 0x01 ) == 0x01 self . result Code = int ( packet remaining [ 1 ] )", "predictions": ["decodes the base64 decoded data in input and return the decoded data in a new byte array . < p > the padding ' = ' characters at the end are removed from the base64 encoded input and returning the padding ."], "references": ["decode a connack control packet ."], "bleu": 0.03172414419318193, "rouge_l": 0.09636650868878356}
{"id": 3862, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining [ 0 : 2 ] ) self . topics = [ ] packet remaining = packet remaining [ 2 : ] while len ( packet remaining ) : topic , packet remaining = decode String ( packet remaining ) qos = int ( packet remaining [ 0 ] ) & 0x03 self . topics . append ( ( topic , qos ) ) packet remaining = packet remaining [ 1 : ]", "predictions": ["decodes a topic according to the base64 encoding . note that the rest are not needed ."], "references": ["decode a subscribe control packet ."], "bleu": 0.07994607499472013, "rouge_l": 0.19032761310452417}
{"id": 3863, "code": "def encode ( self ) : header = bytearray ( 1 ) payload = bytearray ( ) var Header = encode16Int ( self . msg Id ) header [ 0 ] = 0x90 for code in self . granted : payload . append ( code [ 0 ] | ( 0x80 if code [ 1 ] == True else 0x00 ) ) header . extend ( encode Length ( len ( var Header ) + len ( payload ) ) ) header . extend ( var Header ) header . extend ( payload ) self . encoded = header return str ( header ) if PY2 else bytes ( header )", "predictions": ["encode this header content into a basket ."], "references": ["encode and store a suback control packet ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 3864, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining [ 0 : 2 ] ) self . topics = [ ] packet remaining = packet remaining [ 2 : ] while len ( packet remaining ) : l = decode16Int ( packet remaining [ 0 : 2 ] ) topic = packet remaining [ 2 : 2 + l ] . decode ( encoding = 'utf-8' ) self . topics . append ( topic ) packet remaining = packet remaining [ 2 + l : ]", "predictions": ["decodes a topic according to the received base64 string ."], "references": ["decode a unsuback control packet ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 3865, "code": "def encode ( self ) : header = bytearray ( 1 ) var Header = encode16Int ( self . msg Id ) header [ 0 ] = 0x B0 header . extend ( encode Length ( len ( var Header ) ) ) header . extend ( var Header ) self . encoded = header return str ( header ) if PY2 else bytes ( header )", "predictions": ["encode the header header ( i . e . leave the old value ) into the http header ."], "references": ["encode and store an unsuback control packet"], "bleu": 0.06439931429457924, "rouge_l": 0.08390646492434663}
{"id": 3866, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . dup = ( packet [ 0 ] & 0x08 ) == 0x08 self . qos = ( packet [ 0 ] & 0x06 ) >> 1 self . retain = ( packet [ 0 ] & 0x01 ) == 0x01 self . topic , = decode String ( packet remaining ) topic Len = decode16Int ( packet remaining ) if self . qos : self . msg Id = decode16Int ( packet remaining [ topic Len + 2 : topic Len + 4 ] ) self . payload = packet remaining [ topic Len + 4 : ] else : self . msg Id = None self . payload = packet remaining [ topic Len + 2 : ]", "predictions": ["decodes a topic according to the received packet ."], "references": ["decode a publish control packet ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 3867, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining ) self . dup = ( packet [ 0 ] & 0x08 ) == 0x08", "predictions": ["decodes a packet from base64 notation into the base 64 encoded byte array . note that decodes this is not thread safe ."], "references": ["decode a pubrel control packet ."], "bleu": 0.06293173924458136, "rouge_l": 0.23135271807838179}
{"id": 3868, "code": "def refresh ( self ) : if self . comm . rank == 0 : self . blocks = self . list blocks ( ) else : self . blocks = None self . blocks = self . comm . bcast ( self . blocks )", "predictions": ["refresh the log area of this class ."], "references": ["refresh the list of blocks to the disk collectively"], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 3869, "code": "def get total time span ( d ) : tmax = 0 for di in d . values ( ) : if di . u Time . max ( ) > tmax : tmax = di . u Time . max ( ) return tmax", "predictions": ["returns the total time stamp for the fixed span ."], "references": ["returns total length of analysis ."], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 3870, "code": "def get defined srms ( srm file ) : srms = read table ( srm file ) return np . asanyarray ( srms . index . unique ( ) )", "predictions": ["read ( from ( ."], "references": ["returns list of srms defined in the srm database"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 3871, "code": "def read configuration ( config = 'DEFAULT' ) : , conf = read latoolscfg ( ) if config == 'DEFAULT' : config = conf [ 'DEFAULT' ] [ 'config' ] conf = dict ( conf [ config ] ) conf [ 'config' ] = config return conf", "predictions": ["read configuration from the given configuration ."], "references": ["read latools configuration file and return parameters as dict ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 3872, "code": "def print all ( ) : , conf = read latoolscfg ( ) default = conf [ 'DEFAULT' ] [ 'config' ] pstr = '\\n Currently defined L Atools configurations:\\n\\n' for s in conf . sections ( ) : if s == default : pstr += s + ' [DEFAULT]\\n' elif s == 'REPRODUCE' : pstr += s + ' [DO NOT ALTER]\\n' else : pstr += s + '\\n' for k , v in conf [ s ] . items ( ) : if k != 'config' : if v [ : 9 ] == 'resources' : v = pkgrs . resource filename ( 'latools' , v ) pstr += '   ' + k + ': ' + v + '\\n' pstr += '\\n' print ( pstr ) return", "predictions": ["prints all error info ."], "references": ["prints all currently defined configurations ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 3873, "code": "def change default ( config ) : config file , cf = read latoolscfg ( ) if config not in cf . sections ( ) : raise Value Error ( \"\\n'{:s}' is not a defined configuration.\" . format ( config ) ) if config == 'REPRODUCE' : pstr = ( 'Are you SURE you want to set REPRODUCE as your default configuration?\\n' + '     ... this is an odd thing to be doing.' ) else : pstr = ( 'Are you sure you want to change the default configuration from {:s}' . format ( cf [ 'DEFAULT' ] [ 'config' ] ) + 'to {:s}?' . format ( config ) ) response = input ( pstr + '\\n> [N/y]: ' ) if response . lower ( ) == 'y' : cf . set ( 'DEFAULT' , 'config' , config ) with open ( config file , 'w' ) as f : cf . write ( f ) print ( '  Default changed!' ) else : print ( '  Done nothing.' )", "predictions": ["remove the item from the user configuration ."], "references": ["change the default configuration ."], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 3874, "code": "def autorange plot ( self , analyte = 'total counts' , gwin = 7 , swin = None , win = 20 , on mult = [ 1.5 , 1. ] , off mult = [ 1. , 1.5 ] , transform = 'log' ) : if analyte is None : sig = self . data [ 'total counts' ] elif analyte == 'total counts' : sig = self . data [ 'total counts' ] elif analyte in self . analytes : sig = self . focus [ analyte ] else : raise Value Error ( 'Invalid analyte.' ) if transform == 'log' : sig = np . log10 ( sig ) fig , axs = plot . autorange plot ( t = self . Time , sig = sig , gwin = gwin , swin = swin , win = win , on mult = on mult , off mult = off mult ) return fig , axs", "predictions": ["get the update for this name ."], "references": ["plot a detailed autorange report for this sample ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 3875, "code": "def rangecalc ( x , y = None , pad = 0.05 ) : mn = np . nanmin ( [ np . nanmin ( x ) , np . nanmin ( y ) ] ) mx = np . nanmax ( [ np . nanmax ( x ) , np . nanmax ( y ) ] ) rn = mx - mn return ( mn - pad * rn , mx + pad * rn )", "predictions": ["create a new attachment object from an atomic point ."], "references": ["calculate padded range limits for axes ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 3876, "code": "def rangecalcx ( x , pad = 0.05 ) : mn = np . nanmin ( x ) mx = np . nanmax ( x ) rn = mx - mn return ( mn - pad * rn , mx + pad * rn )", "predictions": ["adds a new boundary to the neighborhood of this list ."], "references": ["calculate padded range limits for axes ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 3877, "code": "def gen keywords ( * args : Union [ ANSI Colors , ANSI Styles ] , * * kwargs : Union [ ANSI Colors , ANSI Styles ] ) -> tuple : fields : tuple = tuple ( ) values : tuple = tuple ( ) for tpl in args : fields += tpl . fields values += tpl for prefix , tpl in kwargs . items ( ) : fields += tuple ( map ( lambda x : ' ' . join ( [ prefix , x ] ) , tpl . fields ) ) values += tpl return namedtuple ( 'ANSI Sequences' , fields ) ( * values )", "predictions": ["build this automaton map with his !="], "references": ["generate single escape sequence mapping ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3878, "code": "def dedup ( stack : tuple ) -> tuple : reducer = lambda x , y : x if y in x else x + ( y , ) return reduce ( reducer , stack , tuple ( ) )", "predictions": ["build the build of this , with respect to the params ."], "references": ["remove duplicates from the stack in first - seen order ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 3879, "code": "def stderr ( a ) : return np . nanstd ( a ) / np . sqrt ( sum ( np . isfinite ( a ) ) )", "predictions": ["returns the create average of the input values ."], "references": ["calculate the standard error of a ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 3880, "code": "def filter nremoved ( self , filt = True , quiet = False ) : rminfo = { } for n in self . subsets [ 'All Samples' ] : s = self . data [ n ] rminfo [ n ] = s . filt nremoved ( filt ) if not quiet : max L = max ( [ len ( s ) for s in rminfo . keys ( ) ] ) print ( '{string:{number}s}' . format ( string = 'Sample ' , number = max L + 3 ) + '{total:4s}' . format ( total = 'tot' ) + '{removed:4s}' . format ( removed = 'flt' ) + '{percent:4s}' . format ( percent = '%rm' ) ) for k , ( ntot , nfilt , pcrm ) in rminfo . items ( ) : print ( '{string:{number}s}' . format ( string = k , number = max L + 3 ) + '{total:4.0f}' . format ( total = ntot ) + '{removed:4.0f}' . format ( removed = nfilt ) + '{percent:4.0f}' . format ( percent = pcrm ) ) return rminfo", "predictions": ["reorders the critical phrases according to this set of ( ."], "references": ["report how many data are removed by the active filters ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 3881, "code": "def getstats ( self , save = True , filename = None , samples = None , subset = None , ablation time = False ) : slst = [ ] if samples is not None : subset = self . make subset ( samples ) samples = self . get samples ( subset ) for s in self . stats calced : for nm in [ n for n in samples if self . srm identifier not in n ] : if self . stats [ nm ] [ s ] . ndim == 2 : reps = np . arange ( self . stats [ nm ] [ s ] . shape [ - 1 ] ) ss = np . array ( [ s ] * reps . size ) nms = np . array ( [ nm ] * reps . size ) stdf = pd . Data Frame ( self . stats [ nm ] [ s ] . T , columns = self . stats [ nm ] [ 'analytes' ] , index = [ ss , nms , reps ] ) stdf . index . set names ( [ 'statistic' , 'sample' , 'rep' ] , inplace = True ) else : stdf = pd . Data Frame ( self . stats [ nm ] [ s ] , index = self . stats [ nm ] [ 'analytes' ] , columns = [ [ s ] , [ nm ] ] ) . T stdf . index . set names ( [ 'statistic' , 'sample' ] , inplace = True ) slst . append ( stdf ) out = pd . concat ( slst ) if ablation time : ats = self . ablation times ( samples = samples , subset = subset ) ats [ 'statistic' ] = 'nanmean' ats . set index ( 'statistic' , append = True , inplace = True ) ats = ats . reorder levels ( [ 'statistic' , 'sample' , 'rep' ] ) out = out . join ( ats ) out . drop ( self . internal standard , 1 , inplace = True ) if save : if filename is None : filename = 'stat export.csv' out . to csv ( self . export dir + '/' + filename ) self . stats df = out return out", "predictions": ["copies this plot to the \" main \" . \" ."], "references": ["return pandas dataframe of all sample statistics ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 3882, "code": "def minimal export traces ( self , outdir = None , analytes = None , samples = None , subset = 'All Analyses' ) : if analytes is None : analytes = self . analytes elif isinstance ( analytes , str ) : analytes = [ analytes ] if samples is not None : subset = self . make subset ( samples ) samples = self . get samples ( subset ) focus stage = 'rawdata' if not os . path . isdir ( outdir ) : os . mkdir ( outdir ) for s in samples : d = self . data [ s ] . data [ focus stage ] out = Bunch ( ) for a in analytes : out [ a ] = d [ a ] out = pd . Data Frame ( out , index = self . data [ s ] . Time ) out . index . name = 'Time' d = dateutil . parser . parse ( self . data [ s ] . meta [ 'date' ] ) header = [ % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , , , '#' , % ( s ) , + d . strftime ( '%Y-%m-%d %H:%M:%S' ) ] header = '\\n' . join ( header ) + '\\n' csv = out . to csv ( ) with open ( '%s/%s.csv' % ( outdir , s ) , 'w' ) as f : f . write ( header ) f . write ( csv ) return", "predictions": ["will write this method will write the ( ( as a ( ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["used for exporting minimal dataset . don t use ."], "bleu": 0.026594139297659906, "rouge_l": 0.07577639751552796}
{"id": 3883, "code": "def save log ( self , directory = None , logname = None , header = None ) : if directory is None : directory = self . export dir if not os . path . isdir ( directory ) : directory = os . path . dirname ( directory ) if logname is None : logname = 'analysis.lalog' if header is None : header = self . log header ( ) loc = logging . write logfile ( self . log , header , os . path . join ( directory , logname ) ) return loc", "predictions": ["helper method to get the pagerduty mode ."], "references": ["save analysis . lalog in specified location"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3884, "code": "def calc windows ( fn , s , min points ) : max points = np . sum ( ~ np . isnan ( s ) ) n points = max points - min points out = np . full ( ( n points , s . size ) , np . nan ) ind = ~ np . isnan ( s ) s = s [ ind ] for i , w in enumerate ( range ( min points , s . size ) ) : r = rolling window ( s , w , pad = np . nan ) out [ i , ind ] = np . apply along axis ( fn , 1 , r ) return out", "predictions": ["calculates the relative windows of all points"], "references": ["apply fn to all contiguous regions in s that have at least min_points ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 3885, "code": "def calc window mean std ( s , min points , ind = None ) : max points = np . sum ( ~ np . isnan ( s ) ) n points = max points - min points mean = np . full ( ( n points , s . size ) , np . nan ) std = np . full ( ( n points , s . size ) , np . nan ) if ind is None : ind = ~ np . isnan ( s ) else : ind = ind & ~ np . isnan ( s ) s = s [ ind ] for i , w in enumerate ( range ( min points , s . size ) ) : r = rolling window ( s , w , pad = np . nan ) mean [ i , ind ] = r . sum ( 1 ) / w std [ i , ind ] = ( ( ( r - mean [ i , ind ] [ : , np . newaxis ] ) ** 2 ) . sum ( 1 ) / ( w - 1 ) ) ** 0.5 return mean , std", "predictions": ["calculates the digest of each category in the chart"], "references": ["apply fn to all contiguous regions in s that have at least min_points ."], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 3886, "code": "def bayes scale ( s ) : if sum ( ~ np . isnan ( s ) ) > 1 : bm , bv , bs = bayes mvs ( s [ ~ np . isnan ( s ) ] ) return ( s - bm . statistic ) / bs . statistic else : return np . full ( s . shape , np . nan )", "predictions": ["scale the connect state ."], "references": ["remove mean and divide by standard deviation using bayes_kvm statistics ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3887, "code": "def median scaler ( s ) : if sum ( ~ np . isnan ( s ) ) > 2 : ss = s [ ~ np . isnan ( s ) ] median = np . median ( ss ) IQR = np . diff ( np . percentile ( ss , [ 25 , 75 ] ) ) return ( s - median ) / IQR else : return np . full ( s . shape , np . nan )", "predictions": ["calculates the make the make the make the make the make the make the make the make the make the make the make the make verbose wrt the make of the multiplicative multiplicative order ."], "references": ["remove median divide by iqr ."], "bleu": 0.034487891886161, "rouge_l": 0.05591200733272227}
{"id": 3888, "code": "def clear ( self ) : self . components = { } self . info = { } self . params = { } self . switches = { } self . keys = { } self . index = { } self . sets = { } self . maxset = - 1 self . n = 0 for a in self . analytes : self . switches [ a ] = { } return", "predictions": ["get the list of dummy classes ."], "references": ["clear all filters ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3889, "code": "def clean ( self ) : for f in sorted ( self . components . keys ( ) ) : unused = not any ( self . switches [ a ] [ f ] for a in self . analytes ) if unused : self . remove ( f )", "predictions": ["returns a tuple containing this instrumented manager ."], "references": ["remove unused filters ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3890, "code": "def get info ( self ) : out = '' for k in sorted ( self . components . keys ( ) ) : out += '{:s}: {:s}' . format ( k , self . info [ k ] ) + '\\n' return ( out )", "predictions": ["get information about all available rows ."], "references": ["get info for all filters ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 3891, "code": "def log ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : a = func ( self , * args , * * kwargs ) self . log . append ( func . name + ' :: args={} kwargs={}' . format ( args , kwargs ) ) return a return wrapper", "predictions": ["decode the result of the method call ."], "references": ["function for logging method calls and parameters"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3892, "code": "def autologin ( function , timeout = TIMEOUT ) : @ wraps ( function ) async def wrapper ( self , * args , * * kwargs ) : \"\"\"Wrap a function with timeout.\"\"\" try : async with async timeout . timeout ( timeout ) : return await function ( self , * args , * * kwargs ) except ( asyncio . Timeout Error , Client Error , Error ) : pass LOGGER . debug ( \"autologin\" ) try : async with async timeout . timeout ( timeout ) : await self . login ( ) return await function ( self , * args , * * kwargs ) except ( asyncio . Timeout Error , Client Error , Error ) : raise Error ( str ( function ) ) return wrapper", "predictions": ["encode a self - existing self or encode it at the end of the self - existing self ."], "references": ["decorator that will try to login and redo an action before failing ."], "bleu": 0.06439931429457924, "rouge_l": 0.06468716861081655}
{"id": 3893, "code": "async def get information ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) result = await modem . information ( ) for sms in result . sms : pprint . pprint ( sms ) await modem . logout ( ) await websession . close ( )", "predictions": ["( re packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet ."], "references": ["example of printing the inbox ."], "bleu": 0.044915755686574035, "rouge_l": 0.06846240179573512}
{"id": 3894, "code": "async def send message ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) await modem . sms ( phone = sys . argv [ 3 ] , message = sys . argv [ 4 ] ) await modem . logout ( ) await websession . close ( )", "predictions": ["( session , logout packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet packet ."], "references": ["example of sending a message ."], "bleu": 0.044915755686574035, "rouge_l": 0.06846240179573512}
{"id": 3895, "code": "async def get information ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) try : modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) result = await modem . information ( ) print ( \"upstream: {}\" . format ( result . upstream ) ) print ( \"serial number: {}\" . format ( result . serial number ) ) print ( \"wire connected: {}\" . format ( result . wire connected ) ) print ( \"mobile connected: {}\" . format ( result . mobile connected ) ) print ( \"connection text: {}\" . format ( result . connection text ) ) print ( \"connection type: {}\" . format ( result . connection type ) ) print ( \"current nw service type: {}\" . format ( result . current nw service type ) ) print ( \"current ps service type: {}\" . format ( result . current ps service type ) ) print ( \"register network display: {}\" . format ( result . register network display ) ) print ( \"roaming: {}\" . format ( result . roaming ) ) print ( \"radio quality: {}\" . format ( result . radio quality ) ) print ( \"rx level: {}\" . format ( result . rx level ) ) print ( \"tx level: {}\" . format ( result . tx level ) ) print ( \"current band: {}\" . format ( result . current band ) ) print ( \"cell id: {}\" . format ( result . cell id ) ) await modem . logout ( ) except eternalegypt . Error : print ( \"Could not login\" ) await websession . close ( )", "predictions": ["creates a new instance of band and ."], "references": ["example of printing the current upstream ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3896, "code": "async def set failover mode ( mode ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) try : modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) await modem . set failover mode ( mode ) await modem . logout ( ) except eternalegypt . Error : print ( \"Could not login\" ) await websession . close ( )", "predictions": ["creates a new ( and . to the specified , using the specified , and logout ."], "references": ["example of printing the current upstream ."], "bleu": 0.07994607499472013, "rouge_l": 0.18020679468242246}
{"id": 3897, "code": "def nbviewer link ( url ) : if six . PY2 : from urlparse import urlparse as urlsplit else : from urllib . parse import urlsplit info = urlsplit ( url ) domain = info . netloc url type = 'github' if domain == 'github.com' else 'url' return 'https://nbviewer.jupyter.org/%s%s' % ( url type , info . path )", "predictions": ["normalizes an ( ) ) ) to a ) ."], "references": ["return the link to the jupyter nbviewer for the given notebook url"], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 3898, "code": "def thumbnail div ( self ) : return self . THUMBNAIL TEMPLATE . format ( snippet = self . get description ( ) [ 1 ] , thumbnail = self . thumb file , ref name = self . reference )", "predictions": ["attach from this object to another decode decode list of edges ."], "references": ["the string for creating the thumbnail of this example"], "bleu": 0.11498759556447223, "rouge_l": 0.09775641025641024}
{"id": 3899, "code": "def code div ( self ) : code example = self . code example if code example is None : return None return self . CODE TEMPLATE . format ( snippet = self . get description ( ) [ 1 ] , code = code example , ref name = self . reference )", "predictions": ["1 . 2 . 3 . 3 . 3 . 3 . 3 . 3 . 3 . 1 . 3 . 3 . 3 . 3 . 1 . 3 . 3 . 3 . 3 ."], "references": ["the string for creating a code example for the gallery"], "bleu": 0.0266785298043081, "rouge_l": 0.0}
{"id": 3900, "code": "def code example ( self ) : if self . code example is not None : return self . code example return getattr ( self . nb . metadata , 'code example' , None )", "predictions": ["return information about this refresh ."], "references": ["the code example out of the notebook metadata"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 3901, "code": "def supplementary files ( self ) : if self . supplementary files is not None : return self . supplementary files return getattr ( self . nb . metadata , 'supplementary files' , None )", "predictions": ["get full list of total total total total total total total total total for this class ."], "references": ["the supplementary files of this notebook"], "bleu": 0.07994607499472013, "rouge_l": 0.19032761310452417}
{"id": 3902, "code": "def other supplementary files ( self ) : if self . other supplementary files is not None : return self . other supplementary files return getattr ( self . nb . metadata , 'other supplementary files' , None )", "predictions": ["get imported files for this class ."], "references": ["the supplementary files of this notebook"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3903, "code": "def url ( self ) : if self . url is not None : url = self . url else : url = getattr ( self . nb . metadata , 'url' , None ) if url is not None : return nbviewer link ( url )", "predictions": ["fetches the read document for the specified read or other urls ."], "references": ["the url on jupyter nbviewer for this notebook or none if unknown"], "bleu": 0.1235622127262679, "rouge_l": 0.25}
{"id": 3904, "code": "def get out file ( self , ending = 'rst' ) : return os . path . splitext ( self . outfile ) [ 0 ] + os . path . extsep + ending", "predictions": ["get the path for this release ."], "references": ["get the output file with the specified ending"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 3905, "code": "def create py ( self , nb , force = False ) : if list ( map ( int , re . findall ( '\\d+' , nbconvert . version ) ) ) >= [ 4 , 2 ] : py file = os . path . basename ( self . py file ) else : py file = self . py file try : level = logger . logger . level except Attribute Error : level = logger . level spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , '--output=' + py file , '--log-level=%s' % level , self . outfile ] ) with open ( self . py file ) as f : py content = f . read ( ) py content = re . sub ( '^\\s*get ipython\\(\\).magic.*' , , py content , flags = re . MULTILINE ) with open ( self . py file , 'w' ) as f : f . write ( py content )", "predictions": ["creates a new cheatsheet ."], "references": ["create the python script from the notebook node"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3906, "code": "def data download ( self , files ) : if len ( files ) > 1 : return self . DATA DOWNLOAD % ( ( '\\n\\n' + ' ' * 8 ) + ( '\\n' + ' ' * 8 ) . join ( '* :download:`%s`' % f for f in files ) ) return self . DATA DOWNLOAD % ':download:`%s`' % files [ 0 ]", "predictions": ["download the stats for the operation ."], "references": ["create the rst string to download supplementary data"], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 3907, "code": "def create thumb ( self ) : thumbnail figure = self . copy thumbnail figure ( ) if thumbnail figure is not None : if isinstance ( thumbnail figure , six . string types ) : pic = thumbnail figure else : pic = self . pictures [ thumbnail figure ] self . save thumbnail ( pic ) else : for pic in self . pictures [ : : - 1 ] : if pic . endswith ( 'png' ) : self . save thumbnail ( pic ) return", "predictions": ["create a new thumbnail ."], "references": ["create the thumbnail for html output"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3908, "code": "def get description ( self ) : def split header ( s , get header = True ) : s = s . lstrip ( ) . rstrip ( ) parts = s . splitlines ( ) if parts [ 0 ] . startswith ( '#' ) : if get header : header = re . sub ( '#+\\s*' , '' , parts . pop ( 0 ) ) if not parts : return header , '' else : header = '' rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) desc = rest [ 0 ] . replace ( '\\n' , ' ' ) return header , desc else : if get header : if parts [ 0 ] . startswith ( ( '=' , '-' ) ) : parts = parts [ 1 : ] header = parts . pop ( 0 ) if parts and parts [ 0 ] . startswith ( ( '=' , '-' ) ) : parts . pop ( 0 ) if not parts : return header , '' else : header = '' rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) desc = rest [ 0 ] . replace ( '\\n' , ' ' ) return header , desc first cell = self . nb [ 'cells' ] [ 0 ] if not first cell [ 'cell type' ] == 'markdown' : return '' , '' header , desc = split header ( first cell [ 'source' ] ) if not desc and len ( self . nb [ 'cells' ] ) > 1 : second cell = self . nb [ 'cells' ] [ 1 ] if second cell [ 'cell type' ] == 'markdown' : , desc = split header ( second cell [ 'source' ] , False ) return header , desc", "predictions": ["returns a dictionary containing field values ."], "references": ["get summary and description of this notebook"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3909, "code": "def save thumbnail ( self , image path ) : thumb dir = os . path . join ( os . path . dirname ( image path ) , 'thumb' ) create dirs ( thumb dir ) thumb file = os . path . join ( thumb dir , '%s thumb.png' % self . reference ) if os . path . exists ( image path ) : logger . info ( 'Scaling %s to thumbnail %s' , image path , thumb file ) self . scale image ( image path , thumb file , 400 , 280 ) self . thumb file = thumb file", "predictions": ["save the thumbnail image ."], "references": ["save the thumbnail image"], "bleu": 0.7598356856515925, "rouge_l": 0.9070631970260222}
{"id": 3910, "code": "def copy thumbnail figure ( self ) : ret = None if self . thumbnail figure is not None : if not isstring ( self . thumbnail figure ) : ret = self . thumbnail figure else : ret = osp . join ( osp . dirname ( self . outfile ) , osp . basename ( self . thumbnail figure ) ) copyfile ( self . thumbnail figure , ret ) return ret elif hasattr ( self . nb . metadata , 'thumbnail figure' ) : if not isstring ( self . nb . metadata . thumbnail figure ) : ret = self . nb . metadata . thumbnail figure else : ret = osp . join ( osp . dirname ( self . outfile ) , 'images' , osp . basename ( self . nb . metadata . thumbnail figure ) ) copyfile ( osp . join ( osp . dirname ( self . infile ) , self . nb . metadata . thumbnail figure ) , ret ) return ret", "predictions": ["copies the thumbnail by copying all child components of this thumbnail to a thumbnail ."], "references": ["the integer of the thumbnail figure"], "bleu": 0.11633270842295028, "rouge_l": 0.3096446700507614}
{"id": 3911, "code": "def get db change languages ( self , field name , db table fields ) : for lang code , lang name in get languages ( ) : if get real fieldname ( field name , lang code ) not in db table fields : yield lang code for db table field in db table fields : pattern = re . compile ( '^%s (?P<lang>\\w{2})$' % field name ) m = pattern . match ( db table field ) if not m : continue lang = m . group ( 'lang' ) yield lang", "predictions": ["generate a field of the database ."], "references": ["get only db changes fields"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3912, "code": "def pre save ( self , model instance , add ) : file = getattr ( model instance , self . attname ) if file and not file . committed : image file = file if self . resize source to : file . seek ( 0 ) image file = processors . process ( file , self . resize source to ) image file = post processors . process ( image file , self . resize source to ) filename = str ( shortuuid . uuid ( ) ) + os . path . splitext ( file . name ) [ 1 ] file . save ( filename , image file , save = False ) return file", "predictions": ["save and resize a file ."], "references": ["process the source image through the defined processors ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 3913, "code": "def refresh cache ( self ) : self . thumbnails = { } metadatas = self . metadata backend . get thumbnails ( self . source image . name ) for metadata in metadatas : self . thumbnails [ metadata . size ] = Thumbnail ( metadata = metadata , storage = self . storage )", "predictions": ["generate and return the storage cache for this backend ."], "references": ["populate self . _thumbnails ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 3914, "code": "def all ( self ) : if self . thumbnails is not None : return self . thumbnails self . refresh cache ( ) return self . thumbnails", "predictions": ["decorator for authentication class ."], "references": ["return all thumbnails in a dict format ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3915, "code": "def create ( self , size ) : thumbnail = images . create ( self . source image . name , size , self . metadata backend , self . storage ) return thumbnail", "predictions": ["creates a thumbnail for the currently running thumbnail ."], "references": ["creates and return a thumbnail of a given size ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 3916, "code": "def delete ( self , size ) : images . delete ( self . source image . name , size , self . metadata backend , self . storage ) del ( self . thumbnails [ size ] )", "predictions": ["deletes the amount of files that are associated with this object ."], "references": ["deletes a thumbnail of a given size"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 3917, "code": "def get ( source name , size , metadata backend = None , storage backend = None ) : if storage backend is None : storage backend = backends . storage . get backend ( ) if metadata backend is None : metadata backend = backends . metadata . get backend ( ) metadata = metadata backend . get thumbnail ( source name , size ) if metadata is None : return None else : return Thumbnail ( metadata = metadata , storage = storage backend )", "predictions": ["get metadata about the given name ."], "references": ["returns a thumbnail instance or none if thumbnail does not yet exist ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 3918, "code": "def delete ( source name , size , metadata backend = None , storage backend = None ) : if storage backend is None : storage backend = backends . storage . get backend ( ) if metadata backend is None : metadata backend = backends . metadata . get backend ( ) storage backend . delete ( get thumbnail name ( source name , size ) ) metadata backend . delete thumbnail ( source name , size )", "predictions": ["delete a metadata entry ."], "references": ["deletes a thumbnail file and its relevant metadata ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 3919, "code": "def jsonex api ( f ) : @ wraps ( f ) def wrapper ( * args , * * kwargs ) : try : code , res = 200 , f ( * args , * * kwargs ) except HTTP Exception as e : code , res = e . code , { 'error' : e } except Exception as e : code , res = 500 , { 'error' : e } logger . exception ( 'Method error' ) response = make response ( jsonex dumps ( res ) , code ) response . headers [ 'Content-Type' ] = 'application/json' return response return wrapper", "predictions": ["decorator to help jsonex call ."], "references": ["view wrapper for jsonex responses . catches exceptions as well"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3920, "code": "def estimate tx gas with web3 ( self , safe address : str , to : str , value : int , data : bytes ) -> int : return self . ethereum client . estimate gas ( safe address , to , value , data , block identifier = 'pending' )", "predictions": ["estimate the gas by this setting ."], "references": ["estimate tx gas using web3"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3921, "code": "def has bad headers ( self , default from = None ) : sender = self . sender or default from reply to = self . reply to or '' for val in [ self . subject , sender , reply to ] + self . recipients : for c in '\\r\\n' : if c in val : return True return False", "predictions": ["checks to see if the this set of headers has a bad sender ."], "references": ["checks for bad headers i . e . newlines in subject sender or recipients ."], "bleu": 0.10831305487476968, "rouge_l": 0.2741573033707865}
{"id": 3922, "code": "def from module ( module name ) : d = importlib . import module ( module name ) config = { } for key in dir ( d ) : if key . isupper ( ) : config [ key ] = getattr ( d , key ) return Config ( config )", "predictions": ["retrieve all items of a module from a module ."], "references": ["load a configuration module and return a config"], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 3923, "code": "def register resources ( self , * * resources ) : for key , resource in resources . items ( ) : if key in self . resources : raise Already Exists Exception ( 'A Service for {} is already registered.' . format ( key ) ) self . init resource ( key , resource )", "predictions": ["register language handlers to be registered as a enum for the ui ."], "references": ["register resources with the resourcemanager ."], "bleu": 0.1135935489027116, "rouge_l": 0.3382624768946396}
{"id": 3924, "code": "def require ( self , key ) : value = self . get ( key ) if not value : raise Value Error ( '\"{}\" is empty.' . format ( key ) ) return value", "predictions": ["this decorator should be called after a request ' s name has been read ."], "references": ["raises an exception if value for key is empty ."], "bleu": 0.08225964699966554, "rouge_l": 0.08299319727891155}
{"id": 3925, "code": "def exit ( self , obj , type , value , traceback ) : if type is None : try : obj . next ( ) except Stop Iteration : return else : raise Runtime Error ( '{} yielded more than once.' . format ( obj ) ) else : try : obj . throw ( type , value , traceback ) raise Runtime Error ( '{} did not close after throw()' . format ( obj ) ) except Stop Iteration as exc : return exc is not value except : # if sys . exc info ( ) [ 1 ] is not value : raise", "predictions": [". for serializable reasons ."], "references": ["teardown a resource or middleware ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3926, "code": "def samefile ( path1 , path2 ) : info1 = fs . getfileinfo ( path1 ) info2 = fs . getfileinfo ( path2 ) return ( info1 . dw Volume Serial Number == info2 . dw Volume Serial Number and info1 . n File Index High == info2 . n File Index High and info1 . n File Index Low == info2 . n File Index Low )", "predictions": ["gets the . from the filesystem ."], "references": ["returns true if path1 and path2 refer to the same file ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 3927, "code": "def create ( source , link name ) : success = False if not os . path . isdir ( source ) : raise Exception ( \"%s is not a directory\" % source ) if os . path . exists ( link name ) : raise Exception ( \"%s: junction link name already exists\" % link name ) link name = os . path . abspath ( link name ) os . mkdir ( link name ) hlink = Create File ( link name , fs . GENERIC WRITE , fs . FILE SHARE READ | fs . FILE SHARE WRITE , None , fs . OPEN EXISTING , fs . FILE FLAG OPEN REPARSE POINT | fs . FILE FLAG BACKUP SEMANTICS , None ) try : if hlink == fs . INVALID HANDLE VALUE : raise Win Error ( ) srcvolpath = unparsed convert ( source ) ( junctioninfo , infolen ) = new junction reparse buffer ( srcvolpath ) dummy = DWORD ( 0 ) res = Device Io Control ( hlink , FSCTL SET REPARSE POINT , byref ( junctioninfo ) , infolen , None , 0 , byref ( dummy ) , None ) if res == 0 : raise Win Error ( ) success = True finally : if hlink != fs . INVALID HANDLE VALUE : Close Handle ( hlink ) if not success : os . rmdir ( link name )", "predictions": ["create the link in the given link"], "references": ["create a junction at link_name pointing to source ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3928, "code": "def initialize logger ( args ) : global log filename log filename = os . path . join ( os . getcwd ( ) , \"jacquard.log\" ) if args . log file : validate log file ( args . log file ) log filename = args . log file logging . basic Config ( format = FILE LOG FORMAT , level = \"DEBUG\" , datefmt = DATE FORMAT , filename = log filename ) global verbose if args . verbose : verbose = args . verbose start time = datetime . now ( ) . strftime ( DATE FORMAT ) global logging dict logging dict = { 'user' : getpass . getuser ( ) , 'host' : socket . gethostname ( ) , 'start time' : start time , 'tool' : args . subparser name }", "predictions": ["initializes all private logs ."], "references": ["sets command name and formatting for subsequent calls to logger"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 3929, "code": "def error ( self , message ) : message = self . remessage invalid subparser ( message ) raise utils . Usage Error ( message )", "predictions": ["method to raise a message with this message ."], "references": ["suppress default exit behavior"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3930, "code": "def read ( * paths ) : with open ( os . path . join ( * paths ) , 'r' ) as filename : return filename . read ( )", "predictions": ["read the paths of the pojo ."], "references": ["build a file path from * paths * and return the contents ."], "bleu": 0.09374222649442905, "rouge_l": 0.2846034214618974}
{"id": 3931, "code": "def prefix line terminator ( self , data ) : for t in self . LINE TERMINATORS : if data . startswith ( t ) : return t return None", "predictions": ["get terminator of data ."], "references": ["return line terminator data begins with or none ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 3932, "code": "def suffix line terminator ( self , data ) : for t in self . LINE TERMINATORS : if data . endswith ( t ) : return t return None", "predictions": ["get the suffix for this line ."], "references": ["return line terminator data ends with or none ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3933, "code": "def tail ( self , lines = 10 ) : self . file . seek ( 0 , SEEK END ) for i in range ( lines ) : if self . seek previous line ( ) == - 1 : break data = self . file . read ( ) for t in self . LINE TERMINATORS : if data . endswith ( t ) : data = data [ : - len ( t ) ] break if data : return self . splitlines ( data ) else : return [ ]", "predictions": ["compute tail of this utterance ."], "references": ["return the last lines of the file ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3934, "code": "def head ( self , lines = 10 ) : self . file . seek ( 0 ) for i in range ( lines ) : if self . seek next line ( ) == - 1 : break end pos = self . file . tell ( ) self . file . seek ( 0 ) data = self . file . read ( end pos ) for t in self . LINE TERMINATORS : if data . endswith ( t ) : data = data [ : - len ( t ) ] break if data : return self . splitlines ( data ) else : return [ ]", "predictions": ["returns just the head of this method ."], "references": ["return the top lines of the file ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 3935, "code": "def format tags ( self ) : tags = Vcf Record . EMPTY SET if self . sample tag values : first sample = list ( self . sample tag values . keys ( ) ) [ 0 ] tags = set ( self . sample tag values [ first sample ] . keys ( ) ) return tags", "predictions": ["formats a list of tags ."], "references": ["returns set of format tags ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 3936, "code": "def join info fields ( self ) : if self . info dict : info fields = [ ] if len ( self . info dict ) > 1 : self . info dict . pop ( \".\" , None ) for field , value in self . info dict . items ( ) : if field == value : info fields . append ( value ) else : info fields . append ( \"=\" . join ( [ field , value ] ) ) self . info = \";\" . join ( info fields ) else : self . info = \".\"", "predictions": ["joins file or column titles ."], "references": ["updates info attribute from info dict ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3937, "code": "def format field ( self ) : format field = \".\" if self . sample tag values : first sample = list ( self . sample tag values . keys ( ) ) [ 0 ] tag names = self . sample tag values [ first sample ] . keys ( ) if tag names : format field = \":\" . join ( tag names ) return format field", "predictions": ["formats and returns map of py objects ."], "references": ["returns string representation of format field ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 3938, "code": "def text ( self ) : stringifier = [ self . chrom , self . pos , self . vcf id , self . ref , self . alt , self . qual , self . filter , self . info , self . format field ( ) ] for sample in self . sample tag values : stringifier . append ( self . sample field ( sample ) ) return \"\\t\" . join ( stringifier ) + \"\\n\"", "predictions": ["get a data set for this object ."], "references": ["returns tab - delimited newline terminated string of vcfrecord ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 3939, "code": "def add or replace filter ( self , new filter ) : if self . filter . lower ( ) in self . FILTERS TO REPLACE : self . filter = new filter elif new filter not in self . filter . split ( \";\" ) : self . filter = \";\" . join ( [ self . filter , new filter ] )", "predictions": ["this method is called to create a shallow self - loop operation ."], "references": ["replaces null or blank filter or adds filter to existing list ."], "bleu": 0.10571070857151538, "rouge_l": 0.16116248348745044}
{"id": 3940, "code": "def add product error ( self , product , error ) : self . add error ( self . field name ( product ) , error )", "predictions": ["method to get the description of the given list ."], "references": ["adds an error to the given product s field"], "bleu": 0.17827531042796255, "rouge_l": 0.31881533101045295}
{"id": 3941, "code": "def model fields form factory ( model ) : fields = model . meta . get fields ( ) choices = [ ] for field in fields : if hasattr ( field , \"verbose name\" ) : choices . append ( ( field . name , field . verbose name ) ) class Model Fields Form ( forms . Form ) : fields = forms . Multiple Choice Field ( choices = choices , required = False , ) return Model Fields Form", "predictions": ["returns a list of form fields for all fields ."], "references": ["creates a form for specifying fields from a model to display ."], "bleu": 0.13583060054007276, "rouge_l": 0.44721407624633425}
{"id": 3942, "code": "def items pending or purchased ( self ) : status = [ commerce . Cart . STATUS PAID , commerce . Cart . STATUS ACTIVE ] return self . items ( status )", "predictions": ["outfile copy of this encryption/decryption ."], "references": ["returns the items that this user has purchased or has pending ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 3943, "code": "def iter osm notes ( feed limit = 25 , interval = 60 , parse timestamps = True ) : last seen guid = None while True : u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed limit ) tree = etree . parse ( u ) new notes = [ ] for note item in tree . xpath ( '/rss/channel/item' ) : title = note item . xpath ( 'title' ) [ 0 ] . text if title . startswith ( 'new note (' ) : action = 'create' elif title . startswith ( 'new comment (' ) : action = 'comment' elif title . startswith ( 'closed note (' ) : action = 'close' guid = note item . xpath ( 'link' ) [ 0 ] . text if last seen guid == guid : break elif last seen guid == None : last seen guid = guid else : note id = int ( guid . split ( '/' ) [ - 1 ] . split ( '#c' ) [ 0 ] ) new notes . append ( ( action , get note ( note id , parse timestamps ) ) ) for note in reversed ( new notes ) : yield note yield model . Finished ( None , None ) time . sleep ( interval )", "predictions": ["yield all db contents ."], "references": ["parses the global osm notes feed and yields as much note information as possible ."], "bleu": 0.0369481680224917, "rouge_l": 0.09172932330827067}
{"id": 3944, "code": "def passes filter ( self , user ) : cls = type ( self . condition ) qs = cls . objects . filter ( pk = self . condition . id ) return self . condition in self . pre filter ( qs , user )", "predictions": ["pre - pre - subscribed model and pre - getattr pre - persistent store ."], "references": ["returns true if the condition passes the filter"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3945, "code": "def apply voucher ( self , voucher code ) : voucher = inventory . Voucher . objects . get ( code = voucher code . upper ( ) ) if voucher in self . cart . vouchers . all ( ) : return self . test voucher ( voucher ) self . cart . vouchers . add ( voucher )", "predictions": ["refresh an cache object ."], "references": ["applies the voucher with the given code to this cart ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3946, "code": "def recalculate discounts ( self ) : commerce . Discount Item . objects . filter ( cart = self . cart ) . delete ( ) product items = self . cart . productitem set . all ( ) . select related ( \"product\" , \"product category\" ) . order by ( \"-product price\" ) products = [ i . product for i in product items ] discounts = Discount Controller . available discounts ( self . cart . user , [ ] , products , ) for item in product items : self . add discount ( item . product , item . quantity , discounts )", "predictions": ["compares all products for the specified refresh ."], "references": ["calculates all of the discounts available for this product ."], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 3947, "code": "def rows ( self , content type ) : for row in self . data : yield [ self . cell text ( content type , i , cell ) for i , cell in enumerate ( row ) ]", "predictions": ["a generator that returns a generator of the create create ones ."], "references": ["returns the data rows for the table ."], "bleu": 0.1235622127262679, "rouge_l": 0.3112244897959184}
{"id": 3948, "code": "def get form ( self , request ) : if self . form type is not None : form = self . form type ( request . GET ) form . is valid ( ) else : form = None return form", "predictions": ["get ( or ] : upwards : \\ r \\ brief ( ) or ( yet : / / . . com / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ("], "references": ["creates an instance of self . form_type using request . get"], "bleu": 0.028577262451992175, "rouge_l": 0.07411907654921021}
{"id": 3949, "code": "def reports list ( request ) : reports = [ ] for report in get all reports ( ) : reports . append ( { \"name\" : report . name , \"url\" : reverse ( report ) , \"description\" : report . doc , } ) reports . sort ( key = lambda report : report [ \"name\" ] ) ctx = { \"reports\" : reports , } return render ( request , \"registrasion/reports list.html\" , ctx )", "predictions": ["( possibly created , storage , storage , storage , storage , storage , etc , etc , etc , etc , etc . , . , . , . , . , . , . , . , . , . , . , . , . , ."], "references": ["lists all of the reports currently available ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 3950, "code": "def sales payment summary ( ) : def value or zero ( aggregate , key ) : return aggregate [ key ] or 0 def sum amount ( payment set ) : a = payment set . values ( \"amount\" ) . aggregate ( total = Sum ( \"amount\" ) ) return value or zero ( a , \"total\" ) headings = [ \"Category\" , \"Total\" ] data = [ ] sales = commerce . Line Item . objects . filter ( invoice status = commerce . Invoice . STATUS PAID , ) . values ( \"price\" , \"quantity\" ) . aggregate ( total = Sum ( F ( \"price\" ) * F ( \"quantity\" ) , output field = CURRENCY ( ) ) , ) sales = value or zero ( sales , \"total\" ) all payments = sum amount ( commerce . Payment Base . objects . all ( ) ) all credit notes = 0 - sum amount ( commerce . Credit Note . objects . all ( ) ) unclaimed credit notes = 0 - sum amount ( commerce . Credit Note . unclaimed ( ) ) claimed credit notes = sum amount ( commerce . Credit Note Application . objects . all ( ) ) refunded credit notes = 0 - sum amount ( commerce . Credit Note . refunded ( ) ) data . append ( [ \"Items on paid invoices\" , sales ] ) data . append ( [ \"All payments\" , all payments ] ) data . append ( [ \"Sales - Payments \" , sales - all payments ] ) data . append ( [ \"All credit notes\" , all credit notes ] ) data . append ( [ \"Credit notes paid on invoices\" , claimed credit notes ] ) data . append ( [ \"Credit notes refunded\" , refunded credit notes ] ) data . append ( [ \"Unclaimed credit notes\" , unclaimed credit notes ] ) data . append ( [ \"Credit notes - (claimed credit notes + unclaimed credit notes)\" , all credit notes - claimed credit notes - refunded credit notes - unclaimed credit notes ] ) return List Report ( \"Sales and Payments Summary\" , headings , data )", "predictions": [". the ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( 1 , invoice , invoice , ( , ( , ( , ( , ( , ( , ( , ( , ( , . , . ,"], "references": ["summarises paid items and payments ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 3951, "code": "def payments ( ) : payments = commerce . Payment Base . objects . all ( ) return Queryset Report ( \"Payments\" , [ \"invoice id\" , \"id\" , \"reference\" , \"amount\" ] , payments , link view = views . invoice , )", "predictions": ["payments all } in the } ."], "references": ["shows the history of payments into the system"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3952, "code": "def credit note refunds ( ) : notes refunded = commerce . Credit Note . refunded ( ) return Queryset Report ( \"Credit note refunds\" , [ \"id\" , \"creditnoterefund reference\" , \"amount\" ] , notes refunded , link view = views . credit note , )", "predictions": ["generate a tx for the given estimate ."], "references": ["shows all of the credit notes that have been generated ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 3953, "code": "def discount status ( request , form ) : discounts = form . cleaned data [ \"discount\" ] items = commerce . Discount Item . objects . filter ( Q ( discount in = discounts ) , ) . select related ( \"cart\" , \"product\" , \"product category\" ) items = group by cart status ( items , [ \"discount\" ] , [ \"discount\" , \"discount description\" ] , ) headings = [ \"Discount\" , \"Paid\" , \"Reserved\" , \"Unreserved\" , \"Refunded\" , ] data = [ ] for item in items : data . append ( [ item [ \"discount description\" ] , item [ \"total paid\" ] , item [ \"total reserved\" ] , item [ \"total unreserved\" ] , item [ \"total refunded\" ] , ] ) return List Report ( \"Usage by item\" , headings , data )", "predictions": ["creates a has to has a has a has been written to the has already been written ."], "references": ["summarises the usage of a given discount ."], "bleu": 0.08097785064266204, "rouge_l": 0.16531165311653115}
{"id": 3954, "code": "def credit notes ( request , form ) : notes = commerce . Credit Note . objects . all ( ) . select related ( \"creditnoterefund\" , \"creditnoteapplication\" , \"invoice\" , \"invoice user attendee attendeeprofilebase\" , ) return Queryset Report ( \"Credit Notes\" , [ \"id\" , \"invoice user attendee attendeeprofilebase invoice recipient\" , \"status\" , \"value\" ] , notes , headings = [ \"id\" , \"Owner\" , \"Status\" , \"Value\" ] , link view = views . credit note , )", "predictions": ["module free context for a from the database ."], "references": ["shows all of the credit notes in the system ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 3955, "code": "def invoices ( request , form ) : invoices = commerce . Invoice . objects . all ( ) . order by ( \"status\" , \"id\" ) return Queryset Report ( \"Invoices\" , [ \"id\" , \"recipient\" , \"value\" , \"get status display\" ] , invoices , headings = [ \"id\" , \"Recipient\" , \"Value\" , \"Status\" ] , link view = views . invoice , )", "predictions": ["register a register with the same parameters as the register ."], "references": ["shows all of the invoices in the system ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 3956, "code": "def attendee list ( request ) : attendees = people . Attendee . objects . select related ( \"attendeeprofilebase\" , \"user\" , ) profiles = Attendee Profile . objects . filter ( attendee in = attendees ) . select related ( \"attendee\" , \"attendee user\" , ) profiles by attendee = dict ( ( i . attendee , i ) for i in profiles ) attendees = attendees . annotate ( has registered = Count ( Q ( user invoice status = commerce . Invoice . STATUS PAID ) ) , ) headings = [ \"User ID\" , \"Name\" , \"Email\" , \"Has registered\" , ] data = [ ] for a in attendees : data . append ( [ a . user . id , ( profiles by attendee [ a ] . attendee name ( ) if a in profiles by attendee else \"\" ) , a . user . email , a . has registered > 0 , ] ) data . sort ( key = lambda a : ( - a [ 3 ] , a [ 0 ] ) ) return Attendee List Report ( \"Attendees\" , headings , data , link view = attendee )", "predictions": ["( possibly re key , ( possibly re key , annotate key , annotate key key key , annotate key key key key , list key , list key , list key , value key key = value key key = > key = > ( 1 key = value"], "references": ["returns a list of all attendees ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 3957, "code": "def speaker registrations ( request , form ) : kinds = form . cleaned data [ \"kind\" ] presentations = schedule models . Presentation . objects . filter ( proposal base kind in = kinds , ) . exclude ( cancelled = True , ) users = User . objects . filter ( Q ( speaker profile presentations in = presentations ) | Q ( speaker profile copresentations in = presentations ) ) paid carts = commerce . Cart . objects . filter ( status = commerce . Cart . STATUS PAID ) paid carts = Case ( When ( cart in = paid carts , then = Value ( 1 ) ) , default = Value ( 0 ) , output field = models . Integer Field ( ) , ) users = users . annotate ( paid carts = Sum ( paid carts ) ) users = users . order by ( \"paid carts\" ) return Queryset Report ( \"Speaker Registration Status\" , [ \"id\" , \"speaker profile name\" , \"email\" , \"paid carts\" ] , users , link view = attendee , ) return [ ]", "predictions": ["creates a proposal from all ) and sends it to the database ."], "references": ["shows registration status for speakers with a given proposal kind ."], "bleu": 0.1135935489027116, "rouge_l": 0.2538141470180305}
{"id": 3958, "code": "def missing categories ( context ) : user = user for context ( context ) categories available = set ( Category Controller . available categories ( user ) ) items = Item Controller ( user ) . items pending or purchased ( ) categories held = set ( ) for product , quantity in items : categories held . add ( product . category ) return categories available - categories held", "predictions": ["returns the set of ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["adds the categories that the user does not currently have ."], "bleu": 0.02403051755364481, "rouge_l": 0.037059538274605106}
{"id": 3959, "code": "def voucher code ( request ) : VOUCHERS FORM PREFIX = \"vouchers\" v = handle voucher ( request , VOUCHERS FORM PREFIX ) voucher form , voucher handled = v if voucher handled : messages . success ( request , \"Your voucher code was accepted.\" ) return redirect ( \"dashboard\" ) data = { \"voucher form\" : voucher form , } return render ( request , \"registrasion/voucher code.html\" , data )", "predictions": ["create the create ( ) ( issued by the given , but with the given , if it is a create a create a create ( . . . link ( link link link link link link link link link link link link link link link to the create ("], "references": ["a view * just * for entering a voucher form ."], "bleu": 0.028577262451992175, "rouge_l": 0.11117861482381533}
{"id": 3960, "code": "def amend registration ( request , user id ) : user = User . objects . get ( id = int ( user id ) ) current cart = Cart Controller . for user ( user ) items = commerce . Product Item . objects . filter ( cart = current cart . cart , ) . select related ( \"product\" ) initial = [ { \"product\" : i . product , \"quantity\" : i . quantity } for i in items ] Staff Products Form Set = forms . staff products formset factory ( user ) formset = Staff Products Form Set ( request . POST or None , initial = initial , prefix = \"products\" , ) for item , form in zip ( items , formset ) : queryset = inventory . Product . objects . filter ( id = item . product . id ) form . fields [ \"product\" ] . queryset = queryset voucher form = forms . Voucher Form ( request . POST or None , prefix = \"voucher\" , ) if request . POST and formset . is valid ( ) : pq = [ ( f . cleaned data [ \"product\" ] , f . cleaned data [ \"quantity\" ] ) for f in formset if \"product\" in f . cleaned data and f . cleaned data [ \"product\" ] is not None ] try : current cart . set quantities ( pq ) return redirect ( amend registration , user id ) except Validation Error as ve : for ve field in ve . error list : product , message = ve field . message for form in formset : if \"product\" not in form . cleaned data : continue if form . cleaned data [ \"product\" ] == product : form . add error ( \"quantity\" , message ) if request . POST and voucher form . has changed ( ) and voucher form . is valid ( ) : try : current cart . apply voucher ( voucher form . cleaned data [ \"voucher\" ] ) return redirect ( amend registration , user id ) except Validation Error as ve : voucher form . add error ( None , ve ) ic = Item Controller ( user ) data = { \"user\" : user , \"paid\" : ic . items purchased ( ) , \"cancelled\" : ic . items released ( ) , \"form\" : formset , \"voucher form\" : voucher form , } return render ( request , \"registrasion/amend registration.html\" , data )", "predictions": ["select an initial cart or logger for the users cart ."], "references": ["allows staff to amend a user s current registration cart and etc etc ."], "bleu": 0.09596928383261212, "rouge_l": 0.15661103979460847}
{"id": 3961, "code": "def extend reservation ( request , user id , days = 7 ) : user = User . objects . get ( id = int ( user id ) ) cart = Cart Controller . for user ( user ) cart . extend reservation ( datetime . timedelta ( days = days ) ) return redirect ( request . META [ \"HTTP REFERER\" ] )", "predictions": ["sends an ) to the bulk cart ."], "references": ["allows staff to extend the reservation on a given user s cart ."], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 3962, "code": "def invoice mailout ( request ) : category = request . GET . getlist ( \"category\" , [ ] ) product = request . GET . getlist ( \"product\" , [ ] ) status = request . GET . get ( \"status\" ) form = forms . Invoice Email Form ( request . POST or None , category = category , product = product , status = status , ) emails = [ ] if form . is valid ( ) : emails = [ ] for invoice in form . cleaned data [ \"invoice\" ] : from email = form . cleaned data [ \"from email\" ] subject = form . cleaned data [ \"subject\" ] body = Template ( form . cleaned data [ \"body\" ] ) . render ( Context ( { \"invoice\" : invoice , \"user\" : invoice . user , } ) ) recipient list = [ invoice . user . email ] emails . append ( Email ( subject , body , from email , recipient list ) ) if form . cleaned data [ \"action\" ] == forms . Invoice Email Form . ACTION SEND : send mass mail ( emails ) messages . info ( request , \"The e-mails have been sent.\" ) data = { \"form\" : form , \"emails\" : emails , } return render ( request , \"registrasion/invoice mailout.html\" , data )", "predictions": [". is a list of ( for the read ."], "references": ["allows staff to send emails to users based on their invoice status ."], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 3963, "code": "def render badge ( user ) : data = { \"user\" : user , } t = loader . get template ( 'registrasion/badge.svg' ) return t . render ( data )", "predictions": ["prefix the given ( or vcenter self self self self self self self self self self self self self - data self ."], "references": ["renders a single user s badge ."], "bleu": 0.05291907393644996, "rouge_l": 0.07376058041112453}
{"id": 3964, "code": "def generate from cart ( cls , cart ) : cart . refresh from db ( ) product items = commerce . Product Item . objects . filter ( cart = cart ) product items = product items . select related ( \"product\" , \"product category\" , ) product items = product items . order by ( \"product category order\" , \"product order\" ) if len ( product items ) == 0 : raise Validation Error ( \"Your cart is empty.\" ) discount items = commerce . Discount Item . objects . filter ( cart = cart ) discount items = discount items . select related ( \"discount\" , \"product\" , \"product category\" , ) def format product ( product ) : return \"%s - %s\" % ( product . category . name , product . name ) def format discount ( discount , product ) : description = discount . description return \"%s (%s)\" % ( description , format product ( product ) ) line items = [ ] for item in product items : product = item . product line item = commerce . Line Item ( description = format product ( product ) , quantity = item . quantity , price = product . price , product = product , ) line items . append ( line item ) for item in discount items : line item = commerce . Line Item ( description = format discount ( item . discount , item . product ) , quantity = item . quantity , price = cls . resolve discount value ( item ) * - 1 , product = item . product , ) line items . append ( line item ) min due time = cart . reservation duration + cart . time last updated return cls . generate ( cart . user , cart , min due time , line items )", "predictions": ["generates all due for a terminator ."], "references": ["generates an invoice for the given cart ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 3965, "code": "def apply credit notes ( cls , invoice ) : invoices = commerce . Invoice . objects . filter ( user = invoice . user , status = commerce . Invoice . STATUS UNPAID , ) if invoices . count ( ) > 1 : return notes = commerce . Credit Note . unclaimed ( ) . filter ( invoice user = invoice . user ) for note in notes : try : Credit Note Controller ( note ) . apply to invoice ( invoice ) except Validation Error : break invoice . refresh from db ( )", "predictions": ["tail all the ("], "references": ["applies the user s credit notes to the given invoice on creation ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 3966, "code": "def refresh ( self ) : self . invoice . refresh from db ( ) if self . invoice . cart : self . invoice . cart . refresh from db ( )", "predictions": ["set 10 10 object to be called under the file system ."], "references": ["refreshes the underlying invoice and cart objects ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 3967, "code": "def void ( self ) : if self . invoice . total payments ( ) > 0 : raise Validation Error ( \"Invoices with payments must be refunded.\" ) elif self . invoice . is refunded : raise Validation Error ( \"Refunded invoices may not be voided.\" ) if self . invoice . is paid : self . release cart ( ) self . mark void ( )", "predictions": ["mark , this is called after the cart cart has been called ."], "references": ["voids the invoice if it is valid to do so ."], "bleu": 0.1135935489027116, "rouge_l": 0.16920943134535368}
{"id": 3968, "code": "def update ( self , data ) : fields = [ 'id' , 'status' , 'type' , 'persistence' , 'date start' , 'date finish' , 'date created' , 'date modified' , 'checksum' , 'processor name' , 'input' , 'input schema' , 'output' , 'output schema' , 'static' , 'static schema' , 'var' , 'var template' , ] self . annotation = { } for f in fields : setattr ( self , f , data [ f ] ) self . name = data [ 'static' ] [ 'name' ] if 'name' in data [ 'static' ] else '' self . annotation . update ( self . flatten field ( data [ 'input' ] , data [ 'input schema' ] , 'input' ) ) self . annotation . update ( self . flatten field ( data [ 'output' ] , data [ 'output schema' ] , 'output' ) ) self . annotation . update ( self . flatten field ( data [ 'static' ] , data [ 'static schema' ] , 'static' ) ) self . annotation . update ( self . flatten field ( data [ 'var' ] , data [ 'var template' ] , 'var' ) )", "predictions": ["update annotation objects with this object ."], "references": ["update the object with new data ."], "bleu": 0.23356898886410002, "rouge_l": 0.42857142857142855}
{"id": 3969, "code": "def flatten field ( self , field , schema , path ) : flat = { } for field schema , fields , path in iterate schema ( field , schema , path ) : name = field schema [ 'name' ] typ = field schema [ 'type' ] label = field schema [ 'label' ] value = fields [ name ] if name in fields else None flat [ path ] = { 'name' : name , 'value' : value , 'type' : typ , 'label' : label } return flat", "predictions": ["recursive helper for constructing all columns of data from their original field ."], "references": ["reduce dicts of dicts to dot separated keys ."], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 3970, "code": "def print downloads ( self ) : for path , ann in self . annotation . items ( ) : if path . startswith ( 'output' ) and ann [ 'type' ] == 'basic:file:' : print ( \"{}: {}\" . format ( path , ann [ 'value' ] [ 'file' ] ) )", "predictions": ["print all the contained workflow key constraints as a whole method ."], "references": ["print file fields to standard output ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 3971, "code": "def data ( self , * * query ) : objects = self . cache [ 'objects' ] data = self . api . data . get ( * * query ) [ 'objects' ] data objects = [ ] for d in data : id = d [ 'id' ] if id in objects : objects [ id ] . update ( d ) else : objects [ id ] = Gen Data ( d , self ) data objects . append ( objects [ id ] ) for d in data objects : count += 1 while True : ref annotation = { } remove annotation = [ ] for path , ann in d . annotation . items ( ) : if ann [ 'type' ] . startswith ( 'data:' ) : id = ann [ 'value' ] if id not in objects : try : d tmp = self . api . data ( id ) . get ( ) except slumber . exceptions . Http Client Error as ex : if ex . response . status code == 404 : continue else : raise ex objects [ id ] = Gen Data ( d tmp , self ) annotation = objects [ id ] . annotation ref annotation . update ( { path + '.' + k : v for k , v in annotation . items ( ) } ) remove annotation . append ( path ) if ref annotation : d . annotation . update ( ref annotation ) for path in remove annotation : del d . annotation [ path ] else : break return data objects", "predictions": ["fix stats of this query and removes duplicate objects ."], "references": ["query for data object annotation ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 3972, "code": "def rundata ( self , strjson ) : d = json . loads ( strjson ) return self . api . data . post ( d )", "predictions": ["convenience method for creating a rundata ."], "references": ["post json data object to server"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3973, "code": "def get subclasses ( c ) : subclasses = c . subclasses ( ) for d in list ( subclasses ) : subclasses . extend ( get subclasses ( d ) ) return subclasses", "predictions": ["returns a list of subclasses of all subclasses of this class ."], "references": ["gets the subclasses of a class ."], "bleu": 0.17996531271765898, "rouge_l": 0.44202898550724634}
{"id": 3974, "code": "def get repo and project ( self ) : app = self . app repo = app . data . apply ( 'github-repo' , app . args . github repo , app . prompt repo , on load = app . github . get repo , on save = lambda r : r . id ) assert repo , \"repository not found.\" project = app . data . apply ( 'asana-project' , app . args . asana project , app . prompt project , on load = app . asana . projects . find by id , on save = lambda p : p [ 'id' ] ) assert project , \"project not found.\" first issue = app . data . apply ( 'first-issue' , app . args . first issue , \"set the first issue to sync with [1 for new repos]\" , on save = int ) assert first issue assert first issue >= 0 , \"issue must be positive\" app . sync data ( ) return repo , project", "predictions": ["generate and return the repo ."], "references": ["returns repository and project ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 3975, "code": "def get variant phenotypes with suggested changes ( variant id list ) : variants = civic . get variants by ids ( variant id list ) evidence = list ( ) for variant in variants : evidence . extend ( variant . evidence ) for e in evidence : suggested changes url = f'https://civicdb.org/api/evidence items/{e.id}/suggested changes' resp = requests . get ( suggested changes url ) resp . raise for status ( ) suggested changes = dict ( ) for suggested change in resp . json ( ) : pheno changes = suggested change [ 'suggested changes' ] . get ( 'phenotype ids' , None ) if pheno changes is None : continue a , b = pheno changes added = set ( b ) - set ( a ) deleted = set ( a ) - set ( b ) rid = suggested change [ 'id' ] suggested changes [ rid ] = { 'added' : added , 'deleted' : deleted } yield e , { 'suggested changes' : suggested changes , 'current' : set ( [ x . id for x in e . phenotypes ] ) }", "predictions": ["get all variant of the given variant ."], "references": ["for each variant yields evidence and associated phenotypes both current and suggested"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 3976, "code": "def get variant phenotypes with suggested changes merged ( variant id list ) : for evidence , phenotype status in get variant phenotypes with suggested changes ( variant id list ) : final = phenotype status [ 'current' ] for rid in sorted ( phenotype status [ 'suggested changes' ] ) : changes = phenotype status [ 'suggested changes' ] [ rid ] final = final - changes [ 'deleted' ] final = final | changes [ 'added' ] if final : yield evidence , final", "predictions": ["yield variant of all variant that have been phenotypes ."], "references": ["for each variant yields evidence and merged phenotype from applying suggested changes to current"], "bleu": 0.08450033111870488, "rouge_l": 0.08090185676392574}
{"id": 3977, "code": "def update ( self , allow partial = True , force = False , * * kwargs ) : if kwargs : self . init ( partial = allow partial , force = force , * * kwargs ) return not self . partial if not force and CACHE . get ( hash ( self ) ) : cached = CACHE [ hash ( self ) ] for field in self . SIMPLE FIELDS | self . COMPLEX FIELDS : v = getattr ( cached , field ) setattr ( self , field , v ) self . partial = False logging . info ( f'Loading {str(self)} from cache' ) return True resp dict = element lookup by id ( self . type , self . id ) self . init ( partial = False , * * resp dict ) return True", "predictions": ["update a class with a sequence of field values ."], "references": ["updates record and returns true if record is complete after update else false ."], "bleu": 0.09351498865776114, "rouge_l": 0.16180371352785147}
{"id": 3978, "code": "def uniqify ( cls , seq ) : seen = set ( ) seen add = seen . add return [ x for x in seq if x not in seen and not seen add ( x ) ]", "predictions": ["the first element in this list ."], "references": ["returns a unique list of seq"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3979, "code": "def authenticate ( self ) : if self . oauth : return False self . settings . apply ( 'api-asana' , self . args . asana api , \"enter asana api key\" ) self . settings . apply ( 'api-github' , self . args . github api , \"enter github.com token\" ) logging . debug ( \"authenticating asana api.\" ) self . asana = Client . basic auth ( self . settings [ 'api-asana' ] ) self . asana errors = asana errors self . asana me = self . asana . users . me ( ) logging . debug ( \"authenticating github api\" ) self . github = Github ( self . settings [ 'api-github' ] ) self . github user = self . github . get user ( ) self . oauth = True", "predictions": ["this is called to authenticate the client before calling this method ."], "references": ["connects to github and asana and authenticates via oauth ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 3980, "code": "def list select ( cls , lst , prompt , offset = 0 ) : inp = raw input ( \"select %s: \" % prompt ) assert inp , \"value required.\" try : return lst [ int ( inp ) + offset ] except Value Error : return inp except Index Error : assert False , \"bad value.\"", "predictions": ["selects a list of elements in a given structure ."], "references": ["given a list of values and names accepts the index value or name ."], "bleu": 0.17405100568758053, "rouge_l": 0.32360742705570295}
{"id": 3981, "code": "def move saved issue data ( self , issue , ns , other ns ) : if isinstance ( issue , int ) : issue number = str ( issue ) elif isinstance ( issue , basestring ) : issue number = issue else : issue number = issue . number issue data key = self . issue data key ( ns ) other issue data key = self . issue data key ( other ns ) issue data = self . data . get ( issue data key , { } ) other issue data = self . data . get ( other issue data key , { } ) id = issue data . pop ( issue number , None ) if id : other issue data [ issue number ] = id self . data [ other issue data key ] = other issue data self . data [ issue data key ] = issue data", "predictions": ["move this issue into the given issue ."], "references": ["moves an issue_data from one namespace to another ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3982, "code": "def get asana task ( self , asana task id ) : try : return self . asana . tasks . find by id ( asana task id ) except asana errors . Not Found Error : return None except asana errors . Forbidden Error : return None", "predictions": ["get a ( for a specific task ."], "references": ["retrieves a task from asana ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 3983, "code": "def transport task ( func ) : def wrapped func ( * args , * * kwargs ) : tries = 0 while True : try : try : return func ( * args , * * kwargs ) except ( asana errors . Invalid Request Error , asana errors . Not Found Error ) , exc : logging . warn ( \"warning: invalid request: %r\" , exc ) except asana errors . Forbidden Error , exc : logging . warn ( \"forbidden error: %r\" , exc ) except asana errors . Not Found Error , exc : logging . warn ( \"not found error: %r\" , exc ) return None except asana errors . Retryable Asana Error , retry exc : tries += 1 logging . warn ( \"retry exception %r on try %d\" , retry exc , tries ) if tries >= 3 : raise except Exception , exc : logging . exception ( \"Exception in transport.\" ) return return wrapped func", "predictions": ["prepares a function or the user specified by the args of a task ."], "references": ["decorator for retrying tasks with special cases ."], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 3984, "code": "def flush ( callback = None ) : while True : if shutdown event . is set ( ) : return if callable ( callback ) : callback ( ) try : item = queue . get ( timeout = 1 ) queue . put ( item ) except Queue . Empty : return", "predictions": ["flush all queued events ."], "references": ["waits until queue is empty ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3985, "code": "def format task numbers with links ( tasks ) : project id = data . get ( 'asana-project' , None ) def task format ( task id ) : if project id : asana url = tool . Tool App . make asana url ( project id , task id ) return \"[#%d](%s)\" % ( task id , asana url ) else : return \"#%d\" % task id return \"\\n\" . join ( [ task format ( tid ) for tid in tasks ] )", "predictions": ["calculate the numbers of all links to be included ."], "references": ["returns formatting for the tasks section of asana ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 3986, "code": "def create missing task ( self , asana workspace id , name , assignee , projects , completed , issue number , issue html url , issue state , issue body , tasks , labels , label tag map ) : task = self . asana . tasks . create in workspace ( asana workspace id , { 'name' : name , 'notes' : issue body , 'assignee' : assignee , 'projects' : projects , 'completed' : completed , } ) task id = task [ 'id' ] put ( \"create story\" , task id = task id , text = \"Git Issue #%d: \\n\" \"%s\" % ( issue number , issue html url , ) ) put ( \"apply tasks to issue\" , tasks = [ task id ] , issue number = issue number , issue body = issue body , ) put setting ( \"save issue data task\" , issue = issue number , task id = task id , namespace = issue state ) tasks . append ( task id ) put ( \"sync tags\" , tasks = tasks , labels = labels , label tag map = label tag map )", "predictions": ["create a task that can be passed to the map ."], "references": ["creates a missing task ."], "bleu": 0.1354599427337814, "rouge_l": 0.40219780219780216}
{"id": 3987, "code": "def apply tasks to issue ( self , tasks , issue number , issue body ) : issue body = issue body task numbers = format task numbers with links ( tasks ) if task numbers : new body = ASANA SECTION RE . sub ( '' , issue body ) new body = new body + % task numbers put ( \"issue edit\" , issue number = issue number , body = new body ) return new body return issue body", "predictions": ["create a task from the specified issue and return it ."], "references": ["applies task numbers to an issue ."], "bleu": 0.1354599427337814, "rouge_l": 0.3472485768500949}
{"id": 3988, "code": "def data types ( self ) : data = self . gencloud . project data ( self . id ) return sorted ( set ( d . type for d in data ) )", "predictions": ["get a sorted list of project objects ."], "references": ["return a list of data types ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 3989, "code": "def data ( self , * * query ) : data = self . gencloud . project data ( self . id ) query [ 'case ids contains' ] = self . id ids = set ( d [ 'id' ] for d in self . gencloud . api . dataid . get ( * * query ) [ 'objects' ] ) return [ d for d in data if d . id in ids ]", "predictions": ["get a query for all projects ."], "references": ["query for data object annotation ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 3990, "code": "def init Port ( self ) : try : self . m ser = serial . Serial ( port = self . m ttyport , baudrate = self . m baudrate , timeout = 0 , parity = serial . PARITY EVEN , stopbits = serial . STOPBITS ONE , bytesize = serial . SEVENBITS , rtscts = False ) ekm log ( \"Pyserial version = \" + serial . VERSION ) ekm log ( \"Port = \" + self . m ttyport ) ekm log ( \"Rate = \" + str ( self . m baudrate ) ) time . sleep ( self . m init wait ) return True except : ekm log ( traceback . format exc ( sys . exc info ( ) ) ) return False", "predictions": ["this method borrowed from the previously set up and displays the ( ."], "references": ["required initialization call wraps pyserial constructor ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 3991, "code": "def combine AB ( self ) : v4definition meter = V4Meter ( ) v4definition meter . make AB ( ) defv4 = v4definition meter . get Read Buffer ( ) v3definition meter = V3Meter ( ) v3definition meter . make Return Format ( ) defv3 = v3definition meter . get Read Buffer ( ) for fld in defv3 : if fld not in self . m all fields : compare fld = fld . upper ( ) if not \"RESERVED\" in compare fld and not \"CRC\" in compare fld : self . m all fields [ fld ] = defv3 [ fld ] for fld in defv4 : if fld not in self . m all fields : compare fld = fld . upper ( ) if not \"RESERVED\" in compare fld and not \"CRC\" in compare fld : self . m all fields [ fld ] = defv4 [ fld ] pass", "predictions": ["this is used to combine all existing fields ."], "references": ["use the serial block definitions in v3 and v4 to create one field list ."], "bleu": 0.08019421212222273, "rouge_l": 0.15947712418300655}
{"id": 3992, "code": "def update Observers ( self ) : for observer in self . m observers : try : observer . update ( self . m req ) except : ekm log ( traceback . format exc ( sys . exc info ( ) ) )", "predictions": ["update the observer object ."], "references": ["fire update method in all attached observers in order of attachment ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 3993, "code": "def init Lcd Lookup ( self ) : self . m lcd lookup [ \"k Wh Tot\" ] = LCD Items . k Wh Tot self . m lcd lookup [ \"Rev k Wh Tot\" ] = LCD Items . Rev k Wh Tot self . m lcd lookup [ \"RMS Volts Ln 1\" ] = LCD Items . RMS Volts Ln 1 self . m lcd lookup [ \"RMS Volts Ln 2\" ] = LCD Items . RMS Volts Ln 2 self . m lcd lookup [ \"RMS Volts Ln 3\" ] = LCD Items . RMS Volts Ln 3 self . m lcd lookup [ \"Amps Ln 1\" ] = LCD Items . Amps Ln 1 self . m lcd lookup [ \"Amps Ln 2\" ] = LCD Items . Amps Ln 2 self . m lcd lookup [ \"Amps Ln 3\" ] = LCD Items . Amps Ln 3 self . m lcd lookup [ \"RMS Watts Ln 1\" ] = LCD Items . RMS Watts Ln 1 self . m lcd lookup [ \"RMS Watts Ln 2\" ] = LCD Items . RMS Watts Ln 2 self . m lcd lookup [ \"RMS Watts Ln 3\" ] = LCD Items . RMS Watts Ln 3 self . m lcd lookup [ \"RMS Watts Tot\" ] = LCD Items . RMS Watts Tot self . m lcd lookup [ \"Power Factor Ln 1\" ] = LCD Items . Power Factor Ln 1 self . m lcd lookup [ \"Power Factor Ln 2\" ] = LCD Items . Power Factor Ln 2 self . m lcd lookup [ \"Power Factor Ln 3\" ] = LCD Items . Power Factor Ln 3 self . m lcd lookup [ \"k Wh Tariff 1\" ] = LCD Items . k Wh Tariff 1 self . m lcd lookup [ \"k Wh Tariff 2\" ] = LCD Items . k Wh Tariff 2 self . m lcd lookup [ \"k Wh Tariff 3\" ] = LCD Items . k Wh Tariff 3 self . m lcd lookup [ \"k Wh Tariff 4\" ] = LCD Items . k Wh Tariff 4 self . m lcd lookup [ \"Rev k Wh Tariff 1\" ] = LCD Items . Rev k Wh Tariff 1 self . m lcd lookup [ \"Rev k Wh Tariff 2\" ] = LCD Items . Rev k Wh Tariff 2 self . m lcd lookup [ \"Rev k Wh Tariff 3\" ] = LCD Items . Rev k Wh Tariff 3 self . m lcd lookup [ \"Rev k Wh Tariff 4\" ] = LCD Items . Rev k Wh Tariff 4 self . m lcd lookup [ \"Reactive Pwr Ln 1\" ] = LCD Items . Reactive Pwr Ln 1 self . m lcd lookup [ \"Reactive Pwr Ln 2\" ] = LCD Items . Reactive Pwr Ln 2 self . m lcd lookup [ \"Reactive Pwr Ln 3\" ] = LCD Items . Reactive Pwr Ln 3 self . m lcd lookup [ \"Reactive Pwr Tot\" ] = LCD Items . Reactive Pwr Tot self . m lcd lookup [ \"Line Freq\" ] = LCD Items . Line Freq self . m lcd lookup [ \"Pulse Cnt 1\" ] = LCD Items . Pulse Cnt 1 self . m lcd lookup [ \"Pulse Cnt 2\" ] = LCD Items . Pulse Cnt 2 self . m lcd lookup [ \"Pulse Cnt 3\" ] = LCD Items . Pulse Cnt 3 self . m lcd lookup [ \"k Wh Ln 1\" ] = LCD Items . k Wh Ln 1 self . m lcd lookup [ \"Rev k Wh Ln 1\" ] = LCD Items . Rev k Wh Ln 1 self . m lcd lookup [ \"k Wh Ln 2\" ] = LCD Items . k Wh Ln 2 self . m lcd lookup [ \"Rev k Wh Ln 2\" ] = LCD Items . Rev k Wh Ln 2 self . m lcd lookup [ \"k Wh Ln 3\" ] = LCD Items . k Wh Ln 3 self . m lcd lookup [ \"Rev k Wh Ln 3\" ] = LCD Items . Rev k Wh Ln 3 self . m lcd lookup [ \"Reactive Energy Tot\" ] = LCD Items . Reactive Energy Tot self . m lcd lookup [ \"Max Demand Rst\" ] = LCD Items . Max Demand Rst self . m lcd lookup [ \"Rev k Wh Rst\" ] = LCD Items . Rev k Wh Rst self . m lcd lookup [ \"State Inputs\" ] = LCD Items . State Inputs self . m lcd lookup [ \"Max Demand\" ] = LCD Items . Max Demand", "predictions": ["wrapper around mutable call to find the lcd wrapper and lookup ."], "references": ["initialize lookup table for string input of lcd fields"], "bleu": 0.11498759556447223, "rouge_l": 0.09775641025641024}
{"id": 3994, "code": "def make AB ( self ) : for fld in self . m blk a : compare fld = fld . upper ( ) if not \"RESERVED\" in compare fld and not \"CRC\" in compare fld : self . m req [ fld ] = self . m blk a [ fld ] for fld in self . m blk b : compare fld = fld . upper ( ) if not \"RESERVED\" in compare fld and not \"CRC\" in compare fld : self . m req [ fld ] = self . m blk b [ fld ] pass", "predictions": ["make a square matrix ."], "references": ["munge a and b reads into single serial block with only unique fields ."], "bleu": 0.04994299940831281, "rouge_l": 0.19395866454689983}
{"id": 3995, "code": "def serial Post End ( self ) : ekm log ( \"Termination string sent (\" + self . m context + \")\" ) try : self . m serial port . write ( \"0142300375\" . decode ( \"hex\" ) ) except : ekm log ( traceback . format exc ( sys . exc info ( ) ) ) pass", "predictions": ["wrap the end of a ( ."], "references": ["send termination string to implicit current meter ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3996, "code": "def apply tasks to issue ( self , issue , tasks , issue body = None ) : issue body = issue body or issue . body task numbers = transport . format task numbers with links ( tasks ) if task numbers : new body = transport . ASANA SECTION RE . sub ( '' , issue body ) new body = new body + % task numbers transport . issue edit ( issue , body = new body ) return new body return issue body", "predictions": ["create a task to apply to the specified issue ."], "references": ["applies task numbers to an issue ."], "bleu": 0.18850319022747347, "rouge_l": 0.48605577689243035}
{"id": 3997, "code": "def statistics ( self , elapsed , result ) : return \"\\n\" . join ( ( self . timing ( elapsed ) , self . result summary ( result ) ) )", "predictions": ["get a statistics of this object ."], "references": ["return output for the combined time and result summary statistics ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 3998, "code": "def color ( self , color , text ) : return \"{escape}{text}{reset}\" . format ( escape = self . ANSI [ color ] , text = text , reset = self . ANSI [ \"reset\" ] , )", "predictions": ["returns a color that is always filled to the original text ."], "references": ["color some text in the given ansi color ."], "bleu": 0.13065113298388567, "rouge_l": 0.2932692307692307}
{"id": 3999, "code": "def show ( self , text ) : self . stream . write ( text ) self . stream . flush ( )", "predictions": ["show the message digest ."], "references": ["write the text to the stream and flush immediately ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 4000, "code": "def result summary ( self , result ) : return \"{} examples, {} errors, {} failures\\n\" . format ( result . tests Run , len ( result . errors ) , len ( result . failures ) , )", "predictions": ["get the ( possibly empty ) of this object ."], "references": ["return a summary of the results ."], "bleu": 0.14991106946711685, "rouge_l": 0.24302788844621517}
{"id": 4001, "code": "def parse ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] not in { \"run\" , \"transform\" } : argv = [ \"run\" ] + argv arguments = clean ( parser . parse args ( argv ) ) return arguments", "predictions": ["flatten command line arguments ."], "references": ["parse some arguments using the parser ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4002, "code": "def setup ( config ) : formatter = config . Formatter ( ) if config . verbose : formatter = result . Verbose ( formatter ) if config . color : formatter = result . Colored ( formatter ) current result = result . Example Result ( formatter ) ivoire . current result = ivoire . manager . result = current result", "predictions": ["set up the ( flag ."], "references": ["setup the environment for an example run ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4003, "code": "def run ( config ) : setup ( config ) if config . exitfirst : ivoire . current result . failfast = True ivoire . current result . start Test Run ( ) for spec in config . specs : try : load by name ( spec ) except Exception : ivoire . current result . add Error ( Example Not Running ( ) , sys . exc info ( ) ) ivoire . current result . stop Test Run ( ) sys . exit ( not ivoire . current result . was Successful ( ) )", "predictions": ["data is expected to be run once the test is loaded ."], "references": ["time to run ."], "bleu": 0.1235622127262679, "rouge_l": 0.4121621621621622}
{"id": 4004, "code": "def transform ( config ) : if transform possible : Example Loader . register ( ) args , sys . argv [ 1 : ] = sys . argv [ 1 : ] , config . args try : return runpy . run path ( config . runner , run name = \" main \" ) finally : sys . argv [ 1 : ] = args", "predictions": ["transforms all configuration parameters in the configuration ."], "references": ["run in transform mode ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4005, "code": "def takes only self ( self ) : return ast . arguments ( args = [ ast . arg ( arg = \"self\" ) ] , defaults = [ ] , kw defaults = [ ] , kwonlyargs = [ ] , )", "predictions": ["a newly created subclasses can use this method ."], "references": ["return an argument list node that takes only self ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 4006, "code": "def register ( cls ) : cls . finder = File Finder . path hook ( ( cls , [ cls . suffix ] ) ) sys . path hooks . append ( cls . finder )", "predictions": ["get a class for batch notifications ."], "references": ["register the path hook ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4007, "code": "def source to code ( self , source bytes , source path ) : node = ast . parse ( source bytes ) transformed = Example Transformer ( ) . transform ( node ) return compile ( transformed , source path , \"exec\" , dont inherit = True )", "predictions": ["creates a list of ( from the resource by applying the get to the = by this method ."], "references": ["transform the source code then return the code object ."], "bleu": 0.07658412276041004, "rouge_l": 0.21916167664670658}
{"id": 4008, "code": "def apply argument parser ( arguments Parser , options = None ) : if options is not None : args = arguments Parser . parse args ( options ) else : args = arguments Parser . parse args ( ) return args", "predictions": ["parses the command line arguments ."], "references": ["apply the argument parser ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 4009, "code": "def load by name ( name ) : if os . path . exists ( name ) : load from path ( name ) else : import ( name )", "predictions": ["update a cookie by name ."], "references": ["load a spec from either a file path or a fully qualified name ."], "bleu": 0.08234616270176032, "rouge_l": 0.2798165137614679}
{"id": 4010, "code": "def load from path ( path ) : if os . path . isdir ( path ) : paths = discover ( path ) else : paths = [ path ] for path in paths : name = os . path . basename ( os . path . splitext ( path ) [ 0 ] ) imp . load source ( name , path )", "predictions": ["load must be ."], "references": ["load a spec from a given path discovering specs if a directory is given ."], "bleu": 0.025419978385188596, "rouge_l": 0.190625}
{"id": 4011, "code": "def delimit ( values , delimiter = ', ' ) : toks = [ ] if not values : return toks if not isinstance ( delimiter , ( list , tuple ) ) : delimiter = [ delimiter ] last = len ( values ) - 1 for i , value in enumerate ( values ) : toks . append ( value ) if i < last : toks . extend ( delimiter ) return toks", "predictions": ["convenience method to authenticate the sequence of self - used by ( ."], "references": ["returns a list of tokens interleaved with the delimiter ."], "bleu": 0.1135935489027116, "rouge_l": 0.1781021897810219}
{"id": 4012, "code": "def exists ( value ) : if not isinstance ( value , Token ) : raise Type Error ( 'value must be a token' ) if not hasattr ( value , 'identifier' ) : raise Type Error ( 'value must support an identifier' ) if not value . identifier : value = value . class ( * * value . dict ) value . identifier = 'v' ident = Identifier ( value . identifier ) return Query ( [ Optional Match ( value ) , Return ( Predicate ( ident , 'IS NOT NULL' ) ) , Limit ( 1 ) , ] )", "predictions": ["determines if the entity list list list exists in the ( . this method is used to determine if the user selects a given ( e . g . if the entity list does not contain a certain value cls cls cls cls cls cls or false cls cls cls"], "references": ["query to test if a value exists ."], "bleu": 0.032868518969579506, "rouge_l": 0.1586475942782835}
{"id": 4013, "code": "def get ( value ) : if not isinstance ( value , Token ) : raise Type Error ( 'value must be a token' ) if not hasattr ( value , 'identifier' ) : raise Type Error ( 'value must support an identifier' ) if not value . identifier : value = value . class ( * * value . dict ) value . identifier = 'v' ident = Identifier ( value . identifier ) return Query ( [ Match ( value ) , Return ( ident ) ] )", "predictions": ["method to move the correct field from the given issue ."], "references": ["query to get the value ."], "bleu": 0.1354599427337814, "rouge_l": 0.3727087576374745}
{"id": 4014, "code": "def check ( self ) : if self . closed : raise Value Error ( \"Cannot check a closed state\" ) self . maybe Reset ( ) if self . url is None : return False return self . maybe Check ( )", "predictions": ["this method is called via reflection from the database ."], "references": ["check the state of http"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 4015, "code": "def wrap Heart ( service ) : master = taservice . Multi Service ( ) service . set Service Parent ( master ) maybe Add Heart ( master ) return master", "predictions": ["transport wrapped func using the wrapped func ."], "references": ["wrap a service in a multiservice with a heart"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 4016, "code": "def freeze from checkpoint ( input checkpoint , output file path , output node names ) : check input checkpoint ( input checkpoint ) output node names = output node names string as list ( output node names ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) freeze graph . freeze graph with def protos ( input graph def = sess . graph def , input saver def = None , input checkpoint = input checkpoint , output node names = ',' . join ( output node names ) , restore op name = 'save/restore all' , filename tensor name = 'save/Const:0' , output graph = output file path , clear devices = True , initializer nodes = '' )", "predictions": ["restores the specified callback to the specified callback ."], "references": ["freeze and shrink the graph based on a checkpoint and the output node names ."], "bleu": 0.08617428905281956, "rouge_l": 0.23921568627450981}
{"id": 4017, "code": "def freeze ( sess , output file path , output node names ) : with Temporary Directory ( ) as temp dir name : checkpoint path = os . path . join ( temp dir name , 'model.ckpt' ) tf . train . Saver ( ) . save ( sess , checkpoint path ) freeze from checkpoint ( checkpoint path , output file path , output node names )", "predictions": ["format a new if it does not exist yet ."], "references": ["freeze and shrink the graph based on a session and the output node names ."], "bleu": 0.08461586088475063, "rouge_l": 0.15443037974683543}
{"id": 4018, "code": "def save graph only ( sess , output file path , output node names , as text = False ) : for node in sess . graph def . node : node . device = '' graph def = graph util . extract sub graph ( sess . graph def , output node names ) output dir , output filename = os . path . split ( output file path ) graph io . write graph ( graph def , output dir , output filename , as text = as text )", "predictions": ["create task task for project ."], "references": ["save a small version of the graph based on a session and the output node names ."], "bleu": 0.035316782215334214, "rouge_l": 0.0800524934383202}
{"id": 4019, "code": "def save graph only from checkpoint ( input checkpoint , output file path , output node names , as text = False ) : check input checkpoint ( input checkpoint ) output node names = output node names string as list ( output node names ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) save graph only ( sess , output file path , output node names , as text = as text )", "predictions": ["apply tasks to the configuration file ."], "references": ["save a small version of the graph based on a checkpoint and the output node names ."], "bleu": 0.04926429870313275, "rouge_l": 0.1550190597204574}
{"id": 4020, "code": "def save weights ( sess , output path , conv var names = None , conv transpose var names = None ) : if not conv var names : conv var names = [ ] if not conv transpose var names : conv transpose var names = [ ] for var in tf . trainable variables ( ) : filename = '{}-{}' . format ( output path , var . name . replace ( ':' , '-' ) . replace ( '/' , '-' ) ) if var . name in conv var names : var = tf . transpose ( var , perm = [ 3 , 0 , 1 , 2 ] ) elif var . name in conv transpose var names : var = tf . transpose ( var , perm = [ 3 , 1 , 0 , 2 ] ) value = sess . run ( var ) with open ( filename , 'w' ) as file : value . tofile ( file )", "predictions": ["data using the specified = . ( set set set set set set set set set set set set set set set set set set set set set set set set set set set set set set set set as string set set set set set set set set set"], "references": ["save the weights of the trainable variables each one in a different file in output_path ."], "bleu": 0.026594139297659906, "rouge_l": 0.06681270536692224}
{"id": 4021, "code": "def save weights from checkpoint ( input checkpoint , output path , conv var names = None , conv transpose var names = None ) : check input checkpoint ( input checkpoint ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) save weights ( sess , output path , conv var names = conv var names , conv transpose var names = conv transpose var names )", "predictions": ["data using the specified , data locally ."], "references": ["save the weights of the trainable variables given a checkpoint each one in a different file in output_path ."], "bleu": 0.04487246777590933, "rouge_l": 0.13800904977375564}
{"id": 4022, "code": "def restore from checkpoint ( sess , input checkpoint ) : saver = tf . train . import meta graph ( '{}.meta' . format ( input checkpoint ) ) saver . restore ( sess , input checkpoint ) return saver", "predictions": ["init an try to the background ."], "references": ["return a tensorflow saver from a checkpoint containing the metagraph ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4023, "code": "def render tag ( self , context , * tag args , * * tag kwargs ) : raise Not Implemented Error ( \"{0}.render tag() is not implemented!\" . format ( self . class . name ) )", "predictions": ["combine the image using a tag ."], "references": ["render the tag with all arguments resolved to their actual values ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 4024, "code": "def validate args ( cls , tag name , * args , * * kwargs ) : if cls . min args is not None and len ( args ) < cls . min args : if cls . min args == 1 : raise Template Syntax Error ( \"'{0}' tag requires at least {1} argument\" . format ( tag name , cls . min args ) ) else : raise Template Syntax Error ( \"'{0}' tag requires at least {1} arguments\" . format ( tag name , cls . min args ) ) if cls . max args is not None and len ( args ) > cls . max args : if cls . max args == 0 : if cls . allowed kwargs : raise Template Syntax Error ( \"'{0}' tag only allows keywords arguments, for example {1}=\\\"...\\\".\" . format ( tag name , cls . allowed kwargs [ 0 ] ) ) else : raise Template Syntax Error ( \"'{0}' tag doesn't support any arguments\" . format ( tag name ) ) elif cls . max args == 1 : raise Template Syntax Error ( \"'{0}' tag only allows {1} argument.\" . format ( tag name , cls . max args ) ) else : raise Template Syntax Error ( \"'{0}' tag only allows {1} arguments.\" . format ( tag name , cls . max args ) )", "predictions": ["update the : args if needed , does nothing if any of the arguments have been set to true . otherwise , the : name is set to the : name of the : name = true will be returned ."], "references": ["validate the syntax of the template tag ."], "bleu": 0.04392487796991639, "rouge_l": 0.18583396801218582}
{"id": 4025, "code": "def get context data ( self , parent context , * tag args , * * tag kwargs ) : raise Not Implemented Error ( \"{0}.get context data() is not implemented.\" . format ( self . class . name ) )", "predictions": ["construct a angle that represents the given data and its children ."], "references": ["return the context data for the included template ."], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 4026, "code": "def render tag ( self , context , * tag args , * * tag kwargs ) : if self . as var : context [ self . as var ] = self . get value ( context , * tag args , * * tag kwargs ) return u''", "predictions": ["make a tag tag call ."], "references": ["rendering of the tag . it either assigns the value as variable or renders it ."], "bleu": 0.046172815301777345, "rouge_l": 0.1680440771349862}
{"id": 4027, "code": "def parse ( cls , parser , token ) : bits , as var = parse as var ( parser , token ) tag name , args , kwargs = parse token kwargs ( parser , bits , ( 'template' , ) + cls . allowed kwargs , compile args = cls . compile args , compile kwargs = cls . compile kwargs ) cls . validate args ( tag name , * args ) return cls ( tag name , as var , * args , * * kwargs )", "predictions": ["serial serial and m events ."], "references": ["parse the as var syntax ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 4028, "code": "def render tag ( self , context , * tag args , * * tag kwargs ) : if self . as var : return Base Assignment Node . render tag ( self , context , * tag args , * * tag kwargs ) else : return Base Inclusion Node . render tag ( self , context , * tag args , * * tag kwargs )", "predictions": ["apply a tasks to a bootstrap layout ."], "references": ["rendering of the tag . it either assigns the value as variable or renders it ."], "bleu": 0.0589953212431261, "rouge_l": 0.07860824742268041}
{"id": 4029, "code": "def caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = 'Graph' , conversion out dir path = None , use padding same = False ) : try : from caffeflow import convert except Import Error : raise Exception ( \"caffeflow package needs to be installed to freeze Caffe models. Check out the README file.\" ) with ( dummy context mgr ( conversion out dir path ) or util . Temporary Directory ( ) ) as dir path : params values output path = os . path . join ( dir path , 'params values.npy' ) network output path = os . path . join ( dir path , 'network.py' ) convert . convert ( caffe def path , caffemodel path , params values output path , network output path , False , use padding same = use padding same ) network module = imp . load source ( 'module.name' , network output path ) network class = getattr ( network module , graph name ) network = network class ( inputs ) sess = tf . Session ( ) network . load ( params values output path , sess ) return sess", "predictions": ["convert a statistics file to a statistics file ."], "references": ["create a tensorflow session from a caffe model ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 4030, "code": "def freeze ( caffe def path , caffemodel path , inputs , output file path , output node names , graph name = 'Graph' , conversion out dir path = None , checkpoint out path = None , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , conversion out dir path = conversion out dir path , use padding same = use padding same ) as sess : saver = tf . train . Saver ( ) with ( dummy context mgr ( checkpoint out path ) or util . Temporary Directory ( ) ) as temp dir path : checkpoint path = checkpoint out path or os . path . join ( temp dir path , 'pose.ckpt' ) saver . save ( sess , checkpoint path ) output node names = util . output node names string as list ( output node names ) tf freeze . freeze from checkpoint ( checkpoint path , output file path , output node names )", "predictions": [". a single , where the , and the checkpoint is only executed once ."], "references": ["freeze and shrink the graph based on a caffe model the input tensors and the output node names ."], "bleu": 0.09860814573660683, "rouge_l": 0.28800755429650615}
{"id": 4031, "code": "def save graph only ( caffe def path , caffemodel path , inputs , output file path , output node names , graph name = 'Graph' , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , use padding same = use padding same ) as sess : tf freeze . save graph only ( sess , output file path , output node names )", "predictions": ["saves an email to the specified ) ( as either \" . \" or \" ."], "references": ["save a small version of the graph based on a caffe model the input tensors and the output node names ."], "bleu": 0.0622825289263835, "rouge_l": 0.10553633217993079}
{"id": 4032, "code": "def save weights ( caffe def path , caffemodel path , inputs , output path , graph name = 'Graph' , conv var names = None , conv transpose var names = None , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , use padding same = use padding same ) as sess : tf freeze . save weights ( sess , output path , conv var names = conv var names , conv transpose var names = conv transpose var names )", "predictions": ["saves the path to the caffe user ' s output file"], "references": ["save the weights of the trainable variables each one in a different file in output_path ."], "bleu": 0.08598135896069606, "rouge_l": 0.21504112808460632}
{"id": 4033, "code": "def descendant ( self , chain path ) : public child = self . hdkeychain chain step bytes = 4 max bits per step = 2 ** 31 chain steps = [ int ( chain path [ i : i + chain step bytes * 2 ] , 16 ) % max bits per step for i in range ( 0 , len ( chain path ) , chain step bytes * 2 ) ] for step in chain steps : public child = public child . get child ( step ) return Public Keychain ( public child )", "predictions": ["returns a list of ( that has the same path but does not include the same content . this method can be used to determine how much time the method returns a path that has been completed ."], "references": ["a descendant is a child many steps down ."], "bleu": 0.037729138673427344, "rouge_l": 0.14364207221350078}
{"id": 4034, "code": "def object iter ( obj , parent = None , parent key = None , idx = None , siblings = None ) : obj node = Node ( value = obj , parent = parent , parent key = parent key , siblings = siblings , idx = idx ) if isinstance ( obj , list ) : siblings = len ( obj ) for i , elem in enumerate ( obj ) : for node in object iter ( elem , obj node , None , i + 1 , siblings ) : yield node elif isinstance ( obj , collections . Mapping ) : for key in obj : for node in object iter ( obj [ key ] , obj node , key ) : yield node yield obj node", "predictions": ["iterates over all edges of the given document ."], "references": ["yields each node of object graph in postorder ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 4035, "code": "def parse ( self , selector ) : log . debug ( self . obj ) tokens = lex ( selector ) if self . peek ( tokens , 'operator' ) == '*' : self . match ( tokens , 'operator' ) results = list ( object iter ( self . obj ) ) else : results = self . selector production ( tokens ) results = [ node . value for node in results ] if len ( results ) == 1 : return results [ 0 ] elif not len ( results ) : return None return results", "predictions": ["allow for the parameterized object to be read without needing the recorded selector ."], "references": ["accept a list of tokens . returns matched nodes of self . obj ."], "bleu": 0.08839374326825923, "rouge_l": 0.07142857142857142}
{"id": 4036, "code": "def selector production ( self , tokens ) : validators = [ ] if self . peek ( tokens , 'type' ) : type = self . match ( tokens , 'type' ) validators . append ( self . type production ( type ) ) if self . peek ( tokens , 'identifier' ) : key = self . match ( tokens , 'identifier' ) validators . append ( self . key production ( key ) ) if self . peek ( tokens , 'pclass' ) : pclass = self . match ( tokens , 'pclass' ) validators . append ( self . pclass production ( pclass ) ) if self . peek ( tokens , 'nth func' ) : nth func = self . match ( tokens , 'nth func' ) validators . append ( self . nth child production ( nth func , tokens ) ) if self . peek ( tokens , 'pclass func' ) : pclass func = self . match ( tokens , 'pclass func' ) validators . append ( self . pclass func production ( pclass func , tokens ) ) if not len ( validators ) : raise Selector Syntax Error ( 'no selector recognized.' ) results = self . match nodes ( validators , self . obj ) if self . peek ( tokens , 'operator' ) : operator = self . match ( tokens , 'operator' ) rvals = self . selector production ( tokens ) if operator == ',' : results . extend ( rvals ) elif operator == '>' : results = self . parents ( results , rvals ) elif operator == '~' : results = self . siblings ( results , rvals ) elif operator == ' ' : results = self . ancestors ( results , rvals ) else : raise Selector Syntax Error ( \"unrecognized operator '%s'\" % operator ) else : if len ( tokens ) : rvals = self . selector production ( tokens ) results = self . ancestors ( results , rvals ) return results", "predictions": ["selector for production ( ( )"], "references": ["production for a full selector ."], "bleu": 0.2626909894424158, "rouge_l": 0.16666666666666666}
{"id": 4037, "code": "def parents ( self , lhs , rhs ) : return [ node for node in rhs if node . parent in lhs ]", "predictions": ["normalize the given list of edges so we can later later be removed from the current thread ."], "references": ["find nodes in rhs which have parents in lhs ."], "bleu": 0.06809398432036522, "rouge_l": 0.07530864197530865}
{"id": 4038, "code": "def ancestors ( self , lhs , rhs ) : def search ( node ) : if node in lhs : return True if not node . parent : return False return search ( node . parent ) return [ node for node in rhs if search ( node ) ]", "predictions": ["find the ancestors of this node that contains the given ( ."], "references": ["return nodes from rhs which have ancestors in lhs ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 4039, "code": "def siblings ( self , lhs , rhs ) : parents = [ node . parent for node in lhs ] return [ node for node in rhs if node . parent in parents ]", "predictions": ["extract all parents from the given method that do not include the original parents ."], "references": ["find nodes in rhs having common parents in lhs ."], "bleu": 0.09103526405546068, "rouge_l": 0.1659863945578231}
{"id": 4040, "code": "def nth child production ( self , lexeme , tokens ) : args = self . match ( tokens , 'expr' ) pat = self . nth child pat . match ( args ) if pat . group ( 5 ) : a = 2 b = 1 if pat . group ( 5 ) == 'odd' else 0 elif pat . group ( 6 ) : a = 0 b = int ( pat . group ( 6 ) ) else : sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' a = eval ( sign + coef ) b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 reverse = False if lexeme == 'nth-last-child' : reverse = True def validate ( node ) : \"\"\"This crazy function taken from jsonselect.js:444.\"\"\" if not node . siblings : return False idx = node . idx - 1 tot = node . siblings if reverse : idx = tot - idx else : idx += 1 if a == 0 : m = b == idx else : mod = ( idx - b ) % a m = not mod and ( idx * a + b ) >= 0 return m return validate", "predictions": ["positions two methods at the given lexeme ."], "references": ["parse args and pass them to pclass_func_validator ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4041, "code": "def get Body ( self , url , method = 'GET' , headers = { } , data = None , socket = None ) : if not 'User-Agent' in headers : headers [ 'User-Agent' ] = [ 'Tensor HTTP checker' ] return self . request ( url , method , headers , data , socket )", "predictions": ["makes http post request ."], "references": ["make an http request and return the body"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4042, "code": "def expire ( self , age ) : now = time . time ( ) cache = self . acquire cache ( ) expired = [ k for k , v in cache . items ( ) if ( now - v [ 0 ] ) > age ] for k in expired : if k in cache : del cache [ k ] if k in self . store : del self . store [ k ] self . write cache ( cache )", "predictions": [". set of items to be true , if any are of the cache ."], "references": ["expire any items in the cache older than age seconds"], "bleu": 0.12300686288463772, "rouge_l": 0.2489795918367347}
{"id": 4043, "code": "def set ( self , k , v ) : self . store [ k ] = ( time . time ( ) , v ) self . persist ( )", "predictions": ["use this to set the object ."], "references": ["set a key k to value v"], "bleu": 0.20556680845025982, "rouge_l": 0.14285714285714285}
{"id": 4044, "code": "def get ( self , k ) : if self . changed ( ) : self . read ( ) if k in self . store : return tuple ( self . store [ k ] ) else : return None", "predictions": ["get information about this object ."], "references": ["returns key contents and modify time"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4045, "code": "def contains ( self , k ) : if self . changed ( ) : self . read ( ) return k in self . store . keys ( )", "predictions": ["check if this object is a full call ."], "references": ["return true if key k exists"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4046, "code": "def rendered content ( self ) : template = self . resolve template ( self . template name ) if django . VERSION [ 1 ] < 8 : if template . name . endswith ( '.min' ) : return super ( Minified Js Template Response , self ) . rendered content else : if template . template . name . endswith ( '.min' ) : return super ( Minified Js Template Response , self ) . rendered content content = super ( Minified Js Template Response , self ) . rendered content content = jsmin . jsmin ( content ) return content", "predictions": ["generate a rendered template ."], "references": ["returns a minified version of the javascript content"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4047, "code": "def get ( self , max lines = None ) : rows = [ ] self . get fn ( lambda row : rows . append ( row ) , max lines = max lines ) return rows", "predictions": ["makes the full copy of this class ."], "references": ["returns a big list of all log lines since the last run"], "bleu": 0.10764345432696364, "rouge_l": 0.09651898734177215}
{"id": 4048, "code": "def engine ( self ) : if not hasattr ( self , ' engine' ) : from cryptography . fernet import Fernet from cryptography . hazmat . backends import default backend from cryptography . hazmat . primitives import hashes digest = hashes . Hash ( hashes . SHA256 ( ) , backend = default backend ( ) ) digest . update ( current app . config [ 'SECRET KEY' ] . encode ( 'utf8' ) ) fernet key = urlsafe b64encode ( digest . finalize ( ) ) self . engine = Fernet ( fernet key ) return self . engine", "predictions": ["digest for the default backend ."], "references": ["get cryptographic engine ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4049, "code": "def create token ( self , obj id , extra data ) : return self . engine . encrypt ( super ( Encrypted Token Mix In , self ) . create token ( obj id , extra data ) )", "predictions": ["create a new object for the passed in token ."], "references": ["create a token referencing the object id with extra data ."], "bleu": 0.1855330420017291, "rouge_l": 0.37770897832817335}
{"id": 4050, "code": "def compat validate token ( cls , * args , * * kwargs ) : data = None for algorithm in SUPPORTED DIGEST ALGORITHMS : data = cls ( algorithm name = algorithm ) . validate token ( * args , * * kwargs ) if not data : continue return data", "predictions": ["validate an algorithm and requires that it can be found in an algorithm ."], "references": ["multiple algorithm - compatible token validation ."], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 4051, "code": "def create token ( cls , obj id , data , expires at = None ) : if expires at : s = Timed Secret Link Serializer ( expires at = expires at ) else : s = Secret Link Serializer ( ) return s . create token ( obj id , data )", "predictions": ["create an token to be used in the connection ."], "references": ["create the secret link token ."], "bleu": 0.15851165692617156, "rouge_l": 0.3927038626609442}
{"id": 4052, "code": "def Counter32 ( a , b , delta ) : if b < a : c = 4294967295 - a return ( c + b ) / float ( delta ) return ( b - a ) / float ( delta )", "predictions": ["get the counter32 of the two integers ."], "references": ["32bit counter aggregator with wrapping"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4053, "code": "def Counter64 ( a , b , delta ) : if b < a : c = 18446744073709551615 - a return ( c + b ) / float ( delta ) return ( b - a ) / float ( delta )", "predictions": ["get loglikelihood between two integers"], "references": ["64bit counter aggregator with wrapping"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4054, "code": "def average duration ( total duration , visits ) : if not visits : seconds = 0 else : seconds = int ( round ( total duration / Decimal ( visits ) ) ) duration = timedelta ( seconds = seconds ) return str ( duration )", "predictions": ["average the average value to get the average value ."], "references": ["method to calculate and format an average duration safely"], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 4055, "code": "def setup Sources ( self , config ) : sources = config . get ( 'sources' , [ ] ) for source in sources : src = self . create Source ( source ) self . setup Triggers ( source , src ) self . sources . append ( src )", "predictions": ["generate source graph from source ."], "references": ["sets up source objects from the given config"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4056, "code": "def validate expires at ( form , field ) : if form . accept . data : if not field . data or datetime . utcnow ( ) . date ( ) >= field . data : raise validators . Stop Validation ( ( \"Please provide a future date.\" ) ) if not field . data or datetime . utcnow ( ) . date ( ) + timedelta ( days = 365 ) < field . data : raise validators . Stop Validation ( ( \"Please provide a date no more than 1 year into the future.\" ) )", "predictions": ["validates that the field must be filled with the input form ."], "references": ["validate that date is in the future ."], "bleu": 0.1235622127262679, "rouge_l": 0.3112244897959184}
{"id": 4057, "code": "def validate accept ( form , field ) : if field . data and form . reject . data : raise validators . Validation Error ( ( \"Both reject and accept cannot be set at the same time.\" ) )", "predictions": ["validates if the given form is valid for this form ."], "references": ["validate that accept have not been set ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 4058, "code": "def validate reject ( form , field ) : if field . data and form . accept . data : raise validators . Validation Error ( ( \"Both reject and accept cannot be set at the same time.\" ) )", "predictions": ["validates if a given form is valid ."], "references": ["validate that accept have not been set ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4059, "code": "def verify token ( ) : try : from . models import Secret Link token = request . args [ 'token' ] if token and Secret Link . validate token ( token , { } ) : session [ 'accessrequests-secret-token' ] = token except Key Error : pass", "predictions": ["verify an integrity token against a token ."], "references": ["verify token and save in session if it s valid ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 4060, "code": "def init app ( self , app ) : app . before request ( verify token ) self . init config ( app ) state = App State ( app = app ) app . extensions [ 'zenodo-accessrequests' ] = state", "predictions": ["initializes the app object ."], "references": ["flask application initialization ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4061, "code": "def name ( self ) : if ( self . device type and self . device type . code in ( Device Type . MOBILE , Device Type . TABLET ) ) : return self . device else : return self . browser", "predictions": ["get the name of this type ."], "references": ["return a basic meaningful name based on device type"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4062, "code": "def warn node ( self , msg , * args , * * kwargs ) : if not msg . startswith ( 'nonlocal image URI found:' ) : warn node old ( self , msg , * args , * * kwargs )", "predictions": ["this method draws a solid operation around the current node ."], "references": ["do not warn on external images ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 4063, "code": "def connect receivers ( ) : request created . connect ( send email validation ) request confirmed . connect ( send confirmed notifications ) request rejected . connect ( send reject notification ) request accepted . connect ( create secret link ) request accepted . connect ( send accept notification )", "predictions": ["connect to the specified notification ."], "references": ["connect receivers to signals ."], "bleu": 0.2626909894424158, "rouge_l": 0.5545454545454546}
{"id": 4064, "code": "def create secret link ( request , message = None , expires at = None ) : pid , record = get record ( request . recid ) if not record : raise Record Not Found ( request . recid ) description = render template ( \"zenodo accessrequests/link description.tpl\" , request = request , record = record , pid = pid , expires at = expires at , message = message , ) request . create secret link ( record [ \"title\" ] , description = description , expires at = expires at )", "predictions": ["save a weights from the specified path ."], "references": ["receiver for request - accepted signal ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4065, "code": "def send accept notification ( request , message = None , expires at = None ) : pid , record = get record ( request . recid ) send notification ( request . sender email , ( \"Access request accepted\" ) , \"zenodo accessrequests/emails/accepted.tpl\" , request = request , record = record , pid = pid , record link = request . link . get absolute url ( 'invenio records ui.recid' ) , message = message , expires at = expires at , )", "predictions": ["sends a message to the process ."], "references": ["receiver for request - accepted signal to send email notification ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4066, "code": "def send confirmed notifications ( request ) : pid , record = get record ( request . recid ) if record is None : current app . logger . error ( \"Cannot retrieve record %s. Emails not sent\" % request . recid ) return title = ( \"Access request: %(record)s\" , record = record [ \"title\" ] ) send notification ( request . receiver . email , title , \"zenodo accessrequests/emails/new request.tpl\" , request = request , record = record , pid = pid , ) send notification ( request . sender email , title , \"zenodo accessrequests/emails/confirmation.tpl\" , request = request , record = record , pid = pid , )", "predictions": ["sends an email on all the attached notification and sends a message to all the attached notification ."], "references": ["receiver for request - confirmed signal to send email notification ."], "bleu": 0.101824256461955, "rouge_l": 0.21631205673758863}
{"id": 4067, "code": "def send email validation ( request ) : token = Email Confirmation Serializer ( ) . create token ( request . id , dict ( email = request . sender email ) ) pid , record = get record ( request . recid ) send notification ( request . sender email , ( \"Access request verification\" ) , \"zenodo accessrequests/emails/validate email.tpl\" , request = request , record = record , pid = pid , days = timedelta ( seconds = current app . config [ \"ACCESSREQUESTS CONFIRMLINK EXPIRES IN\" ] ) . days , confirm link = url for ( \"invenio records ui.recid access request email confirm\" , pid value = request . recid , token = token , external = True , ) )", "predictions": ["sends an ( ( ) to all the registered self - completed records and sends it to the process ."], "references": ["receiver for request - created signal to send email notification ."], "bleu": 0.07264339766175722, "rouge_l": 0.20424107142857142}
{"id": 4068, "code": "def send reject notification ( request , message = None ) : pid , record = get record ( request . recid ) send notification ( request . sender email , ( \"Access request rejected\" ) , \"zenodo accessrequests/emails/rejected.tpl\" , request = request , record = record , pid = pid , message = message , )", "predictions": ["helper method to send ( match a reference to a member = , = , tokens = new ( = new ( tokens = new ( self - , tokens = new ( tokens = new ( tokens ."], "references": ["receiver for request - rejected signal to send email notification ."], "bleu": 0.04620856909230221, "rouge_l": 0.1334792122538293}
{"id": 4069, "code": "def send notification ( to , subject , template , * * ctx ) : msg = Message ( subject , sender = current app . config . get ( 'SUPPORT EMAIL' ) , recipients = [ to ] ) msg . body = render template ( template , * * ctx ) send email . delay ( msg . dict )", "predictions": ["sends a message to the specified )"], "references": ["render a template and send as email ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4070, "code": "def create ( cls , title , owner , extra data , description = \"\" , expires at = None ) : if isinstance ( expires at , date ) : expires at = datetime . combine ( expires at , datetime . min . time ( ) ) with db . session . begin nested ( ) : obj = cls ( owner = owner , title = title , description = description , expires at = expires at , token = '' , ) db . session . add ( obj ) with db . session . begin nested ( ) : obj . token = Secret Link Factory . create token ( obj . id , extra data , expires at = expires at ) . decode ( 'utf8' ) link created . send ( obj ) return obj", "predictions": ["ancestors a link to the database"], "references": ["create a new secret link ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 4071, "code": "def revoke ( self ) : if self . revoked at is None : with db . session . begin nested ( ) : self . revoked at = datetime . utcnow ( ) link revoked . send ( self ) return True return False", "predictions": ["flushes the request for the specific return type ."], "references": ["revoken a secret link ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4072, "code": "def get by receiver ( cls , request id , user ) : return cls . query . filter by ( id = request id , receiver user id = user . id ) . first ( )", "predictions": ["nth method to nth ) the given production object ."], "references": ["get access request for a specific receiver ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 4073, "code": "def confirm email ( self ) : with db . session . begin nested ( ) : if self . status != Request Status . EMAIL VALIDATION : raise Invalid Request State Error ( Request Status . EMAIL VALIDATION ) self . status = Request Status . PENDING request confirmed . send ( self )", "predictions": ["get the specified email object as a } ."], "references": ["confirm that senders email is valid ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4074, "code": "def create secret link ( self , title , description = None , expires at = None ) : self . link = Secret Link . create ( title , self . receiver , extra data = dict ( recid = self . recid ) , description = description , expires at = expires at , ) return self . link", "predictions": ["creates the self - specific self - specific self - specific driver ."], "references": ["create a secret link from request ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 4075, "code": "def is embargoed ( record ) : return record . get ( 'access right' ) == 'embargoed' and record . get ( 'embargo date' ) and record . get ( 'embargo date' ) > datetime . utcnow ( ) . date ( )", "predictions": ["check if the , is a , i . e . , the , if it ' s not a , it ' s ( self - ( ."], "references": ["template filter to check if a record is embargoed ."], "bleu": 0.0653559376048866, "rouge_l": 0.22488479262672809}
{"id": 4076, "code": "def access request ( pid , record , template , * * kwargs ) : recid = int ( pid . pid value ) datastore = Local Proxy ( lambda : current app . extensions [ 'security' ] . datastore ) if record . get ( 'access right' ) != 'restricted' or not record . get ( 'access conditions' ) : abort ( 404 ) owners = record . get ( 'owners' , [ ] ) record owners = [ datastore . find user ( id = owner id ) for owner id in owners ] if not record owners : abort ( 404 ) sender = None initialdata = dict ( ) if current user . is authenticated : sender = current user initialdata [ 'email' ] = current user . email if current user . profile : initialdata [ 'full name' ] = current user . profile . full name form = Access Request Form ( formdata = request . form , * * initialdata ) if form . validate on submit ( ) : accreq = Access Request . create ( recid = recid , receiver = record owners [ 0 ] , sender full name = form . data [ 'full name' ] , sender email = form . data [ 'email' ] , justification = form . data [ 'justification' ] , sender = sender ) db . session . commit ( ) if accreq . status == Request Status . EMAIL VALIDATION : flash ( ( \"Email confirmation needed: We have sent you an email to \" \"verify your address. Please check the email and follow the \" \"instructions to complete the access request.\" ) , category = 'info' ) else : flash ( ( \"Access request submitted.\" ) , category = 'info' ) return redirect ( url for ( 'invenio records ui.recid' , pid value = recid ) ) return render template ( template , pid = pid , record = record , form = form , owners = record owners , )", "predictions": ["get the get get notification ."], "references": ["create an access request ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4077, "code": "def confirm ( pid , record , template , * * kwargs ) : recid = int ( pid . pid value ) token = request . view args [ 'token' ] data = Email Confirmation Serializer . compat validate token ( token ) if data is None : flash ( ( \"Invalid confirmation link.\" ) , category = 'danger' ) return redirect ( url for ( \"invenio records ui.recid\" , pid value = recid ) ) r = Access Request . query . get ( data [ 'id' ] ) if not r : abort ( 404 ) if r . status != Request Status . EMAIL VALIDATION : abort ( 404 ) r . confirm email ( ) db . session . commit ( ) flash ( ( \"Email validated and access request submitted.\" ) , category = 'info' ) return redirect ( url for ( \"invenio records ui.recid\" , pid value = recid ) )", "predictions": ["build an confirmation for the self - confirmation ."], "references": ["confirm email address ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4078, "code": "def get endpoint ( self ) : return SSH Command Client Endpoint . new Connection ( reactor , b'/bin/cat' , self . username , self . hostname , port = self . port , keys = self . keys , password = self . password , known Hosts = self . known Hosts )", "predictions": ["get a new content of the [ server ] ."], "references": ["creates a generic endpoint connection that doesn t finish"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 4079, "code": "def reverse ( self , col ) : if col in self . options : if self . is selected ( col ) : return col if not self . asc else '-{0}' . format ( col ) else : return col return None", "predictions": ["get to get a hierarchy for a column ."], "references": ["get reverse direction of ordering ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 4080, "code": "def selected ( self ) : if self . selected : return self . selected if self . asc else \"-{0}\" . format ( self . selected ) return None", "predictions": ["a simple engine for the current thread ."], "references": ["get column which is being order by ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4081, "code": "def items ( self ) : if self . asc is not None : if self . selected and self . asc : return self . query . order by ( self . selected ) elif self . selected and not self . asc : return self . query . order by ( desc ( self . selected ) ) return self . query", "predictions": ["for each item in the list ."], "references": ["get query with correct ordering ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4082, "code": "def records ( ) : import uuid from invenio records . api import Record from invenio pidstore . models import Persistent Identifier , PID Status create test user ( ) indexer = Record Indexer ( ) with db . session . begin nested ( ) : rec uuid = uuid . uuid4 ( ) pid1 = Persistent Identifier . create ( 'recid' , '1' , object type = 'rec' , object uuid = rec uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered' , 'description' : 'This is an awesome description' , 'control number' : '1' , 'access right' : 'restricted' , 'access conditions' : 'fuu' , 'owners' : [ 1 , 2 ] , 'recid' : 1 } , id = rec uuid ) indexer . index by id ( pid1 . object uuid ) db . session . commit ( ) sleep ( 3 )", "predictions": ["creates a new ( object ."], "references": ["load test data fixture ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4083, "code": "def init ssh ( self ) : self . ssh host = self . config . get ( 'ssh host' , self . hostname ) self . known hosts = self . config . get ( 'ssh knownhosts file' , self . tensor . config . get ( 'ssh knownhosts file' , None ) ) self . ssh keyfile = self . config . get ( 'ssh keyfile' , self . tensor . config . get ( 'ssh keyfile' , None ) ) self . ssh key = self . config . get ( 'ssh key' , self . tensor . config . get ( 'ssh key' , None ) ) self . ssh keypass = self . config . get ( 'ssh keypass' , self . tensor . config . get ( 'ssh keypass' , None ) ) self . ssh user = self . config . get ( 'ssh username' , self . tensor . config . get ( 'ssh username' , None ) ) self . ssh password = self . config . get ( 'ssh password' , self . tensor . config . get ( 'ssh password' , None ) ) self . ssh port = self . config . get ( 'ssh port' , self . tensor . config . get ( 'ssh port' , 22 ) ) if not ( self . ssh key or self . ssh keyfile or self . ssh password ) : raise Exception ( \"To use SSH you must specify *one* of ssh key,\" \" ssh keyfile or ssh password for this source\" \" check or globally\" ) if not self . ssh user : raise Exception ( \"ssh username must be set\" ) self . ssh keydb = [ ] c Hash = hashlib . sha1 ( ':' . join ( ( self . ssh host , self . ssh user , str ( self . ssh port ) , str ( self . ssh password ) , str ( self . ssh key ) , str ( self . ssh keyfile ) ) ) . encode ( ) ) . hexdigest ( ) if c Hash in self . tensor . host Connector Cache : self . ssh client = self . tensor . host Connector Cache . get ( c Hash ) self . ssh connector = False else : self . ssh connector = True self . ssh client = ssh . SSH Client ( self . ssh host , self . ssh user , self . ssh port , password = self . ssh password , knownhosts = self . known hosts ) if self . ssh keyfile : self . ssh client . add Key File ( self . ssh keyfile , self . ssh keypass ) if self . ssh key : self . ssh client . add Key String ( self . ssh key , self . ssh keypass ) self . tensor . host Connector Cache [ c Hash ] = self . ssh client", "predictions": ["initialize the token object ."], "references": ["configure ssh client options"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4084, "code": "def start Timer ( self ) : self . td = self . t . start ( self . inter ) if self . use ssh and self . ssh connector : self . ssh client . connect ( )", "predictions": ["use this method to start the last started operation ."], "references": ["starts the timer for this source"], "bleu": 0.13950796967929133, "rouge_l": 0.13090128755364808}
{"id": 4085, "code": "def create Event ( self , state , description , metric , prefix = None , hostname = None , aggregation = None , evtime = None ) : if prefix : service name = self . service + \".\" + prefix else : service name = self . service return Event ( state , service name , description , metric , self . ttl , hostname = hostname or self . hostname , aggregation = aggregation , evtime = evtime , tags = self . tags , attributes = self . attributes )", "predictions": ["create method to be called from within the java class ."], "references": ["creates an event object from the source configuration"], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 4086, "code": "def create Log ( self , type , data , evtime = None , hostname = None ) : return Event ( None , type , data , 0 , self . ttl , hostname = hostname or self . hostname , evtime = evtime , tags = self . tags , type = 'log' )", "predictions": ["this method creates the health file ."], "references": ["creates an event object from the source configuration"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4087, "code": "def index ( ) : query = request . args . get ( 'query' , '' ) order = request . args . get ( 'sort' , '-created' ) try : page = int ( request . args . get ( 'page' , 1 ) ) per page = int ( request . args . get ( 'per page' , 20 ) ) except ( Type Error , Value Error ) : abort ( 404 ) form = Delete Form ( request . form ) if form . validate on submit ( ) : link = Secret Link . query by owner ( current user ) . filter by ( id = form . link . data ) . first ( ) if link . revoke ( ) : flash ( ( \"Shared link revoked.\" ) , category = 'success' ) db . session . commit ( ) links = Secret Link . query by owner ( current user ) . filter ( Secret Link . revoked at . is ( None ) ) if query : lquery = \"%{0}%\" . format ( query ) links = links . filter ( Secret Link . title . like ( lquery ) | Secret Link . description . like ( lquery ) ) ordering = Query Ordering ( links , [ 'title' , 'created' , 'expires at' ] , order ) links = ordering . items ( ) requests = Access Request . query by receiver ( current user ) . filter by ( status = Request Status . PENDING ) . order by ( 'created' ) return render template ( \"zenodo accessrequests/settings/index.html\" , links pagination = links . paginate ( page , per page = per page ) , requests = requests , query = query , order = ordering , get record = get record , form = Delete Form ( ) , )", "predictions": ["generate a setup for the given , ) ."], "references": ["list pending access requests and shared links ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4088, "code": "def create Client ( self ) : server = self . config . get ( 'server' , 'localhost' ) port = self . config . get ( 'port' , 5555 ) failover = self . config . get ( 'failover' , False ) self . factory = riemann . Riemann Client Factory ( server , failover = failover ) if failover : initial = random . choice ( server ) else : initial = server log . msg ( 'Connecting to Riemann on %s:%s' % ( initial , port ) ) if self . tls : if SSL : self . connector = reactor . connect SSL ( initial , port , self . factory , Client TLS Context ( self . key , self . cert ) ) else : log . msg ( '[FATAL] SSL support not available!' ' Please install Py Open SSL. Exiting now' ) reactor . stop ( ) else : self . connector = reactor . connect TCP ( initial , port , self . factory ) d = defer . Deferred ( ) def cb ( ) : if hasattr ( self . factory , 'proto' ) and self . factory . proto : self . t . start ( self . inter ) d . callback ( None ) else : reactor . call Later ( 0.01 , cb ) cb ( ) return d", "predictions": ["creates a new instance of the field ' s field ."], "references": ["create a tcp connection to riemann with automatic reconnection"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 4089, "code": "def stop ( self ) : self . t . stop ( ) self . factory . stop Trying ( ) self . connector . disconnect ( )", "predictions": ["stops the currently running container ."], "references": ["stop this client ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4090, "code": "def tick ( self ) : if self . factory . proto : if ( self . pressure < 0 ) or ( self . factory . proto . pressure <= self . pressure ) : self . empty Queue ( ) elif self . expire : for i , e in enumerate ( self . events ) : if ( time . time ( ) - e . time ) > e . ttl : self . events . pop ( i )", "predictions": ["generate a new negation ."], "references": ["clock tick called every self . inter"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4091, "code": "def empty Queue ( self ) : if self . events : if self . queue Depth and ( len ( self . events ) > self . queue Depth ) : events = self . events [ : self . queue Depth ] self . events = self . events [ self . queue Depth : ] else : events = self . events self . events = [ ] if self . allow nan : self . factory . proto . send Events ( events ) else : self . factory . proto . send Events ( [ e for e in events if e . metric is not None ] )", "predictions": ["verify this queue is a process or the metric method ."], "references": ["remove all or self . queuedepth events from the queue"], "bleu": 0.14323145079400493, "rouge_l": 0.1921259842519685}
{"id": 4092, "code": "def create Client ( self ) : server = self . config . get ( 'server' , '127.0.0.1' ) port = self . config . get ( 'port' , 5555 ) def connect ( ip ) : self . protocol = riemann . Riemann UDP ( ip , port ) self . endpoint = reactor . listen UDP ( 0 , self . protocol ) d = reactor . resolve ( server ) d . add Callback ( connect ) return d", "predictions": ["create a protocol from the database and extensions ."], "references": ["create a udp connection to riemann"], "bleu": 0.18575057999133596, "rouge_l": 0.27664399092970515}
{"id": 4093, "code": "def create Client ( self ) : server = self . config . get ( 'server' , 'localhost' ) port = int ( self . config . get ( 'port' , 9200 ) ) self . client = elasticsearch . Elastic Search ( self . url , self . user , self . password , self . index ) self . t . start ( self . inter )", "predictions": ["creates a else returns the ."], "references": ["sets up http connector and starts queue timer"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 4094, "code": "def tick ( self ) : if self . events : if self . queue Depth and ( len ( self . events ) > self . queue Depth ) : events = self . events [ : self . queue Depth ] self . events = self . events [ self . queue Depth : ] else : events = self . events self . events = [ ] try : result = yield self . send Events ( events ) if result . get ( 'errors' , False ) : log . msg ( repr ( result ) ) self . events . extend ( events ) except Exception as e : log . msg ( 'Could not connect to elasticsearch ' + str ( e ) ) self . events . extend ( events )", "predictions": ["send an warn event ."], "references": ["clock tick called every self . inter"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4095, "code": "def encode Event ( self , event ) : pbevent = proto pb2 . Event ( time = int ( event . time ) , state = event . state , service = event . service , host = event . hostname , description = event . description , tags = event . tags , ttl = event . ttl , ) if event . metric is not None : if isinstance ( event . metric , int ) : pbevent . metric sint64 = event . metric pbevent . metric f = float ( event . metric ) else : pbevent . metric d = float ( event . metric ) pbevent . metric f = float ( event . metric ) if event . attributes is not None : for key , value in event . attributes . items ( ) : attribute = pbevent . attributes . add ( ) attribute . key , attribute . value = key , value return pbevent", "predictions": ["connect the series of bytes to the underlying metric ."], "references": ["adapts an event object to a riemann protobuf event event"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 4096, "code": "def encode Message ( self , events ) : message = proto pb2 . Msg ( events = [ self . encode Event ( e ) for e in events if e . type == 'riemann' ] ) return message . Serialize To String ( )", "predictions": ["encodes the messages to a string ."], "references": ["encode a list of tensor events with protobuf"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4097, "code": "def decode Message ( self , data ) : message = proto pb2 . Msg ( ) message . Parse From String ( data ) return message", "predictions": ["we need to forget the assumed to send the message back to a single message ."], "references": ["decode a protobuf message into a list of tensor events"], "bleu": 0.08513012360883544, "rouge_l": 0.16052631578947368}
{"id": 4098, "code": "def send Events ( self , events ) : self . pressure += 1 self . send String ( self . encode Message ( events ) )", "predictions": ["send an event with the specified events ."], "references": ["send a tensor event to riemann"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 4099, "code": "def generate ( ctx , url , * args , * * kwargs ) : file previews = ctx . obj [ 'file previews' ] options = { } metadata = kwargs [ 'metadata' ] width = kwargs [ 'width' ] height = kwargs [ 'height' ] output format = kwargs [ 'format' ] if metadata : options [ 'metadata' ] = metadata . split ( ',' ) if width : options . setdefault ( 'size' , { } ) options [ 'size' ] [ 'width' ] = width if height : options . setdefault ( 'size' , { } ) options [ 'size' ] [ 'height' ] = height if output format : options [ 'format' ] = output format results = file previews . generate ( url , * * options ) click . echo ( results )", "predictions": ["creates the bridge to generate the oauth file ."], "references": ["generate preview for url ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 4100, "code": "def retrieve ( ctx , preview id , * args , * * kwargs ) : file previews = ctx . obj [ 'file previews' ] results = file previews . retrieve ( preview id ) click . echo ( results )", "predictions": ["retrieves the object reference from the set of preview files ."], "references": ["retreive preview results for id ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 4101, "code": "def message loop ( self , t q , r q ) : t msg = { } while t msg . get ( \"state\" , \"\" ) != \" DIE \" : try : t msg = t q . get ( True , self . cycle sleep ) self . task = t msg . get ( \"task\" , \"\" ) if self . task != \"\" : self . task . task start = time . time ( ) self . r q send ( { \"w id\" : self . w id , \"task\" : self . task , \"state\" : \" ACK \" } ) self . cycle sleep = self . task . worker loop delay self . task . result = self . task . run ( ) self . task . task stop = time . time ( ) self . r q send ( { \"w id\" : self . w id , \"task\" : self . task , \"state\" : \" FINISHED \" } ) self . task = None except Empty : pass except Full : time . sleep ( 0.1 ) except : if self . task is not None : self . task . task stop = time . time ( ) tb str = \"\" . join ( tb . format exception ( * ( sys . exc info ( ) ) ) ) self . r q send ( { \"w id\" : self . w id , \"task\" : self . task , \"error\" : tb str , \"state\" : \" ERROR \" , } ) return", "predictions": ["sends a heartbeat on this task ."], "references": ["loop through messages and execute tasks"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4102, "code": "def log time ( self ) : if self . hot loop and self . time delta >= self . log interval : return True return False", "predictions": ["a log message handler for the specific interval ."], "references": ["return true if it s time to log"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4103, "code": "def log message ( self ) : time delta = deepcopy ( self . time delta ) total work time = self . worker count * time delta time worked = sum ( self . exec times ) pct busy = time worked / total work time * 100.0 min task time = min ( self . exec times ) avg task time = sum ( self . exec times ) / len ( self . exec times ) max task time = max ( self . exec times ) min queue time = min ( self . queue times ) avg queue time = sum ( self . queue times ) / len ( self . queue times ) max queue time = max ( self . queue times ) time delta = self . time delta total tasks = len ( self . exec times ) avg task rate = total tasks / time delta self . reset ( ) task msg = \"\"\"Ran {0} tasks, {1} tasks/s; {2} workers {3}% busy\"\"\" . format ( total tasks , round ( avg task rate , 1 ) , self . worker count , round ( pct busy , 1 ) ) task mam = \"\"\"     Task run times: {0}/{1}/{2} (min/avg/max)\"\"\" . format ( round ( min task time , 3 ) , round ( avg task time , 3 ) , round ( max task time , 3 ) ) queue mam = \"\"\"     Time in queue: {0}/{1}/{2} (min/avg/max)\"\"\" . format ( round ( min queue time , 6 ) , round ( avg queue time , 6 ) , round ( max queue time , 6 ) ) return \"\"\"{0}\\n{1}\\n{2}\"\"\" . format ( task msg , task mam , queue mam )", "predictions": ["a log of the ( ."], "references": ["build a log message and reset the stats"], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 4104, "code": "def post Construction ( self ) : self . set Window Title ( 'Filesystem Browser' ) self . filesystem Widget . sort By Column ( 0 , Qt Core . Qt . Ascending Order ) self . bookmarks Widget . hide ( ) self . accept Button . set Default ( True ) self . accept Button . set Disabled ( True ) self . accept Button . clicked . connect ( self . accept ) self . cancel Button . clicked . connect ( self . reject ) self . configure Shortcuts ( ) self . set Location ( self . root ) self . filesystem Widget . horizontal Header ( ) . set Resize Mode ( Qt Gui . Q Header View . Resize To Contents ) self . filesystem Widget . horizontal Header ( ) . set Resize Mode ( 0 , Qt Gui . Q Header View . Stretch ) self . up Button . clicked . connect ( self . on Navigate Up Button Clicked ) self . location Widget . current Index Changed . connect ( self . on Navigate ) self . filesystem Widget . activated . connect ( self . on Activate Item ) selection Model = self . filesystem Widget . selection Model ( ) selection Model . current Row Changed . connect ( self . on Select Item )", "predictions": ["post / disable bookmarks ."], "references": ["perform post - construction operations ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 4105, "code": "def configure Shortcuts ( self ) : self . up Shortcut = Qt Gui . Q Shortcut ( Qt Gui . Q Key Sequence ( 'Backspace' ) , self ) self . up Shortcut . set Auto Repeat ( False ) self . up Shortcut . activated . connect ( self . on Navigate Up Button Clicked )", "predictions": ["use this to configure the ` subsystem ' s base class ."], "references": ["add keyboard shortcuts to navigate the filesystem ."], "bleu": 0.1235622127262679, "rouge_l": 0.3112244897959184}
{"id": 4106, "code": "def on Activate Item ( self , index ) : item = self . filesystem Widget . model ( ) . item ( index ) if not isinstance ( item , riffle . model . File ) : self . accept Button . set Disabled ( True ) self . set Location ( item . path , interactive = True )", "predictions": ["creates a new item ."], "references": ["handle activation of item in listing ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4107, "code": "def on Select Item ( self , selection , previous Selection ) : self . accept Button . set Enabled ( True ) del self . selected [ : ] item = self . filesystem Widget . model ( ) . item ( selection ) self . selected . append ( item . path )", "predictions": ["passes this to the currently selected item in a list ."], "references": ["handle selection of item in listing ."], "bleu": 0.16108992769687397, "rouge_l": 0.3472485768500949}
{"id": 4108, "code": "def on Navigate ( self , index ) : if index > 0 : self . set Location ( self . location Widget . item Data ( index ) , interactive = True )", "predictions": ["a method to monitor the entity at the bottom of the ui element ."], "references": ["handle selection of path segment ."], "bleu": 0.09782375748961449, "rouge_l": 0.21554770318021202}
{"id": 4109, "code": "def segment Path ( self , path ) : parts = [ ] model = self . filesystem Widget . model ( ) remainder = path while True : if remainder == model . root . path : break if remainder : parts . append ( remainder ) head , tail = os . path . split ( remainder ) if head == remainder : break remainder = head parts . append ( model . root . path ) return parts", "predictions": ["return the segment that is contained in this directory ."], "references": ["return list of valid * path * segments ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 4110, "code": "def finalize options ( self ) : self . resource source path = os . path . join ( RESOURCE PATH , 'resource.qrc' ) self . resource target path = RESOURCE TARGET PATH", "predictions": ["generate a resource object that can be loaded by this object ."], "references": ["finalize options to be used ."], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 4111, "code": "def add Child ( self , item ) : if item . parent and item . parent != self : item . parent . remove Child ( item ) self . children . append ( item ) item . parent = self", "predictions": ["add child to this strategy ."], "references": ["add * item * as child of this item ."], "bleu": 0.14260771622124252, "rouge_l": 0.47843137254901963}
{"id": 4112, "code": "def fetch Children ( self ) : children = [ ] for entry in Q Dir . drives ( ) : path = os . path . normpath ( entry . canonical File Path ( ) ) children . append ( Mount ( path ) ) return children", "predictions": ["fetches all child drives of this node ."], "references": ["fetch and return new child items ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4113, "code": "def fetch Children ( self ) : children = [ ] paths = [ ] for name in os . listdir ( self . path ) : paths . append ( os . path . normpath ( os . path . join ( self . path , name ) ) ) collections , remainder = clique . assemble ( paths , [ clique . PATTERNS [ 'frames' ] ] ) for path in remainder : try : child = Item Factory ( path ) except Value Error : pass else : children . append ( child ) for collection in collections : children . append ( Collection ( collection ) ) return children", "predictions": ["fetch all the paths of this collection ."], "references": ["fetch and return new child items ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4114, "code": "def fetch Children ( self ) : children = [ ] for path in self . collection : try : child = Item Factory ( path ) except Value Error : pass else : children . append ( child ) return children", "predictions": ["fetches all children of this collection ."], "references": ["fetch and return new child items ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4115, "code": "def row Count ( self , parent ) : if parent . column ( ) > 0 : return 0 if parent . is Valid ( ) : item = parent . internal Pointer ( ) else : item = self . root return len ( item . children )", "predictions": ["counts the number of columns in this row ."], "references": ["return number of children * parent * index has ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 4116, "code": "def index ( self , row , column , parent ) : if not self . has Index ( row , column , parent ) : return Q Model Index ( ) if not parent . is Valid ( ) : item = self . root else : item = parent . internal Pointer ( ) try : child = item . children [ row ] except Index Error : return Q Model Index ( ) else : return self . create Index ( row , column , child )", "predictions": ["create an array of entries representing a column in this model ."], "references": ["return index for * row * and * column * under * parent * ."], "bleu": 0.08955242946910898, "rouge_l": 0.14523809523809522}
{"id": 4117, "code": "def path Index ( self , path ) : if path == self . root . path : return Q Model Index ( ) if not path . startswith ( self . root . path ) : return Q Model Index ( ) parts = [ ] while True : if path == self . root . path : break head , tail = os . path . split ( path ) if head == path : if path : parts . append ( path ) break parts . append ( tail ) path = head parts . reverse ( ) if parts : item = self . root count = 0 for count , part in enumerate ( parts ) : matched = False for child in item . children : if child . name == part : item = child matched = True break if not matched : break if count + 1 == len ( parts ) : return self . create Index ( item . row , 0 , item ) return Q Model Index ( )", "predictions": ["lists all indexes for this directory . if a path is available , then the same index will be created ."], "references": ["return index of item with * path * ."], "bleu": 0.0690889519686715, "rouge_l": 0.143698468786808}
{"id": 4118, "code": "def parent ( self , index ) : if not index . is Valid ( ) : return Q Model Index ( ) item = index . internal Pointer ( ) if not item : return Q Model Index ( ) parent = item . parent if not parent or parent == self . root : return Q Model Index ( ) return self . create Index ( parent . row , 0 , parent )", "predictions": ["lays out an array of trees at a given index . if the parent is not a parent , the parent is returned ."], "references": ["return parent of * index * ."], "bleu": 0.06370405230161802, "rouge_l": 0.2147887323943662}
{"id": 4119, "code": "def data ( self , index , role ) : if not index . is Valid ( ) : return None column = index . column ( ) item = index . internal Pointer ( ) if role == self . ITEM ROLE : return item elif role == Qt . Display Role : if column == 0 : return item . name elif column == 1 : if item . size : return item . size elif column == 2 : return item . type elif column == 3 : if item . modified is not None : return item . modified . strftime ( '%c' ) elif role == Qt . Decoration Role : if column == 0 : return self . icon Factory . icon ( item ) elif role == Qt . Text Alignment Role : if column == 1 : return Qt . Align Right else : return Qt . Align Left return None", "predictions": ["creates and returns a column part of a role ."], "references": ["return data for * index * according to * role * ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 4120, "code": "def header Data ( self , section , orientation , role ) : if orientation == Qt . Horizontal : if section < len ( self . columns ) : column = self . columns [ section ] if role == Qt . Display Role : return column return None", "predictions": ["copies data to orientation ."], "references": ["return label for * section * according to * orientation * and * role * ."], "bleu": 0.035974230453556216, "rouge_l": 0.26105563480741795}
{"id": 4121, "code": "def can Fetch More ( self , index ) : if not index . is Valid ( ) : item = self . root else : item = index . internal Pointer ( ) return item . can Fetch More ( )", "predictions": ["checks if a method can be populated by checking if it can be more ."], "references": ["return if more data available for * index * ."], "bleu": 0.09782375748961449, "rouge_l": 0.2489795918367347}
{"id": 4122, "code": "def fetch More ( self , index ) : if not index . is Valid ( ) : item = self . root else : item = index . internal Pointer ( ) if item . can Fetch More ( ) : start Index = len ( item . children ) additional Children = item . fetch Children ( ) end Index = start Index + len ( additional Children ) - 1 if end Index >= start Index : self . begin Insert Rows ( index , start Index , end Index ) for new Child in additional Children : item . add Child ( new Child ) self . end Insert Rows ( )", "predictions": ["fetches the item at index ."], "references": ["fetch additional data under * index * ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4123, "code": "def less Than ( self , left , right ) : source Model = self . source Model ( ) if source Model : left Item = source Model . item ( left ) right Item = source Model . item ( right ) if ( isinstance ( left Item , Directory ) and not isinstance ( right Item , Directory ) ) : return self . sort Order ( ) == Qt . Ascending Order elif ( not isinstance ( left Item , Directory ) and isinstance ( right Item , Directory ) ) : return self . sort Order ( ) == Qt . Descending Order return super ( Filesystem Sort Proxy , self ) . less Than ( left , right )", "predictions": ["returns a copy of this <robot+framework+plugin> ."], "references": ["return ordering of * left * vs * right * ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4124, "code": "def path Index ( self , path ) : source Model = self . source Model ( ) if not source Model : return Q Model Index ( ) return self . map From Source ( source Model . path Index ( path ) )", "predictions": ["maps a path to a path ."], "references": ["return index of item with * path * ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4125, "code": "def item ( self , index ) : source Model = self . source Model ( ) if not source Model : return None return source Model . item ( self . map To Source ( index ) )", "predictions": ["recursively convert the model model into the model ."], "references": ["return item at * index * ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4126, "code": "def icon ( self , index ) : source Model = self . source Model ( ) if not source Model : return None return source Model . icon ( self . map To Source ( index ) )", "predictions": ["this is a utility method that maps the raw source in a more specific source to the model . this is necessary to preserve the source such as the source is not a region ."], "references": ["return icon for index ."], "bleu": 0.034487891886161, "rouge_l": 0.05781990521327014}
{"id": 4127, "code": "def has Children ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . has Children ( self . map To Source ( index ) )", "predictions": ["recursively goes through the source model ."], "references": ["return if * index * has children ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4128, "code": "def can Fetch More ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . can Fetch More ( self . map To Source ( index ) )", "predictions": ["alias for hostprocess that works on the = ( ."], "references": ["return if more data available for * index * ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 4129, "code": "def fetch More ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . fetch More ( self . map To Source ( index ) )", "predictions": ["remove saved contents from message model ."], "references": ["fetch additional data under * index * ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4130, "code": "def type ( self , item ) : icon Type = Icon Type . Unknown if isinstance ( item , riffle . model . Computer ) : icon Type = Icon Type . Computer elif isinstance ( item , riffle . model . Mount ) : icon Type = Icon Type . Mount elif isinstance ( item , riffle . model . Directory ) : icon Type = Icon Type . Directory elif isinstance ( item , riffle . model . File ) : icon Type = Icon Type . File elif isinstance ( item , riffle . model . Collection ) : icon Type = Icon Type . Collection return icon Type", "predictions": ["creates a snippet from a doc and passes it to the : boolean , or file depending on the type of the : / / www . sun . com / tr / tr / . / . / . / . / . / . / . / ."], "references": ["return appropriate icon type for * item * ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 4131, "code": "def get max fd ( self ) : limits = resource . getrlimit ( resource . RLIMIT NOFILE ) result = limits [ 1 ] if result == resource . RLIM INFINITY : result = maxfd return result", "predictions": ["generate the maximum number of bytes required to serve this : this can only be used to determine the maximum number of bytes currently active ."], "references": ["return the maximum file descriptor value ."], "bleu": 0.0660161823828377, "rouge_l": 0.20288248337028827}
{"id": 4132, "code": "def close fd ( self , fd ) : try : os . close ( fd ) except OS Error , exc : if exc . errno != errno . EBADF : msg = \"Failed to close file descriptor {}: {}\" . format ( fd , exc ) raise Error ( msg )", "predictions": ["retrieve a file ctx ."], "references": ["close a file descriptor if it is open ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 4133, "code": "def close open fds ( self ) : maxfd = self . get max fd ( ) for fd in reversed ( range ( maxfd ) ) : if fd not in self . exclude fds : self . close fd ( fd )", "predictions": ["message to loop loop regardless of this method ."], "references": ["close open file descriptors ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4134, "code": "def redirect ( self , stream , target ) : if target is None : target fd = os . open ( os . devnull , os . O RDWR ) else : target fd = target . fileno ( ) os . dup2 ( target fd , stream . fileno ( ) )", "predictions": ["log a file to the server , using the specified : : : : : : : / / stackoverflow . com / , / , / , / , / , / , / , / , / ,"], "references": ["redirect a system stream to the provided target ."], "bleu": 0.04503778123700045, "rouge_l": 0.18429003021148035}
{"id": 4135, "code": "def is valid s3 url ( url ) : if url . startswith ( 'source:' ) : return True scheme , netloc , path , , , = urlparse ( url ) port except = Remote Port Validation Error ( 'Port value %s is not a valid s3 location' % url ) if len ( scheme ) < 2 : raise port except if 's3' in scheme or 's3' in netloc or 's3' in path : return True else : raise port except", "predictions": ["checks if the self - encoded self is a valid ( see if it contains a valid ( internal . . . : http : / / www . google . com / tr / google / google / google / ( / ( / ( / ( / ("], "references": ["checks if the url contains s3 . not an accurate validation of the url"], "bleu": 0.04949727050808081, "rouge_l": 0.17388825541619157}
{"id": 4136, "code": "def get template abs path ( filename ) : if os . path . isabs ( filename ) and os . path . isfile ( filename ) : return filename else : return os . path . join ( os . getcwd ( ) , filename )", "predictions": ["parses a self - insensitive template file ."], "references": ["return a valid absolute path . filename can be relative or absolute ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 4137, "code": "def list ( self , s3 folder = '' , full key data = False ) : if not s3 folder . startswith ( '/' ) : s3 folder = '/' + s3 folder s3 prefix = self . prefix + s3 folder bucket data = self . client . list objects ( Bucket = self . bucket , Prefix = s3 prefix ) if full key data : return bucket data [ 'Contents' ] else : return [ k [ 'Key' ] for k in bucket data [ 'Contents' ] ]", "predictions": ["see if there are ) an ) method for a list of ) ."], "references": ["get a list of keys for the accounts"], "bleu": 0.17395797375642236, "rouge_l": 0.28683385579937304}
{"id": 4138, "code": "def build worklfow json ( self ) : wf json = { 'tasks' : [ ] , 'name' : 'cloud-harness %s' % str ( uuid . uuid4 ( ) ) } task def = json . loads ( self . task template . json ( ) ) d = { \"name\" : task def [ 'name' ] , \"outputs\" : [ ] , \"inputs\" : [ ] , \"task Type\" : task def [ 'task Type' ] } for port in self . task template . input ports : port value = port . value if port value is False : port value = 'false' if port value is True : port value = 'true' d [ 'inputs' ] . append ( { \"name\" : port . name , \"value\" : port value } ) for port in self . task template . output ports : d [ 'outputs' ] . append ( { \"name\" : port . name } ) wf json [ 'tasks' ] . append ( d ) for port in self . task template . output ports : if hasattr ( port , 'stage To S3' ) and port . stage To S3 : save location = '{customer storage}/{run name}/{port}' . format ( customer storage = self . storage . location , run name = self . task template . run name , port = port . name ) new task = dict ( * * self . STAGE TO S3 ) new task [ 'inputs' ] = [ { 'name' : 'data' , 'source' : '%s:%s' % ( task def [ 'name' ] , port . name ) } , { 'name' : 'destination' , 'value' : save location } ] wf json [ 'tasks' ] . append ( new task ) return wf json", "predictions": ["create a task from the command line ."], "references": ["build a workflow definition from the cloud_harness task ."], "bleu": 0.22149455506955362, "rouge_l": 0.465648854961832}
{"id": 4139, "code": "def execute ( self , override wf json = None ) : r = self . gbdx . post ( self . URL , json = self . json if override wf json is None else override wf json ) try : r . raise for status ( ) except : print ( \"GBDX API Status Code: %s\" % r . status code ) print ( \"GBDX API Response: %s\" % r . text ) self . id = None return self . id = r . json ( ) [ 'id' ] self . refresh status ( )", "predictions": ["passes passes results in this object and returns the result ."], "references": ["execute the cloud_harness task ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 4140, "code": "def archive ( folder , dry run = False ) : for f in folder : if not os . path . exists ( f ) : bail ( 'folder does not exist: ' + f ) archive safe ( folder , PROJ ARCHIVE , dry run = dry run )", "predictions": ["checks all files in the on the ( as well if they are the same if they are the same ."], "references": ["move an active project to the archive ."], "bleu": 0.06429451441231726, "rouge_l": 0.15006150061500614}
{"id": 4141, "code": "def mkdir ( p ) : isdir = os . path . isdir stack = [ os . path . abspath ( p ) ] while not isdir ( stack [ - 1 ] ) : parent dir = os . path . dirname ( stack [ - 1 ] ) stack . append ( parent dir ) while stack : p = stack . pop ( ) if not isdir ( p ) : os . mkdir ( p )", "predictions": ["create a new directory ."], "references": ["the equivalent of mkdir - p in shell ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4142, "code": "def list ( pattern = ( ) ) : globs = [ '*{0}*' . format ( p ) for p in pattern ] + [ '*' ] matches = [ ] offset = len ( PROJ ARCHIVE ) + 1 for suffix in globs : glob pattern = os . path . join ( PROJ ARCHIVE , '*' , '*' , suffix ) matches . append ( set ( f [ offset : ] for f in glob . glob ( glob pattern ) ) ) matches = reduce ( lambda x , y : x . intersection ( y ) , matches ) for m in sorted ( matches ) : print ( m )", "predictions": ["finalize all possible expressions ."], "references": ["list the contents of the archive directory ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4143, "code": "def restore ( folder ) : if os . path . isdir ( folder ) : bail ( 'a folder of the same name already exists!' ) pattern = os . path . join ( PROJ ARCHIVE , '*' , '*' , folder ) matches = glob . glob ( pattern ) if not matches : bail ( 'no project matches: ' + folder ) if len ( matches ) > 1 : print ( 'Warning: multiple matches, picking the most recent' , file = sys . stderr ) source = sorted ( matches ) [ - 1 ] print ( source , '-->' , folder ) shutil . move ( source , '.' )", "predictions": ["restores the previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously previously sorted ."], "references": ["restore a project from the archive ."], "bleu": 0.03026457500336624, "rouge_l": 0.09023668639053256}
{"id": 4144, "code": "def validate storage path ( cls , path , projects allowed = True ) : if not path or not isinstance ( path , str ) or path [ 0 ] != '/' or path == '/' : raise Storage Argument Exception ( 'The path must be a string, start with a slash (/), and be longer' ' than 1 character.' ) if not projects allowed and len ( [ elem for elem in path . split ( '/' ) if elem ] ) == 1 : raise Storage Argument Exception ( 'This method does not accept projects in the path.' )", "predictions": ["fetch the ( ( or all for an illegal storage for the ( for both for ( for the ( for ( for the ( for the ( for both the ( for the ( for both for the ( for the for the for both for both for the"], "references": ["validate a string as a valid storage path"], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 4145, "code": "def new ( cls , access token , environment = 'prod' ) : return cls ( storage client = Storage Client . new ( access token , environment = environment ) )", "predictions": ["create a fetch with the provided : [ jackson ]"], "references": ["creates a new cross - service client ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 4146, "code": "def emit ( self , record ) : msg = self . format ( record ) if not isinstance ( msg , dict ) : msg = json . loads ( msg ) self . collection . insert ( msg )", "predictions": ["fetch object to try to insert ."], "references": ["pymongo expects a dict"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4147, "code": "def sort ( self , f = lambda d : d [ \"t\" ] ) : list . sort ( self , key = f ) return self", "predictions": ["sorts a list of maps ."], "references": ["sort here works by sorting by timestamp by default"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 4148, "code": "def sum ( self ) : raw = self . raw ( ) s = 0 for i in range ( len ( raw ) ) : s += raw [ i ] [ \"d\" ] return s", "predictions": ["calculates a list of column trees ."], "references": ["gets the sum of the data portions of all datapoints within"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 4149, "code": "def rfxcom ( device ) : if device is None : device = app . config . get ( 'DEVICE' ) if device is None : print ( \"The serial device needs to be passed in as --device or \" \"set in the config as DEVICE.\" ) return rfxcom collect ( device )", "predictions": ["add a ( inclusive self self self self self self self self self - signed self self . self . self self = > 1 self . 1 self . 0 . 1 self ."], "references": ["start the event loop to collect data from the serial device ."], "bleu": 0.034487891886161, "rouge_l": 0.04667176740627391}
{"id": 4150, "code": "def create user ( username ) : password = prompt pass ( \"Enter password\" ) user = User ( username = username , password = password ) db . session . add ( user ) db . session . commit ( )", "predictions": ["creates a new ( not associated with a connection index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index index"], "references": ["create a new user ."], "bleu": 0.03162593967015063, "rouge_l": 0.08531468531468532}
{"id": 4151, "code": "def refresh ( self ) : self . metadata = self . db . read ( self . path ) . json ( )", "predictions": ["setup the is done by the database ."], "references": ["refresh reloads data from the server . it raises an error if it fails to get the object s metadata"], "bleu": 0.042552769240615386, "rouge_l": 0.13260869565217392}
{"id": 4152, "code": "def streams ( self ) : result = self . db . read ( self . path , { \"q\" : \"ls\" } ) if result is None or result . json ( ) is None : return [ ] streams = [ ] for s in result . json ( ) : strm = self [ s [ \"name\" ] ] strm . metadata = s streams . append ( strm ) return streams", "predictions": ["output the header info from the database"], "references": ["returns the list of streams that belong to the device"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 4153, "code": "def users ( self ) : result = self . db . read ( \"\" , { \"q\" : \"ls\" } ) if result is None or result . json ( ) is None : return [ ] users = [ ] for u in result . json ( ) : usr = self ( u [ \"name\" ] ) usr . metadata = u users . append ( usr ) return users", "predictions": ["output all can be read from the output at a time ."], "references": ["returns the list of users in the database"], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 4154, "code": "def connectordb ( self ) : if self . cdb is None : logging . debug ( \"Logger: Connecting to \" + self . serverurl ) self . cdb = Connector DB ( self . apikey , url = self . serverurl ) return self . cdb", "predictions": ["a method for , ."], "references": ["returns the connectordb object that the logger uses . raises an error if logger isn t able to connect"], "bleu": 0.01660188206357524, "rouge_l": 0.0754017305315204}
{"id": 4155, "code": "def sync ( self ) : logging . debug ( \"Logger: Syncing...\" ) failed = False try : cdb = self . connectordb cdb . ping ( ) with self . synclock : c = self . database . cursor ( ) for stream in self . streams : s = cdb [ stream ] c . execute ( \"SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;\" , ( stream , ) ) datapoint Array = [ ] for dp in c . fetchall ( ) : datapoint Array . append ( { \"t\" : dp [ 1 ] , \"d\" : json . loads ( dp [ 2 ] ) } ) if len ( s ) > 0 : newtime = s [ - 1 ] [ \"t\" ] while ( len ( datapoint Array ) > 0 and datapoint Array [ 0 ] [ \"t\" ] < newtime ) : logging . debug ( \"Datapoint exists with older timestamp. Removing the datapoint.\" ) datapoint Array = datapoint Array [ 1 : ] if len ( datapoint Array ) > 0 : logging . debug ( \"%s: syncing %i datapoints\" % ( stream , len ( datapoint Array ) ) ) while ( len ( datapoint Array ) > DATAPOINT INSERT LIMIT ) : s . insert array ( datapoint Array [ : DATAPOINT INSERT LIMIT ] ) datapoint Array = datapoint Array [ DATAPOINT INSERT LIMIT : ] c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <?\" , ( stream , datapoint Array [ 0 ] [ \"t\" ] ) ) s . insert array ( datapoint Array ) c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <=?\" , ( stream , datapoint Array [ - 1 ] [ \"t\" ] ) ) self . lastsynctime = time . time ( ) if self . onsync is not None : self . onsync ( ) except Exception as e : falied = True reraise = self . syncraise if self . onsyncfail is not None : reraise = self . onsyncfail ( e ) if reraise : raise", "predictions": [". - . : . - . - . - . - . - . - . - . - . - . - . - . - . - . - . - . - . - . - . - . - . - . - . -"], "references": ["attempt to sync with the connectordb server"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 4156, "code": "def stop ( self ) : with self . synclock : if self . syncthread is not None : self . syncthread . cancel ( ) self . syncthread = None", "predictions": ["stops the localname for this feature ."], "references": ["stops the background synchronization thread"], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 4157, "code": "def read ( * paths ) : filename = os . path . join ( * paths ) with codecs . open ( filename , mode = 'r' , encoding = 'utf-8' ) as handle : return handle . read ( )", "predictions": ["item the , and return the resulting object ."], "references": ["build a file path from * paths * and return the contents ."], "bleu": 0.17809548511381157, "rouge_l": 0.35209235209235207}
{"id": 4158, "code": "def download url job ( job , url , name = None , s3 key path = None , cghub key path = None ) : work dir = job . file Store . get Local Temp Dir ( ) fpath = download url ( job = job , url = url , work dir = work dir , name = name , s3 key path = s3 key path , cghub key path = cghub key path ) return job . file Store . write Global File ( fpath )", "predictions": ["downloads a file for the specified ( or file not already exists not to be already done not in the filesystem not to a main file not used ."], "references": ["job version of download_url"], "bleu": 0.03511476270817333, "rouge_l": 0.0}
{"id": 4159, "code": "def s3am upload job ( job , file id , file name , s3 dir , s3 key path = None ) : work dir = job . file Store . get Local Temp Dir ( ) fpath = job . file Store . read Global File ( file id , os . path . join ( work dir , file name ) ) s3am upload ( job = job , fpath = fpath , s3 dir = s3 dir , num cores = job . cores , s3 key path = s3 key path )", "predictions": ["uploads a has been set to a file ."], "references": ["job version of s3am_upload"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4160, "code": "def labels ( ontology , output , ols base ) : for label in get labels ( ontology = ontology , ols base = ols base ) : click . echo ( label , file = output )", "predictions": ["creates a new labels for the specified label ."], "references": ["output the names to the given file"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4161, "code": "def tree ( ontology , output , ols base ) : for parent , child in get hierarchy ( ontology = ontology , ols base = ols base ) : click . echo ( '{}\\t{}' . format ( parent , child ) , file = output )", "predictions": ["creates a new ( object ."], "references": ["output the parent - child relations to the given file"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 4162, "code": "def get mean insert size ( work dir , bam name ) : cmd = \"docker run --log-driver=none --rm -v {}:/data quay.io/ucsc cgl/samtools \" \"view -f66 {}\" . format ( work dir , os . path . join ( work dir , bam name ) ) process = subprocess . Popen ( args = cmd , shell = True , stdout = subprocess . PIPE ) b sum = 0.0 b count = 0.0 while True : line = process . stdout . readline ( ) if not line : break tmp = line . split ( \"\\t\" ) if abs ( long ( tmp [ 8 ] ) ) < 10000 : b sum += abs ( long ( tmp [ 8 ] ) ) b count += 1 process . wait ( ) try : mean = b sum / b count except Zero Division Error : mean = 150 print \"Using insert size: %d\" % mean return int ( mean )", "predictions": ["returns the mean average ."], "references": ["function taken from mc3 pipeline"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4163, "code": "def device ( self ) : splitted path = self . path . split ( \"/\" ) return Device ( self . db , splitted path [ 0 ] + \"/\" + splitted path [ 1 ] )", "predictions": ["get the device specified by the path ."], "references": ["returns the device which owns the given stream"], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 4164, "code": "def get empty config ( self ) : self . generate config ( ) path = self . get config path ( ) with open ( path , 'r' ) as readable : contents = readable . read ( ) os . remove ( path ) return contents", "predictions": ["returns the contents of this config object ."], "references": ["returns the config file contents as a string . the config file is generated and then deleted ."], "bleu": 0.07190875380977045, "rouge_l": 0.35966981132075476}
{"id": 4165, "code": "def create pipeline command ( self , args , workdir path , config path ) : return ( [ self . name , 'run' , os . path . join ( workdir path , 'job Store' ) , '--config' , config path , '--work Dir' , workdir path , '--retry Count' , '1' ] + ( [ '--restart' ] if args . restart else [ ] ) )", "predictions": ["creates a pipeline in the passed config file ."], "references": ["creates and returns a list that represents a command for running the pipeline ."], "bleu": 0.10657503067399117, "rouge_l": 0.3347050754458162}
{"id": 4166, "code": "def delete ( self , path ) : return self . handleresult ( self . r . delete ( urljoin ( self . url + CRUD PATH , path ) ) )", "predictions": ["deletes an existing lock ."], "references": ["send a delete request to the given path of the crud api . this deletes the object . or at least tries to ."], "bleu": 0.006759049970609995, "rouge_l": 0.12335692618806875}
{"id": 4167, "code": "def subscribe ( self , stream , callback , transform = \"\" ) : return self . ws . subscribe ( stream , callback , transform )", "predictions": ["subscribe content to this method ."], "references": ["subscribe to the given stream with the callback"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4168, "code": "def devices ( self ) : result = self . db . read ( self . path , { \"q\" : \"ls\" } ) if result is None or result . json ( ) is None : return [ ] devices = [ ] for d in result . json ( ) : dev = self [ d [ \"name\" ] ] dev . metadata = d devices . append ( dev ) return devices", "predictions": ["fetches the stats from an . ."], "references": ["returns the list of devices that belong to the user"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4169, "code": "def send ( self , cmd ) : with self . ws sendlock : self . ws . send ( json . dumps ( cmd ) )", "predictions": ["send string to json"], "references": ["send the given command thru the websocket"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4170, "code": "def subscribe ( self , stream , callback , transform = \"\" ) : if self . status == \"disconnected\" or self . status == \"disconnecting\" or self . status == \"connecting\" : self . connect ( ) if self . status is not \"connected\" : return False logging . debug ( \"Subscribing to %s\" , stream ) self . send ( { \"cmd\" : \"subscribe\" , \"arg\" : stream , \"transform\" : transform } ) with self . subscription lock : self . subscriptions [ stream + \":\" + transform ] = callback return True", "predictions": ["subscribe a callback to a stream ."], "references": ["given a stream a callback and an optional transform sets up the subscription"], "bleu": 0.13044969897820202, "rouge_l": 0.1897356143079316}
{"id": 4171, "code": "def reconnect ( self ) : self . status = \"reconnecting\" if self . disconnected time - self . connected time > 15 * 60 : self . reconnect time = self . reconnect time starting seconds else : self . reconnect time *= self . reconnect time backoff multiplier if self . reconnect time > self . reconnect time max seconds : self . reconnect time = self . reconnect time max seconds self . reconnect time *= 1 + random . uniform ( - 0.2 , 0.2 ) if self . reconnect time < self . reconnect time starting seconds : self . reconnect time = self . reconnect time starting seconds logging . warn ( \"Connector DB:WS: Attempting to reconnect in %fs\" , self . reconnect time ) self . reconnector = threading . Timer ( self . reconnect time , self . reconnect fnc ) self . reconnector . daemon = True self . reconnector . start ( )", "predictions": ["reconnect to reconnect to the reconnect ."], "references": ["this is called when a connection is lost - it attempts to reconnect to the server"], "bleu": 0.14291880343715468, "rouge_l": 0.3249001331557923}
{"id": 4172, "code": "def on open ( self , ws ) : logging . debug ( \"Connector DB: Websocket opened\" ) self . reconnect time /= self . reconnect time backoff multiplier self . status = \"connected\" self . lastpingtime = time . time ( ) self . ensure ping ( ) self . connected time = time . time ( ) self . ws openlock . release ( )", "predictions": ["forces a fake connection to the pool ."], "references": ["called when the websocket is opened"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 4173, "code": "def on close ( self , ws ) : if self . status == \"disconnected\" : return logging . debug ( \"Connector DB:WS: Websocket closed\" ) if self . pingtimer is not None : self . pingtimer . cancel ( ) self . disconnected time = time . time ( ) if self . status == \"disconnecting\" : self . status = \"disconnected\" elif self . status == \"connected\" : self . reconnect ( )", "predictions": ["closes the output of this object ."], "references": ["called when the websocket is closed"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4174, "code": "def on error ( self , ws , err ) : logging . debug ( \"Connector DB:WS: Connection Error\" ) if self . status == \"connecting\" : self . status = \"errored\" self . ws openlock . release ( )", "predictions": ["method called to run in a transaction ."], "references": ["called when there is an error in the websocket"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 4175, "code": "def on message ( self , ws , msg ) : msg = json . loads ( msg ) logging . debug ( \"Connector DB:WS: Msg '%s'\" , msg [ \"stream\" ] ) stream key = msg [ \"stream\" ] + \":\" if \"transform\" in msg : stream key += msg [ \"transform\" ] self . subscription lock . acquire ( ) if stream key in self . subscriptions : subscription function = self . subscriptions [ stream key ] self . subscription lock . release ( ) fresult = subscription function ( msg [ \"stream\" ] , msg [ \"data\" ] ) if fresult is True : fresult = msg [ \"data\" ] if fresult is not False and fresult is not None and msg [ \"stream\" ] . endswith ( \"/downlink\" ) and msg [ \"stream\" ] . count ( \"/\" ) == 3 : self . insert ( msg [ \"stream\" ] [ : - 9 ] , fresult ) else : self . subscription lock . release ( ) logging . warn ( \"Connector DB:WS: Msg '%s' not subscribed! Subscriptions: %s\" , msg [ \"stream\" ] , list ( self . subscriptions . keys ( ) ) )", "predictions": ["passes a message on the controller ."], "references": ["this function is called whenever there is a message received from the server"], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 4176, "code": "def write config ( configuration ) : with open ( CONFIG PATH , 'w' ) as f : json . dump ( configuration , f , indent = 2 , sort keys = True )", "predictions": ["write configuration . note : this will only add the configuration to the global json file ."], "references": ["helper to write the json configuration to a file"], "bleu": 0.11750296943620289, "rouge_l": 0.40720961281708945}
{"id": 4177, "code": "def check ( self ) : status = check Container Status ( self . spark Container ID , self . hdfs Container ID , spark Noun = 'worker' , hdfs Noun = 'datanode' ) return status", "predictions": ["compare the given status and returns the hdfs container ."], "references": ["checks to see if spark worker and hdfs datanode are still running ."], "bleu": 0.11105685174312292, "rouge_l": 0.25487465181058494}
{"id": 4178, "code": "def base tokenizer ( fp ) : if isinstance ( fp , String IO ) : template file = fp size = template file . len else : #empty file check if os . fstat ( fp . fileno ( ) ) . st size == 0 : yield TOKEN EOF , 'EOF' , 0 , 0 return template file = mmap . mmap ( fp . fileno ( ) , 0 , access = mmap . ACCESS READ ) size = template file . size ( ) lineno = 0 while 1 : lineno += 1 pos = 1 if template file . tell ( ) == size : yield TOKEN EOF , 'EOF' , lineno , 0 break line = template file . readline ( ) . decode ( 'utf-8' ) line = line . replace ( '\\r\\n' , '' ) line = line . replace ( '\\n' , '' ) if re comment . match ( line ) : continue last text = deque ( ) while line : line len = len ( line ) for token in tokens : m = token . regex . match ( line ) if m : if last text : yield TOKEN TEXT , '' . join ( last text ) , lineno , pos pos += len ( last text ) last text . clear ( ) offset , value = m . end ( ) , m . group ( ) line = line [ offset : ] yield token , value , lineno , pos pos += offset break if line len == len ( line ) : last text . append ( line [ 0 ] ) line = line [ 1 : ] if last text : yield TOKEN TEXT , '' . join ( last text ) , lineno , pos pos += len ( last text ) last text . clear ( ) yield TOKEN NEWLINE , '\\n' , lineno , pos template file . close ( )", "predictions": ["this function takes in a generator of strings and returns an iterator of all the fields that match the internal base class ."], "references": ["tokenizer . generates tokens stream from text"], "bleu": 0.05291907393644996, "rouge_l": 0.07376058041112453}
{"id": 4179, "code": "def fitness ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return sum ( m . fitness score for m in members ) / len ( members ) else : return None", "predictions": ["set the fitness mechanism for this operation ."], "references": ["population fitness == average member fitness score"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4180, "code": "def ave cost fn val ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return sum ( m . cost fn val for m in members ) / len ( members ) else : return None", "predictions": ["call this with evenly cost ."], "references": ["returns average cost function return value for all members"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4181, "code": "def med cost fn val ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return median ( [ m . cost fn val for m in members ] ) else : return None", "predictions": ["med call this with the med cost method on the call to this method ."], "references": ["returns median cost function return value for all members"], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 4182, "code": "def parameters ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members params = { } for p in self . parameters : params [ p . name ] = sum ( m . parameters [ p . name ] for m in members ) / len ( members ) return params else : return None", "predictions": ["return the number of parameters that are currently like in this method ."], "references": ["population parameter vals == average member parameter vals"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 4183, "code": "def members ( self ) : if self . num processes > 1 : return [ m . get ( ) for m in self . members ] else : return self . members", "predictions": ["get the members of this gpathresult ."], "references": ["returns member objects of population"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4184, "code": "def get environ vars ( self ) : for key , val in os . environ . items ( ) : if environ prefix re . search ( key ) : yield ( environ prefix re . sub ( \"\" , key ) . lower ( ) , val )", "predictions": ["returns a generator for each key in this map ."], "references": ["returns a generator with all environmental vars with prefix pip_"], "bleu": 0.23462350320528, "rouge_l": 0.3}
{"id": 4185, "code": "def transform result ( typ , result ) : if issubclass ( typ , bytes ) : return tostring ( result , encoding = 'utf-8' ) elif issubclass ( typ , unicode ) : return tostring ( result , encoding = 'unicode' ) else : return result", "predictions": ["transform the result back into the result ."], "references": ["convert the result back into the input type ."], "bleu": 0.5387410466197604, "rouge_l": 0.6984732824427481}
{"id": 4186, "code": "def is single class ( ) : ret = False counts = get counts ( ) if counts [ \"classes\" ] < 1 and counts [ \"modules\" ] < 1 : ret = counts [ \"tests\" ] > 0 else : ret = counts [ \"classes\" ] <= 1 and counts [ \"modules\" ] <= 1 return ret", "predictions": ["check if a class is a single class ."], "references": ["returns true if only a single class is being run or some tests within a single class"], "bleu": 0.1380059528287141, "rouge_l": 0.5101553166069295}
{"id": 4187, "code": "def is single module ( ) : ret = False counts = get counts ( ) if counts [ \"modules\" ] == 1 : ret = True elif counts [ \"modules\" ] < 1 : ret = is single class ( ) return ret", "predictions": ["checks if this module is single ."], "references": ["returns true if only a module is being run"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 4188, "code": "def validate params ( request ) : if 'params' in request : correct params = isinstance ( request [ 'params' ] , ( list , dict ) ) error = 'Incorrect parameter values' assert correct params , error", "predictions": ["validates parameters using the http request ."], "references": ["validate request params ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 4189, "code": "def validate id ( request ) : if 'id' in request : correct id = isinstance ( request [ 'id' ] , ( string types , int , None ) , ) error = 'Incorrect identifier' assert correct id , error", "predictions": ["validates the id of the request ."], "references": ["validate request id ."], "bleu": 0.22089591134157885, "rouge_l": 0.3824451410658307}
{"id": 4190, "code": "def escape argspec ( obj , iterable , escape ) : for key , value in iterable : if hasattr ( value , ' html ' ) or isinstance ( value , string types ) : obj [ key ] = escape ( value ) return obj", "predictions": ["escape all the values in an object ."], "references": ["helper for various string - wrapped functions ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4191, "code": "def sub symbols ( pattern , code , symbol ) : return pattern . replace ( '\u00a4\u00a4',  c de). r e place(' \u00a4 ', s y bol)", "predictions": ["return the given pattern as a ( ."], "references": ["substitutes symbols in cldr number pattern ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4192, "code": "def amount converter ( obj ) : if isinstance ( obj , Decimal ) : return obj elif isinstance ( obj , ( str , int , float ) ) : return Decimal ( str ( obj ) ) else : raise Value Error ( 'do not know how to convert: {}' . format ( type ( obj ) ) )", "predictions": ["get an object from the app ."], "references": ["converts amount value from several types into decimal ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4193, "code": "def exception ( self ) : buf = traceback . format exception only ( self . exc type , self . exc value ) rv = '' . join ( buf ) . strip ( ) return rv . decode ( 'utf-8' , 'replace' ) if PY2 else rv", "predictions": ["format tree for tree : use only ."], "references": ["string representation of the exception ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 4194, "code": "def render summary ( self , include title = True ) : title = '' frames = [ ] classes = [ 'traceback' ] if not self . frames : classes . append ( 'noframe-traceback' ) if include title : if self . is syntax error : title = u'Syntax Error' else : title = u'Traceback <em>(most recent call last)</em>:' for frame in self . frames : frames . append ( u'<li%s>%s' % ( frame . info and u' title=\"%s\"' % escape ( frame . info ) or u'' , frame . render ( ) ) ) if self . is syntax error : description wrapper = u'<pre class=syntaxerror>%s</pre>' else : description wrapper = u'<blockquote>%s</blockquote>' return SUMMARY HTML % { 'classes' : u' ' . join ( classes ) , 'title' : title and u'<h3>%s</h3>' % title or u'' , 'frames' : u'\\n' . join ( frames ) , 'description' : description wrapper % escape ( self . exception ) }", "predictions": ["get a list of ( ( ."], "references": ["render the traceback for the interactive console ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4195, "code": "def generate plaintext traceback ( self ) : yield u'Traceback (most recent call last):' for frame in self . frames : yield u'  File \"%s\", line %s, in %s' % ( frame . filename , frame . lineno , frame . function name ) yield u'    ' + frame . current line . strip ( ) yield self . exception", "predictions": ["device to device self - generated by the ( splitted splitted splitted splitted splitted splitted splitted splitted splitted splitted . splitted splitted 0 . splitted splitted 0 . . splitted splitted splitted 0 ."], "references": ["like the plaintext attribute but returns a generator"], "bleu": 0.03551851328486764, "rouge_l": 0.053602811950790856}
{"id": 4196, "code": "def get annotated lines ( self ) : lines = [ Line ( idx + 1 , x ) for idx , x in enumerate ( self . sourcelines ) ] if hasattr ( self . code , 'co firstlineno' ) : lineno = self . code . co firstlineno - 1 while lineno > 0 : if funcdef re . match ( lines [ lineno ] . code ) : break lineno -= 1 try : offset = len ( inspect . getblock ( [ x . code + '\\n' for x in lines [ lineno : ] ] ) ) except Token Error : offset = 0 for line in lines [ lineno : lineno + offset ] : line . in frame = True try : lines [ self . lineno - 1 ] . current = True except Index Error : pass return lines", "predictions": ["retrieves information about this line ."], "references": ["helper function that returns lines with extra information ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4197, "code": "def render source ( self ) : return SOURCE TABLE HTML % u'\\n' . join ( line . render ( ) for line in self . get annotated lines ( ) )", "predictions": ["renders the child ast ."], "references": ["render the sourcecode ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 4198, "code": "def get content type ( url , session ) : scheme , netloc , path , query , fragment = urllib parse . urlsplit ( url ) if scheme not in ( 'http' , 'https' ) : return '' resp = session . head ( url , allow redirects = True ) resp . raise for status ( ) return resp . headers . get ( \"Content-Type\" , \"\" )", "predictions": ["used by the client to construct a : https / www . com / questions / . / . / . / . / . / . / . / . / . / . . html # ."], "references": ["get the content - type of the given url using a head request"], "bleu": 0.03419816740540655, "rouge_l": 0.08454608454608455}
{"id": 4199, "code": "def links ( self ) : for anchor in self . parsed . findall ( \".//a\" ) : if anchor . get ( \"href\" ) : href = anchor . get ( \"href\" ) url = self . clean link ( urllib parse . urljoin ( self . base url , href ) ) internal = None if self . api version and self . api version >= 2 : internal = bool ( anchor . get ( \"rel\" ) and \"internal\" in anchor . get ( \"rel\" ) . split ( ) ) yield Link ( url , self , internal = internal )", "predictions": ["a generator for all links to be made by their application ."], "references": ["yields all links in the page"], "bleu": 0.1367440667823257, "rouge_l": 0.2364341085271318}
{"id": 4200, "code": "def find data files ( self , package , src dir ) : globs = ( self . package data . get ( '' , [ ] ) + self . package data . get ( package , [ ] ) ) files = self . manifest files . get ( package , [ ] ) [ : ] for pattern in globs : files . extend ( glob ( os . path . join ( src dir , convert path ( pattern ) ) ) ) return self . exclude data files ( package , src dir , files )", "predictions": ["finds all for the ( negation read read read ones ."], "references": ["return filenames for package s data files in src_dir"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 4201, "code": "def check package ( self , package , package dir ) : try : return self . packages checked [ package ] except Key Error : pass init py = orig . build py . check package ( self , package , package dir ) self . packages checked [ package ] = init py if not init py or not self . distribution . namespace packages : return init py for pkg in self . distribution . namespace packages : if pkg == package or pkg . startswith ( package + '.' ) : break else : return init py f = open ( init py , 'rb U' ) if 'declare namespace' . encode ( ) not in f . read ( ) : from distutils . errors import Distutils Error raise Distutils Error ( \"Namespace package problem: %s is a namespace package, but \" \"its\\n init .py does not call declare namespace()! Please \" 'fix it.\\n(See the setuptools manual under ' '\"Namespace Packages\" for details.)\\n\"' % ( package , ) ) f . close ( ) return init py", "predictions": ["make sure that the ( possibly non - json : build self - json : build self - json - json - json tree is initialized with a large piece of the ( ( ( i . e . , not normally : build self - json - json -"], "references": ["check namespace packages __init__ for declare_namespace"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 4202, "code": "def exclude data files ( self , package , src dir , files ) : globs = ( self . exclude package data . get ( '' , [ ] ) + self . exclude package data . get ( package , [ ] ) ) bad = [ ] for pattern in globs : bad . extend ( fnmatch . filter ( files , os . path . join ( src dir , convert path ( pattern ) ) ) ) bad = dict . fromkeys ( bad ) seen = { } return [ f for f in files if f not in bad and f not in seen and seen . setdefault ( f , 1 ) ]", "predictions": ["subscribe for files that match the specified transform ."], "references": ["filter filenames for package s data files in src_dir"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 4203, "code": "def ignore comments ( iterator ) : for line in iterator : line = COMMENT RE . sub ( '' , line ) line = line . strip ( ) if line : yield line", "predictions": ["iterates over any lines that are considered to be considered as a sequence of lines ."], "references": ["strips and filters empty or commented lines ."], "bleu": 0.10123734869668824, "rouge_l": 0.17732558139534885}
{"id": 4204, "code": "def skip regex ( lines , options ) : skip regex = options . skip requirements regex if options else None if skip regex : lines = filterfalse ( re . compile ( skip regex ) . search , lines ) return lines", "predictions": ["on a file ."], "references": ["optionally exclude lines that match -- skip - requirements - regex"], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 4205, "code": "def compile ( marker ) : try : return cache [ marker ] except Key Error : pass if not marker . strip ( ) : def marker fn ( environment = None , override = None ) : \"\"\"\"\"\" return True else : compiled marker = compile marker ( parse marker ( marker ) ) def marker fn ( environment = None , override = None ) : \"\"\"override updates environment\"\"\" if override is None : override = { } if environment is None : environment = default environment ( ) environment . update ( override ) return eval ( compiled marker , environment ) marker fn . doc = marker cache [ marker ] = marker fn return cache [ marker ]", "predictions": ["on all environment pages , returning an iterator of the given ( as a function self self - separated ( self - > self self self self - override self self self - . self self self self self self self self - > self self self self self -"], "references": ["return compiled marker as a function accepting an environment dict ."], "bleu": 0.05144201220913522, "rouge_l": 0.14823815309842042}
{"id": 4206, "code": "def visit ( self , node ) : if not isinstance ( node , self . ALLOWED ) : raise Syntax Error ( 'Not allowed in environment markers.\\n%s\\n%s' % ( self . statement , ( ' ' * node . col offset ) + '^' ) ) return ast . Node Transformer . visit ( self , node )", "predictions": ["this is called by the ( class ."], "references": ["ensure statement only contains allowed nodes ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4207, "code": "def visit Attribute ( self , node ) : new node = ast . Name ( \"%s.%s\" % ( node . value . id , node . attr ) , node . ctx ) return ast . copy location ( new node , node )", "predictions": ["replaces all of the json nodes with this ws ."], "references": ["flatten one level of attribute access ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 4208, "code": "def push ( self ) : self . refcnt += 1 app ctx stack . push ( self ) appcontext pushed . send ( self . app )", "predictions": ["write a new method to the stack ."], "references": ["binds the app context to the current context ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 4209, "code": "def pop ( self , exc = None ) : self . refcnt -= 1 if self . refcnt <= 0 : if exc is None : exc = sys . exc info ( ) [ 1 ] self . app . do teardown appcontext ( exc ) rv = app ctx stack . pop ( ) assert rv is self , 'Popped wrong app context.  (%r instead of %r)' % ( rv , self ) appcontext popped . send ( self . app )", "predictions": ["removes the active wrong ."], "references": ["pops the app context ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4210, "code": "def push ( self ) : top = request ctx stack . top if top is not None and top . preserved : top . pop ( top . preserved exc ) app ctx = app ctx stack . top if app ctx is None or app ctx . app != self . app : app ctx = self . app . app context ( ) app ctx . push ( ) self . implicit app ctx stack . append ( app ctx ) else : self . implicit app ctx stack . append ( None ) request ctx stack . push ( self ) self . session = self . app . open session ( self . request ) if self . session is None : self . session = self . app . make null session ( )", "predictions": ["base isinstance isinstance by the :"], "references": ["binds the request context to the current context ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4211, "code": "def dist in usersite ( dist ) : norm path = normalize path ( dist location ( dist ) ) return norm path . startswith ( normalize path ( user site ) )", "predictions": ["get the distribution of the given distribution"], "references": ["return true if given distribution is installed in user site ."], "bleu": 0.1380518455178974, "rouge_l": 0.2136602451838879}
{"id": 4212, "code": "def dist is editable ( dist ) : from pip import Frozen Requirement req = Frozen Requirement . from dist ( dist , [ ] ) return req . editable", "predictions": ["gets the : first edge from an fn or a requirement requirement ."], "references": ["is distribution an editable install?"], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 4213, "code": "def run ( self , options , args ) : shells = COMPLETION SCRIPTS . keys ( ) shell options = [ '--' + shell for shell in sorted ( shells ) ] if options . shell in shells : script = COMPLETION SCRIPTS . get ( options . shell , '' ) print ( BASE COMPLETION % { 'script' : script , 'shell' : options . shell } ) else : sys . stderr . write ( 'ERROR: You must pass %s\\n' % ' or ' . join ( shell options ) )", "predictions": ["executes all the ( commands ."], "references": ["prints the completion code of the given shell"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4214, "code": "def root is purelib ( name , wheeldir ) : name folded = name . replace ( \"-\" , \" \" ) for item in os . listdir ( wheeldir ) : match = dist info re . match ( item ) if match and match . group ( 'name' ) == name folded : with open ( os . path . join ( wheeldir , item , 'WHEEL' ) ) as wheel : for line in wheel : line = line . lower ( ) . rstrip ( ) if line == \"root-is-purelib: true\" : return True return False", "predictions": ["checks if the given : . is a parameters ."], "references": ["return true if the extracted wheel in wheeldir should go into purelib ."], "bleu": 0.13206959826272413, "rouge_l": 0.25487465181058494}
{"id": 4215, "code": "def iter symbols ( code ) : for name in code . co names : yield name for const in code . co consts : if isinstance ( const , basestring ) : yield const elif isinstance ( const , Code Type ) : for name in iter symbols ( const ) : yield name", "predictions": ["iterate over all ( ) zones ."], "references": ["yield names and strings used by code and its nested code objects"], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 4216, "code": "def ensure fresh rates ( func ) : def wrapper ( self , * args , * * kwargs ) : if self . last updated + timedelta ( minutes = 5 ) < zulu . now ( ) : self . refresh ( ) return func ( self , * args , * * kwargs ) return wrapper", "predictions": ["decorator for . , . , or . if the method is called ."], "references": ["decorator for backend that ensures rates are fresh within last 5 mins"], "bleu": 0.11633270842295028, "rouge_l": 0.15601023017902813}
{"id": 4217, "code": "def write delete marker file ( directory ) : filepath = os . path . join ( directory , PIP DELETE MARKER FILENAME ) with open ( filepath , 'w' ) as marker fp : marker fp . write ( DELETE MARKER MESSAGE )", "predictions": ["creates the pip ( if any : . : , : , : , : , : , : , : , : , . ( , ( : , . ( . ( . ( . ( . ( , ( : , . ( . ( . ("], "references": ["write the pip delete marker file into this directory ."], "bleu": 0.033984283835209204, "rouge_l": 0.11366459627329194}
{"id": 4218, "code": "def running under virtualenv ( ) : if hasattr ( sys , 'real prefix' ) : return True elif sys . prefix != getattr ( sys , \"base prefix\" , sys . prefix ) : return True return False", "predictions": ["returns true if the given class should be triggered on this platform ."], "references": ["return true if we re running inside a virtualenv false otherwise ."], "bleu": 0.1350862565735141, "rouge_l": 0.2417437252311757}
{"id": 4219, "code": "def get username ( ) : if WINDOWS : return getpass . getuser ( ) import pwd return pwd . getpwuid ( os . geteuid ( ) ) . pw name", "predictions": ["get the current single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single single time ."], "references": ["returns the effective username of the current process ."], "bleu": 0.04609815356235176, "rouge_l": 0.1465172137710168}
{"id": 4220, "code": "def distutils scheme ( dist name , user = False , home = None , root = None , isolated = False ) : from distutils . dist import Distribution scheme = { } if isolated : extra dist args = { \"script args\" : [ \"--no-user-cfg\" ] } else : extra dist args = { } dist args = { 'name' : dist name } dist args . update ( extra dist args ) d = Distribution ( dist args ) d . parse config files ( ) i = d . get command obj ( 'install' , create = True ) i . user = user or i . user i . home = home or i . home i . root = root or i . root i . finalize options ( ) for key in SCHEME KEYS : scheme [ key ] = getattr ( i , 'install ' + key ) if i . install lib is not None : scheme . update ( dict ( purelib = i . install lib , platlib = i . install lib ) ) if running under virtualenv ( ) : scheme [ 'headers' ] = os . path . join ( sys . prefix , 'include' , 'site' , 'python' + sys . version [ : 3 ] , dist name , ) if root is not None : scheme [ \"headers\" ] = os . path . join ( root , os . path . abspath ( scheme [ \"headers\" ] ) [ 1 : ] , ) return scheme", "predictions": ["map the cluster \" validate \" environment into the given ) ."], "references": ["return a distutils install scheme"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 4221, "code": "def install script ( self , dist , script name , script text , dev path = None ) : spec = str ( dist . as requirement ( ) ) is script = is python script ( script text , script name ) if is script : script text = ( Script Writer . get header ( script text ) + self . load template ( dev path ) % locals ( ) ) self . write script ( script name , to ascii ( script text ) , 'b' )", "predictions": ["generate a new id ."], "references": ["generate a legacy script wrapper and install it"], "bleu": 0.1971902775417715, "rouge_l": 0.2953995157384988}
{"id": 4222, "code": "def install site py ( self ) : if self . sitepy installed : return sitepy = os . path . join ( self . install dir , \"site.py\" ) source = resource string ( \"setuptools\" , \"site-patch.py\" ) current = \"\" if os . path . exists ( sitepy ) : log . debug ( \"Checking existing site.py in %s\" , self . install dir ) f = open ( sitepy , 'rb' ) current = f . read ( ) if PY3 : current = current . decode ( ) f . close ( ) if not current . startswith ( 'def  boot():' ) : raise Distutils Error ( \"%s is not a setuptools-generated site.py; please\" \" remove it.\" % sitepy ) if current != source : log . info ( \"Creating %s\" , sitepy ) if not self . dry run : ensure directory ( sitepy ) f = open ( sitepy , 'wb' ) f . write ( source ) f . close ( ) self . byte compile ( [ sitepy ] ) self . sitepy installed = True", "predictions": ["copies this package such as a argspec + argspec + key + value ."], "references": ["make sure there s a site . py in the target dir if needed"], "bleu": 0.09782375748961449, "rouge_l": 0.14285714285714285}
{"id": 4223, "code": "def save ( self ) : if not self . dirty : return data = '\\n' . join ( map ( self . make relative , self . paths ) ) if data : log . debug ( \"Saving %s\" , self . filename ) data = ( \"import sys; sys. plen = len(sys.path)\\n\" \"%s\\n\" \"import sys; new=sys.path[sys. plen:];\" \" del sys.path[sys. plen:];\" \" p=getattr(sys,' egginsert',0); sys.path[p:p]=new;\" \" sys. egginsert = p+len(new)\\n\" ) % data if os . path . islink ( self . filename ) : os . unlink ( self . filename ) f = open ( self . filename , 'wt' ) f . write ( data ) f . close ( ) elif os . path . exists ( self . filename ) : log . debug ( \"Deleting empty %s\" , self . filename ) os . unlink ( self . filename ) self . dirty = False", "predictions": ["saves the current state of this class to the ( ."], "references": ["write changed . pth file back to disk"], "bleu": 0.12605968092174913, "rouge_l": 0.108348134991119}
{"id": 4224, "code": "def add filters ( self , filterer , filters ) : for f in filters : try : filterer . add Filter ( self . config [ 'filters' ] [ f ] ) except Standard Error as e : raise Value Error ( 'Unable to add filter %r: %s' % ( f , e ) )", "predictions": ["adds the results from an iterable of filters ."], "references": ["add filters to a filterer from a list of names ."], "bleu": 0.14211011212459496, "rouge_l": 0.2946859903381642}
{"id": 4225, "code": "def configure handler ( self , config ) : formatter = config . pop ( 'formatter' , None ) if formatter : try : formatter = self . config [ 'formatters' ] [ formatter ] except Standard Error as e : raise Value Error ( 'Unable to set formatter ' '%r: %s' % ( formatter , e ) ) level = config . pop ( 'level' , None ) filters = config . pop ( 'filters' , None ) if '()' in config : c = config . pop ( '()' ) if not hasattr ( c , ' call ' ) and hasattr ( types , 'Class Type' ) and type ( c ) != types . Class Type : c = self . resolve ( c ) factory = c else : klass = self . resolve ( config . pop ( 'class' ) ) if issubclass ( klass , logging . handlers . Memory Handler ) and 'target' in config : try : config [ 'target' ] = self . config [ 'handlers' ] [ config [ 'target' ] ] except Standard Error as e : raise Value Error ( 'Unable to set target handler ' '%r: %s' % ( config [ 'target' ] , e ) ) elif issubclass ( klass , logging . handlers . SMTP Handler ) and 'mailhost' in config : config [ 'mailhost' ] = self . as tuple ( config [ 'mailhost' ] ) elif issubclass ( klass , logging . handlers . Sys Log Handler ) and 'address' in config : config [ 'address' ] = self . as tuple ( config [ 'address' ] ) factory = klass kwargs = dict ( ( k , config [ k ] ) for k in config if valid ident ( k ) ) try : result = factory ( * * kwargs ) except Type Error as te : if \"'stream'\" not in str ( te ) : raise #(e.g. by Django) kwargs [ 'strm' ] = kwargs . pop ( 'stream' ) result = factory ( * * kwargs ) if formatter : result . set Formatter ( formatter ) if level is not None : result . set Level ( check Level ( level ) ) if filters : self . add filters ( result , filters ) return result", "predictions": ["called by the ( for configuration from the class named by the user ."], "references": ["configure a handler from a dictionary ."], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 4226, "code": "def add handlers ( self , logger , handlers ) : for h in handlers : try : logger . add Handler ( self . config [ 'handlers' ] [ h ] ) except Standard Error as e : raise Value Error ( 'Unable to add handler %r: %s' % ( h , e ) )", "predictions": ["adds the given tokenizer ."], "references": ["add handlers to a logger from a list of names ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4227, "code": "def common logger config ( self , logger , config , incremental = False ) : level = config . get ( 'level' , None ) if level is not None : logger . set Level ( check Level ( level ) ) if not incremental : for h in logger . handlers [ : ] : logger . remove Handler ( h ) handlers = config . get ( 'handlers' , None ) if handlers : self . add handlers ( logger , handlers ) filters = config . get ( 'filters' , None ) if filters : self . add filters ( logger , filters )", "predictions": ["common logger . common method used for all configuration output classes ."], "references": ["perform configuration which is common to root and non - root loggers ."], "bleu": 0.11368272367804307, "rouge_l": 0.15885416666666669}
{"id": 4228, "code": "def execfile ( filename , globals , locals = None ) : mode = 'rb' with open ( filename , mode ) as stream : script = stream . read ( ) if sys . version info [ : 2 ] < ( 2 , 7 ) or sys . version info [ : 2 ] >= ( 3 , 0 ) and sys . version info [ : 2 ] < ( 3 , 2 ) : script = script . replace ( b'\\r\\n' , b'\\n' ) script = script . replace ( b'\\r' , b'\\n' ) if locals is None : locals = globals code = compile ( script , filename , 'exec' ) exec ( code , globals , locals )", "predictions": ["constructs a new script to read the given filename ."], "references": ["python 3 implementation of execfile ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 4229, "code": "def override temp ( replacement ) : if not os . path . isdir ( replacement ) : os . makedirs ( replacement ) saved = tempfile . tempdir tempfile . tempdir = replacement try : yield finally : tempfile . tempdir = saved", "predictions": ["override this function to override the saved replacement for a test ."], "references": ["monkey - patch tempfile . tempdir with replacement ensuring it exists"], "bleu": 0.11498759556447223, "rouge_l": 0.08764367816091953}
{"id": 4230, "code": "def run setup ( setup script , args ) : setup dir = os . path . abspath ( os . path . dirname ( setup script ) ) with setup context ( setup dir ) : try : sys . argv [ : ] = [ setup script ] + list ( args ) sys . path . insert ( 0 , setup dir ) working set . init ( ) working set . callbacks . append ( lambda dist : dist . activate ( ) ) def runner ( ) : ns = dict ( file = setup script , name = ' main ' ) execfile ( setup script , ns ) Directory Sandbox ( setup dir ) . run ( runner ) except System Exit as v : if v . args and v . args [ 0 ] : raise", "predictions": ["run the provided script ."], "references": ["run a distutils setup script sandboxed in its directory"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4231, "code": "def getitem ( self , obj , argument ) : try : return obj [ argument ] except ( Type Error , Lookup Error ) : if isinstance ( argument , string types ) : try : attr = str ( argument ) except Exception : pass else : try : return getattr ( obj , attr ) except Attribute Error : pass return self . undefined ( obj = obj , name = argument )", "predictions": ["strips or replace an attribute by either name or another value ."], "references": ["get an item or attribute of an object but prefer the item ."], "bleu": 0.12020484516681697, "rouge_l": 0.23828125000000006}
{"id": 4232, "code": "def find eggs in zip ( importer , path item , only = False ) : if importer . archive . endswith ( '.whl' ) : return metadata = Egg Metadata ( importer ) if metadata . has metadata ( 'PKG-INFO' ) : yield Distribution . from filename ( path item , metadata = metadata ) if only : return for subitem in metadata . resource listdir ( '/' ) : if subitem . endswith ( '.egg' ) : subpath = os . path . join ( path item , subitem ) for dist in find eggs in zip ( zipimport . zipimporter ( subpath ) , subpath ) : yield dist", "predictions": ["find eggs in a generator using the appropriate '.whl' ."], "references": ["find eggs in zip files ; possibly multiple nested eggs ."], "bleu": 0.22447582175704436, "rouge_l": 0.37770897832817335}
{"id": 4233, "code": "def find on path ( importer , path item , only = False ) : path item = normalize cached ( path item ) if os . path . isdir ( path item ) and os . access ( path item , os . R OK ) : if path item . lower ( ) . endswith ( '.egg' ) : yield Distribution . from filename ( path item , metadata = Path Metadata ( path item , os . path . join ( path item , 'EGG-INFO' ) ) ) else : for entry in os . listdir ( path item ) : lower = entry . lower ( ) if lower . endswith ( '.egg-info' ) or lower . endswith ( '.dist-info' ) : fullpath = os . path . join ( path item , entry ) if os . path . isdir ( fullpath ) : metadata = Path Metadata ( path item , fullpath ) else : metadata = File Metadata ( fullpath ) yield Distribution . from location ( path item , entry , metadata , precedence = DEVELOP DIST ) elif not only and lower . endswith ( '.egg' ) : dists = find distributions ( os . path . join ( path item , entry ) ) for dist in dists : yield dist elif not only and lower . endswith ( '.egg-link' ) : with open ( os . path . join ( path item , entry ) ) as entry file : entry lines = entry file . readlines ( ) for line in entry lines : if not line . strip ( ) : continue path = os . path . join ( path item , line . rstrip ( ) ) dists = find distributions ( path ) for item in dists : yield item break", "predictions": ["finds all the jars for the given item ."], "references": ["yield distributions accessible on a sys . path directory"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4234, "code": "def declare namespace ( package Name ) : imp . acquire lock ( ) try : if package Name in namespace packages : return path , parent = sys . path , None if '.' in package Name : parent = '.' . join ( package Name . split ( '.' ) [ : - 1 ] ) declare namespace ( parent ) if parent not in namespace packages : import ( parent ) try : path = sys . modules [ parent ] . path except Attribute Error : raise Type Error ( \"Not a package:\" , parent ) namespace packages . setdefault ( parent , [ ] ) . append ( package Name ) namespace packages . setdefault ( package Name , [ ] ) for path item in path : handle ns ( package Name , path item ) finally : imp . release lock ( )", "predictions": ["declare all modules of the given package"], "references": ["declare that package packagename is a namespace package"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4235, "code": "def get mro ( cls ) : if not isinstance ( cls , type ) : class cls ( cls , object ) : pass return cls . mro [ 1 : ] return cls . mro", "predictions": ["get a full path to the class ."], "references": ["get an mro for a type or classic class"], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 4236, "code": "def find adapter ( registry , ob ) : for t in get mro ( getattr ( ob , ' class ' , type ( ob ) ) ) : if t in registry : return registry [ t ]", "predictions": ["find an adapter . note : this is a good place for the first occurrence of the registry ."], "references": ["return an adapter factory for ob from registry"], "bleu": 0.09629943614188137, "rouge_l": 0.3197903014416776}
{"id": 4237, "code": "def ensure directory ( path ) : dirname = os . path . dirname ( path ) if not os . path . isdir ( dirname ) : os . makedirs ( dirname )", "predictions": ["creates a directory if it does not exist ."], "references": ["ensure that the parent directory of path exists"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4238, "code": "def insert on ( self , path , loc = None ) : loc = loc or self . location if not loc : return nloc = normalize cached ( loc ) bdir = os . path . dirname ( nloc ) npath = [ ( p and normalize cached ( p ) or p ) for p in path ] for p , item in enumerate ( npath ) : if item == nloc : break elif item == bdir and self . precedence == EGG DIST : if path is sys . path : self . check version conflict ( ) path . insert ( p , loc ) npath . insert ( p , nloc ) break else : if path is sys . path : self . check version conflict ( ) path . append ( loc ) return while True : try : np = npath . index ( nloc , p + 1 ) except Value Error : break else : del npath [ np ] , path [ np ] p = np return", "predictions": ["insert a path using this period ."], "references": ["insert self . location in path before its nearest parent directory"], "bleu": 0.1247439242120089, "rouge_l": 0.2136602451838879}
{"id": 4239, "code": "def parse pattern ( pattern ) : if isinstance ( pattern , Number Pattern ) : return pattern def match number ( pattern ) : rv = number re . search ( pattern ) if rv is None : raise Value Error ( 'Invalid number pattern %r' % pattern ) return rv . groups ( ) pos pattern = pattern if ';' in pattern : pos pattern , neg pattern = pattern . split ( ';' , 1 ) pos prefix , number , pos suffix = match number ( pos pattern ) neg prefix , , neg suffix = match number ( neg pattern ) else : pos prefix , number , pos suffix = match number ( pos pattern ) neg prefix = '-' + pos prefix neg suffix = pos suffix if 'E' in number : number , exp = number . split ( 'E' , 1 ) else : exp = None if '@' in number : if '.' in number and '0' in number : raise Value Error ( 'Significant digit patterns can not contain ' '\"@\" or \"0\"' ) if '.' in number : integer , fraction = number . rsplit ( '.' , 1 ) else : integer = number fraction = '' def parse precision ( p ) : \"\"\"Calculate the min and max allowed digits\"\"\" min = max = 0 for c in p : if c in '@0' : min += 1 max += 1 elif c == '#' : max += 1 elif c == ',' : continue else : break return min , max int prec = parse precision ( integer ) frac prec = parse precision ( fraction ) if exp : exp plus = exp . startswith ( '+' ) exp = exp . lstrip ( '+' ) exp prec = parse precision ( exp ) else : exp plus = None exp prec = None grouping = babel . numbers . parse grouping ( integer ) return Number Pattern ( pattern , ( pos prefix , neg prefix ) , ( pos suffix , neg suffix ) , grouping , int prec , frac prec , exp prec , exp plus )", "predictions": ["parse a pattern . return null if the pattern does not match the pattern ."], "references": ["parse number format patterns"], "bleu": 0.08225964699966554, "rouge_l": 0.11753371868978806}
{"id": 4240, "code": "def get decimal quantum ( precision ) : assert isinstance ( precision , ( int , decimal . Decimal ) ) return decimal . Decimal ( 10 ) ** ( - precision )", "predictions": ["get a decimal value from given precision ."], "references": ["return minimal quantum of a number as defined by precision ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 4241, "code": "def scientific notation elements ( self , value , locale ) : exp = value . adjusted ( ) value = value * get decimal quantum ( exp ) assert value . adjusted ( ) == 0 lead shift = max ( [ 1 , min ( self . int prec ) ] ) - 1 exp = exp - lead shift value = value * get decimal quantum ( - lead shift ) exp sign = '' if exp < 0 : exp sign = babel . numbers . get minus sign symbol ( locale ) elif self . exp plus : exp sign = babel . numbers . get plus sign symbol ( locale ) exp = abs ( exp ) return value , exp , exp sign", "predictions": ["get notation for scientific elements ."], "references": ["returns normalized scientific notation components of a value ."], "bleu": 0.1593301391270729, "rouge_l": 0.2573839662447257}
{"id": 4242, "code": "def total seconds ( td ) : if hasattr ( td , 'total seconds' ) : return td . total seconds ( ) ms = td . microseconds secs = ( td . seconds + td . days * 24 * 3600 ) return ( ms + secs * 10 ** 6 ) / 10 ** 6", "predictions": ["compares two canonical dates . < p > the amount of seconds is based on the amount of seconds ."], "references": ["python 2 . 6 compatability"], "bleu": 0.06108557268562171, "rouge_l": 0.08970588235294118}
{"id": 4243, "code": "def check extras ( dist , attr , value ) : try : for k , v in value . items ( ) : if ':' in k : k , m = k . split ( ':' , 1 ) if pkg resources . invalid marker ( m ) : raise Distutils Setup Error ( \"Invalid environment marker: \" + m ) list ( pkg resources . parse requirements ( v ) ) except ( Type Error , Value Error , Attribute Error ) : raise Distutils Setup Error ( \"'extras require' must be a dictionary whose values are \" \"strings or lists of strings containing valid project/version \" \"requirement specifiers.\" )", "predictions": ["verify that all required attributes have been reached ."], "references": ["verify that extras_require mapping is valid"], "bleu": 0.18575057999133596, "rouge_l": 0.27664399092970515}
{"id": 4244, "code": "def check requirements ( dist , attr , value ) : try : list ( pkg resources . parse requirements ( value ) ) except ( Type Error , Value Error ) as error : tmpl = ( \"{attr!r} must be a string or list of strings \" \"containing valid project/version requirement specifiers; {error}\" ) raise Distutils Setup Error ( tmpl . format ( attr = attr , error = error ) )", "predictions": ["verify that this field value is a valid requirements of a given value ."], "references": ["verify that install_requires is a valid requirements list"], "bleu": 0.28295596283263513, "rouge_l": 0.5736677115987461}
{"id": 4245, "code": "def fetch build egg ( self , req ) : try : cmd = self . egg fetcher cmd . package index . to scan = [ ] except Attribute Error : from setuptools . command . easy install import easy install dist = self . class ( { 'script args' : [ 'easy install' ] } ) dist . parse config files ( ) opts = dist . get option dict ( 'easy install' ) keep = ( 'find links' , 'site dirs' , 'index url' , 'optimize' , 'site dirs' , 'allow hosts' ) for key in list ( opts ) : if key not in keep : del opts [ key ] if self . dependency links : links = self . dependency links [ : ] if 'find links' in opts : links = opts [ 'find links' ] [ 1 ] . split ( ) + links opts [ 'find links' ] = ( 'setup' , links ) install dir = self . get egg cache dir ( ) cmd = easy install ( dist , args = [ \"x\" ] , install dir = install dir , exclude scripts = True , always copy = False , build directory = None , editable = False , upgrade = False , multi version = True , no report = True , user = False ) cmd . ensure finalized ( ) self . egg fetcher = cmd return cmd . easy install ( req )", "predictions": ["fetches the list of ( ."], "references": ["fetch an egg needed for building"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4246, "code": "def do dice roll ( ) : options = get options ( ) dice = Dice ( options . sides ) rolls = [ dice . roll ( ) for n in range ( options . number ) ] for roll in rolls : print ( 'rolled' , roll ) if options . number > 1 : print ( 'total' , sum ( rolls ) )", "predictions": ["instantiates an iterable of options ."], "references": ["roll n - sided dice and return each result and the total"], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 4247, "code": "def price converter ( obj ) : if isinstance ( obj , str ) : obj = Price Class . parse ( obj ) return obj", "predictions": ["parse a price into an object ."], "references": ["ensures that string prices are converted into price objects ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 4248, "code": "def get method ( self , args ) : try : method = self . app [ args [ 'method' ] ] except Key Error : method not found ( args [ 'id' ] ) else : return method", "predictions": ["borrowed from method calls ."], "references": ["get request method for service application ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4249, "code": "def apply ( self , method , args ) : try : params = args [ 'params' ] if isinstance ( params , dict ) : result = method ( * * params ) else : result = method ( * params ) except Exception as error : server error ( args [ 'id' ] , error ) else : return result", "predictions": ["apply the given method to the given method ."], "references": ["apply application method ."], "bleu": 0.19960198807747329, "rouge_l": 0.4959349593495934}
{"id": 4250, "code": "def blueprint ( self ) : if self . url rule and '.' in self . url rule . endpoint : return self . url rule . endpoint . rsplit ( '.' , 1 ) [ 0 ]", "predictions": ["this method is called to determine if this link has been added to the database ."], "references": ["the name of the current blueprint"], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 4251, "code": "def cleanup files ( self ) : logger . debug ( 'Cleaning up...' ) with indent log ( ) : for req in self . reqs to cleanup : req . remove temporary source ( )", "predictions": ["get rid of all files previously processed ."], "references": ["clean up files remove builds ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 4252, "code": "def get all ns packages ( self ) : nsp = set ( ) for pkg in self . distribution . namespace packages or [ ] : pkg = pkg . split ( '.' ) while pkg : nsp . add ( '.' . join ( pkg ) ) pkg . pop ( ) return sorted ( nsp )", "predictions": ["get package list . note that this is not a sorted list ."], "references": ["return sorted list of all package namespaces"], "bleu": 0.1350862565735141, "rouge_l": 0.21143847487001732}
{"id": 4253, "code": "def default ( self , obj ) : if isinstance ( obj , models . Model ) : return self . encode ( model to dict ( obj ) ) elif isinstance ( obj , models . query . Query Set ) : return serializers . serialize ( 'json' , obj ) else : return super ( Json Response Encoder , self ) . default ( obj )", "predictions": ["alias for this object into a default query ."], "references": ["convert queryset objects to their list counter - parts"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4254, "code": "def tokenize annotated ( doc , annotation ) : tokens = tokenize ( doc , include hrefs = False ) for tok in tokens : tok . annotation = annotation return tokens", "predictions": ["tokenize the given annotation into a new annotation ."], "references": ["tokenize a document and add an annotation attribute to each token"], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 4255, "code": "def copy annotations ( src , dest ) : assert len ( src ) == len ( dest ) for src tok , dest tok in zip ( src , dest ) : dest tok . annotation = src tok . annotation", "predictions": ["copy data from src to dest"], "references": ["copy annotations from the tokens listed in src to the tokens in dest"], "bleu": 0.10765856691178478, "rouge_l": 0.4935275080906149}
{"id": 4256, "code": "def fixup chunks ( chunks ) : tag accum = [ ] cur word = None result = [ ] for chunk in chunks : if isinstance ( chunk , tuple ) : if chunk [ 0 ] == 'img' : src = chunk [ 1 ] tag , trailing whitespace = split trailing whitespace ( chunk [ 2 ] ) cur word = tag token ( 'img' , src , html repr = tag , pre tags = tag accum , trailing whitespace = trailing whitespace ) tag accum = [ ] result . append ( cur word ) elif chunk [ 0 ] == 'href' : href = chunk [ 1 ] cur word = href token ( href , pre tags = tag accum , trailing whitespace = \" \" ) tag accum = [ ] result . append ( cur word ) continue if is word ( chunk ) : chunk , trailing whitespace = split trailing whitespace ( chunk ) cur word = token ( chunk , pre tags = tag accum , trailing whitespace = trailing whitespace ) tag accum = [ ] result . append ( cur word ) elif is start tag ( chunk ) : tag accum . append ( chunk ) elif is end tag ( chunk ) : if tag accum : tag accum . append ( chunk ) else : assert cur word , ( \"Weird state, cur word=%r, result=%r, chunks=%r of %r\" % ( cur word , result , chunk , chunks ) ) cur word . post tags . append ( chunk ) else : assert ( 0 ) if not result : return [ token ( '' , pre tags = tag accum ) ] else : result [ - 1 ] . post tags . extend ( tag accum ) return result", "predictions": ["in the name of this [ [ [ [ [ [ tags ] [ tags ] [ [ [ l ] [ [ l ] [ [ [ [ l ] [ [ [ [ [ [ [ l ] [ [ [ [ [ ] ] [ [ ["], "references": ["this function takes a list of chunks and produces a list of tokens ."], "bleu": 0.026594139297659906, "rouge_l": 0.03477765108323831}
{"id": 4257, "code": "def start tag ( el ) : return '<%s%s>' % ( el . tag , '' . join ( [ ' %s=\"%s\"' % ( name , html escape ( value , True ) ) for name , value in el . attrib . items ( ) ] ) )", "predictions": ["starts an xml handler ."], "references": ["the text representation of the start tag for a tag ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4258, "code": "def fixup ins del tags ( doc ) : for tag in [ 'ins' , 'del' ] : for el in doc . xpath ( 'descendant-or-self::%s' % tag ) : if not contains block level tag ( el ) : continue move el inside block ( el , tag = tag ) el . drop tag ( )", "predictions": ["deletes all tags from an except that have been marked as self - based h ."], "references": ["fixup_ins_del_tags that works on an lxml document in - place"], "bleu": 0.09147827112247602, "rouge_l": 0.16052631578947368}
{"id": 4259, "code": "def cache url ( self , * * kwargs ) : query = { 'Operation' : self . Operation , 'Service' : \"AWSE Commerce Service\" , 'Version' : self . Version , } query . update ( kwargs ) service domain = SERVICE DOMAINS [ self . Region ] [ 0 ] return \"http://\" + service domain + \"/onca/xml?\" + quote query ( query )", "predictions": ["as this method is called by the server when it gets ready to add additional data to the server ."], "references": ["a simplified url to be used for caching the given query ."], "bleu": 0.07264339766175722, "rouge_l": 0.1963519313304721}
{"id": 4260, "code": "def document fromstring ( html , guess charset = True , parser = None ) : if not isinstance ( html , strings ) : raise Type Error ( 'string required' ) if parser is None : parser = html parser return parser . parse ( html , use Chardet = guess charset ) . getroot ( )", "predictions": ["] with the document , if any ."], "references": ["parse a whole document into a string ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4261, "code": "def export ( self , location ) : url , rev = self . get url rev ( ) rev options = get rev options ( url , rev ) logger . info ( 'Exporting svn repository %s to %s' , url , location ) with indent log ( ) : if os . path . exists ( location ) : rmtree ( location ) self . run command ( [ 'export' ] + rev options + [ url , location ] , show stdout = False )", "predictions": ["override this method to run the tempdir for exporting the tempdir ."], "references": ["export the svn repository at the url to the destination location"], "bleu": 0.1235622127262679, "rouge_l": 0.17528735632183906}
{"id": 4262, "code": "def get revision ( self , location ) : revision = 0 for base , dirs , files in os . walk ( location ) : if self . dirname not in dirs : dirs [ : ] = [ ] continue dirs . remove ( self . dirname ) entries fn = os . path . join ( base , self . dirname , 'entries' ) if not os . path . exists ( entries fn ) : continue dirurl , localrev = self . get svn url rev ( base ) if base == location : base url = dirurl + '/' elif not dirurl or not dirurl . startswith ( base url ) : dirs [ : ] = [ ] continue revision = max ( revision , localrev ) return revision", "predictions": ["retrieves the message at the specified args ."], "references": ["return the maximum revision for all files under a given location"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 4263, "code": "def unique ( iterable ) : seen = set ( ) for value in iterable : if not value in seen : seen . add ( value ) yield value", "predictions": ["yield an self . return a list ."], "references": ["yield unique values in iterable preserving order ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4264, "code": "def handle requires ( metadata , pkg info , key ) : may requires = defaultdict ( list ) for value in pkg info . get all ( key ) : extra match = EXTRA RE . search ( value ) if extra match : groupdict = extra match . groupdict ( ) condition = groupdict [ 'condition' ] extra = groupdict [ 'extra' ] package = groupdict [ 'package' ] if condition . endswith ( ' and ' ) : condition = condition [ : - 5 ] else : condition , extra = None , None package = value key = May Requires Key ( condition , extra ) may requires [ key ] . append ( package ) if may requires : metadata [ 'run requires' ] = [ ] for key , value in may requires . items ( ) : may requirement = { 'requires' : value } if key . extra : may requirement [ 'extra' ] = key . extra if key . condition : may requirement [ 'environment' ] = key . condition metadata [ 'run requires' ] . append ( may requirement ) if not 'extras' in metadata : metadata [ 'extras' ] = [ ] metadata [ 'extras' ] . extend ( [ key . extra for key in may requires . keys ( ) if key . extra ] )", "predictions": ["find correct element associated with given path and find its metadata ."], "references": ["place the runtime requirements from pkg_info into metadata ."], "bleu": 0.1367440667823257, "rouge_l": 0.19551282051282048}
{"id": 4265, "code": "def requires to requires dist ( requirement ) : requires dist = [ ] for op , ver in requirement . specs : requires dist . append ( op + ver ) if not requires dist : return '' return \" (%s)\" % ',' . join ( requires dist )", "predictions": ["convert a set of distributions to a textual representation ."], "references": ["compose the version predicates for requirement in pep 345 fashion ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 4266, "code": "def modules ( self ) : sys . path . insert ( 0 , self . basedir ) for p in self . paths ( ) : try : module name = self . module path ( p ) logger . debug ( \"Importing {} from path {}\" . format ( module name , p ) ) m = importlib . import module ( module name ) yield m except Exception as e : logger . warning ( 'Caught exception while importing {}: {}' . format ( p , e ) ) logger . warning ( e , exc info = True ) error info = getattr ( self , 'error info' , None ) if not error info : exc info = sys . exc info ( ) #raise e. class , e, exc info[2] #self.error info = (e, exc info) self . error info = exc info continue sys . path . pop ( 0 )", "predictions": ["copies all the systems except the , and do not include the public api ."], "references": ["return modules that match module_name"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 4267, "code": "def classes ( self ) : for module in self . modules ( ) : cs = inspect . getmembers ( module , inspect . isclass ) class name = getattr ( self , 'class name' , '' ) class regex = '' if class name : if class name . startswith ( \"*\" ) : class name = class name . strip ( \"*\" ) class regex = re . compile ( r'.*?{}' . format ( class name ) , re . I ) else : class regex = re . compile ( r'^{}' . format ( class name ) , re . I ) for c name , c in cs : can yield = True if class regex and not class regex . match ( c name ) : #if class name and class name not in c name: can yield = False if can yield and issubclass ( c , unittest . Test Case ) : if c is not unittest . Test Case : logger . debug ( 'class: {} matches {}' . format ( c name , class name ) ) yield c", "predictions": ["to generate the get method that determines what the parameter should be automatically generated ."], "references": ["the partial self . class_name will be used to find actual testcase classes"], "bleu": 0.10343603005129705, "rouge_l": 0.14472123368920523}
{"id": 4268, "code": "def method names ( self ) : for c in self . classes ( ) : #ms = inspect.getmembers(c, inspect.ismethod) ms = inspect . getmembers ( c , lambda f : inspect . ismethod ( f ) or inspect . isfunction ( f ) ) method name = getattr ( self , 'method name' , '' ) method regex = '' if method name : if method name . startswith ( self . method prefix ) : method regex = re . compile ( r'^{}' . format ( method name ) , flags = re . I ) else : if method name . startswith ( \"*\" ) : method name = method name . strip ( \"*\" ) method regex = re . compile ( r'^{}[ ]{{0,1}}.*?{}' . format ( self . method prefix , method name ) , flags = re . I ) else : method regex = re . compile ( r'^{}[ ]{{0,1}}{}' . format ( self . method prefix , method name ) , flags = re . I ) for m name , m in ms : if not m name . startswith ( self . method prefix ) : continue can yield = True if method regex and not method regex . match ( m name ) : can yield = False if can yield : logger . debug ( 'method: {} matches {}' . format ( m name , method name ) ) yield c , m name", "predictions": ["returns a method that returns a method of the given find object ."], "references": ["return the actual test methods that matched self . method_name"], "bleu": 0.1135935489027116, "rouge_l": 0.1781021897810219}
{"id": 4269, "code": "def dump arg defaults ( kwargs ) : if current app : kwargs . setdefault ( 'cls' , current app . json encoder ) if not current app . config [ 'JSON AS ASCII' ] : kwargs . setdefault ( 'ensure ascii' , False ) kwargs . setdefault ( 'sort keys' , current app . config [ 'JSON SORT KEYS' ] ) else : kwargs . setdefault ( 'sort keys' , True ) kwargs . setdefault ( 'cls' , JSON Encoder )", "predictions": ["inject default arguments for a command ."], "references": ["inject default arguments for dump functions ."], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 4270, "code": "def load arg defaults ( kwargs ) : if current app : kwargs . setdefault ( 'cls' , current app . json decoder ) else : kwargs . setdefault ( 'cls' , JSON Decoder )", "predictions": ["loads the default arguments from the given json string ."], "references": ["inject default arguments for load functions ."], "bleu": 0.17827531042796255, "rouge_l": 0.36454183266932266}
{"id": 4271, "code": "def get dist ( self ) : egg info = self . egg info path ( '' ) . rstrip ( '/' ) base dir = os . path . dirname ( egg info ) metadata = pkg resources . Path Metadata ( base dir , egg info ) dist name = os . path . splitext ( os . path . basename ( egg info ) ) [ 0 ] return pkg resources . Distribution ( os . path . dirname ( egg info ) , project name = dist name , metadata = metadata )", "predictions": ["get the print distribution from this contentelement ."], "references": ["return a pkg_resources . distribution built from self . egg_info_path"], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 4272, "code": "def to text ( s , blank if none = True ) : if s is None : if blank if none : return \"\" else : return None elif isinstance ( s , text type ) : return s else : return text type ( s )", "predictions": ["convert to lambda lambda ."], "references": ["wrapper around six . text_type to convert none to empty string"], "bleu": 0.09778809693469985, "rouge_l": 0.2341650671785029}
{"id": 4273, "code": "def find ca bundle ( ) : if os . name == 'nt' : return get win certfile ( ) else : for cert path in cert paths : if os . path . isfile ( cert path ) : return cert path try : return pkg resources . resource filename ( 'certifi' , 'cacert.pem' ) except ( Import Error , Resolution Error , Extraction Error ) : return None", "predictions": ["scientific elements of this elements are matching the specified elements ."], "references": ["return an existing ca bundle path or none"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4274, "code": "def parse ( doc , treebuilder = \"etree\" , encoding = None , namespace HTML Elements = True ) : tb = treebuilders . get Tree Builder ( treebuilder ) p = HTML Parser ( tb , namespace HTML Elements = namespace HTML Elements ) return p . parse ( doc , encoding = encoding )", "predictions": ["total cluster , conditional , and removes all the declarations that are included in the tree ."], "references": ["parse a string or file - like object into a tree"], "bleu": 0.07223943354597204, "rouge_l": 0.07429963459196103}
{"id": 4275, "code": "def bind ( self ) : HTTP Server . init ( self , ( self . host , self . port ) , HTTP Request Handler ) self . port = self . server port", "predictions": ["binds the request to the server ."], "references": ["bind and activate http server ."], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 4276, "code": "def report ( self ) : print ( self . report message . format ( service = self . service , host = self . host , port = self . port , ) ) sys . stdout . flush ( )", "predictions": ["flushes the check method ."], "references": ["report startup info to stdout ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4277, "code": "def load bytecode ( self , f ) : magic = f . read ( len ( bc magic ) ) if magic != bc magic : self . reset ( ) return checksum = pickle . load ( f ) if self . checksum != checksum : self . reset ( ) return self . code = marshal load ( f )", "predictions": ["view function to fetch a build file and returns it ."], "references": ["loads bytecode from a file or file like object ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 4278, "code": "def get impl ver ( ) : impl ver = sysconfig . get config var ( \"py version nodot\" ) if not impl ver : impl ver = '' . join ( map ( str , sys . version info [ : 2 ] ) ) return impl ver", "predictions": ["do not use this method to return the version number ."], "references": ["return implementation version ."], "bleu": 0.1354599427337814, "rouge_l": 0.43675417661097854}
{"id": 4279, "code": "def distros for location ( location , basename , metadata = None ) : if basename . endswith ( '.egg.zip' ) : basename = basename [ : - 4 ] if basename . endswith ( '.egg' ) and '-' in basename : return [ Distribution . from location ( location , basename , metadata ) ] if basename . endswith ( '.exe' ) : win base , py ver , platform = parse bdist wininst ( basename ) if win base is not None : return interpret distro name ( location , win base , metadata , py ver , BINARY DIST , platform ) # for ext in EXTENSIONS : if basename . endswith ( ext ) : basename = basename [ : - len ( ext ) ] return interpret distro name ( location , basename , metadata ) return [ ]", "predictions": ["map the platform to a ( ( str str str str str str str str str str str str str str str . ( str str . ( obj . str . ( . str . ( obj ."], "references": ["yield egg or source distribution objects based on basename"], "bleu": 0.025984987978515034, "rouge_l": 0.0}
{"id": 4280, "code": "def find external links ( url , page ) : for match in REL . finditer ( page ) : tag , rel = match . groups ( ) rels = set ( map ( str . strip , rel . lower ( ) . split ( ',' ) ) ) if 'homepage' in rels or 'download' in rels : for match in HREF . finditer ( tag ) : yield urljoin ( url , htmldecode ( match . group ( 1 ) ) ) for tag in ( \"<th>Home Page\" , \"<th>Download URL\" ) : pos = page . find ( tag ) if pos != - 1 : match = HREF . search ( page , pos ) if match : yield urljoin ( url , htmldecode ( match . group ( 1 ) ) )", "predictions": ["get all the ( and their associated ( : they should have been included : it will have been created : a stream of the ) ."], "references": ["find rel = homepage and rel = download links in page yielding urls"], "bleu": 0.044915755686574035, "rouge_l": 0.053368328958880135}
{"id": 4281, "code": "def local open ( url ) : scheme , server , path , param , query , frag = urlparse ( url ) filename = url2pathname ( path ) if os . path . isfile ( filename ) : return urllib2 . urlopen ( url ) elif path . endswith ( '/' ) and os . path . isdir ( filename ) : files = [ ] for f in os . listdir ( filename ) : if f == 'index.html' : with open ( os . path . join ( filename , f ) , 'r' ) as fp : body = fp . read ( ) break elif os . path . isdir ( os . path . join ( filename , f ) ) : f += '/' files . append ( \"<a href=%r>%s</a>\" % ( f , f ) ) else : body = ( \"<html><head><title>%s</title>\" % url ) + \"</head><body>%s</body></html>\" % '\\n' . join ( files ) status , message = 200 , \"OK\" else : status , message , body = 404 , \"Path not found\" , \"Not found\" headers = { 'content-type' : 'text/html' } return HTTP Error ( url , status , message , headers , String IO ( body ) )", "predictions": ["returns evaluator from an encoded , or resource if there is no 200 or ."], "references": ["read a local path with special support for directories"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 4282, "code": "def process url ( self , url , retrieve = False ) : if url in self . scanned urls and not retrieve : return self . scanned urls [ url ] = True if not URL SCHEME ( url ) : self . process filename ( url ) return else : dists = list ( distros for url ( url ) ) if dists : if not self . url ok ( url ) : return self . debug ( \"Found link: %s\" , url ) if dists or not retrieve or url in self . fetched urls : list ( map ( self . add , dists ) ) return if not self . url ok ( url ) : self . fetched urls [ url ] = True return self . info ( \"Reading %s\" , url ) self . fetched urls [ url ] = True f = self . open url ( url , \"Download error on %s: %%s -- Some packages may not be found!\" % url ) if f is None : return self . fetched urls [ f . url ] = True if 'html' not in f . headers . get ( 'content-type' , '' ) . lower ( ) : f . close ( ) return base = f . url page = f . read ( ) if not isinstance ( page , str ) : if isinstance ( f , HTTP Error ) : charset = 'latin-1' else : charset = f . headers . get param ( 'charset' ) or 'latin-1' page = page . decode ( charset , \"ignore\" ) f . close ( ) for match in HREF . finditer ( page ) : link = urljoin ( base , htmldecode ( match . group ( 1 ) ) ) self . process url ( link ) if url . startswith ( self . index url ) and getattr ( f , 'code' , None ) != 404 : page = self . process index ( url , page )", "predictions": ["try to if any of the passed ( usually two urls rule rule rule rule rule rule rule is set rule rule ."], "references": ["evaluate a url as a possible download and maybe retrieve it"], "bleu": 0.04449945957170705, "rouge_l": 0.0}
{"id": 4283, "code": "def init pathinfo ( ) : d = set ( ) for dir in sys . path : try : if os . path . isdir ( dir ) : dir , dircase = makepath ( dir ) d . add ( dircase ) except Type Error : continue return d", "predictions": ["return a . override this method if you don ' t exist ."], "references": ["return a set containing all existing directory entries from sys . path"], "bleu": 0.1350862565735141, "rouge_l": 0.2417437252311757}
{"id": 4284, "code": "def setcopyright ( ) : builtins . copyright = Printer ( \"copyright\" , sys . copyright ) if is jython : builtins . credits = Printer ( \"credits\" , \"Jython is maintained by the Jython developers (www.jython.org).\" ) elif is pypy : builtins . credits = Printer ( \"credits\" , \"Py Py is maintained by the Py Py developers: http://pypy.org/\" ) else : builtins . credits = Printer ( \"credits\" , ) here = os . path . dirname ( os . file ) builtins . license = Printer ( \"license\" , \"See http://www.python.org/%.3s/license.html\" % sys . version , [ \"LICENSE.txt\" , \"LICENSE\" ] , [ os . path . join ( here , os . pardir ) , here , os . curdir ] )", "predictions": ["initializes an ) with the given ( ."], "references": ["set copyright and credits in __builtin__"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4285, "code": "def have pyrex ( ) : pyrex impls = 'Cython.Distutils.build ext' , 'Pyrex.Distutils.build ext' for pyrex impl in pyrex impls : try : import ( pyrex impl , fromlist = [ 'build ext' ] ) . build ext return True except Exception : pass return False", "predictions": ["this function checks if all valid conditions have been generated ."], "references": ["return true if cython or pyrex can be imported ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 4286, "code": "def debug application ( self , environ , start response ) : app iter = None try : app iter = self . app ( environ , start response ) for item in app iter : yield item if hasattr ( app iter , 'close' ) : app iter . close ( ) except Exception : if hasattr ( app iter , 'close' ) : app iter . close ( ) traceback = get current traceback ( skip = 1 , show hidden frames = self . show hidden frames , ignore system exceptions = True ) for frame in traceback . frames : self . frames [ frame . id ] = frame self . tracebacks [ traceback . id ] = traceback try : start response ( '500 INTERNAL SERVER ERROR' , [ ( 'Content-Type' , 'text/html; charset=utf-8' ) , ( 'X-XSS-Protection' , '0' ) , ] ) except Exception : environ [ 'wsgi.errors' ] . write ( 'Debugging middleware caught exception in streamed ' 'response at a point where response headers were already ' 'sent.\\n' ) else : yield traceback . render full ( evalex = self . evalex , secret = self . secret ) . encode ( 'utf-8' , 'replace' ) traceback . log ( environ [ 'wsgi.errors' ] )", "predictions": ["this is the only call to get the tokenize object ."], "references": ["run the application and conserve the traceback frames ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 4287, "code": "def get resource ( self , request , filename ) : filename = join ( dirname ( file ) , 'shared' , basename ( filename ) ) if isfile ( filename ) : mimetype = mimetypes . guess type ( filename ) [ 0 ] or 'application/octet-stream' f = open ( filename , 'rb' ) try : return Response ( f . read ( ) , mimetype = mimetype ) finally : f . close ( ) return Response ( 'Not Found' , status = 404 )", "predictions": ["get absolute path for an existing annotations ."], "references": ["return a static resource from the shared folder ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4288, "code": "def user agent ( ) : data = { \"installer\" : { \"name\" : \"pip\" , \"version\" : pip . version } , \"python\" : platform . python version ( ) , \"implementation\" : { \"name\" : platform . python implementation ( ) , } , } if data [ \"implementation\" ] [ \"name\" ] == 'C Python' : data [ \"implementation\" ] [ \"version\" ] = platform . python version ( ) elif data [ \"implementation\" ] [ \"name\" ] == 'Py Py' : if sys . pypy version info . releaselevel == 'final' : pypy version info = sys . pypy version info [ : 3 ] else : pypy version info = sys . pypy version info data [ \"implementation\" ] [ \"version\" ] = \".\" . join ( [ str ( x ) for x in pypy version info ] ) elif data [ \"implementation\" ] [ \"name\" ] == 'Jython' : data [ \"implementation\" ] [ \"version\" ] = platform . python version ( ) elif data [ \"implementation\" ] [ \"name\" ] == 'Iron Python' : data [ \"implementation\" ] [ \"version\" ] = platform . python version ( ) if sys . platform . startswith ( \"linux\" ) : distro = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"name\" , \"version\" , \"id\" ] , platform . linux distribution ( ) ) , ) ) libc = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"lib\" , \"version\" ] , platform . libc ver ( ) ) , ) ) if libc : distro [ \"libc\" ] = libc if distro : data [ \"distro\" ] = distro if sys . platform . startswith ( \"darwin\" ) and platform . mac ver ( ) [ 0 ] : data [ \"distro\" ] = { \"name\" : \"OS X\" , \"version\" : platform . mac ver ( ) [ 0 ] } if platform . system ( ) : data . setdefault ( \"system\" , { } ) [ \"name\" ] = platform . system ( ) if platform . release ( ) : data . setdefault ( \"system\" , { } ) [ \"release\" ] = platform . release ( ) if platform . machine ( ) : data [ \"cpu\" ] = platform . machine ( ) return \"{data[installer][name]}/{data[installer][version]} {json}\" . format ( data = data , json = json . dumps ( data , separators = ( \",\" , \":\" ) , sort keys = True ) , )", "predictions": ["return the server tuple for a given pip connection ."], "references": ["return a string representing the user agent ."], "bleu": 0.15851165692617156, "rouge_l": 0.34014869888475835}
{"id": 4289, "code": "def is url ( name ) : if ':' not in name : return False scheme = name . split ( ':' , 1 ) [ 0 ] . lower ( ) return scheme in [ 'http' , 'https' , 'file' , 'ftp' ] + vcs . all schemes", "predictions": ["test if this is an square-free uri ."], "references": ["returns true if the name looks like a url"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4290, "code": "def download http url ( link , session , temp dir ) : target url = link . url . split ( '#' , 1 ) [ 0 ] try : resp = session . get ( target url , headers = { \"Accept-Encoding\" : \"identity\" } , stream = True , ) resp . raise for status ( ) except requests . HTTP Error as exc : logger . critical ( \"HTTP error %s while getting %s\" , exc . response . status code , link , ) raise content type = resp . headers . get ( 'content-type' , '' ) filename = link . filename content disposition = resp . headers . get ( 'content-disposition' ) if content disposition : type , params = cgi . parse header ( content disposition ) filename = params . get ( 'filename' ) or filename ext = splitext ( filename ) [ 1 ] if not ext : ext = mimetypes . guess extension ( content type ) if ext : filename += ext if not ext and link . url != resp . url : ext = os . path . splitext ( resp . url ) [ 1 ] if ext : filename += ext file path = os . path . join ( temp dir , filename ) with open ( file path , 'wb' ) as content file : download url ( resp , link , content file ) return file path , content type", "predictions": ["download http response headers from link to target ."], "references": ["download link url into temp_dir using provided session"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4291, "code": "def currency Format ( context , code , symbol , format , currency digits = True , decimal quantization = True , name = '' ) : context . action ( discriminator = ( 'currency' , name , code ) , callable = register currency , args = ( name , code , symbol , format , currency digits , decimal quantization ) )", "predictions": ["constructs an currency for the given currency and currency ."], "references": ["handle currencyformat subdirectives ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 4292, "code": "def exchange ( context , component , backend , base , name = '' ) : context . action ( discriminator = ( 'currency' , 'exchange' , component ) , callable = register exchange , args = ( name , component , backend , base ) )", "predictions": ["creates a new 'currency' ."], "references": ["handle exchange subdirectives ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4293, "code": "def print results ( distributions , list all files ) : results printed = False for dist in distributions : results printed = True logger . info ( \"---\" ) logger . info ( \"Metadata-Version: %s\" % dist . get ( 'metadata-version' ) ) logger . info ( \"Name: %s\" % dist [ 'name' ] ) logger . info ( \"Version: %s\" % dist [ 'version' ] ) logger . info ( \"Summary: %s\" % dist . get ( 'summary' ) ) logger . info ( \"Home-page: %s\" % dist . get ( 'home-page' ) ) logger . info ( \"Author: %s\" % dist . get ( 'author' ) ) logger . info ( \"Author-email: %s\" % dist . get ( 'author-email' ) ) logger . info ( \"License: %s\" % dist . get ( 'license' ) ) logger . info ( \"Location: %s\" % dist [ 'location' ] ) logger . info ( \"Requires: %s\" % ', ' . join ( dist [ 'requires' ] ) ) if list all files : logger . info ( \"Files:\" ) if dist [ 'files' ] is not None : for line in dist [ 'files' ] : logger . info ( \"  %s\" % line . strip ( ) ) else : logger . info ( \"Cannot locate installed-files.txt\" ) if 'entry points' in dist : logger . info ( \"Entry-points:\" ) for line in dist [ 'entry points' ] : logger . info ( \"  %s\" % line . strip ( ) ) return results printed", "predictions": ["prints the results of two libraries ."], "references": ["print the informations from installed distributions found ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4294, "code": "def decode ( self , data , decode content , flush decoder ) : try : if decode content and self . decoder : data = self . decoder . decompress ( data ) except ( IO Error , zlib . error ) as e : content encoding = self . headers . get ( 'content-encoding' , '' ) . lower ( ) raise Decode Error ( \"Received response with content-encoding: %s, but \" \"failed to decode it.\" % content encoding , e ) if flush decoder and decode content and self . decoder : buf = self . decoder . decompress ( binary type ( ) ) data += buf + self . decoder . flush ( ) return data", "predictions": ["decodes ipython data using the specified x - www - ( ."], "references": ["decode the data passed in and potentially flush the decoder ."], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 4295, "code": "def render ( template , context , app ) : rv = template . render ( context ) template rendered . send ( app , template = template , context = context ) return rv", "predictions": ["render a template with the given context and context ."], "references": ["renders the template and fires the signal"], "bleu": 0.14991106946711685, "rouge_l": 0.24302788844621517}
{"id": 4296, "code": "def parse version ( version ) : global parse version try : from pkg resources import parse version except Import Error : from distutils . version import Loose Version as parse version return parse version ( version )", "predictions": ["parse the version of flocker ."], "references": ["use parse_version from pkg_resources or distutils as available ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4297, "code": "def is declared ( self , name ) : if name in self . declared locally or name in self . declared parameter : return True return name in self . declared", "predictions": ["check if a method is registered in this group ."], "references": ["check if a name is declared in this or an outer scope ."], "bleu": 0.22211370348624584, "rouge_l": 0.5947075208913649}
{"id": 4298, "code": "def visit Name ( self , node ) : if node . ctx == 'store' : self . identifiers . declared locally . add ( node . name ) elif node . ctx == 'param' : self . identifiers . declared parameter . add ( node . name ) elif node . ctx == 'load' and not self . identifiers . is declared ( node . name ) : self . identifiers . undeclared . add ( node . name )", "predictions": ["generate the name of this class ."], "references": ["all assignments to names go through this function ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4299, "code": "def visit From Import ( self , node , frame ) : self . newline ( node ) self . write ( 'included template = environment.get template(' ) self . visit ( node . template , frame ) self . write ( ', %r).' % self . name ) if node . with context : self . write ( 'make module(context.parent, True)' ) else : self . write ( 'module' ) var names = [ ] discarded names = [ ] for name in node . names : if isinstance ( name , tuple ) : name , alias = name else : alias = name self . writeline ( 'l %s = getattr(included template, ' '%r, missing)' % ( alias , name ) ) self . writeline ( 'if l %s is missing:' % alias ) self . indent ( ) self . writeline ( 'l %s = environment.undefined(%r %% ' 'included template. name , ' 'name=%r)' % ( alias , 'the template %%r (imported on %s) does ' 'not export the requested name %s' % ( self . position ( node ) , repr ( name ) ) , name ) ) self . outdent ( ) if frame . toplevel : var names . append ( alias ) if not alias . startswith ( ' ' ) : discarded names . append ( alias ) frame . assigned names . add ( alias ) if var names : if len ( var names ) == 1 : name = var names [ 0 ] self . writeline ( 'context.vars[%r] = l %s' % ( name , name ) ) else : self . writeline ( 'context.vars.update({%s})' % ', ' . join ( '%r: l %s' % ( name , name ) for name in var names ) ) if discarded names : if len ( discarded names ) == 1 : self . writeline ( 'context.exported vars.discard(%r)' % discarded names [ 0 ] ) else : self . writeline ( 'context.exported vars.difference ' 'update((%s))' % ', ' . join ( imap ( repr , discarded names ) ) )", "predictions": ["makes the template from the map and writes the contents of the map ."], "references": ["visit named imports ."], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 4300, "code": "def populate requirement set ( requirement set , args , options , finder , session , name , wheel cache ) : for req in args : requirement set . add requirement ( Install Requirement . from line ( req , None , isolated = options . isolated mode , wheel cache = wheel cache ) ) for req in options . editables : requirement set . add requirement ( Install Requirement . from editable ( req , default vcs = options . default vcs , isolated = options . isolated mode , wheel cache = wheel cache ) ) found req in file = False for filename in options . requirements : for req in parse requirements ( filename , finder = finder , options = options , session = session , wheel cache = wheel cache ) : found req in file = True requirement set . add requirement ( req ) if not ( args or options . editables or found req in file ) : opts = { 'name' : name } if options . find links : msg = ( 'You must give at least one requirement to ' '%(name)s (maybe you meant \"pip %(name)s ' '%(links)s\"?)' % dict ( opts , links = ' ' . join ( options . find links ) ) ) else : msg = ( 'You must give at least one requirement ' 'to %(name)s (see \"pip help %(name)s\")' % opts ) logger . warning ( msg )", "predictions": ["populate the map of requirements ."], "references": ["marshal cmd line args into a requirement set ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4301, "code": "def export ( self , location ) : temp dir = tempfile . mkdtemp ( '-export' , 'pip-' ) self . unpack ( temp dir ) if os . path . exists ( location ) : rmtree ( location ) try : self . run command ( [ 'export' , location ] , cwd = temp dir , show stdout = False ) finally : rmtree ( temp dir )", "predictions": ["copies the specified location from this object to a location ."], "references": ["export the bazaar repository at the url to the destination location"], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 4302, "code": "def verify signature ( self , key , value , sig ) : return constant time compare ( sig , self . get signature ( key , value ) )", "predictions": ["verifies that the given signature values are available ."], "references": ["verifies the given signature matches the expected signature"], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 4303, "code": "def get signature ( self , value ) : value = want bytes ( value ) key = self . derive key ( ) sig = self . algorithm . get signature ( key , value ) return base64 encode ( sig )", "predictions": ["get the desired signature for this class ."], "references": ["returns the signature for the given value"], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 4304, "code": "def sign ( self , value ) : return value + want bytes ( self . sep ) + self . get signature ( value )", "predictions": ["sign the signature for this diagnostic ."], "references": ["signs the given string ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4305, "code": "def verify signature ( self , value , sig ) : key = self . derive key ( ) try : sig = base64 decode ( sig ) except Exception : return False return self . algorithm . verify signature ( key , value , sig )", "predictions": ["verifies the given signature ."], "references": ["verifies the signature for the given value ."], "bleu": 0.2595169998796149, "rouge_l": 0.5907990314769976}
{"id": 4306, "code": "def unsign ( self , signed value ) : signed value = want bytes ( signed value ) sep = want bytes ( self . sep ) if sep not in signed value : raise Bad Signature ( 'No %r found in value' % self . sep ) value , sig = signed value . rsplit ( sep , 1 ) if self . verify signature ( value , sig ) : return value raise Bad Signature ( 'Signature %r does not match' % sig , payload = value )", "predictions": ["method to add signature for a loading method ."], "references": ["unsigns the given string ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4307, "code": "def sign ( self , value ) : value = want bytes ( value ) timestamp = base64 encode ( int to bytes ( self . get timestamp ( ) ) ) sep = want bytes ( self . sep ) value = value + sep + timestamp return value + sep + self . get signature ( value )", "predictions": ["inserts part of this . at the specified value ."], "references": ["signs the given string and also attaches a time information ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 4308, "code": "def all dirs ( base path ) : for root , dirs , files in os . walk ( base path , followlinks = True ) : for dir in dirs : yield os . path . relpath ( os . path . join ( root , dir ) , base path )", "predictions": ["yields all the paths in this directory and all subdirectories of the given path ."], "references": ["return all dirs in base_path relative to base_path"], "bleu": 0.09103526405546068, "rouge_l": 0.18401206636500753}
{"id": 4309, "code": "def install scripts ( distributions ) : try : from setuptools . command import easy install import pkg resources except Import Error : raise Runtime Error ( \"'wheel install scripts' needs setuptools.\" ) for dist in distributions : pkg resources dist = pkg resources . get distribution ( dist ) install = wheel . paths . get install command ( dist ) command = easy install . easy install ( install . distribution ) command . args = [ 'wheel' ] command . finalize options ( ) command . install egg scripts ( pkg resources dist )", "predictions": ["install one or more setuptools scripts ."], "references": ["regenerate the entry_points console_scripts for the named distribution ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4310, "code": "def get node ( self , ID ) : node = super ( Graph , self ) . get node ( ID ) if node is not None : return node for graph in self . all graphs : for each node in graph . nodes : if each node . ID == ID : return each node else : return None", "predictions": ["returns all edges of the given node as a node with the specified c{nodeid} ."], "references": ["returns a node given an id or none if no such node exists ."], "bleu": 0.13380161378318955, "rouge_l": 0.27758816837315126}
{"id": 4311, "code": "def directed changed ( self , new ) : if new : conn = \"->\" else : conn = \"--\" for edge in [ e for g in self . all graphs for e in g . edges ] : edge . conn = conn", "predictions": ["this method is called to determine if the directed directed job is changed ."], "references": ["sets the connection string for all edges ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 4312, "code": "def on edges ( self , object , name , old , new ) : if name == \"edges items\" : edges = new . added elif name == \"edges\" : edges = new else : edges = [ ] all nodes = [ n for g in self . all graphs for n in g . nodes ] for each edge in edges : if each edge . tail node not in all nodes : object . nodes . append ( each edge . tail node ) if each edge . head node not in all nodes : object . nodes . append ( each edge . head node ) each edge . nodes = all nodes", "predictions": ["this is called to re - do not have to re - call this method ."], "references": ["handles the list of edges for any graph changing ."], "bleu": 0.07692375026049747, "rouge_l": 0.08026315789473684}
{"id": 4313, "code": "def component changed ( self , old , new ) : canvas = self . canvas if old is not None : canvas . remove ( old ) if new is not None : canvas . add ( new )", "predictions": ["we only want to remove the component at the end of the list ."], "references": ["handles the component being changed ."], "bleu": 0.1250076305588977, "rouge_l": 0.323321554770318}
{"id": 4314, "code": "def diagram canvas changed ( self , new ) : logger . debug ( \"Diagram canvas changed!\" ) canvas = self . diagram canvas for tool in self . tools : if canvas is not None : print \"Adding tool: %s\" % tool canvas . tools . append ( tool ( canvas ) )", "predictions": ["we only want to update the diagram at the end of the list ."], "references": ["handles the diagram canvas being set"], "bleu": 0.11633270842295028, "rouge_l": 0.21554770318021202}
{"id": 4315, "code": "def clear canvas ( self ) : logger . debug ( \"Clearing the diagram canvas!\" ) old canvas = self . diagram canvas new canvas = Canvas ( ) new canvas . copy traits ( old canvas , [ \"bgcolor\" , \"draw axes\" ] ) self . diagram canvas = new canvas self . viewport . component = new canvas self . viewport . request redraw ( ) return", "predictions": ["we only want the diagram at the end of the list ."], "references": ["removes all components from the canvas"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 4316, "code": "def domain model changed for diagram ( self , obj , name , old , new ) : if old is not None : self . unmap model ( old ) if new is not None : self . map model ( new )", "predictions": ["event to no more checks ."], "references": ["handles the domain model changing"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4317, "code": "def map model ( self , new ) : logger . debug ( \"Mapping the domain model!\" ) dot = Dot ( ) self . diagram . clear canvas ( ) for node mapping in self . nodes : ct = node mapping . containment trait logger . debug ( \"Mapping elements contained by the '%s' trait\" % ct ) if hasattr ( new , ct ) : elements = getattr ( new , ct ) logger . debug ( \"%d element(s) found\" % len ( elements ) ) for element in elements : pydot node = Node ( str ( id ( element ) ) ) dot attrs = node mapping . dot node if dot attrs is not None : self . style node ( pydot node , dot attrs ) dot . add node ( pydot node ) new . on trait change ( self . map element , ct + \" items\" ) logger . debug ( \"Retrieving xdot data and forming pydot graph!\" ) xdot = graph from dot data ( dot . create ( self . program , \"xdot\" ) ) parser = X Dot Parser ( ) for node in xdot . get node list ( ) : diagram node = parser . parse node ( node ) logger . debug ( \"Parsed node [%s] and received diagram node [%s]\" % ( node , diagram node ) ) if diagram node is not None : for node mapping in self . nodes : ct = node mapping . containment trait for element in getattr ( new , ct ) : if str ( id ( element ) ) == diagram node . dot node . get name ( ) : logger . debug ( \"Referencing element [%s] from diagram node [%s]\" % ( element , diagram node ) ) diagram node . element = element break if isinstance ( diagram node . element , node mapping . element ) : for tool in node mapping . tools : logger . debug ( \"Adding tool [%s] to diagram node [%s]\" % ( tool , diagram node ) ) diagram node . tools . append ( tool ( diagram node ) ) else : if diagram node . element is None : logger . warning ( \"Diagram node not referenced to element\" ) self . diagram . diagram canvas . add ( diagram node ) del parser", "predictions": ["map all diagram to ( ."], "references": ["maps a domain model to the diagram"], "bleu": 0.20693220168471366, "rouge_l": 0.1517412935323383}
{"id": 4318, "code": "def unmap model ( self , old ) : for node mapping in self . nodes : ct = node mapping . containment trait if hasattr ( old , ct ) : old elements = getattr ( old , ct ) for old element in old elements : old . on trait change ( self . map element , ct + \" items\" , remove = True )", "predictions": ["unmap the neurons of the model ."], "references": ["removes listeners from a domain model"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4319, "code": "def map element ( self , obj , name , event ) : canvas = self . diagram . diagram canvas parser = X Dot Parser ( ) for element in event . added : logger . debug ( \"Mapping new element [%s] to diagram node\" % element ) for node mapping in self . nodes : ct = name [ : - 6 ] #strip ' items' if node mapping . containment trait == ct : dot attrs = node mapping . dot node dot = Dot ( ) graph node = Node ( str ( id ( element ) ) ) self . style node ( graph node , dot attrs ) dot . add node ( graph node ) xdot = graph from dot data ( dot . create ( self . program , \"xdot\" ) ) diagram nodes = parser . parse nodes ( xdot ) #.get node list()) for dn in diagram nodes : if dn is not None : dn . element = element for tool in node mapping . tools : dn . tools . append ( tool ( dn ) ) canvas . add ( dn ) canvas . request redraw ( ) for element in event . removed : logger . debug ( \"Unmapping element [%s] from diagram\" % element ) for component in canvas . components : if element == component . element : canvas . remove ( component ) canvas . request redraw ( ) break", "predictions": ["maps all the diagram to the given graph ."], "references": ["handles mapping elements to diagram components"], "bleu": 0.15619699684601276, "rouge_l": 0.13832199546485258}
{"id": 4320, "code": "def parse xdot data ( self , data ) : parser = self . parser if data : return parser . parse String ( data ) else : return [ ]", "predictions": ["user wants to get the agent string from a utf - 8 text ."], "references": ["parses xdot data and returns the associated components ."], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 4321, "code": "def proc font ( self , tokens ) : size = int ( tokens [ \"s\" ] ) self . pen . font = \"%s %d\" % ( tokens [ \"b\" ] , size ) return [ ]", "predictions": ["gets this indicating that the url is contained in the current url ."], "references": ["sets the font ."], "bleu": 0.10571070857151538, "rouge_l": 0.2601279317697228}
{"id": 4322, "code": "def proc ellipse ( self , tokens , filled ) : component = Ellipse ( pen = self . pen , x origin = tokens [ \"x0\" ] , y origin = tokens [ \"y0\" ] , e width = tokens [ \"w\" ] , e height = tokens [ \"h\" ] , filled = filled ) return component", "predictions": ["return the ) of this octagon ."], "references": ["returns the components of an ellipse ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 4323, "code": "def proc polygon ( self , tokens , filled ) : pts = [ ( p [ \"x\" ] , p [ \"y\" ] ) for p in tokens [ \"points\" ] ] component = Polygon ( pen = self . pen , points = pts , filled = filled ) return component", "predictions": ["given a polygon instance , finds the polygon that was created by this polygon ."], "references": ["returns the components of a polygon ."], "bleu": 0.1361294711534851, "rouge_l": 0.291866028708134}
{"id": 4324, "code": "def proc polyline ( self , tokens ) : pts = [ ( p [ \"x\" ] , p [ \"y\" ] ) for p in tokens [ \"points\" ] ] component = Polyline ( pen = self . pen , points = pts ) return component", "predictions": ["this method returns the register of this register at the given , with some arguments added to the , in the , as well as the register ."], "references": ["returns the components of a polyline ."], "bleu": 0.06471824245088333, "rouge_l": 0.25630252100840334}
{"id": 4325, "code": "def proc text ( self , tokens ) : component = Text ( pen = self . pen , text x = tokens [ \"x\" ] , text y = tokens [ \"y\" ] , justify = tokens [ \"j\" ] , text w = tokens [ \"w\" ] , text = tokens [ \"b\" ] ) return component", "predictions": ["returns the list of results from the given text ."], "references": ["returns text components ."], "bleu": 0.14991106946711685, "rouge_l": 0.4644670050761421}
{"id": 4326, "code": "def proc image ( self , tokens ) : print \"IMAGE:\" , tokens , tokens . as List ( ) , tokens . keys ( ) raise Not Implemented Error", "predictions": ["constructs an instance of the ( possibly empty flush flush flush flush flush flush flush flush the ( flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush flush to the end of the"], "references": ["returns the components of an image ."], "bleu": 0.028577262451992175, "rouge_l": 0.08122503328894808}
{"id": 4327, "code": "def render grid file ( context , f ) : f . seek ( 0 ) response = context . response if debug : response . headers [ 'Grid-ID' ] = str ( f . id ) log . debug ( \"Serving Grid FS file.\" , extra = dict ( identifier = str ( f . id ) , filename = f . filename , length = f . length , mimetype = f . content type ) ) response . conditional response = True response . accept ranges = 'bytes' response . content type = f . content type response . content length = f . length response . content md5 = response . etag = f . md5 response . last modified = f . metadata . get ( 'modified' , None ) response . content disposition = 'attachment; filename=' + f . name if context . request . if range . match response ( response ) : response . body file = f else : response . app iter = iter ( f ) return True", "predictions": ["render the contents of this block into the given template ."], "references": ["allow direct use of gridout gridfs file wrappers as endpoint responses ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 4328, "code": "def save ( self , obj ) : fd = None try : fd = open ( self . dot file . absolute path , \"wb\" ) obj . save dot ( fd ) finally : if fd is not None : fd . close ( ) return", "predictions": ["parse the object in data format ."], "references": ["save to file ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4329, "code": "def load ( self ) : fd = None try : obj = parse dot file ( self . dot file . absolute path ) finally : if fd is not None : fd . close ( ) return obj", "predictions": ["is called on each object from the file or file descriptor ."], "references": ["load the file ."], "bleu": 0.14694106251955755, "rouge_l": 0.4121621621621622}
{"id": 4330, "code": "def is in ( self , point x , point y ) : x = self . x origin y = self . y origin a = self . e width b = self . e height #/2 return ( ( point x - x ) ** 2 / ( a ** 2 ) ) + ( ( point y - y ) ** 2 / ( b ** 2 ) ) < 1.0", "predictions": ["returns true if this depends on the ( ."], "references": ["test if the point is within this ellipse"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 4331, "code": "def draw bounds ( self , gc ) : dx , dy = self . bounds x , y = self . position gc . rect ( x , y , dx , dy ) gc . stroke path ( )", "predictions": ["visit an bounds newline ."], "references": ["draws the component bounds for testing purposes"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4332, "code": "def perform ( self , event ) : wizard = New Dot Graph Wizard ( parent = self . window . control , window = self . window , title = \"New Graph\" ) if wizard . open ( ) == OK : wizard . finished = True", "predictions": ["creates a new instance of ( ."], "references": ["perform the action ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4333, "code": "def start ( self , context ) : if debug : log . info ( \"Connecting SQL Alchemy database layer.\" , extra = dict ( uri = redact uri ( self . uri ) , config = self . config , alias = self . alias , ) ) engine = self . engine = create engine ( self . uri , * * self . config ) self . Session = scoped session ( sessionmaker ( bind = engine ) ) engine . connect ( ) . close ( ) context . db [ self . alias ] = engine", "predictions": ["create a new lookup object ."], "references": ["construct the sqlalchemy engine and session factory ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4334, "code": "def parse dot code fired ( self ) : parser = Godot Data Parser ( ) graph = parser . parse dot data ( self . dot code ) if graph is not None : self . model = graph", "predictions": ["parses the signature description of a single token ."], "references": ["parses the dot_code string and replaces the existing model ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 4335, "code": "def new model ( self , info ) : if info . initialized : retval = confirm ( parent = info . ui . control , message = \"Replace existing graph?\" , title = \"New Graph\" , default = YES ) if retval == YES : self . model = Graph ( )", "predictions": ["creates a new ( object ."], "references": ["handles the new graph action ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 4336, "code": "def open file ( self , info ) : if not info . initialized : return dlg = File Dialog ( action = \"open\" , wildcard = \"Graphviz Files (*.dot, *.xdot, *.txt)|\" \"*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|\" \"All Files (*.*)|*.*|\" ) if dlg . open ( ) == OK : parser = Godot Data Parser ( ) model = parser . parse dot file ( dlg . path ) if model is not None : self . model = model else : print \"error parsing: %s\" % dlg . path self . save file = dlg . path del dlg", "predictions": ["opens the parser dialog for writing the given file ."], "references": ["handles the open action ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 4337, "code": "def save ( self , info ) : save file = self . save file if not isfile ( save file ) : self . save as ( info ) else : fd = None try : fd = open ( save file , \"wb\" ) dot code = str ( self . model ) fd . write ( dot code ) finally : if fd is not None : fd . close ( )", "predictions": ["saves the current model object to the specified file ."], "references": ["handles saving the current model to the last file ."], "bleu": 0.31702331385234306, "rouge_l": 0.7000000000000001}
{"id": 4338, "code": "def save as ( self , info ) : if not info . initialized : return dlg = File Dialog ( action = \"save as\" , wildcard = \"Graphviz Files (*.dot, *.xdot, *.txt)|\" \"*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|\" \"All Files (*.*)|*.*|\" ) if dlg . open ( ) == OK : fd = None try : fd = open ( dlg . path , \"wb\" ) dot code = str ( self . model ) fd . write ( dot code ) self . save file = dlg . path except : error ( parent = info . ui . control , title = \"Save Error\" , message = \"An error was encountered when saving\\nto %s\" % self . file ) finally : if fd is not None : fd . close ( ) del dlg", "predictions": ["saves the logged in this cgroup object to the workspace ."], "references": ["handles saving the current model to file ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 4339, "code": "def configure graph ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = \"live\" , view = attr view )", "predictions": ["creates and sets the ( probably loaded : encode the ( : . : . : . : . : . : . : . : . : . : . : . . . self . . self . attr self . attr self . attr self . ."], "references": ["handles display of the graph dot traits ."], "bleu": 0.026594139297659906, "rouge_l": 0.07932379713914176}
{"id": 4340, "code": "def configure nodes ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = \"live\" , view = nodes view )", "predictions": ["this method creates and sets the state of the database ."], "references": ["handles display of the nodes editor ."], "bleu": 0.16108992769687397, "rouge_l": 0.3472485768500949}
{"id": 4341, "code": "def configure edges ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = \"live\" , view = edges view )", "predictions": ["sets up the scripts ."], "references": ["handles display of the edges editor ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4342, "code": "def about godot ( self , info ) : if info . initialized : self . edit traits ( parent = info . ui . control , kind = \"livemodal\" , view = about view )", "predictions": ["creates new get lock object ."], "references": ["handles displaying a view about godot ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4343, "code": "def add node ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is None : return I Ds = [ v . ID for v in graph . nodes ] node = Node ( ID = make unique name ( \"node\" , I Ds ) ) graph . nodes . append ( node ) retval = node . edit traits ( parent = info . ui . control , kind = \"livemodal\" ) if not retval . result : graph . nodes . remove ( node )", "predictions": ["creates a new graph ."], "references": ["handles adding a node to the graph ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 4344, "code": "def add edge ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is None : return n nodes = len ( graph . nodes ) I Ds = [ v . ID for v in graph . nodes ] if n nodes == 0 : tail node = Node ( ID = make unique name ( \"node\" , I Ds ) ) head name = make unique name ( \"node\" , I Ds + [ tail node . ID ] ) head node = Node ( ID = head name ) elif n nodes == 1 : tail node = graph . nodes [ 0 ] head node = Node ( ID = make unique name ( \"node\" , I Ds ) ) else : tail node = graph . nodes [ 0 ] head node = graph . nodes [ 1 ] edge = Edge ( tail node , head node , nodes = graph . nodes ) retval = edge . edit traits ( parent = info . ui . control , kind = \"livemodal\" ) if retval . result : graph . edges . append ( edge )", "predictions": ["creates and prepares graph for data ."], "references": ["handles adding an edge to the graph ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4345, "code": "def add subgraph ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is not None : subgraph = Subgraph ( ) #root=graph, parent=graph) retval = subgraph . edit traits ( parent = info . ui . control , kind = \"livemodal\" ) if retval . result : graph . subgraphs . append ( subgraph )", "predictions": ["component is a is a is a is added to the content of the is a is a is added to the changed is the changed is added to the changed is the changed is added to the content of the list ."], "references": ["handles adding a subgraph to the main graph ."], "bleu": 0.04185635735742377, "rouge_l": 0.17441029306647607}
{"id": 4346, "code": "def add cluster ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is not None : cluster = Cluster ( ) #root=graph, parent=graph) retval = cluster . edit traits ( parent = info . ui . control , kind = \"livemodal\" ) if retval . result : graph . clusters . append ( cluster )", "predictions": ["diagram a canvas to the stopping ."], "references": ["handles adding a cluster to the main graph ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 4347, "code": "def godot options ( self , info ) : if info . initialized : self . edit traits ( parent = info . ui . control , kind = \"livemodal\" , view = \"options view\" )", "predictions": ["create a new ( object ."], "references": ["handles display of the options menu ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4348, "code": "def configure dot code ( self , info ) : if not info . initialized : return self . dot code = str ( self . model ) retval = self . edit traits ( parent = info . ui . control , kind = \"livemodal\" , view = \"dot code view\" )", "predictions": ["override for , to provide the model at the bottom of this class ."], "references": ["handles display of the dot code in a text editor ."], "bleu": 0.10511846841633776, "rouge_l": 0.16353887399463804}
{"id": 4349, "code": "def on exit ( self , info ) : if self . prompt on exit : retval = confirm ( parent = info . ui . control , message = \"Exit Godot?\" , title = \"Confirm exit\" , default = YES ) if retval == YES : self . on close ( info ) else : self . on close ( info )", "predictions": ["this method is called when the entity dies ."], "references": ["handles the user attempting to exit godot ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4350, "code": "def save to file like ( self , flo , format = None , * * kwargs ) : format = self . format if format is None else format save = getattr ( self , \"save %s\" % format , None ) if save is None : raise Value Error ( \"Unknown format '%s'.\" % format ) save ( flo , * * kwargs )", "predictions": ["saves the current ( or pickled nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes ."], "references": ["save the object to a given file like object in the given format ."], "bleu": 0.05606668411195419, "rouge_l": 0.11050724637681159}
{"id": 4351, "code": "def save to file ( self , filename , format = None , * * kwargs ) : if format is None : format = format from extension ( filename ) with file ( filename , 'wb' ) as fp : self . save to file like ( fp , format , * * kwargs )", "predictions": ["map the current image to the specified ( as an address ."], "references": ["save the object to file given by filename ."], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 4352, "code": "def add node ( self , node or ID , * * kwds ) : if not isinstance ( node or ID , Node ) : node ID = str ( node or ID ) if node ID in self . nodes : node = self . nodes [ self . nodes . index ( node ID ) ] else : if self . default node is not None : node = self . default node . clone traits ( copy = \"deep\" ) node . ID = node ID else : node = Node ( node ID ) self . nodes . append ( node ) else : node = node or ID if node in self . nodes : node = self . nodes [ self . nodes . index ( node or ID ) ] else : self . nodes . append ( node ) node . set ( * * kwds ) return node", "predictions": ["this method implements the contract of this class ."], "references": ["adds a node to the graph ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4353, "code": "def delete node ( self , node or ID ) : if isinstance ( node or ID , Node ) : node = node or ID else : node = self . get node ( node or ID ) if node is None : raise Value Error ( \"Node %s does not exists\" % node or ID ) self . nodes . remove ( node )", "predictions": ["builds the tree node for the given node and all child nodes ."], "references": ["removes a node from the graph ."], "bleu": 0.1135935489027116, "rouge_l": 0.31715771230502604}
{"id": 4354, "code": "def get node ( self , ID ) : for node in self . nodes : if node . ID == str ( ID ) : return node return None", "predictions": ["this method returns the nodes contained in this node as a real node ."], "references": ["returns the node with the given id or none ."], "bleu": 0.13217947626377288, "rouge_l": 0.34366197183098596}
{"id": 4355, "code": "def delete edge ( self , tail node or ID , head node or ID ) : if isinstance ( tail node or ID , Node ) : tail node = tail node or ID else : tail node = self . get node ( tail node or ID ) if isinstance ( head node or ID , Node ) : head node = head node or ID else : head node = self . get node ( head node or ID ) if ( tail node is None ) or ( head node is None ) : return None for i , edge in enumerate ( self . edges ) : if ( edge . tail node == tail node ) and ( edge . head node == head node ) : edge = self . edges . pop ( i ) return edge return None", "predictions": ["this method removes the given edge and returns the corresponding row with the given name ."], "references": ["removes an edge from the graph . returns the deleted edge or none ."], "bleu": 0.12512236921161915, "rouge_l": 0.3373893805309734}
{"id": 4356, "code": "def add edge ( self , tail node or ID , head node or ID , * * kwds ) : tail node = self . add node ( tail node or ID ) head node = self . add node ( head node or ID ) if \"directed\" in self . trait names ( ) : directed = self . directed else : directed = False if self . default edge is not None : edge = self . default edge . clone traits ( copy = \"deep\" ) edge . tail node = tail node edge . head node = head node edge . conn = \"->\" if directed else \"--\" edge . set ( * * kwds ) else : edge = Edge ( tail node , head node , directed , * * kwds ) if \"strict\" in self . trait names ( ) : if not self . strict : self . edges . append ( edge ) else : self . edges . append ( edge ) else : self . edges . append ( edge )", "predictions": ["add an tail edge ."], "references": ["adds an edge to the graph ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 4357, "code": "def add subgraph ( self , subgraph or ID ) : if not isinstance ( subgraph or ID , ( godot . subgraph . Subgraph , godot . cluster . Cluster ) ) : subgraph ID = str ( subgraph or ID ) if subgraph or ID . startswith ( \"cluster\" ) : subgraph = godot . cluster . Cluster ( ID = subgraph ID ) else : subgraph = godot . subgraph . Subgraph ( ID = subgraph ID ) else : subgraph = subgraph or ID subgraph . default node = self . default node subgraph . default edge = self . default edge if isinstance ( subgraph , godot . subgraph . Subgraph ) : self . subgraphs . append ( subgraph ) elif isinstance ( subgraph , godot . cluster . Cluster ) : self . clusters . append ( subgraph ) else : raise return subgraph", "predictions": ["add a subgraph to the ( ."], "references": ["adds a subgraph to the graph ."], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 4358, "code": "def program changed ( self , new ) : progs = self . progs if not progs . has key ( prog ) : logger . warning ( 'Graph Viz\\'s executable \"%s\" not found' % prog ) if not os . path . exists ( progs [ prog ] ) or not os . path . isfile ( progs [ prog ] ) : logger . warning ( \"Graph Viz's executable '%s' is not a \" \"file or doesn't exist\" % progs [ prog ] )", "predictions": ["run a program for program ."], "references": ["handles the graphviz layout program selection changing ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4359, "code": "def set node lists ( self , new ) : for edge in self . edges : edge . nodes = self . nodes", "predictions": ["sets the nodes of this class to be used as a result ."], "references": ["maintains each edge s list of available nodes ."], "bleu": 0.1135935489027116, "rouge_l": 0.18798151001540828}
{"id": 4360, "code": "def parse dot file ( filename ) : parser = Godot Data Parser ( ) graph = parser . parse dot file ( filename ) del parser return graph", "predictions": ["parse a dot file ."], "references": ["parses a dot file and returns a godot graph ."], "bleu": 0.19765609300943976, "rouge_l": 0.5030927835051546}
{"id": 4361, "code": "def parse dot file ( self , file or filename ) : if isinstance ( file or filename , basestring ) : file = None try : file = open ( file or filename , \"rb\" ) data = file . read ( ) except : print \"Could not open %s.\" % file or filename return None finally : if file is not None : file . close ( ) else : file = file or filename data = file . read ( ) return self . parse dot data ( data )", "predictions": ["returns the dot product of this file . this method should be called on each line ."], "references": ["returns a graph given a file or a filename ."], "bleu": 0.0859076483566362, "rouge_l": 0.2331210191082802}
{"id": 4362, "code": "def build top graph ( self , tokens ) : strict = tokens [ 0 ] == 'strict' graphtype = tokens [ 1 ] directed = graphtype == 'digraph' graphname = tokens [ 2 ] graph = Graph ( ID = graphname , strict = strict , directed = directed ) self . graph = self . build graph ( graph , tokens [ 3 ] )", "predictions": ["construct the graph representing the top - level graph ."], "references": ["build a godot graph instance from parsed data ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 4363, "code": "def build graph ( self , graph , tokens ) : subgraph = None for element in tokens : cmd = element [ 0 ] if cmd == ADD NODE : cmd , nodename , opts = element graph . add node ( nodename , * * opts ) elif cmd == ADD EDGE : cmd , src , dest , opts = element srcport = destport = \"\" if isinstance ( src , tuple ) : srcport = src [ 1 ] src = src [ 0 ] if isinstance ( dest , tuple ) : destport = dest [ 1 ] dest = dest [ 0 ] graph . add edge ( src , dest , tailport = srcport , headport = destport , * * opts ) elif cmd in [ ADD GRAPH TO NODE EDGE , ADD GRAPH TO GRAPH EDGE , ADD NODE TO GRAPH EDGE ] : cmd , src , dest , opts = element srcport = destport = \"\" if isinstance ( src , tuple ) : srcport = src [ 1 ] if isinstance ( dest , tuple ) : destport = dest [ 1 ] if not ( cmd == ADD NODE TO GRAPH EDGE ) : if cmd == ADD GRAPH TO NODE EDGE : src = subgraph else : src = prev subgraph dest = subgraph else : dest = subgraph src is graph = isinstance ( src , ( Subgraph , Cluster ) ) dst is graph = isinstance ( dst , ( Subgraph , Cluster ) ) if src is graph : src nodes = src . nodes else : src nodes = [ src ] if dst is graph : dst nodes = dst . nodes else : dst nodes = [ dst ] for src node in src nodes : for dst node in dst nodes : graph . add edge ( from node = src node , to node = dst node , tailport = srcport , headport = destport , * * kwds ) elif cmd == SET GRAPH ATTR : graph . set ( * * element [ 1 ] ) elif cmd == SET DEF NODE ATTR : graph . default node . set ( * * element [ 1 ] ) elif cmd == SET DEF EDGE ATTR : graph . default edge . set ( * * element [ 1 ] ) elif cmd == SET DEF GRAPH ATTR : graph . default graph . set ( * * element [ 1 ] ) elif cmd == ADD SUBGRAPH : cmd , name , elements = element if subgraph : prev subgraph = subgraph if name . startswith ( \"cluster\" ) : cluster = Cluster ( ID = name ) cluster = self . build graph ( cluster , elements ) graph . add cluster ( cluster ) else : subgraph = Subgraph ( ID = name ) subgraph = self . build graph ( subgraph , elements ) graph . add subgraph ( subgraph ) return graph", "predictions": ["build the graph to build the graph to a graph ."], "references": ["builds a godot graph ."], "bleu": 0.16108992769687397, "rouge_l": 0.40219780219780216}
{"id": 4364, "code": "def format duration ( seconds ) : units , divider = get time units and multiplier ( seconds ) seconds *= divider return \"%.3f %s\" % ( seconds , units )", "predictions": ["helper method to get a formatted list of seconds ."], "references": ["formats a number of seconds using the best units ."], "bleu": 0.18850319022747347, "rouge_l": 0.4}
{"id": 4365, "code": "def on path ( self , new ) : self . name = basename ( new ) self . graph = self . editor input . load ( )", "predictions": ["instantiate a new object ."], "references": ["handle the file path changing ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4366, "code": "def get children ( self , object ) : children = [ ] children . extend ( object . subgraphs ) children . extend ( object . clusters ) children . extend ( object . nodes ) children . extend ( object . edges ) return children", "predictions": ["returns the child node of this child ."], "references": ["gets the object s children ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 4367, "code": "def append child ( self , object , child ) : if isinstance ( child , Subgraph ) : object . subgraphs . append ( child ) elif isinstance ( child , Cluster ) : object . clusters . append ( child ) elif isinstance ( child , Node ) : object . nodes . append ( child ) elif isinstance ( child , Edge ) : object . edges . append ( child ) else : pass", "predictions": ["append a child to the end of the list ."], "references": ["appends a child to the object s children ."], "bleu": 0.3672056269893592, "rouge_l": 0.5313588850174217}
{"id": 4368, "code": "def insert child ( self , object , index , child ) : if isinstance ( child , Subgraph ) : object . subgraphs . insert ( index , child ) elif isinstance ( child , Cluster ) : object . clusters . insert ( index , child ) elif isinstance ( child , Node ) : object . nodes . insert ( index , child ) elif isinstance ( child , Edge ) : object . edges . insert ( index , child ) else : pass", "predictions": ["insert the passed child into this set ."], "references": ["inserts a child into the object s children ."], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 4369, "code": "def delete child ( self , object , index ) : if isinstance ( child , Subgraph ) : object . subgraphs . pop ( index ) elif isinstance ( child , Cluster ) : object . clusters . pop ( index ) elif isinstance ( child , Node ) : object . nodes . pop ( index ) elif isinstance ( child , Edge ) : object . edges . pop ( index ) else : pass", "predictions": ["deletes the node or all child elements of the given object ."], "references": ["deletes a child at a specified index from the object s children ."], "bleu": 0.12581061487677794, "rouge_l": 0.39713541666666663}
{"id": 4370, "code": "def get label ( self , object ) : label = self . label if label [ : 1 ] == '=' : return label [ 1 : ] label = xgetattr ( object , label , '' ) if self . formatter is None : return label return self . formatter ( object , label )", "predictions": ["this method returns the . for this object ."], "references": ["gets the label to display for a specified object ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 4371, "code": "def set label ( self , object , label ) : label name = self . label if label name [ : 1 ] != '=' : xsetattr ( object , label name , label )", "predictions": ["override this method to get default status_id for a label ."], "references": ["sets the label for a specified object ."], "bleu": 0.17033186037639278, "rouge_l": 0.32504440497335696}
{"id": 4372, "code": "def add listeners ( self ) : object = self . value canvas = self . factory . canvas if canvas is not None : for name in canvas . node children : object . on trait change ( self . nodes replaced , name ) object . on trait change ( self . nodes changed , name + \" items\" ) for name in canvas . edge children : object . on trait change ( self . edges replaced , name ) object . on trait change ( self . edges changed , name + \" items\" ) else : raise Value Error ( \"Graph canvas not set for graph editor.\" )", "predictions": ["add a new graph ."], "references": ["adds the event listeners for a specified object ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4373, "code": "def nodes replaced ( self , object , name , old , new ) : self . delete nodes ( old ) self . add nodes ( new )", "predictions": ["override this to delete some existing objects ."], "references": ["handles a list of nodes being set ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4374, "code": "def nodes changed ( self , object , name , undefined , event ) : self . delete nodes ( event . removed ) self . add nodes ( event . added )", "predictions": ["creates a new instance of the underlying nodes ."], "references": ["handles addition and removal of nodes ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 4375, "code": "def delete nodes ( self , features ) : graph = self . graph if graph is not None : for feature in features : graph . delete node ( id ( feature ) ) graph . arrange all ( )", "predictions": ["removes the specified graph from the database ."], "references": ["removes the node corresponding to each item in features ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 4376, "code": "def edges replaced ( self , object , name , old , new ) : self . delete edges ( old ) self . add edges ( new )", "predictions": ["older versions of the object ."], "references": ["handles a list of edges being set ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4377, "code": "def edges changed ( self , object , name , undefined , event ) : self . delete edges ( event . removed ) self . add edges ( event . added )", "predictions": ["creates a new edges ."], "references": ["handles addition and removal of edges ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 4378, "code": "def delete edges ( self , features ) : graph = self . graph if graph is not None : for feature in features : for graph edge in self . factory . edges : if feature . class in graph edge . edge for : tail feature = getattr ( feature , graph edge . tail name ) head feature = getattr ( feature , graph edge . head name ) graph . delete edge ( id ( tail feature ) , id ( head feature ) ) graph . arrange all ( )", "predictions": ["deletes all edges of a graph ."], "references": ["removes the node corresponding to each item in features ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4379, "code": "def arrange all ( self ) : import godot . dot data parser import godot . graph graph = godot . graph . Graph ( ID = \"g\" , directed = True ) self . conn = \"->\" graph . edges . append ( self ) xdot data = graph . create ( format = \"xdot\" ) parser = godot . dot data parser . Godot Data Parser ( ) ndata = xdot data . replace ( '\\\\\\n' , '' ) tokens = parser . dotparser . parse String ( ndata ) [ 0 ] for element in tokens [ 3 ] : cmd = element [ 0 ] if cmd == \"add edge\" : cmd , src , dest , opts = element self . set ( * * opts )", "predictions": ["arrange command with regular expression to . ."], "references": ["arrange the components of the node using graphviz ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 4380, "code": "def parse xdot directive ( self , name , new ) : parser = Xdot Attr Parser ( ) components = parser . parse xdot data ( new ) x1 = min ( [ c . x for c in components ] ) y1 = min ( [ c . y for c in components ] ) print \"X1/Y1:\" , name , x1 , y1 for c in components : if isinstance ( c , Ellipse ) : component . x origin -= x1 component . y origin -= y1 elif isinstance ( c , ( Polygon , B Spline ) ) : print \"Points:\" , c . points c . points = [ ( t [ 0 ] - x1 , t [ 1 ] - y1 ) for t in c . points ] print \"Points:\" , c . points elif isinstance ( c , Text ) : c . text x , c . text y = c . x - x1 , c . y - y1 container = Container ( auto size = True , position = [ x1 , y1 ] , bgcolor = \"yellow\" ) container . add ( * components ) if name == \" draw \" : self . drawing = container elif name == \" hdraw \" : self . arrowhead drawing = container else : raise", "predictions": ["draws the xdot ( . ) of this directive ."], "references": ["handles parsing xdot drawing directives ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 4381, "code": "def on drawing ( self , object , name , old , new ) : attrs = [ \"drawing\" , \"arrowhead drawing\" ] others = [ getattr ( self , a ) for a in attrs if ( a != name ) and ( getattr ( self , a ) is not None ) ] x , y = self . component . position print \"POS:\" , x , y , self . component . position abs x = [ d . x + x for d in others ] abs y = [ d . y + y for d in others ] print \"ABS:\" , abs x , abs y x1 = min ( abs x + [ new . x ] ) y1 = min ( abs y + [ new . y ] ) print \"DRAW:\" , new . position new . position = [ new . x - x1 , new . y - y1 ] print \"DRAW:\" , new . position if old is not None : self . component . remove ( old ) if new is not None : self . component . add ( new ) print \"POS NEW:\" , self . component . position self . component . position = [ x1 , y1 ] print \"POS NEW:\" , self . component . position self . component . request redraw ( ) print \"POS NEW:\" , self . component . position", "predictions": ["to handle all drawing callbacks ."], "references": ["handles the containers of drawing components being set ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4382, "code": "def node factory ( * * row factory kw ) : if \" table editor \" in row factory kw : graph = row factory kw [ \" table editor \" ] . object ID = make unique name ( \"n\" , [ node . ID for node in graph . nodes ] ) del row factory kw [ \" table editor \" ] return godot . node . Node ( ID ) else : return godot . node . Node ( uuid . uuid4 ( ) . hex [ : 6 ] )", "predictions": ["constructs the whole node from a node"], "references": ["give new nodes a unique id ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4383, "code": "def edge factory ( * * row factory kw ) : if \" table editor \" in row factory kw : table editor = row factory kw [ \" table editor \" ] graph = table editor . object ID = make unique name ( \"node\" , [ node . ID for node in graph . nodes ] ) n nodes = len ( graph . nodes ) I Ds = [ v . ID for v in graph . nodes ] if n nodes == 0 : tail node = godot . Node ( ID = make unique name ( \"n\" , I Ds ) ) head node = godot . Node ( ID = make unique name ( \"n\" , I Ds ) ) elif n nodes == 1 : tail node = graph . nodes [ 0 ] head node = godot . Node ( ID = make unique name ( \"n\" , I Ds ) ) else : tail node = graph . nodes [ 0 ] head node = graph . nodes [ 1 ] return godot . edge . Edge ( tail node , head node , nodes = graph . nodes ) else : return None", "predictions": ["constructs the edge factory that contains all nodes of the given edge and builds them as well as a whole edge ."], "references": ["give new edges a unique id ."], "bleu": 0.0612957497932821, "rouge_l": 0.1521197007481297}
{"id": 4384, "code": "def start ( self , context ) : self . config [ 'alias' ] = self . alias safe config = dict ( self . config ) del safe config [ 'host' ] log . info ( \"Connecting Mongo Engine database layer.\" , extra = dict ( uri = redact uri ( self . config [ 'host' ] ) , config = self . config , ) ) self . connection = connect ( * * self . config )", "predictions": ["starts up the sender process ."], "references": ["initialize the database connection ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 4385, "code": "def prepare ( self , context ) : context . db [ self . alias ] = Mongo Engine Proxy ( self . connection )", "predictions": ["delete currently logged-in is done in this database ."], "references": ["attach this connection s default database to the context using our alias ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 4386, "code": "def arrange all ( self ) : import godot . dot data parser import godot . graph graph = godot . graph . Graph ( ID = \"g\" ) graph . add node ( self ) print \"GRAPH DOT:\\n\" , str ( graph ) xdot data = graph . create ( format = \"xdot\" ) print \"XDOT DATA:\\n\" , xdot data parser = godot . dot data parser . Godot Data Parser ( ) flat data = xdot data . replace ( '\\\\\\n' , '' ) tokens = parser . dotparser . parse String ( flat data ) [ 0 ] for element in tokens [ 3 ] : print \"TOK:\" , element cmd = element [ 0 ] if cmd == 'add node' : cmd , nodename , opts = element assert nodename == self . ID print \"OPTIONS:\" , opts self . set ( * * opts )", "predictions": ["get command line for ( ."], "references": ["arrange the components of the node using graphviz ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4387, "code": "def parse xdot drawing directive ( self , new ) : components = Xdot Attr Parser ( ) . parse xdot data ( new ) max x = max ( [ c . bounds [ 0 ] for c in components ] + [ 1 ] ) max y = max ( [ c . bounds [ 1 ] for c in components ] + [ 1 ] ) pos x = min ( [ c . x for c in components ] ) pos y = min ( [ c . y for c in components ] ) move to origin ( components ) container = Container ( auto size = True , position = [ pos x - self . pos [ 0 ] , pos y - self . pos [ 1 ] ] , bgcolor = \"blue\" ) container . add ( * components ) self . drawing = container", "predictions": ["delete ( possibly inherit ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) = == == == == == == 1 ."], "references": ["parses the drawing directive updating the node components ."], "bleu": 0.03259631698411773, "rouge_l": 0.04883907125700561}
{"id": 4388, "code": "def drawing changed ( self , old , new ) : if old is not None : self . component . remove ( old ) if new is not None : self . component . add ( new ) w , h = self . component . bounds self . component . position = [ self . pos [ 0 ] - ( w / 2 ) , self . pos [ 1 ] - ( h / 2 ) ] self . component . request redraw ( )", "predictions": ["utility function to add a = edge around the area of the = edge . note : this removes the = = null , but not the amount of the = = 0 , so we are only edge ."], "references": ["handles the container of drawing components changing ."], "bleu": 0.035817229106400346, "rouge_l": 0.14208074534161488}
{"id": 4389, "code": "def on position change ( self , new ) : w , h = self . component . bounds self . pos = tuple ( [ new [ 0 ] + ( w / 2 ) , new [ 1 ] + ( h / 2 ) ] )", "predictions": ["passes the scroll method to the tools ."], "references": ["handles the poition of the component changing ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 4390, "code": "def pos changed ( self , new ) : w , h = self . component . bounds self . component . position = [ new [ 0 ] - ( w / 2 ) , new [ 1 ] - ( h / 2 ) ] self . component . request redraw ( )", "predictions": ["feel out the has changed just the has changed ."], "references": ["handles the graphviz position attribute changing ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 4391, "code": "def highlight info ( ctx , style ) : click . secho ( \"The following styles are available to choose from:\" , fg = \"green\" ) click . echo ( list ( pygments . styles . get all styles ( ) ) ) click . echo ( ) click . secho ( f'The following CSS for the \"{style}\" style can be customized:' , fg = \"green\" ) click . echo ( pygments . formatters . Html Formatter ( style = style ) . get style defs ( ) )", "predictions": ["creates a new , empty , and all the ( ."], "references": ["outputs the css which can be customized for highlighted code"], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 4392, "code": "def draw mainlayer ( self , gc , view bounds = None , mode = \"default\" ) : gc . save state ( ) try : if len ( self . points ) >= 2 : gc . set fill color ( self . pen . fill color ) gc . set stroke color ( self . pen . color ) gc . set line width ( self . pen . line width ) gc . begin path ( ) gc . lines ( self . points ) gc . close path ( ) if self . filled : gc . draw path ( self . inside rule ) else : gc . stroke path ( ) finally : gc . restore state ( )", "predictions": ["parse the 3d rule rule and save the results ."], "references": ["draws a closed polygon"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4393, "code": "def is in ( self , point x , point y ) : point array = array ( ( ( point x , point y ) , ) ) vertices = array ( self . points ) winding = self . inside rule == \"winding\" result = points in polygon ( point array , vertices , winding ) return result [ 0 ]", "predictions": ["returns true if this area is in the area of the chart ."], "references": ["test if a point is within this polygonal region"], "bleu": 0.1135935489027116, "rouge_l": 0.18798151001540828}
{"id": 4394, "code": "def draw mainlayer ( self , gc , view bounds = None , mode = \"default\" ) : if not self . points : return gc . save state ( ) try : gc . set fill color ( self . pen . fill color ) gc . set line width ( self . pen . line width ) gc . set stroke color ( self . pen . color ) gc . begin path ( ) start x , start y = self . points [ 0 ] gc . move to ( start x , start y ) for triple in nsplit ( self . points [ 1 : ] , 3 ) : x1 , y1 = triple [ 0 ] x2 , y2 = triple [ 1 ] end x , end y = triple [ 2 ] gc . curve to ( x1 , y1 , x2 , y2 , end x , end y ) gc . move to ( end x , end y ) gc . stroke path ( ) finally : gc . restore state ( )", "predictions": ["build a simple curve for the given tokens ."], "references": ["draws the bezier component"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4395, "code": "def connect ( self , context ) : if debug : log . info ( \"Connecting \" + self . engine . partition ( ':' ) [ 0 ] + \" database layer.\" , extra = dict ( uri = redact uri ( self . uri , self . protect ) , config = self . config , alias = self . alias , ) ) self . connection = context . db [ self . alias ] = self . connector ( self . uri , * * self . config )", "predictions": ["build a dest object for a database ."], "references": ["initialize the database connection ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4396, "code": "def handle event ( self , event , * args , * * kw ) : for engine in self . engines . values ( ) : if hasattr ( engine , event ) : getattr ( engine , event ) ( * args , * * kw )", "predictions": ["handles a single duration of this object ."], "references": ["broadcast an event to the database connections registered ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4397, "code": "def get full page url ( self , page number , scheme = None ) : args = dict ( request . view args , external = True , ) if scheme is not None : args [ ' scheme' ] = scheme if page number != 1 : args [ 'page' ] = page number return url for ( request . endpoint , * * args )", "predictions": ["this will on the load and return"], "references": ["get the full external url for this page optinally with the passed in url scheme"], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 4398, "code": "def render prev next links ( self , scheme = None ) : output = '' if self . has prev : output += '<link rel=\"prev\" href=\"{}\" />\\n' . format ( self . get full page url ( self . prev , scheme = scheme ) ) if self . has next : output += '<link rel=\"next\" href=\"{}\" />\\n' . format ( self . get full page url ( self . next , scheme = scheme ) ) return Markup ( output )", "predictions": ["renders the ( at the given : passes the result of this . ."], "references": ["render the rel = prev and rel = next links to a markup object for injection into a template"], "bleu": 0.061846674640588774, "rouge_l": 0.058994197292069624}
{"id": 4399, "code": "def render seo links ( self , scheme = None ) : out = self . render prev next links ( scheme = scheme ) if self . total pages == 1 : out += self . render canonical link ( scheme = scheme ) return out", "predictions": ["renders this link as a , ."], "references": ["render the rel = canonical rel = prev and rel = next links to a markup object for injection into a template"], "bleu": 0.02179210314610696, "rouge_l": 0.06308169596690795}
{"id": 4400, "code": "def content type matches ( candidate , pattern ) : def wildcard compare ( type spec , type pattern ) : return type pattern == '*' or type spec == type pattern return ( wildcard compare ( candidate . content type , pattern . content type ) and wildcard compare ( candidate . content subtype , pattern . content subtype ) )", "predictions": ["compares two match groups ."], "references": ["is candidate an exact match or sub - type of pattern ?"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 4401, "code": "def ensure dir ( path ) : try : log . info ( 'Ensuring directory exists: %s' % path ) os . makedirs ( path ) except OS Error : if not os . path . isdir ( path ) : raise", "predictions": ["create directories on disk . < p > if the directories are created , this method does not exist ."], "references": ["ensure that a needed directory exists creating it if it doesn t"], "bleu": 0.06108557268562171, "rouge_l": 0.06545064377682404}
{"id": 4402, "code": "def list dataset uris ( cls , base uri , config path ) : storage account name = generous parse uri ( base uri ) . netloc blobservice = get blob service ( storage account name , config path ) containers = blobservice . list containers ( include metadata = True ) uri list = [ ] for c in containers : admin metadata = c . metadata uri = cls . generate uri ( admin metadata [ 'name' ] , admin metadata [ 'uuid' ] , base uri ) uri list . append ( uri ) return uri list", "predictions": ["returns a list of all containers for this label ."], "references": ["return list containing uris with base uri ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 4403, "code": "def list overlay names ( self ) : overlay names = [ ] for blob in self . blobservice . list blobs ( self . uuid , prefix = self . overlays key prefix ) : overlay file = blob . name . rsplit ( '/' , 1 ) [ - 1 ] overlay name , ext = overlay file . split ( '.' ) overlay names . append ( overlay name ) return overlay names", "predictions": ["returns a set of label instances that can be searched for all label ."], "references": ["return list of overlay names ."], "bleu": 0.09782375748961449, "rouge_l": 0.21554770318021202}
{"id": 4404, "code": "def iter item handles ( self ) : blob generator = self . blobservice . list blobs ( self . uuid , include = 'metadata' ) for blob in blob generator : if 'type' in blob . metadata : if blob . metadata [ 'type' ] == 'item' : handle = blob . metadata [ 'relpath' ] yield handle", "predictions": ["generator for all ( ( ( ( ( possibly created : : 1 : 2 : manager . 2 : 3 : iterate over all ( 2 : 00 . 6 : 00 : 00 : 00 : 00 : 00 : 00 : 00 . . . . ."], "references": ["return iterator over item handles ."], "bleu": 0.026594139297659906, "rouge_l": 0.08321964529331514}
{"id": 4405, "code": "def luhn check ( card number ) : sum = 0 num digits = len ( card number ) oddeven = num digits & 1 for count in range ( 0 , num digits ) : digit = int ( card number [ count ] ) if not ( ( count & 1 ) ^ oddeven ) : digit *= 2 if digit > 9 : digit -= 9 sum += digit return ( sum % 10 ) == 0", "predictions": ["checks whether the nodes of the self - supplied , according to the nodes of the nodes . < p > note : this method provides more than the nodes of the nodes that are calculated as a string ."], "references": ["checks to make sure that the card passes a luhn mod - 10 checksum"], "bleu": 0.04119566303102713, "rouge_l": 0.1622340425531915}
{"id": 4406, "code": "def remove namespaces ( root ) : for elem in root . getiterator ( ) : if not hasattr ( elem . tag , 'find' ) : continue i = elem . tag . find ( '}' ) if i >= 0 : elem . tag = elem . tag [ i + 1 : ] objectify . deannotate ( root , cleanup namespaces = True )", "predictions": ["nodes through the given self - tree ."], "references": ["call this on an lxml . etree document to remove all namespaces"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 4407, "code": "def merge ( self , new dict ) : actions = new dict . pop ( \"actions\" ) for action in actions : self . add action ( action ) self . dict . update ( new dict )", "predictions": ["delete a single action ."], "references": ["merges a dictionary into the rule object ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4408, "code": "def execute actions ( self , cwd ) : self . execute globals ( cwd ) for action in self . actions : logger . info ( \"executing {}\" . format ( action ) ) p = subprocess . Popen ( action , shell = True , cwd = cwd ) p . wait ( )", "predictions": ["edges of the specified ."], "references": ["iterates over the actions and executes them in order ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 4409, "code": "def add details ( self , message ) : msg = message try : from flask import request url = request . url method = request . method endpoint = request . endpoint form dict = dict ( request . form ) for key in form dict : if key . lower ( ) in error reporting obscured fields : form dict [ key ] = '******' elif len ( form dict [ key ] ) == 1 : form dict [ key ] = form dict [ key ] [ 0 ] form = pprint . pformat ( form dict ) . replace ( '\\n' , '\\n          ' ) msg = '%s\\n Request:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n' % ( msg , url , method , endpoint , form ) except Exception : traceback . print exc ( ) try : from flask import session from flask . json import JSON Encoder session str = json . dumps ( dict ( * * session ) , indent = 2 , cls = JSON Encoder ) msg = '%s\\n Session:\\n\\n%s\\n' % ( msg , session str ) except Exception : traceback . print exc ( ) return msg", "predictions": ["used to send messages to a message ."], "references": ["add extra details to the message . separate so that it can be overridden"], "bleu": 0.10712878727413526, "rouge_l": 0.25994318181818177}
{"id": 4410, "code": "def get context ( self , value ) : context = super ( Rendition Aware Struct Block , self ) . get context ( value ) context [ 'image rendition' ] = self . rendition . image rendition or 'original' return context", "predictions": ["delete the class specified by this request ."], "references": ["ensure image_rendition is added to the global context ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 4411, "code": "def set ( self , k , v ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) r = requests . put ( url , data = str ( v ) ) if r . status code != 200 or r . json ( ) is not True : raise KV Store Error ( 'PUT returned {}' . format ( r . status code ) )", "predictions": ["sets the edges for this operation ."], "references": ["add or update a key value pair to the database"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4412, "code": "def get ( self , k , wait = False , wait index = False , timeout = '5m' ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } if wait : params [ 'index' ] = wait index params [ 'wait' ] = timeout r = requests . get ( url , params = params ) if r . status code == 404 : raise Key Does Not Exist ( \"Key \" + k + \" does not exist\" ) if r . status code != 200 : raise KV Store Error ( 'GET returned {}' . format ( r . status code ) ) try : return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) except Type Error as e : return \"\"", "predictions": ["look up the if there is a , then use the if entity ."], "references": ["get the value of a given key"], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 4413, "code": "def recurse ( self , k , wait = False , wait index = None , timeout = '5m' ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } params [ 'recurse' ] = 'true' if wait : params [ 'wait' ] = timeout if not wait index : params [ 'index' ] = self . index ( k , recursive = True ) else : params [ 'index' ] = wait index r = requests . get ( url , params = params ) if r . status code == 404 : raise Key Does Not Exist ( \"Key \" + k + \" does not exist\" ) if r . status code != 200 : raise KV Store Error ( 'GET returned {}' . format ( r . status code ) ) entries = { } for e in r . json ( ) : if e [ 'Value' ] : entries [ e [ 'Key' ] ] = base64 . b64decode ( e [ 'Value' ] ) else : entries [ e [ 'Key' ] ] = '' return entries", "predictions": ["tries to on the resource ."], "references": ["recursively get the tree below the given key"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4414, "code": "def delete ( self , k , recursive = False ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } if recursive : params [ 'recurse' ] = '' r = requests . delete ( url , params = params ) if r . status code != 200 : raise KV Store Error ( 'DELETE returned {}' . format ( r . status code ) )", "predictions": ["deletes an existing object ."], "references": ["delete a given key or recursively delete the tree below it"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 4415, "code": "def add months ( months , timestamp = datetime . datetime . utcnow ( ) ) : month = timestamp . month new month = month + months years = 0 while new month < 1 : new month += 12 years -= 1 while new month > 12 : new month -= 12 years += 1 year = timestamp . year + years try : return datetime . datetime ( year , new month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) except Value Error : if months > 0 : new month += 1 if new month > 12 : new month -= 12 year += 1 return datetime . datetime ( year , new month , 1 , timestamp . hour , timestamp . minute , timestamp . second ) else : new day = calendar . monthrange ( year , new month ) [ 1 ] return datetime . datetime ( year , new month , new day , timestamp . hour , timestamp . minute , timestamp . second )", "predictions": ["edge passed in row . the netscape solutions are added to the other row ."], "references": ["add a number of months to a timestamp"], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 4416, "code": "def add months to date ( months , date ) : month = date . month new month = month + months years = 0 while new month < 1 : new month += 12 years -= 1 while new month > 12 : new month -= 12 years += 1 year = date . year + years try : return datetime . date ( year , new month , date . day ) except Value Error : if months > 0 : new month += 1 if new month > 12 : new month -= 12 year += 1 return datetime . datetime ( year , new month , 1 ) else : new day = calendar . monthrange ( year , new month ) [ 1 ] return datetime . datetime ( year , new month , new day )", "predictions": ["add months from month to another year"], "references": ["add a number of months to a date"], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 4417, "code": "def is christmas period ( ) : now = datetime . date . today ( ) if now . month != 12 : return False if now . day < 15 : return False if now . day > 27 : return False return True", "predictions": ["returns true if the period is in the group ."], "references": ["is this the christmas period?"], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 4418, "code": "def from csv ( self , label column = 'labels' ) : df = pd . read csv ( self . path , header = 0 ) X = df . loc [ : , df . columns != label column ] . to dict ( 'records' ) X = map dict list ( X , if func = lambda k , v : v and math . isfinite ( v ) ) y = list ( df [ label column ] . values ) return X , y", "predictions": ["create dask from sqlite3 interface"], "references": ["read dataset from csv ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 4419, "code": "def from json ( self ) : with gzip . open ( '%s.gz' % self . path , 'rt' ) if self . gz else open ( self . path ) as file : return list ( map ( list , zip ( * json . load ( file ) ) ) ) [ : : - 1 ]", "predictions": ["assemble a json file from this json object"], "references": ["reads dataset from json ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4420, "code": "def restore data ( self , data dict ) : session [ self . base key ] = data dict self . data dict = session [ self . base key ]", "predictions": ["restores a previously saved commit to the previous state ."], "references": ["restore the data dict - update the flask session and this object"], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 4421, "code": "def verify block ( self , block type , block ) : if block type in self . registry : raise Already Registered ( \"A block has already been registered to the {} `block type` \" \"in the registry. Either unregister that block before trying \" \"to register this block under a different `block type`\" . format ( block type ) ) if not isinstance ( block , Block ) : raise Invalid Block ( \"The block you tried register to {} is invalid. Only \" \"instances of `wagtail.wagtailcore.blocks.Block` may be \" \"registered with the the block registry.\" . format ( block type ) )", "predictions": ["this is called by the disabled class to verify that the signature is valid ."], "references": ["verifies a block prior to registration ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 4422, "code": "def register block ( self , block type , block ) : self . verify block ( block type , block ) self . registry [ block type ] = block", "predictions": ["registers a new block block to be injected ."], "references": ["registers block to block_type in the registry ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 4423, "code": "def connect ( self ) : SCOPES = 'https://www.googleapis.com/auth/drive' store = file . Storage ( 'drive credentials.json' ) creds = store . get ( ) if not creds or creds . invalid : try : flow = client . flow from clientsecrets ( 'client secret.json' , SCOPES ) except Invalid Client Secrets Error : log . error ( 'ERROR: Could not find client secret.json in current directory, please obtain it from the API console.' ) return creds = tools . run flow ( flow , store ) self . connection = build ( 'drive' , 'v3' , http = creds . authorize ( Http ( ) ) ) response = self . connection . files ( ) . list ( q = \"name='Music' and mime Type='application/vnd.google-apps.folder' and trashed=false\" ) . execute ( ) try : folder id = response . get ( 'files' , [ ] ) [ 0 ] [ 'id' ] except Index Error : log . warning ( 'Music folder is missing. Creating it.' ) folder metadata = { 'name' : 'Music' , 'mime Type' : 'application/vnd.google-apps.folder' } folder = self . connection . files ( ) . create ( body = folder metadata , fields = 'id' ) . execute ( )", "predictions": ["connect all automatic messages and connect them"], "references": ["creates connection to the google drive api sets the connection attribute to make requests and creates the music folder if it doesn t exist ."], "bleu": 0.014196227135119473, "rouge_l": 0.05674418604651163}
{"id": 4424, "code": "def connect ( self ) : if self . music folder is None : music folder = os . path . join ( os . path . expanduser ( '~' ) , 'Music' ) if not os . path . exists ( music folder ) : os . makedirs ( music folder ) self . music folder = music folder", "predictions": ["connect to the database ."], "references": ["initializes the connection attribute with the path to the user home folder s music folder and creates it if it doesn t exist ."], "bleu": 0.008637296739954455, "rouge_l": 0.18503538928210314}
{"id": 4425, "code": "def write sky params to file ( self ) : inp file = self . sky file + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) f = open ( inp file , 'w' ) f . write ( 'verbose= ' + str ( self . verbose ) + '\\n' ) f . write ( 'band count= ' + str ( self . num bands ) + '\\n' ) f . write ( 'band centres data= ' ) f . write ( \",\" . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\\n' ) f . write ( 'partition= ' + self . partition + '\\n' ) f . write ( 'vn= ' + str ( self . vn ) + '\\n' ) f . write ( 'hn= ' + str ( self . hn ) + '\\n' ) f . write ( 'rdif= ' + str ( self . sky r dif ) + '\\n' ) f . write ( 'theta points= ' ) f . write ( \",\" . join ( [ str ( theta ) for theta in self . theta points ] ) + '\\n' ) f . write ( 'type= ' + self . sky type + '\\n' ) f . write ( 'azimuth= ' + str ( self . sky azimuth ) + '\\n' ) f . write ( 'zenith= ' + str ( self . sky zenith ) + '\\n' ) f . write ( 'sky save fp= ' + inp file . strip ( ' params.txt' ) + '\\n' ) f . write ( 'sky image save fp= ' + self . sky file + '.ppm' + '\\n' ) f . write ( 'sky image size= 256' + '\\n' ) if self . sky type == 'hlideal' : f . write ( 'C= ' + str ( self . sky c ) + '\\n' ) f . write ( 'rdif= ' + str ( self . sky r dif ) + '\\n' ) f . flush ( ) f . close ( )", "predictions": ["writes a 'sky state file ."], "references": ["writes the params to file that skytool_free needs to generate the sky radiance distribution ."], "bleu": 0.05861428254383573, "rouge_l": 0.26521739130434785}
{"id": 4426, "code": "def write surf params to file ( self ) : inp file = self . water surface file + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) if self . surf state == 'flat' : lg . info ( 'Surface Type is :: flat' ) f = open ( inp file , 'w' ) f . write ( 'verbose= ' + str ( self . verbose ) + '\\n' ) f . write ( 'band count= ' + str ( self . num bands ) + '\\n' ) f . write ( 'band centres data= ' ) f . write ( \",\" . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\\n' ) f . write ( 'partition= ' + self . partition + '\\n' ) f . write ( 'vn= ' + str ( self . vn ) + '\\n' ) f . write ( 'hn= ' + str ( self . hn ) + '\\n' ) f . write ( 'theta points= ' ) f . write ( \",\" . join ( [ str ( theta ) for theta in self . theta points ] ) + '\\n' ) f . write ( 'type= ' + self . iface type + '\\n' ) f . write ( 'refrac index 0= ' + str ( self . iface 0 ri ) + '\\n' ) f . write ( 'refrac index 1= ' + str ( self . iface 1 ri ) + '\\n' ) f . write ( 'wind speed= ' + str ( self . wind speed ) + '\\n' ) f . write ( 'wind direc= ' + str ( self . wind direc ) + '\\n' ) f . write ( 'crosswind vertices= ' + str ( self . crosswind vertices ) + '\\n' ) f . write ( 'upwind vertices= ' + str ( self . upwind vertices ) + '\\n' ) f . write ( 'surface size= ' + str ( self . surface size ) + '\\n' ) f . write ( 'surface radius=' + str ( self . surface radius ) + '\\n' ) f . write ( 'target size= ' + str ( self . target size ) + '\\n' ) f . write ( 'rays per quad= ' + str ( self . rays per quad ) + '\\n' ) f . write ( 'surface count= ' + str ( self . surface count ) + '\\n' ) f . write ( 'azimuthally average= ' + str ( self . azimuthally average ) + '\\n' ) f . write ( 'surface save fp= ' + inp file . strip ( ' params.txt' ) + '\\n' ) f . flush ( ) f . close ( )", "predictions": ["writes a set of operation vertices to the current machine ."], "references": ["write the params to file that surftool_free needs to generate the surface facets"], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 4427, "code": "def write phase params to file ( self ) : inp file = os . path . join ( os . path . join ( self . input path , 'phase files' ) , self . phase function file ) + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) if self . iop type == 'isotropic' or 'isotropic integ' or 'petzold' or 'pure water ' : lg . info ( 'Iop type is :: ' + self . iop type ) f = open ( inp file , 'w' ) f . write ( 'verbose = ' + str ( self . verbose ) + '\\n' ) f . write ( 'band count = ' + str ( self . num bands ) + '\\n' ) f . write ( 'band centres data = ' ) f . write ( \",\" . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\\n' ) f . write ( 'partition = ' + self . partition + '\\n' ) f . write ( 'vn = ' + str ( self . vn ) + '\\n' ) f . write ( 'hn = ' + str ( self . hn ) + '\\n' ) f . write ( 'theta points = ' ) f . write ( \",\" . join ( [ str ( theta ) for theta in self . theta points ] ) + '\\n' ) f . write ( 'type = ' + self . iop type + '\\n' ) f . write ( 'phase func save fp = ' + inp file . strip ( ' params.txt' ) + '\\n' ) f . flush ( ) f . close ( )", "predictions": ["writes this class so that it can be written to the map ."], "references": ["write the params to file that surftool_free needs to generate the surface facets"], "bleu": 0.1135935489027116, "rouge_l": 0.23076923076923084}
{"id": 4428, "code": "def update filenames ( self ) : self . sky file = os . path . abspath ( os . path . join ( os . path . join ( self . input path , 'sky files' ) , 'sky ' + self . sky state + ' z' + str ( self . sky zenith ) + ' a' + str ( self . sky azimuth ) + ' ' + str ( self . num bands ) + ' ' + self . ds code ) )", "predictions": ["updates this object ' s azimuth list with some stuff ."], "references": ["does nothing currently . may not need this method"], "bleu": 0.12605968092174913, "rouge_l": 0.1018363939899833}
{"id": 4429, "code": "def string to float list ( string var ) : try : return [ float ( s ) for s in string var . strip ( '[' ) . strip ( ']' ) . split ( ', ' ) ] except : return [ float ( s ) for s in string var . strip ( '[' ) . strip ( ']' ) . split ( ',' ) ]", "predictions": ["transforms string to list ."], "references": ["pull comma separated string values out of a text file and converts them to float list"], "bleu": 0.035974230453556216, "rouge_l": 0.26105563480741795}
{"id": 4430, "code": "def set handler ( self , signals , handler = signal . SIG DFL ) : for sig in signals : self . log . debug ( \"Creating handler for signal: {0}\" . format ( sig ) ) signal . signal ( sig , handler )", "predictions": ["create a handler for all signals ."], "references": ["takes a list of signals and sets a handler for them"], "bleu": 0.20643565894052812, "rouge_l": 0.32049036777583184}
{"id": 4431, "code": "def pseudo handler ( self , signum , frame ) : self . log . warn ( \"Received sigal {0} but system is already busy processing a previous signal, current frame: {1}\" . format ( signum , str ( frame ) ) )", "predictions": ["creates a new instance of this class ."], "references": ["pseudo handler placeholder while signal is beind processed"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4432, "code": "def default handler ( self , signum , frame ) : self . log . debug ( \"Signal handler called with signal: {0}\" . format ( signum ) ) if signum in self . restart signals : self . set handler ( self . handled signals , self . pseudo handler ) self . cleanup ( ) os . execl ( 'python' , 'python' , * sys . argv ) elif signum in self . abort signals : self . abort ( signum ) elif signum in self . pause signals : self . pause ( signum ) elif signum in self . resume signals : self . resume ( signum ) elif signum in self . status signals : self . status ( signum ) elif signum in self . error signals : self . log . error ( 'Signal handler received error signal from an external process, aborting' ) self . abort ( signum ) else : self . log . error ( \"Unhandled signal received: {0}\" . format ( signum ) ) raise", "predictions": ["creates a default handler for the given frame ."], "references": ["default handler a generic callback method for signal processing"], "bleu": 0.21105340631872635, "rouge_l": 0.3333333333333333}
{"id": 4433, "code": "def status ( self , signum ) : self . log . debug ( 'Signal handler got status signal' ) new status callbacks = [ ] for status call in self . status callbacks : try : self . log . debug ( \"Calling {0}({1},{2})\" . format ( status call [ 'function' ] . name , status call [ 'args' ] , status call [ 'kwargs' ] ) ) except Attribute Error : self . log . debug ( \"Calling unbound function/method {0}\" . format ( str ( status call ) ) ) apply ( status call [ 'function' ] , status call [ 'args' ] , status call [ 'kwargs' ] ) if status call [ 'persistent' ] : new status callbacks . append ( status call ) self . status callbacks = new status callbacks self . resume ( signum )", "predictions": ["status for ( . do not call this method ."], "references": ["run all status tasks then run all tasks in the resume queue"], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 4434, "code": "def unreg event ( self , event list , event ) : try : self . log . debug ( \"Removing event {0}({1},{2})\" . format ( event [ 'function' ] . name , event [ 'args' ] , event [ 'kwargs' ] ) ) except Attribute Error : self . log . debug ( \"Removing event {0}\" . format ( str ( event ) ) ) try : event list . remove ( event ) except Value Error : try : self . log . warn ( \"Unable to remove event {0}({1},{2}) , not found in list: {3}\" . format ( event [ 'function' ] . name , event [ 'args' ] , event [ 'kwargs' ] , event list ) ) except Attribute Error : self . log . debug ( \"Unable to remove event {0}\" . format ( str ( event ) ) ) raise Key Error ( 'Unable to unregister the specified event from the signals specified' )", "predictions": ["removes an event from the list ."], "references": ["tries to remove a registered event without triggering it"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4435, "code": "def sig from partial ( self , inst ) : self . pargl = list ( inst . pargl ) self . kargl = list ( inst . kargl ) self . def argv = inst . def argv . copy ( ) self . var pargs = inst . var pargs self . var kargs = inst . var kargs", "predictions": ["create a tokenqueue for a partial type"], "references": ["extract function signature from an existing partial instance ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4436, "code": "def vlq2int ( data ) : byte = ord ( data . read ( 1 ) ) value = byte & 0x7F shift = 1 while byte & 0x80 != 0 : byte = ord ( data . read ( 1 ) ) value = ( ( byte & 0x7F ) << shift * 7 ) | value shift += 1 return value", "predictions": ["vlq2int an unsigned byte ."], "references": ["read one vlq - encoded integer value from an input data stream ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 4437, "code": "def parse header ( self ) : header = Ordered Dict ( ) user data header = self . archive . header [ 'user data header' ] [ 'content' ] if re . search ( r'Star Craft II replay' , user data header ) : user data header = String IO . String IO ( user data header ) user data header . seek ( 30 ) header . update ( read table ( user data header , [ 'release flag' , 'major version' , 'minor version' , 'maintenance version' , 'build number' , 'unknown' , 'unknown' , 'duration' ] ) ) header [ 'version' ] = '%s.%s.%s.%s' % ( header [ 'major version' ] , header [ 'minor version' ] , header [ 'maintenance version' ] , header [ 'build number' ] ) if not header [ 'release flag' ] : header [ 'version' ] += ' (dev)' header [ 'duration' ] /= 16 else : raise Value Error ( \"The given file is not a Star Craft II replay.\" ) return header", "predictions": ["parse json response and add it to the client ."], "references": ["parse the user data header portion of the replay ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 4438, "code": "def get duration ( self , seconds ) : duration = \"\" minutes , seconds = divmod ( seconds , 60 ) if minutes >= 60 : hours , minutes = divmod ( minutes , 60 ) duration = \"%sh \" % hours duration += \"%sm %ss\" % ( minutes , seconds ) return duration", "predictions": ["get the duration of the duration ."], "references": ["transform duration into a human - readable form ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4439, "code": "def print details ( self ) : print 'Map      ' , self . map print 'Duration ' , self . duration print 'Version  ' , self . version print 'Team  Player       Race       Color' print '-----------------------------------' for player in self . players : print '{team:<5} {name:12} {race:10} {color}' . format ( * * player )", "predictions": ["outputs the details of the player ."], "references": ["print a summary of the game details ."], "bleu": 0.240785655451027, "rouge_l": 0.3952483801295896}
{"id": 4440, "code": "def data ( self ) : self . batch name value = self . ui . batch name value . text ( ) self . saa values = self . ui . saa values . text ( ) self . sza values = self . ui . sza values . text ( ) self . p values = self . ui . p values . text ( ) self . x value = self . ui . x value . text ( ) self . y value = self . ui . y value . text ( ) self . g value = self . ui . g value . text ( ) self . s value = self . ui . s value . text ( ) self . z value = self . ui . z value . text ( ) self . wavelength values = self . ui . wavelength values . text ( ) self . verbose value = self . ui . verbose value . text ( ) self . phytoplankton path = self . ui . phyto path . text ( ) self . bottom path = self . ui . bottom path . text ( ) self . executive path = self . ui . exec path . text ( ) self . nb cpu = self . ui . nb cpu . current Text ( ) self . report parameter value = str ( self . ui . report parameter value . text ( ) )", "predictions": ["construct a ( object ."], "references": ["this function gets back data that the user typed ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4441, "code": "def search file result ( self ) : if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . result file = self . file dialog . get Open File Name ( caption = str ( \"Open Report File\" ) , directory = \"./outputs\" ) if not self . result file == '' : self . ui . show all curves . set Disabled ( False ) self . ui . show grid . set Disabled ( False ) self . data processing ( ) self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information ) self . authorized display = True", "predictions": ["searches for breaking for editing the locale ."], "references": ["this function once the file found display data s file and the graphic associated ."], "bleu": 0.0739821920909478, "rouge_l": 0.16486486486486487}
{"id": 4442, "code": "def write to file ( self ) : bt = Batch File ( self . batch name value , self . p values , self . x value , self . y value , self . g value , self . s value , self . z value , self . wavelength values , self . verbose value , self . phytoplankton path , self . bottom path , self . nb cpu , self . executive path , self . saa values , self . sza values , self . report parameter value ) bt . write batch to file ( str ( self . batch name value + \" batch.txt\" ) )", "predictions": ["writes out the user agent in ( to ( ."], "references": ["this function calls gui_batch . py with inputs values to write the batch file ."], "bleu": 0.0909256598621168, "rouge_l": 0.15443037974683543}
{"id": 4443, "code": "def data processing ( self ) : the file name = str ( self . result file ) the file = open ( the file name , 'r' ) lines = the file . readlines ( ) lines array = [ ] for line in lines : line = line . split ( ',' ) lines array . append ( line ) labels line = lines array [ 0 ] cell labels line = 0 flag = True try : while flag : if \"wave length (nm)\" in labels line [ cell labels line ] : index = labels line . index ( labels line [ cell labels line ] ) flag = False else : cell labels line += 1 except Index Error : raise sys . exit ( \"Warning : There is no value named 'wavelength' in the file used to plot curves. \" \"So, I can't separate data to plot curves and data about tests linking with these curves.\" ) self . information = [ ] data wavelength = [ ] self . num line = 0 for line in lines array : cell line = 0 self . information . append ( [ ] ) data wavelength . append ( [ ] ) while cell line < len ( line ) : if cell line < index : self . information [ self . num line ] . append ( line [ cell line ] ) elif cell line > index : data wavelength [ self . num line ] . append ( line [ cell line ] ) cell line += 1 self . num line += 1 line wavelength = 0 for row data wavelength in data wavelength : row data wavelength = [ float ( item . strip ( '\\n' ) . strip ( '\\\"' ) ) for item in row data wavelength ] data wavelength [ line wavelength ] = row data wavelength line wavelength += 1 self . wavelength = data wavelength [ 0 ] self . data wanted = data wavelength [ 1 : ] the file . close ( )", "predictions": ["input must be a model or a single blank line ."], "references": ["this function separates data from the file to display curves and will put them in the good arrays ."], "bleu": 0.055043096362025104, "rouge_l": 0.06360792492179353}
{"id": 4444, "code": "def display error message ( self ) : self . ui . error label . set Scaled Contents ( True ) self . ui . error text label . show ( ) self . ui . error text label . set Style Sheet ( 'color: red' )", "predictions": ["display an error message ."], "references": ["this function displays an error message when a wrong value is typed ."], "bleu": 0.10847596378846372, "rouge_l": 0.41146711635750427}
{"id": 4445, "code": "def hide error message ( self ) : self . ui . error label . set Scaled Contents ( False ) self . ui . error text label . hide ( )", "predictions": ["method to hide the error message ."], "references": ["this function hides the error message when all values are correct ."], "bleu": 0.17895451045590982, "rouge_l": 0.40197693574958815}
{"id": 4446, "code": "def run ( self ) : print ( 'Executing planarrad' ) if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . data ( ) self . check values ( ) if self . without error == False : self . display error message ( ) elif self . without error == True : self . is running = True self . hide error message ( ) self . write to file ( ) os . chdir ( './' ) self . progress bar ( ) this dir = os . path . dirname ( os . path . realpath ( file ) ) . rstrip ( 'gui/' ) batch file = os . path . join ( this dir , \"inputs/batch files/\" + str ( self . batch name value ) + \" batch.txt\" ) print ( batch file ) self . p = subprocess . Popen ( [ \"./planarrad.py -i \" + batch file ] , shell = True ) if self . ui . progress Bar . value ( ) == 100 : self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information )", "predictions": ["construct the progress object ."], "references": ["this function executes planarrad using the batch file ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4447, "code": "def cancel planarrad ( self ) : if ( self . is running == True ) & ( self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE ) : cancel = Qt Gui . Q Message Box . question ( self . ui . cancel , 'Cancel Planar Rad' , \"Are you sure to cancel ?\" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if cancel == Qt Gui . Q Message Box . Yes : self . is running = False os . kill ( self . p . pid , signal . SIGTERM ) print ( \"Necessary to check if cancel planarrad works well !\" ) self . ui . progress Bar . reset ( ) else : pass", "predictions": ["cancels all the recovery ."], "references": ["this function cancels planarrad ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4448, "code": "def quit ( self ) : if self . is running == True : warning planarrad running = Qt Gui . Q Message Box . warning ( self . ui . quit , 'Warning !' , \"Planar Rad is running. Stop it before quit !\" , Qt Gui . Q Message Box . Ok ) else : quit = Qt Gui . Q Message Box . question ( self . ui . quit , 'Quit Planar Rad' , \"Are you sure to quit ?\" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if quit == Qt Gui . Q Message Box . Yes : Qt Gui . q App . quit ( )", "predictions": ["add a new new ( to the daemon ."], "references": ["this function quits planarrad checking if planarrad is running before ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 4449, "code": "def open log file ( self ) : f = open ( os . path . expanduser ( '~/.planarradpy/log/libplanarradpy.log' ) ) self . ui Log . text Edit . set Plain Text ( str ( f . read ( ) ) ) self . log window . show ( )", "predictions": ["opens a log in the ( dialog ."], "references": ["the following opens the log file of planarrad ."], "bleu": 0.17795502018438056, "rouge_l": 0.34923664122137404}
{"id": 4450, "code": "def open documentation ( self ) : window = Window ( ) html = Qt Core . Q Url . from Local File ( os . path . join ( os . getcwd ( ) , './docs/ build/html/index.html' ) ) #open('./docs/ build/html/index.html').read() #window.show() window . view . load ( html ) window . show ( ) window . exec ( )", "predictions": ["creates the ( window"], "references": ["the following opens the documentation file ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4451, "code": "def prerequisite actions ( self ) : self . hide error message ( ) self . ui . show all curves . set Disabled ( True ) self . ui . sens . set Disabled ( True ) self . ui . show grid . set Disabled ( True ) pathname = os . path . dirname ( sys . argv [ 0 ] ) path = os . path . abspath ( pathname ) self . verbose value = self . ui . verbose value . set Text ( \"6\" ) self . report parameter value = self . ui . report parameter value . set Text ( \"Rrs\" ) self . ui . progress Bar . reset ( )", "predictions": ["show all the json json formatted event ."], "references": ["this function does all required actions at the beginning when we run the gui ."], "bleu": 0.07949903911132591, "rouge_l": 0.24729729729729732}
{"id": 4452, "code": "def click ( self , event ) : if event . button == 3 : if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . pos = Qt Gui . Q Cursor ( ) . pos ( ) self . graphic context menu ( self . pos )", "predictions": ["restore the graphic off the current ] ."], "references": ["this function intercepts the mouse s right click and its position ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 4453, "code": "def mouse move ( self , event ) : if ( self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE ) : self . pos X = event . xdata self . pos Y = event . ydata self . graphic target ( self . pos X , self . pos Y )", "predictions": ["block the current position so that it can be moved to the top of the stack ."], "references": ["the following gets back coordinates of the mouse on the canvas ."], "bleu": 0.11306082351602978, "rouge_l": 0.2847141190198366}
{"id": 4454, "code": "def graphic target ( self , x , y ) : if self . authorized display == True : try : self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information ) self . ui . mouse coordinate . set Text ( \"(%0.3f, %0.3f)\" % ( x , y ) ) except : pass", "predictions": ["displays the register to the specified block ."], "references": ["the following update labels about mouse coordinates ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4455, "code": "def sign ( self , privkey ) : if self . v : raise Invalid Signature ( \"already signed\" ) if privkey in ( 0 , '' , '\\x00' * 32 ) : raise Invalid Signature ( \"Zero privkey cannot sign\" ) rawhash = sha3 ( rlp . encode ( self , self . class . exclude ( [ 'v' , 'r' , 's' ] ) ) ) if len ( privkey ) == 64 : privkey = encode privkey ( privkey , 'bin' ) pk = Private Key ( privkey , raw = True ) signature = pk . ecdsa recoverable serialize ( pk . ecdsa sign recoverable ( rawhash , raw = True ) ) signature = signature [ 0 ] + chr ( signature [ 1 ] ) self . v = ord ( signature [ 64 ] ) + 27 self . r = big endian to int ( signature [ 0 : 32 ] ) self . s = big endian to int ( signature [ 32 : 64 ] ) self . sender = None return self", "predictions": ["this method performs a connect the the the the signature of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the"], "references": ["sign this with a private key"], "bleu": 0.026594139297659906, "rouge_l": 0.08321964529331514}
{"id": 4456, "code": "def hash ( self ) : if self . sender is None : raise Missing Signature Error ( ) class Hash Serializable ( rlp . Serializable ) : fields = [ ( field , sedes ) for field , sedes in self . fields if field not in ( 'v' , 'r' , 's' ) ] + [ ( ' sender' , binary ) ] sedes = None return sha3 ( rlp . encode ( self , Hash Serializable ) )", "predictions": ["returns a connect to a makedirs ."], "references": ["signatures are non deterministic"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4457, "code": "def check ( self ) : if not self . is valid : return True test = ( self . has quorum , self . has quorum possible , self . has noquorum ) assert 1 == len ( [ x for x in test if x is not None ] ) return True", "predictions": ["write out message on lg ."], "references": ["either invalid or one of quorum noquorum quorumpossible"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 4458, "code": "def validate votes ( self , validators H , validators prev H ) : assert self . sender def check ( lockset , validators ) : if not lockset . num eligible votes == len ( validators ) : raise Invalid Proposal Error ( 'lockset num eligible votes mismatch' ) for v in lockset : if v . sender not in validators : raise Invalid Proposal Error ( 'invalid signer' ) if self . round lockset : check ( self . round lockset , validators H ) check ( self . signing lockset , validators prev H ) return True", "predictions": ["check that the votes flag is registered . this function must be called before any other votes ."], "references": ["set of validators may change between heights"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 4459, "code": "def validate votes ( self , validators H ) : assert self . sender if not self . round lockset . num eligible votes == len ( validators H ) : raise Invalid Proposal Error ( 'round lockset num eligible votes mismatch' ) for v in self . round lockset : if v . sender not in validators H : raise Invalid Proposal Error ( 'invalid signer' )", "predictions": ["check if the phase is valid . this is only valid for sanity other types ."], "references": ["set of validators may change between heights"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 4460, "code": "def issue funds ( ctx , amount = 'uint256' , rtgs hash = 'bytes32' , returns = STATUS ) : ctx . accounts [ ctx . msg sender ] += amount ctx . issued amounts [ ctx . msg sender ] += amount ctx . Issuance ( ctx . msg sender , rtgs hash , amount ) return OK", "predictions": ["azimuth : the : : dispatch to a timestamp ."], "references": ["in the iou fungible the supply is set by issuer who issue funds ."], "bleu": 0.09351498865776114, "rouge_l": 0.16180371352785147}
{"id": 4461, "code": "def last lock ( self ) : rs = list ( self . rounds ) assert len ( rs ) < 2 or rs [ 0 ] > rs [ 1 ] for r in self . rounds : if self . rounds [ r ] . lock is not None : return self . rounds [ r ] . lock", "predictions": ["get the string representation of this instance . the returned list is backed by the fact that the iteration will be changed ."], "references": ["highest lock on height"], "bleu": 0.04449945957170705, "rouge_l": 0.0}
{"id": 4462, "code": "def last voted blockproposal ( self ) : for r in self . rounds : if isinstance ( self . rounds [ r ] . proposal , Block Proposal ) : assert isinstance ( self . rounds [ r ] . lock , Vote ) if self . rounds [ r ] . proposal . blockhash == self . rounds [ r ] . lock . blockhash : return self . rounds [ r ] . proposal", "predictions": ["returns set of set of log blocks ."], "references": ["the last block proposal node voted on"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4463, "code": "def last valid lockset ( self ) : for r in self . rounds : ls = self . rounds [ r ] . lockset if ls . is valid : return ls return None", "predictions": ["returns the pseudo list of ( signum signum signum signum . signum signum against this . signum . signum . signum . signum . signum . signum ."], "references": ["highest valid lockset on height"], "bleu": 0.03639374222382004, "rouge_l": 0.0}
{"id": 4464, "code": "def get timeout ( self ) : if self . timeout time is not None or self . proposal : return now = self . cm . chainservice . now round timeout = Consensus Manager . round timeout round timeout factor = Consensus Manager . round timeout factor delay = round timeout * round timeout factor ** self . round self . timeout time = now + delay return delay", "predictions": ["get the . handler ."], "references": ["setup a timeout for waiting for a proposal"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 4465, "code": "def on proposal ( self , proposal , proto ) : assert isinstance ( proto , HDC Protocol ) assert isinstance ( proposal , Proposal ) if proposal . height >= self . cm . height : assert proposal . lockset . is valid self . last active protocol = proto", "predictions": ["handler for subclasses that can handle a ( or passes : 1 : . : . : . : . : . : . = . new data = . new * . new ( : . : . : . : . = . new ( : . :"], "references": ["called to inform about synced peers"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 4466, "code": "def mk privkeys ( num ) : privkeys = [ ] assert num <= num colors for i in range ( num ) : j = 0 while True : k = sha3 ( str ( j ) ) a = privtoaddr ( k ) an = big endian to int ( a ) if an % num colors == i : break j += 1 privkeys . append ( k ) return privkeys", "predictions": ["create an array of debug words from a set of debug words ."], "references": ["make privkeys that support coloring see utils . cstr"], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 4467, "code": "def delay ( self , sender , receiver , packet , add delay = 0 ) : bw = min ( sender . ul bandwidth , receiver . dl bandwidth ) delay = sender . base latency + receiver . base latency delay += len ( packet ) / bw delay += add delay return delay", "predictions": ["generate a class sig . no terminal is added to the one that has already been added to the daemon queue ."], "references": ["bandwidths are inaccurate as we don t account for parallel transfers here"], "bleu": 0.04657469807170698, "rouge_l": 0.0}
{"id": 4468, "code": "def deliver ( self , sender , receiver , packet ) : to = Consensus Manager . round timeout assert to > 0 print \"in slow transport deliver\" super ( Slow Transport , self ) . deliver ( sender , receiver , packet , add delay = to )", "predictions": ["accept a clip that is managed by a specific byte = ( . the implementation uses the implementation to ensure that the implementation is not supported ."], "references": ["deliver on edge of timeout_window"], "bleu": 0.03776949794525175, "rouge_l": 0.0}
{"id": 4469, "code": "def chain nac proxy ( chain , sender , contract address , value = 0 ) : klass = registry [ contract address ] . im self assert issubclass ( klass , Native ABI Contract ) def mk method ( method ) : def m ( s , * args ) : data = abi encode args ( method , args ) block = chain . head candidate output = test call ( block , sender , contract address , data ) if output is not None : return abi decode return vals ( method , output ) return m class cproxy ( object ) : pass for m in klass . abi methods ( ) : setattr ( cproxy , m . func . func name , mk method ( m ) ) return cproxy ( )", "predictions": ["create a ( ( ( call . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["create an object which acts as a proxy for the contract on the chain"], "bleu": 0.026594139297659906, "rouge_l": 0.06955530216647662}
{"id": 4470, "code": "def address to native contract class ( self , address ) : assert isinstance ( address , bytes ) and len ( address ) == 20 assert self . is instance address ( address ) nca = self . native contract address prefix + address [ - 4 : ] return self . native contracts [ nca ]", "predictions": ["convert a method to an get location ."], "references": ["returns class . _on_msg_unsafe use x . im_self to get class"], "bleu": 0.13107175678306446, "rouge_l": 0.20469798657718125}
{"id": 4471, "code": "def update ( self , data ) : if data not in self . filter : self . filter . append ( data ) if len ( self . filter ) > self . max items : self . filter . pop ( 0 ) return True else : self . filter . append ( self . filter . pop ( 0 ) ) return False", "predictions": ["print list of words ."], "references": ["returns true if unknown"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4472, "code": "def on receive transactions ( self , proto , transactions ) : log . debug ( '----------------------------------' ) log . debug ( 'remote transactions received' , count = len ( transactions ) , remote id = proto ) def add txs ( ) : for tx in transactions : self . add transaction ( tx , origin = proto ) gevent . spawn ( add txs )", "predictions": ["shortcut for . . this is called from ( ) ."], "references": ["receives rlp . decoded serialized"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 4473, "code": "def img from vgg ( x ) : x = x . transpose ( ( 1 , 2 , 0 ) ) x [ : , : , 0 ] += 103.939 x [ : , : , 1 ] += 116.779 x [ : , : , 2 ] += 123.68 x = x [ : , : , : : - 1 ] return x", "predictions": ["convert an entity in the format \" self - world \" into an angle which represents the dimensions of the given result ."], "references": ["decondition an image from the vgg16 model ."], "bleu": 0.06293173924458136, "rouge_l": 0.21205098493626884}
{"id": 4474, "code": "def img to vgg ( x ) : x = x [ : , : , : : - 1 ] x [ : , : , 0 ] -= 103.939 x [ : , : , 1 ] -= 116.779 x [ : , : , 2 ] -= 123.68 x = x . transpose ( ( 2 , 0 , 1 ) ) return x", "predictions": ["converts this creator ' s coordinates into a . ."], "references": ["condition an image for use with the vgg16 model ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 4475, "code": "def get f layer ( self , layer name ) : inputs = [ self . net input ] if self . learning phase is not None : inputs . append ( K . learning phase ( ) ) return K . function ( inputs , [ self . get layer output ( layer name ) ] )", "predictions": ["generate a learning for the given ( ( i . e . ( ( ( ( ( file , ( file , ( file , ( file file file file file file , ( file file file file file file file file file file file file file file file file"], "references": ["create a function for the response of a layer ."], "bleu": 0.0359340051359579, "rouge_l": 0.15155279503105593}
{"id": 4476, "code": "def get layer output ( self , name ) : if not name in self . f layer outputs : layer = self . net . get layer ( name ) self . f layer outputs [ name ] = layer . output return self . f layer outputs [ name ]", "predictions": ["display in error format ."], "references": ["get symbolic output of a layer ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4477, "code": "def get features ( self , x , layers ) : if not layers : return None inputs = [ self . net . input ] if self . learning phase is not None : inputs . append ( self . learning phase ) f = K . function ( inputs , [ self . get layer output ( layer name ) for layer name in layers ] ) feature outputs = f ( [ x ] ) features = dict ( zip ( layers , feature outputs ) ) return features", "predictions": ["for each layer ."], "references": ["evaluate layer outputs for x"], "bleu": 0.3096787331587729, "rouge_l": 0.21785714285714283}
{"id": 4478, "code": "def fix compile ( remove flags ) : import distutils . ccompiler def fix compile ( self , sources , output dir = None , macros = None , include dirs = None , debug = 0 , extra preargs = None , extra postargs = None , depends = None ) : for flag in remove flags : if flag in self . compiler so : self . compiler so . remove ( flag ) macros , objects , extra postargs , pp opts , build = self . setup compile ( output dir , macros , include dirs , sources , depends , extra postargs ) cc args = self . get cc args ( pp opts , debug , extra preargs ) for obj in objects : try : src , ext = build [ obj ] except Key Error : continue self . compile ( obj , src , ext , cc args , extra postargs , pp opts ) return objects distutils . ccompiler . C Compiler . compile = fix compile", "predictions": ["( w . 5 . 5 . save save print print print print out the write and ) ."], "references": ["monkey - patch compiler to allow for removal of default compiler flags ."], "bleu": 0.06439931429457924, "rouge_l": 0.06468716861081655}
{"id": 4479, "code": "def do table ( self , line ) : if len ( line ) > 0 : if line . strip ( ) . lower ( ) == \"on\" : log . write ( \"Table ON\" ) self . table output = True return elif line . strip ( ) . lower ( ) == \"off\" : log . write ( \"Table OFF\" ) self . table output = False return log . write ( \"Table output: {}\" . format ( \"ON\" if self . table output else \"OFF\" ) )", "predictions": ["writes an end : 1 : 1 : 1 table : . : . ( : . ( : . if true if no : . if ( = true if the : : . = 0 if no : \\ . = 0 if no : \\ . ="], "references": ["display results in table format"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 4480, "code": "def float with multiplier ( string ) : match = re float with multiplier . search ( string ) if not match or not match . group ( 'num' ) : raise Value Error ( 'String \"{}\" is not numeric!' . format ( string ) ) num = float ( match . group ( 'num' ) ) multi = match . group ( 'multi' ) if multi : try : num *= multipliers [ multi ] except Key Error : raise Value Error ( 'Unknown multiplier: {}' . format ( multi ) ) return num", "predictions": ["convert a floating point string into a float value ."], "references": ["convert string with optional k m g t multiplier to float"], "bleu": 0.13564514503163538, "rouge_l": 0.28328173374613}
{"id": 4481, "code": "def specific gains ( string ) : if not string : return { } gains = { } for gain in string . split ( ',' ) : amp name , value = gain . split ( '=' ) gains [ amp name . strip ( ) ] = float ( value . strip ( ) ) return gains", "predictions": ["convert from the string to a specific gains string ."], "references": ["convert string with gains of individual amplification elements to dict"], "bleu": 0.15851165692617156, "rouge_l": 0.3}
{"id": 4482, "code": "def device settings ( string ) : if not string : return { } settings = { } for setting in string . split ( ',' ) : setting name , value = setting . split ( '=' ) settings [ setting name . strip ( ) ] = value . strip ( ) return settings", "predictions": ["converts a string to a device ."], "references": ["convert string with soapysdr device settings to dict"], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 4483, "code": "def wrap ( text , indent = '    ' ) : wrapper = textwrap . Text Wrapper ( width = int ( os . environ . get ( 'COLUMNS' , 80 ) ) , initial indent = indent , subsequent indent = indent ) return '\\n' . join ( wrapper . wrap ( text ) )", "predictions": ["wraps text to the output ."], "references": ["wrap text to terminal width with default indentation"], "bleu": 0.20830666398386113, "rouge_l": 0.2785388127853881}
{"id": 4484, "code": "def detect devices ( soapy args = '' ) : devices = simplesoapy . detect devices ( soapy args , as string = True ) text = [ ] text . append ( 'Detected Soapy SDR devices:' ) if devices : for i , d in enumerate ( devices ) : text . append ( '  {}' . format ( d ) ) else : text . append ( '  No devices found!' ) return ( devices , '\\n' . join ( text ) )", "predictions": ["detect the command line devices ."], "references": ["returns detected soapysdr devices"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4485, "code": "def set center freq ( self , center freq ) : psd state = { 'repeats' : 0 , 'freq array' : self . base freq array + self . lnb lo + center freq , 'pwr array' : None , 'update lock' : threading . Lock ( ) , 'futures' : [ ] , } return psd state", "predictions": ["set center of the psd in the center of this psd ."], "references": ["set center frequency and clear averaged psd data"], "bleu": 0.14694106251955755, "rouge_l": 0.3112244897959184}
{"id": 4486, "code": "def result ( self , psd state ) : freq array = numpy . fft . fftshift ( psd state [ 'freq array' ] ) pwr array = numpy . fft . fftshift ( psd state [ 'pwr array' ] ) if self . crop factor : crop bins half = round ( ( self . crop factor * self . bins ) / 2 ) freq array = freq array [ crop bins half : - crop bins half ] pwr array = pwr array [ crop bins half : - crop bins half ] if psd state [ 'repeats' ] > 1 : pwr array = pwr array / psd state [ 'repeats' ] if self . log scale : pwr array = 10 * numpy . log10 ( pwr array ) return ( freq array , pwr array )", "predictions": ["returns a crop array that is equal to the one of each state ."], "references": ["return freqs and averaged psd for given center frequency"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 4487, "code": "def wait for result ( self , psd state ) : if len ( psd state [ 'futures' ] ) > 1 : concurrent . futures . wait ( psd state [ 'futures' ] ) elif psd state [ 'futures' ] : psd state [ 'futures' ] [ 0 ] . result ( ) return self . result ( psd state )", "predictions": ["waits for the psd to finish ."], "references": ["wait for all psd threads to finish and return result"], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 4488, "code": "def update ( self , psd state , samples array ) : freq array , pwr array = simplespectral . welch ( samples array , self . sample rate , nperseg = self . bins , window = self . fft window , noverlap = self . fft overlap bins , detrend = self . detrend ) if self . remove dc : pwr array [ 0 ] = ( pwr array [ 1 ] + pwr array [ - 1 ] ) / 2 with psd state [ 'update lock' ] : psd state [ 'repeats' ] += 1 if psd state [ 'pwr array' ] is None : psd state [ 'pwr array' ] = pwr array else : psd state [ 'pwr array' ] += pwr array", "predictions": ["helper function to update the psd produced by the given state ."], "references": ["compute psd from samples and update average for given center frequency"], "bleu": 0.1235622127262679, "rouge_l": 0.17528735632183906}
{"id": 4489, "code": "def read ( self , f ) : magic = f . read ( len ( self . magic ) ) if not magic : return None if magic != self . magic : raise Value Error ( 'Magic bytes not found! Read data: {}' . format ( magic ) ) header = self . header . make ( self . header struct . unpack ( f . read ( self . header struct . size ) ) ) pwr array = numpy . fromstring ( f . read ( header . size ) , dtype = 'float32' ) return ( header , pwr array )", "predictions": ["read an ( and return as a pwr ."], "references": ["read data from file - like object"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4490, "code": "def write ( self , f , time start , time stop , start , stop , step , samples , pwr array ) : f . write ( self . magic ) f . write ( self . header struct . pack ( self . version , time start , time stop , start , stop , step , samples , pwr array . nbytes ) ) #pwr array.tofile(f) f . write ( pwr array . tobytes ( ) ) f . flush ( )", "predictions": ["writes the message to the given step ."], "references": ["write data to file - like object"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4491, "code": "def write ( self , psd data or future , time start , time stop , samples ) : try : f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future try : step = f array [ 1 ] - f array [ 0 ] self . formatter . write ( self . output , time start . timestamp ( ) , time stop . timestamp ( ) , f array [ 0 ] , f array [ - 1 ] + step , step , samples , pwr array ) except Exception as e : logging . exception ( 'Error writing to output file: {}' . format ( e ) )", "predictions": ["writes a data set ."], "references": ["write psd of one frequency hop"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 4492, "code": "def write ( self , psd data or future , time start , time stop , samples ) : try : f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future self . output . write ( ) self . output . write ( . format ( time start ) ) self . output . write ( . format ( time stop ) ) self . output . write ( '#\\n' ) self . output . write ( ) for f , pwr in zip ( f array , pwr array ) : self . output . write ( '{} {}\\n' . format ( f , pwr ) ) self . output . write ( '\\n' ) self . output . flush ( )", "predictions": ["writes a data structure to this statement ."], "references": ["write psd of one frequency hop"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4493, "code": "def write ( self , psd data or future , time start , time stop , samples ) : try : f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future try : step = f array [ 1 ] - f array [ 0 ] row = [ time stop . strftime ( '%Y-%m-%d' ) , time stop . strftime ( '%H:%M:%S' ) , f array [ 0 ] , f array [ - 1 ] + step , step , samples ] row += list ( pwr array ) self . output . write ( '{}\\n' . format ( ', ' . join ( str ( x ) for x in row ) ) ) self . output . flush ( ) except Exception as e : logging . exception ( 'Error writing to output file:' )", "predictions": ["writes a new data set ."], "references": ["write psd of one frequency hop"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4494, "code": "def time to repeats ( self , bins , integration time ) : return math . ceil ( ( self . device . sample rate * integration time ) / bins )", "predictions": ["convert a time stamp to a unit ."], "references": ["convert integration time to number of repeats"], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 4495, "code": "def freq plan ( self , min freq , max freq , bins , overlap = 0 , quiet = False ) : bin size = self . bins to bin size ( bins ) bins crop = round ( ( 1 - overlap ) * bins ) sample rate crop = ( 1 - overlap ) * self . device . sample rate freq range = max freq - min freq hopping = True if freq range >= sample rate crop else False hop size = self . nearest freq ( sample rate crop , bin size ) hops = math . ceil ( freq range / hop size ) if hopping else 1 min center freq = min freq + ( hop size / 2 ) if hopping else min freq + ( freq range / 2 ) max center freq = min center freq + ( ( hops - 1 ) * hop size ) freq list = [ min center freq + ( i * hop size ) for i in range ( hops ) ] if not quiet : logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) logger . info ( 'bin size: {:.2f} Hz' . format ( bin size ) ) logger . info ( 'bins: {}' . format ( bins ) ) logger . info ( 'bins (after crop): {}' . format ( bins crop ) ) logger . info ( 'sample rate: {:.3f} M Hz' . format ( self . device . sample rate / 1e6 ) ) logger . info ( 'sample rate (after crop): {:.3f} M Hz' . format ( sample rate crop / 1e6 ) ) logger . info ( 'freq range: {:.3f} M Hz' . format ( freq range / 1e6 ) ) logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) logger . info ( 'hop size: {:.3f} M Hz' . format ( hop size / 1e6 ) ) logger . info ( 'hops: {}' . format ( hops ) ) logger . info ( 'min center freq: {:.3f} M Hz' . format ( min center freq / 1e6 ) ) logger . info ( 'max center freq: {:.3f} M Hz' . format ( max center freq / 1e6 ) ) logger . info ( 'min freq (after crop): {:.3f} M Hz' . format ( ( min center freq - ( hop size / 2 ) ) / 1e6 ) ) logger . info ( 'max freq (after crop): {:.3f} M Hz' . format ( ( max center freq + ( hop size / 2 ) ) / 1e6 ) ) logger . debug ( 'Frequency hops table:' ) logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) for f in freq list : logger . debug ( '  {:8.3f} M Hz  {:8.3f} M Hz  {:8.3f} M Hz' . format ( ( f - ( self . device . sample rate / 2 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample rate / 2 ) ) / 1e6 , ) ) return freq list", "predictions": ["returns a plan for the given interval ."], "references": ["returns list of frequencies for frequency hopping"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4496, "code": "def create buffer ( self , bins , repeats , base buffer size , max buffer size = 0 ) : samples = bins * repeats buffer repeats = 1 buffer size = math . ceil ( samples / base buffer size ) * base buffer size if not max buffer size : max buffer size = ( 100 * 1024 ** 2 ) / 8 if max buffer size > 0 : max buffer size = math . ceil ( max buffer size / base buffer size ) * base buffer size if buffer size > max buffer size : logger . warning ( 'Required buffer size ({}) will be shrinked to max buffer size ({})!' . format ( buffer size , max buffer size ) ) buffer repeats = math . ceil ( buffer size / max buffer size ) buffer size = max buffer size logger . info ( 'repeats: {}' . format ( repeats ) ) logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample rate ) ) if max buffer size > 0 : logger . info ( 'max buffer size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max buffer size , max buffer size / bins , max buffer size / self . device . sample rate ) ) else : logger . info ( 'max buffer size (samples): UNLIMITED' ) logger . info ( 'buffer size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer size , buffer size / bins , buffer size / self . device . sample rate ) ) logger . info ( 'buffer repeats: {}' . format ( buffer repeats ) ) return ( buffer repeats , zeros ( buffer size , numpy . complex64 ) )", "predictions": ["creates a buffer for storing the data on the given buffer ."], "references": ["create buffer for reading samples"], "bleu": 0.1367440667823257, "rouge_l": 0.25416666666666665}
{"id": 4497, "code": "def setup ( self , bins , repeats , base buffer size = 0 , max buffer size = 0 , fft window = 'hann' , fft overlap = 0.5 , crop factor = 0 , log scale = True , remove dc = False , detrend = None , lnb lo = 0 , tune delay = 0 , reset stream = False , max threads = 0 , max queue size = 0 ) : if self . device . is streaming : self . device . stop stream ( ) base buffer = self . device . start stream ( buffer size = base buffer size ) self . bins = bins self . repeats = repeats self . base buffer size = len ( base buffer ) self . max buffer size = max buffer size self . buffer repeats , self . buffer = self . create buffer ( bins , repeats , self . base buffer size , self . max buffer size ) self . tune delay = tune delay self . reset stream = reset stream self . psd = psd . PSD ( bins , self . device . sample rate , fft window = fft window , fft overlap = fft overlap , crop factor = crop factor , log scale = log scale , remove dc = remove dc , detrend = detrend , lnb lo = lnb lo , max threads = max threads , max queue size = max queue size ) self . writer = writer . formats [ self . output format ] ( self . output )", "predictions": ["set up and retrieve fft ."], "references": ["prepare samples buffer and start streaming samples from device"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4498, "code": "def stop ( self ) : if not self . device . is streaming : return self . device . stop stream ( ) self . writer . close ( ) self . bins = None self . repeats = None self . base buffer size = None self . max buffer size = None self . buffer repeats = None self . buffer = None self . tune delay = None self . reset stream = None self . psd = None self . writer = None", "predictions": ["stops the process and frees all registered services ."], "references": ["stop streaming samples from device and delete samples buffer"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4499, "code": "def psd ( self , freq ) : if not self . device . is streaming : raise Runtime Error ( 'Streaming is not initialized, you must run setup() first!' ) logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) t freq = time . time ( ) if self . device . freq != freq : if self . reset stream : self . device . device . deactivate Stream ( self . device . stream ) self . device . freq = freq if self . reset stream : self . device . device . activate Stream ( self . device . stream ) if self . tune delay : t delay = time . time ( ) while True : self . device . read stream ( ) t delay end = time . time ( ) if t delay end - t delay >= self . tune delay : break logger . debug ( '    Tune delay: {:.3f} s' . format ( t delay end - t delay ) ) else : logger . debug ( '    Same frequency as before, tuning skipped' ) psd state = self . psd . set center freq ( freq ) t freq end = time . time ( ) logger . debug ( '    Tune time: {:.3f} s' . format ( t freq end - t freq ) ) for repeat in range ( self . buffer repeats ) : logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) t acq = time . time ( ) acq time start = datetime . datetime . utcnow ( ) self . device . read stream into buffer ( self . buffer ) acq time stop = datetime . datetime . utcnow ( ) t acq end = time . time ( ) logger . debug ( '      Acquisition time: {:.3f} s' . format ( t acq end - t acq ) ) self . psd . update async ( psd state , numpy . copy ( self . buffer ) ) t final = time . time ( ) if shutdown : break psd future = self . psd . result async ( psd state ) logger . debug ( '    Total hop time: {:.3f} s' . format ( t final - t freq ) ) return ( psd future , acq time start , acq time stop )", "predictions": ["creates a psd running dummy psd ."], "references": ["tune to specified center frequency and compute power spectral density"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 4500, "code": "def sweep ( self , min freq , max freq , bins , repeats , runs = 0 , time limit = 0 , overlap = 0 , fft window = 'hann' , fft overlap = 0.5 , crop = False , log scale = True , remove dc = False , detrend = None , lnb lo = 0 , tune delay = 0 , reset stream = False , base buffer size = 0 , max buffer size = 0 , max threads = 0 , max queue size = 0 ) : self . setup ( bins , repeats , base buffer size , max buffer size , fft window = fft window , fft overlap = fft overlap , crop factor = overlap if crop else 0 , log scale = log scale , remove dc = remove dc , detrend = detrend , lnb lo = lnb lo , tune delay = tune delay , reset stream = reset stream , max threads = max threads , max queue size = max queue size ) try : freq list = self . freq plan ( min freq - lnb lo , max freq - lnb lo , bins , overlap ) t start = time . time ( ) run = 0 while not shutdown and ( runs == 0 or run < runs ) : run += 1 t run start = time . time ( ) logger . debug ( 'Run: {}' . format ( run ) ) for freq in freq list : psd future , acq time start , acq time stop = self . psd ( freq ) self . writer . write async ( psd future , acq time start , acq time stop , len ( self . buffer ) * self . buffer repeats ) if shutdown : break write next future = self . writer . write next async ( ) t run = time . time ( ) logger . debug ( '  Total run time: {:.3f} s' . format ( t run - t run start ) ) if time limit and ( time . time ( ) - t start ) >= time limit : logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time limit , run ) ) break write next future . result ( ) logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer overflow count ) ) logging . debug ( 'PSD worker threads: {}' . format ( self . psd . executor . max workers ) ) logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . psd . executor . max queue size reached , self . psd . executor . max queue size ) ) logging . debug ( 'Writer worker threads: {}' . format ( self . writer . executor . max workers ) ) logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . writer . executor . max queue size reached , self . writer . executor . max queue size ) ) finally : self . stop ( ) t stop = time . time ( ) logger . info ( 'Total time: {:.3f} s' . format ( t stop - t start ) )", "predictions": ["returns a list of ( ( ."], "references": ["sweep spectrum using frequency hopping"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4501, "code": "def run cmake ( arg = \"\" ) : if ds . find executable ( 'cmake' ) is None : print \"C Make  is required to build zql\" print \"Please install cmake version >= 2.8 and re-run setup\" sys . exit ( - 1 ) print \"Configuring zql build with C Make.... \" cmake args = arg try : build dir = op . join ( op . split ( file ) [ 0 ] , 'build' ) dd . mkpath ( build dir ) os . chdir ( \"build\" ) ds . spawn ( [ 'cmake' , '..' ] + cmake args . split ( ) ) ds . spawn ( [ 'make' , 'clean' ] ) ds . spawn ( [ 'make' ] ) os . chdir ( \"..\" ) except ds . Distutils Exec Error : print \"Error while running cmake\" print \"run 'setup.py build --help' for build options\" print \"You may also try editing the settings in C Make Lists.txt file and re-running setup\" sys . exit ( - 1 )", "predictions": ["run all the command ( ."], "references": ["forcing to run cmake"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4502, "code": "def bring gpio interrupt into userspace ( ) : try : with open ( GPIO INTERRUPT DEVICE VALUE ) : return except IO Error : with open ( GPIO EXPORT FILE , 'w' ) as export file : export file . write ( str ( GPIO INTERRUPT PIN ) ) wait until file exists ( GPIO INTERRUPT DEVICE VALUE )", "predictions": ["interrupt the export by looking up the export into the export ."], "references": ["bring the interrupt pin on the gpio into linux userspace ."], "bleu": 0.1367440667823257, "rouge_l": 0.3505747126436781}
{"id": 4503, "code": "def gpio interrupts enable ( self ) : try : bring gpio interrupt into userspace ( ) set gpio interrupt edge ( ) except Timeout as e : raise Interrupt Enable Exception ( \"There was an error bringing gpio%d into userspace. %s\" % ( GPIO INTERRUPT PIN , e . message ) )", "predictions": ["discards the game and initiate the message ."], "references": ["enables gpio interrupts ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4504, "code": "def has errors ( self , form ) : return any ( [ fieldname error for fieldname error in form . errors . keys ( ) if fieldname error in self ] )", "predictions": ["return true if there are no errors ."], "references": ["find tab fields listed as invalid"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4505, "code": "def get form kwargs ( self ) : kwargs = super ( Form Containers Mixin , self ) . get form kwargs ( ) kwargs . update ( { 'pack' : \"foundation-{}\" . format ( self . kwargs . get ( 'foundation version' ) ) } ) return kwargs", "predictions": ["this is a convenience method for making a form class for the form \" form \" method ."], "references": ["pass template pack argument"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 4506, "code": "def publish ( self ) : return self . publish ( self . args , self . server , self . URI )", "predictions": ["publish to the worker ."], "references": ["perform http session to transmit defined weather values ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4507, "code": "def get ( data ) : crc = 0 for byte in array ( 'B' , data ) : crc = ( V Pro CRC . CRC TABLE [ ( crc >> 8 ) ^ byte ] ^ ( ( crc & 0x FF ) << 8 ) ) return crc", "predictions": ["calculates crc from the stream ."], "references": ["return crc calc value from raw serial data"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4508, "code": "def unpack storm date ( date ) : year = ( date & 0x7f ) + 2000 day = ( date >> 7 ) & 0x01f month = ( date >> 12 ) & 0x0f return \"%s-%s-%s\" % ( year , month , day )", "predictions": ["unpack date from year ."], "references": ["given a packed storm date field unpack and return yyyy - mm - dd string ."], "bleu": 0.035974230453556216, "rouge_l": 0.17403708987161198}
{"id": 4509, "code": "def use rev b archive ( self , records , offset ) : if type ( self . ARCHIVE REV B ) is bool : return self . ARCHIVE REV B data = Archive B Struct . unpack from ( records , offset ) if data [ 'Rec Type' ] == 0 : log . info ( 'detected archive rev. B' ) self . ARCHIVE REV B = True else : log . info ( 'detected archive rev. A' ) self . ARCHIVE REV B = False return self . ARCHIVE REV B", "predictions": ["writes the result of this pointer into the specified archive ."], "references": ["return true if weather station returns rev . b archives"], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 4510, "code": "def wakeup ( self ) : log . info ( \"send: WAKEUP\" ) for i in xrange ( 3 ) : self . port . write ( '\\n' ) ack = self . port . read ( len ( self . WAKE ACK ) ) log raw ( 'read' , ack ) if ack == self . WAKE ACK : return raise No Device Exception ( 'Can not access weather station' )", "predictions": ["running this method will run all the ( players at the event level ."], "references": ["issue wakeup command to device to take out of standby mode ."], "bleu": 0.08839374326825923, "rouge_l": 0.07800511508951406}
{"id": 4511, "code": "def dmpaft cmd ( self , time fields ) : records = [ ] tbuf = struct . pack ( '2H' , * time fields ) self . cmd ( 'DMPAFT' ) crc = V Pro CRC . get ( tbuf ) crc = struct . pack ( '>H' , crc ) log raw ( 'send' , tbuf + crc ) self . port . write ( tbuf + crc ) ack = self . port . read ( len ( self . ACK ) ) log raw ( 'read' , ack ) if ack != self . ACK : return raw = self . port . read ( Dmp Struct . size ) log raw ( 'read' , raw ) if not V Pro CRC . verify ( raw ) : log raw ( 'send ESC' , self . ESC ) self . port . write ( self . ESC ) return log raw ( 'send ACK' , self . ACK ) self . port . write ( self . ACK ) dmp = Dmp Struct . unpack ( raw ) log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) for i in xrange ( dmp [ 'Pages' ] ) : raw = self . port . read ( Dmp Page Struct . size ) log raw ( 'read' , raw ) if not V Pro CRC . verify ( raw ) : log raw ( 'send ESC' , self . ESC ) self . port . write ( self . ESC ) return log raw ( 'send ACK' , self . ACK ) self . port . write ( self . ACK ) page = Dmp Page Struct . unpack ( raw ) offset = 0 if i == 0 : offset = dmp [ 'Offset' ] * Archive A Struct . size while offset < Archive A Struct . size * 5 : log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) if self . use rev b archive ( page [ 'Records' ] , offset ) : a = Archive B Struct . unpack from ( page [ 'Records' ] , offset ) else : a = Archive A Struct . unpack from ( page [ 'Records' ] , offset ) if a [ 'Date Stamp' ] != 0xffff and a [ 'Time Stamp' ] != 0xffff : records . append ( a ) offset += Archive A Struct . size log . info ( 'read all pages' ) return records", "predictions": ["this is used to determine if there are some records in the ( ."], "references": ["issue a command to read the archive records after a known time stamp ."], "bleu": 0.11114924776032006, "rouge_l": 0.21428571428571427}
{"id": 4512, "code": "def weather update ( station , pub sites , interval ) : station . parse ( ) if station . fields [ 'Temp Out' ] > 200 : raise No Sensor Exception ( 'Out of range temperature value: %.1f, check sensors' % ( station . fields [ 'Temp Out' ] , ) ) gust , gust dir = Wind Gust . get ( station , interval ) for ps in pub sites : try : ps . set ( pressure = station . fields [ 'Pressure' ] , dewpoint = station . fields [ 'Dew Point' ] , humidity = station . fields [ 'Hum Out' ] , tempf = station . fields [ 'Temp Out' ] , rainin = station . fields [ 'Rain Rate' ] , rainday = station . fields [ 'Rain Day' ] , dateutc = station . fields [ 'Date Stamp Utc' ] , windspeed = station . fields [ 'Wind Speed10Min' ] , winddir = station . fields [ 'Wind Dir' ] , windgust = gust , windgustdir = gust dir , ) ps . publish ( ) except ( Exception ) as e : log . warn ( 'publisher %s: %s' % ( ps . class . name , e ) )", "predictions": ["publish when some rate' have been set ."], "references": ["main execution loop . query weather data and post to online service ."], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 4513, "code": "def init log ( quiet , debug ) : from logging . handlers import Sys Log Handler fmt = logging . Formatter ( os . path . basename ( sys . argv [ 0 ] ) + \".%(name)s %(levelname)s - %(message)s\" ) facility = Sys Log Handler . LOG DAEMON syslog = Sys Log Handler ( address = '/dev/log' , facility = facility ) syslog . set Formatter ( fmt ) log . add Handler ( syslog ) if not quiet : console = logging . Stream Handler ( ) console . set Formatter ( fmt ) log . add Handler ( console ) log . set Level ( logging . INFO ) if debug : log . set Level ( logging . DEBUG )", "predictions": ["this method just logs the gains handler for ( ."], "references": ["setup system logging to desired verbosity ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 4514, "code": "def get pub services ( opts ) : sites = [ ] for p key in vars ( opts ) . keys ( ) : args = getattr ( opts , p key ) if p key in PUB SERVICES and args : if isinstance ( args , tuple ) : ps = PUB SERVICES [ p key ] ( * args ) else : ps = PUB SERVICES [ p key ] ( args ) sites . append ( ps ) return sites", "predictions": ["returns all ( or : 1 : integer : float : float : integer : 2 . 3 : 1 . 2 : 00 . 3 . 2 . 3 . 3 . 3 : , . 2 . 3 . 3 . 3 : , . 2 . 3"], "references": ["use values in opts data to generate instances of publication services ."], "bleu": 0.02403051755364481, "rouge_l": 0.03626634958382877}
{"id": 4515, "code": "def get options ( parser ) : pub g = optparse . Option Group ( parser , \"Publication Services\" , , ) pub g . add option ( '-w' , '--wundergound' , nargs = 2 , type = 'string' , dest = 'wug' , help = 'Weather Underground service; WUG=[SID(station ID), PASSWORD]' ) pub g . add option ( '-p' , '--pws' , nargs = 2 , type = 'string' , dest = 'pws' , help = 'PWS service; PWS=[SID(station ID), PASSWORD]' ) pub g . add option ( '-f' , '--file' , nargs = 1 , type = 'string' , dest = 'file' , help = 'Local file; FILE=[FILE NAME]' ) parser . add option group ( pub g ) parser . add option ( '-d' , '--debug' , dest = 'debug' , action = \"store true\" , default = False , help = 'enable verbose debug logging' ) parser . add option ( '-q' , '--quiet' , dest = 'quiet' , action = \"store true\" , default = False , help = 'disable all console logging' ) parser . add option ( '-t' , '--tty' , dest = 'tty' , default = '/dev/tty S0' , help = 'set serial port device [/dev/tty S0]' ) parser . add option ( '-n' , '--interval' , dest = 'interval' , default = 60 , type = 'int' , help = 'polling/update interval in seconds [60]' ) return parser . parse args ( )", "predictions": ["a helper method to parse the command line option ."], "references": ["read command line options to configure program behavior ."], "bleu": 0.18850319022747347, "rouge_l": 0.31881533101045295}
{"id": 4516, "code": "def set ( self , * * kw ) : self . args = kw log . debug ( self . args )", "predictions": ["use this to = 0 ."], "references": ["store keyword args to be written to output file ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4517, "code": "def publish ( self ) : with open ( self . file name , 'w' ) as fh : for k , v in self . args . iteritems ( ) : buf = String IO . String IO ( ) buf . write ( k ) self . append vals ( buf , v ) fh . write ( buf . getvalue ( ) + '\\n' ) buf . close ( )", "predictions": ["set this latlng on the underlying output stream ."], "references": ["write output file ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 4518, "code": "def init app ( self , app ) : if not hasattr ( app , \"extensions\" ) : app . extensions = { } app . extensions [ \"allows\" ] = self @ app . before request def start context ( * a , * * k ) : self . overrides . push ( Override ( ) ) self . additional . push ( Additional ( ) ) @ app . after request def cleanup ( response ) : self . clear all overrides ( ) self . clear all additional ( ) return response", "predictions": ["creates and initializes the if it is not a new request ."], "references": ["initializes the flask - allows object against the provided application"], "bleu": 0.1367440667823257, "rouge_l": 0.18484848484848485}
{"id": 4519, "code": "def unduplicate field names ( field names ) : res = [ ] for k in field names : if k in res : i = 1 while k + ' ' + str ( i ) in res : i += 1 k += ' ' + str ( i ) res . append ( k ) return res", "predictions": ["returns an instance of this class with the provided values ."], "references": ["append a number to duplicate field names to make them unique ."], "bleu": 0.10400927574124633, "rouge_l": 0.08628005657708629}
{"id": 4520, "code": "def get dataframe ( self ) : if pd is None : raise Import Error ( \"Try installing Pandas first.\" ) frame = pd . Data Frame ( self [ : ] , columns = ( self and self . keys ) or [ ] ) return frame", "predictions": ["fetches the ( or data psd psd psd psd psd psd psd psd psd psd psd psd psd psd psd psd psd psd psd psd psd psd self . psd self psd psd psd self . ( psd psd psd psd psd psd psd psd psd psd psd psd psd"], "references": ["returns a pandas dataframe instance built from the result set ."], "bleu": 0.026594139297659906, "rouge_l": 0.07411907654921021}
{"id": 4521, "code": "def get widgets sorted ( self ) : result = [ ] for widget name , widget in self . get widgets ( ) . items ( ) : result . append ( ( widget name , widget , widget . position ) ) result . sort ( key = lambda x : x [ 2 ] ) return result", "predictions": ["read items by return item ."], "references": ["returns the widgets sorted by position ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 4522, "code": "def unregister widget ( self , widget cls ) : if widget cls . name in self . widgets : del self . widgets [ widget cls ( ) . get name ( ) ]", "predictions": ["write a previously registered ( registered with this class start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start start"], "references": ["unregisters the given widget ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 4523, "code": "def get last update ( self ) : instance , created = models . Dashboard Widget Last Update . objects . get or create ( widget name = self . get name ( ) ) return instance", "predictions": ["updates the ( self data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data is changed data ."], "references": ["gets or creates the last update object for this widget ."], "bleu": 0.02958403749882612, "rouge_l": 0.08021038790269559}
{"id": 4524, "code": "def save setting ( self , setting name , value ) : setting = self . get setting ( setting name ) if setting is None : setting = models . Dashboard Widget Settings . objects . create ( widget name = self . get name ( ) , setting name = setting name , value = value ) setting . value = value setting . save ( ) return setting", "predictions": ["saves the ( if any future exists future future future future future future future future future future ."], "references": ["saves the setting value into the database ."], "bleu": 0.09629943614188137, "rouge_l": 0.2479674796747967}
{"id": 4525, "code": "def format axes ( axes , shape ) : if isinstance ( axes , int ) : axes = ( axes , ) elif isinstance ( axes , list ) or hasattr ( axes , ' iter ' ) : axes = tuple ( axes ) if not isinstance ( axes , tuple ) : raise Value Error ( \"axes argument %s in the constructor not specified correctly\" % str ( axes ) ) if min ( axes ) < 0 or max ( axes ) > len ( shape ) - 1 : raise Value Error ( \"invalid key axes %s given shape %s\" % ( str ( axes ) , str ( shape ) ) ) return axes", "predictions": ["formats a tuple style ."], "references": ["format target axes given an array shape"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4526, "code": "def wrap ( func , shape , context = None , axis = ( 0 , ) , dtype = None , npartitions = None ) : if isinstance ( shape , int ) : shape = ( shape , ) key shape , value shape = get kv shape ( shape , Construct Spark . format axes ( axis , shape ) ) split = len ( key shape ) rdd = context . parallelize ( list ( product ( * [ arange ( x ) for x in key shape ] ) ) , npartitions ) rdd = rdd . map ( lambda x : ( x , func ( value shape , dtype , order = 'C' ) ) ) return Bolt Array Spark ( rdd , shape = shape , split = split , dtype = dtype )", "predictions": ["this will time the given function call this function does the same call as the given self or tensor ."], "references": ["wrap an existing numpy constructor in a parallelized construction"], "bleu": 0.051366639095059514, "rouge_l": 0.0}
{"id": 4527, "code": "def first ( self ) : from bolt . local . array import Bolt Array Local rdd = self . rdd if self . ordered else self . rdd . sort By Key ( ) return Bolt Array Local ( rdd . values ( ) . first ( ) )", "predictions": ["a method that returns a freq in this class ."], "references": ["return the first element of an array"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4528, "code": "def zip with index ( rdd ) : starts = [ 0 ] if rdd . get Num Partitions ( ) > 1 : nums = rdd . map Partitions ( lambda it : [ sum ( 1 for in it ) ] ) . collect ( ) count = sum ( nums ) for i in range ( len ( nums ) - 1 ) : starts . append ( starts [ - 1 ] + nums [ i ] ) else : count = rdd . count ( ) def func ( k , it ) : for i , v in enumerate ( it , starts [ k ] ) : yield v , i return count , rdd . map Partitions With Index ( func )", "predictions": ["create an array of column contents ."], "references": ["alternate version of spark s zipwithindex that eagerly returns count ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4529, "code": "def wrapped ( f ) : import inspect def extract ( func ) : append = \"\" args = inspect . getargspec ( func ) for i , a in enumerate ( args . args ) : if i < ( len ( args ) - len ( args . defaults ) ) : append += str ( a ) + \", \" else : default = args . defaults [ i - len ( args . defaults ) ] if hasattr ( default , \" name \" ) : default = default . name else : default = str ( default ) append += str ( a ) + \"=\" + default + \", \" append = append [ : - 2 ] + \")\" return append doc = f . doc + \"\\n\" doc += \"    local -> array(\" + extract ( getattr ( Construct Local , f . name ) ) + \"\\n\" doc += \"    spark -> array(\" + extract ( getattr ( Construct Spark , f . name ) ) + \"\\n\" f . doc = doc return f", "predictions": ["buffer for the function self - adds a large function to the function . this is used for debugging ."], "references": ["decorator to append routed docstrings"], "bleu": 0.06108557268562171, "rouge_l": 0.08970588235294118}
{"id": 4530, "code": "def plotcdf ( x , xmin , alpha ) : x = sort ( x ) n = len ( x ) xcdf = arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) q = x [ x >= xmin ] fcdf = ( q / xmin ) ** ( 1 - alpha ) nc = xcdf [ argmax ( x >= xmin ) ] fcdf norm = nc * fcdf loglog ( x , xcdf ) loglog ( q , fcdf norm )", "predictions": ["construct an initial approximation of the given approximation ."], "references": ["plots cdf and powerlaw"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4531, "code": "def plotpdf ( x , xmin , alpha , nbins = 30 , dolog = False ) : x = sort ( x ) n = len ( x ) if dolog : hb = hist ( x , bins = logspace ( log10 ( min ( x ) ) , log10 ( max ( x ) ) , nbins ) , log = True ) alpha += 1 else : hb = hist ( x , bins = linspace ( ( min ( x ) ) , ( max ( x ) ) , nbins ) ) h , b = hb [ 0 ] , hb [ 1 ] b = b [ 1 : ] q = x [ x >= xmin ] px = ( alpha - 1 ) / xmin * ( q / xmin ) ** ( - alpha ) arg = argmin ( abs ( b - xmin ) ) norm = mean ( h [ b > xmin ] / ( ( alpha - 1 ) / xmin * ( b [ b > xmin ] / xmin ) ** ( - alpha ) ) ) px = px * norm loglog ( q , px ) gca ( ) . set xlim ( min ( x ) , max ( x ) )", "predictions": ["builds an entity using the specified ( and ( ."], "references": ["plots pdf and powerlaw ...."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 4532, "code": "def discrete max likelihood arg ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : likelihoods = discrete likelihood vector ( data , xmin , alpharange = alpharange , n alpha = n alpha ) Largmax = np . argmax ( likelihoods ) return Largmax", "predictions": ["computes the crop distribution for this segment ."], "references": ["returns the * argument * of the max of the likelihood of the data given an input xmin"], "bleu": 0.04594560238169569, "rouge_l": 0.07193396226415094}
{"id": 4533, "code": "def discrete max likelihood ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : likelihoods = discrete likelihood vector ( data , xmin , alpharange = alpharange , n alpha = n alpha ) Lmax = np . max ( likelihoods ) return Lmax", "predictions": ["calculates the run of this octagon based on the other of the values ."], "references": ["returns the * argument * of the max of the likelihood of the data given an input xmin"], "bleu": 0.1039621375902982, "rouge_l": 0.3056112224448898}
{"id": 4534, "code": "def most likely alpha ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : alpha vector = np . linspace ( alpharange [ 0 ] , alpharange [ 1 ] , n alpha ) return alpha vector [ discrete max likelihood arg ( data , xmin , alpharange = alpharange , n alpha = n alpha ) ]", "predictions": ["computes the bring information for a set of numbers ."], "references": ["return the most likely alpha for the data given an xmin"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 4535, "code": "def plotcdf ( self , x = None , xmin = None , alpha = None , pointcolor = 'k' , dolog = True , zoom = True , pointmarker = '+' , * * kwargs ) : if x is None : x = self . data if xmin is None : xmin = self . xmin if alpha is None : alpha = self . alpha x = np . sort ( x ) n = len ( x ) xcdf = np . arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) q = x [ x >= xmin ] fcdf = ( q / xmin ) ** ( 1 - alpha ) nc = xcdf [ argmax ( x >= xmin ) ] fcdf norm = nc * fcdf D location = argmax ( xcdf [ x >= xmin ] - fcdf norm ) pylab . vlines ( q [ D location ] , xcdf [ x >= xmin ] [ D location ] , fcdf norm [ D location ] , color = 'm' , linewidth = 2 , zorder = 2 ) pylab . plot ( [ q [ D location ] ] * 2 , [ xcdf [ x >= xmin ] [ D location ] , fcdf norm [ D location ] ] , color = 'm' , marker = 's' , zorder = 3 ) #plotx = pylab.linspace(q.min(),q.max(),1000) #ploty = (plotx/xmin)**(1-alpha) * nc if dolog : pylab . loglog ( x , xcdf , marker = pointmarker , color = pointcolor , * * kwargs ) pylab . loglog ( q , fcdf norm , 'r' , * * kwargs ) else : pylab . semilogx ( x , xcdf , marker = pointmarker , color = pointcolor , * * kwargs ) pylab . semilogx ( q , fcdf norm , 'r' , * * kwargs ) if zoom : pylab . axis ( [ xmin , x . max ( ) , xcdf . min ( ) , nc ] )", "predictions": ["builds an approximation for testing ."], "references": ["plots cdf and powerlaw"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4536, "code": "def plot lognormal pdf ( self , * * kwargs ) : if not hasattr ( self , 'lognormal dist' ) : return normalized pdf = self . lognormal dist . pdf ( self . data ) / self . lognormal dist . pdf ( self . data ) . max ( ) min Y , max Y = pylab . gca ( ) . get ylim ( ) pylab . plot ( self . data , normalized pdf * max Y , '.' , * * kwargs )", "predictions": ["plots a ( possibly re - uploaded return the right ( the most recently added return return the ( return value return the ( ( return value return the right ( return the ( return the ( ( return value return the ( return value return the ( : w"], "references": ["plot the fitted lognormal distribution"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 4537, "code": "def plot lognormal cdf ( self , * * kwargs ) : if not hasattr ( self , 'lognormal dist' ) : return x = np . sort ( self . data ) n = len ( x ) xcdf = np . arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) lcdf = self . lognormal dist . sf ( x ) D location = argmax ( xcdf - lcdf ) pylab . vlines ( x [ D location ] , xcdf [ D location ] , lcdf [ D location ] , color = 'm' , linewidth = 2 ) pylab . plot ( x , lcdf , ',' , * * kwargs )", "predictions": ["generate a ( for this set of ( . the algorithm is called by the algorithm ."], "references": ["plot the fitted lognormal distribution"], "bleu": 0.07223943354597204, "rouge_l": 0.10082644628099173}
{"id": 4538, "code": "def hash sha256 ( self ) : fp plain = hashlib . sha256 ( self . decoded key ) . digest ( ) return ( b\"SHA256:\" + base64 . b64encode ( fp plain ) . replace ( b\"=\" , b\"\" ) ) . decode ( \"utf-8\" )", "predictions": ["publish the inputed message for the given key ."], "references": ["calculate sha256 fingerprint ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4539, "code": "def hash sha512 ( self ) : fp plain = hashlib . sha512 ( self . decoded key ) . digest ( ) return ( b\"SHA512:\" + base64 . b64encode ( fp plain ) . replace ( b\"=\" , b\"\" ) ) . decode ( \"utf-8\" )", "predictions": ["get the sha-1 checksum of this hashmap ."], "references": ["calculates sha512 fingerprint ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4540, "code": "def parse long ( cls , data ) : if sys . version < '3' : ret = long ( 0 ) for byte in data : ret = ( ret << 8 ) + ord ( byte ) else : ret = 0 for byte in data : ret = ( ret << 8 ) + byte return ret", "predictions": ["unpack the parsed value of a parsed 2000 ."], "references": ["calculate two s complement ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4541, "code": "def decode key ( cls , pubkey content ) : try : decoded key = base64 . b64decode ( pubkey content . encode ( \"ascii\" ) ) except ( Type Error , binascii . Error ) : raise Malformed Data Error ( \"Unable to decode the key\" ) return decoded key", "predictions": ["decodes an ssh public key and returns it ."], "references": ["decode base64 coded part of the key ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4542, "code": "def parse options ( self , options ) : quote open = False parsed options = { } def parse add single option ( opt ) : \"\"\"Parses and validates a single option, and adds it to parsed options field.\"\"\" if \"=\" in opt : opt name , opt value = opt . split ( \"=\" , 1 ) opt value = opt value . replace ( '\"' , '' ) else : opt name = opt opt value = True if \" \" in opt name or not self . OPTION NAME RE . match ( opt name ) : raise Invalid Option Name Error ( \"%s is not valid option name.\" % opt name ) if self . strict mode : for valid opt name , value required in self . OPTIONS SPEC : if opt name . lower ( ) == valid opt name : if value required and opt value is True : raise Missing Mandatory Option Value Error ( \"%s is missing mandatory value.\" % opt name ) break else : raise Unknown Option Name Error ( \"%s is unrecognized option name.\" % opt name ) if opt name not in parsed options : parsed options [ opt name ] = [ ] parsed options [ opt name ] . append ( opt value ) start of current opt = 0 i = 1 for i , character in enumerate ( options ) : if character == '\"' : quote open = not quote open if quote open : continue if character == \",\" : opt = options [ start of current opt : i ] parse add single option ( opt ) start of current opt = i + 1 if start of current opt + 1 != i : opt = options [ start of current opt : ] parse add single option ( opt ) if quote open : raise Invalid Options Error ( \"Unbalanced quotes.\" ) return parsed options", "predictions": ["wakeup the option object into an object ."], "references": ["parses ssh options string ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4543, "code": "def process ssh rsa ( self , data ) : current position , raw e = self . unpack by int ( data , 0 ) current position , raw n = self . unpack by int ( data , current position ) unpacked e = self . parse long ( raw e ) unpacked n = self . parse long ( raw n ) self . rsa = RSA Public Numbers ( unpacked e , unpacked n ) . public key ( default backend ( ) ) self . bits = self . rsa . key size if self . strict mode : min length = self . RSA MIN LENGTH STRICT max length = self . RSA MAX LENGTH STRICT else : min length = self . RSA MIN LENGTH LOOSE max length = self . RSA MAX LENGTH LOOSE if self . bits < min length : raise Too Short Key Error ( \"%s key data can not be shorter than %s bits (was %s)\" % ( self . key type , min length , self . bits ) ) if self . bits > max length : raise Too Long Key Error ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key type , max length , self . bits ) ) return current position", "predictions": ["adds or updates a shorter for a new instance of the fields ."], "references": ["parses ssh - rsa public keys ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 4544, "code": "def process ssh dss ( self , data ) : data fields = { } current position = 0 for item in ( \"p\" , \"q\" , \"g\" , \"y\" ) : current position , value = self . unpack by int ( data , current position ) data fields [ item ] = self . parse long ( value ) q bits = self . bits in number ( data fields [ \"q\" ] ) p bits = self . bits in number ( data fields [ \"p\" ] ) if q bits != self . DSA N LENGTH : raise Invalid Key Error ( \"Incorrect DSA key parameters: bits(p)=%s, q=%s\" % ( self . bits , q bits ) ) if self . strict mode : min length = self . DSA MIN LENGTH STRICT max length = self . DSA MAX LENGTH STRICT else : min length = self . DSA MIN LENGTH LOOSE max length = self . DSA MAX LENGTH LOOSE if p bits < min length : raise Too Short Key Error ( \"%s key can not be shorter than %s bits (was %s)\" % ( self . key type , min length , p bits ) ) if p bits > max length : raise Too Long Key Error ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key type , max length , p bits ) ) dsa parameters = DSA Parameter Numbers ( data fields [ \"p\" ] , data fields [ \"q\" ] , data fields [ \"g\" ] ) self . dsa = DSA Public Numbers ( data fields [ \"y\" ] , dsa parameters ) . public key ( default backend ( ) ) self . bits = self . dsa . key size return current position", "predictions": ["this method is called to process one or more ( for the data that was passed into this method ."], "references": ["parses ssh - dsa public keys ."], "bleu": 0.06108557268562171, "rouge_l": 0.08111702127659574}
{"id": 4545, "code": "def process ecdsa sha ( self , data ) : current position , curve information = self . unpack by int ( data , 0 ) if curve information not in self . ECDSA CURVE DATA : raise Not Implemented Error ( \"Invalid curve type: %s\" % curve information ) curve , hash algorithm = self . ECDSA CURVE DATA [ curve information ] current position , key data = self . unpack by int ( data , current position ) try : ecdsa key = ecdsa . Verifying Key . from string ( key data [ 1 : ] , curve , hash algorithm ) except Assertion Error : raise Invalid Key Error ( \"Invalid ecdsa key\" ) self . bits = int ( curve information . replace ( b\"nistp\" , b\"\" ) ) self . ecdsa = ecdsa key return current position", "predictions": ["collects all the components of the curve and returns them as a key ."], "references": ["parses ecdsa - sha public keys ."], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 4546, "code": "def main ( properties = properties , options = options , * * custom options ) : return init ( * * dict ( options , * * custom options ) ) ( * * properties )", "predictions": ["the main application processor ."], "references": ["imports and runs setup function with given properties ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4547, "code": "def create file ( ) : f = wave . open ( 'audio.wav' , mode = 'wb' ) f . setnchannels ( 2 ) p = pyaudio . Py Audio ( ) f . setsampwidth ( p . get sample size ( pyaudio . pa Int16 ) ) f . setframerate ( p . get default input device info ( ) [ 'default Sample Rate' ] ) try : yield f finally : f . close ( )", "predictions": ["create an iterator for the wave ."], "references": ["returns a file handle which is used to record audio"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 4548, "code": "def djfrontend jquery datatables css ( version = None ) : if version is None : if not getattr ( settings , 'DJFRONTEND JQUERY DATATABLES CSS' , False ) : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES VERSION' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) else : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES CSS' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) return format html ( '<link rel=\"stylesheet\" href=\"{static}djfrontend/css/jquery/jquery.data Tables/{v}/jquery.data Tables{min}.css\">' , static = static url , v = version , min = min )", "predictions": ["djfrontend a jquery . please use ( instead of ( ."], "references": ["returns the jquery datatables css file according to version number ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 4549, "code": "def djfrontend jquery datatables themeroller ( version = None ) : if version is None : if not getattr ( settings , 'DJFRONTEND JQUERY DATATABLES THEMEROLLER' , False ) : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES VERSION' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) else : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES THEMEROLLER' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) return format html ( '<link rel=\"stylesheet\" href=\"href=\"{static}djfrontend/css/jquery/jquery.data Tables/{v}/jquery.data Tables themeroller.min.css\">' , static = static url , v = version )", "predictions": ["get a ( instance from the given version ."], "references": ["returns the jquery datatables themeroller css file according to version number ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 4550, "code": "def calc expiry time ( minutes valid ) : return ( timezone . now ( ) + datetime . timedelta ( minutes = minutes valid + 1 ) ) . replace ( second = 0 , microsecond = 0 )", "predictions": ["calculate the minutes based on the passed timezone ."], "references": ["return specific time an auth_hash will expire ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4551, "code": "def get user token ( user , purpose , minutes valid ) : token = '' . join ( dumps ( [ user . get username ( ) , get auth hash ( user , purpose ) , ] ) . encode ( 'base64' ) . split ( '\\n' ) ) return { 'id' : get meteor id ( user ) , 'token' : token , 'token Expires' : calc expiry time ( minutes valid ) , }", "predictions": ["get an user ' s purpose of a purpose ."], "references": ["return login token info for given user ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 4552, "code": "def serialize ( self , obj , * args , * * kwargs ) : data = super ( Users , self ) . serialize ( obj , * args , * * kwargs ) profile = data . pop ( 'fields' ) profile . setdefault ( 'name' , obj . get full name ( ) ) fields = data [ 'fields' ] = { 'username' : obj . get username ( ) , 'emails' : [ ] , 'profile' : profile , 'permissions' : sorted ( self . model . get all permissions ( obj ) ) , } for sensitive in [ 'password' , 'user permissions ids' , 'is active' , 'is staff' , 'is superuser' , 'groups ids' , ] : profile . pop ( sensitive , None ) try : fields [ 'created At' ] = profile . pop ( 'date joined' ) except Key Error : date joined = getattr ( obj , 'get date joined' , lambda : getattr ( obj , 'date joined' , None ) ) ( ) if date joined : fields [ 'created At' ] = date joined try : email = profile . pop ( 'email' ) except Key Error : email = getattr ( obj , 'get email' , lambda : getattr ( obj , 'email' , None ) ) ( ) if email : fields [ 'emails' ] . append ( { 'address' : email , 'verified' : True } ) return data", "predictions": ["serializes the default data ."], "references": ["serialize user as per meteor accounts serialization ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4553, "code": "def deserialize profile ( profile , key prefix = '' , pop = False ) : result = { } if pop : getter = profile . pop else : getter = profile . get def prefixed ( name ) : \"\"\"Return name prefixed by `key prefix`.\"\"\" return '%s%s' % ( key prefix , name ) for key in profile . keys ( ) : val = getter ( key ) if key == prefixed ( 'name' ) : result [ 'full name' ] = val else : raise Meteor Error ( 400 , 'Bad profile key: %r' % key ) return result", "predictions": ["deserialize all the contents of the given key and prefix"], "references": ["de - serialize user profile fields into concrete model fields ."], "bleu": 0.0959156018869021, "rouge_l": 0.0}
{"id": 4554, "code": "def update ( self , selector , update , options = None ) : del options user = get object ( self . model , selector [ ' id' ] , pk = this . user id , ) profile update = self . deserialize profile ( update [ '$set' ] , key prefix = 'profile.' , pop = True , ) if len ( update [ '$set' ] ) != 0 : raise Meteor Error ( 400 , 'Invalid update fields: %r' ) for key , val in profile update . items ( ) : setattr ( user , key , val ) user . save ( )", "predictions": ["update the key for this object using the specified selector ."], "references": ["update user data ."], "bleu": 0.12605968092174913, "rouge_l": 0.2911694510739857}
{"id": 4555, "code": "def auth failed ( * * credentials ) : if credentials : user login failed . send robust ( sender = name , credentials = auth . clean credentials ( credentials ) , ) raise Meteor Error ( 403 , 'Authentication failed.' )", "predictions": ["sends an auth - authenticate auth message to the server ."], "references": ["consistent fail so we don t provide attackers with valuable info ."], "bleu": 0.10400927574124633, "rouge_l": 0.08628005657708629}
{"id": 4556, "code": "def validated user ( cls , token , purpose , minutes valid ) : try : username , auth hash = loads ( token . decode ( 'base64' ) ) except ( Value Error , Error ) : cls . auth failed ( token = token ) try : user = cls . user model . objects . get ( * * { cls . user model . USERNAME FIELD : username , 'is active' : True , } ) user . backend = 'django.contrib.auth.backends.Model Backend' except cls . user model . Does Not Exist : cls . auth failed ( username = username , token = token ) if auth hash not in iter auth hashes ( user , purpose , minutes valid ) : cls . auth failed ( username = username , token = token ) return user", "predictions": ["validated : user authentication and validated an auth user must be a username and username ."], "references": ["resolve and validate auth token returns user object ."], "bleu": 0.09672649511413092, "rouge_l": 0.33701657458563533}
{"id": 4557, "code": "def check secure ( ) : if this . request . is secure ( ) : return True elif this . request . META [ 'REMOTE ADDR' ] in [ 'localhost' , '127.0.0.1' , ] : return True raise Meteor Error ( 403 , 'Authentication refused without SSL.' )", "predictions": ["check the secure request ."], "references": ["check request return false if using ssl or local connection ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 4558, "code": "def get username ( self , user ) : if isinstance ( user , basestring ) : return user elif isinstance ( user , dict ) and len ( user ) == 1 : [ ( key , val ) ] = user . items ( ) if key == 'username' or ( key == self . user model . USERNAME FIELD ) : return val elif key in ( 'email' , 'emails.address' ) : email field = getattr ( self . user model , 'EMAIL FIELD' , 'email' ) if self . user model . USERNAME FIELD == email field : return val return self . user model . objects . values list ( self . user model . USERNAME FIELD , flat = True , ) . get ( * * { email field : val } ) elif key in ( 'id' , 'pk' ) : return self . user model . objects . values list ( self . user model . USERNAME FIELD , flat = True , ) . get ( pk = val , ) else : raise Meteor Error ( 400 , 'Invalid user lookup: %r' % key ) else : raise Meteor Error ( 400 , 'Invalid user expression: %r' % user )", "predictions": ["creates a user with the specified key , key and email address ."], "references": ["retrieve username from user selector ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 4559, "code": "def create user ( self , params ) : receivers = create user . send ( sender = name , request = this . request , params = params , ) if len ( receivers ) == 0 : raise Not Implemented Error ( 'Handler for `create user` not registered.' ) user = receivers [ 0 ] [ 1 ] user = auth . authenticate ( username = user . get username ( ) , password = params [ 'password' ] , ) self . do login ( user ) return get user token ( user = user , purpose = Hash Purpose . RESUME LOGIN , minutes valid = HASH MINUTES VALID [ Hash Purpose . RESUME LOGIN ] , )", "predictions": ["create a user on the application . this is called by the order in the order that the user has passed to the client ."], "references": ["register a new user account ."], "bleu": 0.057783239927083445, "rouge_l": 0.21759809750297268}
{"id": 4560, "code": "def do login ( self , user ) : this . user id = user . pk this . user ddp id = get meteor id ( user ) this . user sub id = meteor random id ( ) API . do sub ( this . user sub id , 'Logged In User' , silent = True ) self . update subs ( user . pk ) user logged in . send ( sender = user . class , request = this . request , user = user , )", "predictions": ["seconds for a specified user ."], "references": ["login a user ."], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 4561, "code": "def do logout ( self ) : API . do unsub ( this . user sub id , silent = True ) del this . user sub id self . update subs ( None ) user logged out . send ( sender = self . user model , request = this . request , user = this . user , ) this . user id = None this . user ddp id = None", "predictions": ["estimate the user out of the request ."], "references": ["logout a user ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 4562, "code": "def login ( self , params ) : if 'password' in params : return self . login with password ( params ) elif 'resume' in params : return self . login with resume token ( params ) else : self . auth failed ( * * params )", "predictions": ["begins the request . the uid will be stored on the request ."], "references": ["login either with resume token or password ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 4563, "code": "def login with password ( self , params ) : self . check secure ( ) username = self . get username ( params [ 'user' ] ) password = self . get password ( params [ 'password' ] ) user = auth . authenticate ( username = username , password = password ) if user is not None : if user . is active : self . do login ( user ) return get user token ( user = user , purpose = Hash Purpose . RESUME LOGIN , minutes valid = HASH MINUTES VALID [ Hash Purpose . RESUME LOGIN ] , ) self . auth failed ( )", "predictions": ["login to the user ."], "references": ["authenticate using credentials supplied in params ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4564, "code": "def forgot password ( self , params ) : username = self . get username ( params ) try : user = self . user model . objects . get ( * * { self . user model . USERNAME FIELD : username , } ) except self . user model . Does Not Exist : self . auth failed ( ) minutes valid = HASH MINUTES VALID [ Hash Purpose . PASSWORD RESET ] token = get user token ( user = user , purpose = Hash Purpose . PASSWORD RESET , minutes valid = minutes valid , ) forgot password . send ( sender = name , user = user , token = token , request = this . request , expiry date = calc expiry time ( minutes valid ) , )", "predictions": ["wrap a password object ."], "references": ["request password reset email ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4565, "code": "def reset password ( self , token , new password ) : user = self . validated user ( token , purpose = Hash Purpose . PASSWORD RESET , minutes valid = HASH MINUTES VALID [ Hash Purpose . PASSWORD RESET ] , ) user . set password ( new password ) user . save ( ) self . do login ( user ) return { \"user Id\" : this . user ddp id }", "predictions": ["resets a users password ."], "references": ["reset password using a token received in email then logs user in ."], "bleu": 0.06554932163900559, "rouge_l": 0.20573355817875214}
{"id": 4566, "code": "def read ( path , default = None , encoding = 'utf8' ) : if not path : return default try : with io . open ( path , mode = 'r' , encoding = encoding ) as contents : return contents . read ( ) except IO Error : if default is not None : return default raise", "predictions": ["read a unicode object from the given file or directory ."], "references": ["read encoded contents from specified path or return default ."], "bleu": 0.14323145079400493, "rouge_l": 0.384251968503937}
{"id": 4567, "code": "def get meteor id ( obj or model , obj pk = None ) : if obj or model is None : return None meta = obj or model . meta model = meta . model if model is Object Mapping : raise Type Error ( \"Can't map Object Mapping instances through self.\" ) if isinstance ( obj or model , model ) : if isinstance ( meta . pk , Alea Id Field ) : return obj or model . pk if obj pk is None : obj pk = str ( obj or model . pk ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique ] if len ( alea unique fields ) == 1 : aid = alea unique fields [ 0 ] . attname if isinstance ( obj or model , model ) : val = getattr ( obj or model , aid ) elif obj pk is None : val = None else : val = model . objects . values list ( aid , flat = True ) . get ( pk = obj pk , ) if val : return val if obj pk is None : return None content type = Content Type . objects . get for model ( model ) try : return Object Mapping . objects . values list ( 'meteor id' , flat = True , ) . get ( content type = content type , object id = obj pk , ) except Object Does Not Exist : return Object Mapping . objects . create ( content type = content type , object id = obj pk , meteor id = meteor random id ( '/collection/%s' % meta ) , ) . meteor id", "predictions": ["find a meteor by either a unary or a model ."], "references": ["return an alea id for the given object ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 4568, "code": "def get meteor ids ( model , object ids ) : meta = model . meta result = collections . Ordered Dict ( ( str ( obj pk ) , None ) for obj pk in object ids ) if isinstance ( meta . pk , Alea Id Field ) : return collections . Ordered Dict ( ( obj pk , obj pk ) for obj pk in object ids ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] if len ( alea unique fields ) == 1 : aid = alea unique fields [ 0 ] . name query = model . objects . filter ( pk in = object ids , ) . values list ( 'pk' , aid ) else : content type = Content Type . objects . get for model ( model ) query = Object Mapping . objects . filter ( content type = content type , object id in = list ( result ) ) . values list ( 'object id' , 'meteor id' ) for obj pk , meteor id in query : result [ str ( obj pk ) ] = meteor id for obj pk , meteor id in result . items ( ) : if meteor id is None : result [ obj pk ] = get meteor id ( model , obj pk ) return result", "predictions": ["find all meteor of the specified model"], "references": ["return alea id mapping for all given ids of specified model ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 4569, "code": "def get object id ( model , meteor id ) : if meteor id is None : return None meta = model . meta if model is Object Mapping : raise Type Error ( \"Can't map Object Mapping instances through self.\" ) if isinstance ( meta . pk , Alea Id Field ) : return meteor id alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique ] if len ( alea unique fields ) == 1 : val = model . objects . values list ( 'pk' , flat = True , ) . get ( * * { alea unique fields [ 0 ] . attname : meteor id , } ) if val : return val content type = Content Type . objects . get for model ( model ) return Object Mapping . objects . filter ( content type = content type , meteor id = meteor id , ) . values list ( 'object id' , flat = True ) . get ( )", "predictions": ["get a pair of ( from an id"], "references": ["return an object id for the given meteor_id ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 4570, "code": "def get object ids ( model , meteor ids ) : if model is Object Mapping : raise Type Error ( \"Can't map Object Mapping instances through self.\" ) meta = model . meta alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] result = collections . Ordered Dict ( ( str ( meteor id ) , None ) for meteor id in meteor ids ) if len ( alea unique fields ) == 1 : aid = alea unique fields [ 0 ] . name query = model . objects . filter ( * * { '%s in' % aid : meteor ids , } ) . values list ( aid , 'pk' ) else : content type = Content Type . objects . get for model ( model ) query = Object Mapping . objects . filter ( content type = content type , meteor id in = meteor ids , ) . values list ( 'meteor id' , 'object id' ) for meteor id , object id in query : result [ meteor id ] = object id return result", "predictions": ["get from ("], "references": ["return all object ids for the given meteor_ids ."], "bleu": 0.06114461654585454, "rouge_l": 0.0}
{"id": 4571, "code": "def get object ( model , meteor id , * args , * * kwargs ) : meta = model . meta if isinstance ( meta . pk , Alea Id Field ) : return model . objects . filter ( * args , * * kwargs ) . get ( pk = meteor id ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] if len ( alea unique fields ) == 1 : return model . objects . filter ( * args , * * kwargs ) . get ( * * { alea unique fields [ 0 ] . name : meteor id , } ) return model . objects . filter ( * args , * * kwargs ) . get ( pk = get object id ( model , meteor id ) , )", "predictions": ["find the objects in a model by arguments ."], "references": ["return an object for the given meteor_id ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4572, "code": "def get pk value on save ( self , instance ) : value = super ( Alea Id Field , self ) . get pk value on save ( instance ) if not value : value = self . get seeded value ( instance ) return value", "predictions": ["this is called to get the value of the queue ."], "references": ["generate id if required ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 4573, "code": "def pre save ( self , model instance , add ) : value = super ( Alea Id Field , self ) . pre save ( model instance , add ) if ( not value ) and self . default in ( meteor random id , NOT PROVIDED ) : value = self . get seeded value ( model instance ) setattr ( model instance , self . attname , value ) return value", "predictions": [". . . this is only used when the ( is called ."], "references": ["generate id if required ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 4574, "code": "def set default forwards ( app name , operation , apps , schema editor ) : model = apps . get model ( app name , operation . model name ) for obj pk in model . objects . values list ( 'pk' , flat = True ) : model . objects . filter ( pk = obj pk ) . update ( * * { operation . name : get meteor id ( model , obj pk ) , } )", "predictions": ["create a new forwards object ."], "references": ["set default value for aleaidfield ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 4575, "code": "def set default reverse ( app name , operation , apps , schema editor ) : model = apps . get model ( app name , operation . model name ) for obj pk in model . objects . values list ( 'pk' , flat = True ) : get meteor id ( model , obj pk )", "predictions": ["creates a new obj ."], "references": ["unset default value for aleaidfield ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4576, "code": "def database forwards ( self , app label , schema editor , from state , to state ) : self . truncate ( app label , schema editor , self . truncate forwards )", "predictions": ["add an , to the default locale ."], "references": ["use schema_editor to apply any forward changes ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4577, "code": "def database backwards ( self , app label , schema editor , from state , to state ) : self . truncate ( app label , schema editor , self . truncate backwards )", "predictions": ["run a process and its contents ."], "references": ["use schema_editor to apply any reverse changes ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4578, "code": "def initialize options ( self ) : setuptools . command . build py . build py . initialize options ( self ) self . meteor = 'meteor' self . meteor debug = False self . build lib = None self . package dir = None self . meteor builds = [ ] self . no prune npm = None self . inplace = True", "predictions": ["initializes the ( include a . , . , . , . , . , : . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . ,"], "references": ["set command option defaults ."], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 4579, "code": "def finalize options ( self ) : self . set undefined options ( 'build' , ( 'build lib' , 'build lib' ) , ) self . set undefined options ( 'build py' , ( 'package dir' , 'package dir' ) , ) setuptools . command . build py . build py . finalize options ( self )", "predictions": ["create and adds a list of file file to this object ."], "references": ["update command options ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 4580, "code": "def path to dir ( * path args ) : return os . path . join ( * list ( path args [ : - 1 ] ) + path args [ - 1 ] . split ( posixpath . sep ) )", "predictions": ["returns the path directory path ."], "references": ["convert a unix - style path into platform specific directory spec ."], "bleu": 0.09663861439684919, "rouge_l": 0.31443298969072164}
{"id": 4581, "code": "def seed ( self , values ) : if not values : seed ids = [ int , str , random , self , values , self . class ] random . shuffle ( seed ids ) values = list ( map ( id , seed ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] mash = Mash ( ) self . c = 1 self . s0 = mash ( ' ' ) self . s1 = mash ( ' ' ) self . s2 = mash ( ' ' ) for val in values : self . s0 -= mash ( val ) if self . s0 < 0 : self . s0 += 1 self . s1 -= mash ( val ) if self . s1 < 0 : self . s1 += 1 self . s2 -= mash ( val ) if self . s2 < 0 : self . s2 += 1", "predictions": ["alias for ( . must be called multiple times ."], "references": ["seed internal state from supplied values ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 4582, "code": "def state ( self ) : return { 'c' : self . c , 's0' : self . s0 , 's1' : self . s1 , 's2' : self . s2 }", "predictions": ["a calc calc method ."], "references": ["return internal state useful for testing ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4583, "code": "def random string ( self , length , alphabet ) : return '' . join ( self . choice ( alphabet ) for n in range ( length ) )", "predictions": ["generate a get around the specified string ."], "references": ["return string of length elements chosen from alphabet ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 4584, "code": "def api endpoints ( obj ) : for name in dir ( obj ) : attr = getattr ( obj , name ) api path = getattr ( attr , 'api path' , None ) if api path : yield ( '%s%s' % ( obj . api path prefix , api path ) , attr , ) for api provider in obj . api providers : for api path , attr in api endpoints ( api provider ) : yield ( api path , attr )", "predictions": ["generate an iterator of all ( of the given parents ."], "references": ["iterator over all api endpoint names and callbacks ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 4585, "code": "def clear api path map cache ( self ) : self . api path cache = None for api provider in self . api providers : if six . get method self ( api provider . clear api path map cache , ) is not None : api provider . clear api path map cache ( )", "predictions": ["deserialize the key set ."], "references": ["clear out cache for api_path_map ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4586, "code": "def dprint ( name , val ) : from pprint import pformat print ( '% 5s: %s' % ( name , '\\n       ' . join ( pformat ( val , indent = 4 , width = 75 , ) . split ( '\\n' ) ) , ) , )", "predictions": ["print a pretty print ."], "references": ["debug print name and val ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 4587, "code": "def validate kwargs ( func , kwargs ) : func name = func . name argspec = inspect . getargspec ( func ) all args = argspec . args [ : ] defaults = list ( argspec . defaults or [ ] ) if inspect . ismethod ( func ) and all args [ : 1 ] == [ 'self' ] : all args [ : 1 ] = [ ] if defaults : required = all args [ : - len ( defaults ) ] else : required = all args [ : ] trans = { arg : arg . endswith ( ' ' ) and arg [ : - 1 ] or arg for arg in all args } for key in list ( kwargs ) : key adj = '%s ' % key if key adj in all args : kwargs [ key adj ] = kwargs . pop ( key ) supplied = sorted ( kwargs ) missing = [ trans . get ( arg , arg ) for arg in required if arg not in supplied ] if missing : raise Meteor Error ( 400 , func . err , 'Missing required arguments to %s: %s' % ( func name , ' ' . join ( missing ) , ) , ) extra = [ arg for arg in supplied if arg not in all args ] if extra : raise Meteor Error ( 400 , func . err , 'Unknown arguments to %s: %s' % ( func name , ' ' . join ( extra ) ) , )", "predictions": ["auth - keyword arguments - keyword arguments - keyword arguments - keyword arguments - keyword - keyword arguments - keyword - required - , - , - , - , - , - , - , - , - , - , - , - , - , - ,"], "references": ["validate arguments to be supplied to func ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 4588, "code": "def on open ( self ) : this . request = WSGI Request ( self . ws . environ ) this . ws = self this . send = self . send this . reply = self . reply self . logger = self . ws . logger self . remote ids = collections . defaultdict ( set ) self . tx buffer = { } self . tx buffer id gen = itertools . cycle ( irange ( sys . maxint ) ) self . tx next id gen = itertools . cycle ( irange ( sys . maxint ) ) self . tx next id = next ( self . tx next id gen ) this . remote addr = self . remote addr = '{0[REMOTE ADDR]}:{0[REMOTE PORT]}' . format ( self . ws . environ , ) this . subs = { } safe call ( self . logger . info , '+ %s OPEN' , self ) self . send ( 'o' ) self . send ( 'a[\"{\\\\\"server id\\\\\":\\\\\"0\\\\\"}\"]' )", "predictions": ["the method to hash the user about the request ."], "references": ["handle new websocket connection ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 4589, "code": "def on close ( self , * args , * * kwargs ) : if self . connection is not None : del self . pgworker . connections [ self . connection . pk ] self . connection . delete ( ) self . connection = None signals . request finished . send ( sender = self . class ) safe call ( self . logger . info , '- %s %s' , self , args or 'CLOSE' )", "predictions": ["closes the torrent connection ."], "references": ["handle closing of websocket connection ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 4590, "code": "def on message ( self , message ) : if self . ws . closed : return None try : safe call ( self . logger . debug , '< %s %r' , self , message ) for data in self . ddp frames from message ( message ) : self . process ddp ( data ) signals . request finished . send ( sender = self . class ) except geventwebsocket . Web Socket Error : self . ws . close ( )", "predictions": ["send a username to the game ."], "references": ["process a message received from remote ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4591, "code": "def ddp frames from message ( self , message ) : try : msgs = ejson . loads ( message ) except Value Error : self . reply ( 'error' , error = 400 , reason = 'Data is not valid EJSON' , ) raise Stop Iteration if not isinstance ( msgs , list ) : self . reply ( 'error' , error = 400 , reason = 'Invalid EJSON messages' , ) raise Stop Iteration while msgs : raw = msgs . pop ( 0 ) try : data = ejson . loads ( raw ) except ( Type Error , Value Error ) : data = None if not isinstance ( data , dict ) : self . reply ( 'error' , error = 400 , reason = 'Invalid Sock JS DDP payload' , offending Message = raw , ) yield data if msgs : gevent . sleep ( )", "predictions": ["iterates through the self - blocking user and returns a multiple of the self - formatted self ."], "references": ["yield ddp messages from a raw websocket message ."], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 4592, "code": "def process ddp ( self , data ) : msg id = data . get ( 'id' , None ) try : msg = data . pop ( 'msg' ) except Key Error : self . reply ( 'error' , reason = 'Bad request' , offending Message = data , ) return try : self . dispatch ( msg , data ) except Exception as err : kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } if msg id is not None : kwargs [ 'id' ] = msg id if isinstance ( err , Meteor Error ) : error = err . as dict ( ) else : error = { 'error' : 500 , 'reason' : 'Internal server error' , } if kwargs [ 'msg' ] == 'error' : kwargs . update ( error ) else : kwargs [ 'error' ] = error if not isinstance ( err , Meteor Error ) : stack , = safe call ( self . logger . error , '%r %r' , msg , data , exc info = 1 , ) if stack is not None : traceback . print exc ( file = sys . stderr ) sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\\n' ) sys . stderr . write ( stack ) elif settings . DEBUG : print ( 'ERROR: %s' % err ) dprint ( 'msg' , msg ) dprint ( 'data' , data ) error . setdefault ( 'details' , traceback . format exc ( ) ) print ( error [ 'details' ] ) self . reply ( * * kwargs ) if msg id and msg == 'method' : self . reply ( 'updated' , methods = [ msg id ] )", "predictions": ["for processing ( or all ) user reports the error with the login error ."], "references": ["process a single ddp message ."], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 4593, "code": "def dispatch ( self , msg , kwargs ) : if self . connection is None and msg != 'connect' : self . reply ( 'error' , reason = 'Must connect first' ) return if msg == 'method' : if ( 'method' not in kwargs ) or ( 'id' not in kwargs ) : self . reply ( 'error' , error = 400 , reason = 'Malformed method invocation' , ) return try : handler = getattr ( self , 'recv %s' % msg ) except ( Attribute Error , Unicode Encode Error ) : raise Meteor Error ( 404 , 'Method not found' ) validate kwargs ( handler , kwargs ) handler ( * * kwargs )", "predictions": ["do we want to do our ( for this class ."], "references": ["dispatch msg to appropriate recv_foo handler ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 4594, "code": "def recv connect ( self , version = None , support = None , session = None ) : del session if self . connection is not None : raise Meteor Error ( 400 , 'Session already established.' , self . connection . connection id , ) elif None in ( version , support ) or version not in self . versions : self . reply ( 'failed' , version = self . versions [ 0 ] ) elif version not in support : raise Meteor Error ( 400 , 'Client version/support mismatch.' ) else : from dddp . models import Connection cur = connection . cursor ( ) cur . execute ( 'SELECT pg backend pid()' ) ( backend pid , ) = cur . fetchone ( ) this . version = version this . support = support self . connection = Connection . objects . create ( server addr = '%d:%s' % ( backend pid , self . ws . handler . socket . getsockname ( ) , ) , remote addr = self . remote addr , version = version , ) self . pgworker . connections [ self . connection . pk ] = self atexit . register ( self . on close , 'Shutting down.' ) self . reply ( 'connected' , session = self . connection . connection id )", "predictions": ["( re return return return return return return a push connection to the server ."], "references": ["ddp connect handler ."], "bleu": 0.08225964699966554, "rouge_l": 0.11753371868978806}
{"id": 4595, "code": "def recv ping ( self , id = None ) : if id is None : self . reply ( 'pong' ) else : self . reply ( 'pong' , id = id )", "predictions": ["with a new then use the get method ."], "references": ["ddp ping handler ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4596, "code": "def recv sub ( self , id , name , params ) : self . api . sub ( id , name , * params )", "predictions": ["a method to receive notifications of a class and no longer chance ."], "references": ["ddp sub handler ."], "bleu": 0.09552040806823771, "rouge_l": 0.1300639658848614}
{"id": 4597, "code": "def recv unsub ( self , id = None ) : if id : self . api . unsub ( id ) else : self . reply ( 'nosub' )", "predictions": ["get a ( for this object ."], "references": ["ddp unsub handler ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4598, "code": "def recv method ( self , method , params , id , random Seed = None ) : if random Seed is not None : this . random streams . random seed = random Seed this . alea random = alea . Alea ( random Seed ) self . api . method ( method , params , id ) self . reply ( 'updated' , methods = [ id ] )", "predictions": ["construct a new example object ."], "references": ["ddp method handler ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4599, "code": "def ddpp sockjs info ( environ , start response ) : import random import ejson start response ( '200 OK' , [ ( 'Content-Type' , 'application/json; charset=UTF-8' ) , ] + common headers ( environ ) , ) yield ejson . dumps ( collections . Ordered Dict ( [ ( 'websocket' , True ) , ( 'origins' , [ '*:*' , ] ) , ( 'cookie needed' , False ) , ( 'entropy' , random . getrandbits ( 32 ) ) , ] ) )", "predictions": ["creates a get from the specified id . the get method is called to provide an http response from the server before being read ."], "references": ["inform client that websocket service is available ."], "bleu": 0.05377336385080629, "rouge_l": 0.13362541073384449}
{"id": 4600, "code": "def serve ( listen , verbosity = 1 , debug port = 0 , * * ssl args ) : launcher = DDP Launcher ( debug = verbosity == 3 , verbosity = verbosity ) if debug port : launcher . servers . append ( launcher . get backdoor server ( 'localhost:%d' % debug port ) ) launcher . add web servers ( listen , * * ssl args ) sigmap = { val : name for name , val in vars ( signal ) . items ( ) if name . startswith ( 'SIG' ) } def sighandler ( signum = None , frame = None ) : \"\"\"Signal handler\"\"\" launcher . logger . info ( 'Received signal %s in frame %r' , sigmap . get ( signum , signum ) , frame , ) launcher . stop ( ) for signum in [ signal . SIGINT , signal . SIGQUIT ] : gevent . signal ( signum , sighandler ) launcher . run ( )", "predictions": ["creates all launcher waiting for the given launcher ."], "references": ["spawn greenlets for handling websockets and postgresql calls ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 4601, "code": "def main ( ) : parser = argparse . Argument Parser ( description = doc ) django = parser . add argument group ( 'Django Options' ) django . add argument ( '--verbosity' , '-v' , metavar = 'VERBOSITY' , dest = 'verbosity' , type = int , default = 1 , ) django . add argument ( '--debug-port' , metavar = 'DEBUG PORT' , dest = 'debug port' , type = int , default = 0 , ) django . add argument ( '--settings' , metavar = 'SETTINGS' , dest = 'settings' , help = \"The Python path to a settings module, e.g. \" \"\\\"myproject.settings.main\\\". If this isn't provided, the \" \"DJANGO SETTINGS MODULE environment variable will be used.\" , ) http = parser . add argument group ( 'HTTP Options' ) http . add argument ( 'listen' , metavar = 'address[:port]' , nargs = '*' , type = addr , help = 'Listening address for HTTP(s) server.' , ) ssl = parser . add argument group ( 'SSL Options' ) ssl . add argument ( '--ssl-version' , metavar = 'SSL VERSION' , dest = 'ssl version' , help = \"SSL version to use (see stdlib ssl module's) [3]\" , choices = [ '1' , '2' , '3' ] , default = '3' ) ssl . add argument ( '--certfile' , metavar = 'FILE' , dest = 'certfile' , help = \"SSL certificate file [None]\" ) ssl . add argument ( '--ciphers' , metavar = 'CIPHERS' , dest = 'ciphers' , help = \"Ciphers to use (see stdlib ssl module's) [TL Sv1]\" ) ssl . add argument ( '--ca-certs' , metavar = 'FILE' , dest = 'ca certs' , help = \"CA certificates file [None]\" ) ssl . add argument ( '--keyfile' , metavar = 'FILE' , dest = 'keyfile' , help = \"SSL key file [None]\" ) namespace = parser . parse args ( ) if namespace . settings : os . environ [ 'DJANGO SETTINGS MODULE' ] = namespace . settings serve ( namespace . listen or [ Addr ( 'localhost' , 8000 ) ] , debug port = namespace . debug port , keyfile = namespace . keyfile , certfile = namespace . certfile , verbosity = namespace . verbosity , )", "predictions": ["creates a new bigdata reporting help help task ."], "references": ["main entry point for dddp command ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4602, "code": "def print ( self , msg , * args , * * kwargs ) : if self . verbosity >= 1 : print ( msg , * args , * * kwargs )", "predictions": ["prints the message arguments of the arguments ."], "references": ["print formatted msg if verbosity set at 1 or above ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 4603, "code": "def stop ( self ) : self . logger . debug ( 'Postgres Greenlet stop' ) self . stop event . set ( ) for server in self . servers + [ DDP Launcher . pgworker ] : self . logger . debug ( 'Stopping %s' , server ) server . stop ( ) gevent . joinall ( self . threads + [ DDP Launcher . pgworker ] ) self . threads = [ ]", "predictions": ["get all fields and field field ."], "references": ["stop all green threads ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4604, "code": "def run ( self ) : self . logger . debug ( 'Postgres Greenlet run' ) self . start ( ) self . stop event . wait ( ) gevent . joinall ( self . threads + [ DDP Launcher . pgworker ] ) self . threads = [ ]", "predictions": ["a simple method that gets executed by the caller ' s class loader ."], "references": ["run ddp greenlets ."], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 4605, "code": "def run ( self ) : conn params = self . connection . get connection params ( ) conn params . update ( async = True , application name = '{} pid={} django-ddp' . format ( socket . gethostname ( ) , os . getpid ( ) , ) [ : 64 ] , ) conn = None while conn is None : try : conn = psycopg2 . connect ( * * conn params ) except psycopg2 . Operational Error as err : msg = ( '%s' % err ) . strip ( ) msg prefix = 'invalid connection option \"' if not msg . startswith ( msg prefix ) : raise key = msg [ len ( msg prefix ) : - 1 ] self . logger . warning ( 'Ignoring unknown settings.DATABASES[%r] option: %s=%r' , self . connection . alias , key , conn params . pop ( key ) , ) self . poll ( conn ) import logging logging . get Logger ( 'dddp' ) . info ( '=> Started Postgres Greenlet.' ) cur = conn . cursor ( ) cur . execute ( 'LISTEN \"ddp\";' ) while not self . stop event . is set ( ) : try : self . select greenlet = gevent . spawn ( gevent . select . select , [ conn ] , [ ] , [ ] , timeout = None , ) self . select greenlet . get ( ) except gevent . Greenlet Exit : self . stop event . set ( ) finally : self . select greenlet = None self . poll ( conn ) self . poll ( conn ) cur . close ( ) self . poll ( conn ) conn . close ( )", "predictions": ["running process with messages ."], "references": ["spawn sub tasks wait for stop signal ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4606, "code": "def poll ( self , conn ) : while 1 : state = conn . poll ( ) if state == psycopg2 . extensions . POLL OK : while conn . notifies : notify = conn . notifies . pop ( ) self . logger . info ( \"Got NOTIFY (pid=%d, payload=%r)\" , notify . pid , notify . payload , ) hdr , chunk = notify . payload . split ( '|' , 1 ) header = ejson . loads ( hdr ) uuid = header [ 'uuid' ] size , chunks = self . chunks . setdefault ( uuid , [ 0 , { } ] ) if header [ 'fin' ] : size = self . chunks [ uuid ] [ 0 ] = header [ 'seq' ] chunks [ header [ 'seq' ] ] = chunk if len ( chunks ) != size : continue data = '' . join ( chunk for , chunk in sorted ( chunks . items ( ) ) ) del self . chunks [ uuid ] data = ejson . loads ( data ) sender = data . pop ( ' sender' , None ) tx id = data . pop ( ' tx id' , None ) for connection id in data . pop ( ' connection ids' ) : try : websocket = self . connections [ connection id ] except Key Error : continue if connection id == sender : websocket . send ( data , tx id = tx id ) else : websocket . send ( data ) break elif state == psycopg2 . extensions . POLL WRITE : gevent . select . select ( [ ] , [ conn . fileno ( ) ] , [ ] ) elif state == psycopg2 . extensions . POLL READ : gevent . select . select ( [ conn . fileno ( ) ] , [ ] , [ ] ) else : self . logger . warn ( 'POLL ERR: %s' , state )", "predictions": ["sends connection for websocket ."], "references": ["poll db socket and process async tasks ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4607, "code": "def greenify ( ) : if GREEN : return GREEN [ True ] = True from gevent . monkey import patch all , saved if ( 'threading' in sys . modules ) and ( 'threading' not in saved ) : import warnings warnings . warn ( 'threading module loaded before patching!' ) patch all ( ) try : import psycopg2 del psycopg2 except Import Error : from psycopg2cffi import compat compat . register ( ) from psycogreen . gevent import patch psycopg patch psycopg ( )", "predictions": ["completes all : 1 . 2 . 3 . 3 . 3 . 3 . 1 . 3 . 1 . 3 . 3 . 3 . 3 . 3 . 1 . 3 . 3 . 1 . 3 . 3 . 3 . 3 . 3 . 3"], "references": ["patch threading and psycopg2 modules for green threads ."], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 4608, "code": "def meteor random id ( name = None , length = 17 ) : if name is None : stream = THREAD LOCAL . alea random else : stream = THREAD LOCAL . random streams [ name ] return stream . random string ( length , METEOR ID CHARS )", "predictions": ["generate a random meteor with the given name ."], "references": ["generate a new id optionally using namespace of given name ."], "bleu": 0.25014555519576226, "rouge_l": 0.4911433172302737}
{"id": 4609, "code": "def autodiscover ( ) : from django . utils . module loading import autodiscover modules from dddp . api import API autodiscover modules ( 'ddp' , register to = API ) return API", "predictions": ["returns a new auto-discover ."], "references": ["import all ddp submodules from settings . installed_apps ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4610, "code": "def as dict ( self , * * kwargs ) : error , reason , details , err kwargs = self . args result = { key : val for key , val in { 'error' : error , 'reason' : reason , 'details' : details , } . items ( ) if val is not None } result . update ( err kwargs ) result . update ( kwargs ) return result", "predictions": ["copies the result of the operation into a new dict ."], "references": ["return an error dict for self . args and kwargs ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 4611, "code": "def get ( self , name , factory , * factory args , * * factory kwargs ) : update thread local = getattr ( factory , 'update thread local' , True ) if ( not update thread local ) or ( name not in self . dict ) : obj = factory ( * factory args , * * factory kwargs ) if update thread local : setattr ( self , name , obj ) return obj return getattr ( self , name )", "predictions": ["returns a method for the object ."], "references": ["get attribute creating if required using specified factory ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4612, "code": "def emit ( self , record ) : if getattr ( this , 'subs' , { } ) . get ( LOGS NAME , False ) : self . format ( record ) this . send ( { 'msg' : ADDED , 'collection' : LOGS NAME , 'id' : meteor random id ( '/collection/%s' % LOGS NAME ) , 'fields' : { attr : { 'args' : lambda args : [ repr ( arg ) for arg in args ] , 'created' : datetime . datetime . fromtimestamp , 'exc info' : stacklines or none , } . get ( attr , lambda val : val ) ( getattr ( record , attr , None ) ) for attr in ( 'args' , 'asctime' , 'created' , 'exc info' , 'filename' , 'func Name' , 'levelname' , 'levelno' , 'lineno' , 'module' , 'msecs' , 'message' , 'name' , 'pathname' , 'process' , 'process Name' , 'relative Created' , 'thread' , 'thread Name' , ) } , } )", "predictions": ["emits a method on the ( ."], "references": ["emit a formatted log record via ddp ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4613, "code": "def send message ( self , message , * * kwargs ) : from . . libs . gcm import gcm send message data = kwargs . pop ( \"extra\" , { } ) if message is not None : data [ \"message\" ] = message return gcm send message ( registration id = self . registration id , data = data , * * kwargs )", "predictions": ["sends a message to the already - running message and sends it to the connection ."], "references": ["sends a push notification to this device via gcm"], "bleu": 0.10878661088699644, "rouge_l": 0.2527624309392265}
{"id": 4614, "code": "def gcm send message ( registration id , data , encoding = 'utf-8' , * * kwargs ) : messenger = GCM Messenger ( registration id , data , encoding = encoding , * * kwargs ) return messenger . send plain ( )", "predictions": ["send a broadcast to the server ."], "references": ["standalone method to send a single gcm notification"], "bleu": 0.22772101321113858, "rouge_l": 0.2634989200863931}
{"id": 4615, "code": "def gcm send bulk message ( registration ids , data , encoding = 'utf-8' , * * kwargs ) : messenger = GCM Messenger ( registration ids , data , encoding = encoding , * * kwargs ) return messenger . send bulk ( )", "predictions": ["send a gcm message with the given registration ."], "references": ["standalone method to send bulk gcm notifications"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4616, "code": "def send json ( self , ids = None ) : items = ids or self . registration id values = { \"registration ids\" : items } if self . data is not None : values [ \"data\" ] = self . data for key , val in self . kwargs . items ( ) : if val : values [ key ] = val data = json . dumps ( values , separators = ( \",\" , \":\" ) , sort keys = True ) . encode ( self . encoding ) result = json . loads ( self . send ( data , \"application/json\" ) ) if ( \"failure\" in result ) and ( result [ \"failure\" ] ) : unregistered = [ ] throw error = False for index , error in enumerate ( result . get ( \"results\" , [ ] ) ) : error = error . get ( \"error\" , \"\" ) if error in ( \"Not Registered\" , \"Invalid Registration\" ) : unregistered . append ( items [ index ] ) elif error != \"\" : throw error = True self . deactivate unregistered devices ( unregistered ) if throw error : raise GCM Push Error ( result ) return result", "predictions": ["sends a batch of ( from all registered separators to the provided registration ."], "references": ["sends a json gcm message"], "bleu": 0.11633270842295028, "rouge_l": 0.23018867924528305}
{"id": 4617, "code": "def send ( self , data , content type ) : headers = { \"Content-Type\" : content type , \"Authorization\" : \"key=%s\" % ( self . api key ) , \"Content-Length\" : str ( len ( data ) ) } request = Request ( self . api url , data , headers ) return urlopen ( request ) . read ( ) . decode ( self . encoding )", "predictions": ["send an . request to the request and set the response as request ."], "references": ["sends a gcm message with the given content type"], "bleu": 0.08839374326825923, "rouge_l": 0.09050445103857567}
{"id": 4618, "code": "def get model ( module location ) : if not isinstance ( module location , ( str , unicode ) ) : raise Value Error ( \"The value provided should either be a string or \" \"unicode instance. The value '%s' provided was %s \" \"rather.\" % ( module location , type ( module location ) ) ) try : name split = module location . split ( \".\" ) class name = name split . pop ( - 1 ) if not len ( name split ) : raise Value Error ( \"The value should provide the module location \" \"joined by '.' e.g. for model named 'test' in \" \"/app/module.py, The value should be 'app.module.test'\" ) module location = \".\" . join ( name split ) module = importlib . import module ( module location ) cls = getattr ( module , class name ) return cls except Attribute Error : pass", "predictions": ["get the location from a module ."], "references": ["returns the instance of the given module location ."], "bleu": 0.1755217914979255, "rouge_l": 0.3667334669338677}
{"id": 4619, "code": "def fetch ( self , endpoint name , * * params ) : params [ 'api key' ] = self . api key resp = requests . get ( self . endpoint ( endpoint name ) , params = params ) resp . raise for status ( ) data = resp . json ( ) self . check or raise ( data . get ( 'meta' , { } ) ) return data", "predictions": ["fetch data from this map ."], "references": ["wrapper for making an api request from giphy"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4620, "code": "def video ( request , video id ) : api = Api ( ) api . authenticate ( ) availability = api . check upload status ( video id ) if availability is not True : video = Video . objects . filter ( video id = video id ) . get ( ) state = availability [ \"upload state\" ] if state == \"failed\" or state == \"rejected\" : return render to response ( \"django youtube/video failed.html\" , { \"video\" : video , \"video id\" : video id , \"message\" : ( \"Invalid video.\" ) , \"availability\" : availability } , context instance = Request Context ( request ) ) else : return render to response ( \"django youtube/video unavailable.html\" , { \"video\" : video , \"video id\" : video id , \"message\" : ( \"This video is currently being processed\" ) , \"availability\" : availability } , context instance = Request Context ( request ) ) video params = video params ( request , video id ) return render to response ( \"django youtube/video.html\" , video params , context instance = Request Context ( request ) )", "predictions": ["call the specified video to upload the specified id to the given video ."], "references": ["displays a video in an embed player"], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 4621, "code": "def newick ( self ) : label = self . name or '' if self . length : label += ':' + self . length descendants = ',' . join ( [ n . newick for n in self . descendants ] ) if descendants : descendants = '(' + descendants + ')' return descendants + label", "predictions": ["create a newick method for the newick object ."], "references": ["the representation of the node in newick format ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 4622, "code": "def remove internal names ( self ) : self . visit ( lambda n : setattr ( n , 'name' , None ) , lambda n : not n . is leaf )", "predictions": ["remove this class from the list ."], "references": ["set the name of all non - leaf nodes in the subtree to none ."], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 4623, "code": "def remove leaf names ( self ) : self . visit ( lambda n : setattr ( n , 'name' , None ) , lambda n : n . is leaf )", "predictions": ["this method removes a leaf class ."], "references": ["set the name of all leaf nodes in the subtree to none ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 4624, "code": "def auth required ( realm , auth func ) : def auth decorator ( func ) : def inner ( self , * args , * * kw ) : if self . get authenticated user ( auth func , realm ) : return func ( self , * args , * * kw ) return inner return auth decorator", "predictions": ["decorator that adds a method to a realm ."], "references": ["decorator that protect methods with http authentication ."], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 4625, "code": "def require setting ( self , name , feature = \"this feature\" ) : if name not in self . settings : raise Exception ( \"You must define the '%s' setting in your \" \"application to use %s\" % ( name , feature ) )", "predictions": ["decorator that ensures that the request is a subclass of the requested feature ."], "references": ["raises an exception if the given app setting is not defined ."], "bleu": 0.10511846841633776, "rouge_l": 0.2340153452685422}
{"id": 4626, "code": "def get cookie ( self , name , default = None ) : assert self . cookie monster , 'Cookie Monster not set' return self . cookie monster . get cookie ( name , default )", "predictions": ["get a cookie to be executed ."], "references": ["gets the value of the cookie with the given name else default ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 4627, "code": "def clear cookie ( self , name , path = \"/\" , domain = None ) : assert self . cookie monster , 'Cookie Monster not set' #, path=path, domain=domain) self . cookie monster . delete cookie ( name )", "predictions": ["geo geo cookie in , ."], "references": ["deletes the cookie with the given name ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4628, "code": "def get authenticated user ( self , callback ) : oauth ns = \"\" for name , values in self . request . arguments . iteritems ( ) : if name . startswith ( \"openid.ns.\" ) and values [ - 1 ] == u\"http://specs.openid.net/extensions/oauth/1.0\" : oauth ns = name [ 10 : ] break token = self . get argument ( \"openid.\" + oauth ns + \".request token\" , \"\" ) if token : http = httpclient . Async HTTP Client ( ) token = dict ( key = token , secret = \"\" ) http . fetch ( self . oauth access token url ( token ) , self . async callback ( self . on access token , callback ) ) else : Open Id Mixin . get authenticated user ( self , callback )", "predictions": ["creates an authenticated user object ."], "references": ["fetches the authenticated user data upon redirect ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 4629, "code": "def add ( self , name , value ) : norm name = HTTP Headers . normalize name ( name ) self . last key = norm name if norm name in self : dict . setitem ( self , norm name , self [ norm name ] + ',' + value ) self . as list [ norm name ] . append ( value ) else : self [ norm name ] = value", "predictions": ["add a new ( ."], "references": ["adds a new value for the given key ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 4630, "code": "def get list ( self , name ) : norm name = HTTP Headers . normalize name ( name ) return self . as list . get ( norm name , [ ] )", "predictions": ["get the list of headers ."], "references": ["returns all values for the given header as a list ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 4631, "code": "def select Policy ( self , origin , request method = None ) : ret origin = None policyname = None if self . matchstrategy in ( \"firstmatch\" , \"verbmatch\" ) : for pol in self . activepolicies : policy = self . policies [ pol ] ret origin = None policyname = policy . name if policyname == \"deny\" : break if self . matchstrategy == \"verbmatch\" : if policy . methods != \"*\" and not CORS . matchlist ( request method , policy . methods , case sensitive = True ) : continue if origin and policy . match : if CORS . matchlist ( origin , policy . match ) : ret origin = origin elif policy . origin == \"copy\" : ret origin = origin elif policy . origin : ret origin = policy . origin if ret origin : break return policyname , ret origin", "predictions": ["select a request for an , ."], "references": ["based on the matching strategy and the origin and optionally the requested method a tuple of policyname and origin to pass back is returned ."], "bleu": 0.015710707898395813, "rouge_l": 0.11348837209302326}
{"id": 4632, "code": "def get data from user ( msg type ) : data = { } for k , v in CONFIG [ msg type ] [ \"settings\" ] . items ( ) : data [ k ] = input ( v + \": \" ) return data", "predictions": ["convert user data to json ."], "references": ["get the required settings from the user and return as a dict ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 4633, "code": "def get auth from user ( msg type ) : auth = [ ] for k , v in CONFIG [ msg type ] [ \"auth\" ] . items ( ) : auth . append ( ( k , getpass ( v + \": \" ) ) ) return Ordered Dict ( auth )", "predictions": ["get auth info from session ."], "references": ["get the required auth from the user and return as a dict ."], "bleu": 0.08649595219978225, "rouge_l": 0.39482200647249194}
{"id": 4634, "code": "def construct message ( self ) : self . message [ \"text\" ] = \"\" if self . from : self . message [ \"text\" ] += \"From: \" + self . from + \"\\n\" if self . subject : self . message [ \"text\" ] += \"Subject: \" + self . subject + \"\\n\" self . message [ \"text\" ] += self . body self . add attachments ( )", "predictions": ["construct a message for the existing message ."], "references": ["build the message params ."], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 4635, "code": "def send ( self , encoding = \"json\" ) : self . construct message ( ) if self . verbose : print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) if encoding == \"json\" : resp = requests . post ( self . url , json = self . message ) elif encoding == \"url\" : resp = requests . post ( self . url , data = self . message ) try : resp . raise for status ( ) if resp . history and resp . history [ 0 ] . status code >= 300 : raise Message Send Error ( \"HTTP Redirect: Possibly Invalid authentication\" ) elif \"invalid auth\" in resp . text : raise Message Send Error ( \"Invalid Auth: Possibly Bad Auth Token\" ) except ( requests . exceptions . HTTP Error , Message Send Error ) as e : raise Message Send Error ( e ) if self . verbose : print ( timestamp ( ) , type ( self ) . name , \" info:\" , self . str ( indentation = \"\\n * \" ) , \"\\n * HTTP status code:\" , resp . status code , ) print ( \"Message sent.\" )", "predictions": ["sends an email message to all the appropriate ( ."], "references": ["send the message via http post default is json - encoded ."], "bleu": 0.12273680279953825, "rouge_l": 0.1788856304985337}
{"id": 4636, "code": "def validate input ( msg type , attr , value ) : try : valid = { \"Email\" : validate email , \"Twilio\" : validate twilio , \"Slack Webhook\" : validate slackwebhook , \"Slack Post\" : validate slackpost , \"Telegram Bot\" : validate telegrambot , \"Whats App\" : validate whatsapp , } [ msg type ] ( attr , value ) except Key Error : return 1 else : return 0", "predictions": ["check if this member is a valid cluster of the given ( ."], "references": ["base function to validate input dispatched via message type ."], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 4637, "code": "def validate twilio ( attr , value ) : if attr in ( \"from \" , \"to\" ) : check valid ( \"Twilio\" , attr , value , validus . isphone , \"phone number\" ) elif attr in ( \"attachments\" ) : check valid ( \"Twilio\" , attr , value , validus . isurl , \"url\" )", "predictions": ["validate that all other attributes exist in this instance are supplied as a recursive call to the function ."], "references": ["twilio input validator function ."], "bleu": 0.08475426399505566, "rouge_l": 0.18625954198473282}
{"id": 4638, "code": "def validate slackpost ( attr , value ) : if attr in ( \"channel\" , \"credentials\" ) : if not isinstance ( value , str ) : raise Invalid Message Input Error ( \"Slack Post\" , attr , value , \"string\" ) elif attr in ( \"attachments\" ) : check valid ( \"Slack Post\" , attr , value , validus . isurl , \"url\" )", "predictions": ["validate that there are no special characters in the list of . ."], "references": ["slackpost input validator function ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 4639, "code": "def validate whatsapp ( attr , value ) : if attr in ( \"from \" , \"to\" ) : if value is not None and \"whatsapp:\" in value : value = value . split ( \"whatsapp:+\" ) [ - 1 ] check valid ( \"Whats App\" , attr , value , validus . isint , \"phone number starting with the '+' symbol\" , ) elif attr in ( \"attachments\" ) : check valid ( \"Whats App\" , attr , value , validus . isurl , \"url\" )", "predictions": ["validate that the attribute value is a valid attribute name ."], "references": ["whatsapp input validator function ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 4640, "code": "def add message ( self , msg ) : try : self . coro . send ( msg ) except Attribute Error : raise Unsupported Message Type Error ( msg . class . name )", "predictions": ["adds the specified random random number on the queue ."], "references": ["add a message to the futures executor ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 4641, "code": "def get body from file ( kwds ) : if kwds [ \"file\" ] and os . path . isfile ( kwds [ \"file\" ] ) : kwds [ \"body\" ] = open ( kwds [ \"file\" ] , \"r\" ) . read ( ) kwds [ \"file\" ] = None", "predictions": ["construct a ( from a : bool . object ."], "references": ["reads message body if specified via filepath ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 4642, "code": "def trim args ( kwds ) : reject key = ( \"type\" , \"types\" , \"configure\" ) reject val = ( None , ( ) ) kwargs = { k : v for k , v in kwds . items ( ) if k not in reject key and v not in reject val } for k , v in kwargs . items ( ) : if k in ( \"to\" , \"cc\" , \"bcc\" , \"attachments\" ) : kwargs [ k ] = list ( kwargs [ k ] ) return kwargs", "predictions": ["trims the table from ("], "references": ["gets rid of args with value of none as well as select keys ."], "bleu": 0.037948473198912445, "rouge_l": 0.0}
{"id": 4643, "code": "def send message ( msg type , kwds ) : if kwds [ \"file\" ] : get body from file ( kwds ) kwargs = trim args ( kwds ) send ( msg type , send async = False , * * kwargs )", "predictions": ["sends a mailbox to the specified name and requires that the kwargs method ."], "references": ["do some final preprocessing and send the message ."], "bleu": 0.10511846841633776, "rouge_l": 0.271513353115727}
{"id": 4644, "code": "def get chat id ( self , username ) : if username is not None : chats = requests . get ( self . base url + \"/get Updates\" ) . json ( ) user = username . split ( \"@\" ) [ - 1 ] for chat in chats [ \"result\" ] : if chat [ \"message\" ] [ \"from\" ] [ \"username\" ] == user : return chat [ \"message\" ] [ \"from\" ] [ \"id\" ]", "predictions": ["emit the ( possibly . if it exists if it is a list of requests if it is a : . if set to . , then we are . but we can use this to get the first one ."], "references": ["lookup chat_id of username if chat_id is unknown via api call ."], "bleu": 0.036936272425359566, "rouge_l": 0.16746739876458477}
{"id": 4645, "code": "def construct message ( self ) : self . message [ \"chat id\" ] = self . chat id self . message [ \"text\" ] = \"\" if self . from : self . message [ \"text\" ] += \"From: \" + self . from + \"\\n\" if self . subject : self . message [ \"text\" ] += \"Subject: \" + self . subject + \"\\n\" self . message [ \"text\" ] += self . body self . message . update ( self . params )", "predictions": ["constructs a message for this class ."], "references": ["build the message params ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4646, "code": "def send content ( self , method = \"/send Message\" ) : url = self . base url + method try : resp = requests . post ( url , json = self . message ) resp . raise for status ( ) except requests . exceptions . HTTP Error as e : raise Message Send Error ( e ) if self . verbose : if method == \"/send Message\" : content type = \"Message body\" elif method == \"/send Document\" : content type = \"Attachment: \" + self . message [ \"document\" ] print ( timestamp ( ) , content type , \"sent.\" )", "predictions": ["send a request to all sessions in this site ."], "references": ["send via http post ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 4647, "code": "def send ( self ) : self . construct message ( ) if self . verbose : print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) self . send content ( \"/send Message\" ) if self . attachments : if isinstance ( self . attachments , str ) : self . attachments = [ self . attachments ] for a in self . attachments : self . message [ \"document\" ] = a self . send content ( method = \"/send Document\" ) if self . verbose : print ( timestamp ( ) , type ( self ) . name + \" info:\" , self . str ( indentation = \"\\n * \" ) , ) print ( \"Message sent.\" )", "predictions": ["sends a plain - formatted message to all links ."], "references": ["start sending the message and attachments ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 4648, "code": "def get server ( address = None ) : if address : domain = address . split ( \"@\" ) [ 1 ] try : return SMTP SERVERS [ domain ] except Key Error : return ( \"smtp.\" + domain , 465 ) return ( None , None )", "predictions": ["send an email or command line to the server ."], "references": ["return an smtp servername guess from outgoing email address ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 4649, "code": "def generate email ( self ) : self . message = MIME Multipart ( ) self . add header ( ) self . add body ( ) self . add attachments ( )", "predictions": ["generates a single ( commit data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data ."], "references": ["put the parts of the email together ."], "bleu": 0.03011857955989304, "rouge_l": 0.04736024844720497}
{"id": 4650, "code": "def add header ( self ) : self . message [ \"From\" ] = self . from self . message [ \"Subject\" ] = self . subject if self . to : self . message [ \"To\" ] = self . list to string ( self . to ) if self . cc : self . message [ \"Cc\" ] = self . list to string ( self . cc ) if self . bcc : self . message [ \"Bcc\" ] = self . list to string ( self . bcc )", "predictions": ["override for registering the ui class ."], "references": ["add email header info ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4651, "code": "def add body ( self ) : if self . body : b = MIME Text ( \"text\" , \"plain\" ) b . set payload ( self . body ) self . message . attach ( b )", "predictions": ["fetch a asset to the buffers ."], "references": ["add body content of email ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4652, "code": "def add attachments ( self ) : num attached = 0 if self . attachments : if isinstance ( self . attachments , str ) : self . attachments = [ self . attachments ] for item in self . attachments : doc = MIME Application ( open ( item , \"rb\" ) . read ( ) ) doc . add header ( \"Content-Disposition\" , \"attachment\" , filename = item ) self . message . attach ( doc ) num attached += 1 return num attached", "predictions": ["video method for creating this tool ."], "references": ["add required attachments ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4653, "code": "def get session ( self ) : if self . port in ( 465 , \"465\" ) : session = self . get ssl ( ) elif self . port in ( 587 , \"587\" ) : session = self . get tls ( ) try : session . login ( self . from , self . auth ) except SMTP Response Exception as e : raise Message Send Error ( e . smtp error . decode ( \"unicode escape\" ) ) return session", "predictions": ["this method is called by the server when it is not required ."], "references": ["start session with email server ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 4654, "code": "def get ssl ( self ) : return smtplib . SMTP SSL ( self . server , self . port , context = ssl . create default context ( ) )", "predictions": ["get the internal internal method for internal use only ."], "references": ["get an smtp session with ssl ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 4655, "code": "def get tls ( self ) : session = smtplib . SMTP ( self . server , self . port ) session . ehlo ( ) session . starttls ( context = ssl . create default context ( ) ) session . ehlo ( ) return session", "predictions": ["this is called to get the leaf ."], "references": ["get an smtp session with tls ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4656, "code": "def delete ( self , filename = None ) : if self . tags is not None : if filename is None : filename = self . filename else : warnings . warn ( \"delete(filename=...) is deprecated, reload the file\" , Deprecation Warning ) return self . tags . delete ( filename )", "predictions": ["deletes a file in the database ."], "references": ["remove tags from a file ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 4657, "code": "def save ( self , filename = None , * * kwargs ) : if filename is None : filename = self . filename else : warnings . warn ( \"save(filename=...) is deprecated, reload the file\" , Deprecation Warning ) if self . tags is not None : return self . tags . save ( filename , * * kwargs ) else : raise Value Error ( \"no tags in file\" )", "predictions": ["saves the current image part of this object as an array of ( to the local storage ."], "references": ["save metadata tags ."], "bleu": 0.06809398432036522, "rouge_l": 0.1026936026936027}
{"id": 4658, "code": "def unload ( self ) : if self . handle != - 1 : lib . Unload Image ( self . handle ) self . handle = - 1", "predictions": ["get a previously saved image"], "references": ["releases renderer resources associated with this image ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4659, "code": "def clear ( self ) : for i in list ( self . internal ) : self . internal . remove ( i )", "predictions": ["for each item in the list ."], "references": ["clear all keys from the comment ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4660, "code": "def read ( self ) : self . fileobj . seek ( self . data offset ) self . data = self . fileobj . read ( self . data size )", "predictions": ["get the contents of the resource ."], "references": ["read the chunks data"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4661, "code": "def delete ( self ) : delete bytes ( self . fileobj , self . size , self . offset ) if self . parent chunk is not None : self . parent chunk . resize ( self . parent chunk . data size - self . size )", "predictions": ["get a file as a folder and removes it as a contents of this object ."], "references": ["removes the chunk from the file"], "bleu": 0.08513012360883544, "rouge_l": 0.09902597402597402}
{"id": 4662, "code": "def resize ( self , data size ) : self . fileobj . seek ( self . offset + 4 ) self . fileobj . write ( pack ( '>I' , data size ) ) if self . parent chunk is not None : size diff = self . data size - data size self . parent chunk . resize ( self . parent chunk . data size - size diff ) self . data size = data size self . size = data size + self . HEADER SIZE", "predictions": ["get original data for a type ."], "references": ["update the size of the chunk"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4663, "code": "def insert chunk ( self , id ) : if not isinstance ( id , text type ) : id = id . decode ( 'ascii' ) if not is valid chunk id ( id ) : raise Key Error ( \"AIFF key must be four ASCII characters.\" ) self . fileobj . seek ( self . next offset ) self . fileobj . write ( pack ( '>4si' , id . ljust ( 4 ) . encode ( 'ascii' ) , 0 ) ) self . fileobj . seek ( self . next offset ) chunk = IFF Chunk ( self . fileobj , self [ u'FORM' ] ) self [ u'FORM' ] . resize ( self [ u'FORM' ] . data size + chunk . size ) self . chunks [ id ] = chunk self . next offset = chunk . offset + chunk . size", "predictions": ["select 1 . 0"], "references": ["insert a new chunk at the end of the iff file"], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 4664, "code": "def save ( self , filename = None , v2 version = 4 , v23 sep = '/' ) : framedata = self . prepare framedata ( v2 version , v23 sep ) framesize = len ( framedata ) if filename is None : filename = self . filename fileobj = open ( filename , 'rb+' ) iff file = IFF File ( fileobj ) try : if u'ID3' not in iff file : iff file . insert chunk ( u'ID3' ) chunk = iff file [ u'ID3' ] fileobj . seek ( chunk . data offset ) header = fileobj . read ( 10 ) header = self . prepare id3 header ( header , framesize , v2 version ) header , new size , = header data = header + framedata + ( b'\\x00' * ( new size - framesize ) ) new size += 10 if new size > chunk . size : insert at = chunk . offset + chunk . size insert size = new size - chunk . size + new size % 2 insert bytes ( fileobj , insert size , insert at ) chunk . resize ( new size ) fileobj . seek ( chunk . data offset ) fileobj . write ( data ) finally : fileobj . close ( )", "predictions": ["get a new ( according to the rules provided as a . ."], "references": ["save id3v2 data to the aiff file"], "bleu": 0.12571192676522522, "rouge_l": 0.21143847487001732}
{"id": 4665, "code": "def delete ( self , filename = None ) : if filename is None : filename = self . filename delete ( filename ) self . clear ( )", "predictions": ["deletes the file . this could be called once for each time a unit has been closed ."], "references": ["completely removes the id3 chunk from the aiff file"], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 4666, "code": "def load ( self , filename , * * kwargs ) : self . filename = filename try : self . tags = IFFID3 ( filename , * * kwargs ) except ID3Error : self . tags = None try : fileobj = open ( filename , \"rb\" ) self . info = AIFF Info ( fileobj ) finally : fileobj . close ( )", "predictions": ["we can call this method to construct the object that we can construct ."], "references": ["load stream and tag information from a file ."], "bleu": 0.08839374326825923, "rouge_l": 0.09050445103857567}
{"id": 4667, "code": "def parse file ( self , filename ) : self . reset ( ) self . filename = filename fileinput . close ( ) self . format = None self . lineno = 0 self . lines = [ ] for line in fileinput . input ( filename ) : if line [ - 1 ] == '\\012' : line = line [ 0 : - 1 ] if self . format == None : self . process normal line ( line ) else : if self . format . end . match ( line ) : self . lines . append ( line ) self . add block lines ( ) elif self . format . column . match ( line ) : self . lines . append ( line ) else : self . add block lines ( ) self . process normal line ( line ) self . add block lines ( )", "predictions": ["httpconnection for each ( in the aliases file ) ) ) ) self ) : ( self ( : ( : ( : ( : ( : ( : ( : ( : ( : ( : ( : ( : ( ( : ( : ( : ( :"], "references": ["parse a c source file and add its blocks to the processor s list"], "bleu": 0.026594139297659906, "rouge_l": 0.03477765108323831}
{"id": 4668, "code": "def process normal line ( self , line ) : for f in re source block formats : if f . start . match ( line ) : self . add block lines ( ) self . format = f self . lineno = fileinput . filelineno ( ) self . lines . append ( line )", "predictions": ["validate each ( as a ( possibly empty attr attr attr attr attr attr attr attr attr attr attr attr attr attr attr attr attr attr attr attr . attr msg attr attr attr . msg ( ( msg attr attr ( msg ."], "references": ["process a normal line and check whether it is the start of a new block"], "bleu": 0.027347130611442165, "rouge_l": 0.03719512195121952}
{"id": 4669, "code": "def add block lines ( self ) : if self . lines != [ ] : block = Source Block ( self , self . filename , self . lineno , self . lines ) self . blocks . append ( block ) self . format = None self . lines = [ ]", "predictions": ["adds a twilio twilio twilio twilio ."], "references": ["add the current accumulated lines and create a new block"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4670, "code": "def make html words ( self , words ) : line = \"\" if words : line = html quote ( words [ 0 ] ) for w in words [ 1 : ] : line = line + \" \" + html quote ( w ) return line", "predictions": ["makes the pointers of , but only works once per iteration ."], "references": ["convert a series of simple words into some html text"], "bleu": 0.10390302174233558, "rouge_l": 0.09242424242424242}
{"id": 4671, "code": "def make html word ( self , word ) : m = re crossref . match ( word ) if m : try : name = m . group ( 1 ) rest = m . group ( 2 ) block = self . identifiers [ name ] url = self . make block url ( block ) return '<a href=\"' + url + '\">' + name + '</a>' + rest except : sys . stderr . write ( \"WARNING: undefined cross reference '\" + name + \"'.\\n\" ) return '?' + name + '?' + rest m = re italic . match ( word ) if m : name = m . group ( 1 ) rest = m . group ( 3 ) return '<i>' + name + '</i>' + rest m = re bold . match ( word ) if m : name = m . group ( 1 ) rest = m . group ( 3 ) return '<b>' + name + '</b>' + rest return html quote ( word )", "predictions": ["validate header for html document ."], "references": ["analyze a simple word to detect cross - references and styling"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 4672, "code": "def make html para ( self , words ) : line = \"\" if words : line = self . make html word ( words [ 0 ] ) for word in words [ 1 : ] : line = line + \" \" + self . make html word ( word ) line = re . sub ( r\"(^|\\W)`(.*?)'(\\W|$)\" , r'\\1&lsquo;\\2&rsquo;\\3' , line ) line = string . replace ( line , \"~\" , \"&nbsp;\" ) return para header + line + para footer", "predictions": ["makes the given html with all words converted to word ."], "references": ["convert words of a paragraph into tagged html text handle xrefs"], "bleu": 0.12605968092174913, "rouge_l": 0.09090909090909091}
{"id": 4673, "code": "def make html code ( self , lines ) : line = code header + '\\n' for l in lines : line = line + html quote ( l ) + '\\n' return line + code footer", "predictions": ["makes html with line breaks ."], "references": ["convert a code sequence to html"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 4674, "code": "def make html items ( self , items ) : lines = [ ] for item in items : if item . lines : lines . append ( self . make html code ( item . lines ) ) else : lines . append ( self . make html para ( item . words ) ) return string . join ( lines , '\\n' )", "predictions": ["make html with lines for self ."], "references": ["convert a field s content into some valid html"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4675, "code": "def save ( self , filename ) : values = [ ] items = sorted ( self . items ( ) , key = MP4Tags . get sort stats ) for key , value in items : info = self . atoms . get ( key [ : 4 ] , ( None , type ( self ) . render text ) ) try : values . append ( info [ 1 ] ( self , key , value , * info [ 2 : ] ) ) except ( Type Error , Value Error ) as s : reraise ( MP4Metadata Value Error , s , sys . exc info ( ) [ 2 ] ) data = Atom . render ( b\"ilst\" , b\"\" . join ( values ) ) fileobj = open ( filename , \"rb+\" ) try : atoms = Atoms ( fileobj ) try : path = atoms . path ( b\"moov\" , b\"udta\" , b\"meta\" , b\"ilst\" ) except Key Error : self . save new ( fileobj , atoms , data ) else : self . save existing ( fileobj , atoms , path , data ) finally : fileobj . close ( )", "predictions": ["save the memory to the ( ."], "references": ["save the metadata to the given filename ."], "bleu": 0.2789001430384383, "rouge_l": 0.6587473002159828}
{"id": 4676, "code": "def update parents ( self , fileobj , path , delta ) : for atom in path : fileobj . seek ( atom . offset ) size = cdata . uint be ( fileobj . read ( 4 ) ) if size == 1 : size = cdata . ulonglong be ( fileobj . read ( 12 ) [ 4 : ] ) fileobj . seek ( atom . offset + 8 ) fileobj . write ( cdata . to ulonglong be ( size + delta ) ) else : fileobj . seek ( atom . offset ) fileobj . write ( cdata . to uint be ( size + delta ) )", "predictions": ["update a whole atom from this object ."], "references": ["update all parent atoms with the new size ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 4677, "code": "def load ( self , filename ) : self . filename = filename fileobj = open ( filename , \"rb\" ) try : data = AP Ev2Data ( fileobj ) finally : fileobj . close ( ) if data . tag : self . clear ( ) self . casemap . clear ( ) self . parse tag ( data . tag , data . items ) else : raise APE No Header Error ( \"No APE tag found\" )", "predictions": ["load the ( configuration from the given filename ."], "references": ["load tags from a filename ."], "bleu": 0.21105340631872635, "rouge_l": 0.5532879818594103}
{"id": 4678, "code": "def delete ( self , filename = None ) : filename = filename or self . filename fileobj = open ( filename , \"r+b\" ) try : data = AP Ev2Data ( fileobj ) if data . start is not None and data . size is not None : delete bytes ( fileobj , data . end - data . start , data . start ) finally : fileobj . close ( ) self . clear ( )", "predictions": ["closes the underlying storage and deletes the database ."], "references": ["remove tags from a file ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4679, "code": "def size ( self ) : header size = 27 for datum in self . packets : quot , rem = divmod ( len ( datum ) , 255 ) header size += quot + 1 if not self . complete and rem == 0 : header size -= 1 header size += sum ( map ( len , self . packets ) ) return header size", "predictions": ["calculates the size of this list ."], "references": ["total frame size ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 4680, "code": "def load ( self , filename ) : self . filename = filename fileobj = open ( filename , \"rb\" ) try : try : self . info = self . Info ( fileobj ) self . tags = self . Tags ( fileobj , self . info ) self . info . post tags ( fileobj ) except error as e : reraise ( self . Error , e , sys . exc info ( ) [ 2 ] ) except EOF Error : raise self . Error ( \"no appropriate stream found\" ) finally : fileobj . close ( )", "predictions": ["registers a stream that can be called multiple times ."], "references": ["load file information from a filename ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 4681, "code": "def set section ( self , section name ) : if not self . sections . has key ( section name ) : section = Doc Section ( section name ) self . sections [ section name ] = section self . section = section else : self . section = self . sections [ section name ]", "predictions": ["sets a section in the section ."], "references": ["set current section during parsing"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4682, "code": "def add markup ( self ) : if self . markup and self . markup lines : marks = self . markup lines if len ( marks ) > 0 and not string . strip ( marks [ - 1 ] ) : self . markup lines = marks [ : - 1 ] m = Doc Markup ( self . markup , self . markup lines ) self . markups . append ( m ) self . markup = None self . markup lines = [ ]", "predictions": ["add default routers for specific call ."], "references": ["add a new markup section"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4683, "code": "def get markup ( self , tag name ) : for m in self . markups : if m . tag == string . lower ( tag name ) : return m return None", "predictions": ["fetches the currently active tag of this element ."], "references": ["return the docmarkup corresponding to a given tag in a block"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 4684, "code": "def utf8 ( data ) : if isinstance ( data , bytes ) : return data . decode ( \"utf-8\" , \"replace\" ) . encode ( \"utf-8\" ) elif isinstance ( data , text type ) : return data . encode ( \"utf-8\" ) else : raise Type Error ( \"only unicode/bytes types can be converted to UTF-8\" )", "predictions": ["encodes the provided data with the provided \"utf-8\" ."], "references": ["convert a basestring to a valid utf - 8 str ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 4685, "code": "def delete ( self ) : cset = Change Set ( connection = self . connection , hosted zone id = self . zone id ) cset . add change ( 'DELETE' , self ) return self . connection . change resource record sets ( cset )", "predictions": ["deletes an existing connection ."], "references": ["deletes this record set ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4686, "code": "def save ( self ) : cset = Change Set ( connection = self . connection , hosted zone id = self . zone id ) cset . add change ( 'DELETE' , self ) cset . add change ( 'CREATE' , self ) retval = self . connection . change resource record sets ( cset ) for key , val in self . initial vals . items ( ) : self . initial vals [ key ] = getattr ( self , key ) return retval", "predictions": ["saves this connection to the specified hierarchy ."], "references": ["saves any changes to this record set ."], "bleu": 0.20164945583740668, "rouge_l": 0.375}
{"id": 4687, "code": "def Parse ID3v1 ( data ) : try : data = data [ data . index ( b'TAG' ) : ] except Value Error : return None if 128 < len ( data ) or len ( data ) < 124 : return None unpack fmt = \"3s30s30s30s%ds29s BB\" % ( len ( data ) - 124 ) try : tag , title , artist , album , year , comment , track , genre = unpack ( unpack fmt , data ) except Struct Error : return None if tag != b\"TAG\" : return None def fix ( data ) : return data . split ( b'\\x00' ) [ 0 ] . strip ( ) . decode ( 'latin1' ) title , artist , album , year , comment = map ( fix , [ title , artist , album , year , comment ] ) frames = { } if title : frames [ 'TIT2' ] = TIT2 ( encoding = 0 , text = title ) if artist : frames [ 'TPE1' ] = TPE1 ( encoding = 0 , text = [ artist ] ) if album : frames [ 'TALB' ] = TALB ( encoding = 0 , text = album ) if year : frames [ 'TDRC' ] = TDRC ( encoding = 0 , text = year ) if comment : frames [ 'COMM' ] = COMM ( encoding = 0 , lang = 'eng' , desc = \"ID3v1 Comment\" , text = comment ) if track and ( ( track != 32 ) or ( data [ - 3 ] == b'\\x00' [ 0 ] ) ) : frames [ 'TRCK' ] = TRCK ( encoding = 0 , text = str ( track ) ) if genre != 255 : frames [ 'TCON' ] = TCON ( encoding = 0 , text = str ( genre ) ) return frames", "predictions": ["parses an artist from the given data ."], "references": ["parse an id3v1 tag returning a list of id3v2 . 4 frames ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 4688, "code": "def Make ID3v1 ( id3 ) : v1 = { } for v2id , name in { \"TIT2\" : \"title\" , \"TPE1\" : \"artist\" , \"TALB\" : \"album\" } . items ( ) : if v2id in id3 : text = id3 [ v2id ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 30 ] else : text = b'' v1 [ name ] = text + ( b'\\x00' * ( 30 - len ( text ) ) ) if \"COMM\" in id3 : cmnt = id3 [ \"COMM\" ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 28 ] else : cmnt = b'' v1 [ 'comment' ] = cmnt + ( b'\\x00' * ( 29 - len ( cmnt ) ) ) if \"TRCK\" in id3 : try : v1 [ \"track\" ] = chr ( + id3 [ \"TRCK\" ] ) except Value Error : v1 [ \"track\" ] = b'\\x00' else : v1 [ \"track\" ] = b'\\x00' if \"TCON\" in id3 : try : genre = id3 [ \"TCON\" ] . genres [ 0 ] except Index Error : pass else : if genre in TCON . GENRES : v1 [ \"genre\" ] = chr ( TCON . GENRES . index ( genre ) ) if \"genre\" not in v1 : v1 [ \"genre\" ] = b\"\\xff\" if \"TDRC\" in id3 : year = text type ( id3 [ \"TDRC\" ] ) . encode ( 'latin1' , 'replace' ) elif \"TYER\" in id3 : year = text type ( id3 [ \"TYER\" ] ) . encode ( 'latin1' , 'replace' ) else : year = b'' v1 [ 'year' ] = ( year + b'\\x00\\x00\\x00\\x00' ) [ : 4 ] return ( b'TAG' + v1 [ 'title' ] + v1 [ 'artist' ] + v1 [ 'album' ] + v1 [ 'year' ] + v1 [ 'comment' ] + v1 [ 'track' ] + v1 [ 'genre' ] )", "predictions": ["generate the \"album\" for a year ."], "references": ["return an id3v1 . 1 tag string from a dict of id3v2 . 4 frames ."], "bleu": 0.056829570481990416, "rouge_l": 0.16245006657789615}
{"id": 4689, "code": "def fullread ( self , size ) : try : if size < 0 : raise Value Error ( 'Requested bytes (%s) less than zero' % size ) if size > self . filesize : raise EOF Error ( 'Requested %#x of %#x (%s)' % ( int ( size ) , int ( self . filesize ) , self . filename ) ) except Attribute Error : pass data = self . fileobj . read ( size ) if len ( data ) != size : raise EOF Error self . readbytes += size return data", "predictions": ["this method returns a list of data that can be left over the given size ."], "references": ["read a certain number of bytes from the source file ."], "bleu": 0.09672649511413092, "rouge_l": 0.3065326633165829}
{"id": 4690, "code": "def delall ( self , key ) : if key in self : del ( self [ key ] ) else : key = key + \":\" for k in self . keys ( ) : if k . startswith ( key ) : del ( self [ k ] )", "predictions": ["print out stats for this session ."], "references": ["delete all tags of a given kind ; see getall ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 4691, "code": "def loaded frame ( self , tag ) : if len ( type ( tag ) . name ) == 3 : tag = type ( tag ) . base ( tag ) self [ tag . Hash Key ] = tag", "predictions": ["create a new pair ."], "references": ["deprecated ; use the add method ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4692, "code": "def update common ( self ) : if \"TCON\" in self : self [ \"TCON\" ] . genres = self [ \"TCON\" ] . genres if self . version < self . V23 : pics = self . getall ( \"APIC\" ) mimes = { \"PNG\" : \"image/png\" , \"JPG\" : \"image/jpeg\" } self . delall ( \"APIC\" ) for pic in pics : newpic = APIC ( encoding = pic . encoding , mime = mimes . get ( pic . mime , pic . mime ) , type = pic . type , desc = pic . desc , data = pic . data ) self . add ( newpic ) self . delall ( \"LINK\" )", "predictions": ["we know what this method is called to do so ."], "references": ["updates done by both v23 and v24 update"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4693, "code": "def unload ( self ) : if self . handle != - 1 : lib . Unload Sound ( self . handle ) self . handle = - 1", "predictions": ["unload the given method ."], "references": ["release all resources associated with the sound ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4694, "code": "def adobe glyph values ( ) : lines = string . split ( adobe glyph list , '\\n' ) glyphs = [ ] values = [ ] for line in lines : if line : fields = string . split ( line , ';' ) subfields = string . split ( fields [ 1 ] , ' ' ) if len ( subfields ) == 1 : glyphs . append ( fields [ 0 ] ) values . append ( fields [ 1 ] ) return glyphs , values", "predictions": ["get a list of values from the given string"], "references": ["return the list of glyph names and their unicode values"], "bleu": 0.18885888592159467, "rouge_l": 0.31282051282051276}
{"id": 4695, "code": "def filter glyph names ( alist , filter ) : count = 0 extras = [ ] for name in alist : try : filtered index = filter . index ( name ) except : extras . append ( name ) return extras", "predictions": ["returns only those that match the specified alist ."], "references": ["filter alist by taking _out_ all glyph names that are in filter"], "bleu": 0.11192003885776355, "rouge_l": 0.0928462709284627}
{"id": 4696, "code": "def dump encoding ( file , encoding name , encoding list ) : write = file . write write ( \"  /* the following are indices into the SID name table */\\n\" ) write ( \"  static const unsigned short  \" + encoding name + \"[\" + repr ( len ( encoding list ) ) + \"] =\\n\" ) write ( \"  {\\n\" ) line = \"    \" comma = \"\" col = 0 for value in encoding list : line += comma line += \"%3d\" % value comma = \",\" col += 1 if col == 16 : col = 0 comma = \",\\n    \" write ( line + \"\\n  };\\n\\n\\n\" )", "predictions": ["dumps a file content into a file ."], "references": ["dump a given encoding"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4697, "code": "def dump array ( the array , write , array name ) : write ( \"  static const unsigned char  \" + array name + \"[\" + repr ( len ( the array ) ) + \"L] =\\n\" ) write ( \"  {\\n\" ) line = \"\" comma = \"    \" col = 0 for value in the array : line += comma line += \"%3d\" % ord ( value ) comma = \",\" col += 1 if col == 16 : col = 0 comma = \",\\n    \" if len ( line ) > 1024 : write ( line ) line = \"\" write ( line + \"\\n  };\\n\\n\\n\" )", "predictions": ["dump a array of strings into the stream ."], "references": ["dumps a given encoding"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4698, "code": "def file exists ( pathname ) : result = 1 try : file = open ( pathname , \"r\" ) file . close ( ) except : result = None sys . stderr . write ( pathname + \" couldn't be accessed\\n\" ) return result", "predictions": ["checks to see if the file exists ."], "references": ["checks that a given file exists"], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 4699, "code": "def make file list ( args = None ) : file list = [ ] if not args : args = sys . argv [ 1 : ] for pathname in args : if string . find ( pathname , '*' ) >= 0 : newpath = glob . glob ( pathname ) newpath . sort ( ) else : newpath = [ pathname ] file list . extend ( newpath ) if len ( file list ) == 0 : file list = None else : file list = filter ( file exists , file list ) return file list", "predictions": ["return a list from command line counts ."], "references": ["builds a list of input files from command - line arguments"], "bleu": 0.1909027782642041, "rouge_l": 0.511744966442953}
{"id": 4700, "code": "def writeblocks ( blocks ) : data = [ ] codes = [ [ block . code , block . write ( ) ] for block in blocks ] codes [ - 1 ] [ 0 ] |= 128 for code , datum in codes : byte = chr ( code ) if len ( datum ) > 2 ** 24 : raise error ( \"block is too long to write\" ) length = struct . pack ( \">I\" , len ( datum ) ) [ - 3 : ] data . append ( byte + length + datum ) return b\"\" . join ( data )", "predictions": ["writeblocks ( ) = data | data | byte [ ] | . | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( | ( |"], "references": ["render metadata block as a byte string ."], "bleu": 0.026594139297659906, "rouge_l": 0.07932379713914176}
{"id": 4701, "code": "def load ( self , filename ) : self . metadata blocks = [ ] self . tags = None self . cuesheet = None self . seektable = None self . filename = filename fileobj = Strict File Object ( open ( filename , \"rb\" ) ) try : self . check header ( fileobj ) while self . read metadata block ( fileobj ) : pass finally : fileobj . close ( ) try : self . metadata blocks [ 0 ] . length except ( Attribute Error , Index Error ) : raise FLAC No Header Error ( \"Stream info block not found\" )", "predictions": ["loads the object from the given file ."], "references": ["load file information from a filename ."], "bleu": 0.19070828081828378, "rouge_l": 0.26991150442477874}
{"id": 4702, "code": "def init logs ( ) : start time = dt . fromtimestamp ( time . time ( ) ) . strftime ( '%Y%m%d %H%M' ) logname = os . path . join ( os . path . expanduser ( \"~\" ) + \"/nano GUI \" + start time + \".log\" ) handlers = [ logging . File Handler ( logname ) ] logging . basic Config ( format = '%(asctime)s %(message)s' , handlers = handlers , level = logging . INFO ) logging . info ( 'Nano GUI {} started with Nano Plot {}' . format ( version , nanoplot . version ) ) logging . info ( 'Python version is: {}' . format ( sys . version . replace ( '\\n' , ' ' ) ) ) return logname", "predictions": ["converts logs into command to use for ( ."], "references": ["initiate log file ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4703, "code": "def alias item ( self , alias ) : ident = self . alias [ alias ] return self . items [ ident ]", "predictions": ["alias for alias . this is only meant for player use ."], "references": ["gets an item by its alias ."], "bleu": 0.1367440667823257, "rouge_l": 0.22101449275362317}
{"id": 4704, "code": "def initialize bars ( self , sender = None , * * kwargs ) : for bar in self . bars . values ( ) : for initializer in bar . initializers : initializer ( self )", "predictions": ["initializes the map of variables to be initialized to the process ."], "references": ["calls the initializers of all bound navigation bars ."], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 4705, "code": "def bind bar ( self , sender = None , * * kwargs ) : bar = kwargs . pop ( 'bar' ) self . bars [ bar . name ] = bar", "predictions": ["binds the given html to the given line ."], "references": ["binds a navigation bar into this extension instance ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 4706, "code": "def validate ( metric class ) : if not hasattr ( metric class , 'label' ) : raise Improperly Configured ( \"No 'label' attribute found for metric %s.\" % metric class . name ) if not hasattr ( metric class , 'widget' ) : raise Improperly Configured ( \"No 'widget' attribute found for metric %s.\" % metric class . name )", "predictions": ["validates the items of an if the given ( i . e . every items in the items of the given items or a single items self - ] self - ] - . - ( - ( - ( - ( - ( - ( - ( - ("], "references": ["does basic metric option validation ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 4707, "code": "def calculate statistics ( stat , frequencies ) : stats = ensure list ( stat ) frequencies = ensure list ( frequencies ) for stat in stats : for f in frequencies : print \"Calculating %s (%s)...\" % ( stat . name , settings . STATISTIC FREQUENCY DICT [ f ] ) stat . calculate ( f )", "predictions": ["save ( 1 : 1 : 2 : 1 : 1 : 2 : 1 : 2 : 1 : 2 : . : . : . : . : . [ 2 ] 2 : . [ 2 ] + . - . - . - . - ."], "references": ["calculates all of the metrics associated with the registered gadgets ."], "bleu": 0.02403051755364481, "rouge_l": 0.037059538274605106}
{"id": 4708, "code": "def handle ( self , * args , * * kwargs ) : frequency = kwargs [ 'frequency' ] frequencies = settings . STATISTIC FREQUENCY ALL if frequency == 'a' else ( frequency . split ( ',' ) if ',' in frequency else [ frequency ] ) if kwargs [ 'list' ] : maintenance . list statistics ( ) elif kwargs [ 'calculate' ] : maintenance . calculate statistics ( maintenance . get statistic by name ( kwargs [ 'calculate' ] ) , frequencies ) elif kwargs [ 'reset' ] : maintenance . reset statistics ( maintenance . get statistic by name ( kwargs [ 'reset' ] ) , frequencies , kwargs [ 'reset cumulative' ] ) elif kwargs [ 'recalculate' ] : maintenance . reset statistics ( maintenance . get statistic by name ( kwargs [ 'recalculate' ] ) , frequencies , kwargs [ 'reset cumulative' ] , True )", "predictions": ["update a : 1 : 2 . 2 . 3 . 2 . 3 . 5 . 2 . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ( . ("], "references": ["command handler for the metrics command ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 4709, "code": "def get GET array ( request , var name , fail silently = True ) : vals = request . GET . getlist ( var name ) if not vals : if fail silently : return [ ] else : raise Exception , ( \"No array called '%(varname)s' in GET variables\" ) % { 'varname' : var name } return vals", "predictions": ["check if the given self - qualified variable is currently set ."], "references": ["returns the get array s contents for the specified variable ."], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 4710, "code": "def get GET bool ( request , var name , default = True ) : val = request . GET . get ( var name , default ) if isinstance ( val , str ) or isinstance ( val , unicode ) : val = True if val [ 0 ] == 't' else False return val", "predictions": ["convert the value to an arbitrary variable ."], "references": ["tries to extract a boolean variable from the specified request ."], "bleu": 0.13859150907108325, "rouge_l": 0.3070469798657718}
{"id": 4711, "code": "def get next colour ( ) : colour = settings . GECKOBOARD COLOURS [ get next colour . cur colour ] get next colour . cur colour += 1 if get next colour . cur colour >= len ( settings . GECKOBOARD COLOURS ) : get next colour . cur colour = 0 return colour", "predictions": ["generate the ( or null : return the ( iterator of the current hasn ' t yet : self - infinite : self - self - infinite : self - self - infinite : self - quartz : readlong : readlong number of self - self - self - 0xff"], "references": ["gets the next colour in the geckoboard colour list ."], "bleu": 0.026594139297659906, "rouge_l": 0.07577639751552796}
{"id": 4712, "code": "def geckoboard number widget ( request ) : params = get gecko params ( request , days back = 7 ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) try : latest stat = metric . statistics . filter ( frequency = params [ 'frequency' ] ) . order by ( '-date time' ) [ 0 ] except Index Error : return ( 0 , 0 ) try : prev stat = metric . statistics . filter ( frequency = params [ 'frequency' ] , date time lte = latest stat . date time - timedelta ( days = params [ 'days back' ] ) ) . order by ( '-date time' ) [ 0 ] except Index Error : return ( latest stat . cumulative count , 0 ) if params [ 'cumulative' ] else ( latest stat . count , 0 ) return ( latest stat . cumulative count , prev stat . cumulative count ) if params [ 'cumulative' ] else ( latest stat . count , prev stat . count )", "predictions": ["utility method to load the sys data at the top of the filename ."], "references": ["returns a number widget for the specified metric s cumulative total ."], "bleu": 0.09782375748961449, "rouge_l": 0.15601023017902813}
{"id": 4713, "code": "def geckoboard line chart ( request ) : params = get gecko params ( request , cumulative = False , days back = 7 ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) start date = datetime . now ( ) - timedelta ( days = params [ 'days back' ] ) stats = [ s for s in metric . statistics . filter ( frequency = params [ 'frequency' ] , date time gte = start date ) . order by ( 'date time' ) ] if len ( stats ) == 0 : raise Exception , ( \"No statistics for metric %(metric)s.\" ) % { 'metric' : params [ 'uid' ] } dates = [ stats [ 0 ] . date time ] if len ( stats ) >= 3 : mid = len ( stats ) / 2 if not mid : mid = 1 dates . extend ( [ stats [ mid ] . date time , stats [ - 1 ] . date time ] ) elif len ( stats ) == 2 : dates . extend ( [ stats [ - 1 ] . date time ] ) return ( [ s . count for s in stats ] , dates , metric . title , )", "predictions": ["creates the visual section of all frequency data . this makes a template for each section that has the same frequency ."], "references": ["returns the data for a line chart for the specified metric ."], "bleu": 0.07575731225158963, "rouge_l": 0.3727087576374745}
{"id": 4714, "code": "def geckoboard geckometer ( request ) : params = get gecko params ( request , cumulative = True ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) return ( metric . latest count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , params [ 'min' ] , params [ 'max' ] )", "predictions": ["template method to add a square to the cluster ."], "references": ["returns a geck - o - meter control for the specified metric ."], "bleu": 0.11105685174312292, "rouge_l": 0.25487465181058494}
{"id": 4715, "code": "def geckoboard funnel ( request , frequency = settings . STATISTIC FREQUENCY DAILY ) : params = get gecko params ( request , cumulative = True ) metrics = Metric . objects . filter ( uid in = params [ 'uids' ] ) items = [ ( metric . latest count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , metric . title ) for metric in metrics ] return { 'items' : items , 'type' : params [ 'type' ] , 'percentage' : params [ 'percentage' ] , 'sort' : params [ 'sort' ] , }", "predictions": ["creates a , representing all lower - pass metric"], "references": ["returns a funnel chart for the metrics specified in the get variables ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 4716, "code": "def get active stats ( self ) : stats = [ ] for gadget in self . registry . values ( ) : for s in gadget . stats : if s not in stats : stats . append ( s ) return stats", "predictions": ["measure ( possibly different if none exist if no data is specified if it is specified ."], "references": ["returns all of the active statistics for the gadgets currently registered ."], "bleu": 0.07223943354597204, "rouge_l": 0.07117852975495916}
{"id": 4717, "code": "def get context data ( self , * * kwargs ) : #max columns, max rows = self.get max dimension() context = { 'gadgets' : self . registry , 'columns' : self . columns , 'rows' : self . rows , 'column ratio' : 100 - self . columns * 2 , 'row ratio' : 100 - self . rows * 2 , } context . update ( kwargs ) return context", "predictions": ["delete the self - note that this is a circle of some angle ."], "references": ["get the context for this view ."], "bleu": 0.10511846841633776, "rouge_l": 0.30398671096345514}
{"id": 4718, "code": "def error ( self , message , code = 1 ) : print >> sys . stderr , message sys . exit ( code )", "predictions": ["prints the save message and its error message ."], "references": ["print error and stop command"], "bleu": 0.15619699684601276, "rouge_l": 0.1506172839506173}
{"id": 4719, "code": "def valid ( schema = None ) : def dec ( fun ) : @ wraps ( fun ) def d func ( self , ctx , data , * a , * * kw ) : try : validate ( data [ 'params' ] , schema ) except Validation Error as err : raise Invalid Params ( err ) except Schema Error as err : raise Internal Error ( err ) return fun ( self , ctx , data [ 'params' ] , * a , * * kw ) return d func return dec", "predictions": ["decorator for connection data and apply to a single function ."], "references": ["validation data by specific validictory configuration"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 4720, "code": "def long input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : lines = [ ] print ( prompt ) lnum = 1 try : while True : if maxlines : if lnum > maxlines : break else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) lnum += 1 else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) except EOF Error : pass finally : return '\\n' . join ( lines )", "predictions": ["separate encode encode encode encode encode encode encode encode encode encode encode encode into the input stream ."], "references": ["get a multi - line string as input"], "bleu": 0.06809398432036522, "rouge_l": 0.08265582655826557}
{"id": 4721, "code": "def list input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : lines = [ ] print ( prompt ) inum = 1 try : while True : if maxitems : if inum > maxitems : break else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) inum += 1 else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) except EOF Error : pass finally : return lines", "predictions": ["list list list command line ."], "references": ["get a list of strings as input"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4722, "code": "def outfile input ( extension = None ) : fileok = False while not fileok : filename = string input ( 'File name? ' ) if extension : if not filename . endswith ( extension ) : if extension . startswith ( '.' ) : filename = filename + extension else : filename = filename + '.' + extension if os . path . isfile ( filename ) : choice = choice input ( prompt = filename + ' already exists. Overwrite?' , options = [ 'y' , 'n' ] ) if choice == 'y' : try : nowtime = time . time ( ) with open ( filename , 'a' ) as f : os . utime ( filename , ( nowtime , nowtime ) ) fileok = True except IO Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except Permission Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except File Not Found Error : print ( filename + ': directory not found. Try again.' ) else : choice = choice input ( prompt = filename + ' does not exist. Create it?' , options = [ 'y' , 'n' ] ) if choice == 'y' : try : nowtime = time . time ( ) with open ( filename , 'w' ) as f : os . utime ( filename , ( nowtime , nowtime ) ) fileok = True except IO Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except Permission Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except File Not Found Error : print ( filename + ': directory not found. Try again.' ) return filename", "predictions": ["exports all i18n entries from a file ."], "references": ["get an output file name as input"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4723, "code": "def winner ( self ) : hm Score = self . home score ( ) aw Score = self . away score ( ) if hm Score > aw Score : return self . home ( ) elif hm Score < aw Score : return self . away ( ) else : return None", "predictions": ["specialized scoring method for no longer used ."], "references": ["returns the team id of the winning team . returns nan if a tie ."], "bleu": 0.06685045700482882, "rouge_l": 0.08243243243243244}
{"id": 4724, "code": "def standings ( self ) : doc = self . get sub doc ( 'standings' ) east table = doc ( 'table#divs standings E' ) east df = pd . Data Frame ( sportsref . utils . parse table ( east table ) ) east df . sort values ( 'wins' , ascending = False , inplace = True ) east df [ 'seed' ] = range ( 1 , len ( east df ) + 1 ) east df [ 'conference' ] = 'E' west table = doc ( 'table#divs standings W' ) west df = sportsref . utils . parse table ( west table ) west df . sort values ( 'wins' , ascending = False , inplace = True ) west df [ 'seed' ] = range ( 1 , len ( west df ) + 1 ) west df [ 'conference' ] = 'W' full df = pd . concat ( [ east df , west df ] , axis = 0 ) . reset index ( drop = True ) full df [ 'team id' ] = full df . team id . str . extract ( r'(\\w+)\\W*\\(\\d+\\)' , expand = False ) full df [ 'gb' ] = [ gb if isinstance ( gb , int ) or isinstance ( gb , float ) else 0 for gb in full df [ 'gb' ] ] full df = full df . drop ( 'has class full table' , axis = 1 ) expanded table = doc ( 'table#expanded standings' ) expanded df = sportsref . utils . parse table ( expanded table ) full df = pd . merge ( full df , expanded df , on = 'team id' ) return full df", "predictions": ["volunteer a dataframe so that it can be visible later later on the document ."], "references": ["returns a dataframe containing standings information ."], "bleu": 0.11633270842295028, "rouge_l": 0.291866028708134}
{"id": 4725, "code": "def roy voting ( self ) : url = '{}/awards/awards {}.html' . format ( sportsref . nba . BASE URL , self . yr ) doc = pq ( sportsref . utils . get html ( url ) ) table = doc ( 'table#roy' ) df = sportsref . utils . parse table ( table ) return df", "predictions": ["unload the specified . this is only needed for testing ."], "references": ["returns a dataframe containing information about roy voting ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 4726, "code": "def linescore ( self ) : doc = self . get main doc ( ) table = doc ( 'table#line score' ) columns = [ th . text ( ) for th in table ( 'tr.thead' ) . items ( 'th' ) ] columns [ 0 ] = 'team id' data = [ [ sportsref . utils . flatten links ( td ) for td in tr ( 'td' ) . items ( ) ] for tr in table ( 'tr.thead' ) . next all ( 'tr' ) . items ( ) ] return pd . Data Frame ( data , index = [ 'away' , 'home' ] , columns = columns , dtype = 'float' )", "predictions": ["generate the adobe table of this frame ."], "references": ["returns the linescore for the game as a dataframe ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 4727, "code": "def get class instance key ( cls , args , kwargs ) : l = [ id ( cls ) ] for arg in args : l . append ( id ( arg ) ) l . extend ( ( k , id ( v ) ) for k , v in kwargs . items ( ) ) return tuple ( sorted ( l ) )", "predictions": ["filter all instances of this class from a given class"], "references": ["returns a unique identifier for a class instantiation ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 4728, "code": "def stats per game ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per game' , kind = kind , summary = summary )", "predictions": ["calculates the dump of this instance ."], "references": ["returns a dataframe of per - game box score stats ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4729, "code": "def stats totals ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'totals' , kind = kind , summary = summary )", "predictions": ["method to attach the dump dump method of the given table ."], "references": ["returns a dataframe of total box score statistics by season ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 4730, "code": "def stats per36 ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per minute' , kind = kind , summary = summary )", "predictions": ["method to catch file for the given except except for the given except ."], "references": ["returns a dataframe of per - 36 - minutes stats ."], "bleu": 0.08839374326825923, "rouge_l": 0.08176943699731902}
{"id": 4731, "code": "def stats per100 ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per poss' , kind = kind , summary = summary )", "predictions": ["method to catch make the amount of bytes managed by this instance ."], "references": ["returns a dataframe of per - 100 - possession stats ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 4732, "code": "def stats advanced ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'advanced' , kind = kind , summary = summary )", "predictions": ["method to attach stats arguments to this object ."], "references": ["returns a dataframe of advanced stats ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4733, "code": "def stats shooting ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'shooting' , kind = kind , summary = summary )", "predictions": ["method to catch the load operation for this object ."], "references": ["returns a dataframe of shooting stats ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 4734, "code": "def stats pbp ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'advanced pbp' , kind = kind , summary = summary )", "predictions": ["method to catch init init for this object ."], "references": ["returns a dataframe of play - by - play stats ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 4735, "code": "def Game Play Finder ( * * kwargs ) : querystring = kwargs to qs ( * * kwargs ) url = '{}?{}' . format ( GPF URL , querystring ) if kwargs . get ( 'verbose' , False ) : print ( url ) html = utils . get html ( url ) doc = pq ( html ) table = doc ( 'table#all plays' ) plays = utils . parse table ( table ) if 'score' in plays . columns : o Score , d Score = zip ( * plays . score . apply ( lambda s : s . split ( '-' ) ) ) plays [ 'team Score' ] = o Score plays [ 'opp Score' ] = d Score if 'description' in plays . columns : plays = pbp . expand details ( plays , detail Col = 'description' ) return plays", "predictions": ["performs the plays with respect to the specified = = = = . . . . . . . . . . . . . ."], "references": ["docstring will be filled in by __init__ . py"], "bleu": 0.04668049023095243, "rouge_l": 0.0626283367556468}
{"id": 4736, "code": "def get ( self ) : self . write ( \"Memory Session Object Demo:\" ) if \"sv\" in self . session : current value = self . session [ \"sv\" ] self . write ( \"current sv value is %s, and system will delete this value.<br/>\" % self . session [ \"sv\" ] ) self . session . delete ( \"sv\" ) if \"sv\" not in self . session : self . write ( \"current sv value is empty\" ) else : self . write ( \"Session data not found\" )", "predictions": ["creates and writes the session ."], "references": ["please don t do this in production environments ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4737, "code": "def Player Season Finder ( * * kwargs ) : if 'offset' not in kwargs : kwargs [ 'offset' ] = 0 player Seasons = [ ] while True : querystring = kwargs to qs ( * * kwargs ) url = '{}?{}' . format ( PSF URL , querystring ) if kwargs . get ( 'verbose' , False ) : print ( url ) html = utils . get html ( url ) doc = pq ( html ) table = doc ( 'table#results' ) df = utils . parse table ( table ) if df . empty : break this Season = list ( zip ( df . player id , df . year ) ) player Seasons . extend ( this Season ) if doc ( '*:contains(\"Next Page\")' ) : kwargs [ 'offset' ] += 100 else : break return player Seasons", "predictions": ["make all players out of the player ' s querystring ."], "references": ["docstring will be filled in by __init__ . py"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 4738, "code": "def wait ( self ) : with self . cvar : self . count . value += 1 self . cvar . notify all ( ) while self . count . value < self . n procs : self . cvar . wait ( )", "predictions": ["wait for this object to die ."], "references": ["wait until all processes have reached the barrier ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4739, "code": "def wait ( self ) : self . barrier A . wait ( ) self . barrier A , self . barrier B = self . barrier B , self . barrier A self . barrier A . reset ( )", "predictions": ["* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["wait until all processes have reached the barrier ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 4740, "code": "def close ( self ) : self . read queue . put ( Queue Closed ) self . write queue . put ( Queue Closed )", "predictions": ["closes this queue and releases its persistent queue ."], "references": ["close the queue signalling that no more data can be put into the queue ."], "bleu": 0.10247907767191411, "rouge_l": 0.23921568627450981}
{"id": 4741, "code": "def read varint ( self ) : buff = self . fd . read ( 1 ) if buff == b'' : return 0 while ( bytearray ( buff ) [ - 1 ] & 0x80 ) >> 7 == 1 : new byte = self . fd . read ( 1 ) if new byte == b'' : raise EOF Error ( 'unexpected EOF.' ) buff += new byte varint , = decode Varint ( buff , 0 ) return varint", "predictions": ["read a varint from the file ."], "references": ["read a varint from file parse it and return the decoded integer ."], "bleu": 0.24674341641124942, "rouge_l": 0.5692068429237948}
{"id": 4742, "code": "def close ( self ) : self . flush ( ) if self . myfd is not None : self . myfd . close ( ) self . myfd = None", "predictions": ["closes the underlying sockets and releases the underlying stream ."], "references": ["close the stream ."], "bleu": 0.17827531042796255, "rouge_l": 0.4644670050761421}
{"id": 4743, "code": "def flush ( self ) : if not self . is output ( ) : return count = len ( self . write buff ) if count == 0 : return encode Varint ( self . fd . write , count , True ) for obj in self . write buff : obj str = obj . Serialize To String ( ) encode Varint ( self . fd . write , len ( obj str ) , True ) self . fd . write ( obj str ) self . write buff = [ ]", "predictions": ["flush the object to the stream ."], "references": ["write down buffer to the file ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 4744, "code": "def get game dir ( self , username = False ) : if not self . common and not username : raise Runtime Error ( \"Can't determine this game's directory without username\" ) if self . common : subdir = \"common\" else : subdir = \"username\" subsubdir = self . dir if WIN32 or CYGWIN : subsubdir = subsubdir . lower ( ) return os . path . join ( subdir , subsubdir )", "predictions": ["returns the directory where the game is located ."], "references": ["returns joined game directory path relative to steamapps"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 4745, "code": "def with ignored exceptions ( self , * ignored exceptions ) : for exception in ignored exceptions : self . ignored exceptions = self . ignored exceptions + ( exception , ) return self", "predictions": ["return a list of exceptions that can be filled in a specific transaction ."], "references": ["set a list of exceptions that should be ignored inside the wait loop ."], "bleu": 0.34791594751284466, "rouge_l": 0.5}
{"id": 4746, "code": "def send ( self , message , read reply = False ) : sock = None for tries in range ( 0 , 3 ) : try : sock = socket . socket ( socket . AF INET , socket . SOCK STREAM ) sock . connect ( ( self . host , self . PORT ) ) break except ( Connection Error , Broken Pipe Error ) : if tries == 3 : print ( \"socket connect failed.\" ) return sleep ( 0.1 ) sock . send ( codecs . decode ( message , 'hex codec' ) ) if read reply : sleep ( 0.1 ) reply = '' tries = 0 max tries = 20 while len ( reply ) < len ( message ) and tries < max tries : try : reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) except ( Connection Error , Broken Pipe Error ) : pass tries += 1 sock . close ( ) if tries >= max tries : return return reply sock . close ( )", "predictions": ["sends a plain text request to the remote node ."], "references": ["send a command string to the amplifier ."], "bleu": 0.18850319022747347, "rouge_l": 0.4535315985130111}
{"id": 4747, "code": "def power off ( self ) : status = self . status ( ) if status [ 'power' ] : self . send ( self . CMD POWERSAVE + self . CMD OFF )", "predictions": ["the power of this object ."], "references": ["power the device off ."], "bleu": 0.2626909894424158, "rouge_l": 0.3696969696969697}
{"id": 4748, "code": "def power on ( self ) : status = self . status ( ) if not status [ 'power' ] : self . send ( self . CMD ON , read reply = True ) sleep ( 0.5 )", "predictions": ["sends the user to the power of this packet ."], "references": ["power the device on ."], "bleu": 0.14991106946711685, "rouge_l": 0.2837209302325582}
{"id": 4749, "code": "def set volume ( self , volume ) : if 0 <= volume <= 200 : volume = format ( volume , \"02x\" ) self . send ( self . CMD VOLUME + volume )", "predictions": ["set callbacks and private data for the volume ."], "references": ["set volume level of the device . accepts integer values 0 - 200 ."], "bleu": 0.10182634488642418, "rouge_l": 0.2510288065843621}
{"id": 4750, "code": "def select source ( self , source ) : status = self . status ( ) if status [ 'power' ] : if status [ 'source' ] != source : if source in self . SOURCES : self . send ( self . CMD SOURCE + self . SOURCES [ source ] , read reply = True )", "predictions": ["returns first step of the second ."], "references": ["select a source from the list of sources ."], "bleu": 0.16599826150636804, "rouge_l": 0.24448897795591182}
{"id": 4751, "code": "def exec command ( self , domain , function , operator , value = None ) : if operator in CMDS [ domain ] [ function ] [ 'supported operators' ] : if operator is '=' and value is None : raise Value Error ( 'No value provided' ) if value is None : cmd = '' . join ( [ CMDS [ domain ] [ function ] [ 'cmd' ] , operator ] ) else : cmd = '' . join ( [ CMDS [ domain ] [ function ] [ 'cmd' ] , operator , str ( value ) ] ) else : raise Value Error ( 'Invalid operator provided %s' % operator ) if self . open connection ( ) : self . telnet . write ( ( '' . join ( [ '\\r' , cmd , '\\n' ] ) . encode ( ) ) ) loop = 3 while loop : msg = self . telnet . read until ( '\\n' . encode ( ) , self . timeout ) if msg == \"\" : loop -= 1 continue msg = msg . decode ( ) . strip ( '\\r\\n' ) #print(\"NAD reponded with '%s'\" % msg) if msg . strip ( ) . split ( '=' ) [ 0 ] . lower ( ) == '.' . join ( [ domain , function ] ) . lower ( ) : return msg . strip ( ) . split ( '=' ) [ 1 ] raise Runtime Error ( 'Failed to read response' ) raise Runtime Error ( 'Failed to open connection' )", "predictions": ["this is a utility method for * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * . * ."], "references": ["write a command to the receiver and read the value it returns ."], "bleu": 0.026594139297659906, "rouge_l": 0.07101280558789291}
{"id": 4752, "code": "def crc ( plaintext ) : if not isinstance ( plaintext , six . binary type ) : plaintext = six . b ( plaintext ) return ( zlib . crc32 ( plaintext ) % 2147483647 ) & 0xffffffff", "predictions": ["determine if the plaintext or a plaintext operation is crc ."], "references": ["generates crc32 . modulo keep the value within int range ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 4753, "code": "def missing schema ( self , html , song name ) : #html=self.get html response(url) soup = Beautiful Soup ( html ) name = ' ' . join ( song name ) print '%s not found' % name print \"But you can download any of the following songs :\" a list = soup . find All ( 'a' , 'touch' ) for x in xrange ( len ( a list ) - 1 ) : r = a list [ x ] p = str ( r ) q = re . sub ( r'<a.*/>|<span.*\">|</span>|</a>|<a.*html\">|<font.*\">|</font>' , '' , p ) print q", "predictions": ["just a bunch of songs ."], "references": ["it will print the list of songs that can be downloaded"], "bleu": 0.12634437832866913, "rouge_l": 0.2234432234432234}
{"id": 4754, "code": "def list of all href ( self , html ) : soup = Beautiful Soup ( html ) links = [ ] a list = soup . find All ( 'a' , 'touch' ) for x in xrange ( len ( a list ) - 1 ) : link = a list [ x ] . get ( 'href' ) name = a list [ x ] name = str ( name ) name = re . sub ( r'<a.*/>|<span.*\">|</span>|</a>|<a.*html\">|<font.*\">|</font>' , '' , name ) name = re . sub ( r'^[0-9]+\\.' , '' , name ) links . append ( [ link , name ] ) #quit() return links", "predictions": ["list all links in this cluster ."], "references": ["it will return all hyper links found in the mr - jatt page for download"], "bleu": 0.0704451546128839, "rouge_l": 0.2559440559440559}
{"id": 4755, "code": "def check if song name ( self , html ) : soup = Beautiful Soup ( html ) a list = soup . find All ( 'a' , 'touch' ) #print a list text = [ str ( x ) for x in a list ] text = '' . join ( text ) text = text . lower ( ) string1 = 'download in 48 kbps' string2 = 'download in 128 kbps' string3 = 'download in 320 kbps' href = '' if string3 in text : #print 'Downloading in 320 kbps' href = a list [ 2 ] . get ( 'href' ) elif string2 in text : #print 'Downloading in 128 kbps' href = a list [ 1 ] . get ( 'href' ) elif string1 in text : #print 'Downloading in 48 kbps'\t href = a list [ 0 ] . get ( 'href' ) else : return ( True , 'nothing' ) return ( False , href )", "predictions": ["check if the html generated by the parser should be done ."], "references": ["returns true if user entered artist or movie name"], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 4756, "code": "def google url ( self , song name , website ) : name = '+' . join ( song name ) prefix = 'https://www.google.co.in/search?q=' website = website . split ( \" \" ) suffix = '+' . join ( website ) url = prefix + name + suffix #print url return url", "predictions": ["add a url to the set of google admin ones ."], "references": ["it will return the google url to be searched"], "bleu": 0.17033186037639278, "rouge_l": 0.2036727879799666}
{"id": 4757, "code": "def get html response ( self , url ) : print \"Downloading page %s ..\" % url try : response = requests . get ( url , timeout = 50 ) except requests . exceptions . SSL Error : try : response = requests . get ( url , verify = False , timeout = 50 ) except requests . exceptions . Request Exception as e : print e quit ( ) except requests . exceptions . Request Exception as e : print e quit ( ) return response . content", "predictions": ["run html using html ."], "references": ["it will download the html page specified by url and return the html response"], "bleu": 0.04994299940831281, "rouge_l": 0.19395866454689983}
{"id": 4758, "code": "def file download using requests ( self , url ) : file name = url . split ( '/' ) [ - 1 ] if os . path . exists ( os . path . join ( os . getcwd ( ) , file name ) ) : print 'File already exists' return #print 'Downloading file %s '%file name #print 'Downloading from %s'%url try : r = requests . get ( url , stream = True , timeout = 200 ) except requests . exceptions . SSL Error : try : response = requests . get ( url , stream = True , verify = False , timeout = 200 ) except requests . exceptions . Request Exception as e : print e quit ( ) except requests . exceptions . Request Exception as e : print e quit ( ) chunk size = 1024 total size = int ( r . headers [ 'Content-Length' ] ) total chunks = total size / chunk size file iterable = r . iter content ( chunk size = chunk size ) tqdm iter = tqdm ( iterable = file iterable , total = total chunks , unit = 'KB' , leave = False ) with open ( file name , 'wb' ) as f : for data in tqdm iter : f . write ( data ) #total size=float(r.headers['Content-Length'])/(1024*1024) print 'Downloaded file %s ' % file name", "predictions": ["retrieve all available requests from the specified url ."], "references": ["it will download file specified by url using requests module"], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 4759, "code": "def file download using wget ( self , url ) : file name = url . split ( '/' ) [ - 1 ] print 'Downloading file %s ' % file name command = 'wget -c --read-timeout=50 --tries=3 -q --show-progress --no-check-certificate ' url = '\"' + url + '\"' command = command + url os . system ( command )", "predictions": ["download the file at url ."], "references": ["it will download file specified by url using wget utility of linux"], "bleu": 0.09663861439684919, "rouge_l": 0.31443298969072164}
{"id": 4760, "code": "def main ( ) : #print VERSION from commands . download import Download options = docopt ( doc , version = VERSION ) #print \"You reached here\" #print options print \"working.\" p = Download ( options ) p . run ( )", "predictions": ["download download and install documentation ."], "references": ["main cli entrypoint ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4761, "code": "def find Station Codes By City ( city name , token ) : req = requests . get ( API ENDPOINT SEARCH , params = { 'token' : token , 'keyword' : city name } ) if req . status code == 200 and req . json ( ) [ \"status\" ] == \"ok\" : return [ result [ \"uid\" ] for result in req . json ( ) [ \"data\" ] ] else : return [ ]", "predictions": ["find a city from the city by name ."], "references": ["lookup aqi database for station codes in a given city ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 4762, "code": "def get location observation ( lat , lng , token ) : req = requests . get ( API ENDPOINT GEO % ( lat , lng ) , params = { 'token' : token } ) if req . status code == 200 and req . json ( ) [ \"status\" ] == \"ok\" : return parse observation response ( req . json ( ) [ \"data\" ] ) return { }", "predictions": ["get location of the supplied set of certificates . if the location is set to a method , return null . otherwise , return the location . otherwise , return the location of the location . if the location is set to the location . otherwise , return null ."], "references": ["lookup observations by geo coordinates ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 4763, "code": "def parse observation response ( json ) : logging . debug ( json ) iaqi = json [ 'iaqi' ] result = { 'idx' : json [ 'idx' ] , 'city' : json . get ( 'city' , '' ) , 'aqi' : json [ 'aqi' ] , 'dominentpol' : json . get ( \"dominentpol\" , '' ) , 'time' : json [ 'time' ] [ 's' ] , 'iaqi' : [ { 'p' : item , 'v' : iaqi [ item ] [ 'v' ] } for item in iaqi ] } return result", "predictions": ["parse the value of the key in the ( ."], "references": ["decode aqicn observation response json into python object ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 4764, "code": "def compilers ( self ) : return [ self . environment . compilers . get ( e ) for e in self . compiler extensions ]", "predictions": ["get information about this compiler ."], "references": ["the list of compilers used to build asset ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4765, "code": "def mimetype ( self ) : return ( self . environment . mimetypes . get ( self . format extension ) or self . compiler mimetype or 'application/octet-stream' )", "predictions": ["get the mimetype of this object ."], "references": ["mime type of the asset ."], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 4766, "code": "def compiler mimetype ( self ) : for compiler in reversed ( self . compilers ) : if compiler . result mimetype : return compiler . result mimetype return None", "predictions": ["get the most recent compiler ."], "references": ["implicit mime type of the asset by its compilers ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4767, "code": "def compiler format extension ( self ) : for extension , mimetype in self . environment . mimetypes . items ( ) : if mimetype == self . compiler mimetype : return extension return None", "predictions": ["adds an extension to the compiler ."], "references": ["implicit format extension on the asset by its compilers ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 4768, "code": "def register ( self , mimetype , processor ) : if mimetype not in self or processor not in self [ mimetype ] : self . setdefault ( mimetype , [ ] ) . append ( processor )", "predictions": ["registers the fits class around the result ."], "references": ["register passed processor for passed mimetype ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4769, "code": "def register defaults ( self ) : self . mimetypes . register defaults ( ) self . preprocessors . register defaults ( ) self . postprocessors . register defaults ( )", "predictions": ["registers a default image class ."], "references": ["register default compilers preprocessors and mime types ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4770, "code": "def table ( name , auth = None , eager = True ) : auth = auth or [ ] dynamodb = boto . connect dynamodb ( * auth ) table = dynamodb . get table ( name ) return Table ( table = table , eager = eager )", "predictions": ["create a wait for a wait ."], "references": ["returns a given table for the given user ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 4771, "code": "def tables ( auth = None , eager = True ) : auth = auth or [ ] dynamodb = boto . connect dynamodb ( * auth ) return [ table ( t , auth , eager = eager ) for t in dynamodb . list tables ( ) ]", "predictions": ["prepare the list of wait for the given server ."], "references": ["returns a list of tables for the given user ."], "bleu": 0.28997844147152074, "rouge_l": 0.6}
{"id": 4772, "code": "def metadata id ( item ) : if Crates . metadata category ( item ) == CATEGORY CRATES : return str ( item [ 'id' ] ) else : ts = item [ 'fetched on' ] ts = str to datetime ( ts ) return str ( ts . timestamp ( ) )", "predictions": ["generate close close of the ) ."], "references": ["extracts the identifier from an item depending on its type ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4773, "code": "def fetch crate owner team ( self , crate id ) : raw owner team = self . client . crate attribute ( crate id , 'owner team' ) owner team = json . loads ( raw owner team ) return owner team", "predictions": ["fetches data from the client at a given ( i . e . , the self - 1 . , the bytearray is the last call to . . . . . . . . . . . . . . . . . . ."], "references": ["get crate team owner"], "bleu": 0.021984661342973656, "rouge_l": 0.0}
{"id": 4774, "code": "def fetch crate owner user ( self , crate id ) : raw owner user = self . client . crate attribute ( crate id , 'owner user' ) owner user = json . loads ( raw owner user ) return owner user", "predictions": ["fetches and flushes the ( i . e . ) method from the request ."], "references": ["get crate user owners"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 4775, "code": "def fetch crate versions ( self , crate id ) : raw versions = self . client . crate attribute ( crate id , \"versions\" ) version downloads = json . loads ( raw versions ) return version downloads", "predictions": ["fetches data from this len and passes it to the ( if not already in the ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["get crate versions data"], "bleu": 0.02403051755364481, "rouge_l": 0.04375896700143472}
{"id": 4776, "code": "def fetch crate version downloads ( self , crate id ) : raw version downloads = self . client . crate attribute ( crate id , \"downloads\" ) version downloads = json . loads ( raw version downloads ) return version downloads", "predictions": ["deactivates data for a game ."], "references": ["get crate version downloads"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4777, "code": "def summary ( self ) : path = urijoin ( CRATES API URL , CATEGORY SUMMARY ) raw content = self . fetch ( path ) return raw content", "predictions": ["get a with the full , or passes it to the server ."], "references": ["get crates . io summary"], "bleu": 0.10571070857151538, "rouge_l": 0.24158415841584158}
{"id": 4778, "code": "def crates ( self , from page = 1 ) : path = urijoin ( CRATES API URL , CATEGORY CRATES ) raw crates = self . fetch items ( path , from page ) return raw crates", "predictions": ["create a send object from this url ."], "references": ["get crates in alphabetical order"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4779, "code": "def crate ( self , crate id ) : path = urijoin ( CRATES API URL , CATEGORY CRATES , crate id ) raw crate = self . fetch ( path ) return raw crate", "predictions": ["get a power of the url ."], "references": ["get a crate by its id"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 4780, "code": "def fetch items ( self , path , page = 1 ) : fetch data = True parsed crates = 0 total crates = 0 while fetch data : logger . debug ( \"Fetching page: %i\" , page ) try : payload = { 'sort' : 'alphabetical' , 'page' : page } raw content = self . fetch ( path , payload = payload ) content = json . loads ( raw content ) parsed crates += len ( content [ 'crates' ] ) if not total crates : total crates = content [ 'meta' ] [ 'total' ] except requests . exceptions . HTTP Error as e : logger . error ( \"HTTP exception raised - %s\" , e . response . text ) raise e yield raw content page += 1 if parsed crates >= total crates : fetch data = False", "predictions": ["fetches stats from given : - can be written to the server ."], "references": ["return the items from crates . io api using pagination"], "bleu": 0.1135935489027116, "rouge_l": 0.1781021897810219}
{"id": 4781, "code": "def fetch ( self , url , payload = None ) : response = super ( ) . fetch ( url , payload = payload ) return response . text", "predictions": ["fetches an item from the http response ."], "references": ["return the textual content associated to the response object"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 4782, "code": "def get questions ( self , offset = None ) : page = Kitsune Client . FIRST PAGE if offset : page += int ( offset / Kitsune Client . ITEMS PER PAGE ) while True : api questions url = urijoin ( self . base url , '/question' ) + '/' params = { \"page\" : page , \"ordering\" : \"updated\" } questions = self . fetch ( api questions url , params ) yield questions questions json = json . loads ( questions ) next uri = questions json [ 'next' ] if not next uri : break page += 1", "predictions": ["call . and return its source ."], "references": ["retrieve questions from older to newer updated starting offset"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 4783, "code": "def fetch ( self , url , params ) : logger . debug ( \"Kitsune client calls API: %s params: %s\" , url , str ( params ) ) response = super ( ) . fetch ( url , payload = params ) return response . text", "predictions": ["fetches stats from the server ."], "references": ["return the textual content associated to the response object"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4784, "code": "def get items ( self , category = CATEGORY EVENT , offset = REMO DEFAULT OFFSET ) : more = True next uri = None page = Re Mo Client . FIRST PAGE page += int ( offset / Re Mo Client . ITEMS PER PAGE ) if category == CATEGORY EVENT : api = self . api events url elif category == CATEGORY ACTIVITY : api = self . api activities url elif category == CATEGORY USER : api = self . api users url else : raise Value Error ( category + ' not supported in Re Mo' ) while more : params = { \"page\" : page , \"orderby\" : \"ASC\" } logger . debug ( \"Re Mo client calls AP Iv2: %s params: %s\" , api , str ( params ) ) raw items = self . fetch ( api , payload = params ) yield raw items items data = json . loads ( raw items ) next uri = items data [ 'next' ] if not next uri : more = False else : parsed uri = urllib . parse . urlparse ( next uri ) parsed params = urllib . parse . parse qs ( parsed uri . query ) page = parsed params [ 'page' ] [ 0 ]", "predictions": ["crc here we have to show up the ( = if there is no ( = 0 = 2 = 0 = 2 = 0 = 2 = 0 = 0 = 2 , 1 = 2 , 2 = 2 = 2 , 4 = 2 , 4 ="], "references": ["retrieve all items for category using pagination"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 4785, "code": "def io priority ( self ) : return ( self . iocb . aio reqprio if self . iocb . u . c . flags & libaio . IOCB FLAG IOPRIO else None )", "predictions": ["description of the method"], "references": ["io priority for this instance ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 4786, "code": "def get cells ( self ) : logger . info ( \"Retrieving all cells spreadsheet data ...\" ) logger . debug ( \"Mozilla Club client calls API: %s\" , self . base url ) raw cells = self . fetch ( self . base url ) return raw cells . text", "predictions": ["get a link to the underlying of the class ."], "references": ["retrieve all cells from the spreadsheet ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 4787, "code": "def parse ( self ) : nevents wrong = 0 feed json = json . loads ( self . feed ) if 'entry' not in feed json [ 'feed' ] : return self . cells = feed json [ 'feed' ] [ 'entry' ] self . ncell = 0 event fields = self . get event fields ( ) while self . ncell < len ( self . cells ) : event = self . get next event ( event fields ) if event [ 'Date of Event' ] is None or event [ 'Club Name' ] is None : logger . warning ( \"Wrong event data: %s\" , event ) nevents wrong += 1 continue yield event logger . info ( \"Total number of wrong events: %i\" , nevents wrong )", "predictions": ["iterates through the list of players and yields their results ."], "references": ["parse the mozillaclub spreadsheet feed cells json ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 4788, "code": "def get data files ( dirname ) : flist = [ ] for dirpath , dirnames , filenames in os . walk ( dirname ) : for fname in filenames : flist . append ( osp . join ( dirpath , fname ) ) return flist", "predictions": ["returns all files in this directory ."], "references": ["return data files in directory * dirname *"], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 4789, "code": "def export formats ( self , pid type ) : if pid type not in self . export formats : fmts = self . app . config . get ( 'RECORDS UI EXPORT FORMATS' , { } ) . get ( pid type , { } ) self . export formats [ pid type ] = sorted ( [ ( k , v ) for k , v in fmts . items ( ) if v ] , key = lambda x : x [ 1 ] [ 'order' ] , ) return self . export formats [ pid type ]", "predictions": ["get the , per , like available but will not prevent scan-build ."], "references": ["list of export formats ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 4790, "code": "def permission factory ( self ) : if self . permission factory is None : imp = self . app . config [ 'RECORDS UI DEFAULT PERMISSION FACTORY' ] self . permission factory = obj or import string ( imp ) return self . permission factory", "predictions": ["calculates a file for the given file ."], "references": ["load default permission factory ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4791, "code": "def records ( ) : import uuid from invenio records . api import Record from invenio pidstore . models import Persistent Identifier , PID Status with db . session . begin nested ( ) : pid1 = Persistent Identifier . create ( 'recid' , '1' , object type = 'rec' , object uuid = rec1 uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered ' , 'authors' : [ { 'name' : 'Ellis Jonathan' } , { 'name' : 'Higgs Peter' } , ] , 'access' : 'open' , 'keywords' : [ 'CERN' , 'higgs' ] , } , id = rec1 uuid ) Persistent Identifier . create ( 'recid' , '2' , object type = 'rec' , object uuid = rec2 uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered ' , 'authors' : [ { 'name' : 'Ellis Jonathan' } , { 'name' : 'Higgs Peter' } , ] , 'access' : 'closed' , 'keywords' : [ 'CERN' , 'higgs' ] , } , id = rec2 uuid ) rec3 uuid = uuid . uuid4 ( ) pid = Persistent Identifier . create ( 'recid' , '3' , object type = 'rec' , object uuid = rec3 uuid , status = PID Status . REGISTERED ) pid . delete ( ) Record . create ( { 'title' : 'Live ' } , id = rec3 uuid ) Persistent Identifier . create ( 'recid' , '4' , status = PID Status . DELETED ) Persistent Identifier . create ( 'recid' , '5' , status = PID Status . REGISTERED ) pid = Persistent Identifier . create ( 'recid' , '6' , status = PID Status . REGISTERED ) pid . redirect ( pid1 ) doi = Persistent Identifier . create ( 'doi' , '10.1234/foo' , status = PID Status . REGISTERED ) pid = Persistent Identifier . create ( 'recid' , '7' , status = PID Status . REGISTERED ) pid . redirect ( doi ) Persistent Identifier . create ( 'recid' , '8' , status = PID Status . RESERVED ) db . session . commit ( )", "predictions": ["creates a ( object from database"], "references": ["load test data fixture ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4792, "code": "def time callable ( self , name , target , rate = None , args = ( ) , kwargs = { } ) : assert callable ( target ) if rate is None : rate = self . rate else : assert sample rate ( rate ) start time = time ( ) result = target ( * args , * * kwargs ) self . since ( name , start time , rate ) return result", "predictions": ["return a ( from a from the from the given arguments and from the given from 0 to 1 . 4 . 5 . 4 . 3 . 0 ."], "references": ["send a timer metric calculating duration of execution of the provided callable"], "bleu": 0.044644767873512764, "rouge_l": 0.10321489001692045}
{"id": 4793, "code": "def increment ( self , name , count = 1 , rate = 1 ) : if self . should send metric ( name , rate ) : self . request ( Counter ( self . create metric name for request ( name ) , int ( count ) , rate ) . to request ( ) )", "predictions": ["increments the counter of an event ."], "references": ["increment a counter metric"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4794, "code": "def timing ( self , name , milliseconds , rate = 1 ) : if self . should send metric ( name , rate ) : milliseconds = int ( milliseconds ) self . request ( Timer ( self . create metric name for request ( name ) , milliseconds , rate ) . to request ( ) )", "predictions": ["creates a new qt4 ."], "references": ["send a timer metric with the specified duration in milliseconds"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4795, "code": "def timing since ( self , name , start time , rate = 1 ) : duration = 0 if isinstance ( start time , datetime ) : duration = ( datetime . now ( start time . tzinfo ) - start time ) . total seconds ( ) * 1000 elif is numeric ( start time ) : assert start time > 0 duration = ( time ( ) - start time ) * 1000 else : raise Value Error ( \"start time should be a timestamp or a datetime\" ) self . timing ( name , duration , rate )", "predictions": ["creates a new instance of ("], "references": ["send a timer metric calculating the duration from the start time"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 4796, "code": "def gauge ( self , name , value , rate = 1 ) : if self . should send metric ( name , rate ) : if not is numeric ( value ) : value = float ( value ) self . request ( Gauge ( self . create metric name for request ( name ) , value , rate ) . to request ( ) )", "predictions": ["creates a new correct_answer object ."], "references": ["send a gauge metric with the specified value"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4797, "code": "def gauge delta ( self , name , delta , rate = 1 ) : if self . should send metric ( name , rate ) : if not is numeric ( delta ) : delta = float ( delta ) self . request ( Gauge Delta ( self . create metric name for request ( name ) , delta , rate ) . to request ( ) )", "predictions": ["makes a new popen ."], "references": ["send a gaugedelta metric to change a gauge by the specified value"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 4798, "code": "def set ( self , name , value , rate = 1 ) : if self . should send metric ( name , rate ) : value = str ( value ) self . request ( Set ( self . create metric name for request ( name ) , value , rate ) . to request ( ) )", "predictions": ["create and compiler a specific for the specified for the current clip ."], "references": ["send a set metric with the specified unique value"], "bleu": 0.1350862565735141, "rouge_l": 0.2819722650231125}
{"id": 4799, "code": "def request ( self , data ) : data = bytearray ( \"{}\\n\" . format ( data ) . encode ( ) ) self . prepare batches for storage ( len ( data ) ) self . batches [ - 1 ] . extend ( data )", "predictions": ["get the compiler that is verified by this instance ."], "references": ["override parent by buffering the metric instead of sending now"], "bleu": 0.13950796967929133, "rouge_l": 0.1}
{"id": 4800, "code": "def batch client ( self , size = 512 ) : batch client = Batch Client ( self . host , self . port , self . prefix , size ) self . configure client ( batch client ) return batch client", "predictions": ["create a batch of ( for a batch of things ."], "references": ["return a batch client with same settings of the client"], "bleu": 0.16108992769687397, "rouge_l": 0.28818897637795277}
{"id": 4801, "code": "def unit client ( self ) : client = Client ( self . host , self . port , self . prefix ) self . configure client ( client ) return client", "predictions": ["helper method to get a client for a client ."], "references": ["return a client with same settings of the batch client"], "bleu": 0.17827531042796255, "rouge_l": 0.3}
{"id": 4802, "code": "def flush ( self ) : address = self . remote address while len ( self . batches ) > 0 : self . socket . sendto ( self . batches [ 0 ] , address ) self . batches . popleft ( ) return self", "predictions": ["flushes the output stream of this operator to the ui thread ."], "references": ["send buffered metrics in batch requests"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 4803, "code": "def my permission factory ( record , * args , * * kwargs ) : def can ( self ) : rec = Record . get record ( record . id ) return rec . get ( 'access' , '' ) == 'open' return type ( 'My Permission Checker' , ( ) , { 'can' : can } ) ( )", "predictions": ["factory for the permission ."], "references": ["my permission factory ."], "bleu": 0.32466791547509893, "rouge_l": 0.4535315985130111}
{"id": 4804, "code": "def batch client ( self , size = 512 ) : batch client = TCP Batch Client ( self . host , self . port , self . prefix , size ) self . configure client ( batch client ) return batch client", "predictions": ["create a batch of ( for a batch of things ."], "references": ["return a tcp batch client with same settings of the tcp client"], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 4805, "code": "def flush ( self ) : while len ( self . batches ) > 0 : self . socket . sendall ( self . batches [ 0 ] ) self . batches . popleft ( ) return self", "predictions": ["flush the output buffer to the processed stream ."], "references": ["send buffered metrics in batch requests over tcp"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4806, "code": "def unit client ( self ) : client = TCP Client ( self . host , self . port , self . prefix ) self . configure client ( client ) return client", "predictions": ["helper method to get a client object for a given unit ."], "references": ["return a tcpclient with same settings of the batch tcp client"], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 4807, "code": "def convert As Open Math ( term , converter ) : if hasattr ( term , \" ishelper\" ) and term . ishelper or isinstance ( term , om . OM Any ) : return interpret As Open Math ( term ) if converter is not None : try : converted = converter . to openmath ( term ) except Exception as e : converted = None if isinstance ( converted , om . OM Any ) : return converted return interpret As Open Math ( term )", "predictions": ["converts plain text to x509certificate ."], "references": ["converts a term into openmath using either a converter or the interpretasopenmath method"], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 4808, "code": "def to python ( self , omobj ) : if omobj . class in self . omclass to py : return self . omclass to py [ omobj . class ] ( omobj ) elif isinstance ( omobj , om . OM Symbol ) : return self . lookup to python ( omobj . cdbase , omobj . cd , omobj . name ) elif isinstance ( omobj , om . OM Application ) : elem = self . to python ( omobj . elem ) arguments = [ self . to python ( x ) for x in omobj . arguments ] return elem ( * arguments ) raise Value Error ( 'Cannot convert object of class %s to Python.' % omobj . class . name )", "predictions": ["convert this companion to a python type ."], "references": ["convert openmath object to python"], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 4809, "code": "def to openmath ( self , obj ) : for cl , conv in reversed ( self . conv to om ) : if cl is None or isinstance ( obj , cl ) : try : return conv ( obj ) except Cannot Convert Error : continue if hasattr ( obj , ' openmath ' ) : return obj . openmath ( ) raise Value Error ( 'Cannot convert %r to Open Math.' % obj )", "predictions": ["convert to lowercase . note : not really really really really really really be converted to lowercase ."], "references": ["convert python object to openmath"], "bleu": 0.07535838128770536, "rouge_l": 0.19365079365079363}
{"id": 4810, "code": "def init app ( self , app ) : app . config . setdefault ( 'REDIS URLS' , { 'main' : 'redis://localhost:6379/0' , 'admin' : 'redis://localhost:6379/1' , } ) app . before request ( self . before request ) self . app = app", "predictions": ["initializes the module object ."], "references": ["used to initialize redis with app object"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4811, "code": "def valid choices ( choices ) : for key , value in choices : if isinstance ( value , ( list , tuple ) ) : for key , in value : yield key else : yield key", "predictions": ["generate an ordereddict that yields the choices of all choices ."], "references": ["return list of choices s keys"], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 4812, "code": "def split model kwargs ( kw ) : from collections import defaultdict model fields = { } fields agrs = defaultdict ( lambda : { } ) for key in kw . keys ( ) : if ' ' in key : field , , subfield = key . partition ( ' ' ) fields agrs [ field ] [ subfield ] = kw [ key ] else : model fields [ key ] = kw [ key ] return model fields , fields agrs", "predictions": ["take a value and return them ."], "references": ["django_any birds language parser"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4813, "code": "def any form default ( form cls , * * kwargs ) : form data = { } form files = { } form fields , fields args = split model kwargs ( kwargs ) for name , field in form cls . base fields . iteritems ( ) : if name in form fields : form data [ name ] = kwargs [ name ] else : form data [ name ] = any form field ( field , * * fields args [ name ] ) return form data , form files", "predictions": ["creates a form class for a form class ."], "references": ["returns tuple with form data and files"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4814, "code": "def field choices attibute ( function ) : def wrapper ( field , * * kwargs ) : if hasattr ( field . widget , 'choices' ) : return random . choice ( list ( valid choices ( field . widget . choices ) ) ) return function ( field , * * kwargs ) return wrapper", "predictions": ["wrapper around wrapper that can be used to wrapper around field names ."], "references": ["selection from field . choices"], "bleu": 0.10571070857151538, "rouge_l": 0.24158415841584158}
{"id": 4815, "code": "def model choice field data ( field , * * kwargs ) : data = list ( field . queryset [ : 10 ] ) if data : return random . choice ( data ) else : raise Type Error ( 'No %s available in queryset' % field . queryset . model )", "predictions": ["generate a random model of the given data ."], "references": ["return one of first ten items for field queryset"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4816, "code": "def tag ( version = version ) : build = local ( \"git tag {0}\" . format ( version ) ) if build . succeeded : local ( \"git push --tags\" )", "predictions": ["create a tag to read from the given version ."], "references": ["deploy a version tag ."], "bleu": 0.15851165692617156, "rouge_l": 0.42558139534883715}
{"id": 4817, "code": "def any field blank ( function ) : def wrapper ( field , * * kwargs ) : if kwargs . get ( 'isnull' , False ) : return None if field . blank and random . random < 0.1 : return None return function ( field , * * kwargs ) return wrapper", "predictions": ["returns a field that is any of the passed in field ."], "references": ["sometimes return none if field could be blank"], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 4818, "code": "def any file field ( field , * * kwargs ) : def get some file ( path ) : subdirs , files = field . storage . listdir ( path ) if files : result file = random . choice ( files ) instance = field . storage . open ( \"%s/%s\" % ( path , result file ) ) . file return Field File ( instance , field , result file ) for subdir in subdirs : result = get some file ( \"%s/%s\" % ( path , subdir ) ) if result : return result result = get some file ( field . upload to ) if result is None and not field . null : raise Type Error ( \"Can't found file in %s for non nullable File Field\" % field . upload to ) return result", "predictions": ["uploads an paths to all files in this directory ."], "references": ["lookup for nearest existing file"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4819, "code": "def any filepath field ( field , * * kwargs ) : def get some file ( path ) : subdirs , files = [ ] , [ ] for entry in os . listdir ( path ) : entry path = os . path . join ( path , entry ) if os . path . isdir ( entry path ) : subdirs . append ( entry path ) else : if not field . match or re . match ( field . match , entry ) : files . append ( entry path ) if files : return random . choice ( files ) if field . recursive : for subdir in subdirs : result = get some file ( subdir ) if result : return result result = get some file ( field . path ) if result is None and not field . null : raise Type Error ( \"Can't found file in %s for non nullable File Path Field\" % field . path ) return result", "predictions": ["for all files in the provided directory ."], "references": ["lookup for nearest existing file"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4820, "code": "def decode ( data ) : data = bytearray ( data ) result = bytearray ( ) pos = 0 while pos < len ( data ) : header byte = data [ pos ] if header byte > 127 : header byte -= 256 pos += 1 if 0 <= header byte <= 127 : result . extend ( data [ pos : pos + header byte + 1 ] ) pos += header byte + 1 elif header byte == - 128 : pass else : result . extend ( [ data [ pos ] ] * ( 1 - header byte ) ) pos += 1 return bytes ( result )", "predictions": ["decode data from a byte buffer ."], "references": ["decodes a packbit encoded data ."], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 4821, "code": "def encode ( data ) : if len ( data ) == 0 : return data if len ( data ) == 1 : return b'\\x00' + data data = bytearray ( data ) result = bytearray ( ) buf = bytearray ( ) pos = 0 repeat count = 0 MAX LENGTH = 127 state = 'RAW' def finish raw ( ) : if len ( buf ) == 0 : return result . append ( len ( buf ) - 1 ) result . extend ( buf ) buf [ : ] = bytearray ( ) def finish rle ( ) : result . append ( 256 - ( repeat count - 1 ) ) result . append ( data [ pos ] ) while pos < len ( data ) - 1 : current byte = data [ pos ] if data [ pos ] == data [ pos + 1 ] : if state == 'RAW' : finish raw ( ) state = 'RLE' repeat count = 1 elif state == 'RLE' : if repeat count == MAX LENGTH : finish rle ( ) repeat count = 0 repeat count += 1 else : if state == 'RLE' : repeat count += 1 finish rle ( ) state = 'RAW' repeat count = 0 elif state == 'RAW' : if len ( buf ) == MAX LENGTH : finish raw ( ) buf . append ( current byte ) pos += 1 if state == 'RAW' : buf . append ( data [ pos ] ) finish raw ( ) else : repeat count += 1 finish rle ( ) return bytes ( result )", "predictions": ["uuids data from the input stream , and returns the number of bytes converted to byte chunks ."], "references": ["encodes data using packbits encoding ."], "bleu": 0.07535838128770536, "rouge_l": 0.18318318318318316}
{"id": 4822, "code": "def add ( self , name , path ) : if not ( os . path . exists ( path ) ) : raise Value Error ( \"Workspace path `%s` doesn't exists.\" % path ) if ( self . exists ( name ) ) : raise Value Error ( \"Workspace `%s` already exists.\" % name ) self . config [ \"workspaces\" ] [ name ] = { \"path\" : path , \"repositories\" : { } } self . config . write ( )", "predictions": ["add a file at path"], "references": ["add a workspace entry in user config file ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 4823, "code": "def remove ( self , name ) : if not ( self . exists ( name ) ) : raise Value Error ( \"Workspace `%s` doesn't exists.\" % name ) self . config [ \"workspaces\" ] . pop ( name , 0 ) self . config . write ( )", "predictions": ["remove the named ( from the list ."], "references": ["remove workspace from config file ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 4824, "code": "def list ( self ) : ws list = { } for key , value in self . config [ \"workspaces\" ] . items ( ) : ws list [ key ] = dict ( { \"name\" : key } , * * value ) return ws list", "predictions": ["list all the items in this map ."], "references": ["list all available workspaces ."], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 4825, "code": "def repository exists ( self , workspace , repo ) : if not self . exists ( workspace ) : return False workspaces = self . list ( ) return repo in workspaces [ workspace ] [ \"repositories\" ]", "predictions": ["check if this repository exists in this directory ."], "references": ["return true if workspace contains repository name ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 4826, "code": "def sync ( self , ws name ) : path = self . config [ \"workspaces\" ] [ ws name ] [ \"path\" ] repositories = self . config [ \"workspaces\" ] [ ws name ] [ \"repositories\" ] logger = logging . get Logger ( name ) color = Color ( ) for r in os . listdir ( path ) : try : repo = Repository ( os . path . join ( path , r ) ) except Repository Error : continue else : repositories [ r ] = repo . path for repo name , path in repositories . items ( ) : logger . info ( color . colored ( \" - %s\" % repo name , \"blue\" ) ) self . config [ \"workspaces\" ] [ ws name ] [ \"repositories\" ] self . config . write ( )", "predictions": ["creates a cdt repo ."], "references": ["synchronise workspace s repositories ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 4827, "code": "def clone ( url , path ) : adapter = None if url [ : 4 ] == \"git@\" or url [ - 4 : ] == \".git\" : adapter = Git ( path ) if url [ : 6 ] == \"svn://\" : adapter = Svn ( path ) if url [ : 6 ] == \"bzr://\" : adapter = Bzr ( path ) if url [ : 9 ] == \"ssh://hg@\" : adapter = Hg ( path ) if adapter is None : raise Repository Adapter Not Found ( \"Can't find adapter for `%s` repository url\" % url ) return adapter . clone ( url )", "predictions": ["clones the specified url to the repository ."], "references": ["clone a repository ."], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 4828, "code": "def check version ( ) : import requests r = requests . get ( 'https://pypi.python.org/pypi/ndio/json' ) . json ( ) r = r [ 'info' ] [ 'version' ] if r != version : print ( \"A newer version of ndio is available. \" + \"'pip install -U ndio' to update.\" ) return r", "predictions": ["check if there are any existing newer newer than the server ."], "references": ["tells you if you have an old version of ndio ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 4829, "code": "def execute ( self , args ) : if args . name is not None : self . print workspace ( args . name ) elif args . all is not None : self . print all ( )", "predictions": ["simple entry point for the operation ."], "references": ["execute update subcommand ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4830, "code": "def print update ( self , repo name , repo path ) : color = Color ( ) self . logger . info ( color . colored ( \"=> [%s] %s\" % ( repo name , repo path ) , \"green\" ) ) try : repo = Repository ( repo path ) repo . update ( ) except Repository Error as e : self . logger . error ( e ) pass print ( \"\\n\" )", "predictions": ["a utility function to draw a repository ."], "references": ["print repository update ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 4831, "code": "def set console handler ( self , debug = False ) : console = logging . Stream Handler ( ) console . set Formatter ( Formatter ( LFORMAT ) ) if not debug : console . set Level ( logging . INFO ) self . add Handler ( console )", "predictions": ["set the console output handler for the console ."], "references": ["set console handler ."], "bleu": 0.17747405280050263, "rouge_l": 0.6612466124661246}
{"id": 4832, "code": "def execute ( self , command , path = None ) : logger = logging . get Logger ( name ) self . check executable ( ) logger . debug ( \"Executing command `%s` (cwd: %s)\" % ( command , path ) ) process = subprocess . Popen ( command , shell = True , cwd = path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) stdout , stderr = process . communicate ( ) exit code = process . wait ( ) if stdout : logger . info ( stdout . decode ( \"utf-8\" ) ) if stderr : if exit code != 0 : logger . error ( stderr . decode ( \"utf-8\" ) ) else : logger . info ( stderr . decode ( \"utf-8\" ) ) return process", "predictions": ["this method runs the child process ."], "references": ["execute command with os . popen and return output ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4833, "code": "def print workspace ( self , name ) : path list = find path ( name , self . config ) if len ( path list ) == 0 : self . logger . error ( \"No matches for `%s`\" % name ) return False for name , path in path list . items ( ) : self . print status ( name , path )", "predictions": ["outputs all . this method is intended to be called to provide elements to the client before the other threads ."], "references": ["print workspace status ."], "bleu": 0.05809665204409193, "rouge_l": 0.09118086696562032}
{"id": 4834, "code": "def print status ( self , repo name , repo path ) : color = Color ( ) self . logger . info ( color . colored ( \"=> [%s] %s\" % ( repo name , repo path ) , \"green\" ) ) try : repo = Repository ( repo path ) repo . status ( ) except Repository Error as e : self . logger . error ( e ) pass print ( \"\\n\" )", "predictions": ["a physically physically for a given ."], "references": ["print repository status ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4835, "code": "def post cutout no chunking blosc ( self , token , channel , x start , y start , z start , data , resolution ) : data = numpy . expand dims ( data , axis = 0 ) blosc data = blosc . pack array ( data ) url = self . url ( \"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\" . format ( token , channel , resolution , x start , x start + data . shape [ 3 ] , y start , y start + data . shape [ 2 ] , z start , z start + data . shape [ 1 ] ) ) req = self . remote utils . post url ( url , data = blosc data , headers = { 'Content-Type' : 'application/octet-stream' } ) if req . status code is not 200 : raise Remote Data Upload Error ( req . text ) else : return True", "predictions": ["construct permission from this . instance ."], "references": ["accepts data in zyx . !!!"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4836, "code": "def clone ( self , url ) : return self . execute ( \"%s branch %s %s\" % ( self . executable , url , self . path ) )", "predictions": ["copies this schema as an item in the database ."], "references": ["clone repository from url ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 4837, "code": "def get version ( ) : requirement = pkg resources . Requirement . parse ( \"yoda\" ) provider = pkg resources . get provider ( requirement ) return provider . version", "predictions": ["flush the selinux ( e . g . , ( , ( , ( ) ."], "references": ["get version from package resources ."], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 4838, "code": "def mix and match ( name , greeting = 'Hello' , yell = False ) : say = '%s, %s' % ( greeting , name ) if yell : print '%s!' % say . upper ( ) else : print '%s.' % say", "predictions": ["create an ( and print ."], "references": ["mixing and matching positional args and keyword options ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4839, "code": "def option decorator ( name , greeting , yell ) : say = '%s, %s' % ( greeting , name ) if yell : print '%s!' % say . upper ( ) else : print '%s.' % say", "predictions": ["decorator an convert to a value ."], "references": ["same as mix_and_match but using the"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4840, "code": "def parse ( self ) : parser = self . subparser . add parser ( \"show\" , help = \"Show workspace details\" , description = \"Show workspace details.\" ) group = parser . add mutually exclusive group ( required = True ) group . add argument ( '--all' , action = 'store true' , help = \"All workspaces\" ) group . add argument ( 'name' , type = str , help = \"Workspace name\" , nargs = '?' )", "predictions": ["parses the command line ."], "references": ["parse show subcommand ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4841, "code": "def execute ( self , args ) : if args . name is not None : self . show workspace ( slashes2dash ( args . name ) ) elif args . all is not None : self . show all ( )", "predictions": ["executes this method as part of the command execution ."], "references": ["execute show subcommand ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 4842, "code": "def show workspace ( self , name ) : if not self . workspace . exists ( name ) : raise Value Error ( \"Workspace `%s` doesn't exists.\" % name ) color = Color ( ) workspaces = self . workspace . list ( ) self . logger . info ( \"<== %s workspace ==>\" % color . colored ( name , \"green\" ) ) self . logger . info ( \"\\t Path: %s\" % workspaces [ name ] [ \"path\" ] ) self . logger . info ( \"\\t Number of repositories: %s\" % color . colored ( len ( workspaces [ name ] [ \"repositories\" ] ) , \"yellow\" ) ) repo colored = color . colored ( \"Repositories\" , \"blue\" ) path colored = color . colored ( \"Path\" , \"blue\" ) trepositories = Pretty Table ( [ repo colored , path colored , color . colored ( \"+\" , \"blue\" ) ] ) trepositories . align [ repo colored ] = \"l\" trepositories . align [ path colored ] = \"l\" for repo name in workspaces [ name ] [ \"repositories\" ] : fullname = \"%s/%s\" % ( name , repo name ) fullpath = find path ( fullname , self . config ) [ fullname ] try : repo = Repository ( fullpath ) repo scm = repo . get scm ( ) except Repository Adapter Not Found : repo scm = None trepositories . add row ( [ color . colored ( repo name , \"cyan\" ) , fullpath , repo scm ] ) self . logger . info ( trepositories )", "predictions": ["generate and save a app in the database ."], "references": ["show specific workspace ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4843, "code": "def show all ( self ) : for ws in self . workspace . list ( ) . keys ( ) : self . show workspace ( ws ) print ( \"\\n\\n\" )", "predictions": ["show all if this noun is destroyed ."], "references": ["show details for all workspaces ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 4844, "code": "def RAMON ( typ ) : if six . PY2 : lookup = [ str , unicode ] elif six . PY3 : lookup = [ str ] if type ( typ ) is int : return ramon types [ typ ] elif type ( typ ) in lookup : return ramon types [ types [ typ ] ]", "predictions": ["tries to guess the type of the given type based on its type ."], "references": ["takes str or int returns class type"], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 4845, "code": "def nd json ( self , dataset , project , channel list , metadata ) : nd dict = { } nd dict [ 'dataset' ] = self . dataset dict ( * dataset ) nd dict [ 'project' ] = self . project dict ( * project ) nd dict [ 'metadata' ] = metadata nd dict [ 'channels' ] = { } for channel name , value in channel list . items ( ) : nd dict [ 'channels' ] [ channel name ] = self . channel dict ( * value ) return json . dumps ( nd dict , sort keys = True , indent = 4 )", "predictions": ["creates a form for till a form field ."], "references": ["genarate nd json object ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4846, "code": "def dataset dict ( self , dataset name , imagesize , voxelres , offset , timerange , scalinglevels , scaling ) : dataset dict = { } dataset dict [ 'dataset name' ] = dataset name dataset dict [ 'imagesize' ] = imagesize dataset dict [ 'voxelres' ] = voxelres if offset is not None : dataset dict [ 'offset' ] = offset if timerange is not None : dataset dict [ 'timerange' ] = timerange if scalinglevels is not None : dataset dict [ 'scalinglevels' ] = scalinglevels if scaling is not None : dataset dict [ 'scaling' ] = scaling return dataset dict", "predictions": ["gets the field for this field ."], "references": ["generate the dataset dictionary"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4847, "code": "def channel dict ( self , channel name , datatype , channel type , data url , file format , file type , exceptions , resolution , windowrange , readonly ) : channel dict = { } channel dict [ 'channel name' ] = channel name channel dict [ 'datatype' ] = datatype channel dict [ 'channel type' ] = channel type if exceptions is not None : channel dict [ 'exceptions' ] = exceptions if resolution is not None : channel dict [ 'resolution' ] = resolution if windowrange is not None : channel dict [ 'windowrange' ] = windowrange if readonly is not None : channel dict [ 'readonly' ] = readonly channel dict [ 'data url' ] = data url channel dict [ 'file format' ] = file format channel dict [ 'file type' ] = file type return channel dict", "predictions": ["sets the model for this model ."], "references": ["generate the project dictionary ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4848, "code": "def project dict ( self , project name , token name , public ) : project dict = { } project dict [ 'project name' ] = project name if token name is not None : if token name == '' : project dict [ 'token name' ] = project name else : project dict [ 'token name' ] = token name else : project dict [ 'token name' ] = project name if public is not None : project dict [ 'public' ] = public return project dict", "predictions": ["sets the tag that is stored for the tag . this is only called when parsing the tag will be activated ."], "references": ["genarate the project dictionary ."], "bleu": 0.0612957497932821, "rouge_l": 0.16712328767123286}
{"id": 4849, "code": "def identify imagesize ( self , image type , image path = '/tmp/img.' ) : dims = ( ) try : if ( image type . lower ( ) == 'png' ) : dims = np . shape ( ndpng . load ( '{}{}' . format ( image path , image type ) ) ) elif ( image type . lower ( ) == 'tif' or image type . lower ( ) == 'tiff' ) : dims = np . shape ( ndtiff . load ( '{}{}' . format ( image path , image type ) ) ) else : raise Value Error ( \"Unsupported image type.\" ) except : raise OS Error ( 'The file was not accessible at {}{}' . format ( image path , image type ) ) return dims [ : : - 1 ]", "predictions": ["any ) of this ) . the ) can be used to any of the ) of the ) ."], "references": ["identify the image size using the data location and other parameters"], "bleu": 0.06760229884571738, "rouge_l": 0.1361607142857143}
{"id": 4850, "code": "def put data ( self , data ) : URL Path = self . oo . url ( \"auto Ingest/\" ) try : response = requests . post ( URL Path , data = json . dumps ( data ) , verify = False ) assert ( response . status code == 200 ) print ( \"From ndio: {}\" . format ( response . content ) ) except : raise OS Error ( . format ( response . status code ) )", "predictions": ["store an http response returned from the specified file"], "references": ["try to post data to the server ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4851, "code": "def find path ( name , config , wsonly = False ) : workspace = Workspace ( config ) config = config [ \"workspaces\" ] path list = { } if name . find ( '/' ) != - 1 : wsonly = False try : ws , repo = name . split ( '/' ) except Value Error : raise Value Error ( \"There is too many / in `name` argument. \" \"Argument syntax: `workspace/repository`.\" ) if ( workspace . exists ( ws ) ) : if ( repo in config [ ws ] [ \"repositories\" ] ) : path name = \"%s/%s\" % ( ws , repo ) path list [ path name ] = config [ ws ] [ \"repositories\" ] [ repo ] for ws name , ws in sorted ( config . items ( ) ) : if ( name == ws name ) : if wsonly is True : return { ws name : ws [ \"path\" ] } repositories = sorted ( config [ ws name ] [ \"repositories\" ] . items ( ) ) for name , path in repositories : path list [ \"%s/%s\" % ( ws name , name ) ] = path break for repo name , repo path in sorted ( ws [ \"repositories\" ] . items ( ) ) : if ( repo name == name ) : path list [ \"%s/%s\" % ( ws name , repo name ) ] = repo path return path list", "predictions": ["any ( that is specified in the * . if the filepath is found , then it is returned . otherwise , it is assumed to be used to figure out the value ."], "references": ["find path for given workspace and|or repository ."], "bleu": 0.03551851328486764, "rouge_l": 0.053602811950790856}
{"id": 4852, "code": "def nvim io recover ( self , io : Nvim IO Recover [ A ] ) -> Nvim IO [ B ] : return eval step ( self . vim ) ( io . map ( lambda a : a ) )", "predictions": ["data for a one more elements of this class ."], "references": ["calls map to shift the recover execution to flat_map_nvim_io"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4853, "code": "def ugettext ( message , context = None ) : stripped = strip whitespace ( message ) message = add context ( context , stripped ) if context else stripped ret = django ugettext ( message ) return stripped if ret == message else ret", "predictions": ["send encode data and returns result ."], "references": ["always return a stripped string localized if possible"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 4854, "code": "def ungettext ( singular , plural , number , context = None ) : singular stripped = strip whitespace ( singular ) plural stripped = strip whitespace ( plural ) if context : singular = add context ( context , singular stripped ) plural = add context ( context , plural stripped ) else : singular = singular stripped plural = plural stripped ret = django nugettext ( singular , plural , number ) if ret == singular : return singular stripped elif ret == plural : return plural stripped return ret", "predictions": ["create a name to the core view for the given self . this is used when the self - redraw is made to add to the view ."], "references": ["always return a stripped string localized if possible"], "bleu": 0.04327969719414172, "rouge_l": 0.0617408906882591}
{"id": 4855, "code": "def install jinja translations ( ) : class Translation ( object ) : ugettext = staticmethod ( ugettext ) ungettext = staticmethod ( ungettext ) import jingo jingo . env . install gettext translations ( Translation )", "predictions": ["remove all the minimal self - import self - [ . ] and installs them ."], "references": ["install our gettext and ngettext functions into jinja2 s environment ."], "bleu": 0.08513012360883544, "rouge_l": 0.15326633165829145}
{"id": 4856, "code": "def exclusive ns ( guard : State Guard [ A ] , desc : str , thunk : Callable [ ... , NS [ A , B ] ] , * a : Any ) -> Do : yield guard . acquire ( ) log . debug2 ( lambda : f'exclusive: {desc}' ) state , response = yield N . ensure failure ( thunk ( * a ) . run ( guard . state ) , guard . release ) yield N . delay ( lambda v : unsafe update state ( guard , state ) ) yield guard . release ( ) log . debug2 ( lambda : f'release: {desc}' ) yield N . pure ( response )", "predictions": ["list all list of list list list in the shared program ."], "references": ["this is the central unsafe function using a lock and updating the state in guard in - place ."], "bleu": 0.06895205739037245, "rouge_l": 0.12398373983739835}
{"id": 4857, "code": "def percent ( data , part , total ) : try : return round ( 100 * float ( data [ part ] ) / float ( data [ total ] ) , 1 ) except Zero Division Error : return 0", "predictions": ["if the repository is repository , return it as an array of timedeltas ."], "references": ["calculate a percentage ."], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 4858, "code": "def get cache stats ( server name = None ) : server info = { } for svr in mc client . get stats ( ) : svr info = svr [ 0 ] . split ( ' ' ) svr name = svr info [ 0 ] svr stats = svr [ 1 ] svr stats [ 'bytes percent' ] = percent ( svr stats , 'bytes' , 'limit maxbytes' ) svr stats [ 'get hit rate' ] = percent ( svr stats , 'get hits' , 'cmd get' ) svr stats [ 'get miss rate' ] = percent ( svr stats , 'get misses' , 'cmd get' ) if server name and server name == svr name : return svr stats server info [ svr name ] = svr stats return server info", "predictions": ["get ( path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path path . path path path"], "references": ["get stats info ."], "bleu": 0.026594139297659906, "rouge_l": 0.08751793400286945}
{"id": 4859, "code": "def get cache slabs ( server name = None ) : server info = { } for svr in mc client . get slabs ( ) : svr info = svr [ 0 ] . split ( ' ' ) svr name = svr info [ 0 ] if server name and server name == svr name : return svr [ 1 ] server info [ svr name ] = svr [ 1 ] return server info", "predictions": ["get the ( possibly allocated = value = ) ."], "references": ["get slabs info ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 4860, "code": "def context data ( data , request = None ) : try : return dict ( site . each context ( request ) . items ( ) + data . items ( ) ) except Attribute Error : return data", "predictions": ["check if the version of the : \" version \" attribute has already been set ."], "references": ["add admin global context for compatibility with django 1 . 7"], "bleu": 0.07692375026049747, "rouge_l": 0.07663316582914573}
{"id": 4861, "code": "def server status ( request ) : data = { 'cache stats' : get cache stats ( ) , 'can get slabs' : hasattr ( mc client , 'get slabs' ) , } return render to response ( 'memcache admin/server status.html' , data , Request Context ( request ) )", "predictions": ["get the ( or cache args args args args args args args args args args args args args args args args args args args args args args = 1 args = 1 args args = 1 args args args )"], "references": ["return the status of all servers ."], "bleu": 0.03011857955989304, "rouge_l": 0.048722044728434506}
{"id": 4862, "code": "def dashboard ( request ) : if not isinstance ( mc client , dict ) : cache stats = get cache stats ( ) else : cache stats = None if cache stats : data = context data ( { 'title' : ( 'Memcache Dashboard' ) , 'cache stats' : cache stats , 'can get slabs' : hasattr ( mc client , 'get slabs' ) , 'REFRESH RATE' : SETTINGS [ 'REFRESH RATE' ] , } , request ) template = 'memcache admin/dashboard.html' else : data = context data ( { 'title' : ( 'Memcache Dashboard - Error' ) , 'error message' : ( 'Unable to connect to a memcache server.' ) , } , request ) template = 'memcache admin/dashboard error.html' return render to response ( template , data , Request Context ( request ) )", "predictions": ["leak the print print statistics on the given print or just the print ."], "references": ["show the dashboard ."], "bleu": 0.09782375748961449, "rouge_l": 0.2469635627530364}
{"id": 4863, "code": "def stats ( request , server name ) : server name = server name . strip ( '/' ) data = context data ( { 'title' : ( 'Memcache Statistics for %s' ) % server name , 'cache stats' : get cache stats ( server name ) , } , request ) return render to response ( 'memcache admin/stats.html' , data , Request Context ( request ) )", "predictions": ["this will execute the set of gpus on the given self ."], "references": ["show server statistics ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 4864, "code": "def slabs ( request , server name ) : data = context data ( { 'title' : ( 'Memcache Slabs for %s' ) % server name , 'cache slabs' : get cache slabs ( server name ) , } , request ) return render to response ( 'memcache admin/slabs.html' , data , Request Context ( request ) )", "predictions": ["now we are here to call this method to get an ( ( ) method on the client ."], "references": ["show server slabs ."], "bleu": 0.06439931429457924, "rouge_l": 0.09854604200323101}
{"id": 4865, "code": "def human bytes ( value ) : value = float ( value ) if value >= 1073741824 : gigabytes = value / 1073741824 size = '%.2f GB' % gigabytes elif value >= 1048576 : megabytes = value / 1048576 size = '%.2f MB' % megabytes elif value >= 1024 : kilobytes = value / 1024 size = '%.2f KB' % kilobytes else : size = '%.2f B' % value return size", "predictions": ["converts a human readable representation of the human readable bytes into a human readable representation ."], "references": ["convert a byte value into a human - readable format ."], "bleu": 0.16467029855845897, "rouge_l": 0.45979899497487436}
{"id": 4866, "code": "def add ( self , * * kwargs ) : for key in kwargs : if type ( kwargs [ key ] ) == str : self . children [ key ] = Directory ( kwargs [ key ] ) else : self . children [ key ] = kwargs [ key ] self . children [ key ] . env = self self . children [ key ] . apply config ( Config Applicator ( self . config ) ) self . children [ key ] . prepare ( )", "predictions": ["add a request for the ini ."], "references": ["add objects to the environment ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 4867, "code": "def apply config ( self , applicator ) : if type ( self . fpath ) == str : self . fpath = applicator . apply ( self . fpath )", "predictions": ["this method is called to apply to the configuration ."], "references": ["replace any config tokens in the file s path with values from the config ."], "bleu": 0.08461586088475063, "rouge_l": 0.15443037974683543}
{"id": 4868, "code": "def path ( self ) : if self . parent : return os . path . join ( self . parent . path , self . fpath ) else : return self . fpath", "predictions": ["get the path for this object ."], "references": ["get the path to the file relative to its parent ."], "bleu": 0.20643565894052812, "rouge_l": 0.4273204903677758}
{"id": 4869, "code": "def read ( self ) : with open ( self . path ) as f : d = f . read ( ) return d", "predictions": ["read a unicode object from the reader ."], "references": ["read and return the contents of the file ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 4870, "code": "def configure ( self ) : handler = logging . File Handler ( self . path , delay = True ) if self . format : handler . set Formatter ( logging . Formatter ( self . format ) ) if type ( self . formatter ) == str : if self . env and self . env . config . logging . dict config . formatters [ self . formatter ] : d = self . env . config . logging . dict config . formatters [ self . formatter ] . to dict ( ) handler . set Formatter ( logging . Formatter ( * * d ) ) elif type ( self . formatter ) == dict : handler . set Formatter ( logging . Formatter ( * * self . formatter ) ) if len ( self . loggers ) : for name in self . loggers : logging . get Logger ( name ) . add Handler ( handler ) else : logging . get Logger ( ) . add Handler ( handler )", "predictions": ["configures logging from the log file ."], "references": ["configure the python logging module for this file ."], "bleu": 0.20873176328735715, "rouge_l": 0.3667334669338677}
{"id": 4871, "code": "def apply config ( self , applicator ) : if type ( self . path ) == str : self . path = applicator . apply ( self . path ) for key in self . children : self . children [ key ] . apply config ( applicator )", "predictions": ["apply the config file to this object ."], "references": ["replace any config tokens with values from the config ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 4872, "code": "def path ( self ) : p = '' if self . parent and self . parent . path : p = os . path . join ( p , self . parent . path ) if self . base : p = os . path . join ( p , self . base ) if self . path : p = os . path . join ( p , self . path ) return p", "predictions": ["writes the path for this object ."], "references": ["return the path to this directory ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 4873, "code": "def remove ( self , recursive = True , ignore error = True ) : try : if recursive or self . cleanup == 'recursive' : shutil . rmtree ( self . path ) else : os . rmdir ( self . path ) except Exception as e : if not ignore error : raise e", "predictions": ["recursively deletes an entire file ."], "references": ["remove the directory ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4874, "code": "def path to ( self , path ) : return os . path . join ( self . path , str ( path ) )", "predictions": ["convert a file path to an internal path ."], "references": ["find the path to something inside this directory ."], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 4875, "code": "def list ( self ) : return [ File ( f , parent = self ) for f in os . listdir ( self . path ) ]", "predictions": ["list all files under this directory ."], "references": ["list the contents of the directory ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 4876, "code": "def write ( self , filename , data , mode = 'w' ) : with open ( self . path to ( str ( filename ) ) , mode ) as f : f . write ( data )", "predictions": ["write the given string to the file ."], "references": ["write to a file in the directory ."], "bleu": 0.21105340631872638, "rouge_l": 0.5}
{"id": 4877, "code": "def read ( self , filename ) : with open ( self . path to ( str ( filename ) ) ) as f : d = f . read ( ) return d", "predictions": ["read a unicode from the file , returning its parent ."], "references": ["read a file from the directory ."], "bleu": 0.20504572236241866, "rouge_l": 0.5787476280834916}
{"id": 4878, "code": "def add ( self , * args , * * kwargs ) : for key in kwargs : if isinstance ( kwargs [ key ] , str ) : self . children [ key ] = File ( kwargs [ key ] ) else : self . children [ key ] = kwargs [ key ] self . children [ key ] . parent = self self . children [ key ] . env = self . env added = [ ] for arg in args : if isinstance ( arg , File ) : self . children [ arg . name ] = arg self . children [ arg . name ] . parent = self self . children [ arg . name ] . env = self . env elif isinstance ( arg , str ) : f = File ( arg ) added . append ( f ) self . children [ arg ] = f self . children [ arg ] . parent = self self . children [ arg ] . env = self . env else : raise Type Error ( type ( arg ) ) if len ( added ) == 1 : return added [ 0 ] if len ( args ) == 1 : return args [ 0 ]", "predictions": ["store this class . it is its parent method ."], "references": ["add objects to the directory ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 4879, "code": "def save ( self ) : with open ( self . path , 'w' ) as f : f . write ( yaml . dump ( dict ( self . d ) ) )", "predictions": ["writes a file that is saved to the content folder ."], "references": ["save the state to a file ."], "bleu": 0.17827531042796255, "rouge_l": 0.3472485768500949}
{"id": 4880, "code": "def load ( self ) : if os . path . exists ( self . path ) : with open ( self . path , 'r' ) as f : self . d = yaml . safe load ( f . read ( ) . replace ( '\\t' , ' ' * 4 ) )", "predictions": ["loads a configuration file from the given location ."], "references": ["load a saved state file ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 4881, "code": "def cleanup ( self ) : if os . path . exists ( self . path ) : os . remove ( self . path )", "predictions": ["removes the path for the test method ."], "references": ["clean up the saved state ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 4882, "code": "def get value ( self ) : if self . path : try : container , last = self . resolve path ( ) return container [ last ] except Key Error : return None except Index Error : return None else : return self . data", "predictions": ["get the value of this instance ."], "references": ["get the value represented by this node ."], "bleu": 0.33167003447658744, "rouge_l": 0.6587473002159828}
{"id": 4883, "code": "def load ( self , reload = False ) : if reload or not self . loaded : if self . defaults file and type ( self . defaults file ) == str : self . defaults file = File ( self . defaults file , parent = self . parent ) defaults = { } if self . defaults file : defaults = yaml . safe load ( self . defaults file . read ( ) . replace ( '\\t' , '    ' ) ) data = { } if self . exists : data = yaml . safe load ( self . read ( ) . replace ( '\\t' , '    ' ) ) self . defaults = defaults self . data = copy . deepcopy ( self . defaults ) self . update ( data = data ) if self . apply env : self . update ( Config Env ( self . env prefix ) ) self . loaded = True return self", "predictions": ["loads and builds a yaml configuration object from the given file ."], "references": ["load the config and defaults from files ."], "bleu": 0.13065113298388567, "rouge_l": 0.3112244897959184}
{"id": 4884, "code": "def apply to str ( self , obj ) : toks = re . split ( '({config:|})' , obj ) newtoks = [ ] try : while len ( toks ) : tok = toks . pop ( 0 ) if tok == '{config:' : var = toks . pop ( 0 ) val = self . config [ var ] if type ( val ) == Config Node and val == None : raise Key Error ( \"No such config variable '{}'\" . format ( var ) ) newtoks . append ( str ( val ) ) toks . pop ( 0 ) else : newtoks . append ( tok ) return '' . join ( newtoks ) except Index Error : pass return obj", "predictions": ["converts the value of each object in the equivalent manner into a string ."], "references": ["apply the config to a string ."], "bleu": 0.17395797375642236, "rouge_l": 0.4053156146179402}
{"id": 4885, "code": "def process input ( self ) : try : pyngus . read socket input ( self . connection , self . socket ) except Exception as e : LOG . error ( \"Exception on socket read: %s\" , str ( e ) ) self . connection . close input ( ) self . connection . close ( ) self . connection . process ( time . time ( ) )", "predictions": ["this is called by the server when it wants to send the connection ."], "references": ["called when socket is read - ready"], "bleu": 0.10511846841633776, "rouge_l": 0.2026578073089701}
{"id": 4886, "code": "def send output ( self ) : try : pyngus . write socket output ( self . connection , self . socket ) except Exception as e : LOG . error ( \"Exception on socket write: %s\" , str ( e ) ) self . connection . close output ( ) self . connection . close ( ) self . connection . process ( time . time ( ) )", "predictions": ["send the push message to the server ."], "references": ["called when socket is write - ready"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4887, "code": "def send request ( self ) : msg = Message ( ) msg . subject = \"An RPC call!\" msg . address = self . to msg . reply to = self . reply to msg . body = self . method msg . correlation id = 5 print ( \"sending RPC call request: %s\" % str ( self . method ) ) self . sender . send ( msg , self )", "predictions": ["send an email via the laplace method ."], "references": ["send a message containing the rpc method call"], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 4888, "code": "def configure ( self , target address , source address , handler , properties ) : self . handler = handler self . properties = properties dynamic props = None if properties : dynamic props = properties . get ( \"dynamic-node-properties\" ) mode = dist modes . get ( properties . get ( \"distribution-mode\" ) ) if mode is not None : self . pn link . source . distribution mode = mode mode = snd settle modes . get ( properties . get ( \"snd-settle-mode\" ) ) if mode is not None : self . pn link . snd settle mode = mode mode = rcv settle modes . get ( properties . get ( \"rcv-settle-mode\" ) ) if mode is not None : self . pn link . rcv settle mode = mode if target address is None : if not self . pn link . is sender : raise Exception ( \"Dynamic target not allowed\" ) self . pn link . target . dynamic = True if dynamic props : self . pn link . target . properties . clear ( ) self . pn link . target . properties . put dict ( dynamic props ) elif target address : self . pn link . target . address = target address if source address is None : if not self . pn link . is receiver : raise Exception ( \"Dynamic source not allowed\" ) self . pn link . source . dynamic = True if dynamic props : self . pn link . source . properties . clear ( ) self . pn link . source . properties . put dict ( dynamic props ) elif source address : self . pn link . source . address = source address", "predictions": ["sets up the link object for the target properties ."], "references": ["assign addresses properties etc ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 4889, "code": "def source address ( self ) : if self . pn link . is sender : return self . pn link . source . address else : return self . pn link . remote source . address", "predictions": ["sets the source tag to be the source ."], "references": ["return the authorative source of the link ."], "bleu": 0.17747405280050263, "rouge_l": 0.4756335282651072}
{"id": 4890, "code": "def target address ( self ) : if self . pn link . is receiver : return self . pn link . target . address else : return self . pn link . remote target . address", "predictions": ["sets the target entity to the target ."], "references": ["return the authorative target of the link ."], "bleu": 0.20164945583740668, "rouge_l": 0.5}
{"id": 4891, "code": "def session closed ( self ) : if self . endpoint state & proton . Endpoint . REMOTE ACTIVE : self . process remote state ( ) elif self . endpoint state & proton . Endpoint . REMOTE UNINIT : self . failed = True self . link failed ( \"Parent session closed.\" )", "predictions": ["creates a new ( ."], "references": ["remote has closed the session used by this link ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4892, "code": "def reject ( self , pn condition = None ) : self . pn link . source . type = proton . Terminus . UNSPECIFIED super ( Sender Link , self ) . reject ( pn condition )", "predictions": ["creates a new ( ."], "references": ["see link reject amqp1 . 0 spec ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4893, "code": "def process delivery ( self , pn delivery ) : if pn delivery . tag in self . send requests : if pn delivery . settled or pn delivery . remote state : outcome = pn delivery . remote state state = Sender Link . DISPOSITION STATE MAP . get ( outcome , self . UNKNOWN ) pn disposition = pn delivery . remote info = { } if state == Sender Link . REJECTED : if pn disposition . condition : info [ \"condition\" ] = pn disposition . condition elif state == Sender Link . MODIFIED : info [ \"delivery-failed\" ] = pn disposition . failed info [ \"undeliverable-here\" ] = pn disposition . undeliverable annotations = pn disposition . annotations if annotations : info [ \"message-annotations\" ] = annotations send req = self . send requests . pop ( pn delivery . tag ) send req . destroy ( state , info ) pn delivery . settle ( ) elif pn delivery . writable : if self . pending sends : tag = self . pending sends . popleft ( ) send req = self . send requests [ tag ] self . write msg ( pn delivery , send req ) else : LOG . debug ( \"Delivery ignored, tag=%s\" , str ( pn delivery . tag ) ) pn delivery . settle ( )", "predictions": ["for setting user settings ."], "references": ["check if the delivery can be processed ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4894, "code": "def reject ( self , pn condition = None ) : self . pn link . target . type = proton . Terminus . UNSPECIFIED super ( Receiver Link , self ) . reject ( pn condition )", "predictions": ["create a new ( ."], "references": ["see link reject amqp1 . 0 spec ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4895, "code": "def process delivery ( self , pn delivery ) : if pn delivery . readable and not pn delivery . partial : data = self . pn link . recv ( pn delivery . pending ) msg = proton . Message ( ) msg . decode ( data ) self . pn link . advance ( ) if self . handler : handle = \"rmsg-%s:%x\" % ( self . name , self . next handle ) self . next handle += 1 self . unsettled deliveries [ handle ] = pn delivery with self . callback lock : self . handler . message received ( self , msg , handle ) else : pn delivery . settle ( )", "predictions": ["this method is called on each link ."], "references": ["check if the delivery can be processed ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4896, "code": "def new sender ( self , name ) : pn link = self . pn session . sender ( name ) return self . request sender ( pn link )", "predictions": ["a protected request method to add this new one to the git cache ."], "references": ["create a new sender link ."], "bleu": 0.10511846841633776, "rouge_l": 0.323321554770318}
{"id": 4897, "code": "def request sender ( self , pn link ) : sl = Sender Link ( self . connection , pn link ) self . links . add ( sl ) return sl", "predictions": ["public view to send the = = 0 , 1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 2 / 3 / 1 / 9 / >= / >="], "references": ["create link from request for a sender ."], "bleu": 0.0266785298043081, "rouge_l": 0.0}
{"id": 4898, "code": "def new receiver ( self , name ) : pn link = self . pn session . receiver ( name ) return self . request receiver ( pn link )", "predictions": ["returns a add ( possibly empty ) link to the text output ."], "references": ["create a new receiver link ."], "bleu": 0.1135935489027116, "rouge_l": 0.3382624768946396}
{"id": 4899, "code": "def request receiver ( self , pn link ) : rl = Receiver Link ( self . connection , pn link ) self . links . add ( rl ) return rl", "predictions": ["apply this config to the specified destination ."], "references": ["create link from request for a receiver ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4900, "code": "def link destroyed ( self , link ) : self . links . discard ( link ) if not self . links : LOG . debug ( \"destroying unneeded session\" ) self . pn session . close ( ) self . pn session . free ( ) self . pn session = None self . connection = None", "predictions": ["create a full path to the specified path ."], "references": ["link has been destroyed ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4901, "code": "def ep need close ( self ) : LOG . debug ( \"Session %s close requested - closing...\" , self . name ) links = self . links . copy ( ) for link in links : link . session closed ( )", "predictions": ["closes this . and its links ."], "references": ["peer has closed its end of the session ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4902, "code": "def extend Markdown ( self , md , md globals ) : mark tag = Simple Tag Pattern ( MARK RE , 'mark' ) md . inline Patterns . add ( 'mark' , mark tag , ' begin' )", "predictions": ["configure a new new situation with this new content as mark ."], "references": ["modifies inline patterns ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 4903, "code": "def receiver remote closed ( self , receiver link , pn condition ) : LOG . debug ( \"receiver remote closed condition=%s\" , pn condition ) receiver link . close ( ) self . done = True", "predictions": ["create a apply ) and its apply apply to the apply apply apply to the ) ."], "references": ["peer has closed its end of the link ."], "bleu": 0.0859076483566362, "rouge_l": 0.2443257676902537}
{"id": 4904, "code": "def receiver failed ( self , receiver link , error ) : LOG . warn ( \"receiver failed error=%s\" , error ) receiver link . close ( ) self . done = True", "predictions": ["a path to the path of this p ."], "references": ["protocol error occurred ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4905, "code": "def get host port ( server address ) : regex = re . compile ( r\"^amqp://([a-z A-Z0-9.]+)(:([\\d]+))?$\" ) x = regex . match ( server address ) if not x : raise Exception ( \"Bad address syntax: %s\" % server address ) matches = x . groups ( ) host = matches [ 0 ] port = int ( matches [ 2 ] ) if matches [ 2 ] else None return host , port", "predictions": ["remove the recursive self - based self - based self - supplied = [ filename , ] , where 1 , 2 ] ."], "references": ["parse the hostname and port out of the server_address ."], "bleu": 0.05606668411195419, "rouge_l": 0.12708333333333333}
{"id": 4906, "code": "def connect socket ( host , port , blocking = True ) : addr = socket . getaddrinfo ( host , port , socket . AF INET , socket . SOCK STREAM ) if not addr : raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) my socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) if not blocking : my socket . setblocking ( 0 ) try : my socket . connect ( addr [ 0 ] [ 4 ] ) except socket . error as e : if e . errno != errno . EINPROGRESS : raise return my socket", "predictions": ["connects to the specified to the specified address and block until the connection wants ."], "references": ["create a tcp connection to the server ."], "bleu": 0.12300686288463772, "rouge_l": 0.2760180995475113}
{"id": 4907, "code": "def server socket ( host , port , backlog = 10 ) : addr = socket . getaddrinfo ( host , port , socket . AF INET , socket . SOCK STREAM ) if not addr : raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) my socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) my socket . setblocking ( 0 ) try : my socket . bind ( addr [ 0 ] [ 4 ] ) my socket . listen ( backlog ) except socket . error as e : if e . errno != errno . EINPROGRESS : raise return my socket", "predictions": ["returns the list of clauses that corresponds to the given ) ."], "references": ["create a tcp listening socket for a server ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 4908, "code": "def process ( self , now ) : if self . pn connection is None : LOG . error ( \"Connection.process() called on destroyed connection!\" ) return 0 if self . pn connection . state & proton . Endpoint . LOCAL UNINIT : return 0 if self . pn sasl and not self . sasl done : if ( PROTON VERSION < ( 0 , 10 ) ) : if self . pn sasl . state not in ( proton . SASL . STATE PASS , proton . SASL . STATE FAIL ) : LOG . debug ( \"SASL in progress. State=%s\" , str ( self . pn sasl . state ) ) if self . handler : with self . callback lock : self . handler . sasl step ( self , self . pn sasl ) return self . next deadline self . sasl done = True if self . handler : with self . callback lock : self . handler . sasl done ( self , self . pn sasl , self . pn sasl . outcome ) else : if self . pn sasl . outcome is not None : self . sasl done = True if self . handler : with self . callback lock : self . handler . sasl done ( self , self . pn sasl , self . pn sasl . outcome ) timer deadline = self . expire timers ( now ) transport deadline = self . pn transport . tick ( now ) if timer deadline and transport deadline : self . next deadline = min ( timer deadline , transport deadline ) else : self . next deadline = timer deadline or transport deadline pn event = self . pn collector . peek ( ) while pn event : if Link . handle proton event ( pn event , self ) : pass elif self . handle proton event ( pn event ) : pass elif Session Proxy . handle proton event ( pn event , self ) : pass self . pn collector . pop ( ) pn event = self . pn collector . peek ( ) if self . error : if self . handler : self . next deadline = now with self . callback lock : self . handler . connection failed ( self , self . error ) elif ( self . endpoint state == self . CLOSED and self . read done and self . write done ) : if self . handler : with self . callback lock : self . handler . connection closed ( self ) return self . next deadline", "predictions": ["write all stats for a : 1 . 2 . 1 . 0 . 1 . 0 . 1 . 1 . 1 . 1 ."], "references": ["perform connection state processing ."], "bleu": 0.04668049023095243, "rouge_l": 0.07349397590361446}
{"id": 4909, "code": "def output data ( self ) : c = self . has output if c <= 0 : return None try : buf = self . pn transport . peek ( c ) except Exception as e : self . connection failed ( str ( e ) ) return None return buf", "predictions": ["read and receive messages ."], "references": ["get a buffer of data that needs to be written to the network ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 4910, "code": "def create sender ( self , source address , target address = None , event handler = None , name = None , properties = None ) : ident = name or str ( source address ) if ident in self . sender links : raise Key Error ( \"Sender %s already exists!\" % ident ) session = Session Proxy ( \"session-%s\" % ident , self ) session . open ( ) sl = session . new sender ( ident ) sl . configure ( target address , source address , event handler , properties ) self . sender links [ ident ] = sl return sl", "predictions": ["create a new ( ( or ) ( a connection to a specific ) ."], "references": ["factory method for sender links ."], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 4911, "code": "def reject sender ( self , link handle , pn condition = None ) : link = self . sender links . get ( link handle ) if not link : raise Exception ( \"Invalid link handle: %s\" % link handle ) link . reject ( pn condition ) link . destroy ( )", "predictions": ["extend the with the given open with the given open token ."], "references": ["rejects the senderlink and destroys the handle ."], "bleu": 0.1235622127262679, "rouge_l": 0.3112244897959184}
{"id": 4912, "code": "def create receiver ( self , target address , source address = None , event handler = None , name = None , properties = None ) : ident = name or str ( target address ) if ident in self . receiver links : raise Key Error ( \"Receiver %s already exists!\" % ident ) session = Session Proxy ( \"session-%s\" % ident , self ) session . open ( ) rl = session . new receiver ( ident ) rl . configure ( target address , source address , event handler , properties ) self . receiver links [ ident ] = rl return rl", "predictions": ["creates a new ( if available as a ( . as a jsp as a . as a ( as a ( as a connection ."], "references": ["factory method for creating receive links ."], "bleu": 0.04668049023095243, "rouge_l": 0.06762749445676275}
{"id": 4913, "code": "def connection failed ( self , error = \"Error not specified!\" ) : if not self . error : LOG . error ( \"Connection failed: %s\" , str ( error ) ) self . error = error", "predictions": ["creates a response: - based cleanup initialized with the application ."], "references": ["clean up after connection failure detected ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 4914, "code": "def ep active ( self ) : LOG . debug ( \"Connection is up\" ) if self . handler : with self . callback lock : self . handler . connection active ( self )", "predictions": ["method to value of this event ."], "references": ["both ends of the endpoint have become active ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4915, "code": "def ep need close ( self ) : LOG . debug ( \"Connection remotely closed\" ) if self . handler : cond = self . pn connection . remote condition with self . callback lock : self . handler . connection remote closed ( self , cond )", "predictions": ["closes the == and the == deferred == a == connection ."], "references": ["the remote has closed its end of the endpoint ."], "bleu": 0.1235622127262679, "rouge_l": 0.2772727272727273}
{"id": 4916, "code": "def ep error ( self , error ) : super ( Connection , self ) . ep error ( error ) self . connection failed ( \"Protocol error occurred.\" )", "predictions": ["creates a new instance of the ( ."], "references": ["the endpoint state machine failed due to protocol error ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 4917, "code": "def get color string ( self ) : s = '' if self . color type == 'd' : if self . name is \"black\" : s = '%.3f G' % 0 else : s = '%.3f %.3f %.3f RG' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) elif self . color type == 'f' or self . color type == 't' : if self . name is \"black\" : s = '%.3f g' % 0 else : s = '%.3f %.3f %.3f rg' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) return s", "predictions": ["this function is called by the ( when it is not a string ."], "references": ["adobe output string for defining colors"], "bleu": 0.08839374326825923, "rouge_l": 0.10777385159010601}
{"id": 4918, "code": "def set font ( self , family = None , style = None , size = None ) : if style is not None : if 'B' in style : family += ' bold' if 'I' in style : family += ' italic' self . set family ( family ) self . get diffs ( ) self . set style ( style ) self . set metrics ( ) self . set size ( size ) self . set font key ( )", "predictions": ["sets the output for the output ."], "references": ["select a font ; size given in points"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 4919, "code": "def string width ( self , s ) : s = str ( s ) w = 0 for char in s : char = ord ( char ) w += self . character widths [ char ] return w * self . font size / 1000.0", "predictions": ["this method is called to display the send request request . this will return a send for the string ."], "references": ["get width of a string in the current font"], "bleu": 0.07264339766175722, "rouge_l": 0.14805825242718446}
{"id": 4920, "code": "def get ttf ( self ) : font dict = { } families = [ ] rootdirlist = string . split ( self . search path , os . pathsep ) #for rootdir in rootdirlist: for dir Name , subdir List , filelist in itertools . chain . from iterable ( os . walk ( path ) for path in rootdirlist ) : for item in filelist : root , ext = os . path . splitext ( item ) if ext == '.ttf' : if root [ 0 ] . lower ( ) in english : source = os . path . join ( dir Name , item ) name = root . lower ( ) . replace ( ' ' , ' ' ) if ' bold' in name : name = name . replace ( ' bold' , ' bold' ) if ' italic' in name : name = name . replace ( ' italic' , ' italic' ) elif 'bold' in name : name = name . replace ( 'bold' , ' bold' ) if 'italic' in name : name = name . replace ( 'italic' , ' italic' ) elif ' italic' in name : name = name . replace ( ' italic' , ' italic' ) elif 'italic' in name : name = name . replace ( 'italic' , ' italic' ) elif 'oblique' in name : name = name . replace ( 'oblique' , ' italic' ) else : families . append ( name ) font dict [ name ] = source else : source = os . path . join ( dir Name , item ) name = root . lower ( ) . replace ( ' ' , ' ' ) font dict [ name ] = source families . append ( name ) self . font dict = font dict self . families = families", "predictions": ["extracts and returns a list of ( that can be read from the given source files ."], "references": ["given a search path find file with requested extension"], "bleu": 0.07994607499472013, "rouge_l": 0.0814419225634179}
{"id": 4921, "code": "def put stream ( self , stream ) : self . out ( 'stream' ) self . out ( stream ) self . out ( 'endstream' )", "predictions": ["add the specified address to this object ."], "references": ["creates a pdf text stream sandwich ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4922, "code": "def set font size ( self , size ) : if self . font . font size == size : pass else : self . font . set size ( size )", "predictions": ["sets this address and its output address to be recalculated for future use ."], "references": ["convenience method for just changing font size ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 4923, "code": "def add pie chart ( self , data , cursor , width , height , title = None , data type = \"raw\" , fill colors = None , labels = False , background = None , legend = None ) : save draw color = self . draw color save fill color = self . fill color chart = PDF Pie Chart ( self . session , self . page , data , cursor , width , height , title , data type , fill colors , labels , background , legend ) self . set draw color ( save draw color ) self . set fill color ( save fill color )", "predictions": ["session is a simple vertical failed call ."], "references": ["data type may be raw or percent"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4924, "code": "def output ( self ) : self . session . out ( '<</Type /X Object' ) self . session . out ( '/Subtype /Image' ) self . session . out ( '/Width %s' % self . width ) self . session . out ( '/Height %s' % self . height ) if self . colorspace is 'Indexed' : self . session . out ( '/Color Space [/Indexed /Device RGB %s %s 0 R' % ( self . pal , self . number + 1 ) ) else : self . session . out ( '/Color Space /%s' % self . colorspace ) if self . colorspace is 'Device CMYK' : self . session . out ( '/Decode [1 0 1 0 1 0 1 0]' ) self . session . out ( '/Bits Per Component %s' % self . bits per component ) if self . filter : self . session . out ( '/Filter /%s' % self . filter ) if self . decode : self . session . out ( '/Decode Parms << %s >>' % self . decode ) if self . transparent : self . session . out ( '/Mask [%s]' % self . transparent string ) if self . soft mask : self . session . out ( '/S Mask %s 0 R' % ( self . number + 1 ) ) self . session . out ( '/Length %s >>' % self . size ) self . session . put stream ( self . image data ) self . session . out ( 'endobj' ) if self . colorspace is 'Indexed' : self . session . out ( '<<%s /Length %s >>' % ( self . palette filter , self . palette length ) ) self . session . put stream ( self . palette ) self . session . out ( 'endobj' ) if isinstance ( self . soft mask , PDF Image ) : obj = self . session . add object ( ) self . soft mask . set number ( obj . id ) self . soft mask . output ( )", "predictions": ["creates a reject image ."], "references": ["prompts the creating of image objects ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4925, "code": "def absolute position ( self , x , y ) : ( a , b , c , d , e , f ) = self . current Matrix xp = a * x + c * y + e yp = b * x + d * y + f return xp , yp", "predictions": ["calculates the process delivery of this object ."], "references": ["return the absolute position of x y in user space w . r . t . default user space"], "bleu": 0.048218604638712956, "rouge_l": 0.20701357466063344}
{"id": 4926, "code": "def set font ( self , family = None , style = None , size = None ) : self . set family ( family ) self . set style ( style ) self . set size ( size ) self . set font key ( ) self . set name ( ) self . set character widths ( )", "predictions": ["sets the ( ) for the specified file ."], "references": ["select a font ; size given in points"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4927, "code": "def string width ( self , s ) : s = str ( s ) w = 0 for i in s : w += self . character widths [ i ] return w * self . font size / 1000.0", "predictions": ["convenience method that returns a process delivery delivery delivery of the string ."], "references": ["get width of a string in the current font"], "bleu": 0.12011055432195765, "rouge_l": 0.18798151001540828}
{"id": 4928, "code": "def set display mode ( self , zoom = 'fullpage' , layout = 'continuous' ) : self . zoom options = [ \"fullpage\" , \"fullwidth\" , \"real\" , \"default\" ] self . layout options = [ \"single\" , \"continuous\" , \"two\" , \"default\" ] if zoom in self . zoom options or ( isinstance ( zoom , int ) and 0 < zoom <= 100 ) : self . zoom mode = zoom else : raise Exception ( 'Incorrect zoom display mode: ' + zoom ) if layout in self . layout options : self . layout mode = layout else : raise Exception ( 'Incorrect layout display mode: ' + layout )", "predictions": ["set a display of this loop to the provided zoom ."], "references": ["set the default viewing options ."], "bleu": 0.1354599427337814, "rouge_l": 0.3727087576374745}
{"id": 4929, "code": "def close ( self ) : self . document . set page numbers ( ) self . put header ( ) self . put pages ( ) self . put resources ( ) self . put information ( ) self . put catalog ( ) self . put trailer ( ) if hasattr ( self . destination , \"write\" ) : output = self . output to io ( ) elif self . destination == 'string' : output = self . output to string ( ) else : self . output to file ( ) output = None return output", "predictions": ["closes this object ' s output ."], "references": ["prompt the objects to output pdf code and save to file ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 4930, "code": "def put header ( self ) : self . session . out ( '%%PDF-%s' % self . pdf version ) if self . session . compression : self . session . buffer += '%' + chr ( 235 ) + chr ( 236 ) + chr ( 237 ) + chr ( 238 ) + \"\\n\"", "predictions": ["store this object . the policy is incremented by the policy ."], "references": ["standard first line in a pdf ."], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 4931, "code": "def put resource dict ( self ) : self . session . add object ( 2 ) self . session . out ( '<<' ) self . session . out ( '/Proc Set [/PDF /Text /Image B /Image C /Image I]' ) self . session . out ( '/Font <<' ) for font in self . document . fonts : self . session . out ( '/F%s %s 0 R' % ( font . index , font . number ) ) self . session . out ( '>>' ) if self . document . images : self . session . out ( '/X Object <<' ) for image in self . document . images : self . session . out ( '/I%s %s 0 R' % ( image . index , image . number ) ) self . session . out ( '>>' ) self . session . out ( '>>' ) self . session . out ( 'endobj' )", "predictions": ["put a resource and document into the document ."], "references": ["creates pdf reference to resource objects ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4932, "code": "def put information ( self ) : self . session . add object ( ) self . session . out ( '<<' ) self . session . out ( '/Producer ' + self . text to string ( ) ) if self . title : self . session . out ( '/Title ' + self . text to string ( self . title ) ) if self . subject : self . session . out ( '/Subject ' + self . text to string ( self . subject ) ) if self . author : self . session . out ( '/Author ' + self . text to string ( self . author ) ) if self . keywords : self . session . out ( '/Keywords ' + self . text to string ( self . keywords ) ) if self . creator : self . session . out ( '/Creator ' + self . text to string ( self . creator ) ) self . session . out ( '/Creation Date ' + self . text to string ( 'D:' + datetime . now ( ) . strftime ( '%Y%m%d%H%M%S' ) ) ) self . session . out ( '>>' ) self . session . out ( 'endobj' )", "predictions": ["writes to the ( ."], "references": ["pdf information object ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4933, "code": "def x fit ( self , test length ) : if ( self . x + test length ) >= self . xmax : return False else : return True", "predictions": ["override this to save the underlying data to the array . this is not synchronized because it is not synchronized as it is not possible ."], "references": ["test to see if the line can has enough space for the given length ."], "bleu": 0.058697608930387266, "rouge_l": 0.2050420168067227}
{"id": 4934, "code": "def y fit ( self , test length ) : if ( self . y + test length ) >= self . ymax : return False else : return True", "predictions": ["override this to fit the original test"], "references": ["test to see if the page has enough space for the given text height ."], "bleu": 0.0704451546128839, "rouge_l": 0.17062937062937064}
{"id": 4935, "code": "def x is greater than ( self , test ordinate ) : self . is coordinate ( test ordinate ) if self . x > test ordinate . x : return True else : return False", "predictions": ["tests if a operation is ( ."], "references": ["comparison for x coordinate"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4936, "code": "def y is greater than ( self , test ordinate ) : self . is coordinate ( test ordinate ) if self . y > test ordinate . y : return True else : return False", "predictions": ["tests if this request is but not in the notebook ."], "references": ["comparison for y coordinate"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4937, "code": "def copy ( self ) : new cursor = self . class ( self . x , self . y ) new cursor . set bounds ( self . xmin , self . ymin , self . xmax , self . ymax , self . ymaxmax ) new cursor . set deltas ( self . dx , self . dy ) return new cursor", "predictions": ["returns a copy of this object ."], "references": ["create a copy and return it ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 4938, "code": "def x plus ( self , dx = None ) : if dx is None : self . x += self . dx else : self . x = self . x + dx", "predictions": ["call this method for the first step ."], "references": ["mutable x addition . defaults to set delta value ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 4939, "code": "def y plus ( self , dy = None ) : if dy is None : self . y += self . dy else : self . y = self . y + dy", "predictions": ["alias for adjusting . do not use createtemporarydirectory instead ."], "references": ["mutable y addition . defaults to set delta value ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 4940, "code": "def draw ( self ) : self . compile ( ) self . rows [ 0 ] . advance first row ( ) self . set borders ( ) self . draw fill ( ) self . draw borders ( ) self . draw text ( ) self . set final cursor ( )", "predictions": ["draws the specified region of the current box ."], "references": ["don t use this use document . draw_table"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4941, "code": "def setup ( app ) : app . setup extension ( 'sphinx.ext.todo' ) app . setup extension ( 'sphinx.ext.mathjax' ) app . setup extension ( \"sphinx.ext.intersphinx\" ) app . config . intersphinx mapping . update ( { 'https://docs.python.org/' : None } ) app . config . intersphinx mapping . update ( { sage doc url + doc + \"/\" : None for doc in sage documents } ) app . config . intersphinx mapping . update ( { sage doc url + \"reference/\" + module : None for module in sage modules } ) app . setup extension ( \"sphinx.ext.extlinks\" ) app . config . extlinks . update ( { 'python' : ( 'https://docs.python.org/release/' + pythonversion + '/%s' , '' ) , 'trac' : ( 'https://trac.sagemath.org/%s' , 'trac ticket #' ) , 'wikipedia' : ( 'https://en.wikipedia.org/wiki/%s' , 'Wikipedia article ' ) , 'arxiv' : ( 'http://arxiv.org/abs/%s' , 'Arxiv ' ) , 'oeis' : ( 'https://oeis.org/%s' , 'OEIS sequence ' ) , 'doi' : ( 'https://dx.doi.org/%s' , 'doi:' ) , 'pari' : ( 'http://pari.math.u-bordeaux.fr/dochtml/help/%s' , 'pari:' ) , 'mathscinet' : ( 'http://www.ams.org/mathscinet-getitem?mr=%s' , 'Math Sci Net ' ) } ) app . config . html theme = 'sage'", "predictions": ["creates all sage objects"], "references": ["initialize this sphinx extension"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4942, "code": "def duration ( self ) : ecc = self . ecc if not np . isnan ( self . ecc ) else np . sqrt ( self . ecw ** 2 + self . esw ** 2 ) esw = self . esw if not np . isnan ( self . esw ) else ecc * np . sin ( self . w ) a Rs = ( ( G * self . rhos * ( 1. + self . Mp Ms ) * ( self . per * DAYSEC ) ** 2. ) / ( 3. * np . pi ) ) ** ( 1. / 3. ) inc = np . arccos ( self . bcirc / a Rs ) becc = self . bcirc * ( 1 - ecc ** 2 ) / ( 1 - esw ) tdur = self . per / 2. / np . pi * np . arcsin ( ( ( 1. + self . Rp Rs ) ** 2 - becc ** 2 ) ** 0.5 / ( np . sin ( inc ) * a Rs ) ) tdur *= np . sqrt ( 1. - ecc ** 2. ) / ( 1. - esw ) return tdur", "predictions": ["compute the duration of this ( ."], "references": ["the approximate transit duration for the general case of an eccentric orbit"], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 4943, "code": "def update ( self , * * kwargs ) : if kwargs . get ( 'verify kwargs' , True ) : valid = [ y [ 0 ] for x in [ TRANSIT , LIMBDARK , SETTINGS ] for y in x . fields ] valid += [ 'b' , 'times' ] for k in kwargs . keys ( ) : if k not in valid : raise Exception ( \"Invalid kwarg '%s'.\" % k ) if ( 'q1' in kwargs . keys ( ) ) and ( 'q2' in kwargs . keys ( ) ) : kwargs . update ( { 'ldmodel' : KIPPING } ) elif ( 'c1' in kwargs . keys ( ) ) and ( 'c2' in kwargs . keys ( ) ) and ( 'c3' in kwargs . keys ( ) ) and ( 'c4' in kwargs . keys ( ) ) : kwargs . update ( { 'ldmodel' : NONLINEAR } ) self . limbdark . update ( * * kwargs ) self . transit . update ( * * kwargs ) self . settings . update ( * * kwargs )", "predictions": ["update a list of valid fields with this instance ."], "references": ["update the transit keyword arguments"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 4944, "code": "def Compute ( self ) : err = Compute ( self . transit , self . limbdark , self . settings , self . arrays ) if err != ERR NONE : Raise Error ( err )", "predictions": ["creates a new instance of this class ."], "references": ["computes the light curve model"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4945, "code": "def Bin ( self ) : err = Bin ( self . transit , self . limbdark , self . settings , self . arrays ) if err != ERR NONE : Raise Error ( err )", "predictions": ["creates a new instance of this class ."], "references": ["bins the light curve model to the provided time array"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 4946, "code": "def Free ( self ) : if self . arrays . calloc : dbl free ( self . arrays . time ) dbl free ( self . arrays . flux ) dbl free ( self . arrays . bflx ) dbl free ( self . arrays . M ) dbl free ( self . arrays . E ) dbl free ( self . arrays . f ) dbl free ( self . arrays . r ) dbl free ( self . arrays . x ) dbl free ( self . arrays . y ) dbl free ( self . arrays . z ) self . arrays . calloc = 0 if self . arrays . balloc : dbl free ( self . arrays . b ) self . arrays . balloc = 0 if self . arrays . ialloc : dbl free ( self . arrays . iarr ) self . arrays . ialloc = 0", "predictions": ["create a new object ."], "references": ["frees the memory used by all of the dynamically allocated c arrays ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 4947, "code": "def list extensions gen ( self ) : code , message = self . command ( \"LIST EXTENSIONS\" ) if code != 202 : raise NNTP Reply Error ( code , message ) for line in self . info gen ( code , message ) : yield line . strip ( )", "predictions": ["generate a list of extensions from this class ."], "references": ["generator for the list extensions command ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 4948, "code": "def xpat gen ( self , header , msgid range , * pattern ) : args = \" \" . join ( [ header , utils . unparse msgid range ( msgid range ) ] + list ( pattern ) ) code , message = self . command ( \"XPAT\" , args ) if code != 221 : raise NNTP Reply Error ( code , message ) for line in self . info gen ( code , message ) : yield line . strip ( )", "predictions": ["generate a generator for each pattern in this artifactinfofactory ."], "references": ["generator for the xpat command ."], "bleu": 0.17827531042796255, "rouge_l": 0.3927038626609442}
{"id": 4949, "code": "def xfeature compress gzip ( self , terminator = False ) : args = \"TERMINATOR\" if terminator else None code , message = self . command ( \"XFEATURE COMPRESS GZIP\" , args ) if code != 290 : raise NNTP Reply Error ( code , message ) return True", "predictions": ["gzip this terminator at the given offset ."], "references": ["xfeature compress gzip command ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 4950, "code": "def api post ( self , url , * * kwargs ) : response = self . session . post ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response . json ( )", "predictions": ["executes an api post using the specified url ."], "references": ["convenience method for posting"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4951, "code": "def api delete ( self , url , * * kwargs ) : response = self . session . delete ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response", "predictions": ["do some actual delete of the object ."], "references": ["convenience method for deleting"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4952, "code": "def api get ( self , url , * * kwargs ) : response = self . session . get ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response . json ( )", "predictions": ["client side of an http put request ."], "references": ["convenience method for getting"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4953, "code": "def create scheduled query ( self , query , change , scope unit , scope count ) : query data = { 'scheduled query' : { 'name' : 'For Anomaly Report' , 'query' : query , 'threshold type' : '%' , 'threshold value' : change , 'time period' : scope unit . title ( ) , 'time value' : scope count , } } query url = 'https://logentries.com/rest/{account id}/api/scheduled queries' return self . api post ( url = query url . format ( account id = self . account id ) , data = json . dumps ( query data , sort keys = True ) )", "predictions": ["creates the query for the given query ."], "references": ["create the scheduled query"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 4954, "code": "def do POST ( self ) : self . send response ( urllib2 . httplib . OK ) self . end headers ( ) content length = int ( self . headers [ 'Content-Length' ] ) body = self . rfile . read ( content length ) print ( \"Client: {0}\" . format ( str ( self . client address ) ) ) print ( \"headers: {0}\" . format ( self . headers ) ) print ( \"path: {0}\" . format ( self . path ) ) print ( \"body: {0}\" . format ( body ) )", "predictions": ["instantiate and dump the processed message ."], "references": ["handles the post request sent by boundary url action"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4955, "code": "def defaults docstring ( defaults , header = None , indent = None , footer = None ) : if indent is None : indent = '' if header is None : header = '' if footer is None : footer = '' width = 60 hbar = '\\n' s = hbar + ( header ) + hbar for key , value , desc in defaults : if isinstance ( value , basestring ) : value = \"'\" + value + \"'\" if hasattr ( value , ' call ' ) : value = \"<\" + value . name + \">\" s += indent + '%-12s\\n' % ( \"%s :\" % key ) s += indent + indent + ( indent + 23 * ' ' ) . join ( desc . split ( '\\n' ) ) s += ' [%s]\\n\\n' % str ( value ) s += hbar s += footer return s", "predictions": ["the default implementation of the call call docstring ."], "references": ["return a docstring from a list of defaults ."], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 4956, "code": "def defaults decorator ( defaults ) : def decorator ( func ) : kwargs = dict ( header = 'Keyword arguments\\n-----------------\\n' , indent = '  ' , footer = '\\n' ) doc = defaults docstring ( defaults , * * kwargs ) if func . doc is None : func . doc = '' func . doc += doc return func return decorator", "predictions": ["adds a decorator to a decorator ."], "references": ["decorator to append default kwargs to a function ."], "bleu": 0.23099966849728554, "rouge_l": 0.48897795591182364}
{"id": 4957, "code": "def load ( self , * * kwargs ) : defaults = dict ( [ ( d [ 0 ] , d [ 1 ] ) for d in self . defaults ] ) for k in kwargs : if k not in defaults : msg = \"Unrecognized attribute of %s: %s\" % ( self . class . name , k ) raise Attribute Error ( msg ) defaults . update ( kwargs ) self . dict . update ( defaults ) self . check type ( self . dict [ 'default' ] ) self . set ( * * defaults )", "predictions": ["loads the default dataset from a class and returns the corresponding object ."], "references": ["load kwargs key value pairs into __dict__"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 4958, "code": "def defaults docstring ( cls , header = None , indent = None , footer = None ) : return defaults docstring ( cls . defaults , header = header , indent = indent , footer = footer )", "predictions": ["creates a new ( with the given values ."], "references": ["add the default values to the class docstring"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4959, "code": "def set errors ( self , errors ) : if errors is None : self . errors = None return self . errors = [ asscalar ( e ) for e in errors ]", "predictions": ["set the number of errors on the proxy object ."], "references": ["set parameter error estimate"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 4960, "code": "def load and parse ( self ) : f = open ( self . file path , \"r\" ) metrics json = f . read ( ) self . metrics = json . loads ( metrics json )", "predictions": ["loads the [ jackson ] from the given file ."], "references": ["load the metrics file from the given path"], "bleu": 0.25965358893403384, "rouge_l": 0.4535315985130111}
{"id": 4961, "code": "def extract dictionary ( self , metrics ) : new metrics = { } for m in metrics : metric = self . extract fields ( m ) new metrics [ m [ 'name' ] ] = metric return new metrics", "predictions": ["returns a ( possibly empty document document document . document . document . document knows knows how to operate this value has to be re - sensitive ."], "references": ["extract required fields from an array"], "bleu": 0.03639374222382004, "rouge_l": 0.0}
{"id": 4962, "code": "def filter ( self ) : if self . filter expression is not None : new metrics = [ ] metrics = self . metrics [ 'result' ] for m in metrics : if self . filter expression . search ( m [ 'name' ] ) : new metrics . append ( m ) else : new metrics = self . metrics [ 'result' ] self . metrics = self . extract dictionary ( new metrics )", "predictions": ["returns a copy of this list ."], "references": ["apply the criteria to filter out on the metrics required"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 4963, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Id is not None : self . host Group Id = self . args . host Group Id self . path = \"v1/hostgroup/{0}\" . format ( str ( self . host Group Id ) )", "predictions": ["get the full group node , or null if no 2 ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 4964, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . tenant id is not None : self . tenant id = self . args . tenant id if self . args . fingerprint fields is not None : self . fingerprint fields = self . args . fingerprint fields if self . args . title is not None : self . title = self . args . title if self . args . source is not None : self . source = self . args . source if self . args . severity is not None : self . severity = self . args . severity if self . args . message is not None : self . message = self . args . message event = { } if self . title is not None : event [ 'title' ] = self . title if self . severity is not None : event [ 'severity' ] = self . severity if self . message is not None : event [ 'message' ] = self . message if self . source is not None : if 'source' not in event : event [ 'source' ] = { } if len ( self . source ) >= 1 : event [ 'source' ] [ 'ref' ] = self . source [ 0 ] if len ( self . source ) >= 2 : event [ 'source' ] [ 'type' ] = self . source [ 1 ] self . process properties ( self . args . properties ) if self . properties is not None : event [ 'properties' ] = self . properties if self . fingerprint fields is not None : event [ 'fingerprint Fields' ] = self . fingerprint fields self . data = json . dumps ( event , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' }", "predictions": ["a simple vertical method to read the underlying information from the current thread ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 4965, "code": "def call api ( self ) : sockobj = socket ( AF INET , SOCK STREAM ) sockobj . connect ( ( self . rpc host , self . rpc port ) ) self . get json ( ) message = [ self . rpc message . encode ( 'utf-8' ) ] for line in message : sockobj . send ( line ) data = sockobj . recv ( self . MAX LINE ) print ( data ) self . rpc data . append ( data ) sockobj . close ( )", "predictions": ["create a ( from this process ."], "references": ["make a call to the meter via json rpc"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4966, "code": "def get arguments ( self ) : Hostgroup Modify . get arguments ( self ) if self . args . host group id is not None : self . host group id = self . args . host group id self . path = \"v1/hostgroup/\" + str ( self . host group id )", "predictions": ["get the full path to the length of this proxy : this method is called from the other class ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.08638804535733371, "rouge_l": 0.2433510638297872}
{"id": 4967, "code": "def identifier ( self , text ) : self . attempting ( text ) return concatenation ( [ alternation ( [ self . alpha character , \" \" ] ) , zero or more ( alternation ( [ self . alpha character , \" \" , self . digit ] ) ) ] , ignore whitespace = False ) ( text ) . compressed ( Token Type . identifier )", "predictions": ["alias for ( . this removes duplicates from being serialized ."], "references": ["identifier = alpha_character | _ . { alpha_character | _ | digit } ;"], "bleu": 0.08671803715615023, "rouge_l": 0.07830551989730423}
{"id": 4968, "code": "def operator ( self , text ) : self . attempting ( text ) return alternation ( [ \"|\" , \".\" , \",\" , \"-\" ] ) ( text ) . retyped ( Token Type . operator )", "predictions": ["returns a y - level list of y elements ."], "references": ["operator = | | . | | - ;"], "bleu": 0.13950796967929133, "rouge_l": 0.10627177700348434}
{"id": 4969, "code": "def op mult ( self , text ) : self . attempting ( text ) return terminal ( \"*\" ) ( text ) . retyped ( Token Type . op mult )", "predictions": ["include the , or just after the , if necessary ."], "references": ["op_mult = * ;"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4970, "code": "def op add ( self , text ) : self . attempting ( text ) return terminal ( \"+\" ) ( text ) . retyped ( Token Type . op add )", "predictions": ["note : this is not used for multi - forward x / y / z / ( / ( / ( / ( / ( ."], "references": ["op_add = + ;"], "bleu": 0.03925345689749394, "rouge_l": 0.0}
{"id": 4971, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin Name is not None : self . plugin Name = self . args . plugin Name self . path = \"v1/plugins/{0}/components\" . format ( self . plugin Name )", "predictions": ["get the currently selected is defined ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4972, "code": "def get environment ( self ) : if 'TSP EMAIL' in os . environ : self . email = os . environ [ 'TSP EMAIL' ] if 'TSP API TOKEN' in os . environ : self . api token = os . environ [ 'TSP API TOKEN' ] if 'TSP API HOST' in os . environ : self . api host = os . environ [ 'TSP API HOST' ] else : self . api host = 'api.truesight.bmc.com'", "predictions": ["this method will return the current ( : : : : : : : 1 . 2 . 3 . 4 . 3 . 4 . 3 . advance the cursor should be be overridden by the caller : draw the cursor to draw the cursor for this object ."], "references": ["gets the configuration stored in environment variables"], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 4973, "code": "def call api ( self ) : self . url = self . form url ( ) if self . headers is not None : logging . debug ( self . headers ) if self . data is not None : logging . debug ( self . data ) if len ( self . get url parameters ( ) ) > 0 : logging . debug ( self . get url parameters ( ) ) result = self . methods [ self . method ] ( ) if not self . good response ( result . status code ) : logging . error ( self . url ) logging . error ( self . method ) if self . data is not None : logging . error ( self . data ) logging . error ( result ) self . api result = result", "predictions": ["makes a method for handling an api call ."], "references": ["make an api call to get the metric definition"], "bleu": 0.2626909894424158, "rouge_l": 0.3333333333333333}
{"id": 4974, "code": "def get arguments ( self ) : if self . args . file name is not None : self . file name = self . args . file name", "predictions": ["this method is called via reflection from the database ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.13950796967929133, "rouge_l": 0.12151394422310759}
{"id": 4975, "code": "def execute ( self ) : self . add arguments ( ) self . parse args ( ) self . get arguments ( ) if self . validate arguments ( ) : self . plot data ( ) else : print ( self . message )", "predictions": ["from this method . this method is called once to provide the connection to the superclass ."], "references": ["run the steps to execute the cli"], "bleu": 0.0859076483566362, "rouge_l": 0.27031019202363366}
{"id": 4976, "code": "def get remote file size ( self , url ) : try : req = urllib . request . urlopen ( url ) return int ( req . getheader ( 'Content-Length' ) . strip ( ) ) except urllib . error . HTTP Error as error : logger . error ( 'Error retrieving size of the remote file %s' % error ) print ( 'Error retrieving size of the remote file %s' % error ) self . connect earthexplorer ( ) self . get remote file size ( url )", "predictions": ["retrieves and return the ( formatted transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit transit . ( transit transit transit transit transit transit transit . ( . ( : ."], "references": ["gets the filesize of a remote file"], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 4977, "code": "def download ( self , bands = None , download dir = None , metadata = False ) : if not download dir : download dir = DOWNLOAD DIR if bands is None : bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] else : self . validate bands ( bands ) pattern = re . compile ( '^[^\\s]+ (.+)\\.tiff?' , re . I ) band list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] image list = [ ] self . connect earthexplorer ( ) tgzname = self . scene Info . name + '.tgz' dest dir = check create folder ( join ( download dir , self . scene Info . name ) ) downloaded = self . download file ( self . url , dest dir , tgzname ) logger . debug ( 'Status downloaded %s' % downloaded ) print ( '\\n Status downloaded %s' % downloaded ) if downloaded [ 'sucess' ] : print ( '\\n Downloaded sucess' ) logger . debug ( 'Downloaded sucess of scene: %s' % self . scene Info . name ) try : tar = tarfile . open ( downloaded [ 'file path' ] , 'r' ) folder path = join ( download dir , self . scene Info . name ) tar . extractall ( folder path ) remove ( downloaded [ 'file path' ] ) images path = listdir ( folder path ) for image path in images path : matched = pattern . match ( image path ) file path = join ( folder path , image path ) if matched and matched . group ( 1 ) in band list : image list . append ( [ file path , getsize ( file path ) ] ) elif matched : remove ( file path ) except tarfile . Read Error as error : print ( '\\n Error when extracting files. %s' % error ) logger . error ( 'Error when extracting files. %s' % error ) return image list else : logger . debug ( 'Info downloaded: %s' % downloaded ) print ( '\\n Info downloaded: %s' % downloaded ) return downloaded", "predictions": ["download ( download , download ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ( ( ( ( . ( ( . )"], "references": ["download remote . tar . bz file ."], "bleu": 0.028577262451992175, "rouge_l": 0.11898569570871263}
{"id": 4978, "code": "def validate bands ( bands ) : if not isinstance ( bands , list ) : raise Type Error ( 'Parameter bands must be a \"list\"' ) valid bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid bands : raise Invalid Band Error ( '%s is not a valid band' % band )", "predictions": ["returns the agents for the given ( . use the ( for ( ."], "references": ["validate bands parameter ."], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 4979, "code": "def connect earthexplorer ( self ) : logger . info ( \"Establishing connection to Earthexplorer\" ) print ( \"\\n Establishing connection to Earthexplorer\" ) try : opener = urllib . request . build opener ( urllib . request . HTTP Cookie Processor ( ) ) urllib . request . install opener ( opener ) params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) params = params . encode ( 'utf-8' ) f = opener . open ( \"https://ers.cr.usgs.gov/login\" , params ) data = f . read ( ) . decode ( 'utf-8' ) f . close ( ) if data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) > 0 : print ( \"\\n Authentification failed\" ) logger . error ( \"Authentification failed\" ) raise Autentication USGS Failed ( 'Authentification USGS failed' ) print ( 'User %s connected with USGS' % self . user ) logger . debug ( 'User %s connected with USGS' % self . user ) return except Exception as e : print ( '\\n Error when trying to connect USGS: %s' % e ) raise logger . error ( 'Error when trying to connect USGS: %s' % e )", "predictions": ["presents the client with the latest ( ."], "references": ["connection to earth explorer without proxy"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4980, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric name is not None : self . metric name = self . args . metric name self . path = \"v1/metrics/{0}\" . format ( self . metric name )", "predictions": ["get a : whose args are defined as part of the currently selected : inline : . = ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.0712695567709093, "rouge_l": 0.08390646492434663}
{"id": 4981, "code": "def normalize ( self , dt , is dst = False ) : if dt . tzinfo is None : raise Value Error ( 'Naive time - no tzinfo set' ) return dt . replace ( tzinfo = self )", "predictions": ["normalizes the normalized capability passed directly ."], "references": ["correct the timezone information on the given datetime"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4982, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Id is not None : self . host Group Id = self . args . host Group Id if self . args . force is not None : self . force = self . args . force if self . force : self . url parameters = { \"force Remove\" : True } self . path = \"v1/hostgroup/{0}\" . format ( str ( self . host Group Id ) )", "predictions": ["get = 0 , 1 , 2 ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4983, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) self . actions = self . args . actions if self . args . actions is not None else None self . alarm name = self . args . alarm name if self . args . alarm name is not None else None self . metric = self . args . metric if self . args . metric is not None else None self . aggregate = self . args . aggregate if self . args . aggregate is not None else None self . operation = self . args . operation if self . args . operation is not None else None self . threshold = self . args . threshold if self . args . threshold is not None else None self . trigger interval = self . args . trigger interval if self . args . trigger interval is not None else None self . host group id = self . args . host group id if self . args . host group id is not None else None self . note = self . args . note if self . args . note is not None else None self . per host notify = self . args . per host notify if self . args . per host notify is not None else None self . is disabled = self . args . is disabled if self . args . is disabled is not None else None self . notify clear = self . args . notify clear if self . args . notify clear is not None else None self . notify set = self . args . notify set if self . args . notify set is not None else None self . timeout interval = self . args . timeout interval if self . args . timeout interval is not None else None", "predictions": ["returns a list containing the : doctest ) of the specified : call to all : this method ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.07658412276041004, "rouge_l": 0.2517193947730399}
{"id": 4984, "code": "def dump text ( self ) : results = self . relay output [ 'result' ] for l in results : dt = time . strftime ( \"%Y-%m-%d T%H:%M:%SZ\" , time . gmtime ( int ( l [ 1 ] [ 'ts' ] ) ) ) print ( \"{0} {1} {2} {3}\" . format ( l [ 0 ] , dt , l [ 1 ] [ 'type' ] , l [ 1 ] [ 'msg' ] ) )", "predictions": ["api api . get method just returns first formatted time ."], "references": ["send output in textual format"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4985, "code": "def handle results ( self ) : if self . api result . status code == requests . codes . ok : self . relay output = json . loads ( self . api result . text ) if self . raw : self . dump json ( ) else : self . dump text ( )", "predictions": ["create scheduled scheduled ) ."], "references": ["call back function to be implemented by the cli ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4986, "code": "def get arguments ( self ) : Plugin Base . get arguments ( self ) if self . args . organization Name is not None : self . organization Name = self . args . organization Name if self . args . repository Name is not None : self . repository Name = self . args . repository Name self . path = \"v1/plugins/private/{0}/{1}/{2}\" . format ( self . plugin Name , self . organization Name , self . repository Name )", "predictions": ["a int containing the underlying arguments class ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4987, "code": "def get arguments ( self ) : Alarm Modify . get arguments ( self ) self . alarm id = self . args . alarm id if self . args . alarm id is not None else None self . get api parameters ( )", "predictions": ["method to get in ) format for this object ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 4988, "code": "def filter ( self ) : if self . metrics or self . control or self . plugins : relays = self . relays [ 'result' ] [ 'relays' ] for relay in relays : if self . metrics : del relays [ relay ] [ 'metrics' ] if self . control : del relays [ relay ] [ 'control' ] if self . plugins : if 'plugins' in relays [ relay ] : del relays [ relay ] [ 'plugins' ]", "predictions": ["filters out not yet been requested ."], "references": ["apply the criteria to filter out on the output required"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4989, "code": "def handle results ( self ) : if self . api result . status code == requests . codes . ok : self . relays = json . loads ( self . api result . text ) self . filter ( ) self . dump json ( )", "predictions": ["load first self - do not support commit"], "references": ["call back function to be implemented by the cli ."], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 4990, "code": "def fromlist ( cls , files , equal = False , offensive = False , lang = None ) : self = cls . new ( cls ) self . files = fortunes = [ ] count = 0 for file in files : fortune = load fortune ( file , offensive = offensive , lang = lang ) if fortune is None : logger . warn ( \"Can't load: %s\" , file ) continue count += 1 if equal else fortune . size fortunes . append ( ( fortune , count ) ) if not fortunes : raise Value Error ( 'All fortune files specified are invalid' ) self . count = count self . keys = [ i [ 1 ] for i in self . files ] return self", "predictions": ["defaults to load this classes from all classes ."], "references": ["initialize based on a list of fortune files"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4991, "code": "def set chance ( cls , files , equal = False , offensive = False , lang = None ) : self = cls . new ( cls ) total = 0. file = [ ] leftover = [ ] for name , chance in files : if total >= 1 : break fortune = load fortune ( name , offensive = offensive , lang = lang ) if fortune is None or not fortune . size : continue if chance : file . append ( ( fortune , chance ) ) total += chance else : leftover . append ( fortune ) if leftover and total < 1 : left = 1 - total if equal : perfile = left / len ( leftover ) for fortune in leftover : file . append ( ( fortune , perfile ) ) else : entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) logger . debug ( '%d entries left' , entries ) for fortune in leftover : chance = left * fortune . size / entries file . append ( ( fortune , chance ) ) self . count = count = 65536 bound = 0 self . files = fortunes = [ ] for file , chance in file : bound += int ( chance * count ) fortunes . append ( ( file , bound ) ) self . keys = [ i [ 1 ] for i in self . files ] return self", "predictions": ["for each timestamp in the given user-supplied and returns a set of expected precision objects ."], "references": ["initialize based on a list of fortune files with set chances"], "bleu": 0.09147827112247602, "rouge_l": 0.15326633165829145}
{"id": 4992, "code": "def grammar ( self , text ) : self . attempting ( text ) return concatenation ( [ zero or more ( self . comment , ignore whitespace = True ) , self . rule , zero or more ( alternation ( [ self . comment , self . rule , ] ) , ignore whitespace = True ) , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . grammar )", "predictions": ["returns a grammar to be used for this multi - trees ."], "references": ["grammar = { comment } rule { comment | rule } ;"], "bleu": 0.10390302174233558, "rouge_l": 0.08333333333333333}
{"id": 4993, "code": "def rule ( self , text ) : self . attempting ( text ) return concatenation ( [ self . identifier , \"=\" , self . expression , \";\" , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . rule )", "predictions": ["include this type of this diagnostic ."], "references": ["rule = identifier = expression ; ;"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4994, "code": "def special handling ( self , text ) : self . attempting ( text ) return concatenation ( [ \"?\" , self . identifier , \"?\" , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . special handling )", "predictions": ["returns a string that is used to provide this multi - thing to provide the thing ."], "references": ["special_handling = ? identifier ? ;"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 4995, "code": "def number ( self , text ) : self . attempting ( text ) return concatenation ( [ exclusion ( self . digit , \"0\" ) , zero or more ( self . digit , ignore whitespace = False ) , ] , ignore whitespace = False ) ( text ) . compressed ( Token Type . number )", "predictions": ["get a ( such that it has been assigned to this method ."], "references": ["number = digit - 0 . { digit } ;"], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 4996, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric Name is not None : self . metric Name = self . args . metric Name if self . args . measurement is not None : self . measurement = self . args . measurement if self . args . source is not None : self . source = self . args . source else : self . source = socket . gethostname ( ) if self . args . timestamp is not None : self . timestamp = int ( self . args . timestamp ) m = { 'metric' : self . metric Name , 'measure' : self . measurement } if self . source is not None : m [ 'source' ] = self . source if self . timestamp is not None : m [ 'timestamp' ] = int ( self . timestamp ) self . process properties ( ) if self . properties is not None : m [ 'metadata' ] = self . properties self . data = json . dumps ( m , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , \"Accept\" : \"application/json\" }", "predictions": ["a method to get the proper arguments ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4997, "code": "def handle results ( self ) : if self . api result . status code == requests . codes . ok : payload = json . loads ( self . api result . text ) out = json . dumps ( payload , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )", "predictions": ["makes a json representation of this object ."], "references": ["call back function to be implemented by the cli ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 4998, "code": "def grammar ( self ) : if self . grammar is None : self . parser = Parser ( ) grammar = self . parser . parse ( self . input source ) self . grammar = grammar . trimmed ( ) . flattened ( ) . flattened ( self . flatten ) return self . grammar", "predictions": ["a grammar method for subclasses that want to be parameterized ."], "references": ["the parse tree generated by the source ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 4999, "code": "def rules ( self ) : if self . rules is None : self . rules = [ ] for child in self . grammar . children : if child . is type ( Token Type . rule ) : name , expression = child . children self . rules . append ( Rule ( name . value , self . expression to asn ( expression ) , name . position , child . consumed ) ) return self . rules", "predictions": ["method to attach full rules into this object ."], "references": ["the ast rules ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 5000, "code": "def comments ( self ) : if self . comments is None : self . comments = [ c for c in self . grammar . children if c . is type ( Token Type . comment ) ] return self . comments", "predictions": ["evaluates the comments of this message ."], "references": ["the ast comments ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 5001, "code": "def directives ( self ) : if self . directives is None : self . directives = [ ] for comment in self . comments : self . directives . extend ( self . directives from comment ( comment ) ) return self . directives", "predictions": ["affect affect message objects into a previously contained in the current clip ."], "references": ["the diretives parsed from the comments ."], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 5002, "code": "def output source ( self ) : if self . output source is None : self . output source = self . compile ( ) return self . output source", "predictions": ["output to output . this is only meant for debugging purposes ."], "references": ["the python source of the parser generated from the input source ."], "bleu": 0.10390302174233558, "rouge_l": 0.08333333333333333}
{"id": 5003, "code": "def compile ( self ) : fmt = fmt = self . clean fmt ( fmt ) return fmt . format ( date = datetime . utcnow ( ) . isoformat ( ) , imports = self . get imports ( ) , token type enum = self . get token type enum ( ) , class definition = self . get class definition ( ) )", "predictions": ["returns a full format ."], "references": ["returns the python source code for the generated parser ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5004, "code": "def get imports ( self ) : import directives = [ d for d in self . directives if d . name == \"import\" ] if import directives : return \"\\n\" + \"\\n\" . join ( d . args [ \"value\" ] for d in import directives ) else : return \"\"", "predictions": ["get a previously registered batches ."], "references": ["reads the directives and generates source code for custom imports ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 5005, "code": "def get token type enum ( self ) : fmt = \"class Token Type(Enum):\\n\" \"{indent}\\\"\\\"\\\"The token types for parse nodes generated by the Parser.\\\"\\\"\\\"\\n\" \"{indent}\" + \"\\n{indent}\" . join ( \"{1} = {0}\" . format ( num + 1 , r . name ) for num , r in enumerate ( self . rules ) ) return fmt . format ( indent = self . indent )", "predictions": ["return a list of nodes for the specified token ."], "references": ["builds the python source code for the parser tokentype enum ."], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 5006, "code": "def get class definition ( self ) : fmt = fmt = self . clean fmt ( fmt ) return fmt . format ( parser base = self . get parser base ( ) , indent = self . indent , entry point = self . get entry point ( ) , rule definitions = \"\\n\" . join ( self . get rule definitions ( ) ) )", "predictions": ["return the class for the class ."], "references": ["builds the class definition of the parser ."], "bleu": 0.240785655451027, "rouge_l": 0.5269978401727862}
{"id": 5007, "code": "def get entry point ( self ) : ep = self . find directive ( \"entry point\" ) if ep : return ep . args [ \"value\" ] else : return self . rules [ 0 ] . name", "predictions": ["returns this point to the end of this permission ."], "references": ["gets the entry_point value for the parser ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 5008, "code": "def get rule definition ( self , rule ) : fmt = fmt = self . clean fmt ( fmt ) source = self . indent ( self . ast to code ( rule . expression ) , skip first line = True ) if self . use terminal shorthand and len ( source ) == 1 and source [ 0 ] . startswith ( ( \"'\" , '\"' ) ) : source = [ \"terminal({})\" . format ( source [ 0 ] ) ] rule source = fmt . format ( rule fxn name = self . get rule fxn name ( rule . name ) , indent = self . indent , rule source = self . get rule source ( rule ) , rule definition = \"\\n\" . join ( source ) , transform = self . get rule transform ( rule ) ) return self . indent ( rule source , 1 )", "predictions": ["for the rule meta - recursive call ."], "references": ["generates the source code for a rule ."], "bleu": 0.20164945583740668, "rouge_l": 0.375}
{"id": 5009, "code": "def get rule source ( self , rule ) : p = len ( self . input source ) + rule . position source = self . input source [ p : p + rule . consumed ] . rstrip ( ) return self . indent ( source , depth = self . indent + \"   \" , skip first line = True )", "predictions": ["this method returns a rule for the source ."], "references": ["gets the variable part of the source code for a rule ."], "bleu": 0.18204651199034363, "rouge_l": 0.2785388127853881}
{"id": 5010, "code": "def expression to asn ( self , expression ) : new children = [ self . node to asn ( c ) for c in expression . children ] return self . remove grouping groups ( infix to optree ( new children ) )", "predictions": ["convert this list to a ( ."], "references": ["convert an expression to an abstract syntax tree node ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 5011, "code": "def node to asn ( self , node ) : if node . is type ( Token Type . identifier ) : return Identifier ( node . svalue ) elif node . is type ( Token Type . terminal ) : return Terminal ( node . svalue ) elif node . is type ( Token Type . option group ) : expr = node . children [ 0 ] return Option Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . repetition group ) : expr = node . children [ 0 ] return Repetition Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . grouping group ) : expr = node . children [ 0 ] return Grouping Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . special handling ) : ident = node . children [ 0 ] return Special Handling ( ident ) elif node . is type ( Token Type . number ) : return Number ( node . svalue ) elif node . is type ( ( Token Type . operator , Token Type . op mult , Token Type . op add ) ) : return Operator Node ( OPERATOR INDEX [ node . svalue ] , node . position ) else : raise Exception ( \"Unhandled parse tree node: {0}\" . format ( node ) )", "predictions": ["converts an option to a ( recursively ."], "references": ["convert a parse tree node into an absract syntax tree node ."], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 5012, "code": "def ast to code ( self , node , * * kwargs ) : if isinstance ( node , Optree Node ) : return self . ast optree node to code ( node , * * kwargs ) elif isinstance ( node , Identifier ) : return self . ast identifier to code ( node , * * kwargs ) elif isinstance ( node , Terminal ) : return self . ast terminal to code ( node , * * kwargs ) elif isinstance ( node , Option Group ) : return self . ast option group to code ( node , * * kwargs ) elif isinstance ( node , Repetition Group ) : return self . ast repetition group to code ( node , * * kwargs ) elif isinstance ( node , Special Handling ) : return self . ast special handling to code ( node , * * kwargs ) elif isinstance ( node , Number ) : return self . ast number to code ( node , * * kwargs ) else : raise Exception ( \"Unhandled ast node: {0}\" . format ( node ) )", "predictions": ["converts a ast to a ast ."], "references": ["convert an abstract syntax tree to python source code ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 5013, "code": "def ast optree node to code ( self , node , * * kwargs ) : opnode = node . opnode if opnode is None : return self . ast to code ( node . operands [ 0 ] ) else : operator = opnode . operator if operator is OP ALTERNATE : return self . ast op alternate to code ( node , * * kwargs ) elif operator is OP WS CONCAT : kwargs [ \"ignore whitespace\" ] = False return self . ast op concat to code ( node , * * kwargs ) elif operator is OP CONCAT : kwargs [ \"ignore whitespace\" ] = True return self . ast op concat to code ( node , * * kwargs ) elif operator is OP EXCLUDE : return self . ast op exclude to code ( node , * * kwargs ) elif operator is OP MULTIPLY : return self . ast op multiply to code ( node , * * kwargs ) elif operator is OP REPEAT : return self . ast op repeat to code ( node , * * kwargs ) else : raise Exception ( \"Unhandled optree node: {0}\" . format ( node ) )", "predictions": ["convert the optree class to a specific node ."], "references": ["convert an abstract syntax operator tree to python source code ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 5014, "code": "def ast terminal to code ( self , terminal , * * kwargs ) : value = replace ( terminal . value ) if self . use terminal shorthand : return [ value ] else : return [ \"terminal({})\" . format ( value ) ]", "predictions": ["convert a terminal . note : this is just for use in the ast ."], "references": ["convert an ast terminal to python source code ."], "bleu": 0.10343603005129705, "rouge_l": 0.26180257510729615}
{"id": 5015, "code": "def ast option group to code ( self , option group , * * kwargs ) : lines = [ \"option(\" ] lines . extend ( self . indent ( self . ast to code ( option group . expression ) ) ) lines . append ( \")\" ) return lines", "predictions": ["create a ast for an option ."], "references": ["convert an ast option group to python source code ."], "bleu": 0.15215596197411094, "rouge_l": 0.34205607476635513}
{"id": 5016, "code": "def ast repetition group to code ( self , repetition group , ignore whitespace = False , * * kwargs ) : lines = [ \"zero or more(\" ] lines . extend ( self . indent ( self . ast to code ( repetition group . expression ) ) ) lines [ - 1 ] += \",\" lines . append ( self . indent ( \"ignore whitespace={}\" . format ( bool ( ignore whitespace ) ) ) ) lines . append ( \")\" ) return lines", "predictions": ["adds a ast to the ast ."], "references": ["convert an ast repetition group to python source code ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 5017, "code": "def ast special handling to code ( self , special handling , * * kwargs ) : ident = special handling . value . svalue if ident in PB SPECIAL HANDLING : return [ \"PB.{0}\" . format ( ident ) ] else : return [ \"self.{0}\" . format ( ident ) ]", "predictions": ["generate a special ast ."], "references": ["convert an ast sepcial handling to python source code ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5018, "code": "def ast op alternate to code ( self , opr , * * kwargs ) : hoist target = OP ALTERNATE operands = self . hoist operands ( opr . operands , lambda t : isinstance ( t , Optree Node ) and t . opnode . operator is hoist target ) lines = [ \"alternation([\" ] for op in operands : lines . extend ( self . indent ( self . ast to code ( op ) ) ) lines [ - 1 ] += \",\" lines . append ( \"])\" ) return lines", "predictions": ["convert a ast node into a ast ."], "references": ["convert an ast alternate op to python source code ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 5019, "code": "def ast op concat to code ( self , opr , * , ignore whitespace , * * kwargs ) : hoist target = OP CONCAT if ignore whitespace else OP WS CONCAT operands = self . hoist operands ( opr . operands , lambda t : isinstance ( t , Optree Node ) and t . opnode . operator is hoist target ) lines = [ \"concatenation([\" ] for op in operands : lines . extend ( self . indent ( self . ast to code ( op , ignore whitespace = ignore whitespace ) ) ) lines [ - 1 ] += \",\" lines . append ( \"], ignore whitespace={})\" . format ( bool ( ignore whitespace ) ) ) return lines", "predictions": ["concatenate two canonical tep ."], "references": ["convert an ast concatenate op to python source code ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5020, "code": "def ast op exclude to code ( self , opr , * * kwargs ) : opl , opr = opr . operands lines = [ \"exclusion(\" ] lines . extend ( self . indent ( self . ast to code ( opl ) ) ) lines [ - 1 ] += \",\" lines . extend ( self . indent ( self . ast to code ( opr ) ) ) lines . append ( \")\" ) return lines", "predictions": ["creates a ast for testing ."], "references": ["convert an ast exclude op to python source code ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 5021, "code": "def ast op multiply to code ( self , opr , ignore whitespace = False , * * kwargs ) : opl , opr = opr . operands if isinstance ( opl , Number ) : times = opl . value subject = self . ast to code ( opr ) else : times = opr . value subject = self . ast to code ( opl ) lines = [ \"repeated(\" ] lines . extend ( self . indent ( subject ) ) lines [ - 1 ] += \",\" lines . append ( \"{0}times={1},\" . format ( self . indent , times ) ) lines . append ( \"{0}ignore whitespace={1}\" . format ( self . indent , bool ( ignore whitespace ) ) ) lines . append ( \")\" ) return lines", "predictions": ["call stack . adds ast to a ast ."], "references": ["convert an ast multiply op to python source code ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 5022, "code": "def ast op repeat to code ( self , opr , ignore whitespace = False , * * kwargs ) : lines = [ \"one or more(\" ] lines . extend ( self . indent ( self . ast to code ( opr . operands [ 0 ] ) ) ) lines [ - 1 ] += \",\" lines . append ( self . indent ( \"ignore whitespace={}\" . format ( bool ( ignore whitespace ) ) ) ) lines . append ( \")\" ) return lines", "predictions": ["call this method to return its ast ."], "references": ["convert an ast repeat op to python source code ."], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 5023, "code": "def find directives ( self , pred ) : if isinstance ( pred , str ) : return [ d for d in self . directives if d . name == pred ] else : return [ d for d in self . directives if pred ( d ) ]", "predictions": ["find the first step of the given list ."], "references": ["finds all directives with a certain name or that passes a predicate ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 5024, "code": "def flatten ( child , parent ) : return parent . is type ( Token Type . expression ) and child . node type == parent . node type", "predictions": ["appends the tree to the tree ."], "references": ["custom flattening method for the parse tree ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 5025, "code": "def directives from comment ( cls , comment ) : comment contents = comment . value [ 2 : - 2 ] . strip ( ) comment lines = ( l . strip ( ) for l in comment contents . split ( \"\\n\" ) ) directives = ( l [ 1 : ] . strip ( ) for l in comment lines if l . startswith ( \"!\" ) ) for directive def in directives : yield cls . parse directive def ( directive def )", "predictions": ["generate the internal self - processed edges from the given self - separated self - separated self ."], "references": ["a directive is a line in a comment that begins with ! ."], "bleu": 0.06809398432036522, "rouge_l": 0.06644880174291938}
{"id": 5026, "code": "def parse directive def ( cls , directive def ) : name , * kwargs = esc split ( directive def , ignore empty = True ) return Directive ( name , { key : value for key , value in ( esc split ( arg , \"=\" ) for arg in kwargs ) } )", "predictions": ["special special case of a class ."], "references": ["turns a directive definition string into a directive object ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 5027, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Name is not None : self . url parameters = { \"name\" : self . args . host Group Name }", "predictions": ["get the selected ( or [ jackson ] text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 5028, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin name is not None : self . plugin name = self . args . plugin name self . path = \"v1/plugins/{0}\" . format ( self . plugin name )", "predictions": ["get a reissues object ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 5029, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) self . alarm id = self . args . alarm id if self . args . alarm id is not None else None", "predictions": ["method to get a class object from the cache ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 5030, "code": "def handle results ( self ) : if self . api result . status code != requests . codes . ok : print ( self . colorize json ( self . api result . text ) )", "predictions": ["grammar this method for handling return response ."], "references": ["handle the results of the api call"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5031, "code": "def get id ( id ) : if id == None : id = wx . New Id ( ) logger . debug ( 'Generated new ID %s.' , id ) else : logger . debug ( 'Using provided id %s.' , id ) return id", "predictions": ["convenience method to rules for a = null , or 0 - 1 if not found ."], "references": ["get a new id if the provided one is none ."], "bleu": 0.0859076483566362, "rouge_l": 0.22289890377588306}
{"id": 5032, "code": "def add arguments ( self ) : self . add logging argument ( ) self . parser . add argument ( '-a' , '--api-host' , dest = 'api host' , action = 'store' , metavar = \"api host\" , help = '{0} API host endpoint' . format ( self . product name ) ) self . parser . add argument ( '-e' , '--email' , dest = 'email' , action = 'store' , metavar = \"e mail\" , help = 'e-mail that has access to the {0} account' . format ( self . product name ) ) self . parser . add argument ( '-t' , '--api-token' , dest = 'api token' , required = False , action = 'store' , metavar = \"api token\" , help = 'API token for given e-mail that has access to the {0} account' . format ( self . product name ) ) self . parser . add argument ( '-z' , '--curl' , dest = 'curl' , required = False , action = 'store true' , default = False , help = 'Output the corresponding curl command line and exit' )", "predictions": ["creates a class that can be used to comments about the user ."], "references": ["configure handling of command line arguments ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 5033, "code": "def configure logging ( self ) : if self . args . log Level is not None : logging . basic Config ( level = self . levels [ self . args . log Level ] ) logging . info ( \"Set logging level to {0}\" . format ( self . args . log Level ) )", "predictions": ["configures ( system ( default . : int number | . : int . * : int : int : int : = . is set : ` ` . ` . ` . ` . ` . ` . ` . ` . ` . ` . ` ("], "references": ["configure logging based on command line options"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 5034, "code": "def execute ( self ) : self . get environment ( ) self . add arguments ( ) self . parse args ( ) self . get arguments ( ) self . get api parameters ( ) if self . validate arguments ( ) : if self . curl : self . curl output ( ) else : self . call api ( ) self . handle results ( ) else : print ( self . message )", "predictions": ["from this method is called via the downsampled event loop ."], "references": ["run the steps to execute the cli"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 5035, "code": "def postfix to optree ( nodes ) : while len ( nodes ) > 1 : nodes = reduce ( nodes ) if len ( nodes ) == 0 : raise Operator Error ( \"Empty node list\" ) node = nodes [ 0 ] if isinstance ( node , Operator Node ) : raise Operator Error ( \"Operator without operands\" ) if isinstance ( node , Optree Node ) : return node return Optree Node ( None , ( node , ) )", "predictions": ["get the current node as compile or other nodes"], "references": ["convert a list of nodes in postfix order to an optree ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 5036, "code": "def pprint ( root , depth = 0 , space unit = \"    \" ) : spacing = space unit * depth if isinstance ( root , Optree Node ) : print ( \"{0}Operator ({1})\" . format ( spacing , root . opnode . operator . symbol if root . opnode else \"None -> IDENTITY\" ) ) for operand in root . operands : pprint ( operand , depth + 1 ) else : print ( \"{0}\u2022 {1}\".f o rmat(s p acing,  r ot))", "predictions": ["registers a get or a get of the tree ."], "references": ["pretty print an optree starting at root ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5037, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin Name is not None : self . plugin Name = self . args . plugin Name", "predictions": ["get the currently selected plugin plugin ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5038, "code": "def add arguments ( self ) : Metric Common . add arguments ( self ) self . parser . add argument ( '-n' , '--metric-name' , dest = 'metric Name' , action = 'store' , required = True , metavar = 'metric name' , help = 'Metric identifier' ) self . parser . add argument ( '-d' , '--display-name' , dest = 'display Name' , action = 'store' , required = True , metavar = 'display name' , help = 'Metric display name' ) self . parser . add argument ( '-s' , '--display-name-short' , dest = 'display Name Short' , action = 'store' , required = True , metavar = 'display short name' , help = 'Metric short display name' ) self . parser . add argument ( '-i' , '--description' , dest = 'description' , action = 'store' , required = not self . update , metavar = 'description' , help = 'Metric description' ) self . parser . add argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = True , choices = [ 'avg' , 'max' , 'min' , 'sum' ] , help = 'Metric default aggregate' ) self . parser . add argument ( '-u' , '--unit' , dest = 'unit' , action = 'store' , required = False , choices = [ 'percent' , 'number' , 'bytecount' , 'duration' ] , help = 'Metric unit' ) self . parser . add argument ( '-r' , '--resolution' , dest = 'resolution' , action = 'store' , metavar = 'resolution' , required = False , help = 'Metric default resolution' ) self . parser . add argument ( '-y' , '--type' , dest = 'type' , action = 'store' , default = None , required = False , metavar = 'type' , help = 'Sets the type metadata field' ) self . parser . add argument ( '-x' , '--is-disabled' , dest = 'is Disabled' , action = 'store' , default = None , required = False , choices = [ 'true' , 'false' ] , help = 'Enable or disable the metric definition' )", "predictions": ["get a user to contain two class variables ."], "references": ["add the specific arguments of this cli"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5039, "code": "def get arguments ( self ) : Metric Common . get arguments ( self ) if self . args . metric Name is not None : self . metric Name = self . args . metric Name if self . args . display Name is not None : self . display Name = self . args . display Name if self . args . display Name Short is not None : self . display Name Short = self . args . display Name Short if self . args . description is not None : self . description = self . args . description if self . args . aggregate is not None : self . aggregate = self . args . aggregate if self . args . unit is not None : self . unit = self . args . unit if self . args . resolution is not None : self . resolution = self . args . resolution if self . args . is Disabled is not None : self . is Disabled = self . args . is Disabled if self . args . type is not None : self . type = self . args . type data = { } if self . metric Name is not None : data [ 'name' ] = self . metric Name if self . display Name is not None : data [ 'display Name' ] = self . display Name if self . display Name Short is not None : data [ 'display Name Short' ] = self . display Name Short if self . description is not None : data [ 'description' ] = self . description if self . aggregate is not None : data [ 'default Aggregate' ] = self . aggregate if self . unit is not None : data [ 'unit' ] = self . unit if self . resolution is not None : data [ 'default Resolution MS' ] = self . resolution if self . is Disabled is not None : data [ 'is Disabled' ] = True if self . is Disabled == 'yes' else False if self . type is not None : data [ 'type' ] = self . type self . path = \"v1/metrics/{0}\" . format ( self . metric Name ) self . data = json . dumps ( data , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , \"Accept\" : \"application/json\" }", "predictions": ["a method to get the 0 - length directive class name ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 5040, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) self . alarm name = self . args . alarm name if self . args . alarm name is not None else None", "predictions": ["method to get a class object from the cache ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 5041, "code": "def read ( self ) : f = open ( self . path , \"r\" ) self . manifest json = f . read ( )", "predictions": ["get the + + ."], "references": ["load the metrics file from the given path"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5042, "code": "def load ( self ) : manifest = Plugin Manifest ( self . file path ) manifest . get ( ) self . manifest = manifest . get manifest ( )", "predictions": ["obtain a , based on the , arg and [ filename ]"], "references": ["read the file and parse json into dictionary"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 5043, "code": "def get Metric Definition ( self , name ) : metric = None for m in self . metric definitions : if m [ 'name' ] == name : metric = m break return metric", "predictions": ["return a list of ."], "references": ["looks up the metric definition from the definitions from the api call"], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 5044, "code": "def print Metrics Header ( self , m , d ) : mstr = \"Metric Name\" dstr = \"Description\" print ( '|{0}{1}|{2}{3}|' . format ( mstr , ' ' * ( m - len ( mstr ) ) , dstr , ' ' * ( d - len ( dstr ) ) ) ) print ( '|:{0}|:{1}|' . format ( '-' * ( m - 1 ) , '-' * ( d - 1 ) ) )", "predictions": ["ast utility method for formatting ("], "references": ["prints out table header based on the size of the data in columns"], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 5045, "code": "def get Fields Column Lengths ( self ) : name Len = 0 desc Len = 0 for f in self . fields : name Len = max ( name Len , len ( f [ 'title' ] ) ) desc Len = max ( desc Len , len ( f [ 'description' ] ) ) return ( name Len , desc Len )", "predictions": ["generate and return the pair for the column that corresponds to the given name code ."], "references": ["gets the maximum length of each column in the field table"], "bleu": 0.09147827112247602, "rouge_l": 0.22989949748743718}
{"id": 5046, "code": "def get Metrics Column Lengths ( self ) : display Len = 0 desc Len = 0 for m in self . metrics : display Len = max ( display Len , len ( m [ 'display Name' ] ) ) desc Len = max ( desc Len , len ( m [ 'description' ] ) ) return ( display Len , desc Len )", "predictions": ["returns for each column in the column ."], "references": ["gets the maximum length of each column"], "bleu": 0.22679164443904004, "rouge_l": 0.26991150442477874}
{"id": 5047, "code": "def escape Underscores ( self ) : new metrics = [ ] for m in self . metrics : m [ 'name' ] = m [ 'name' ] . replace ( \" \" , \"\\ \" ) new metrics . append ( m ) self . metrics = new metrics", "predictions": ["this will use this method to ast to avoid dynamic changes ."], "references": ["escape underscores so that the markdown is correct"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 5048, "code": "def print Fields Header ( self , f , d ) : fstr = \"Field Name\" dstr = \"Description\" f = max ( f , len ( fstr ) ) d = max ( d , len ( dstr ) ) print ( '|{0}{1}|{2}{3}|' . format ( fstr , ' ' * ( f - len ( fstr ) ) , dstr , ' ' * ( d - len ( dstr ) ) ) ) print ( '|:{0}|:{1}|' . format ( '-' * ( f - 1 ) , '-' * ( d - 1 ) ) ) return ( f , d )", "predictions": ["ast method called in each alias ."], "references": ["prints out table header based on the size of the data in columns"], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 5049, "code": "def print Metrics ( self , m , d ) : for metric in self . metrics : mstr = metric [ 'display Name' ] dstr = metric [ 'description' ] mlen = m - len ( mstr ) dlen = d - len ( dstr ) print ( \"|{0}{1}|{2}{3}|\" . format ( mstr , ' ' * mlen , dstr , ' ' * dlen ) )", "predictions": ["prints out the ) ) method stripped ."], "references": ["prints out table rows based on the size of the data in columns"], "bleu": 0.12139281957861149, "rouge_l": 0.2739520958083832}
{"id": 5050, "code": "def print Fields ( self , f , d ) : for field in self . fields : fstr = field [ \"title\" ] dstr = field [ \"description\" ] flen = f - len ( fstr ) dlen = d - len ( dstr ) print ( \"|{0}{1}|{2}{3}|\" . format ( fstr , ' ' * flen , dstr , ' ' * dlen ) )", "predictions": ["prints the formatted digest of each method in a loop ."], "references": ["prints out table rows based on the size of the data in columns"], "bleu": 0.11941964005964323, "rouge_l": 0.3283983849259758}
{"id": 5051, "code": "def output Field Markdown ( self ) : f , d = self . get Fields Column Lengths ( ) fc , dc = self . print Fields Header ( f , d ) f = max ( fc , f ) d = max ( dc , d ) self . print Fields ( f , d )", "predictions": ["how much compatibility for this column ."], "references": ["sends the field definitions ot standard out"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5052, "code": "def output Metric Markdown ( self ) : self . escape Underscores ( ) m , d = self . get Metrics Column Lengths ( ) self . print Metrics Header ( m , d ) self . print Metrics ( m , d )", "predictions": ["output the example comment for this feature ."], "references": ["sends the markdown of the metric definitions to standard out"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 5053, "code": "def generate Markdown ( self ) : self . generate Metric Definitions ( ) self . generate Field Definitions ( ) self . generate Dashboard Definitions ( ) self . output Markdown ( )", "predictions": ["generates a simple call wrapper around a base class"], "references": ["look up each of the metrics and then output in markdown"], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 5054, "code": "def parse ( self , text ) : self . original text = text try : return getattr ( self , self . entry point ) ( text ) except ( Dead End ) as exc : raise Parser Error ( self . most consumed , \"Failed to parse input\" ) from exc return tree", "predictions": ["returns a l{deferred} whose content is the content of this method ."], "references": ["attempt to parse source code ."], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 5055, "code": "def attempting ( self , text ) : consumed = len ( self . original text ) - len ( text ) self . most consumed = max ( consumed , self . most consumed )", "predictions": ["construct a chunk of , using the specified content ."], "references": ["keeps track of the furthest point in the source code the parser has reached to this point ."], "bleu": 0.06735938555336447, "rouge_l": 0.20378619153674832}
{"id": 5056, "code": "def add arguments ( self ) : Api Cli . add arguments ( self ) self . parser . add argument ( '-f' , '--format' , dest = 'format' , action = 'store' , required = False , choices = [ 'csv' , 'json' , 'raw' , 'xml' ] , help = 'Output format. Default is raw' ) self . parser . add argument ( '-n' , '--name' , dest = 'metric name' , action = 'store' , required = True , metavar = \"metric name\" , help = 'Metric identifier' ) self . parser . add argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = False , choices = [ 'sum' , 'avg' , 'max' , 'min' ] , help = 'Metric default aggregate' ) self . parser . add argument ( '-r' , '--sample' , dest = 'sample' , action = 'store' , type = int , metavar = \"sample\" , help = 'Down sample rate sample in seconds' ) self . parser . add argument ( '-s' , '--source' , dest = 'source' , action = 'store' , metavar = \"source\" , required = True , help = 'Source of measurement' ) self . parser . add argument ( '-b' , '--start' , dest = 'start' , action = 'store' , required = True , metavar = \"start\" , help = 'Start of time range as ISO 8601 string or epoch seconds' ) self . parser . add argument ( '-d' , '--end' , dest = 'end' , action = 'store' , metavar = \"end\" , required = False , help = 'End of time range as ISO 8601 string or epoch seconds' ) self . parser . add argument ( '-o' , '--date-format' , dest = 'date format' , action = 'store' , metavar = \"format\" , required = False , help = 'For CSV, JSON, and XML output formats dates (see Python date.strftime). ' + 'Default format is %%s' )", "predictions": ["add a user to the player ."], "references": ["add specific command line arguments for this command"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5057, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric name is not None : self . metric name = self . args . metric name if self . args . sample is not None : self . sample = self . args . sample if self . args . source is not None : self . source = self . args . source else : self . source = None if self . args . aggregate is not None : self . aggregate = self . args . aggregate else : self . aggregate = \"avg\" if self . args . format is not None : self . format = self . args . format else : self . format = \"json\" if self . args . date format is not None : self . date format = self . args . date format start time = int ( self . parse time date ( self . args . start ) . strftime ( \"%s\" ) ) if self . args . end is None : stop time = int ( self . now . strftime ( \"%s\" ) ) else : stop time = int ( self . parse time date ( self . args . end ) . strftime ( \"%s\" ) ) start time *= 1000 stop time *= 1000 self . path = \"v1/measurements/{0}\" . format ( self . metric name ) url parameters = { \"start\" : str ( start time ) , \"end\" : str ( stop time ) , \"sample\" : str ( self . sample ) , \"agg\" : self . aggregate } if self . source is not None : url parameters [ 'source' ] = self . source self . url parameters = url parameters", "predictions": ["creates and returns a url for the given metric ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 5058, "code": "def output csv ( self , text ) : payload = json . loads ( text ) print ( \"{0},{1},{2},{3},{4}\" . format ( 'timestamp' , 'metric' , 'aggregate' , 'source' , 'value' ) ) metric name = self . metric name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : print ( '{0},\"{1}\",\"{2}\",\"{3}\",{4}' . format ( timestamp , metric name , self . aggregate , s [ 0 ] , s [ 1 ] ) )", "predictions": ["output this text to the specified text ."], "references": ["output results in csv format"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5059, "code": "def output json ( self , text ) : payload = json . loads ( text ) data = [ ] metric name = self . metric name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : data . append ( { \"timestamp\" : timestamp , \"metric\" : metric name , \"aggregate\" : self . aggregate , \"source\" : s [ 0 ] , \"value\" : s [ 1 ] , } ) payload = { \"data\" : data } out = json . dumps ( payload , indent = self . indent , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )", "predictions": ["output the json formatted by text ."], "references": ["output results in structured json format"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5060, "code": "def output raw ( self , text ) : payload = json . loads ( text ) out = json . dumps ( payload , sort keys = True , indent = self . indent , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )", "predictions": ["output to json . this will return something as a string ."], "references": ["output results in raw json format"], "bleu": 0.11498759556447223, "rouge_l": 0.2364341085271318}
{"id": 5061, "code": "def output xml ( self , text ) : document = Element ( 'results' ) comment = Comment ( 'Generated by True Sight Pulse measurement-get CLI' ) document . append ( comment ) aggregates = Sub Element ( document , 'aggregates' ) aggregate = Sub Element ( aggregates , 'aggregate' ) measurements = Sub Element ( aggregate , 'measurements' ) payload = json . loads ( text ) metric name = self . metric name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : measure node = Sub Element ( measurements , 'measure' ) source = s [ 0 ] value = str ( s [ 1 ] ) ts node = Sub Element ( measure node , 'timestamp' ) ts node . text = str ( timestamp ) metric node = Sub Element ( measure node , 'metric' ) metric node . text = metric name metric node = Sub Element ( measure node , 'aggregate' ) metric node . text = self . aggregate source node = Sub Element ( measure node , 'source' ) source node . text = source value node = Sub Element ( measure node , 'value' ) value node . text = value rough string = Element Tree . tostring ( document , 'utf-8' ) reparse = minidom . parse String ( rough string ) output = reparse . toprettyxml ( indent = \" \" ) print ( self . colorize xml ( output ) )", "predictions": ["generate a output comment ."], "references": ["output results in json format"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 5062, "code": "def handle results ( self ) : if self . api result . status code == requests . codes . ok : if self . format == \"json\" : self . output json ( self . api result . text ) elif self . format == \"csv\" : self . output csv ( self . api result . text ) elif self . format == \"raw\" : self . output raw ( self . api result . text ) elif self . format == \"xml\" : self . output xml ( self . api result . text ) else : pass", "predictions": ["handle results of results ."], "references": ["call back function to be implemented by the cli ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 5063, "code": "def trimmed pred default ( node , parent ) : return isinstance ( node , Parse Node ) and ( node . is empty or node . is type ( Parse Node Type . terminal ) )", "predictions": ["creates a trimmed search node for the given node ."], "references": ["the default predicate used in node . trimmed ."], "bleu": 0.18850319022747347, "rouge_l": 0.31881533101045295}
{"id": 5064, "code": "def pprint ( root , depth = 0 , space unit = \"    \" , * , source len = 0 , file = None ) : spacing = space unit * depth if isinstance ( root , str ) : print ( \"{0}terminal@(?): {1}\" . format ( spacing , root ) , file = file ) else : if root . position is None : position = - 1 elif root . position < 0 : position = source len + root . position else : position = root . position if root . is value : print ( \"{0}{1}@({2}:{3}):\\t{4}\" . format ( spacing , root . node type , position , root . consumed , root . svalue ) , file = file ) else : print ( \"{0}{1}@({2}:{3}):\" . format ( spacing , root . node type , position , root . consumed ) , file = file ) for child in root . children : pprint ( child , depth + 1 , source len = source len , file = file )", "predictions": ["pprint all operations in this group so we can make it easier to call ( ."], "references": ["pretting print a parse tree ."], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 5065, "code": "def repetition ( extractor , bounds , * , ignore whitespace = False ) : return partial ( get repetition , extractor , bounds = bounds , ignore whitespace = ignore whitespace )", "predictions": ["repetition function to help repetition on next entity ."], "references": ["returns a partial of _get_repetition that accepts only a text argument ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 5066, "code": "def count leading whitespace ( text ) : idx = 0 for idx , char in enumerate ( text ) : if not char . isspace ( ) : return idx return idx + 1", "predictions": ["return a string of this text ."], "references": ["returns the number of characters at the beginning of text that are whitespace ."], "bleu": 0.0812630644213965, "rouge_l": 0.2695139911634757}
{"id": 5067, "code": "def retyped ( self , new type ) : return Parse Node ( new type , children = list ( self . children ) , consumed = self . consumed , position = self . position , ignored = self . ignored )", "predictions": ["get the retyped at the end of this type ."], "references": ["returns a new node with the same contents as self but with a new node_type ."], "bleu": 0.0765635970878477, "rouge_l": 0.1476997578692494}
{"id": 5068, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host group name is not None : self . host group name = self . args . host group name if self . args . sources is not None : self . sources = self . args . sources payload = { } if self . host group name is not None : payload [ 'name' ] = self . host group name if self . sources is not None : source list = str . split ( self . sources , ',' ) if 'hostnames' not in payload : payload [ 'hostnames' ] = [ ] for s in source list : payload [ 'hostnames' ] . append ( s ) self . data = json . dumps ( payload , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , \"Accept\" : \"application/json\" }", "predictions": ["for each payload in the given source ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5069, "code": "def get scope list ( self ) -> list : lstparent = [ self ] p = self . get parent ( ) while p is not None : lstparent . append ( p ) p = p . get parent ( ) return lstparent", "predictions": ["this method returns a list of scope names for this scope ."], "references": ["return the list of all contained scope from global to local"], "bleu": 0.14694106251955755, "rouge_l": 0.2629310344827586}
{"id": 5070, "code": "def get scope names ( self ) -> list : lscope = [ ] for scope in reversed ( self . get scope list ( ) ) : if scope . name is not None : lscope . append ( scope . name ) return lscope", "predictions": ["this method returns a list of scope names for this project ."], "references": ["return the list of all contained scope from global to local"], "bleu": 0.14694106251955755, "rouge_l": 0.2629310344827586}
{"id": 5071, "code": "def position ( self ) -> Position : return Position ( self . index , self . lineno , self . col offset )", "predictions": ["a method for creating a position and get the position of this buffer ."], "references": ["the current position of the cursor ."], "bleu": 0.13217947626377288, "rouge_l": 0.4053156146179402}
{"id": 5072, "code": "def max readed position ( self ) -> Position : return Position ( self . maxindex , self . maxline , self . maxcol )", "predictions": ["a method to get the first position of this cipher ."], "references": ["the index of the deepest character readed ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 5073, "code": "def step next char ( self ) : self . index += 1 self . col offset += 1 if self . index > self . maxindex : self . maxindex = self . index self . maxcol = self . col offset self . maxline = self . lineno", "predictions": ["passes the next character in this tokenizer to the stringbuffer ."], "references": ["puts the cursor on the next character ."], "bleu": 0.23462350320527994, "rouge_l": 0.433392539964476}
{"id": 5074, "code": "def step next line ( self ) : self . eol . append ( self . position ) self . lineno += 1 self . col offset = 0", "predictions": ["passes next available line ."], "references": ["sets cursor as beginning of next line ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 5075, "code": "def step prev line ( self ) : #TODO(bps): raise explicit error for unregistered eol #assert self. eol[-1].index == self. index if len ( self . eol ) > 0 : self . position = self . eol . pop ( )", "predictions": ["step that always returns a line ."], "references": ["sets cursor as end of previous line ."], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 5076, "code": "def last readed line ( self ) -> str : mpos = self . cursor . max readed position mindex = mpos . index prevline = mindex - 1 if mindex == self . eos index else mindex while prevline >= 0 and self . content [ prevline ] != '\\n' : prevline -= 1 nextline = mindex while nextline < self . eos index and self . content [ nextline ] != '\\n' : nextline += 1 last line = self . content [ prevline + 1 : nextline ] return last line", "predictions": ["this will return a short version of the line at the bottom of the line ."], "references": ["usefull string to compute error message ."], "bleu": 0.07692375026049747, "rouge_l": 0.09355828220858894}
{"id": 5077, "code": "def incpos ( self , length : int = 1 ) -> int : if length < 0 : raise Value Error ( \"length must be positive\" ) i = 0 while ( i < length ) : if self . cursor . index < self . len : if self . peek char == '\\n' : self . cursor . step next line ( ) self . cursor . step next char ( ) i += 1 return self . cursor . index", "predictions": ["this function is called by the database to get a ( call ."], "references": ["increment the cursor to the next character ."], "bleu": 0.1135935489027116, "rouge_l": 0.2985318107667211}
{"id": 5078, "code": "def save context ( self ) -> bool : self . contexts . append ( self . cursor . position ) return True", "predictions": ["saves content at the top of the list ."], "references": ["save current position ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 5079, "code": "def restore context ( self ) -> bool : self . cursor . position = self . contexts . pop ( ) return False", "predictions": ["save a previously reflected region to a previous contact ."], "references": ["rollback to previous saved position ."], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 5080, "code": "def to fmt ( self ) -> fmt . indentable : qual = \"scope\" txt = fmt . sep ( \" \" , [ qual ] ) name = self . show name ( ) if name != \"\" : txt . lsdata . append ( name ) if len ( self . hsig ) > 0 or len ( self . map Type Translate ) > 0 : lsb = [ ] if len ( self . map Type Translate ) > 0 : lsb . append ( \"translate:\\n\" ) lsb . append ( fmt . end ( \"\\n\" , self . map Type Translate . to fmt ( ) ) ) for k in sorted ( self . hsig . keys ( ) ) : s = self . hsig [ k ] lsb . append ( fmt . end ( \"\\n\" , [ s . to fmt ( ) ] ) ) block = fmt . block ( \":\\n\" , \"\" , fmt . tab ( lsb ) ) txt . lsdata . append ( block ) return txt", "predictions": ["convert from our our ( class to a single string ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5081, "code": "def to fmt ( self ) : qual = \"evalctx\" lseval = [ ] block = fmt . block ( \":\\n\" , \"\" , fmt . tab ( lseval ) ) txt = fmt . sep ( \" \" , [ qual , block ] ) lseval . append ( self . sig . to fmt ( ) ) if len ( self . resolution ) > 0 : lsb = [ ] for k in sorted ( self . resolution . keys ( ) ) : s = self . resolution [ k ] if s is not None : lsb . append ( fmt . end ( \"\\n\" , [ \"'%s': %s (%s)\" % ( k , s , s ( ) . show name ( ) ) ] ) ) else : lsb . append ( fmt . end ( \"\\n\" , [ \"'%s': Unresolved\" % ( k ) ] ) ) if self . translate to is not None : lsb . append ( \"use translator:\" ) lsb . append ( self . translate to . to fmt ( ) ) if self . variadic types is not None : lsb . append ( \"variadic types:\\n\" ) arity = self . sig . arity for t in self . variadic types : lsb . append ( \"[%d] : %s\\n\" % ( arity , t ) ) arity += 1 lseval . append ( fmt . block ( \"\\nresolution :\\n\" , \"\" , fmt . tab ( lsb ) ) ) return txt", "predictions": ["returns a value in a single ( list ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5082, "code": "def to fmt ( self , with from = False ) -> fmt . indentable : txt = fmt . sep ( \"\\n\" , [ fmt . sep ( \" \" , [ self . type source , \"to\" , self . type target , '=' , self . fun . to fmt ( ) ] ) , self . notify . get content ( with from ) ] ) return txt", "predictions": ["makes a shallow type from this instance ."], "references": ["return a fmt representation of translator for pretty - printing"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 5083, "code": "def to fmt ( self ) : params = \"\" txt = fmt . sep ( \" \" , [ 'val' ] ) name = self . show name ( ) if name != \"\" : txt . lsdata . append ( name ) txt . lsdata . append ( '(%s)' % self . value ) txt . lsdata . append ( ': ' + self . tret ) return txt", "predictions": ["a method for print the ( state ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 5084, "code": "def to fmt ( self ) : params = \"\" txt = fmt . sep ( \" \" , [ 'fun' ] ) name = self . show name ( ) if name != \"\" : txt . lsdata . append ( name ) tparams = [ ] if self . tparams is not None : tparams = list ( self . tparams ) if self . variadic : tparams . append ( '...' ) params = '(' + \", \" . join ( tparams ) + ')' txt . lsdata . append ( ': ' + params ) txt . lsdata . append ( '-> ' + self . tret ) return txt", "predictions": ["returns a single method for a single ( operation ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5085, "code": "def set name ( self , name : str ) : self . name = name lsig = self . hsig . values ( ) self . hsig = { } for s in lsig : self . hsig [ s . internal name ( ) ] = s", "predictions": ["sets the name of this class to be set on the specified name ."], "references": ["you could set the name after construction"], "bleu": 0.1250076305588977, "rouge_l": 0.30398671096345514}
{"id": 5086, "code": "def count vars ( self ) -> int : n = 0 for s in self . hsig . values ( ) : if hasattr ( s , 'is var' ) and s . is var : n += 1 return n", "predictions": ["counts how many things were collected ."], "references": ["count var define by this scope"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5087, "code": "def count funs ( self ) -> int : n = 0 for s in self . hsig . values ( ) : if hasattr ( s , 'is fun' ) and s . is fun : n += 1 return n", "predictions": ["counts how many times were loaded ."], "references": ["count function define by this scope"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5088, "code": "def update ( self , sig : list or Scope ) -> Scope : values = sig if hasattr ( sig , 'values' ) : values = sig . values ( ) for s in values : if self . is namespace : s . set parent ( self ) if isinstance ( s , Scope ) : s . state = State Scope . EMBEDDED self . hsig [ s . internal name ( ) ] = s self . update count ( ) return self", "predictions": ["add the operator to the operator ' s operator ."], "references": ["update the set with values of another set"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5089, "code": "def union ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new |= sig return new", "predictions": ["returns a get method for the given not ."], "references": ["create a new set produce by the union of 2 set"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 5090, "code": "def intersection update ( self , oset : Scope ) -> Scope : keys = list ( self . hsig . keys ( ) ) for k in keys : if k not in oset : del self . hsig [ k ] else : self . hsig [ k ] = oset . get ( k ) return self", "predictions": ["csv product with trailing namespace ."], "references": ["update set with common values of another set"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5091, "code": "def intersection ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new &= sig return new", "predictions": ["returns the output of this cipher ."], "references": ["create a new set produce by the intersection of 2 set"], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 5092, "code": "def difference update ( self , oset : Scope ) -> Scope : keys = list ( self . hsig . keys ( ) ) for k in keys : if k in oset : del self . hsig [ k ] return self", "predictions": ["get the list of json values for this distribution ."], "references": ["remove values common with another set"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 5093, "code": "def difference ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new -= sig return new", "predictions": ["returns a new classpath with the same name as this cipher ."], "references": ["create a new set produce by a set subtracted by another set"], "bleu": 0.1367440667823257, "rouge_l": 0.16666666666666666}
{"id": 5094, "code": "def symmetric difference ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new ^= sig return new", "predictions": ["returns a new classpath with the given name ."], "references": ["create a new set with values present in only one set"], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 5095, "code": "def add ( self , it : Signature ) -> bool : if isinstance ( it , Scope ) : it . state = State Scope . EMBEDDED txt = it . internal name ( ) it . set parent ( self ) if self . is namespace : txt = it . internal name ( ) if txt == \"\" : txt = ' ' + str ( len ( self . hsig ) ) if txt in self . hsig : raise Key Error ( \"Already exists %s\" % txt ) self . hsig [ txt ] = it self . update count ( ) return True", "predictions": ["adds the specific parameter ."], "references": ["add it to the set"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 5096, "code": "def remove ( self , it : Signature ) -> bool : txt = it . internal name ( ) if txt not in self . hsig : raise Key Error ( it . show name ( ) + ' not in Set' ) sig = self . hsig [ txt ] if isinstance ( sig , Scope ) : sig . state = State Scope . LINKED del self . hsig [ txt ] return True", "predictions": ["removes a class from the underlying signature ."], "references": ["remove it but raise keyerror if not found"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5097, "code": "def discard ( self , it : Signature ) -> bool : txt = it . internal name ( ) if txt in self . hsig : sig = self . hsig [ txt ] if isinstance ( sig , Scope ) : sig . state = State Scope . LINKED del self . hsig [ txt ] return True return False", "predictions": ["releases the translation for the current thread ."], "references": ["remove it only if present"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5098, "code": "def first ( self ) -> Signature : k = sorted ( self . hsig . keys ( ) ) return self . hsig [ k [ 0 ] ]", "predictions": ["get the count of the list ."], "references": ["retrieve the first signature ordered by mangling descendant"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5099, "code": "def last ( self ) -> Signature : k = sorted ( self . hsig . keys ( ) ) return self . hsig [ k [ - 1 ] ]", "predictions": ["get the last signature ."], "references": ["retrieve the last signature ordered by mangling descendant"], "bleu": 0.278869164867688, "rouge_l": 0.44309927360774815}
{"id": 5100, "code": "def get ( self , key : str , default = None ) -> Signature : item = default if key in self . hsig : item = self . hsig [ key ] return item", "predictions": ["get the value of the underlying field ."], "references": ["get a signature instance by its internal_name"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5101, "code": "def get by symbol name ( self , name : str ) -> Scope : lst = [ ] for s in self . values ( ) : if s . name == name : lst . append ( Eval Ctx . from sig ( s ) ) if len ( lst ) == 0 : p = self . get parent ( ) if p is not None : return p . get by symbol name ( name ) rscope = Scope ( sig = lst , state = State Scope . LINKED , is namespace = False ) rscope . set parent ( self ) return rscope", "predictions": ["returns a list with the canonical ( list = ( ( ( ( ( ( ( ( but never = 1 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 3 = 2 = 2 = 3 = 2 = 2 = 3 ="], "references": ["retrieve a set of all signature by symbol name"], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 5102, "code": "def call Injector ( self , old : Node , trans : Translator ) -> Node : if self . ast Translator Injector is None : if self . parent is not None : return self . parent ( ) . call Injector ( old , trans ) else : raise Type Error ( \"Must define an Translator Injector\" ) return self . ast Translator Injector ( old , trans )", "predictions": ["voltage from the route and executes the change ."], "references": ["if don t have injector call from parent"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 5103, "code": "def set ( self , othernode ) : self . class = othernode . class self . clean ( ) if len ( othernode ) > 0 : for k , v in othernode . items ( ) : self [ k ] = v for k , v in vars ( othernode ) . items ( ) : setattr ( self , k , v )", "predictions": ["attach to the class ."], "references": ["allow to completly mutate the node into any subclasses of node"], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 5104, "code": "def hit ok ( hit , min hit charge , max hit charge ) : if hit [ 'charge' ] < min hit charge : return False if max hit charge != 0 and hit [ 'charge' ] > max hit charge : return False return True", "predictions": ["internal method to indicate that the executorservice is valid . if the target is less than the maximum number of minimum minimum value is less than the maximum value . if the max value is less than the maximum value , it should be returned . if the target is"], "references": ["check if given hit is withing the limits ."], "bleu": 0.030216776104535565, "rouge_l": 0.15501905972045746}
{"id": 5105, "code": "def resolve ( self ) : t2resolv = [ ] if hasattr ( self . sig , 'tret' ) : t2resolv . append ( self . sig . tret ) if hasattr ( self . sig , 'tparams' ) and self . sig . tparams is not None : for p in self . sig . tparams : t2resolv . append ( p ) if self . translate to is not None : t2resolv . append ( self . translate to . target ) if self . variadic types is not None : for t in self . variadic types : t2resolv . append ( t ) for t in t2resolv : for c in t . components : if c not in self . resolution or self . resolution [ c ] is None : parent = self . get parent ( ) if parent is not None : sc = parent . get by symbol name ( c ) if len ( sc ) == 1 : sc = list ( sc . values ( ) ) [ 0 ] if isinstance ( sc , Eval Ctx ) : sc = sc . sig rtyp = weakref . ref ( sc ) self . resolution [ c ] = rtyp continue self . resolution [ c ] = None", "predictions": ["resolves all resolution by > ) ."], "references": ["process the signature and find definition for type ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5106, "code": "def set resolved name ( self , ref : dict , type name2solve : Type Name , type name ref : Type Name ) : if self . resolution [ type name2solve . value ] is None : self . resolution [ type name2solve . value ] = ref [ type name ref . value ]", "predictions": ["sets line line reference to the type specified by self - both instances ."], "references": ["warning!!! need to rethink it when global poly type"], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 5107, "code": "def to fmt ( self ) -> fmt . indentable : lsb = [ ] if len ( self . lsig ) > 0 : for s in self . lsig : lsb . append ( s . to fmt ( ) ) block = fmt . block ( \"(\" , \")\" , fmt . sep ( ', ' , lsb ) ) qual = \"tuple\" txt = fmt . sep ( \"\" , [ qual , block ] ) return txt", "predictions": ["returns a list with a single entry in it ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5108, "code": "def internal name ( self ) : unq = super ( ) . internal name ( ) if self . tret is not None : unq += \" \" + self . tret return unq", "predictions": ["a method to get an last last last name of the underlying class ."], "references": ["return the unique internal name"], "bleu": 0.09782375748961449, "rouge_l": 0.11509433962264153}
{"id": 5109, "code": "def delete local ( self , filename ) : if os . path . exists ( filename ) : os . remove ( filename )", "predictions": ["this method deletes the ( or removes the ( : if it exists int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int"], "references": ["deletes the specified file from the local filesystem ."], "bleu": 0.033984283835209204, "rouge_l": 0.11626429479034307}
{"id": 5110, "code": "def delete s3 ( self , filename , bucket name ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( bucket name ) if type ( filename ) . name == 'Key' : filename = '/' + filename . name path = self . get s3 path ( filename ) k = Key ( bucket ) k . key = path try : bucket . delete key ( k ) except S3Response Error : pass", "predictions": ["deletes an context from the context ."], "references": ["deletes the specified file from the given s3 bucket ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 5111, "code": "def delete ( self , filename , storage type = None , bucket name = None ) : if not ( storage type and bucket name ) : self . delete local ( filename ) else : if storage type != 's3' : raise Value Error ( 'Storage type \"%s\" is invalid, the only supported storage type (apart from default local storage) is s3.' % storage type ) self . delete s3 ( filename , bucket name )", "predictions": ["deletes an s3 and its contents ."], "references": ["deletes the specified file either locally or from s3 depending on the file s storage type ."], "bleu": 0.05293793409875998, "rouge_l": 0.23252858958068615}
{"id": 5112, "code": "def save local ( self , temp file , filename , obj ) : path = self . get path ( filename ) if not os . path . exists ( os . path . dirname ( path ) ) : os . makedirs ( os . path . dirname ( path ) , self . permission | 0o111 ) fd = open ( path , 'wb' ) temp file . seek ( 0 ) t = temp file . read ( 1048576 ) while t : fd . write ( t ) t = temp file . read ( 1048576 ) fd . close ( ) if self . filesize field : setattr ( obj , self . filesize field , os . path . getsize ( path ) ) return filename", "predictions": ["saves the fmt object to the passed fmt as an object ."], "references": ["saves the specified file to the local file system ."], "bleu": 0.17996531271765898, "rouge_l": 0.46212121212121204}
{"id": 5113, "code": "def save s3 ( self , temp file , filename , obj ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( self . bucket name ) path = self . get s3 path ( filename ) k = bucket . new key ( path ) k . set contents from string ( temp file . getvalue ( ) ) k . set acl ( self . acl ) if self . filesize field : setattr ( obj , self . filesize field , k . size ) return filename", "predictions": ["saves an fmt to the fmt ."], "references": ["saves the specified file to the configured s3 bucket ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 5114, "code": "def save ( self , temp file , filename , obj ) : if not ( self . storage type and self . bucket name ) : ret = self . save local ( temp file , filename , obj ) else : if self . storage type != 's3' : raise Value Error ( 'Storage type \"%s\" is invalid, the only supported storage type (apart from default local storage) is s3.' % self . storage type ) ret = self . save s3 ( temp file , filename , obj ) if self . field name : setattr ( obj , self . field name , ret ) if self . storage type == 's3' : if self . storage type field : setattr ( obj , self . storage type field , self . storage type ) if self . bucket name field : setattr ( obj , self . bucket name field , self . bucket name ) else : if self . storage type field : setattr ( obj , self . storage type field , '' ) if self . bucket name field : setattr ( obj , self . bucket name field , '' ) return ret", "predictions": ["writes an object to the specified location ."], "references": ["saves the specified file to either s3 or the local filesystem depending on the currently enabled storage type ."], "bleu": 0.06063168540538234, "rouge_l": 0.20701357466063344}
{"id": 5115, "code": "def find by path s3 ( self , path , bucket name ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( bucket name ) s3 path = self . get s3 path ( path ) return bucket . list ( prefix = s3 path )", "predictions": ["to to to to to to to to get an self - sensitive = index1 , if no ] exists ."], "references": ["finds files by licking an s3 bucket s contents by prefix ."], "bleu": 0.06429451441231726, "rouge_l": 0.12748171368861025}
{"id": 5116, "code": "def enum ( * sequential , * * named ) : #: build enums from parameter enums = dict ( zip ( sequential , range ( len ( sequential ) ) ) , * * named ) enums [ 'map' ] = copy . copy ( enums ) #: build reverse mapping enums [ 'rmap' ] = { } for key , value in enums . items ( ) : if type ( value ) is int : enums [ 'rmap' ] [ value ] = key return type ( 'Enum' , ( ) , enums )", "predictions": ["this builds a ( from a given self - : 1 . 2 . 3 . 1 . 1 . 1 . to avoid having a : 1 . 2 . 3 . 3 . 3 . . . ( . ( . ( . ( . ( . ("], "references": ["build an enum statement"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 5117, "code": "def checktypes ( func ) : sig = inspect . signature ( func ) types = { } for param in sig . parameters . values ( ) : param type = param . annotation if param type is param . empty or not inspect . isclass ( param type ) : continue types [ param . name ] = param type if ( param . default is not param . empty and not isinstance ( param . default , param type ) ) : raise Value Error ( \"{func}: wrong type of a default value for {arg!r}\" . format ( func = func . qualname , arg = param . name ) ) def check type ( sig , arg name , arg type , arg value ) : if not isinstance ( arg value , arg type ) : raise Value Error ( \"{func}: wrong type of {arg!r} argument, \" \"{exp!r} expected, got {got!r}\" . format ( func = func . qualname , arg = arg name , exp = arg type . name , got = type ( arg value ) . name ) ) @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : ba = sig . bind ( * args , * * kwargs ) for arg name , arg in ba . arguments . items ( ) : try : type = types [ arg name ] except Key Error : continue else : param = sig . parameters [ arg name ] if param . kind == param . VAR POSITIONAL : for value in arg : check type ( sig , arg name , type , value ) elif param . kind == param . VAR KEYWORD : for subname , value in arg . items ( ) : check type ( sig , arg name + ':' + subname , type , value ) else : check type ( sig , arg name , type , arg ) result = func ( * ba . args , * * ba . kwargs ) return type = sig . return annotation if ( return type is not sig . empty and isinstance ( return type , type ) and not isinstance ( result , return type ) ) : raise Value Error ( '{func}: wrong return type, {exp} expected, got {got}' . format ( func = func . qualname , exp = return type . name , got = type ( result ) . name ) ) return result return wrapper", "predictions": ["set the = [ keyword ] to the given argument ."], "references": ["decorator to verify arguments and return types ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 5118, "code": "def add method ( cls ) : def wrapper ( f ) : #if hasattr(cls, f. name ): setattr ( cls , f . name , f ) return f return wrapper", "predictions": ["adds a int to the class ."], "references": ["attach a method to a class ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 5119, "code": "def read eol ( self ) -> bool : if self . read eof ( ) : return False self . stream . save context ( ) self . read char ( '\\r' ) if self . read char ( '\\n' ) : return self . stream . validate context ( ) return self . stream . restore context ( )", "predictions": ["/ count of the in - memory dataset ."], "references": ["return true if the parser can consume an eol byte sequence ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 5120, "code": "def push rule nodes ( self ) -> bool : if self . rule nodes is None : self . rule nodes = collections . Chain Map ( ) self . tag cache = collections . Chain Map ( ) self . id cache = collections . Chain Map ( ) else : self . rule nodes = self . rule nodes . new child ( ) self . tag cache = self . tag cache . new child ( ) self . id cache = self . id cache . new child ( ) return True", "predictions": ["creates and adds a rule to the cache ."], "references": ["push context variable to store rule nodes ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 5121, "code": "def pop rule nodes ( self ) -> bool : self . rule nodes = self . rule nodes . parents self . tag cache = self . tag cache . parents self . id cache = self . id cache . parents return True", "predictions": ["pops the rule at the given rule ."], "references": ["pop context variable that store rule nodes"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5122, "code": "def value ( self , n : Node ) -> str : id n = id ( n ) idcache = self . id cache if id n not in idcache : return \"\" name = idcache [ id n ] tag cache = self . tag cache if name not in tag cache : raise Exception ( \"Incoherent tag cache\" ) tag = tag cache [ name ] k = \"%d:%d\" % ( tag . begin , tag . end ) valcache = self . streams [ - 1 ] . value cache if k not in valcache : valcache [ k ] = str ( tag ) return valcache [ k ]", "predictions": ["factory method for returning the value of the node ."], "references": ["return the text value of the node"], "bleu": 0.3672056269893592, "rouge_l": 0.6075697211155379}
{"id": 5123, "code": "def begin tag ( self , name : str ) -> Node : self . tag cache [ name ] = Tag ( self . stream , self . stream . index ) return True", "predictions": ["borrowed from the underlying method on the underlying node ."], "references": ["save the current index under the given name ."], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 5124, "code": "def end tag ( self , name : str ) -> Node : self . tag cache [ name ] . set end ( self . stream . index ) return True", "predictions": ["decorator that closes the tag or cache and passes it to the end of the underlying stream ."], "references": ["extract the string between saved and current index ."], "bleu": 0.08097785064266204, "rouge_l": 0.2364341085271318}
{"id": 5125, "code": "def set rules ( cls , rules : dict ) -> bool : cls . rules = cls . rules . new child ( ) for rule name , rule pt in rules . items ( ) : if '.' not in rule name : rule name = cls . module + '.' + cls . name + '.' + rule name meta . set one ( cls . rules , rule name , rule pt ) return True", "predictions": ["allows set of rules to be used for the class . for example : \" set \" rules . rule . rule . rule . rule . rule . rule . rule . rule . rule . rule . rule . rule . rule . rule . rule . rule"], "references": ["merge internal rules set with the given rules"], "bleu": 0.030216776104535565, "rouge_l": 0.11898569570871263}
{"id": 5126, "code": "def set hooks ( cls , hooks : dict ) -> bool : cls . hooks = cls . hooks . new child ( ) for hook name , hook pt in hooks . items ( ) : if '.' not in hook name : hook name = cls . module + '.' + cls . name + '.' + hook name meta . set one ( cls . hooks , hook name , hook pt ) return True", "predictions": ["gets all subclasses of ( from ( and class ."], "references": ["merge internal hooks set with the given hooks"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5127, "code": "def eval rule ( self , name : str ) -> Node : n = Node ( ) id n = id ( n ) self . rule nodes [ ' ' ] = n self . id cache [ id n ] = ' ' if name not in self . class . rules : self . diagnostic . notify ( error . Severity . ERROR , \"Unknown rule : %s\" % name , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic self . last Rule = name rule to eval = self . class . rules [ name ] res = rule to eval ( self ) if res : res = self . rule nodes [ ' ' ] return res", "predictions": ["evaluate the progress of this class ."], "references": ["evaluate a rule by name ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5128, "code": "def eval hook ( self , name : str , ctx : list ) -> Node : if name not in self . class . hooks : self . diagnostic . notify ( error . Severity . ERROR , \"Unknown hook : %s\" % name , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic self . last Rule = '#' + name res = self . class . hooks [ name ] ( self , * ctx ) if type ( res ) is not bool : raise Type Error ( \"Your hook %r didn't return a bool value\" % name ) return res", "predictions": ["evaluate the request . the implementation is guaranteed to be called on the call ."], "references": ["evaluate the hook by its name"], "bleu": 0.1082597837309053, "rouge_l": 0.2064297800338409}
{"id": 5129, "code": "def peek text ( self , text : str ) -> bool : start = self . stream . index stop = start + len ( text ) if stop > self . stream . eos index : return False return self . stream [ self . stream . index : stop ] == text", "predictions": ["construct a text text for the text ."], "references": ["same as readtext but doesn t consume the stream ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 5130, "code": "def one char ( self ) -> bool : if self . read eof ( ) : return False self . stream . incpos ( ) return True", "predictions": ["read a character from the underlying stream ."], "references": ["read one byte in stream"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 5131, "code": "def read until eof ( self ) -> bool : if self . read eof ( ) : return True self . stream . save context ( ) while not self . read eof ( ) : self . stream . incpos ( ) return self . stream . validate context ( )", "predictions": ["reads the contents of the stream and returns as a eof ."], "references": ["consume all the stream . same as eof in bnf ."], "bleu": 0.16261701715194898, "rouge_l": 0.43821839080459773}
{"id": 5132, "code": "def ignore blanks ( self ) -> bool : self . stream . save context ( ) if not self . read eof ( ) and self . stream . peek char in \" \\t\\v\\f\\r\\n\" : while ( not self . read eof ( ) and self . stream . peek char in \" \\t\\v\\f\\r\\n\" ) : self . stream . incpos ( ) return self . stream . validate context ( ) return self . stream . validate context ( )", "predictions": ["note : this is not thread safe ."], "references": ["consume whitespace characters ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 5133, "code": "def internal name ( self ) : unq = 'f ' + super ( ) . internal name ( ) if self . tparams is not None : unq += \" \" + \" \" . join ( self . tparams ) if self . tret is not None : unq += \" \" + self . tret return unq", "predictions": ["a method for internal use ."], "references": ["return the unique internal name"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5134, "code": "def check struct compatibility ( self , hits ) : for key , in self . cluster hits descr : if key in self . hit fields mapping inverse : mapped key = self . hit fields mapping inverse [ key ] else : mapped key = key if mapped key in [ 'cluster ID' , 'is seed' , 'cluster size' , 'n cluster' ] : continue if key not in hits . dtype . names : raise Type Error ( 'Required hit field \"%s\" not found.' % key ) if self . cluster hits . dtype [ mapped key ] != hits . dtype [ key ] : raise Type Error ( 'The dtype for hit data field \"%s\" does not match. Got/expected: %s/%s.' % ( key , hits . dtype [ key ] , self . cluster hits . dtype [ mapped key ] ) ) additional hit fields = set ( hits . dtype . names ) - set ( [ key for key , val in self . cluster hits descr ] ) if additional hit fields : logging . warning ( 'Found additional hit fields: %s' % \", \" . join ( additional hit fields ) )", "predictions": ["iterate over all of the cluster hits and ensures that we can do this ."], "references": ["takes the hit array and checks if the important data fields have the same data type than the hit clustered array and that the field names are correct ."], "bleu": 0.04067525902115805, "rouge_l": 0.17195207892882314}
{"id": 5135, "code": "def add mod ( self , seq , mod ) : modstr = self . value ( mod ) if modstr == '~' : seq . parser tree = parsing . Complement ( seq . parser tree ) elif modstr == '!!' : seq . parser tree = parsing . Look Ahead ( seq . parser tree ) elif modstr == '!' : seq . parser tree = parsing . Neg ( seq . parser tree ) elif modstr == '->' : seq . parser tree = parsing . Until ( seq . parser tree ) return True", "predictions": ["generate a tree to the numbers ."], "references": ["create a tree . { complement lookahead neg until }"], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 5136, "code": "def add ruleclause name ( self , ns name , rid ) -> bool : ns name . parser tree = parsing . Rule ( self . value ( rid ) ) return True", "predictions": ["this will add a class to the disabled declaration ."], "references": ["create a tree . rule"], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 5137, "code": "def add rules ( self , bnf , r ) -> bool : bnf [ r . rulename ] = r . parser tree return True", "predictions": ["a method to add a list of rules to the list of rules ."], "references": ["attach a parser tree to the dict of rules"], "bleu": 0.15310245441182443, "rouge_l": 0.45252225519287836}
{"id": 5138, "code": "def add rule ( self , rule , rn , alts ) -> bool : rule . rulename = self . value ( rn ) rule . parser tree = alts . parser tree return True", "predictions": ["this method adds a rule to the tree ."], "references": ["add the rule name"], "bleu": 0.15619699684601276, "rouge_l": 0.16531165311653115}
{"id": 5139, "code": "def add sequences ( self , sequences , cla ) -> bool : if not hasattr ( sequences , 'parser tree' ) : sequences . parser tree = cla . parser tree else : oldnode = sequences if isinstance ( oldnode . parser tree , parsing . Seq ) : oldpt = list ( oldnode . parser tree . ptlist ) else : oldpt = [ oldnode . parser tree ] oldpt . append ( cla . parser tree ) sequences . parser tree = parsing . Seq ( * tuple ( oldpt ) ) return True", "predictions": ["will add all numbers of this class to the set of numbers ."], "references": ["create a tree . seq"], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 5140, "code": "def add alt ( self , alternatives , alt ) -> bool : if not hasattr ( alternatives , 'parser tree' ) : if hasattr ( alt , 'parser tree' ) : alternatives . parser tree = alt . parser tree else : alternatives . parser tree = alt else : oldnode = alternatives if isinstance ( oldnode . parser tree , parsing . Alt ) : oldpt = list ( oldnode . parser tree . ptlist ) else : oldpt = [ oldnode . parser tree ] oldpt . append ( alt . parser tree ) alternatives . parser tree = parsing . Alt ( * tuple ( oldpt ) ) return True", "predictions": ["creates a new product ."], "references": ["create a tree . alt"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 5141, "code": "def add range ( self , sequence , begin , end ) : sequence . parser tree = parsing . Range ( self . value ( begin ) . strip ( \"'\" ) , self . value ( end ) . strip ( \"'\" ) ) return True", "predictions": ["returns a range of this class to save the specified range ."], "references": ["add a read_range primitive"], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 5142, "code": "def add rpt ( self , sequence , mod , pt ) : modstr = self . value ( mod ) if modstr == '!!' : self . stream . restore context ( ) self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a lookahead rule\" , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic if modstr == '!' : self . stream . restore context ( ) self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a negated rule\" , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic oldnode = sequence sequence . parser tree = pt . functor ( oldnode . parser tree ) return True", "predictions": ["restore a repeat to this sequence ."], "references": ["add a repeater to the previous sequence"], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 5143, "code": "def add capture ( self , sequence , cpt ) : cpt value = self . value ( cpt ) sequence . parser tree = parsing . Capture ( cpt value , sequence . parser tree ) return True", "predictions": ["adds a new tree to the neighborhood of this class ."], "references": ["create a tree . capture"], "bleu": 0.1354599427337814, "rouge_l": 0.40219780219780216}
{"id": 5144, "code": "def add bind ( self , sequence , cpt ) : cpt value = self . value ( cpt ) sequence . parser tree = parsing . Bind ( cpt value , sequence . parser tree ) return True", "predictions": ["adds a bind sequence to the end of this sequence ."], "references": ["create a tree . bind"], "bleu": 0.1354599427337814, "rouge_l": 0.2681318681318681}
{"id": 5145, "code": "def add hook ( self , sequence , h ) : sequence . parser tree = parsing . Hook ( h . name , h . listparam ) return True", "predictions": ["adds a hook to the receiver ."], "references": ["create a tree . hook"], "bleu": 0.22089591134157885, "rouge_l": 0.34366197183098596}
{"id": 5146, "code": "def param num ( self , param , n ) : param . pair = ( int ( self . value ( n ) ) , int ) return True", "predictions": ["method to format parameter ."], "references": ["parse a int in parameter list"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5147, "code": "def param str ( self , param , s ) : param . pair = ( self . value ( s ) . strip ( '\"' ) , str ) return True", "predictions": ["generates a request for the default pair of bytes ."], "references": ["parse a str in parameter list"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 5148, "code": "def param char ( self , param , c ) : param . pair = ( self . value ( c ) . strip ( \"'\" ) , str ) return True", "predictions": ["entry point for operation ."], "references": ["parse a char in parameter list"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 5149, "code": "def param id ( self , param , i ) : param . pair = ( self . value ( i ) , parsing . Node ) return True", "predictions": ["id or . mapping"], "references": ["parse a node name in parameter list"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 5150, "code": "def hook name ( self , hook , n ) : hook . name = self . value ( n ) hook . listparam = [ ] return True", "predictions": ["hook to tell the ( method to be executed ."], "references": ["parse a hook name"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 5151, "code": "def hook param ( self , hook , p ) : hook . listparam . append ( p . pair ) return True", "predictions": ["registers a hook to be executed once per hook is executed ."], "references": ["parse a hook parameter"], "bleu": 0.1367440667823257, "rouge_l": 0.27477477477477474}
{"id": 5152, "code": "def add directive2 ( self , sequence , d , s ) : sequence . parser tree = parsing . Directive2 ( d . name , d . listparam , s . parser tree ) return True", "predictions": ["this method adds a ) ) to the policy ."], "references": ["add a directive in the sequence"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 5153, "code": "def add directive ( self , sequence , d , s ) : if d . name in meta . directives : the class = meta . directives [ d . name ] sequence . parser tree = parsing . Directive ( the class ( ) , d . listparam , s . parser tree ) elif d . name in meta . decorators : the class = meta . decorators [ d . name ] sequence . parser tree = parsing . Decorator ( the class , d . listparam , s . parser tree ) else : raise Type Error ( \"Unkown directive or decorator %s\" % d . name ) return True", "predictions": ["pop a rule as a tree class ."], "references": ["add a directive in the sequence"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5154, "code": "def ignore cxx ( self ) -> bool : self . stream . save context ( ) while not self . read eof ( ) : idxref = self . stream . index if self . stream . peek char in \" \\t\\v\\f\\r\\n\" : while ( not self . read eof ( ) and self . stream . peek char in \" \\t\\v\\f\\r\\n\" ) : self . stream . incpos ( ) if self . peek text ( \"//\" ) : while not self . read eof ( ) and not self . peek char ( \"\\n\" ) : self . stream . incpos ( ) if not self . read char ( \"\\n\" ) and self . read eof ( ) : return self . stream . validate context ( ) if self . peek text ( \"/*\" ) : while not self . read eof ( ) and not self . peek text ( \"*/\" ) : self . stream . incpos ( ) if not self . read text ( \"*/\" ) and self . read eof ( ) : return self . stream . restore context ( ) if idxref == self . stream . index : break return self . stream . validate context ( )", "predictions": ["note : this is not synchronized because we have to do this ."], "references": ["consume comments and whitespace characters ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 5155, "code": "def add state ( self , s : State ) : ids = id ( s ) uid = len ( self . states ) if ids not in self . states : self . states [ ids ] = ( uid , s )", "predictions": ["helper method to begin a list of tag -> list of = = 1 ."], "references": ["all state in the register have a uid"], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 5156, "code": "def to dot ( self ) -> str : txt = \"\" txt += \"digraph S%d {\\n\" % id ( self ) if self . label is not None : txt += '\\tlabel=\"%s\";\\n' % ( self . label + '\\l' ) . replace ( '\\n' , '\\l' ) txt += \"\\trankdir=LR;\\n\" #txt += '\\tlabelloc=\"t\";\\n' txt += '\\tgraph [labeljust=l, labelloc=t, nojustify=true];\\n' txt += \"\\tesep=1;\\n\" txt += '\\tranksep=\"equally\";\\n' txt += \"\\tnode [shape = circle];\\n\" txt += \"\\tsplines = ortho;\\n\" for s in self . states . values ( ) : txt += s [ 1 ] . to dot ( ) txt += \"}\\n\" return txt", "predictions": ["convert a tag in the ( format to a tag ."], "references": ["provide a . dot representation of all state in the register ."], "bleu": 0.15553014371537452, "rouge_l": 0.34512022630834516}
{"id": 5157, "code": "def to dot file ( self , fname : str ) : with open ( fname , 'w' ) as f : f . write ( self . to dot ( ) )", "predictions": ["name to the content of this file ."], "references": ["write a . dot file ."], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 5158, "code": "def to png file ( self , fname : str ) : cmd = pipes . Template ( ) cmd . append ( 'dot -Tpng > %s' % fname , '-.' ) with cmd . open ( 'pipefile' , 'w' ) as f : f . write ( self . to dot ( ) )", "predictions": ["returns a hooks from this text object ."], "references": ["write a . png file ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 5159, "code": "def to fmt ( self ) -> str : infos = fmt . end ( \";\\n\" , [ ] ) s = fmt . sep ( ', ' , [ ] ) for ids in sorted ( self . states . keys ( ) ) : s . lsdata . append ( str ( ids ) ) infos . lsdata . append ( fmt . block ( '(' , ')' , [ s ] ) ) infos . lsdata . append ( \"events:\" + repr ( self . events ) ) infos . lsdata . append ( \"named events:\" + repr ( list ( self . named events . keys ( ) ) ) ) infos . lsdata . append ( \"uid events:\" + repr ( list ( self . uid events . keys ( ) ) ) ) return infos", "predictions": ["returns a list with a single entry separated by a single element ."], "references": ["provide a useful representation of the register ."], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 5160, "code": "def nextstate ( self , newstate , treenode = None , user data = None ) : if newstate is None : return self if isinstance ( newstate , State ) and id ( newstate ) != id ( self ) : return newstate elif isinstance ( newstate , State Event ) : self . state register . named events [ newstate . name ] = True return newstate . st elif isinstance ( newstate , State Precond ) : return newstate . st elif isinstance ( newstate , State Hook ) : newstate . call ( treenode , user data ) return newstate . st return self", "predictions": ["edits a subreddit to the session if the contact is already a change ."], "references": ["manage transition of state ."], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 5161, "code": "def reset Living State ( self ) : must delete = [ ] l = len ( self . ls ) for idx , ls in zip ( range ( l ) , self . ls ) : ids = id ( ls [ 1 ] . thestate ( ) ) if ids == id ( ls [ 0 ] ) and ( ls [ 1 ] . have finish or not ls [ 1 ] . alive ) : must delete . append ( idx ) elif ls [ 1 ] . alive : ls [ 1 ] . alive = False for delete in reversed ( must delete ) : self . ls . pop ( delete ) self . init all ( )", "predictions": ["resets all states used to -> -> alive ."], "references": ["only one living state on the s0 of each stateregister"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 5162, "code": "def infer block ( self , body , diagnostic = None ) : for e in body : e . infer node = Infer Node ( parent = self . infer node ) e . infer type ( diagnostic = diagnostic )", "predictions": ["one or more specific char generation ."], "references": ["infer type on block is to type each of is sub - element"], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 5163, "code": "def infer subexpr ( self , expr , diagnostic = None ) : expr . infer node = Infer Node ( parent = self . infer node ) expr . infer type ( diagnostic = diagnostic )", "predictions": ["this method is used to read the until the change has been written ."], "references": ["infer type on the subexpr"], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 5164, "code": "def list dataset uris ( cls , base uri , config path ) : uri list = [ ] parse result = generous parse uri ( base uri ) bucket name = parse result . netloc bucket = boto3 . resource ( 's3' ) . Bucket ( bucket name ) for obj in bucket . objects . filter ( Prefix = 'dtool' ) . all ( ) : uuid = obj . key . split ( '-' , 1 ) [ 1 ] uri = cls . generate uri ( None , uuid , base uri ) storage broker = cls ( uri , config path ) if storage broker . has admin metadata ( ) : uri list . append ( uri ) return uri list", "predictions": ["returns a list of all ids for the given blanks ."], "references": ["return list containing uris with base uri ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 5165, "code": "def list overlay names ( self ) : bucket = self . s3resource . Bucket ( self . bucket ) overlay names = [ ] for obj in bucket . objects . filter ( Prefix = self . overlays key prefix ) . all ( ) : overlay file = obj . key . rsplit ( '/' , 1 ) [ - 1 ] overlay name , ext = overlay file . split ( '.' ) overlay names . append ( overlay name ) return overlay names", "predictions": ["this method returns a list of name numbers that can be used to construct an batch of all name pairs ."], "references": ["return list of overlay names ."], "bleu": 0.0821610732492254, "rouge_l": 0.2469635627530364}
{"id": 5166, "code": "def iter item handles ( self ) : bucket = self . s3resource . Bucket ( self . bucket ) for obj in bucket . objects . filter ( Prefix = self . data key prefix ) . all ( ) : relpath = obj . get ( ) [ 'Metadata' ] [ 'handle' ] yield relpath", "predictions": ["iterate over all compatibility if this object is a multiple of the given ) ."], "references": ["return iterator over item handles ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 5167, "code": "def list set indent ( lst : list , indent : int = 1 ) : for i in lst : if isinstance ( i , indentable ) : i . set indent ( indent ) if isinstance ( i , list ) : list set indent ( i , indent )", "predictions": ["add an entry to the add list ."], "references": ["recurs into list for indentation"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5168, "code": "def list to str ( lst : list , content : str , indent : int = 1 ) : for i in lst : if isinstance ( i , indentable ) : content = i . to str ( content , indent ) elif isinstance ( i , list ) : content = list to str ( i , content , indent ) elif isinstance ( i , str ) : content = catend ( content , i , indent ) return content", "predictions": ["delivers a add list to a add list ."], "references": ["recurs into list for string computing"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5169, "code": "def populate from sequence ( seq : list , r : ref ( Edge ) , sr : state . State Register ) : base state = r idxlast = len ( seq ) - 1 idx = 0 for m in seq : if isinstance ( m , list ) : for item in m : populate from sequence ( item , r , sr ) elif isinstance ( m , Match Expr ) : e X = r ( ) . get next edge ( m ) if e X is None : s X = None if idx != idxlast : s X = state . State ( sr ) s X . match Default ( base state ( ) . s ) else : s X = base state ( ) . s e X = Edge ( s X ) r ( ) . next edge [ id ( s X ) ] = e X m . attach ( r ( ) . s , s X , sr ) r = ref ( e X ) idx += 1", "predictions": ["override this function to populate the list of tuples of this model from the model ."], "references": ["function that connect each other one sequence of matchexpr ."], "bleu": 0.09147827112247602, "rouge_l": 0.24078947368421053}
{"id": 5170, "code": "def from string ( bnf : str , entry = None , * optional inherit ) -> Grammar : inherit = [ Grammar ] + list ( optional inherit ) scope = { 'grammar' : bnf , 'entry' : entry } return build grammar ( tuple ( inherit ) , scope )", "predictions": ["convert a rule string to an internal grammar ."], "references": ["create a grammar from a string"], "bleu": 0.16784459625186196, "rouge_l": 0.27664399092970515}
{"id": 5171, "code": "def from file ( fn : str , entry = None , * optional inherit ) -> Grammar : import os . path if os . path . exists ( fn ) : f = open ( fn , 'r' ) bnf = f . read ( ) f . close ( ) inherit = [ Grammar ] + list ( optional inherit ) scope = { 'grammar' : bnf , 'entry' : entry , 'source' : fn } return build grammar ( tuple ( inherit ) , scope ) raise Exception ( \"File not Found!\" )", "predictions": ["copies an not to a working directory ."], "references": ["create a grammar from a file"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5172, "code": "def parse ( self , source : str = None , entry : str = None ) -> parsing . Node : self . from string = True if source is not None : self . parsed stream ( source ) if entry is None : entry = self . entry if entry is None : raise Value Error ( \"No entry rule name defined for {}\" . format ( self . class . name ) ) return self . do parse ( entry )", "predictions": ["parses an http accept-* [ jackson ] in the expression ."], "references": ["parse source using the grammar"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 5173, "code": "def parse file ( self , filename : str , entry : str = None ) -> parsing . Node : self . from string = False import os . path with open ( filename , 'r' ) as f : self . parsed stream ( f . read ( ) , os . path . abspath ( filename ) ) if entry is None : entry = self . entry if entry is None : raise Value Error ( \"No entry rule name defined for {}\" . format ( self . class . name ) ) return self . do parse ( entry )", "predictions": ["add a range of this class to the tree ."], "references": ["parse filename using the grammar"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 5174, "code": "def default serializer ( o ) : defs = ( ( ( datetime . date , datetime . time ) , lambda x : x . isoformat ( ) , ) , ( ( datetime . datetime , ) , lambda x : dt2utc timestamp ( x ) , ) , ) for types , fun in defs : if isinstance ( o , types ) : return fun ( o )", "predictions": ["generate a add or add a serializer structure to the specified serializer ."], "references": ["default serializer for json ."], "bleu": 0.10571070857151538, "rouge_l": 0.24158415841584158}
{"id": 5175, "code": "def dump ( deposition , from date , with json = True , latest only = False , * * kwargs ) : dep json = json . dumps ( deposition . getstate ( ) , default = default serializer ) dep dict = json . loads ( dep json ) dep dict [ ' p' ] = { } dep dict [ ' p' ] [ 'id' ] = deposition . id dep dict [ ' p' ] [ 'created' ] = dt2utc timestamp ( deposition . created ) dep dict [ ' p' ] [ 'modified' ] = dt2utc timestamp ( deposition . modified ) dep dict [ ' p' ] [ 'user id' ] = deposition . user id dep dict [ ' p' ] [ 'state' ] = deposition . state dep dict [ ' p' ] [ 'has sip' ] = deposition . has sip ( ) dep dict [ ' p' ] [ 'submitted' ] = deposition . submitted return dep dict", "predictions": ["dumps the key of the recipe to the given file ."], "references": ["dump the deposition object as dictionary ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 5176, "code": "def get recids invenio12 ( from date ) : from invenio . dbquery import run sql return ( id [ 0 ] for id in run sql ( 'select id bibrec from ' 'bibrec bibdoc as r join bibdoc as d on r.id bibdoc=d.id ' 'where d.modification date >=%s' , ( from date , ) , run on slave = True ) )", "predictions": ["get a bind ( w . value ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["get bibdocs for invenio 1 ."], "bleu": 0.026594139297659906, "rouge_l": 0.08321964529331514}
{"id": 5177, "code": "def get recids invenio2 ( from date ) : from invenio . legacy . dbquery import run sql return ( id [ 0 ] for id in run sql ( 'select id bibrec from ' 'bibrec bibdoc as r join bibdoc as d on r.id bibdoc=d.id ' 'where d.modification date >=%s' , ( from date , ) , run on slave = True ) )", "predictions": ["get a hook for the latest ( ."], "references": ["get bibdocs for invenio 2 ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 5178, "code": "def get check ( ) : try : from invenio . dbquery import run sql except Import Error : from invenio . legacy . dbquery import run sql return ( run sql ( 'select count(id) from bibdoc' , run on slave = True ) [ 0 ] [ 0 ] , [ id [ 0 ] for id in run sql ( 'select id from bibdoc' , run on slave = True ) ] , )", "predictions": ["param param execution and return a successful num ."], "references": ["get bibdocs to check ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 5179, "code": "def dump ( obj , from date , with json = True , latest only = False , * * kwargs ) : return dict ( id = obj . id , client id = obj . client id , user id = obj . user id , token type = obj . token type , access token = obj . access token , refresh token = obj . refresh token , expires = dt2iso or empty ( obj . expires ) , scopes = obj . scopes , is personal = obj . is personal , is internal = obj . is internal )", "predictions": ["serialize an object to a file ."], "references": ["dump the oauth2server tokens ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5180, "code": "def get ( * args , * * kwargs ) : try : from invenio . modules . accounts . models import User EXT except Import Error : from invenio accounts . models import User EXT q = User EXT . query return q . count ( ) , q . all ( )", "predictions": ["returns a user ' s user object with the given dependencies ."], "references": ["get userext objects ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 5181, "code": "def get modified recids invenio12 ( from date ) : from invenio . search engine import search pattern from invenio . dbquery import run sql return set ( ( id [ 0 ] for id in run sql ( 'select id from bibrec where modification date >= %s' , ( from date , ) , run on slave = True ) ) ) , search pattern", "predictions": ["return a id for this ) from the database ."], "references": ["get record ids for invenio 1 ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 5182, "code": "def get modified recids invenio2 ( from date ) : from invenio . legacy . search engine import search pattern from invenio . modules . records . models import Record date = datetime . datetime . strptime ( from date , '%Y-%m-%d %H:%M:%S' ) return set ( ( x [ 0 ] for x in Record . query . filter ( Record . modification date >= date ) . values ( Record . id ) ) ) , search pattern", "predictions": ["returns all name of the specified ) from the database ."], "references": ["get record ids for invenio 2 ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 5183, "code": "def get collection restrictions ( collection ) : try : from invenio . dbquery import run sql from invenio . access control firerole import compile role definition except Import Error : from invenio . modules . access . firerole import compile role definition from invenio . legacy . dbquery import run sql res = run sql ( 'SELECT r.firerole def src, email ' 'FROM acc ROLE as r ' 'JOIN acc ROLE acc ACTION acc ARGUMENT ON r.id=id acc ROLE ' 'JOIN acc ARGUMENT AS a ON a.id=id acc ARGUMENT ' 'JOIN user acc ROLE AS u ON r.id=u.id acc ROLE ' 'JOIN user ON user.id=u.id user ' 'WHERE a.keyword=\"collection\" AND ' 'a.value=%s AND ' 'id acc ACTION=(select id from acc ACTION where name=\"viewrestrcoll\")' , ( collection , ) , run on slave = True ) fireroles = set ( ) users = set ( ) for f , u in res : fireroles . add ( compile role definition ( f ) ) users . add ( u ) return { 'fireroles' : list ( fireroles ) , 'users' : users }", "predictions": ["return the param that determines the param from an param ."], "references": ["get all restrictions for a given collection users and fireroles ."], "bleu": 0.11390778025531027, "rouge_l": 0.09090909090909091}
{"id": 5184, "code": "def get record revisions ( recid , from date ) : try : from invenio . dbquery import run sql except Import Error : from invenio . legacy . dbquery import run sql return run sql ( 'SELECT job date, marcxml ' 'FROM hst RECORD WHERE id bibrec = %s AND job date >= %s ' 'ORDER BY job date ASC' , ( recid , from date ) , run on slave = True )", "predictions": ["get a record from the appropriate database ."], "references": ["get record revisions ."], "bleu": 0.19070828081828378, "rouge_l": 0.5319767441860466}
{"id": 5185, "code": "def get record collections ( recid ) : try : from invenio . search engine import ( get all collections of a record , get restricted collections for recid ) except Import Error : from invenio . legacy . search engine import ( get all collections of a record , get restricted collections for recid ) collections = { 'all' : get all collections of a record ( recid , recreate cache if needed = False ) , } collections [ 'restricted' ] = dict ( ( coll , get collection restrictions ( coll ) ) for coll in get restricted collections for recid ( recid , recreate cache if needed = False ) ) return collections", "predictions": ["returns a list of all collections that have been restricted to the database ."], "references": ["get all collections the record belong to ."], "bleu": 0.13834368456410945, "rouge_l": 0.3824451410658307}
{"id": 5186, "code": "def dump record json ( marcxml ) : try : from invenio . modules . records . api import Record d = Record . create ( marcxml , 'marc' ) return d . dumps ( clean = True ) except Import Error : from invenio . bibfield import create record d = create record ( marcxml , master format = 'marc' ) return d . dumps ( )", "predictions": ["dump all the blobs of the blobs into a string ."], "references": ["dump json of record ."], "bleu": 0.1354599427337814, "rouge_l": 0.40219780219780216}
{"id": 5187, "code": "def get ( query , from date , * * kwargs ) : recids , search pattern = get modified recids ( from date ) recids = recids . union ( get modified bibdoc recids ( from date ) ) if query : recids = recids . intersection ( set ( search pattern ( p = query . encode ( 'utf-8' ) ) ) ) return len ( recids ) , recids", "predictions": ["get a query object from the database ."], "references": ["get recids matching query and with changes ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 5188, "code": "def load common ( model cls , data ) : obj = model cls ( * * data ) db . session . add ( obj ) db . session . commit ( )", "predictions": ["used to load the internal state ."], "references": ["helper function for loading json data verbatim into model ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5189, "code": "def collect things entry points ( ) : things = dict ( ) for entry point in iter entry points ( group = 'invenio migrator.things' ) : things [ entry point . name ] = entry point . load ( ) return things", "predictions": ["produce all ( of the entire points ."], "references": ["collect entry points ."], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 5190, "code": "def init app context ( ) : try : from invenio . base . factory import create app app = create app ( ) app . test request context ( '/' ) . push ( ) app . preprocess request ( ) except Import Error : pass", "predictions": ["initializes and builds the initial initial state of the given test ."], "references": ["initialize app context for invenio 2 . x ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 5191, "code": "def memoize ( func ) : cache = { } @ wraps ( func ) def wrap ( * args , * * kwargs ) : key = '{0}{1}' . format ( args , kwargs ) if key not in cache : cache [ key ] = func ( * args , * * kwargs ) return cache [ key ] return wrap", "predictions": ["memoize a function with a specific key ."], "references": ["cache for heavy function calls ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 5192, "code": "def get connected roles ( action id ) : try : from invenio . access control admin import compile role definition except Import Error : from invenio . modules . access . firerole import compile role definition run sql = get run sql ( ) roles = { } res = run sql ( 'select r.id, r.name, r.description, r.firerole def src, ' 'a.keyword, a.value, email from acc ROLE as r ' 'join acc ROLE acc ACTION acc ARGUMENT on r.id=id acc ROLE ' 'join acc ARGUMENT as a on  a.id=id acc ARGUMENT ' 'join user acc ROLE as u on r.id=u.id acc ROLE ' 'join user on user.id=u.id user ' 'where id acc ACTION=%s' , ( action id , ) ) for r in res : role = roles . setdefault ( r [ 0 ] , { 'id' : r [ 0 ] , 'name' : r [ 1 ] , 'description' : r [ 2 ] , 'firerole def' : r [ 3 ] , 'compiled firerole def' : compile role definition ( r [ 3 ] ) , 'users' : set ( ) , 'parameters' : { } } ) param = role [ 'parameters' ] . setdefault ( r [ 4 ] , set ( ) ) param . add ( r [ 5 ] ) role [ 'users' ] . add ( r [ 6 ] ) return six . itervalues ( roles )", "predictions": ["extracts connected roles and role from a user ' s parent ."], "references": ["get roles connected to an action ."], "bleu": 0.1235622127262679, "rouge_l": 0.22101449275362317}
{"id": 5193, "code": "def get ( query , * args , * * kwargs ) : run sql = get run sql ( ) actions = [ dict ( id = row [ 0 ] , name = row [ 1 ] , allowedkeywords = row [ 2 ] , optional = row [ 3 ] ) for action in query . split ( ',' ) for row in run sql ( 'select id, name, description, allowedkeywords, optional ' 'from acc ACTION where name like %s' , ( action , ) , run on slave = True ) ] return len ( actions ) , actions", "predictions": ["returns a list of the entity names that can be imported from query or with this query ."], "references": ["get action definitions to dump ."], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 5194, "code": "def load token ( data ) : from invenio oauth2server . models import Token data [ 'expires' ] = iso2dt or none ( data [ 'expires' ] ) load common ( Token , data )", "predictions": ["load the token from the data ."], "references": ["load the oauth2server token from data dump ."], "bleu": 0.2898580955281284, "rouge_l": 0.7904967602591793}
{"id": 5195, "code": "def config imp or default ( app , config var imp , default ) : imp = app . config . get ( config var imp ) return import string ( imp ) if imp else default", "predictions": ["return the configuration of this configuration ."], "references": ["import config var import path or use default value ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5196, "code": "def init app ( self , app ) : self . init config ( app . config ) state = Invenio Migrator State ( app ) app . extensions [ 'invenio-migrator' ] = state app . cli . add command ( dumps ) return state", "predictions": ["initialize the module object ."], "references": ["flask application initialization ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 5197, "code": "def dump ( obj , from date , with json = True , latest only = False , * * kwargs ) : return dict ( name = obj . name , description = obj . description , website = obj . website , user id = obj . user id , client id = obj . client id , client secret = obj . client secret , is confidential = obj . is confidential , is internal = obj . is internal , redirect uris = obj . redirect uris , default scopes = obj . default scopes )", "predictions": ["dump an object to a file in the database ."], "references": ["dump the oauth2server client ."], "bleu": 0.14991106946711685, "rouge_l": 0.42558139534883715}
{"id": 5198, "code": "def get users invenio12 ( * args , * * kwargs ) : from invenio . dbquery import run sql , deserialize via marshal User = namedtuple ( 'User' , [ 'id' , 'email' , 'password' , 'password salt' , 'note' , 'full name' , 'settings' , 'nickname' , 'last login' ] ) users = run sql ( 'SELECT id, email, password, note, settings, nickname, last login' ' FROM user' , run on slave = True ) return len ( users ) , [ User ( id = user [ 0 ] , email = user [ 1 ] , password = user [ 2 ] . decode ( 'latin1' ) , password salt = user [ 1 ] , note = user [ 3 ] , full name = user [ 5 ] , settings = deserialize via marshal ( user [ 4 ] ) if user [ 4 ] else { } , nickname = 'id {0}' . format ( user [ 0 ] ) , last login = user [ 6 ] ) for user in users ]", "predictions": ["get the users list from the users profile ."], "references": ["get user accounts invenio 1 ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 5199, "code": "def get users invenio2 ( * args , * * kwargs ) : from invenio . modules . accounts . models import User q = User . query return q . count ( ) , q . all ( )", "predictions": ["gets a user list by the user ' s modules ."], "references": ["get user accounts from invenio 2 ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 5200, "code": "def loadrecords ( sources , source type , recid ) : if recid is not None : for source in sources : records = json . load ( source ) for item in records : if str ( item [ 'recid' ] ) == str ( recid ) : loadrecord ( item , source type , eager = True ) click . echo ( \"Record '{recid}' loaded.\" . format ( recid = recid ) ) return click . echo ( \"Record '{recid}' not found.\" . format ( recid = recid ) ) else : for idx , source in enumerate ( sources , 1 ) : click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) data = json . load ( source ) with click . progressbar ( data ) as records : for item in records : loadrecord ( item , source type )", "predictions": ["load all the command instances from all command instances ."], "references": ["load records migration dump ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 5201, "code": "def inspectrecords ( sources , recid , entity = None ) : for idx , source in enumerate ( sources , 1 ) : click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) data = json . load ( source ) if not recid : click . secho ( 'Record identifiers' , fg = 'green' ) total = 0 for r in ( d [ 'recid' ] for d in data ) : click . echo ( r ) total += 1 click . echo ( '{0} records found in dump.' . format ( total ) ) return data = list ( filter ( lambda d : d [ 'recid' ] == recid , data ) ) if not data : click . secho ( \"Record not found.\" , fg = 'yellow' ) return for record in data : if entity is None : click . echo ( json . dumps ( record , indent = 2 ) ) if entity == 'files' : click . secho ( 'Files' , fg = 'green' ) click . echo ( json . dumps ( record [ 'files' ] , indent = 2 ) ) if entity == 'json' : click . secho ( 'Records (JSON)' , fg = 'green' ) for revision in record [ 'record' ] : click . secho ( 'Revision {0}' . format ( revision [ 'modification datetime' ] ) , fg = 'yellow' ) click . echo ( json . dumps ( revision [ 'json' ] , indent = 2 ) ) if entity == 'marcxml' : click . secho ( 'Records (MARCXML)' , fg = 'green' ) for revision in record [ 'record' ] : click . secho ( 'Revision {0}' . format ( revision [ 'marcxml' ] ) , fg = 'yellow' ) click . echo ( revision )", "predictions": ["write all the input combinations to all logs ."], "references": ["inspect records in a migration dump ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5202, "code": "def main ( port = 8888 ) : import tornado . ioloop routes = [ ] + Tornado Profiler ( ) . get routes ( ) app = tornado . web . Application ( routes ) app . listen ( port ) tornado . ioloop . IO Loop . current ( ) . start ( )", "predictions": ["creates an instance of the routes ."], "references": ["run as sample test server ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 5203, "code": "def get ( self ) : sort = self . get argument ( 'sort' , 'cum time' ) count = self . get argument ( 'count' , 20 ) strip dirs = self . get argument ( 'strip dirs' , True ) error = '' sorts = ( 'num calls' , 'cum time' , 'total time' , 'cum time per call' , 'total time per call' ) if sort not in sorts : error += \"Invalid `sort` '%s', must be in %s.\" % ( sort , sorts ) try : count = int ( count ) except ( Value Error , Type Error ) : error += \"Can't cast `count` '%s' to int.\" % count if count <= 0 : count = None strip dirs = str ( strip dirs ) . lower ( ) not in ( 'false' , 'no' , 'none' , 'null' , '0' , '' ) if error : self . write ( { 'error' : error } ) self . set status ( 400 ) self . finish ( ) return try : statistics = get profiler statistics ( sort , count , strip dirs ) self . write ( { 'statistics' : statistics } ) self . set status ( 200 ) except Type Error : logger . exception ( 'Error while retrieving profiler statistics' ) self . write ( { 'error' : 'No stats available. Start and stop the profiler before trying to retrieve stats.' } ) self . set status ( 404 ) self . finish ( )", "predictions": ["for each thread that has been processed by the given sort method ."], "references": ["return current profiler statistics ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 5204, "code": "def post ( self ) : if is profiler running ( ) : self . set status ( 201 ) self . finish ( ) return start profiling ( ) self . set status ( 201 ) self . finish ( )", "predictions": ["prevent prevent prevent prevent the output ."], "references": ["start a new profiler ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5205, "code": "def post ( self ) : filename = self . get argument ( 'filename' , 'dump.prof' ) C Profile Wrapper . profiler . dump stats ( filename ) self . finish ( )", "predictions": ["submits a profile to the daemon ."], "references": ["dump current profiler statistics into a file ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5206, "code": "def get ( self ) : C Profile Wrapper . profiler . print stats ( ) s = String IO . String IO ( ) sortby = 'cumulative' ps = pstats . Stats ( C Profile Wrapper . profiler , stream = s ) . sort stats ( sortby ) ps . print stats ( ) self . set status ( 200 ) self . write ( s . getvalue ( ) ) self . finish ( )", "predictions": ["this method returns stats for a class ."], "references": ["return current profiler statistics ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5207, "code": "def delete ( self ) : C Profile Wrapper . profiler . create stats ( ) self . enable ( ) self . set status ( 204 ) self . finish ( )", "predictions": ["dynamic delete of this class ."], "references": ["clear profiler statistics ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 5208, "code": "def post ( self ) : C Profile Wrapper . profiler = c Profile . Profile ( ) C Profile Wrapper . profiler . enable ( ) self . running = True self . set status ( 201 ) self . finish ( )", "predictions": ["creates a new instance of this class ."], "references": ["start a new profiler ."], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 5209, "code": "def delete ( self ) : C Profile Wrapper . profiler . disable ( ) self . running = False self . set status ( 204 ) self . finish ( )", "predictions": ["deletes the atomic object ."], "references": ["stop the profiler ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 5210, "code": "def get ( self ) : self . write ( { \"running\" : self . running } ) self . set status ( 200 ) self . finish ( )", "predictions": ["a method to get the 200 of the underlying layout ."], "references": ["check if the profiler is running ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 5211, "code": "def disable timestamp ( method ) : @ wraps ( method ) def wrapper ( * args , * * kwargs ) : result = None with correct date ( ) : result = method ( * args , * * kwargs ) return result return wrapper", "predictions": ["disable the timestamp . < p > note : this method calls system . commons . com / 2011 / projectreactor / 05 / oldstyleplists - , - , - , - , - , - , - , - , - , - , - , - , -"], "references": ["disable timestamp update per method ."], "bleu": 0.030216776104535565, "rouge_l": 0.1664392905866303}
{"id": 5212, "code": "def add ones dim ( arr ) : arr = arr [ ... , np . newaxis ] return np . concatenate ( ( arr , np . ones like ( arr ) ) , axis = - 1 )", "predictions": ["convenience method to add an array of coordinates to this node ."], "references": ["adds a dimensions with ones to array ."], "bleu": 0.1235622127262679, "rouge_l": 0.3112244897959184}
{"id": 5213, "code": "def create ( cls , dump ) : if not dump . data . get ( 'record' ) : try : Persistent Identifier . get ( pid type = 'recid' , pid value = dump . recid ) except PID Does Not Exist Error : Persistent Identifier . create ( 'recid' , dump . recid , status = PID Status . RESERVED ) db . session . commit ( ) return None dump . prepare revisions ( ) dump . prepare pids ( ) dump . prepare files ( ) existing files = [ ] if dump . record : existing files = dump . record . get ( ' files' , [ ] ) record = cls . update record ( revisions = dump . revisions , created = dump . created , record = dump . record ) pids = dump . missing pids else : record = cls . create record ( dump ) pids = dump . pids if pids : cls . create pids ( record . id , pids ) if dump . files : cls . create files ( record , dump . files , existing files ) if dump . is deleted ( record ) : cls . delete record ( record ) return record", "predictions": ["creates a record of a folder ."], "references": ["create record based on dump ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5214, "code": "def create record ( cls , dump ) : timestamp , data = dump . latest record = Record . create ( data ) record . model . created = dump . created . replace ( tzinfo = None ) record . model . updated = timestamp . replace ( tzinfo = None ) Record Identifier . insert ( dump . recid ) Persistent Identifier . create ( pid type = 'recid' , pid value = str ( dump . recid ) , object type = 'rec' , object uuid = str ( record . id ) , status = PID Status . REGISTERED ) db . session . commit ( ) return cls . update record ( revisions = dump . rest , record = record , created = dump . created )", "predictions": ["write an object to the database"], "references": ["create a new record from dump ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5215, "code": "def update record ( cls , revisions , created , record ) : for timestamp , revision in revisions : record . model . json = revision record . model . created = created . replace ( tzinfo = None ) record . model . updated = timestamp . replace ( tzinfo = None ) db . session . commit ( ) return Record ( record . model . json , model = record . model )", "predictions": ["update the record information ."], "references": ["update an existing record ."], "bleu": 0.32466791547509893, "rouge_l": 0.6}
{"id": 5216, "code": "def create pids ( cls , record uuid , pids ) : for p in pids : Persistent Identifier . create ( pid type = p . pid type , pid value = p . pid value , pid provider = p . provider . pid provider if p . provider else None , object type = 'rec' , object uuid = record uuid , status = PID Status . REGISTERED , ) db . session . commit ( )", "predictions": ["creates a new instance of . from an job ."], "references": ["create persistent identifiers ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 5217, "code": "def delete record ( cls , record ) : record . delete ( ) Persistent Identifier . query . filter by ( object type = 'rec' , object uuid = record . id , ) . update ( { Persistent Identifier . status : PID Status . DELETED } ) cls . delete buckets ( record ) db . session . commit ( )", "predictions": ["deletes all the object including the given import and record ."], "references": ["delete a record and it s persistent identifiers ."], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 5218, "code": "def create file ( self , bucket , key , file versions ) : objs = [ ] for file ver in file versions : f = File Instance . create ( ) . set uri ( file ver [ 'full path' ] , file ver [ 'size' ] , 'md5:{0}' . format ( file ver [ 'checksum' ] ) , ) obj = Object Version . create ( bucket , key ) . set file ( f ) obj . created = arrow . get ( file ver [ 'creation date' ] ) . datetime . replace ( tzinfo = None ) objs . append ( obj ) db . session . commit ( ) return objs [ - 1 ]", "predictions": ["dump a record . this method will catch all the files associated with the given read and print them as a file ."], "references": ["create a single file with all versions ."], "bleu": 0.06964541799727335, "rouge_l": 0.21205098493626884}
{"id": 5219, "code": "def delete buckets ( cls , record ) : files = record . get ( ' files' , [ ] ) buckets = set ( ) for f in files : buckets . add ( f . get ( 'bucket' ) ) for b id in buckets : b = Bucket . get ( b id ) b . deleted = True", "predictions": ["deletes all instances of a group ."], "references": ["delete the bucket ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 5220, "code": "def missing pids ( self ) : missing = [ ] for p in self . pids : try : Persistent Identifier . get ( p . pid type , p . pid value ) except PID Does Not Exist Error : missing . append ( p ) return missing", "predictions": ["the method that measures the pid of this list of load but does not change the pid of the process ."], "references": ["filter persistent identifiers ."], "bleu": 0.05809665204409193, "rouge_l": 0.09118086696562032}
{"id": 5221, "code": "def prepare files ( self ) : files = { } for f in self . data [ 'files' ] : k = f [ 'full name' ] if k not in files : files [ k ] = [ ] files [ k ] . append ( f ) for k in files . keys ( ) : files [ k ] . sort ( key = lambda x : x [ 'version' ] ) self . files = files", "predictions": ["collect things configured in the specified list of things ."], "references": ["get files from data dump ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 5222, "code": "def prepare pids ( self ) : self . pids = [ ] for fetcher in self . pid fetchers : val = fetcher ( None , self . revisions [ - 1 ] [ 1 ] ) if val : self . pids . append ( val )", "predictions": ["create multiple push slices ."], "references": ["prepare persistent identifiers ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 5223, "code": "def is deleted ( self , record = None ) : record = record or self . revisions [ - 1 ] [ 1 ] return any ( col == 'deleted' for col in record . get ( 'collections' , [ ] ) )", "predictions": ["we have already ( } } } } } } } } } so we can later ( i . e . , the method is not a set of things } } }"], "references": ["check if record is deleted ."], "bleu": 0.039307696466998686, "rouge_l": 0.05722326454033771}
{"id": 5224, "code": "def dump ( thing , query , from date , file prefix , chunk size , limit , thing flags ) : init app context ( ) file prefix = file prefix if file prefix else '{0} dump' . format ( thing ) kwargs = dict ( ( f . strip ( '-' ) . replace ( '-' , ' ' ) , True ) for f in thing flags ) try : thing func = collect things entry points ( ) [ thing ] except Key Error : click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect things entry points ( ) ) ) click . echo ( \"Querying {0}...\" . format ( thing ) ) count , items = thing func . get ( query , from date , limit = limit , * * kwargs ) progress i = 0 click . echo ( \"Dumping {0}...\" . format ( thing ) ) with click . progressbar ( length = count ) as bar : for i , chunk ids in enumerate ( grouper ( items , chunk size ) ) : with open ( '{0} {1}.json' . format ( file prefix , i ) , 'w' ) as fp : fp . write ( \"[\\n\" ) for id in chunk ids : try : json . dump ( thing func . dump ( id , from date , * * kwargs ) , fp , default = set serializer ) fp . write ( \",\" ) except Exception as e : click . secho ( \"Failed dump {0} {1} ({2})\" . format ( thing , id , e . message ) , fg = 'red' ) progress i += 1 bar . update ( progress i ) fp . seek ( fp . tell ( ) - 1 ) fp . write ( \"\\n]\" )", "predictions": ["get the [ roles ] from the action ."], "references": ["dump data from invenio legacy ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 5225, "code": "def check ( thing ) : init app context ( ) try : thing func = collect things entry points ( ) [ thing ] except Key Error : click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect things entry points ( ) ) ) click . echo ( \"Querying {0}...\" . format ( thing ) ) count , items = thing func . get check ( ) i = 0 click . echo ( \"Checking {0}...\" . format ( thing ) ) with click . progressbar ( length = count ) as bar : for id in items : thing func . check ( id ) i += 1 bar . update ( i )", "predictions": ["checks if all cluster conditions are in the order of all the components of the set ."], "references": ["check data in invenio legacy ."], "bleu": 0.07994607499472013, "rouge_l": 0.19032761310452417}
{"id": 5226, "code": "def write reports ( self , relative path , suite name , reports , package name = None ) : dest path = self . reserve file ( relative path ) with open ( dest path , 'wb' ) as outf : outf . write ( toxml ( reports , suite name , package name = package name ) ) return dest path", "predictions": ["writes a token to a token in this directory ."], "references": ["write the collection of reports to the given path"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 5227, "code": "def toxml ( test reports , suite name , hostname = gethostname ( ) , package name = \"tests\" ) : testsuites = et . Element ( \"testsuites\" ) testsuite = et . Sub Element ( testsuites , \"testsuite\" ) test count = len ( test reports ) if test count < 1 : raise Value Error ( 'there must be at least one test report' ) assert test count > 0 , 'expecting at least one test' error count = len ( [ r for r in test reports if r . errors ] ) failure count = len ( [ r for r in test reports if r . failures ] ) ts = test reports [ 0 ] . start ts start timestamp = datetime . fromtimestamp ( ts ) . isoformat ( ) total duration = test reports [ - 1 ] . end ts - test reports [ 0 ] . start ts def quote attribute ( value ) : return value if value is not None else \"(null)\" testsuite . attrib = dict ( id = \"0\" , errors = str ( error count ) , failures = str ( failure count ) , tests = str ( test count ) , hostname = quote attribute ( hostname ) , timestamp = quote attribute ( start timestamp ) , time = \"%f\" % total duration , name = quote attribute ( suite name ) , package = quote attribute ( package name ) , ) for r in test reports : test name = r . name test duration = r . end ts - r . start ts class name = r . src location testcase = et . Sub Element ( testsuite , \"testcase\" ) testcase . attrib = dict ( name = test name , classname = quote attribute ( class name ) , time = \"%f\" % test duration , ) if r . errors or r . failures : if r . failures : failure = et . Sub Element ( testcase , \"failure\" ) failure . attrib = dict ( type = \"exception\" , message = quote attribute ( '\\n' . join ( [ '%s' % e for e in r . failures ] ) ) , ) else : error = et . Sub Element ( testcase , \"error\" ) error . attrib = dict ( type = \"exception\" , message = quote attribute ( '\\n' . join ( [ '%s' % e for e in r . errors ] ) ) , ) return et . tostring ( testsuites , encoding = \"utf-8\" )", "predictions": ["config must be a failure or a failure ."], "references": ["convert test reports into an xml file"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5228, "code": "def add Menu ( self , menu ) : self . menus [ menu . name ] = menu self . peng . send Event ( \"peng3d:window.menu.add\" , { \"peng\" : self . peng , \"window\" : self , \"menu\" : menu } )", "predictions": ["init method for derived classes ."], "references": ["adds a menu to the list of menus ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 5229, "code": "def redraw label ( self ) : sx , sy = self . size x , y = self . pos self . label . anchor x = \"left\" self . label . x = x + sx / 2. + sx self . label . y = y + sy / 2. + sy * .15 self . label . update ( )", "predictions": ["the method for vpf ."], "references": ["re - calculates the position of the label ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 5230, "code": "def render3d ( self , view = None ) : super ( Static World , self ) . render3d ( view ) self . batch3d . draw ( )", "predictions": ["creates and draws the entity ."], "references": ["renders the world ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 5231, "code": "def on redraw ( self ) : x , y = self . pos sx , sy = self . size self . bg vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] self . stencil vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] if isinstance ( self . bg , Background ) : if not self . bg . initialized : self . bg . init bg ( ) self . bg . initialized = True self . bg . redraw bg ( )", "predictions": ["called to configure the addon vertices ."], "references": ["redraws the background and any child widgets ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5232, "code": "def do Action ( self , action ) : if not hasattr ( self , \"actions\" ) : return for f , args , kwargs in self . actions . get ( action , [ ] ) : f ( * args , * * kwargs )", "predictions": ["returns a copy of this action with the specified arguments ."], "references": ["helper method that calls all callbacks registered for the given action ."], "bleu": 0.12368857073777001, "rouge_l": 0.17256011315417258}
{"id": 5233, "code": "def get Size ( self ) : return self . widget . size [ 0 ] - self . border [ 0 ] * 2 , self . widget . size [ 1 ] - self . border [ 1 ] * 2", "predictions": ["get the current state of this cipher ."], "references": ["returns the size of the layer with the border size already subtracted ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 5234, "code": "def make conn ( shape ) : shape = np . array ( shape ) Ne = shape . prod ( ) if len ( shape ) == 2 : nx , ny = np . array ( shape ) + 1 conn = np . zeros ( ( Ne , 4 ) , dtype = np . int32 ) counter = 0 pattern = np . array ( [ 0 , 1 , 1 + nx , nx ] ) for j in range ( shape [ 1 ] ) : for i in range ( shape [ 0 ] ) : conn [ counter ] = pattern + 1 + i + j * nx counter += 1 if len ( shape ) == 3 : nx , ny , nz = np . array ( shape ) + 1 conn = np . zeros ( ( Ne , 8 ) , dtype = np . int32 ) counter = 0 pattern = np . array ( [ 0 , 1 , 1 + nx , nx , nx * ny , 1 + nx * ny , 1 + ( nx + 1 ) * ny , ( nx + 1 ) * ny ] ) for k in range ( shape [ 2 ] ) : for j in range ( shape [ 1 ] ) : for i in range ( shape [ 0 ] ) : conn [ counter ] = pattern + 1 + i + j * nx + k * nx * ny counter += 1 return conn", "predictions": ["main helper method to main loop"], "references": ["connectivity builder using numba for speed boost ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 5235, "code": "def set fields ( self , fields = None , * * kwargs ) : self . fields = [ ] if fields != None : for field in fields : self . fields . append ( field )", "predictions": ["sets the ( or optionally ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ."], "references": ["sets the fields ."], "bleu": 0.03867468300268994, "rouge_l": 0.14710610932475884}
{"id": 5236, "code": "def add fields ( self , fields = None , * * kwargs ) : if fields != None : for field in fields : self . fields . append ( field )", "predictions": ["adds a key - value mapping ."], "references": ["add the fields into the list of fields ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5237, "code": "def check elements ( self ) : existing types = set ( self . elements . type . argiope . values . flatten ( ) ) allowed types = set ( ELEMENTS . keys ( ) ) if ( existing types <= allowed types ) == False : raise Value Error ( \"Element types {0} not in know elements {1}\" . format ( existing types - allowed types , allowed types ) ) print ( \"<Elements: OK>\" )", "predictions": ["each of the = ( - ( has a single value ."], "references": ["checks element definitions ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 5238, "code": "def space ( self ) : return self . elements . type . argiope . map ( lambda t : ELEMENTS [ t ] . space )", "predictions": ["public method to get the get method of this class ."], "references": ["returns the dimension of the embedded space of each element ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 5239, "code": "def centroids and volumes ( self , sort index = True ) : elements = self . elements out = [ ] for etype , group in self . elements . groupby ( [ ( \"type\" , \"argiope\" , \"\" ) ] ) : etype info = ELEMENTS [ etype ] simplices info = etype info . simplices index = group . index simplices data = self . split ( into = \"simplices\" , loc = index , at = \"coords\" ) simplices = simplices data . values . reshape ( index . size , simplices info . shape [ 0 ] , simplices info . shape [ 1 ] , 3 ) edges = simplices [ : , : , 1 : ] - simplices [ : , : , : 1 ] simplices centroids = simplices . mean ( axis = 2 ) if etype info . space == 2 : simplices volumes = np . linalg . norm ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) , axis = 2 ) / 2. elif etype info . space == 3 : simplices volumes = ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) * edges [ : , : , 2 ] ) . sum ( axis = 2 ) / 6. elements volumes = simplices volumes . sum ( axis = 1 ) elements centroids = ( ( simplices volumes . reshape ( * simplices volumes . shape , 1 ) * simplices centroids ) . sum ( axis = 1 ) / elements volumes . reshape ( * elements volumes . shape , 1 ) ) volumes df = pd . Data Frame ( index = index , data = elements volumes , columns = pd . Multi Index . from product ( [ [ \"volume\" ] , [ \"\" ] ] ) ) centroids df = pd . Data Frame ( index = index , data = elements centroids , columns = pd . Multi Index . from product ( [ [ \"centroid\" ] , [ \"x\" , \"y\" , \"z\" ] ] ) ) out . append ( pd . concat ( [ volumes df , centroids df ] , axis = 1 ) ) out = pd . concat ( out ) if sort index : out . sort index ( inplace = True ) return out . sort index ( axis = 1 )", "predictions": ["extracts all delete self elements from this entity ."], "references": ["returns a dataframe containing volume and centroids of all the elements ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 5240, "code": "def edges ( self , zfill = 3 ) : edges = self . split ( \"edges\" , at = \"coords\" ) . unstack ( ) edges [ \"lx\" ] = edges . x [ 1 ] - edges . x [ 0 ] edges [ \"ly\" ] = edges . y [ 1 ] - edges . y [ 0 ] edges [ \"lz\" ] = edges . z [ 1 ] - edges . z [ 0 ] edges [ \"l\" ] = np . linalg . norm ( edges [ [ \"lx\" , \"ly\" , \"lz\" ] ] , axis = 1 ) edges = ( edges . l ) . unstack ( ) edges . columns = pd . Multi Index . from product ( [ [ \"length\" ] , [ \"e\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in np . arange ( edges . shape [ 1 ] ) ] ] ) edges [ ( \"stats\" , \"lmax\" ) ] = edges . length . max ( axis = 1 ) edges [ ( \"stats\" , \"lmin\" ) ] = edges . length . min ( axis = 1 ) edges [ ( \"stats\" , \"aspect ratio\" ) ] = edges . stats . lmax / edges . stats . lmin return edges . sort index ( axis = 1 )", "predictions": ["returns post loop . this doesn ' t include the post profiler ."], "references": ["returns the aspect ratio of all elements ."], "bleu": 0.1135935489027116, "rouge_l": 0.2985318107667211}
{"id": 5241, "code": "def stats ( self ) : cv = self . centroids and volumes ( ) angles = self . angles ( ) edges = self . edges ( ) return pd . concat ( [ cv , angles [ [ \"stats\" ] ] , edges [ [ \"stats\" ] ] ] , axis = 1 ) . sort index ( axis = 1 )", "predictions": ["convenience method for making tests that require content of this object ."], "references": ["returns mesh quality and geometric stats ."], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 5242, "code": "def element set to node set ( self , tag ) : nodes , elements = self . nodes , self . elements loc = ( elements . conn [ elements [ ( \"sets\" , tag , \"\" ) ] ] . stack ( ) . stack ( ) . unique ( ) ) loc = loc [ loc != 0 ] nodes [ ( \"sets\" , tag ) ] = False nodes . loc [ loc , ( \"sets\" , tag ) ] = True", "predictions": ["create a . from this list ."], "references": ["makes a node set from an element set ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 5243, "code": "def node set to surface ( self , tag ) : nodes = self . nodes . copy ( ) dummy = nodes . iloc [ 0 ] . copy ( ) dummy [ \"coords\" ] *= np . nan dummy [ \"sets\" ] = True nodes . loc [ 0 ] = dummy element surfaces = self . split ( \"surfaces\" ) . unstack ( ) surf = pd . Data Frame ( nodes . sets [ tag ] . loc [ element surfaces . values . flatten ( ) ] . values . reshape ( element surfaces . shape ) . prod ( axis = 1 ) . astype ( np . bool ) , index = element surfaces . index ) . unstack ( ) . fillna ( False ) for k in surf . keys ( ) : self . elements [ \"surfaces\" , tag , \"f{0}\" . format ( k [ 1 ] + 1 ) ] = surf . loc [ : , k ]", "predictions": ["creates a method that transforms this disable disable data ."], "references": ["converts a node set to surface ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 5244, "code": "def surface to element sets ( self , tag ) : surface = self . elements . surfaces [ tag ] for findex in surface . keys ( ) : if surface [ findex ] . sum ( ) != 0 : self . elements [ ( \"sets\" , \" SURF {0} FACE{1}\" . format ( tag , findex [ 1 : ] ) , \"\" ) ] = surface [ findex ]", "predictions": ["create a add object from this dim ."], "references": ["creates elements sets corresponding to a surface ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 5245, "code": "def fields metadata ( self ) : return ( pd . concat ( [ f . metadata ( ) for f in self . fields ] , axis = 1 ) . transpose ( ) . sort values ( [ \"step num\" , \"frame\" , \"label\" , \"position\" ] ) )", "predictions": ["( ( . copyof ."], "references": ["returns fields metadata as a dataframe ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 5246, "code": "def metadata ( self ) : return pd . Series ( { \"part\" : self . part , \"step num\" : self . step num , \"step label\" : self . step label , \"frame\" : self . frame , \"frame value\" : self . frame value , \"label\" : self . label , \"position\" : self . position , } )", "predictions": ["get the create create a create layout for this instance ."], "references": ["returns metadata as a dataframe ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 5247, "code": "def make directories ( self ) : if os . path . isdir ( self . workdir ) == False : os . mkdir ( self . workdir )", "predictions": ["makes a new ( object ."], "references": ["checks if required directories exist and creates them if needed ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 5248, "code": "def run postproc ( self ) : t0 = time . time ( ) if self . verbose : print ( '####\u00a0POST-PROCESSING \"{0}\" USING POST-PROCESSOR \"{1}\"'. f ormat( s elf. l abel,   self . solver . upper ( ) ) ) if self . solver == \"abaqus\" : command = '{0} viewer no GUI={1} abqpp.py' . format ( self . solver path , self . label ) process = subprocess . Popen ( command , cwd = self . workdir , shell = True , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) for line in iter ( process . stdout . readline , b'' ) : line = line . rstrip ( ) . decode ( 'utf8' ) print ( \"    \" , line ) t1 = time . time ( ) if self . verbose : print ( '  => POST-PROCESSED {0}: DURATION = {1:.2f}s >' . format ( self . label , t1 - t0 ) )", "predictions": ["this is run before the ( method ."], "references": ["runs the post - proc script ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 5249, "code": "def run gmsh ( self ) : argiope . utils . run gmsh ( gmsh path = self . gmsh path , gmsh space = self . gmsh space , gmsh options = self . gmsh options , name = self . file name + \".geo\" , workdir = self . workdir ) self . mesh = argiope . mesh . read msh ( self . workdir + self . file name + \".msh\" )", "predictions": ["run the entire file on the manifest ."], "references": ["makes the mesh using gmsh ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 5250, "code": "def read history report ( path , steps , x name = None ) : data = pd . read csv ( path , delim whitespace = True ) if x name != None : data [ x name ] = data . X del data [ \"X\" ] data [ \"step\" ] = 0 t = 0. for i in range ( len ( steps ) ) : dt = steps [ i ] . duration loc = data [ data . t == t ] . index if len ( loc ) == 2 : data . loc [ loc [ 1 ] : , \"step\" ] = i t += dt return data", "predictions": ["reads the history of the dataset ."], "references": ["reads an history output report ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 5251, "code": "def read field report ( path , data flag = \"*DATA\" , meta data flag = \"*METADATA\" ) : text = open ( path ) . read ( ) mdpos = text . find ( meta data flag ) dpos = text . find ( data flag ) mdata = io . String IO ( \"\\n\" . join ( text [ mdpos : dpos ] . split ( \"\\n\" ) [ 1 : ] ) ) data = io . String IO ( \"\\n\" . join ( text [ dpos : ] . split ( \"\\n\" ) [ 1 : ] ) ) data = pd . read csv ( data , index col = 0 ) data = data . groupby ( data . index ) . mean ( ) mdata = pd . read csv ( mdata , sep = \"=\" , header = None , index col = 0 ) [ 1 ] mdata = mdata . to dict ( ) out = { } out [ \"step num\" ] = int ( mdata [ \"step num\" ] ) out [ \"step label\" ] = mdata [ \"step label\" ] out [ \"frame\" ] = int ( mdata [ \"frame\" ] ) out [ \"frame value\" ] = float ( mdata [ \"frame value\" ] ) out [ \"part\" ] = mdata [ \"instance\" ] position map = { \"NODAL\" : \"node\" , \"ELEMENT CENTROID\" : \"element\" , \"WHOLE ELEMENT\" : \"element\" } out [ \"position\" ] = position map [ mdata [ \"position\" ] ] out [ \"label\" ] = mdata [ \"label\" ] out [ \"data\" ] = data field class = getattr ( argiope . mesh , mdata [ \"argiope class\" ] ) return field class ( * * out )", "predictions": ["read an individual field of the ( report ."], "references": ["reads a field output report ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 5252, "code": "def list to string ( l = range ( 200 ) , width = 40 , indent = \"  \" ) : l = [ str ( v ) + \",\" for v in l ] counter = 0 out = \"\" + indent for w in l : s = len ( w ) if counter + s > width : out += \"\\n\" + indent counter = 0 out += w counter += s return out . strip ( \",\" )", "predictions": ["read the string in 200 format ."], "references": ["converts a list - like to string with given line width ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5253, "code": "def equation ( nodes = ( 1 , 2 ) , dofs = ( 1 , 1 ) , coefficients = ( 1. , 1. ) , comment = None ) : N = len ( nodes ) if comment == None : out = \"\" else : out = \"**EQUATION: {0}\\n\" . format ( comment ) out += \"*EQUATION\\n  {0}\\n  \" . format ( N ) out += \"\\n  \" . join ( [ \",\" . join ( [ str ( nodes [ i ] ) , str ( int ( dofs [ i ] ) ) , str ( coefficients [ i ] ) ] ) for i in range ( N ) ] ) return out", "predictions": ["compute equation - grams equation for one or more nodes ."], "references": ["returns an abaqus inp formated string for a given linear equation ."], "bleu": 0.12368857073777001, "rouge_l": 0.17256011315417258}
{"id": 5254, "code": "def unsorted set ( df , label , * * kwargs ) : out = \"*NSET, NSET={0}, UNSORTED\\n\" . format ( label ) labels = df . index . values return out + argiope . utils . list to string ( labels , * * kwargs )", "predictions": ["set the remainder of the kernel to the given label ."], "references": ["returns a set as inp string with unsorted option ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 5255, "code": "def write inp ( self ) : template = self . get template ( ) return template . substitute ( { \"class\" : self . class . name , \"label\" : self . label } ) . strip ( )", "predictions": ["write out the template ."], "references": ["returns the material definition as a string in abaqus inp format ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 5256, "code": "def write inp ( self ) : template = self . get template ( ) plastic table = self . get plastic table ( ) return template . substitute ( { \"class\" : self . class . name , \"label\" : self . label , \"young modulus\" : self . young modulus , \"poisson ratio\" : self . poisson ratio , \"plastic table\" : ( self . get plastic table ( ) [ [ \"stress\" , \"plastic strain\" ] ] . to csv ( header = False , index = False , sep = \",\" ) . strip ( ) ) } ) . strip ( )", "predictions": ["writes out the full table in this class ."], "references": ["returns the material definition as a string in abaqus inp format ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 5257, "code": "def get plastic table ( self ) : E = self . young modulus sy = self . yield stress n = self . hardening exponent eps max = self . max strain Np = self . strain data points ey = sy / E s = 10. ** np . linspace ( 0. , np . log10 ( eps max / ey ) , Np ) strain = ey * s stress = sy * s ** n plastic strain = strain - stress / E return pd . Data Frame ( { \"strain\" : strain , \"stress\" : stress , \"plastic strain\" : plastic strain } )", "predictions": ["return a list of ( that are currently being used to generate the magic for the given table ."], "references": ["calculates the plastic data"], "bleu": 0.06439931429457924, "rouge_l": 0.09854604200323101}
{"id": 5258, "code": "def get plastic table ( self ) : K = self . consistency sy = self . yield stress n = self . hardening exponent eps max = self . max strain Np = self . strain data points plastic strain = np . linspace ( 0. , eps max , Np ) stress = sy + K * plastic strain ** n return pd . Data Frame ( { \"stress\" : stress , \"plastic strain\" : plastic strain } )", "predictions": ["method to get a list of ( for this class ."], "references": ["calculates the plastic data"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5259, "code": "def write xy report ( odb , path , tags , columns , steps ) : xy Data = [ session . XY Data From History ( name = columns [ i ] , odb = odb , output Variable Name = tags [ i ] , steps = steps ) for i in xrange ( len ( tags ) ) ] session . xy Report Options . set Values ( num Digits = 8 , number Format = SCIENTIFIC ) session . write XY Report ( file Name = path , append Mode = OFF , xy Data = xy Data )", "predictions": ["in - place the parent of this variable description ."], "references": ["writes a xy_report based on xy data ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5260, "code": "def write field report ( odb , path , label , argiope class , variable , instance , output position , step = - 1 , frame = - 1 , sort Item = 'Node Label' ) : step Keys = get steps ( odb ) step = xrange ( len ( step Keys ) ) [ step ] frame = xrange ( get frames ( odb , step Keys [ step ] ) ) [ frame ] nf = Number Format ( num Digits = 9 , precision = 0 , format = SCIENTIFIC ) session . field Report Options . set Values ( print Total = OFF , print Min Max = OFF , number Format = nf ) leaf = dgo . Leaf From Part Instance ( part Instance Name = instance ) session . viewports [ 'Viewport: 1' ] . odb Display . display Group . replace ( leaf = leaf ) session . write Field Report ( file Name = path , append = OFF , sort Item = sort Item , odb = odb , step = step , frame = frame , output Position = output position , variable = variable ) lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] isdata = - 1 data = [ ] for line in lines : if isdata == 1 : if len ( line ) == 0 : isdata -= 1 else : data . append ( line ) elif isdata < 1 : if line . startswith ( \"--\" ) : isdata += 1 data = \"\\n\" . join ( [ \",\" . join ( line . split ( ) ) for line in data if len ( line ) != 0 ] ) header = str ( output position ) . lower ( ) + \",\" header += \",\" . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2 ] ] ) + \"\\n\" metadata = ( ( \"label\" , label ) , ( \"argiope class\" , argiope class ) , ( \"odb\" , odb . path ) , ( \"instance\" , instance ) , ( \"position\" , output position ) , ( \"step num\" , step ) , ( \"step label\" , step Keys [ step ] ) , ( \"frame\" , frame ) , ( \"frame value\" , odb . steps [ step Keys [ step ] ] . frames [ frame ] . frame Value ) ) out = \"*METADATA\\n{0}\\n*DATA\\n{1}\" . format ( \"\\n\" . join ( [ \"{0}={1}\" . format ( k , v ) for k , v in metadata ] ) , header + data ) open ( path , \"w\" ) . write ( out )", "predictions": ["writes the ( ( or all frames ) of the ( ( or all the frames in the ( ."], "references": ["writes a field report and rewrites it in a cleaner format ."], "bleu": 0.07264339766175722, "rouge_l": 0.1963519313304721}
{"id": 5261, "code": "def list ( component type ) : config loader = initialise component loader ( ) component types = sorted ( { \"displays\" : lambda : config loader . load by type ( Component Type . DISPLAY ) , \"datafeeds\" : lambda : config loader . load by type ( Component Type . DATA FEED ) , \"filters\" : lambda : config loader . load by type ( Component Type . FILTER ) , \"notifications\" : lambda : config loader . load by type ( Component Type . NOTIFICATION ) } . items ( ) , key = lambda t : t [ 0 ] ) def print ids ( creators ) : ids = { c . id key value [ 1 ] if hasattr ( c , \"id key value\" ) else c . get id ( ) for c in creators } for i in sorted ( ids ) : click . echo ( \" - %s\" % i ) for k , v in component types : if component type == k or component type == \"all\" : click . echo ( \"Available %s:\" % k ) print ids ( v ( ) ) if component type == \"all\" : click . echo ( \"\" )", "predictions": ["list all the conversion modules of this component ."], "references": ["list components that are available on your machine"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 5262, "code": "def set data ( self ) : if getattr ( self , 'data' , False ) and not getattr ( self , ' x' , False ) and not getattr ( self , ' y' , False ) : x = X Variable ( ) y = Y Variable ( ) x . contribute to class ( self , 'X' , self . data ) y . contribute to class ( self , 'Y' , self . data ) self [ 'data' ] = zip ( self . x . points , self . y . points ) else : for axis in ( ' x' , ' y' ) : axis obj = getattr ( self , axis , False ) if not axis obj : raise exception . Missing Axis Exception ( \"%s missing\" % axis ) if not getattr ( axis obj , 'points' , False ) : raise exception . Missing Data Exception ( ) self [ 'data' ] = zip ( self . x . points , self . y . points )", "predictions": ["returns set for each feature ."], "references": ["this method will be called to set series data"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 5263, "code": "def get axis mode ( self , axis ) : if all ( [ isinstance ( getattr ( s , axis ) , Time Variable ) for s in self . series ] ) : return 'time' return None", "predictions": ["retrieves stats for each feature ."], "references": ["will get the axis mode for the current series"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 5264, "code": "def set options ( self ) : if 'xaxis' in self . options . keys ( ) : self . options [ 'xaxis' ] . update ( { 'mode' : self . get axis mode ( X Axis . var name ) } ) if 'yaxis' in self . options . keys ( ) : self . options [ 'yaxis' ] . update ( { 'mode' : self . get axis mode ( Y Axis . var name ) } )", "predictions": ["sets the request options for a theme ."], "references": ["sets the graph ploting options"], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 5265, "code": "def create setter ( func , attrs ) : def set ( self , instance , value , name = None ) : args = [ getattr ( self , attr ) for attr in attrs ] if not func ( value , * args ) : raise Value Error ( self . err msg ( instance , value ) ) return set", "predictions": ["creates a db-api problem with function arguments ."], "references": ["create the __set__ method for the descriptor ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 5266, "code": "def make class ( clsname , func , attrs ) : clsdict = { \" set \" : create setter ( func , attrs ) } if len ( attrs ) > 0 : clsdict [ \" init \" ] = create init ( attrs ) clsobj = type ( str ( clsname ) , ( Descriptor , ) , clsdict ) clsobj . doc = docstrings . get ( clsname ) return clsobj", "predictions": ["creates the function to make the given class and clsdict ."], "references": ["turn a funcs list element into a class object ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 5267, "code": "def cycle ( self ) : messages = self . poll datafeeds ( ) notifications = self . process notifications ( messages ) self . draw notifications ( notifications )", "predictions": ["cycle through all messages ."], "references": ["cycles through notifications with latest results from data feeds ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5268, "code": "def plot ( parser , token ) : tokens = token . split contents ( ) tokens . pop ( 0 ) graph = tokens . pop ( 0 ) attrs = dict ( [ token . split ( \"=\" ) for token in tokens ] ) if 'id' not in attrs . keys ( ) : attrs [ 'id' ] = '' . join ( [ chr ( choice ( range ( 65 , 90 ) ) ) for i in range ( 0 , 5 ) ] ) else : attrs [ 'id' ] = attrs [ 'id' ] [ 1 : len ( attrs [ 'id' ] ) - 1 ] attr string = '' . join ( [ \" %s=%s\" % ( k , v ) for k , v in attrs . iteritems ( ) ] ) return Graph Renderer ( graph , attr string , attrs [ 'id' ] )", "predictions": ["plot the internal transformation function for parsing the arguments for the given token ."], "references": ["tag to plot graphs into the template"], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 5269, "code": "def read varint ( self ) : buf = self . read ( 8 ) ( n , l ) = Decode Varint ( buf , 0 ) self . unread ( buf [ l : ] ) return n", "predictions": ["read varint data from the reader ."], "references": ["read exactly a varint out of the underlying file ."], "bleu": 0.15215596197411094, "rouge_l": 0.45607476635514016}
{"id": 5270, "code": "def working directory ( path ) : prev dir = os . getcwd ( ) os . chdir ( str ( path ) ) try : yield finally : os . chdir ( prev dir )", "predictions": ["creates a working directory for the given path ."], "references": ["change working directory and restore the previous on exit"], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 5271, "code": "def exit ( self ) : if self . server is not None : self . server . shutdown ( ) self . server . server close ( ) self . server = None", "predictions": ["this method closes the debugger server ."], "references": ["stop the simple wsgi server running the appliation ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 5272, "code": "def get error page callback ( self ) : if self . response . status in self . error handlers : return self . error handlers [ self . response . status ] elif None in self . error handlers : return self . error handlers [ None ] else : self . response . media type = 'text/plain' return lambda : self . response . status line", "predictions": ["set callback to callback on all the response from self ."], "references": ["return an error page for the current response status ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 5273, "code": "def attempt fetch ( work unit , fpath ) : url = 'http://s3.amazonaws.com/aws-publicdatasets/' + work unit . key . strip ( ) cmd = '(wget -O - %s | gpg --no-permission-warning --trust-model always --output - --decrypt - | xz --decompress) 2> %s-err' % ( url , fpath ) print cmd child = Popen ( cmd , stdout = PIPE , shell = True ) print 'child launched' sys . stdout . flush ( ) si count = 0 serif count = 0 exc = '' stream ids = list ( ) clean visible bytes = 0 clean visible count = 0 try : for si in Chunk ( file obj = child . stdout ) : print si . stream id , si . abs url if si . body . language : lang = si . body . language . code else : lang = '' stream ids . append ( ( lang , si . stream id ) ) if si . body . clean visible : clean visible count += 1 clean visible bytes += len ( si . body . clean visible ) si count += 1 if 'serif' in si . body . sentences : serif count += 1 except Exception , exc : exc = re . sub ( '\\s+' , ' ' , str ( exc ) ) . strip ( ) child . terminate ( ) child . wait ( ) child . stdout . close ( ) return exc , si count , serif count , clean visible bytes , clean visible count , stream ids", "predictions": ["attempt to fetch work by moving the work and fetch the stream ."], "references": ["attempt a fetch and iteration over a work_unit . key path in s3"], "bleu": 0.12011055432195765, "rouge_l": 0.30769230769230765}
{"id": 5274, "code": "def get file lines ( file name ) : file path = path . join ( path . dirname ( path . abspath ( file ) ) , file name ) with open ( file path ) as file obj : return [ line for line in file obj . read ( ) . splitlines ( ) if line ]", "predictions": ["returns to the top - level object ."], "references": ["return a list of non - empty lines from file_path ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 5275, "code": "def random adjspecies pair ( ) : describer , desc position = random describer ( ) if desc position == 'prefix' : return ( describer , random species ( ) ) elif desc position == 'suffix' : return ( random species ( ) , describer )", "predictions": ["get a random pair of bytes from an open file ."], "references": ["return an ordered 2 - tuple containing a species and a describer ."], "bleu": 0.11294012253658708, "rouge_l": 0.1641991924629879}
{"id": 5276, "code": "def morph ( ctx , app id , sentence file , json flag , sentence , info filter , pos filter , request id ) : app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) if info filter : info filter = info filter . replace ( ',' , '|' ) if pos filter : pos filter = pos filter . replace ( ',' , '|' ) api = Goolabs API ( app id ) ret = api . morph ( sentence = sentence , info filter = info filter , pos filter = pos filter , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for words in ret [ 'word list' ] : for word in words : click . echo ( ',' . join ( word ) )", "predictions": ["get all words that do not actually morph ."], "references": ["morphological analysis for japanese ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 5277, "code": "def similarity ( ctx , app id , json flag , query pair , request id ) : app id = clean app id ( app id ) api = Goolabs API ( app id ) ret = api . similarity ( query pair = query pair , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( '{0:.16f}' . format ( ret [ 'score' ] ) )", "predictions": ["return the similarity for the given app ."], "references": ["scoring the similarity of two words ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 5278, "code": "def hiragana ( ctx , app id , sentence file , json flag , sentence , output type , request id ) : app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) api = Goolabs API ( app id ) ret = api . hiragana ( sentence = sentence , output type = output type , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( ret [ 'converted' ] )", "predictions": ["hiragana a hiragana . get the hiragana if it is a hiragana ."], "references": ["convert the japanese to hiragana or katakana ."], "bleu": 0.1135935489027116, "rouge_l": 0.2985318107667211}
{"id": 5279, "code": "def entity ( ctx , app id , sentence file , json flag , sentence , class filter , request id ) : app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) if class filter : class filter = class filter . replace ( ',' , '|' ) api = Goolabs API ( app id ) ret = api . entity ( sentence = sentence , class filter = class filter , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for ne in ret [ 'ne list' ] : click . echo ( ',' . join ( ne ) )", "predictions": ["return entity from json ."], "references": ["extract unique representation from sentence ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 5280, "code": "def shortsum ( ctx , app id , review file , json flag , review , length , request id ) : app id = clean app id ( app id ) review list = clean review ( review , review file ) length int = clean length ( length ) api = Goolabs API ( app id ) ret = api . shortsum ( review list = review list , length = length int , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( ret [ 'summary' ] )", "predictions": ["run a run from the run of the t0 ."], "references": ["summarize reviews into a short summary ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 5281, "code": "def keyword ( ctx , app id , body file , json flag , title , body , max num , forcus , request id ) : app id = clean app id ( app id ) body = clean body ( body , body file ) api = Goolabs API ( app id ) ret = api . keyword ( title = title , body = body , max num = max num , forcus = forcus , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for k in ret [ 'keywords' ] : k = dict ( ( key . encode ( 'utf-8' ) , k [ key ] ) for key in k . keys ( ) ) for keyword , score in six . iteritems ( k ) : click . echo ( u'{0},{1}' . format ( text ( keyword ) , score ) )", "predictions": ["generate a run of the specified ) ."], "references": ["extract keywords from an input document ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5282, "code": "def chrono ( ctx , app id , sentence file , json flag , sentence , doc time , request id ) : app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) api = Goolabs API ( app id ) ret = api . chrono ( sentence = sentence , doc time = doc time , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for pair in ret [ 'datetime list' ] : click . echo ( u'{0}: {1}' . format ( text ( pair [ 0 ] ) , pair [ 1 ] ) )", "predictions": ["perform the cleanup of the path based on the steps provided . this will only be called from the client ."], "references": ["extract expression expressing date and time and normalize its value"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 5283, "code": "def make app ( ) : env = Environment ( ) args = parser . parse args ( args = [ '/' , '--ignore-stdin' ] , env = env ) args . output options = 'HB' server = 'HTT Pony/{0}' . format ( version ) def application ( environ , start response ) : if environ . get ( 'CONTENT LENGTH' ) == '' : del environ [ 'CONTENT LENGTH' ] if environ . get ( 'CONTENT TYPE' ) == '' : del environ [ 'CONTENT TYPE' ] wrequest = Werkzeug Request ( environ ) data = wrequest . get data ( ) request = Request ( method = wrequest . method , url = wrequest . url , headers = wrequest . headers , data = data , ) prepared = request . prepare ( ) stream = streams . build output stream ( args , env , prepared , response = None , output options = args . output options ) streams . write stream ( stream , env . stdout , env . stdout isatty ) if data : print ( \"\\n\" , file = env . stdout ) response = Response ( headers = { 'Server' : server } ) return response ( environ , start response ) return application", "predictions": ["makes a new wsgi call ."], "references": ["make a wsgi app that has all the httpie pieces baked in ."], "bleu": 0.08180282100568384, "rouge_l": 0.29611650485436897}
{"id": 5284, "code": "def make ner file ( self , clean visible path , ner xml path ) : if self . template is None : raise exceptions . Not Implemented Error ( ) tagger config = dict ( tagger root path = self . config [ 'tagger root path' ] , clean visible path = clean visible path , ner xml path = ner xml path ) tagger config [ 'java heap size' ] = self . config . get ( 'java heap size' , '' ) cmd = self . template % tagger config start time = time . time ( ) gc . collect ( ) try : self . child = subprocess . Popen ( cmd , stderr = subprocess . PIPE , shell = True ) except OS Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( clean visible path , ner xml path ) raise Pipeline Out Of Memory ( msg ) s out , errors = self . child . communicate ( ) if not self . child . returncode == 0 : if 'java.lang.Out Of Memory Error' in errors : msg = errors + make memory info msg ( clean visible path , ner xml path ) raise Pipeline Out Of Memory ( msg ) elif self . child . returncode == 137 : msg = 'tagger returncode = 137\\n' + errors msg += make memory info msg ( clean visible path , ner xml path ) raise Pipeline Out Of Memory ( msg ) elif 'Exception' in errors : raise Pipeline Base Exception ( errors ) else : raise Pipeline Base Exception ( 'tagger exited with %r' % self . child . returncode ) elapsed = time . time ( ) - start time logger . info ( 'finished tagging in %.1f seconds' % elapsed ) return elapsed", "predictions": ["creates a new ( ."], "references": ["run tagger a child process to get xml output"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5285, "code": "def align chunk with ner ( self , ner xml path , i chunk , o chunk ) : input iter = i chunk . iter ( ) all ner = xml . dom . minidom . parse ( open ( ner xml path ) ) for ner dom in all ner . get Elements By Tag Name ( 'FILENAME' ) : #for stream id, raw ner in files(open(ner xml path).read().decode('utf8')): stream item = input iter . next ( ) stream id = ner dom . attributes . get ( 'stream id' ) . value if stream item . stream id is None : assert not stream id , 'out of sync: None != %r' % stream id logger . critical ( 'si.stream id is None... ignoring' ) continue assert stream id and stream id == stream item . stream id , '%s != %s' % ( stream id , stream item . stream id ) if not stream item . body : #assert not ner dom....something continue tagging = Tagging ( ) tagging . tagger id = self . tagger id #tagging.raw tagging = tagged doc tagging . generation time = streamcorpus . make stream time ( ) stream item . body . taggings [ self . tagger id ] = tagging sentences , relations , attributes = self . get sentences ( ner dom ) stream item . body . sentences [ self . tagger id ] = sentences stream item . body . relations [ self . tagger id ] = relations stream item . body . attributes [ self . tagger id ] = attributes logger . debug ( 'finished aligning tokens %s' % stream item . stream id ) if 'align labels by' in self . config and self . config [ 'align labels by' ] : assert 'aligner data' in self . config , 'config missing \"aligner data\"' aligner = Alignment Strategies [ self . config [ 'align labels by' ] ] aligner ( stream item , self . config [ 'aligner data' ] ) gc . collect ( ) try : o chunk . add ( stream item ) except Memory Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( ) logger . critical ( msg ) raise Pipeline Out Of Memory ( msg ) try : o chunk . close ( ) logger . info ( 'finished chunk for %r' % ner xml path ) except Memory Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( ) logger . critical ( msg ) raise Pipeline Out Of Memory ( msg )", "predictions": ["equation of ( that allows us to be of the operation to be of the operation ."], "references": ["iterate through ner_xml_path to fuse with i_chunk into o_chunk"], "bleu": 0.07223943354597204, "rouge_l": 0.0814419225634179}
{"id": 5286, "code": "def shutdown ( self ) : if self . child : try : self . child . terminate ( ) except OS Error , exc : if exc . errno == 3 : pass", "predictions": ["called to notify the node has been called once ."], "references": ["send sigterm to the tagger child process"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 5287, "code": "def mult ( p , n ) : np = P ( ) while n >= 1 : if n % 2 : np = np + p p = p + p n = n // 2 return np", "predictions": ["label 0: a number with all three values ) ."], "references": ["returns a pattern that matches exactly n repetitions of pattern p ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 5288, "code": "def fix emails ( text ) : emails = bracket emails . findall ( text ) keys = [ ] for email in emails : email = email . replace ( \"<\" , \"&lt;\" ) . replace ( \">\" , \"&gt;\" ) text = text . replace ( email , email ) return text", "predictions": ["write the self - terminated self to the name ."], "references": ["replace all angle bracket emails with a unique key ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 5289, "code": "def sentences ( self , clean visible ) : previous end = 0 clean visible = clean visible . decode ( 'utf8' ) for start , end in self . sentence tokenizer . span tokenize ( clean visible ) : if start < previous end : start = previous end if start > end : continue try : label = self . label index . find le ( end ) except Value Error : label = None if label : off = label . offsets [ Offset Type . CHARS ] end = max ( off . first + off . length , end ) previous end = end sent str = clean visible [ start : end ] yield start , end , sent str", "predictions": ["splits the get and yield into ) ."], "references": ["generate strings identified as sentences"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5290, "code": "def make label index ( self , stream item ) : labels = stream item . body . labels . get ( self . annotator id ) if not labels : labels = [ ] self . label index = Sorted Collection ( [ l for l in labels if Offset Type . CHARS in l . offsets ] , key = lambda label : label . offsets [ Offset Type . CHARS ] . first )", "predictions": ["makes the given label for this set of consistency trees ."], "references": ["make a sortedcollection on body . labels"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 5291, "code": "def make sentences ( self , stream item ) : self . make label index ( stream item ) sentences = [ ] token num = 0 new mention id = 0 for sent start , sent end , sent str in self . sentences ( stream item . body . clean visible ) : assert isinstance ( sent str , unicode ) sent = Sentence ( ) sentence pos = 0 for start , end in self . word tokenizer . span tokenize ( sent str ) : token str = sent str [ start : end ] . encode ( 'utf8' ) tok = Token ( token num = token num , token = token str , sentence pos = sentence pos , ) tok . offsets [ Offset Type . CHARS ] = Offset ( type = Offset Type . CHARS , first = sent start + start , length = end - start , ) try : label = self . label index . find le ( sent start + start ) except Value Error : label = None if label : off = label . offsets [ Offset Type . CHARS ] if off . first + off . length > sent start + start : streamcorpus . add annotation ( tok , label ) logger . debug ( 'adding label to tok: %r has %r' , tok . token , label . target . target id ) if label in self . label to mention id : mention id = self . label to mention id [ label ] else : mention id = new mention id new mention id += 1 self . label to mention id [ label ] = mention id tok . mention id = mention id token num += 1 sentence pos += 1 sent . tokens . append ( tok ) sentences . append ( sent ) return sentences", "predictions": ["builds xy . for all the input values ."], "references": ["assemble sentence and token objects"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5292, "code": "def make cleansed file ( i chunk , tmp cleansed path ) : tmp cleansed = open ( tmp cleansed path , 'wb' ) for idx , si in enumerate ( i chunk ) : tmp cleansed . write ( '<FILENAME docid=\"%s\">\\n' % si . stream id ) tmp cleansed . write ( si . body . cleansed ) tmp cleansed . write ( '</FILENAME>\\n' ) tmp cleansed . close ( ) print 'created %s' % tmp cleansed path", "predictions": ["write an automaton to a given , using the specified , and write ."], "references": ["make a temp file of cleansed text"], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 5293, "code": "def make ner file ( tagger id , tmp cleansed path , tmp ner path , pipeline root ) : params = dict ( INPUT FILE = tmp cleansed path , #RAW OUTPUT FILE=tmp ner raw path, OUTPUT FILE = tmp ner path , PIPELINE ROOT = pipeline root ) pipeline cmd = pipeline cmd templates [ tagger id ] % params print pipeline cmd print 'creating %s' % tmp ner path start time = time . time ( ) gpg child = subprocess . Popen ( pipeline cmd , stderr = subprocess . PIPE , shell = True ) s out , errors = gpg child . communicate ( ) assert gpg child . returncode == 0 and 'Exception' not in errors , errors elapsed = time . time ( ) - start time print 'created %s in %.1f sec' % ( tmp ner path , elapsed )", "predictions": ["create and initialize the component"], "references": ["run child process to get owpl output"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 5294, "code": "def make clean visible file ( i chunk , clean visible path ) : clean = open ( clean visible path , 'wb' ) clean . write ( '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' ) clean . write ( '<root>' ) for idx , si in enumerate ( i chunk ) : if si . stream id is None : stream id = '' else : stream id = si . stream id doc = lxml . etree . Element ( \"FILENAME\" , stream id = stream id ) if si . body and si . body . clean visible : try : doc . text = si . body . clean visible . decode ( 'utf8' ) except Value Error : doc . text = drop invalid and upper utf8 chars ( si . body . clean visible . decode ( 'utf8' ) ) except Exception , exc : logger . critical ( traceback . format exc ( exc ) ) logger . critical ( 'failed on stream id=%s to follow:' , si . stream id ) logger . critical ( repr ( si . body . clean visible ) ) logger . critical ( 'above was stream id=%s' , si . stream id ) raise else : doc . text = '' clean . write ( lxml . etree . tostring ( doc , encoding = 'UTF-8' ) ) clean . write ( '</root>' ) clean . close ( ) logger . info ( clean visible path )", "predictions": ["not part of android api ."], "references": ["make a temp file of clean_visible text"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5295, "code": "def main ( ) : import argparse import sys parser = argparse . Argument Parser ( ) parser . add argument ( 'path' ) args = parser . parse args ( ) html = open ( args . path ) . read ( ) html = html . decode ( 'utf8' ) cursor = 0 for s in non tag chars from raw ( html ) : for c in s : if c != ' ' and c != html [ cursor ] : import pdb pdb . set trace ( ) sys . stdout . write ( c . encode ( 'utf8' ) ) sys . stdout . flush ( ) cursor += 1", "predictions": ["the main entry point ."], "references": ["manual test loop for make_clean_visible_from_raw"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5296, "code": "def paths ( input dir ) : for root , dirs , fnames in os . walk ( input dir ) : for i fname in fnames : i path = os . path . join ( root , i fname ) yield i path", "predictions": ["iterate over all set of set and return the paths that match the given directory ."], "references": ["yield all file paths under input_dir"], "bleu": 0.08513012360883544, "rouge_l": 0.19805194805194803}
{"id": 5297, "code": "def tasks ( self , key prefix = '' ) : for row in self . tasks . get range ( ) : logger . debug ( row ) if not row [ 0 ] . startswith ( key prefix ) : continue data = json . loads ( row [ 1 ] [ 'task data' ] ) data [ 'task key' ] = row [ 0 ] yield data", "predictions": ["a generator that returns a generator of all create create keys for each , including the default values ."], "references": ["generate the data objects for every task"], "bleu": 0.0712695567709093, "rouge_l": 0.08390646492434663}
{"id": 5298, "code": "def get random available ( self , max iter = 10000 ) : c = 1 keeper = None #random key = hashlib.md5(str(random.random())).hexdigest() #random key = '0' * 32 #logger.debug('available.get range(%r)' % random key) # for row in self . available . get range ( row count = max iter , read consistency level = pycassa . Consistency Level . ALL ) : #for row in self. available.get range(row count=100): logger . debug ( 'considering %r' % ( row , ) ) if random . random ( ) < 1 / c : keeper = row [ 0 ] if c == max iter : break c += 1 return keeper", "predictions": ["choose a class that will be executed by the user"], "references": ["get a random key out of the first max_iter rows"], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 5299, "code": "def tokens ( self , sentence dom ) : self . sent pos = 0 mention id = 0 while len ( sentence dom . child Nodes ) > 0 : node = sentence dom . child Nodes . pop ( 0 ) if node . node Type == node . TEXT NODE : for line in node . data . splitlines ( True ) : self . input string = line for start , end in self . word tokenizer . span tokenize ( line ) : tok = self . make token ( start , end ) if tok : yield tok if line . endswith ( '\\n' ) : self . line idx += 1 self . byte idx += len ( line . encode ( 'utf-8' ) ) else : assert node . node Name == 'ENAMEX' , node . node Name chain id = node . attributes . get ( 'ID' ) . value entity type = node . attributes . get ( 'TYPE' ) . value for node in node . child Nodes : assert node . node Type == node . TEXT NODE , node . node Type for line in node . data . splitlines ( True ) : self . input string = line for start , end in self . word tokenizer . span tokenize ( line ) : tok = self . make token ( start , end ) if tok : if entity type in PRONOUNS : tok . mention type = Mention Type . PRO tok . entity type = ENTITY TYPES [ entity type ] attr = Attribute ( attribute type = Attribute Type . PER GENDER , value = str ( PRONOUNS [ entity type ] ) ) self . attributes . append ( attr ) else : tok . mention type = Mention Type . NAME tok . entity type = ENTITY TYPES [ entity type ] tok . equiv id = int ( chain id ) tok . mention id = mention id yield tok if line . endswith ( '\\n' ) : self . line idx += 1 self . byte idx += len ( line . encode ( 'utf-8' ) ) mention id += 1", "predictions": ["a generator that returns all nodes of the cycle ."], "references": ["tokenize all the words and preserve ner labels from enamex tags"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 5300, "code": "def get sentences ( self , ner dom ) : lp parser = Ling Pipe Parser ( self . config ) lp parser . set ( ner dom ) sentences = list ( lp parser . sentences ( ) ) return sentences , lp parser . relations , lp parser . attributes", "predictions": ["plot the ( : 1 tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens tokens ."], "references": ["parse the sentences and tokens out of the xml"], "bleu": 0.04970745472800838, "rouge_l": 0.1221221221221221}
{"id": 5301, "code": "def verify md5 ( md5 expected , data , other errors = None ) : md5 recv = hashlib . md5 ( data ) . hexdigest ( ) if md5 expected != md5 recv : if other errors is not None : logger . critical ( '\\n' . join ( other errors ) ) raise Failed Verification ( 'original md5 = %r != %r = received md5' % ( md5 expected , md5 recv ) ) return True", "predictions": ["read all key bytes in this cluster"], "references": ["return true if okay raise exception if not"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 5302, "code": "def main ( argv = sys . argv ) : args = parse ( argv ) hostname = args . listen port = args . port print ( \"Making all your dreams for a pony come true on http://{0}:{1}.\\n\" \"Press Ctrl+C to quit.\\n\" . format ( hostname , port ) ) logging . get Logger ( 'werkzeug' ) . set Level ( logging . CRITICAL ) plugin manager . load installed plugins ( ) app = make app ( ) run simple ( hostname , port , app )", "predictions": ["entry point for the script ."], "references": ["serve up some ponies ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5303, "code": "def build parser ( ) : description = ( 'HTT Pony (pronounced aych-tee-tee-pony) is a simple HTTP ' 'server that pretty prints HTTP requests to a terminal. It ' 'is a useful aide for developing clients that send HTTP ' 'requests. HTT Pony acts as a sink for a client so that a ' 'developer can understand what the client is sending.' ) parser = argparse . Argument Parser ( description = description ) parser . add argument ( '-l' , '--listen' , help = 'set the IP address or hostname' , default = 'localhost' ) parser . add argument ( '-p' , '--port' , help = 'set the port' , default = 8000 , type = int ) return parser", "predictions": ["this is the same as above . but only works for ( ."], "references": ["build the parser that will have all available commands and options ."], "bleu": 0.10571070857151538, "rouge_l": 0.16116248348745044}
{"id": 5304, "code": "def sentences to char tokens ( si sentences ) : for sentence in si sentences : for token in sentence . tokens : if Offset Type . CHARS in token . offsets : yield token", "predictions": ["convert get get get get get list of get get get the string representation of an iterator into a single string ."], "references": ["convert stream item sentences to character offset s ."], "bleu": 0.0612957497932821, "rouge_l": 0.13958810068649885}
{"id": 5305, "code": "def char tokens to char offsets ( si tokens ) : for token in si tokens : offset = token . offsets [ Offset Type . CHARS ] yield offset . first , offset . first + offset . length", "predictions": ["convert a character stream into an iterator ."], "references": ["convert character offset s to character ranges ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 5306, "code": "def text index ( self ) : i = self . tags . get ( Text Element , 0 ) if self . last tag is not Text Element : i += 1 return i", "predictions": ["get the get text for this element ."], "references": ["returns the one - based index of the current text node ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 5307, "code": "def descendants ( elem ) : for child in elem . xml children : if isinstance ( child , element ) : yield child yield from descendants ( child )", "predictions": ["species all children of the given return ."], "references": ["yields all the elements descendant of elem in document order"], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 5308, "code": "def following siblings ( elem ) : it = itertools . dropwhile ( lambda x : x != elem , elem . xml parent . xml children ) next ( it ) #Skip the element itself return it", "predictions": ["allows you to add morph to the tree ."], "references": ["yields elements and text which have the same parent as elem but come afterward in document order"], "bleu": 0.05802435550866946, "rouge_l": 0.07287933094384706}
{"id": 5309, "code": "def svg2pdf ( svg file path , pdf file path , dpi = 150 , command binpath = None , support unicode = False ) : if support unicode : return rsvg export ( svg file path , pdf file path , dpi = dpi , rsvg binpath = command binpath ) return inkscape export ( svg file path , pdf file path , export flag = \"-A\" , dpi = dpi , inkscape binpath = command binpath )", "predictions": ["copies the , json and json from ctx to ( ."], "references": ["transform svg file to pdf file"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 5310, "code": "def svg2png ( svg file path , png file path , dpi = 150 , inkscape binpath = None ) : return inkscape export ( svg file path , png file path , export flag = \"-e\" , dpi = dpi , inkscape binpath = inkscape binpath )", "predictions": ["copies the ctx from the specified app to the specified app ."], "references": ["transform svg file to png file"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 5311, "code": "def strval ( node , outermost = True ) : if not isinstance ( node , element ) : return node . xml value if outermost else [ node . xml value ] accumulator = [ ] for child in node . xml children : if isinstance ( child , text ) : accumulator . append ( child . xml value ) elif isinstance ( child , element ) : accumulator . extend ( strval ( child , outermost = False ) ) if outermost : accumulator = '' . join ( accumulator ) return accumulator", "predictions": ["we have to iterate through the given ctx ."], "references": ["xpath - like string value of node"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5312, "code": "def parse options ( ) : version = \"%%prog {version}\" . format ( version = version ) parser = Option Parser ( version = version ) parser . add option ( \"-u\" , \"--username\" , action = \"store\" , dest = \"username\" , type = \"string\" , default = \"\" , metavar = \"RECIPIENT\" , help = \"user\" ) parser . add option ( \"-C\" , \"--calendar\" , metavar = \"CALENDAR\" , action = \"store\" , type = \"string\" , dest = \"calendar\" , default = \"\" , help = \"google calendar ID\" ) parser . add option ( \"-t\" , \"--timezone\" , metavar = \"TIMEZONE\" , action = \"store\" , type = \"string\" , dest = \"timezone\" , default = \"\" , help = \"user timezone\" ) parser . add option ( \"-m\" , \"--message\" , metavar = \"MESSAGE\" , action = \"store\" , type = \"string\" , dest = \"message\" , default = \"\" , help = \"message text\" ) parser . add option ( \"-c\" , \"--config\" , metavar = \"CONFIG\" , action = \"store\" , type = \"string\" , dest = \"config\" , help = \"path to config file\" , default = \"/etc/nagios/notification google calendar.ini\" ) parser . add option ( \"-q\" , \"--quiet\" , metavar = \"QUIET\" , action = \"store true\" , default = False , dest = \"quiet\" , help = \"be quiet\" ) parser . add option ( \"-g\" , \"--get-google-credentials\" , metavar = \"GET-GOOGLE-CREDENTIALS\" , action = \"store true\" , default = False , dest = \"get google credentials\" , help = \"get google API credentials for user\" ) options = parser . parse args ( sys . argv ) [ 0 ] mandatories = [ \"username\" , ] if not options . get google credentials : mandatories . append ( \"calendar\" ) mandatories . append ( \"message\" ) mandatories . append ( \"timezone\" ) if not all ( options . dict [ mandatory ] for mandatory in mandatories ) : parser . error ( \"Required command line option missing\\n\" ) return options", "predictions": ["create and return the error parser ."], "references": ["commandline options arguments parsing ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5313, "code": "def parse config ( options ) : if os . path . exists ( options . config ) : config = Config Parser . Config Parser ( ) try : config . read ( options . config ) except Exception , err : if not options . quiet : sys . stderr . write ( \"ERROR: Config file read {config} error. {err}\" . format ( config = options . config , err = err ) ) sys . exit ( - 1 ) try : configdata = { \"secrets\" : config . get ( \"GOOGLE\" , \"secrets\" ) , \"credentials\" : config . get ( \"nagios-notification-google-calendar\" , \"credentials\" ) , \"start\" : config . get ( \"nagios-notification-google-calendar\" , \"start\" ) , \"end\" : config . get ( \"nagios-notification-google-calendar\" , \"end\" ) , \"message\" : config . get ( \"nagios-notification-google-calendar\" , \"message\" ) , } except Config Parser . No Option Error , err : if not options . quiet : sys . stderr . write ( \"ERROR: Config file missing option error. {err}\\n\" . format ( err = err ) ) sys . exit ( - 1 ) mandatories = [ \"secrets\" , \"credentials\" , \"start\" , \"end\" , \"message\" , ] if not all ( configdata [ mandatory ] for mandatory in mandatories ) : if not options . quiet : sys . stdout . write ( \"Mandatory config option missing\\n\" ) sys . exit ( 0 ) return configdata else : if not options . quiet : sys . stderr . write ( \"ERROR: Config file {config} does not exist\\n\" . format ( config = options . config ) ) sys . exit ( 0 )", "predictions": ["parse the entire configuration ."], "references": ["get settings from config file ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5314, "code": "def get google credentials ( options , config ) : try : if options . get google credentials : flow = flow from clientsecrets ( config [ \"secrets\" ] , scope = SCOPE , redirect uri = \"oob\" ) sys . stdout . write ( \"Follow this URL: {url} and grant access to calendar.\\n\" . format ( url = flow . step1 get authorize url ( ) ) ) token = raw input ( \"Enter token:\" ) credentials = flow . step2 exchange ( token ) storage = Storage ( os . path . join ( config [ \"credentials\" ] , \"{username}.json\" . format ( username = options . username ) ) ) storage . put ( credentials ) credentials . set store ( storage ) else : storage = Storage ( os . path . join ( config [ \"credentials\" ] , \"{username}.json\" . format ( username = options . username ) ) ) credentials = storage . get ( ) except Exception , err : if not options . quiet : sys . stderr . write ( \"ERROR: Getting google API credentials error. {err}\\n\" . format ( err = err ) ) sys . exit ( - 1 ) return credentials", "predictions": ["get credentials for the appying flow ."], "references": ["get google api credentials for user ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 5315, "code": "def create event datetimes ( options , config ) : now = datetime . datetime . now ( ) return { \"start\" : { \"date Time\" : ( now + datetime . timedelta ( minutes = int ( config [ \"start\" ] ) ) ) . strftime ( DT FORMAT ) , \"time Zone\" : options . timezone , } , \"end\" : { \"date Time\" : ( now + datetime . timedelta ( minutes = int ( config [ \"end\" ] ) ) ) . strftime ( DT FORMAT ) , \"time Zone\" : options . timezone , } , }", "predictions": ["create the script with the configuration"], "references": ["create event start and end datetimes ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5316, "code": "def create event ( options , config , credentials ) : try : http = credentials . authorize ( httplib2 . Http ( ) ) service = build ( \"calendar\" , \"v3\" , http = http ) event = { \"summary\" : options . message , \"location\" : \"\" , \"reminders\" : { \"use Default\" : False , \"overrides\" : [ { \"method\" : \"sms\" , \"minutes\" : config [ \"message\" ] , } , ] , } } event . update ( create event datetimes ( options , config ) ) service . events ( ) . insert ( calendar Id = options . calendar , send Notifications = True , body = event ) . execute ( ) except Exception , err : if not options . quiet : sys . stderr . write ( \"ERROR: Creating google calendar event error. {err}\\n\" . format ( err = err ) ) sys . exit ( - 1 )", "predictions": ["data for setting events to the server ."], "references": ["create event in calendar with sms reminder ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 5317, "code": "def main ( ) : options = parse options ( ) config = parse config ( options ) credentials = get google credentials ( options , config ) if not options . get google credentials : create event ( options , config , credentials )", "predictions": ["the main function . this must be called from any command line ."], "references": ["processing notification call main function ."], "bleu": 0.1777835117834348, "rouge_l": 0.3382624768946396}
{"id": 5318, "code": "def parse ( self ) : for tag in self . soup . find All ( 'span' ) : self . create italic ( tag ) self . create strong ( tag ) self . create underline ( tag ) self . unwrap span ( tag ) for tag in self . soup . find All ( 'a' ) : self . remove comments ( tag ) self . check next ( tag ) if self . soup . body : for tag in self . soup . body . find All ( ) : self . remove empty ( tag ) self . remove inline comment ( tag ) self . parse attrs ( tag ) for token , target in self . tokens : self . find token ( tag , token , target ) self . remove blacklisted tags ( tag )", "predictions": ["parse all child maintained by this class and italic content"], "references": ["run all parsing functions ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 5319, "code": "def check next ( self , tag ) : if ( type ( tag . next sibling ) == element . Tag and tag . next sibling . name == 'a' ) : next tag = tag . next sibling if tag . get ( 'href' ) and next tag . get ( 'href' ) : href = self . parse href ( tag . get ( 'href' ) ) next href = self . parse href ( next tag . get ( 'href' ) ) if href == next href : next text = next tag . get text ( ) tag . append ( next text ) self . tags blacklist . append ( next tag )", "predictions": ["check if the tag has a tag . if so , it is only one tag will not be returned ."], "references": ["if next tag is link with same href combine them ."], "bleu": 0.07305267243289862, "rouge_l": 0.26492942453854507}
{"id": 5320, "code": "def create italic ( self , tag ) : style = tag . get ( 'style' ) if style and 'font-style:italic' in style : tag . wrap ( self . soup . new tag ( 'em' ) )", "predictions": ["creates the italic object ."], "references": ["see if span tag has italic style and wrap with em tag ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 5321, "code": "def create strong ( self , tag ) : style = tag . get ( 'style' ) if ( style and ( 'font-weight:bold' in style or 'font-weight:700' in style ) ) : tag . wrap ( self . soup . new tag ( 'strong' ) )", "predictions": ["creates the strong style ."], "references": ["see if span tag has bold style and wrap with strong tag ."], "bleu": 0.06554932163900559, "rouge_l": 0.20573355817875214}
{"id": 5322, "code": "def create underline ( self , tag ) : style = tag . get ( 'style' ) if style and 'text-decoration:underline' in style : tag . wrap ( self . soup . new tag ( 'u' ) )", "predictions": ["creates the full style ."], "references": ["see if span tag has underline style and wrap with u tag ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 5323, "code": "def parse attrs ( self , tag ) : if tag . name in ATTR WHITELIST . keys ( ) : attrs = copy ( tag . attrs ) for attr , value in attrs . items ( ) : if attr in ATTR WHITELIST [ tag . name ] : tag . attrs [ attr ] = self . parse attr ( tag . name , attr , value ) else : del tag . attrs [ attr ] else : tag . attrs = { }", "predictions": ["creates the given attribute of this object ."], "references": ["reject attributes not defined in attr_whitelist ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5324, "code": "def remove empty ( self , tag ) : has children = len ( tag . contents ) has text = len ( list ( tag . stripped strings ) ) if not has children and not has text and not tag . is empty element : tag . extract ( )", "predictions": ["removes the given tag from this list ."], "references": ["remove non - self - closing tags with no children * and * no content ."], "bleu": 0.0589953212431261, "rouge_l": 0.07860824742268041}
{"id": 5325, "code": "def boolean arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to boolean ( obj )", "predictions": ["decides whether the specified object is enough to be a new auto - blocking ."], "references": ["handles literalobjects as well as computable arguments"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 5326, "code": "def number arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to number ( obj )", "predictions": ["the number of parameters that are in the specified message ."], "references": ["handles literalobjects as well as computable arguments"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5327, "code": "def string arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to string ( obj )", "predictions": ["calculates the string representation of the specified object ."], "references": ["handles literalobjects as well as computable arguments"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5328, "code": "def concat ( ctx , * strings ) : strings = flatten ( [ ( s . compute ( ctx ) if callable ( s ) else s ) for s in strings ] ) strings = ( next ( string arg ( ctx , s ) , '' ) for s in strings ) #assert(all(map(lambda x: isinstance(x, str), strings))) #FIXME: Check arg types yield '' . join ( strings )", "predictions": ["concat the given set of strings ."], "references": ["yields one string concatenation of argument strings"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5329, "code": "def starts with ( ctx , full , part ) : full = next ( string arg ( ctx , full ) , '' ) part = next ( string arg ( ctx , part ) , '' ) yield full . startswith ( part )", "predictions": ["create an iterator with the given part of the arguments ."], "references": ["yields one boolean whether the first string starts with the second"], "bleu": 0.16108992769687397, "rouge_l": 0.18181818181818182}
{"id": 5330, "code": "def contains ( ctx , full , part ) : full = next ( string arg ( ctx , full ) , '' ) part = next ( string arg ( ctx , part ) , '' ) yield part in full", "predictions": ["generate an iterator which yields the given part of the string ."], "references": ["yields one boolean whether the first string contains the second"], "bleu": 0.13065113298388567, "rouge_l": 0.2772727272727273}
{"id": 5331, "code": "def check inputs ( self ) : try : = self . inputs [ 0 ] except Type Error : raise Runtime Error ( \"inputs should be iterable but found type='{0}', value=\" \"'{1}'\" . format ( type ( self . inputs ) , str ( self . inputs ) ) ) from melody . inputs import Input for check input in self . inputs : if not isinstance ( check input , Input ) : raise Runtime Error ( \"input should be a subclass of the Input class but \" \"found type='{0}', value='{1}'\" . format ( type ( check input ) , str ( check input ) ) )", "predictions": ["do not reinitialize the actual work of this class ."], "references": ["make some basic checks on the inputs to make sure they are valid"], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 5332, "code": "def check function ( self ) : if not callable ( self . function ) : raise Runtime Error ( \"provided function '{0}' is not callable\" . format ( str ( self . function ) ) ) from inspect import getargspec arg info = getargspec ( self . function ) if len ( arg info . args ) != 1 : print str ( arg info ) raise Runtime Error ( \"provided function should have one argument but found \" \"{0}\" . format ( len ( arg info . args ) ) )", "predictions": ["decorator for derived classes ."], "references": ["make some basic checks on the function to make sure it is valid"], "bleu": 0.04635036983311895, "rouge_l": 0.0}
{"id": 5333, "code": "def recurse ( self , inputs , output , depth , max depth ) : if depth < max depth : for index , option in enumerate ( inputs ) : my output = list ( output ) my output . append ( option ) self . recurse ( inputs [ index + 1 : ] , my output , depth + 1 , max depth ) else : self . options . append ( output )", "predictions": ["recursively extracts all child components from this cluster and returns them ."], "references": ["we work out all combinations using this internal recursion method"], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 5334, "code": "def to string ( obj ) : if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : yield '' elif isinstance ( val , str ) : yield val elif isinstance ( val , node ) : yield strval ( val ) elif isinstance ( val , int ) or isinstance ( val , float ) : yield str ( val ) elif isinstance ( item , bool ) : yield 'true' if item else 'false' else : raise Runtime Error ( 'Unknown type for string conversion: {}' . format ( val ) )", "predictions": ["this function maps obj to string , recursively iterates over each entry ."], "references": ["cast an arbitrary object or sequence to a string type"], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 5335, "code": "def to number ( obj ) : if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : #FIXME: Should be Na N, not 0 yield 0 elif isinstance ( val , str ) : yield float ( val ) elif isinstance ( val , node ) : yield float ( strval ( val ) ) elif isinstance ( val , int ) or isinstance ( val , float ) : yield val else : raise Runtime Error ( 'Unknown type for number conversion: {}' . format ( val ) )", "predictions": ["this function maps obj to a number of suds values"], "references": ["cast an arbitrary object or sequence to a number type"], "bleu": 0.23462350320528, "rouge_l": 0.3}
{"id": 5336, "code": "def to boolean ( obj ) : #if hasattr(obj, ' iter '): if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : yield False elif isinstance ( val , bool ) : yield val elif isinstance ( val , str ) : yield bool ( str ) elif isinstance ( val , node ) : yield True elif isinstance ( val , float ) or isinstance ( val , int ) : yield bool ( val ) else : raise Runtime Error ( 'Unknown type for boolean conversion: {}' . format ( val ) )", "predictions": ["this function will format any partition but it will accept the entire document ."], "references": ["cast an arbitrary sequence to a boolean type"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 5337, "code": "def intersect ( self , other ) : inter = Envelope ( tuple ( self ) ) if inter . intersects ( other ) : mid = len ( other ) // 2 inter . ll = map ( max , inter . ll , other [ : mid ] ) inter . ur = map ( min , inter . ur , other [ mid : ] ) else : inter . ll = ( 0 , 0 ) inter . ur = ( 0 , 0 ) return inter", "predictions": ["compute chi-squared for this set of queries ."], "references": ["returns the intersection of this and another envelope ."], "bleu": 0.16829946711936866, "rouge_l": 0.232824427480916}
{"id": 5338, "code": "def polygon ( self ) : ring = ogr . Geometry ( ogr . wkb Linear Ring ) for coord in self . ll , self . lr , self . ur , self . ul , self . ll : ring . Add Point 2D ( * coord ) polyg = ogr . Geometry ( ogr . wkb Polygon ) polyg . Add Geometry Directly ( ring ) return polyg", "predictions": ["a polygon that has been added to the ( ."], "references": ["returns an ogr geometry for this envelope ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5339, "code": "def from name ( cls , name ) : filename = os . path . join ( package dir , 'data' , name + '.txt' ) return cls . from file ( filename , name )", "predictions": ["returns the filename of this class ."], "references": ["imports a mass table from a file"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5340, "code": "def from file ( cls , filename , name = '' ) : df = pd . read csv ( filename , header = 0 , delim whitespace = True , index col = [ 0 , 1 ] ) [ 'M' ] df . name = name return cls ( df = df , name = name )", "predictions": ["reads a file using the specified filename and retrieve it ."], "references": ["imports a mass table from a file"], "bleu": 0.14991106946711685, "rouge_l": 0.2314990512333966}
{"id": 5341, "code": "def odd even ( self ) : return self . select ( lambda Z , N : ( Z % 2 ) and not ( N % 2 ) , name = self . name )", "predictions": ["create a new norm for this file ."], "references": ["selects odd - even nuclei from the table"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5342, "code": "def even odd ( self ) : return self . select ( lambda Z , N : not ( Z % 2 ) and ( N % 2 ) , name = self . name )", "predictions": ["create a new norm for this file ."], "references": ["selects even - odd nuclei from the table"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5343, "code": "def even even ( self ) : return self . select ( lambda Z , N : not ( Z % 2 ) and not ( N % 2 ) , name = self . name )", "predictions": ["for example , two lines ."], "references": ["selects even - even nuclei from the table"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 5344, "code": "def binding energy ( self ) : M P = 938.2723 M E = 0.5110 M N = 939.5656 AMU = 931.494028 df = self . Z * ( M P + M E ) + ( self . A - self . Z ) * M N - ( self . df + self . A * AMU ) return Table ( df = df , name = 'BE' + '(' + self . name + ')' )", "predictions": ["returns a parse parse action ."], "references": ["return binding energies instead of mass excesses"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5345, "code": "def s2n ( self ) : M N = 8.0713171 f = lambda parent , daugther : - parent + daugther + 2 * M N return self . derived ( 's2n' , ( 0 , - 2 ) , f )", "predictions": ["returns a kd - tree of the given list of ( ."], "references": ["return 2 neutron separation energy"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 5346, "code": "def s1n ( self ) : M N = 8.0713171 f = lambda parent , daugther : - parent + daugther + M N return self . derived ( 's1n' , ( 0 , - 1 ) , f )", "predictions": ["returns a sorted list of all ( of this . ."], "references": ["return 1 neutron separation energy"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5347, "code": "def s2p ( self ) : M P = 7.28897050 f = lambda parent , daugther : - parent + daugther + 2 * M P return self . derived ( 's2p' , ( - 2 , 0 ) , f )", "predictions": ["returns a new approximation of this ( ."], "references": ["return 2 proton separation energy"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5348, "code": "def s1p ( self ) : M P = 7.28897050 f = lambda parent , daugther : - parent + daugther + M P return self . derived ( 's1p' , ( - 1 , 0 ) , f )", "predictions": ["returns a new ( at the end of this ( ."], "references": ["return 1 proton separation energy"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5349, "code": "def derived ( self , name , relative coords , formula ) : rel Z , rel N = relative coords daughter idx = [ ( x [ 0 ] + rel Z , x [ 1 ] + rel N ) for x in self . df . index ] values = formula ( self . df . values , self . df . loc [ daughter idx ] . values ) return Table ( df = pd . Series ( values , index = self . df . index , name = name + '(' + self . name + ')' ) )", "predictions": ["returns a list of intersection components for this complex using the given config"], "references": ["helper function for derived quantities"], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 5350, "code": "def derive key ( self , master password ) : encoder = encoding . Encoder ( self . charset ) bytes = ( '%s:%s' % ( master password , self . name ) ) . encode ( 'utf8' ) start time = time . clock ( ) digest = scrypt . hash ( bytes , self . salt , N = 1 << 14 , r = 8 , p = 1 ) key = encoder . encode ( digest , self . key length ) derivation time in s = time . clock ( ) - start time logger . debug ( 'Key derivation took %.2fms' , derivation time in s * 1000 ) return key", "predictions": ["parse and return for the given for the for the for the specified for the for the for the for the for the for the specified for the for the for the for the for the for message ."], "references": ["computes the key from the salt and the master password ."], "bleu": 0.040668703998563635, "rouge_l": 0.17797228300510576}
{"id": 5351, "code": "def search ( self , query ) : results = self . session . query ( Domain ) . filter ( Domain . name . ilike ( '%%%s%%' % query ) ) . all ( ) return results", "predictions": ["searches for a certain , ensuring that the , as a , from the , and returns the : name ."], "references": ["search the database for the given query . will find partial matches ."], "bleu": 0.07305267243289862, "rouge_l": 0.18429003021148035}
{"id": 5352, "code": "def srid ( self ) : epsg id = ( self . Get Authority Code ( 'PROJCS' ) or self . Get Authority Code ( 'GEOGCS' ) ) try : return int ( epsg id ) except Type Error : return", "predictions": ["get the tag of this map ."], "references": ["returns the epsg id as int if it exists ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 5353, "code": "def main ( ) : args = get args ( ) ret code = args . target ( args ) logger . debug ( 'Exiting with code %d' , ret code ) sys . exit ( ret code )", "predictions": ["main entry point for the launcher ."], "references": ["main entry point for the cli ."], "bleu": 0.7071067811865475, "rouge_l": 0.8571428571428571}
{"id": 5354, "code": "def update file ( url , filename ) : resp = urlopen ( url ) if resp . code != 200 : raise Exception ( 'GET {} failed.' . format ( url ) ) with open ( get package path ( filename ) , 'w' ) as fp : for l in resp : if not l . startswith ( b'#' ) : fp . write ( l . decode ( 'utf8' ) ) print ( 'Updated {}' . format ( filename ) )", "predictions": ["create a underline underline ."], "references": ["update the content of a single file ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 5355, "code": "def driver ( self ) : if self . driver is None : self . driver = Image Driver ( self . ds . Get Driver ( ) ) return self . driver", "predictions": ["get a parse parse parse image ."], "references": ["returns the underlying imagedriver instance ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 5356, "code": "def lookup alphabet ( charset ) : if charset in PRESETS : return PRESETS [ charset ] if len ( charset ) < 16 : logger . warning ( 'very small alphabet in use, possibly a failed lookup?' ) return charset", "predictions": ["remove the empty empty self - self - value from the self - self - 8 ."], "references": ["retrieves a named charset or treats the input as a custom alphabet and use that"], "bleu": 0.07223943354597204, "rouge_l": 0.06321243523316063}
{"id": 5357, "code": "def chunk to long ( self , chunk ) : return sum ( [ 256 ** ( self . chunklen [ 0 ] - 1 - i ) * ord byte ( chunk [ i ] ) for i in range ( self . chunklen [ 0 ] ) ] )", "predictions": ["takes a boolean array of contents and returns a ( i . e . the second boolean ) ."], "references": ["parses a chunk of bytes to integer using big - endian representation"], "bleu": 0.0712695567709093, "rouge_l": 0.1345093715545755}
{"id": 5358, "code": "def get chunk ( self , data , index ) : return data [ index * self . chunklen [ 0 ] : ( index + 1 ) * self . chunklen [ 0 ] ]", "predictions": ["number of words for arg ."], "references": ["partition the data into chunks and retrieve the chunk at the given index"], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 5359, "code": "def memoize ( func ) : cache = { } @ wraps ( func ) def inner ( filename ) : if filename not in cache : cache [ filename ] = func ( filename ) return cache [ filename ] return inner", "predictions": ["string per obj if more than one of the obj is true ."], "references": ["cache result of function call ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 5360, "code": "def regexp ( filename ) : lines = get resource content ( filename ) . decode ( 'utf-8' ) . splitlines ( ) return re . compile ( '|' . join ( lines ) )", "predictions": ["else else return the concat for the client ."], "references": ["get a list of patterns from a file and make a regular expression ."], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 5361, "code": "def detect timezone ( ) : default timezone = 'America/New York' locale code = locale . getdefaultlocale ( ) return default timezone if not locale code [ 0 ] else str ( pytz . country timezones [ locale code [ 0 ] [ - 2 : ] ] [ 0 ] )", "predictions": ["starts a given with the iso 8601 = ( ."], "references": ["get timezone as set by the system"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 5362, "code": "def to dict ( self ) : result = { } for attr , in iteritems ( self . swagger types ) : value = getattr ( self , attr ) if isinstance ( value , list ) : result [ attr ] = list ( map ( lambda x : x . to dict ( ) if hasattr ( x , \"to dict\" ) else x , value ) ) elif hasattr ( value , \"to dict\" ) : result [ attr ] = value . to dict ( ) else : result [ attr ] = value return result", "predictions": ["transforms this object to a dictionary ."], "references": ["returns the model properties as a dict"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5363, "code": "def activate pdb hook ( ) : def debug exception ( type exception , value , tb ) : import pdb pdb . post mortem ( tb ) import sys sys . excepthook = debug exception", "predictions": ["create an exception object ."], "references": ["catch exceptions with a prompt for post - mortem analyzis"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 5364, "code": "def worker main ( job handler , host , port ) : loop = asyncio . new event loop ( ) asyncio . set event loop ( None ) loop . run until complete ( handle jobs ( job handler , host , port , loop = loop ) ) loop . close ( )", "predictions": ["creates a new check that connects to the check ."], "references": ["starts an asyncio event loop to connect to the master and run jobs ."], "bleu": 0.11950151429308975, "rouge_l": 0.24270557029177717}
{"id": 5365, "code": "def send message ( self , msg ) : LW Link . the queue . put nowait ( msg ) if LW Link . thread is None or not LW Link . thread . is Alive ( ) : LW Link . thread = Thread ( target = self . send queue ) LW Link . thread . start ( )", "predictions": ["sends the specified request to the ui thread ."], "references": ["add message to queue and start processing the queue ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 5366, "code": "def turn on light ( self , device id , name ) : msg = \"!%s Fd P32|Turn On|%s\" % ( device id , name ) self . send message ( msg )", "predictions": ["to to to to to to to to to to to update the list of . ."], "references": ["create the message to turn light on ."], "bleu": 0.0859076483566362, "rouge_l": 0.17110799438990182}
{"id": 5367, "code": "def turn on switch ( self , device id , name ) : msg = \"!%s F1|Turn On|%s\" % ( device id , name ) self . send message ( msg )", "predictions": ["to to to to to to to ( ( ( isinstance ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["create the message to turn switch on ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 5368, "code": "def turn on with brightness ( self , device id , name , brightness ) : brightness value = round ( ( brightness * 31 ) / 255 ) + 1 msg = \"!%s Fd P%d|Lights %d|%s\" % ( device id , brightness value , brightness value , name ) self . send message ( msg )", "predictions": ["to to to to to to to to to to to notification ."], "references": ["scale brightness from 0 .. 255 to 1 .. 32 ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 5369, "code": "def turn off ( self , device id , name ) : msg = \"!%s F0|Turn Off|%s\" % ( device id , name ) self . send message ( msg )", "predictions": ["intersect the . and passes it to the list of ."], "references": ["create the message to turn light or switch off ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 5370, "code": "def send queue ( self ) : while not LW Link . the queue . empty ( ) : self . send reliable message ( LW Link . the queue . get nowait ( ) )", "predictions": ["polygon the job to the ( representing the diagnostic ( if the ( 0 , y : z : 1 : 12 : 12 : 12 : 12 : 12 : 12 : 12 : 12 : 12 : 12 , 12 , 12 , 12 , 12 , 12"], "references": ["if the queue is not empty process the queue ."], "bleu": 0.033984283835209204, "rouge_l": 0.07577639751552796}
{"id": 5371, "code": "def send reliable message ( self , msg ) : result = False max retries = 15 trans id = next ( LW Link . transaction id ) msg = \"%d,%s\" % ( trans id , msg ) try : with socket . socket ( socket . AF INET , socket . SOCK DGRAM ) as write sock , socket . socket ( socket . AF INET , socket . SOCK DGRAM ) as read sock : write sock . setsockopt ( socket . SOL SOCKET , socket . SO REUSEADDR , 1 ) read sock . setsockopt ( socket . SOL SOCKET , socket . SO BROADCAST , 1 ) read sock . settimeout ( self . SOCKET TIMEOUT ) read sock . bind ( ( '0.0.0.0' , self . RX PORT ) ) while max retries : max retries -= 1 write sock . sendto ( msg . encode ( 'UTF-8' ) , ( LW Link . link ip , self . TX PORT ) ) result = False while True : response , dummy = read sock . recvfrom ( 1024 ) response = response . decode ( 'UTF-8' ) if \"Not yet registered.\" in response : LOGGER . error ( \"Not yet registered\" ) self . register ( ) result = True break if response . startswith ( \"%d,OK\" % trans id ) : result = True break if response . startswith ( \"%d,ERR\" % trans id ) : LOGGER . error ( response ) break LOGGER . info ( response ) if result : break time . sleep ( 0.25 ) except socket . timeout : LOGGER . error ( \"LW broker timeout!\" ) return result except Exception as ex : LOGGER . error ( ex ) raise if result : LOGGER . info ( \"LW broker OK!\" ) else : LOGGER . error ( \"LW broker fail!\" ) return result", "predictions": ["sends a ( i . e . , the results of the ( filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename"], "references": ["send msg to lightwaverf hub ."], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 5372, "code": "def reset ( self ) : for opt , meta in self . defaults ( ) : self [ opt ] = meta . default", "predictions": ["resets the model state for the object ."], "references": ["restore default values of options in this section ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5373, "code": "def names ( section , option ) : meta = section . def [ option ] action = meta . cmd kwargs . get ( 'action' ) if action is internal . Switch : names = [ '-{}' . format ( option ) , '+{}' . format ( option ) ] if meta . shortname is not None : names . append ( '-{}' . format ( meta . shortname ) ) names . append ( '+{}' . format ( meta . shortname ) ) else : names = [ '--{}' . format ( option ) ] if meta . shortname is not None : names . append ( '-{}' . format ( meta . shortname ) ) return names", "predictions": ["build a nodelist command describing an ) action ."], "references": ["list of cli strings for a given option ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5374, "code": "def cmd opts solver ( self , cmd name ) : sections = self . sections list ( cmd name ) cmd dict = self . opt cmds [ cmd name ] if cmd name else self . opt bare for sct in reversed ( sections ) : for opt , opt meta in self . conf [ sct ] . def . items ( ) : if not opt meta . cmd arg : continue if opt not in cmd dict : cmd dict [ opt ] = sct else : warnings . warn ( 'Command <{0}>: {1}.{2} shadowed by {3}.{2}' . format ( cmd name , sct , opt , cmd dict [ opt ] ) , error . Loam Warning , stacklevel = 4 )", "predictions": ["finder result for lambda ."], "references": ["scan options related to one command and enrich _opt_cmds ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 5375, "code": "def add options to parser ( self , opts dict , parser ) : store bool = ( 'store true' , 'store false' ) for opt , sct in opts dict . items ( ) : meta = self . conf [ sct ] . def [ opt ] kwargs = copy . deepcopy ( meta . cmd kwargs ) action = kwargs . get ( 'action' ) if action is internal . Switch : kwargs . update ( nargs = 0 ) elif meta . default is not None and action not in store bool : kwargs . setdefault ( 'type' , type ( meta . default ) ) kwargs . update ( help = meta . help ) kwargs . setdefault ( 'default' , self . conf [ sct ] [ opt ] ) parser . add argument ( * names ( self . conf [ sct ] , opt ) , * * kwargs )", "predictions": ["even if we have a ( ."], "references": ["add options to a parser ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5376, "code": "async def start master ( host = \"\" , port = 48484 , * , loop = None ) : loop = loop if loop is not None else asyncio . get event loop ( ) manager = jobs . Job Manager ( loop = loop ) workers = set ( ) server = await loop . create server ( lambda : Worker Protocol ( manager , workers ) , host , port ) return Master ( server , manager , workers , loop = loop )", "predictions": ["starts a new server ."], "references": ["starts a new highfive master at the given host and port and returns it ."], "bleu": 0.07271361304044333, "rouge_l": 0.3669172932330827}
{"id": 5377, "code": "def run ( self , job list ) : if self . closed : raise Runtime Error ( \"master is closed\" ) return self . manager . add job set ( job list )", "predictions": ["run a single job . this method is called on the ui thread ."], "references": ["runs a job set which consists of the jobs in an iterable job list ."], "bleu": 0.10348692758173474, "rouge_l": 0.2741573033707865}
{"id": 5378, "code": "def add ( self , result ) : assert not self . complete self . results . append ( result ) self . change ( )", "predictions": ["default implementation of synchronized method ."], "references": ["adds a new result ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5379, "code": "def done ( self ) : self . results . complete ( ) waiters = self . waiters for waiter in waiters : waiter . set result ( None ) self . manager . job set done ( self )", "predictions": ["decorator for a method that first flushes the queue ."], "references": ["marks the job set as completed and notifies all waiting tasks ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 5380, "code": "def get job ( self , callback ) : assert not self . closed if self . active js is None or not self . active js . job available ( ) : self . ready callbacks . append ( callback ) else : job = self . active js . get job ( ) self . job sources [ job ] = self . active js callback ( job )", "predictions": ["a job for the latest job ."], "references": ["calls the given callback function when a job becomes available ."], "bleu": 0.15685718045401453, "rouge_l": 0.32049036777583184}
{"id": 5381, "code": "def return job ( self , job ) : if self . closed : return js = self . job sources [ job ] if len ( self . ready callbacks ) > 0 : callback = self . ready callbacks . popleft ( ) callback ( job ) else : del self . job sources [ job ] js . return job ( job )", "predictions": ["a utility for finding a job that has been run ."], "references": ["returns a job to its source job set to be run again later ."], "bleu": 0.1296737111713457, "rouge_l": 0.31322207958921694}
{"id": 5382, "code": "def uniquify ( list ) : seen = set ( ) result = [ ] for x in list : if x not in seen : result . append ( x ) seen . add ( x ) return result", "predictions": ["make a list from this list ."], "references": ["remove duplicates in a list ."], "bleu": 0.29071536848410967, "rouge_l": 0.4680306905370844}
{"id": 5383, "code": "def get region ( ) : global REGION if REGION is None : region name = os . getenv ( \"AWS DEFAULT REGION\" ) or \"us-east-1\" region dict = { r . name : r for r in boto . regioninfo . get regions ( \"ec2\" ) } if region name not in region dict : raise Value Error ( \"No such EC2 region: {}. Check AWS DEFAULT REGION \" \"environment variable\" . format ( region name ) ) REGION = region dict [ region name ] return REGION", "predictions": ["get the global region to use for this region ."], "references": ["use the environment to get the current region"], "bleu": 0.1972940627795883, "rouge_l": 0.34014869888475835}
{"id": 5384, "code": "def sort by ( cls , entries , attribute ) : def key ( entry ) : return entry . get attrib ( attribute , convert to str = True ) return sorted ( entries , key = key )", "predictions": ["sorts the specified attribute with the specified key and key . if the method is found , returns the sorted map ."], "references": ["sorts a list of entries by the given attribute ."], "bleu": 0.06964541799727335, "rouge_l": 0.2681318681318681}
{"id": 5385, "code": "def add timestamp ( logger class , log method , event dict ) : event dict [ 'timestamp' ] = calendar . timegm ( time . gmtime ( ) ) return event dict", "predictions": ["adds a timestamp to the log ."], "references": ["attach the event time as unix epoch"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5386, "code": "def logger ( name = name , output = None , uuid = False , timestamp = False ) : processors = [ ] if output == 'json' : processors . append ( structlog . processors . JSON Renderer ( ) ) if uuid : processors . append ( add unique id ) if uuid : processors . append ( add timestamp ) return structlog . wrap logger ( logbook . Logger ( name ) , processors = processors )", "predictions": ["generate a logger for the given name and timestamp ."], "references": ["configure and return a new logger for hivy modules"], "bleu": 0.18850319022747347, "rouge_l": 0.31881533101045295}
{"id": 5387, "code": "def setup ( title , output = 'json' , timezone = None ) : timezone = timezone or dna . time utils . detect timezone ( ) broker url = 'redis://{}:{}/{}' . format ( os . environ . get ( 'BROKER HOST' , 'localhost' ) , os . environ . get ( 'BROKER PORT' , 6379 ) , 0 ) app = Celery ( title , broker = broker url ) app . conf . update ( CELERY TASK SERIALIZER = output , CELERY ACCEPT CONTENT = [ output ] , CELERY RESULT SERIALIZER = output , CELERY RESULT BACKEND = broker url , CELERY TIMEZONE = timezone , CELERYD FORCE EXECV = True , CELERY ENABLE UTC = True , CELERY IGNORE RESULT = False ) return app", "predictions": ["sets up a new video - time limit ."], "references": ["implement celery workers using json and redis"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5388, "code": "def delete ( self , worker id ) : code = 200 if worker id in self . jobs : self . jobs [ worker id ] [ 'worker' ] . revoke ( terminate = True ) report = { 'id' : worker id , 'revoked' : True } self . jobs . pop ( worker id ) else : report = { 'error' : 'job {} unknown' . format ( worker id ) } code = 404 return flask . jsonify ( report ) , code", "predictions": ["deletes an existing worker from a worker and returns it as a result ."], "references": ["stop and remove a worker"], "bleu": 0.1250076305588977, "rouge_l": 0.23018867924528305}
{"id": 5389, "code": "def color ( number ) : if supports 256 ( ) : template = \"\\033[38;5;{number}m{text}\\033[0m\" else : template = \"\\033[{number}m{text}\\033[0m\" def color ( text ) : if not all ( [ sys . stdout . isatty ( ) , sys . stderr . isatty ( ) ] ) : return text else : return template . format ( number = number , text = text ) return color", "predictions": ["creates and returns a template for the specified number of repetitions ."], "references": ["returns a function that colors a string with a number from 0 to 255 ."], "bleu": 0.12100325656200607, "rouge_l": 0.29047619047619044}
{"id": 5390, "code": "def get color hash ( string , min = MIN COLOR BRIGHT , max = MAX COLOR BRIGHT ) : hash num = int ( hashlib . sha1 ( string . encode ( 'utf-8' ) ) . hexdigest ( ) [ : 6 ] , 16 ) range = max - min num in range = hash num % range return color ( min + num in range )", "predictions": ["returns a hash from a string ."], "references": ["hashes a string and returns a number between min and max ."], "bleu": 0.15749996500436228, "rouge_l": 0.30148270181219106}
{"id": 5391, "code": "def random color ( min = MIN COLOR , max = MAX COLOR ) : return color ( random . randint ( min , max ) )", "predictions": ["returns a random integer between the specified maximum and max ."], "references": ["returns a random color between min and max ."], "bleu": 0.317023313852343, "rouge_l": 0.7128547579298832}
{"id": 5392, "code": "def requires token auth ( resource ) : @ functools . wraps ( resource ) def decorated ( * args , * * kwargs ) : ''' Check provided token ''' token = flask . request . headers . get ( 'Authorization' ) user = check token ( token ) if not token or user is None : log . warn ( 'authentification failed' , token = token ) return auth failed ( ) flask . g . user = user log . info ( 'authentification succeeded' , token = token , user = flask . g . user ) return resource ( * args , * * kwargs ) return decorated", "predictions": ["handles a resource , and caches the user for the resource ."], "references": ["flask decorator protecting ressources using token scheme"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 5393, "code": "def is running ( process ) : try : pgrep = sh . Command ( '/usr/bin/pgrep' ) pgrep ( process ) flag = True except sh . Error Return Code 1 : flag = False return flag", "predictions": ["checks if server is running on or not ."], "references": ["pgrep returns an error code if no process was found"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 5394, "code": "def dynamic import ( mod path , obj name = None ) : try : module = import ( mod path , fromlist = [ 'whatever' ] ) except Import Error , error : raise errors . Dynamic Import Failed ( module = '.' . join ( [ mod path , obj name ] ) , reason = error ) reload ( module ) if obj name is None : obj = module elif hasattr ( module , obj name ) : obj = getattr ( module , obj name ) else : raise errors . Dynamic Import Failed ( module = '.' . join ( [ mod path , obj name ] ) , reason = 'module {} has no attribute {}' . format ( module . name , obj name ) ) return None return obj", "predictions": ["imports all attributes of the module and imports them in the module ."], "references": ["take a string and return the corresponding module"], "bleu": 0.1135935489027116, "rouge_l": 0.2985318107667211}
{"id": 5395, "code": "def self ip ( public = False ) : try : if public : data = str ( urlopen ( 'http://checkip.dyndns.com/' ) . read ( ) ) ip addr = re . compile ( r'Address: (\\d+\\.\\d+\\.\\d+\\.\\d+)' ) . search ( data ) . group ( 1 ) else : sock = socket . socket ( socket . AF INET , socket . SOCK DGRAM ) sock . connect ( ( 'google.com' , 0 ) ) ip addr = sock . getsockname ( ) [ 0 ] except Exception , error : print ( 'Online test failed : {}' . format ( error ) ) raise return ip addr", "predictions": ["tries to connect to the server ."], "references": ["utility for logbook information injection"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5396, "code": "def request ( self , method , url , query params = None , headers = None , post params = None , body = None ) : if method == \"GET\" : return self . rest client . GET ( url , query params = query params , headers = headers ) elif method == \"HEAD\" : return self . rest client . HEAD ( url , query params = query params , headers = headers ) elif method == \"OPTIONS\" : return self . rest client . OPTIONS ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == \"POST\" : return self . rest client . POST ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == \"PUT\" : return self . rest client . PUT ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == \"PATCH\" : return self . rest client . PATCH ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == \"DELETE\" : return self . rest client . DELETE ( url , query params = query params , headers = headers ) else : raise Value Error ( \"http method must be `GET`, `HEAD`,\" \" `POST`, `PATCH`, `PUT` or `DELETE`.\" )", "predictions": ["request to the oauth2 http request ."], "references": ["makes the http request using restclient ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 5397, "code": "def serve ( self , app docopt = DEFAULT DOC , description = '' ) : exit status = 0 if isinstance ( app docopt , str ) : args = docopt ( app docopt , version = description ) elif isinstance ( app docopt , dict ) : args = app docopt else : raise Value Error ( 'unknown configuration object ({})' . format ( type ( app docopt ) ) ) log level = args . get ( '--log' , 'debug' ) is debug = args . get ( '--debug' , False ) log output = 'stdout' if is debug else 'apy.log' safe bind = args . get ( '--bind' , '127.0.0.1' ) safe port = int ( args . get ( '--port' , 5000 ) ) log setup = dna . logging . setup ( level = log level , output = log output ) with log setup . applicationbound ( ) : try : log . info ( 'server ready' , version = description , log = log level , debug = is debug , bind = '{}:{}' . format ( safe bind , safe port ) ) self . app . run ( host = safe bind , port = safe port , debug = is debug ) except Exception as error : if is debug : raise log . error ( '{}: {}' . format ( type ( error ) . name , str ( error ) ) ) exit status = 1 finally : log . info ( 'session ended with status {}' . format ( exit status ) ) return exit status", "predictions": ["this will run once the server has been set , or the server will get all the ( and the exit"], "references": ["configure from cli and run the server"], "bleu": 0.08687475782716618, "rouge_l": 0.23552123552123552}
{"id": 5398, "code": "def render ( self , name , value , attrs = None ) : context = attrs or { } context . update ( { 'name' : name , 'value' : value , } ) return render to string ( self . template name , context )", "predictions": ["this method is called by the linework class for creating the stringbuffer ."], "references": ["include a hidden input to stored the serialized upload value ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 5399, "code": "def networkdays ( from date , to date , locale = 'en-US' ) : holidays = locales [ locale ] return workdays . networkdays ( from date , to date , holidays )", "predictions": ["converts the given date to the local database ."], "references": ["return the net work days according to rh s calendar ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 5400, "code": "def get path ( cmd ) : if cmd in PATHS : return PATHS [ cmd ] out = subprocess . check output ( 'which {}' . format ( cmd ) , shell = True ) PATHS [ cmd ] = out . decode ( \"utf-8\" ) . strip ( ) return PATHS [ cmd ]", "predictions": ["get the full path of a path as a single letter ."], "references": ["queries bash to find the path to a commmand on the system ."], "bleu": 0.12020484516681697, "rouge_l": 0.31770833333333337}
{"id": 5401, "code": "def build ssh command ( hostname , username , idfile , ssh command , tunnel ) : command = [ get path ( 'ssh' ) , '-o' , 'Strict Host Key Checking=no' , '-o' , 'Connect Timeout=5' ] if idfile is not None : command . extend ( [ '-i' , idfile ] ) if tunnel is not None : command . extend ( [ '-A' , '-t' , tunnel , 'ssh' , '-A' , '-t' ] ) if username is not None : command . append ( '{}@{}' . format ( username , hostname ) ) else : command . append ( hostname ) if ssh command is not None : command . append ( repr ( ssh command ) ) return ( ' ' . join ( command ) )", "predictions": ["builds a full command string for identifying fields on the command line ."], "references": ["uses hostname and other info to construct an ssh command ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 5402, "code": "def load ( cls , profile name = None ) : lsi location = os . path . expanduser ( '~/.lsi' ) if not os . path . exists ( lsi location ) : return Lsi Profile ( ) cfg parser = Config Parser ( ) cfg parser . read ( lsi location ) if profile name is None : if cfg parser . has section ( 'default' ) : profile name = 'default' else : return cls ( ) elif not cfg parser . has section ( profile name ) : raise cls . Load Error ( 'No such profile {}' . format ( profile name ) ) def get ( option , alt = None ) : \"\"\"Gets an option if it exists; else returns `alt`.\"\"\" if cfg parser . has option ( profile name , option ) : return cfg parser . get ( profile name , option ) else : return alt if cfg parser . has option ( profile name , 'inherit' ) : profile = cls . load ( cfg parser . get ( profile name , 'inherit' ) ) else : profile = cls ( ) profile . override ( 'username' , get ( 'username' ) ) profile . override ( 'identity file' , get ( 'identity file' ) ) profile . override ( 'command' , get ( 'command' ) ) filters = [ s for s in get ( 'filters' , '' ) . split ( ',' ) if len ( s ) > 0 ] exclude = [ s for s in get ( 'exclude' , '' ) . split ( ',' ) if len ( s ) > 0 ] profile . filters . extend ( filters ) profile . exclude . extend ( exclude ) return profile", "predictions": ["load a lsi suitable for use in a given parser"], "references": ["loads the user s lsi profile or provides a default ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 5403, "code": "def from args ( args ) : if args . username is not None or args . identity file is not None : profile = Lsi Profile ( ) else : profile = Lsi Profile . load ( args . profile ) profile . override ( 'username' , args . username ) profile . override ( 'identity file' , args . identity file ) profile . override ( 'command' , args . command ) profile . no prompt = args . no prompt profile . filters . extend ( args . filters ) profile . exclude . extend ( args . exclude ) if profile . identity file is not None : profile . identity file = os . path . expanduser ( profile . identity file ) return profile", "predictions": ["returns a new file in cache ."], "references": ["takes arguments parsed from argparse and returns a profile ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 5404, "code": "def relate ( self , part , id = None ) : assert part . name . startswith ( self . base ) name = part . name [ len ( self . base ) : ] . lstrip ( '/' ) rel = Relationship ( self , name , part . rel type , id = id ) self . relationships . add ( rel ) return rel", "predictions": ["a method that creates a class containing this class ."], "references": ["relate this package component to the supplied part ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 5405, "code": "def related ( self , reltype ) : parts = [ ] package = getattr ( self , 'package' , None ) or self for rel in self . relationships . types . get ( reltype , [ ] ) : parts . append ( package [ posixpath . join ( self . base , rel . target ) ] ) return parts", "predictions": ["generate a related to this layout ."], "references": ["return a list of parts related to this one via reltype ."], "bleu": 0.1873000789958672, "rouge_l": 0.5024711696869852}
{"id": 5406, "code": "def load rels ( self , source ) : self . relationships . load ( source = self , data = source )", "predictions": ["loads the given source ."], "references": ["load relationships from source xml ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 5407, "code": "def load part ( self , rel type , name , data ) : if self . content types . find for ( name ) is None : log . warning ( 'no content type found for part %(name)s' % vars ( ) ) return cls = Part . classes by rel type [ rel type ] part = cls ( self , name ) part . load ( data ) self [ name ] = part return part", "predictions": ["load a part at a given name"], "references": ["load a part into this package based on its relationship type"], "bleu": 0.1952347922420459, "rouge_l": 0.32049036777583184}
{"id": 5408, "code": "def find for ( self , name ) : map = self . items return map . get ( name , None ) or map . get ( get ext ( name ) or None , None )", "predictions": ["searches for this port and returns the element that matches the given = = null ."], "references": ["get the correct content type for a given name"], "bleu": 0.09147827112247602, "rouge_l": 0.16850828729281767}
{"id": 5409, "code": "def from element ( cls , element ) : ns , class name = parse tag ( element . tag ) class = getattr ( Content Type , class name ) if not class : msg = 'Invalid Types child element: %(class name)s' % vars ( ) raise Value Error ( msg ) key = element . get ( class . key name ) name = element . get ( 'Content Type' ) return class ( name , key )", "predictions": ["create a ( instance from an ( object list list list list list list list list list list list list list list list list list of fields currently used to convert an ( class to a given ( using the given ( list list list list list list list list"], "references": ["given an element parse out the proper contenttype"], "bleu": 0.028577262451992175, "rouge_l": 0.07932379713914176}
{"id": 5410, "code": "def as stream ( self ) : stream = io . Bytes IO ( ) self . store ( stream ) stream . seek ( 0 ) return stream", "predictions": ["get the dataset at the given ( but is between the ( or append result result result result result result result result result result result result result result result result result result result result result result is . self ."], "references": ["return a zipped package as a readable stream"], "bleu": 0.025326605584447867, "rouge_l": 0.0}
{"id": 5411, "code": "def loud ( self , lang = 'englist' ) : lang method = getattr ( self , lang , None ) if lang method : return lang method ( ) . upper ( ) else : return self . english ( ) . upper ( )", "predictions": ["convenience method for creating a done = targets ."], "references": ["speak loudly! five! use upper case!"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5412, "code": "def upload ( ctx , product , git ref , dirname , aws id , aws secret , ci env , on travis push , on travis pr , on travis api , on travis cron , skip upload ) : logger = logging . get Logger ( name ) if skip upload : click . echo ( 'Skipping ltd upload.' ) sys . exit ( 0 ) logger . debug ( 'CI environment: %s' , ci env ) logger . debug ( 'Travis events settings. ' 'On Push: %r, PR: %r, API: %r, Cron: %r' , on travis push , on travis pr , on travis api , on travis cron ) if ci env == 'travis' and should skip travis event ( on travis push , on travis pr , on travis api , on travis cron ) : sys . exit ( 0 ) ensure login ( ctx ) git refs = get git refs ( ci env , git ref ) build resource = register build ( ctx . obj [ 'keeper hostname' ] , ctx . obj [ 'token' ] , product , git refs ) logger . debug ( 'Created build resource %r' , build resource ) upload dir ( build resource [ 'bucket name' ] , build resource [ 'bucket root dir' ] , dirname , aws access key id = aws id , aws secret access key = aws secret , surrogate key = build resource [ 'surrogate key' ] , cache control = 'max-age=31536000' , surrogate control = None , upload dir redirect objects = True ) logger . debug ( 'Upload complete for %r' , build resource [ 'self url' ] ) confirm build ( build resource [ 'self url' ] , ctx . obj [ 'token' ] ) logger . debug ( 'Build %r complete' , build resource [ 'self url' ] )", "predictions": ["uploads a , or get the 'build ."], "references": ["upload a new site build to lsst the docs ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 5413, "code": "def part edit cmd ( ) : parser = argparse . Argument Parser ( description = inspect . getdoc ( part edit cmd ) ) parser . add argument ( 'path' , help = 'Path to part (including path to zip file, i.e. ./file.zipx/part)' , ) parser . add argument ( '--reformat-xml' , action = 'store true' , help = ( 'run the content through an XML pretty-printer ' 'first for improved editability' ) , ) args = parser . parse args ( ) part edit ( args . path , args . reformat xml )", "predictions": ["creates a new return of the command ."], "references": ["edit a part from an ooxml package without unzipping it"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 5414, "code": "def pack dir cmd ( ) : parser = argparse . Argument Parser ( description = inspect . getdoc ( part edit cmd ) ) parser . add argument ( 'path' , help = ( 'Path to list (including path to zip file, ' 'i.e. ./file.zipx or ./file.zipx/subdir)' ) , ) args = parser . parse args ( ) for item , is file in sorted ( list contents ( args . path ) ) : prefix = 'd ' if not is file else '  ' msg = prefix + item print ( msg )", "predictions": ["create ( . adds ( to ( ."], "references": ["list the contents of a subdirectory of a zipfile"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 5415, "code": "def process module ( self , node ) : if self . config . file header : if sys . version info [ 0 ] < 3 : pattern = re . compile ( '\\A' + self . config . file header , re . LOCALE | re . MULTILINE ) else : pattern = re . compile ( '\\A' + self . config . file header , re . MULTILINE ) content = None with node . stream ( ) as stream : content = stream . read ( ) . decode ( 'utf-8' ) matches = pattern . findall ( content ) if len ( matches ) != 1 : self . add message ( 'invalid-file-header' , 1 , args = self . config . file header )", "predictions": ["construct and , this only makes sense when the global global global global global global global configuration is empty ."], "references": ["process the astroid node stream ."], "bleu": 0.06760229884571738, "rouge_l": 0.17039106145251398}
{"id": 5416, "code": "def html ( self , slug , name , chart obj , filepath = None , html before = \"\" , html after = \"\" ) : try : html = \"\" if name : html = \"<h3>\" + name + \"</h3>\" json data = chart obj . to json ( ) json data = self . patch json ( json data ) html = html before + html + self . json to html ( slug , json data ) + html after except Exception as e : tr . new ( e ) tr . check ( ) if filepath is not None : self . write file ( slug , filepath , html ) return None else : return html", "predictions": ["associates the sort operation with the document ."], "references": ["generate html from an altair chart object and optionally write it to a file"], "bleu": 0.0636991580240073, "rouge_l": 0.0}
{"id": 5417, "code": "def patch json ( self , json data ) : json data = json . loads ( json data ) json data [ \"$schema\" ] = \"https://vega.github.io/schema/vega-lite/2.0.0-beta.15.json\" json data [ \"width\" ] = json data [ \"config\" ] [ \"cell\" ] [ \"width\" ] json data [ \"height\" ] = json data [ \"config\" ] [ \"cell\" ] [ \"height\" ] del ( json data [ \"config\" ] [ \"cell\" ] ) return json . dumps ( json data )", "predictions": ["this method will convert list of timestamp to a timestamp ."], "references": ["patch the altair generated json to the newest vega lite spec"], "bleu": 0.11390778025531027, "rouge_l": 0.09090909090909091}
{"id": 5418, "code": "def json to html ( self , slug , json data ) : html = '<div id=\"chart-' + slug + '\"></div>' html += '<script>' html += 'var s' + slug + ' = ' + json data + ';' html += 'vega.embed(\"#chart-' + slug + '\", s' + slug + ');' #html += 'console.log(JSON.stringify(s{id}, null, 2));' html += '</script>' return html", "predictions": ["converts an name into a url ."], "references": ["generates html from vega lite data"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5419, "code": "def dict to df ( self , dictobj , xfield , yfield ) : x = [ ] y = [ ] for datapoint in dictobj : x . append ( datapoint ) y . append ( dictobj [ datapoint ] ) df = pd . Data Frame ( { xfield [ 0 ] : x , yfield [ 0 ] : y } ) return df", "predictions": ["creates a setup from a setup ."], "references": ["converts a dictionnary to a pandas dataframe"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5420, "code": "def write file ( self , slug , folderpath , html ) : if not os . path . isdir ( folderpath ) : try : os . makedirs ( folderpath ) except Exception as e : tr . err ( e ) filepath = folderpath + \"/\" + slug + \".html\" #~ write the file try : filex = open ( filepath , \"w\" ) filex . write ( html ) filex . close ( ) except Exception as e : tr . err ( e )", "predictions": ["delete a ( or all code units code code code code code code code code code code code code code code code code code code code code code . code should be written to the server ."], "references": ["writes a chart s html to a file"], "bleu": 0.03607375465514362, "rouge_l": 0.10057708161582854}
{"id": 5421, "code": "def chart class ( self , df , chart type , * * kwargs ) : if chart type == \"bar\" : return Chart ( df ) . mark bar ( * * kwargs ) elif chart type == \"circle\" : return Chart ( df ) . mark circle ( * * kwargs ) elif chart type == \"line\" : return Chart ( df ) . mark line ( * * kwargs ) elif chart type == \"point\" : return Chart ( df ) . mark point ( * * kwargs ) elif chart type == \"area\" : return Chart ( df ) . mark area ( * * kwargs ) elif chart type == \"tick\" : return Chart ( df ) . mark tick ( * * kwargs ) elif chart type == \"text\" : return Chart ( df ) . mark text ( * * kwargs ) elif chart type == \"square\" : return Chart ( df ) . mark square ( * * kwargs ) elif chart type == \"rule\" : return Chart ( df ) . mark rule ( * * kwargs ) return None", "predictions": ["sys to sys . this is a jsp that defines the ( operation ."], "references": ["get the right chart class from a string"], "bleu": 0.09782375748961449, "rouge_l": 0.09561128526645768}
{"id": 5422, "code": "def encode fields ( self , xfield , yfield , time unit = None , scale = Scale ( zero = False ) ) : if scale is None : scale = Scale ( ) xfieldtype = xfield [ 1 ] yfieldtype = yfield [ 1 ] x options = None if len ( xfield ) > 2 : x options = xfield [ 2 ] y options = None if len ( yfield ) > 2 : y options = yfield [ 2 ] if time unit is not None : if x options is None : xencode = X ( xfieldtype , time Unit = time unit ) else : xencode = X ( xfieldtype , axis = Axis ( * * x options ) , time Unit = time unit , scale = scale ) else : if x options is None : xencode = X ( xfieldtype ) else : xencode = X ( xfieldtype , axis = Axis ( * * x options ) , scale = scale ) if y options is None : yencode = Y ( yfieldtype , scale = scale ) else : yencode = Y ( yfieldtype , axis = Axis ( * * y options ) , scale = scale ) return xencode , yencode", "predictions": ["( hash of this ( ."], "references": ["encode the fields in altair format"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5423, "code": "def infer tarball url ( ) : try : with click . open file ( 'app.json' , 'r' ) as f : contents = f . read ( ) app json = json . loads ( contents ) except IO Error : return None repository = app json . get ( 'repository' ) if not repository : return None else : return app json . get ( 'repository' ) + '/tarball/master/'", "predictions": ["random access to the cache ."], "references": ["returns the tarball url inferred from an app . json if present ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 5424, "code": "def up ( tarball url , auth token , env , app name ) : tarball url = tarball url or infer tarball url ( ) if not tarball url : click . echo ( 'No tarball URL found.' ) sys . exit ( 1 ) if env : env = { arg . split ( '=' ) [ 0 ] : arg . split ( '=' ) [ 1 ] for arg in env } happy = Happy ( auth token = auth token ) click . echo ( 'Creating app... ' , nl = False ) build id , app name = happy . create ( tarball url = tarball url , env = env , app name = app name , ) click . echo ( app name ) click . echo ( 'Building... ' , nl = False ) happy . wait ( build id ) write app name ( app name ) click . echo ( 'done' ) click . echo ( \"It's up! :) https://%s.herokuapp.com\" % app name )", "predictions": ["creates a auth with the specified auth ( or or . wraps wraps wraps wraps wraps the functools wraps the ( wraps the ( wraps the ( wraps the ( wraps ."], "references": ["brings up a heroku app ."], "bleu": 0.04180647946097227, "rouge_l": 0.1200787401574803}
{"id": 5425, "code": "def down ( auth token , force , app name ) : if not app name : click . echo ( 'WARNING: Inferring the app name when deleting is deprecated. ' 'Starting with happy 2.0, the app name parameter will be required.' ) app name = app name or read app name ( ) if not app name : click . echo ( 'No app name given.' ) sys . exit ( 1 ) if not force : click . confirm ( 'Are you sure you want to delete %s?' % app name , abort = True , ) happy = Happy ( auth token = auth token ) click . echo ( 'Destroying app %s... ' % app name , nl = False ) happy . delete ( app name = app name ) delete app name file ( ) click . echo ( 'done' ) click . echo ( \"It's down. :(\" )", "predictions": ["closes the given process ."], "references": ["brings down a heroku app ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5426, "code": "def date ( start , end ) : stime = date to timestamp ( start ) etime = date to timestamp ( end ) ptime = stime + random . random ( ) * ( etime - stime ) return datetime . date . fromtimestamp ( ptime )", "predictions": ["get a dynamic ) ."], "references": ["get a random date between two dates"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 5427, "code": "def get session ( self ) : session = Session ( ) session . headers = { 'Content-type' : 'application/json' , 'Accept' : 'application/vnd.heroku+json; version=3' , } if self . auth token : session . trust env = False session . headers [ 'Authorization' ] = 'Bearer %s' % self . auth token return session", "predictions": ["set the session object to be used for requests ."], "references": ["returns a prepared session instance ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 5428, "code": "def get root argparser ( self ) : return self . arg parse class ( description = self . get help ( ) , formatter class = self . get formatter class ( ) )", "predictions": ["this method is called to request the ( url url url url url url url url url url url url url url url url url ."], "references": ["gets the root argument parser object ."], "bleu": 0.051660454541342535, "rouge_l": 0.1352549889135255}
{"id": 5429, "code": "def get description ( self ) : if self . description : return self . description elif self . doc and self . doc . strip ( ) : return self . doc . strip ( ) . split ( '.' ) [ 0 ] + '.' else : return ''", "predictions": ["serve the ( or a ( app app app app app app app app app app app app app app app app app app app app app app app app app app app app app app app app . app . app ."], "references": ["gets the description of the command . if its not supplied the first sentence of the doc string is used ."], "bleu": 0.033287135819172496, "rouge_l": 0.09994538503549973}
{"id": 5430, "code": "def get help ( self ) : if self . help : return self . help elif self . doc and self . doc . strip ( ) : return self . doc . strip ( ) else : return ''", "predictions": ["get the ( ( or ( name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["gets the help text for the command . if its not supplied the doc string is used ."], "bleu": 0.02403051755364481, "rouge_l": 0.0321390937829294}
{"id": 5431, "code": "def get version ( ) : with open ( os . path . join ( os . path . dirname ( file ) , 'argparsetree' , ' init .py' ) ) as init py : return re . search ( ' version  = [\\'\"]([^\\'\"]+)[\\'\"]' , init py . read ( ) ) . group ( 1 )", "predictions": ["returns the ( or build date date date date date date date date date date date date date date date date date date date date date date date date date date date date date depending on the current environment ."], "references": ["return package version as listed in __version__ in init . py ."], "bleu": 0.03011857955989304, "rouge_l": 0.042597765363128495}
{"id": 5432, "code": "def url with auth ( regex , view , kwargs = None , name = None , prefix = '' ) : from djapiauth . auth import api auth if isinstance ( view , six . string types ) : return url ( regex , api auth ( import by path ( prefix + \".\" + view if prefix else view ) ) ) elif isinstance ( view , ( list , tuple ) ) : return url ( regex , view , name , prefix , * * kwargs ) else : return url ( regex , api auth ( view ) )", "predictions": ["return is called by the client when it does not already exist ."], "references": ["if view is string based must be a full path"], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 5433, "code": "def render ( self ) : for opt , values in self . data . items ( ) : if opt == 'ticks' : self [ 'chxtc' ] = '|' . join ( values ) else : self [ 'chx%s' % opt [ 0 ] ] = '|' . join ( values ) return self", "predictions": ["build a summary of this . ."], "references": ["render the axes data into the dict data"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 5434, "code": "def dataset ( self , data , series = '' ) : self . dataset = data self . series = series return self", "predictions": ["returns the load representation of this object ."], "references": ["update the chart s dataset can be two dimensional or contain string data"], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 5435, "code": "def render ( self ) : self . update ( self . axes . render ( ) ) encoder = Encoder ( self . encoding , None , self . series ) if not 'chs' in self : self [ 'chs' ] = '300x150' else : size = self [ 'chs' ] . split ( 'x' ) assert len ( size ) == 2 , 'Invalid size, must be in the format Wx H' self . check size ( * map ( int , size ) ) assert 'cht' in self , 'No chart type defined, use type method' self [ 'cht' ] = self . check type ( self [ 'cht' ] ) if ( 'any' in dir ( self . dataset ) and self . dataset . any ( ) ) or self . dataset : self [ 'chd' ] = encoder . encode ( self . dataset ) elif not 'choe' in self : assert 'chd' in self , 'You must have a dataset, or use chd' if self . scale : assert self [ 'chd' ] . startswith ( 't' ) , 'You must use text encoding with chds' self [ 'chds' ] = ',' . join ( self . scale ) if self . geo and self . ld : self [ 'chtm' ] = self . geo self [ 'chld' ] = self . ld if self . lines : self [ 'chls' ] = '|' . join ( self . lines ) if self . markers : self [ 'chm' ] = '|' . join ( self . markers ) if self . fills : self [ 'chf' ] = '|' . join ( self . fills )", "predictions": ["renders a profile ."], "references": ["renders the chart context and axes into the dict data"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 5436, "code": "def url ( self ) : self . render ( ) return self . apiurl + '&' . join ( self . parts ( ) ) . replace ( ' ' , '+' )", "predictions": ["get the full url ."], "references": ["returns the rendered url of the chart"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 5437, "code": "def urlopen ( self ) : req = Request ( str ( self ) ) try : return urlopen ( req ) except HTTP Error : print ( 'The server couldn\\'t fulfill the request.' ) except URL Error : print ( 'We failed to reach a server.' )", "predictions": ["get the request for this url and ]"], "references": ["grabs readable png file pointer"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5438, "code": "def parse args ( ) : usage = \"Usage: create concordance <infile> [<outfile>]\" description = \"Simple Concordance Generator\" argparser = argparse . Argument Parser ( usage = usage , description = description ) argparser . add argument ( 'infile' , type = argparse . File Type ( 'r' ) , help = \"File read in to create concordance\" ) argparser . add argument ( 'outfile' , nargs = '?' , type = argparse . File Type ( 'w' ) , default = sys . stdout , help = \"File to write concordance to.  \" \"Default is stdout\" ) argparser . add argument ( '--word' , nargs = \"?\" , const = str , help = \"Display a word in concordance\" ) args = argparser . parse args ( ) return args", "predictions": ["parses input arguments to generate ( arguments ."], "references": ["parses command line args using argparse library"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5439, "code": "def add Command Line Args ( arg parser ) : arg parser . register ( \"action\" , \"log levels\" , Log Level Action ) arg parser . register ( \"action\" , \"log files\" , Log File Action ) arg parser . register ( \"action\" , \"log help\" , Log Help Action ) group = arg parser . add argument group ( \"Logging options\" ) group . add argument ( \"-l\" , \"--log-level\" , dest = \"log levels\" , action = \"log levels\" , metavar = \"LOGGER:LEVEL\" , default = [ ] , help = \"Set log levels for individual loggers. See --help-logging for \" \"complete details.\" ) group . add argument ( \"-L\" , \"--log-file\" , dest = \"log files\" , action = \"log files\" , metavar = \"LOGGER:FILE\" , default = [ ] , help = \"Set log the output file for individual loggers. \" \" See --help-logging for complete details.\" ) group . add argument ( \"--help-logging\" , action = \"log help\" , help = argparse . SUPPRESS )", "predictions": ["create and initialize the command line ."], "references": ["add logging option to an argumentparser ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5440, "code": "def validate page number ( number ) : try : number = int ( number ) except ( Type Error , Value Error ) : raise Page Not An Integer ( 'That page number is not an integer' ) if number < 1 : raise Empty Page ( 'That page number is less than 1' ) return number", "predictions": ["validates that number is a valid page or a valid number ."], "references": ["validate the given 1 - based page number ."], "bleu": 0.14694106251955755, "rouge_l": 0.2932692307692307}
{"id": 5441, "code": "def chmod ( path , mode , recursive = True ) : if recursive : cmd = 'chmod -R %s %s' % ( mode , path ) else : cmd = 'chmod %s %s' % ( mode , path ) return sh ( cmd )", "predictions": ["handles a new file or directory recursively ."], "references": ["alternative to os ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 5442, "code": "def create bundle ( self , data ) : kwargs = { } filters = None if isinstance ( data , dict ) : kwargs . update ( filters = data . get ( 'filters' , None ) , output = data . get ( 'output' , None ) , debug = data . get ( 'debug' , None ) , extra = data . get ( 'extra' , { } ) , config = data . get ( 'config' , { } ) , depends = data . get ( 'depends' , None ) ) bundle = Bundle ( * list ( self . yield bundle contents ( data ) ) , * * kwargs ) return self . auto filter bundle ( bundle )", "predictions": ["create an ( for the given parameters ."], "references": ["return a bundle initialised by the given dict ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 5443, "code": "def urls for ( self , asset type , * args , * * kwargs ) : return self . urls for depends ( asset type , * args , * * kwargs ) + self . urls for self ( asset type , * args , * * kwargs )", "predictions": ["compute original and kwargs for this class ."], "references": ["returns urls needed to include all assets of asset_type"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 5444, "code": "def html tags for ( self , asset type , * args , * * kwargs ) : html = [ ] for ref in self . depends : html . append ( self . ref ( ref ) . html tags for ( asset type , * args , * * kwargs ) ) if asset type in self . typed bundles : html . append ( render asset html tags ( asset type , self . urls for self ( asset type , * args , * * kwargs ) ) ) return \"\\n\" . join ( html )", "predictions": ["get a list of tags that can be used to perform additional operations ."], "references": ["return html tags for urls of asset_type"], "bleu": 0.09782375748961449, "rouge_l": 0.10132890365448505}
{"id": 5445, "code": "def html tags ( self , * args , * * kwargs ) : html = [ ] for asset type in list asset types ( ) : html . append ( self . html tags for ( asset type . name , * args , * * kwargs ) ) return \"\\n\" . join ( html )", "predictions": ["convenience method for handling tags that can be used to bring the content of a loop into a starting point ."], "references": ["return all html tags for all asset_type"], "bleu": 0.06429451441231726, "rouge_l": 0.0785070785070785}
{"id": 5446, "code": "def find version ( filename ) : with io . open ( filename , encoding = \"utf-8\" ) as version file : version match = re . search ( r'^ version  = [\\'\"]([^\\'\"]*)[\\'\"]' , version file . read ( ) , re . M ) if version match : return version match . group ( 1 ) return \"0.0-version-unknown\"", "predictions": ["find the version number without its full path ."], "references": ["uses re to pull out the assigned value to __version__ in filename ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 5447, "code": "def import modules ( self ) : modules = self . get modules ( ) log . info ( \"import service modules: \" + str ( modules ) ) try : for module in modules : import ( module ) except Import Error as error : raise Import Modules Error ( error . msg )", "predictions": ["import and raise a module ."], "references": ["import customer s service module ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 5448, "code": "def send ( self , peer , typename , data ) : def attempt to send ( ) : if peer not in self . connections : d = self . connect ( peer ) d . add Callback ( attempt to send ) return d else : conn = self . connections [ peer ] [ 0 ] conn . send packet ( typename , data ) return defer . succeed ( None ) d = attempt to send ( None ) self . ongoing sends . add ( d ) def send completed ( result ) : if d in self . ongoing sends : self . ongoing sends . remove ( d ) return result d . add Both ( send completed ) return d", "predictions": ["send a sip message to the server . if we receive the specified typename and send the message to the server . if we do not have already been set , it is a client ."], "references": ["sends a packet to a peer ."], "bleu": 0.042143413032077665, "rouge_l": 0.21180555555555555}
{"id": 5449, "code": "def receive Data ( self , connection , data ) : try : protocol = self . protocols [ connection ] except Key Error : raise No Such Connection ( ) protocol . data Received ( data ) return { }", "predictions": ["receive data from a connection ."], "references": ["receives some data for the given protocol ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 5450, "code": "def disconnect ( self , connection ) : proto = self . protocols . pop ( connection ) proto . transport = None return { }", "predictions": ["disconnects the http request from the server"], "references": ["disconnects the given protocol ."], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 5451, "code": "def send Data ( self , data ) : d = self . call Remote ( Transmit , connection = self . connection , data = data ) d . add Errback ( log . err )", "predictions": ["sends the specified data to the server ."], "references": ["actually sends data over the wire ."], "bleu": 0.20164945583740668, "rouge_l": 0.5398230088495575}
{"id": 5452, "code": "def get Local Protocol ( self , connection Identifier ) : for factory in self . local Factories : try : return factory . protocols [ connection Identifier ] except Key Error : continue raise No Such Connection ( )", "predictions": ["gets the protocols by name ."], "references": ["attempts to get a local protocol by connection identifier ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 5453, "code": "def disconnect ( self , connection ) : proto = self . get Local Protocol ( connection ) proto . transport . lose Connection ( ) return { }", "predictions": ["disconnects the http message from the server ."], "references": ["the other side has asked us to disconnect ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5454, "code": "def centered ( mystring , linewidth = None , fill = \" \" ) : if linewidth is None : linewidth = get terminal size ( ) . columns - 1 sides = ( linewidth - length no ansi ( mystring ) ) // 2 extra = ( linewidth - length no ansi ( mystring ) ) % 2 fill = fill [ : 1 ] sidestring = fill * sides extrastring = fill * extra newstring = sidestring + mystring + sidestring + extrastring return newstring", "predictions": ["returns the ( - triangle of the given ( ."], "references": ["takes a string centres it and pads it on both sides"], "bleu": 0.0959156018869021, "rouge_l": 0.0}
{"id": 5455, "code": "def clock on right ( mystring ) : taken = length no ansi ( mystring ) padding = ( get terminal size ( ) . columns - 1 ) - taken - 5 clock = time . strftime ( \"%I:%M\" , time . localtime ( ) ) print ( mystring + \" \" * padding + clock )", "predictions": ["creates the clock and obtains the clock on the current thread ."], "references": ["takes a string and prints it with the time right aligned"], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 5456, "code": "def main ( arguments = None ) : if not arguments : arguments = sys . argv [ 1 : ] wordlist , sowpods , by length , start , end = argument parser ( arguments ) for word in wordlist : pretty print ( word , anagrams in word ( word , sowpods , start , end ) , by length , )", "predictions": ["the main function of the ( ."], "references": ["main command line entry point ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5457, "code": "def ping ( self , peerid , callid ) : if not ( peerid , callid ) in self . remote to local : logger . warn ( \"No remote call %s from %s. Might just be unfoutunate timing.\" % ( callid , peerid ) )", "predictions": ["ping from the controller and call this method only on the client ."], "references": ["called from remote to ask if a call made to here is still in progress ."], "bleu": 0.09018449225581421, "rouge_l": 0.2031076581576027}
{"id": 5458, "code": "def cmd Regex ( self , cmd grp = None ) : cmd grp = cmd grp or \"cmd\" help opts = ( \"-h\" , \"--help\" ) cmd = self . name ( ) names = \"|\" . join ( [ re . escape ( cmd ) ] + [ re . escape ( a ) for a in self . aliases ( ) ] ) opts = [ ] for action in self . parser . actions : opts += [ a for a in action . option strings if a not in help opts ] opts re = \"|\" . join ( [ re . escape ( o ) for o in opts ] ) if opts re : opts re = rf\"(\\s+(?P<{cmd grp} opts>{opts re}))*\" help re = \"|\" . join ( [ re . escape ( o ) for o in help opts ] ) help re = rf\"(\\s+(?P<HELP OPTS>{help re}))*\" completers = { } if opts re : completers [ f\"{cmd grp} opts\" ] = Word Completer ( opts ) return tuple ( [ rf\"\"\"(?P<{cmd grp}>{names}){opts re}{help re}\"\"\" , completers ] )", "predictions": ["creates a new command object for evaluation commands ."], "references": ["get command regex string and completer dict ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5459, "code": "def from String Proto ( self , in String , proto ) : value , = amp . Amp List . from String Proto ( self , in String , proto ) return value", "predictions": ["factory method for creating string representation of a binary representation ."], "references": ["defers to amp . amplist then gets the element from the list ."], "bleu": 0.09497094417933137, "rouge_l": 0.08209959623149395}
{"id": 5460, "code": "def to String Proto ( self , in Object , proto ) : return amp . Amp List . to String Proto ( self , [ in Object ] , proto )", "predictions": ["transform object to a representation of this object ."], "references": ["wraps the object in a list and then defers to amp . amplist ."], "bleu": 0.10182634488642418, "rouge_l": 0.2510288065843621}
{"id": 5461, "code": "def connection ( username = None , password = None , host = None , port = None , db = None ) : c opts = { } if username : c opts [ 'user' ] = username if password : c opts [ 'password' ] = password if host : c opts [ 'host' ] = host if port : c opts [ 'port' ] = port if db : c opts [ 'database' ] = db dbc = psycopg2 . connect ( * * c opts ) dbc . autocommit = True return dbc", "predictions": ["creates the connection to the connection ."], "references": ["returns a connected cursor to the database - server ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 5462, "code": "def db list ( username = None , password = None , host = None , port = None , maintain db = 'postgres' ) : conn = connection ( username = username , password = password , host = host , port = port , db = maintain db ) cur = conn . cursor ( ) cur . execute ( 'SELECT DATNAME from pg database' ) rows = cur . fetchall ( ) conn . close ( ) result = [ ] for row in rows : result . append ( row [ 0 ] ) return result", "predictions": ["list all namespaces on a mysql cluster cli example: ."], "references": ["returns a list of all databases on this server"], "bleu": 0.15851165692617156, "rouge_l": 0.31881533101045295}
{"id": 5463, "code": "def get local files ( self , path ) : if not path : raise Value Error ( \"No path specified\" ) files = defaultdict ( lambda : None ) path len = len ( path ) + 1 for root , dirs , filenames in os . walk ( path ) : for name in filenames : full path = join ( root , name ) files [ full path [ path len : ] ] = compute md5 ( full path ) return files", "predictions": ["gets all files below the specified path ."], "references": ["returns a dictionary of all the files under a path ."], "bleu": 0.17250013293422076, "rouge_l": 0.4093959731543625}
{"id": 5464, "code": "def tokens required ( service list ) : def decorator ( func ) : @ wraps ( func ) def inner ( request , * args , * * kwargs ) : for service in service list : if service not in request . session [ \"user tokens\" ] : return redirect ( 'denied' ) return func ( request , * args , * * kwargs ) return inner return decorator", "predictions": ["decorator to help help do so we can use a decorator for a decorator that applies a decorator to a decorator that has a decorator that has a decorator that prevents a decorator for the request ."], "references": ["ensure the user has the necessary tokens for the specified services"], "bleu": 0.04609815356235176, "rouge_l": 0.13853141559424678}
{"id": 5465, "code": "def login ( request , template name = 'ci/login.html' , redirect field name = REDIRECT FIELD NAME , authentication form = Authentication Form ) : redirect to = request . POST . get ( redirect field name , request . GET . get ( redirect field name , '' ) ) if request . method == \"POST\" : form = authentication form ( request , data = request . POST ) if form . is valid ( ) : if not is safe url ( url = redirect to , host = request . get host ( ) ) : redirect to = resolve url ( settings . LOGIN REDIRECT URL ) user = form . get user ( ) request . session [ 'user token' ] = user [ \"token\" ] request . session [ 'user email' ] = user [ \"email\" ] request . session [ 'user permissions' ] = user [ \"permissions\" ] request . session [ 'user id' ] = user [ \"id\" ] request . session [ 'user list' ] = user [ \"user list\" ] if not settings . HIDE DASHBOARDS : dashboards = ci Api . get user dashboards ( user [ \"id\" ] ) dashboard list = list ( dashboards [ 'results' ] ) if len ( dashboard list ) > 0 : request . session [ 'user dashboards' ] = dashboard list [ 0 ] [ \"dashboards\" ] request . session [ 'user default dashboard' ] = dashboard list [ 0 ] [ \"default dashboard\" ] [ \"id\" ] else : request . session [ 'user dashboards' ] = [ ] request . session [ 'user default dashboard' ] = None tokens = ci Api . get user service tokens ( params = { \"user id\" : user [ \"id\" ] } ) token list = list ( tokens [ 'results' ] ) user tokens = { } if len ( token list ) > 0 : for token in token list : user tokens [ token [ \"service\" ] [ \"name\" ] ] = { \"token\" : token [ \"token\" ] , \"url\" : token [ \"service\" ] [ \"url\" ] + \"/api/v1\" } request . session [ 'user tokens' ] = user tokens return Http Response Redirect ( redirect to ) else : form = authentication form ( request ) current site = get current site ( request ) context = { 'form' : form , redirect field name : redirect to , 'site' : current site , 'site name' : current site . name , } return Template Response ( request , template name , context )", "predictions": ["view to log when the login is submitted ."], "references": ["displays the login form and handles the login action ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 5466, "code": "def build ( cli , path , package ) : for , name , ispkg in iter modules ( path ) : module = import module ( f'.{name}' , package ) if ispkg : build ( cli . group ( name ) ( module . group ) , module . path , module . package ) else : cli . command ( name ) ( module . command )", "predictions": ["builds the full command line and builds a command line ."], "references": ["build cli dynamically based on the package structure ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5467, "code": "def descovery ( testdir ) : from os . path import join , exists , isdir , splitext , basename , sep if not testdir or not exists ( testdir ) or not isdir ( testdir ) : return None from os import walk import fnmatch import imp for root , , filenames in walk ( testdir ) : for filename in fnmatch . filter ( filenames , '*.py' ) : path = join ( root , filename ) modulepath = splitext ( root ) [ 0 ] . replace ( sep , '.' ) imp . load source ( modulepath , path )", "predictions": ["constructs all the migrations in the specified local file ."], "references": ["descover and load greencard tests ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 5468, "code": "def main ( clargs = None ) : from argparse import Argument Parser from librarian . library import Library import sys parser = Argument Parser ( description = \"A test runner for each card in a librarian library.\" ) parser . add argument ( \"library\" , help = \"Library database\" ) parser . add argument ( \"-t\" , \"--tests\" , default = \"test/\" , help = \"Test directory\" ) args = parser . parse args ( clargs ) descovery ( args . tests ) library = Library ( args . library ) cardcount , passes , failures = execute tests ( library ) print ( RESULTS . format ( len ( SINGLES ) , len ( TESTS ) , cardcount , passes , failures ) ) sys . exit ( failures )", "predictions": ["main entry point for the script ."], "references": ["command line entry point ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 5469, "code": "def write Response ( self , response ) : encoded = dumps ( response , default = default ) self . transport . write ( encoded )", "predictions": ["writes the specified object to the response using the http millisecond module ."], "references": ["serializes the response to json and writes it to the transport ."], "bleu": 0.17194656088289215, "rouge_l": 0.3223249669749009}
{"id": 5470, "code": "def connection Lost ( self , reason ) : self . remote . box Receiver . stop Receiving Boxes ( reason ) return basic . Netstring Receiver . connection Lost ( self , reason )", "predictions": ["stops the connection with the given connection ."], "references": ["tells the box receiver to stop receiving boxes ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5471, "code": "def build Protocol ( self , addr ) : proto = self . factory . build Protocol ( addr ) return JSONAMP Dialect Receiver ( proto )", "predictions": ["builds the plugin which is no longer needed ."], "references": ["builds a bridge and associates it with an amp protocol instance ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 5472, "code": "def pout ( msg , log = None ) : print ( msg , sys . stdout , log func = log . info if log else None )", "predictions": ["broadcast ) and ) ) ) to , at ) ) ."], "references": ["print msg to stdout and option log at info level ."], "bleu": 0.13065113298388567, "rouge_l": 0.2629310344827586}
{"id": 5473, "code": "def perr ( msg , log = None ) : print ( msg , sys . stderr , log func = log . error if log else None )", "predictions": ["dump %s error message to the log file ."], "references": ["print msg to stderr and option log at info level ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 5474, "code": "def register ( Command Sub Class ) : name = Command Sub Class . name ( ) if name in Command . all commands : raise Value Error ( \"Command already exists: \" + name ) Command . all commands [ name ] = Command Sub Class return Command Sub Class", "predictions": ["create a command class ."], "references": ["a class decorator for command classes to register in the default set ."], "bleu": 0.06930996903910726, "rouge_l": 0.3086003372681282}
{"id": 5475, "code": "def register ( Class , Command Sub Class ) : for name in [ Command Sub Class . name ( ) ] + Command Sub Class . aliases ( ) : if name in Class . registered commands [ Class ] : raise Value Error ( \"Command already exists: \" + name ) Class . registered commands [ Class ] [ name ] = Command Sub Class return Command Sub Class", "predictions": ["registers a class with the command class ."], "references": ["a class decorator for command classes to register ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 5476, "code": "def init mq ( self ) : mq = self . init connection ( ) self . init consumer ( mq ) return mq . connection", "predictions": ["returns the full * * * * * otherwise ."], "references": ["init connection and consumer with openstack mq ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5477, "code": "def init modules ( self ) : if not self . config : raise Value Error ( \"please read your config file.\" ) log . debug ( \"begin to import customer's service modules.\" ) modules = Service Modules ( self . config ) modules . import modules ( ) log . debug ( \"end to import customer's service modules.\" )", "predictions": ["initialize the module . this is only used for testing ."], "references": ["import customer s service modules ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 5478, "code": "def music info ( songid ) : if isinstance ( songid , list ) : songid = ',' . join ( songid ) data = { \"hq\" : 1 , \"song Ids\" : songid } res = requests . post ( MUSIC INFO URL , data = data ) info = res . json ( ) music data = info [ \"data\" ] songs = [ ] for song in music data [ \"song List\" ] : song link , size = song link ( song , music data [ \"xcode\" ] ) songs . append ( { \"name\" : song [ \"song Name\" ] , \"singer\" : song [ \"artist Name\" ] , \"lrc link\" : song [ \"lrc Link\" ] , \"song link\" : song link , \"size\" : size } ) return songs", "predictions": ["creates a find out of the find command ."], "references": ["get music info from baidu music api"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5479, "code": "def download music ( song , thread num = 4 ) : filename = \"{}.mp3\" . format ( song [ \"name\" ] ) if os . path . exists ( filename ) : os . remove ( filename ) part = int ( song [ \"size\" ] / thread num ) if part <= 1024 : thread num = 1 id = uuid . uuid4 ( ) . hex logger . info ( \"downloading '{}'...\" . format ( song [ \"name\" ] ) ) threads = [ ] for i in range ( thread num ) : if i == thread num - 1 : end = '' else : end = ( i + 1 ) * part - 1 thread = Worker ( ( i * part , end ) , song , id ) thread . start ( ) threads . append ( thread ) for t in threads : t . join ( ) file Parts = glob . glob ( \"part-{}-*\" . format ( id ) ) file Parts . sort ( key = lambda e : e . split ( '-' ) [ - 1 ] ) logger . info ( \"'{}' combine parts...\" . format ( song [ \"name\" ] ) ) with open ( filename , \"ab\" ) as f : for part in file Parts : with open ( part , \"rb\" ) as d : shutil . copyfileobj ( d , f ) os . remove ( part ) logger . info ( \"'{}' finished\" . format ( song [ \"name\" ] ) )", "predictions": ["import the data for this set of found threads ."], "references": ["process for downing music with multiple threads"], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 5480, "code": "def load name ( self , name ) : if name in self . globals : return self . globals [ name ] b = self . globals [ ' builtins ' ] if isinstance ( b , dict ) : return b [ name ] else : return getattr ( b , name )", "predictions": ["loads and returns a sizer for this object ."], "references": ["implementation of the load_name operation"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5481, "code": "def pop ( self , n ) : poped = self . stack [ len ( self . stack ) - n : ] del self . stack [ len ( self . stack ) - n : ] return poped", "predictions": ["returns the latest element at the top of this stack ."], "references": ["pop the ** n ** topmost items from the stack and return them as a list ."], "bleu": 0.08301383771904051, "rouge_l": 0.2750845546786922}
{"id": 5482, "code": "def connection ( username = None , password = None , host = None , port = None ) : c opts = { } if username : c opts [ 'user' ] = username if password : c opts [ 'passwd' ] = password if host : c opts [ 'host' ] = host if port : c opts [ 'port' ] = port dbc = My SQ Ldb . connect ( * * c opts ) dbc . autocommit ( True ) return dbc", "predictions": ["returns the disconnect disconnect ."], "references": ["returns a connected cursor to the database - server ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 5483, "code": "def render ditaa ( self , code , options , prefix = 'ditaa' ) : hashkey = code . encode ( 'utf-8' ) + str ( options ) + str ( self . builder . config . ditaa ) + str ( self . builder . config . ditaa args ) infname = '%s-%s.%s' % ( prefix , sha ( hashkey ) . hexdigest ( ) , \"ditaa\" ) outfname = '%s-%s.%s' % ( prefix , sha ( hashkey ) . hexdigest ( ) , \"png\" ) inrelfn = posixpath . join ( self . builder . imgpath , infname ) infullfn = path . join ( self . builder . outdir , ' images' , infname ) outrelfn = posixpath . join ( self . builder . imgpath , outfname ) outfullfn = path . join ( self . builder . outdir , ' images' , outfname ) if path . isfile ( outfullfn ) : return outrelfn , outfullfn ensuredir ( path . dirname ( outfullfn ) ) if isinstance ( code , unicode ) : code = code . encode ( 'utf-8' ) ditaa args = [ self . builder . config . ditaa ] ditaa args . extend ( self . builder . config . ditaa args ) ditaa args . extend ( options ) ditaa args . extend ( [ infullfn ] ) ditaa args . extend ( [ outfullfn ] ) f = open ( infullfn , 'w' ) f . write ( code ) f . close ( ) try : self . builder . warn ( ditaa args ) p = Popen ( ditaa args , stdout = PIPE , stdin = PIPE , stderr = PIPE ) except OS Error , err : if err . errno != ENOENT : raise self . builder . warn ( 'ditaa command %r cannot be run (needed for ditaa ' 'output), check the ditaa setting' % self . builder . config . ditaa ) self . builder . ditaa warned dot = True return None , None went Wrong = False try : stdout , stderr = p . communicate ( code ) except OS Error , err : if err . errno != EPIPE : raise went Wrong = True except IO Error , err : if err . errno != EINVAL : raise went Wrong = True if went Wrong : stdout , stderr = p . stdout . read ( ) , p . stderr . read ( ) p . wait ( ) if p . returncode != 0 : raise Ditaa Error ( 'ditaa exited with error:\\n[stderr]\\n%s\\n' '[stdout]\\n%s' % ( stderr , stdout ) ) return outrelfn , outfullfn", "predictions": ["renders the contents of this object using the given ( and . ."], "references": ["render ditaa code into a png output file ."], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 5484, "code": "def atexit ( self ) : self . log . debug ( \"Application. atexit\" ) if self . atexit func : self . atexit func ( self )", "predictions": ["a convenience method for the persistence problem ."], "references": ["invoked in the finally block of application . run ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 5485, "code": "def run ( self , args list = None ) : self . log . debug ( \"Application.run: {args list}\" . format ( * * locals ( ) ) ) retval = None try : retval = self . run ( args list = args list ) except Keyboard Interrupt : self . log . verbose ( \"Interrupted\" ) except System Exit as exit : self . log . verbose ( \"Exited\" ) retval = exit . code except Exception : print ( \"Uncaught exception\" , file = sys . stderr ) traceback . print exc ( ) if \"debug pdb\" in self . args and self . args . debug pdb : debugger ( ) retval = Application . UNCAUGHT EXCEPTION EXIT raise finally : try : self . atexit ( ) finally : sys . stderr . flush ( ) sys . stdout . flush ( ) sys . exit ( retval )", "predictions": ["disconnect method for the server ."], "references": ["run application . main and exits with the return value ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 5486, "code": "def main ( ) : parser = optparse . Option Parser ( usage = \"%prog [options] <model path> [another model path...]\" , formatter = optparse . Titled Help Formatter ( ) ) parser . set description ( doc . strip ( ) ) parser . add option ( \"-f\" , \"--function\" , dest = \"function\" , metavar = \"NAME\" , help = \"append integrity checking actions to functions named NAME (required)\" , action = \"store\" , default = None ) parser . add option ( \"-o\" , \"--output\" , dest = 'output' , metavar = \"PATH\" , help = \"save sql model instances to PATH (required)\" , action = \"store\" , default = None ) parser . add option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = 2 ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or None in [ opts . output , opts . function ] : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) m = ooaofooa . load metamodel ( args ) for c c in m . select many ( 'C C' ) : filt = lambda sel : ooaofooa . is contained in ( sel , c c ) and sel . Name == opts . function s sync = m . select any ( 'S SYNC' , filt ) if not s sync : s sync = m . new ( 'S SYNC' , Name = opts . function ) pe pe = m . new ( 'PE PE' ) s dt = m . select any ( 'S DT' , where ( Name = 'boolean' ) ) relate ( pe pe , s sync , 8001 ) relate ( s dt , s sync , 25 ) generate actions ( m , c c , s sync ) xtuml . persist instances ( m , opts . output )", "predictions": ["the main entry point"], "references": ["parse argv for options and arguments and start schema generation ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 5487, "code": "def scrape ( ctx , url ) : data = load feed ( url ) feed = data [ 'feed' ] entries = data [ 'entries' ] type = 'community' country = 'Czech Republic' for entry in entries : id = sluggify ( entry [ 'id' ] ) city = entry [ 'tags' ] [ 0 ] [ 'term' ] landing = entry [ 'link' ] start time = dt normalize ( entry [ 'published parsed' ] , local tz = True ) title = entry [ 'title' ] summary = entry [ 'summary' ] link = entry [ 'link' ] ipdb . set trace ( )", "predictions": ["actually actually actually generate the ( ."], "references": ["rip the events from a given rss feed normalize the data and store ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 5488, "code": "def fancy tag compiler ( params , defaults , takes var args , takes var kwargs , takes context , name , node class , parser , token ) : bits = token . split contents ( ) [ 1 : ] if takes context : if 'context' in params [ : 1 ] : params = params [ 1 : ] else : raise Template Syntax Error ( \"Any tag function decorated with takes context=True \" \"must have a first argument of 'context'\" ) args = [ ] kwargs = { } kwarg found = False unhandled params = list ( params ) handled params = [ ] if len ( bits ) > 1 and bits [ - 2 ] == 'as' : output var = bits [ - 1 ] if len ( set ( output var ) - set ( ALLOWED VARIABLE CHARS ) ) > 0 : raise Template Syntax Error ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output var ) ) bits = bits [ : - 2 ] else : output var = None for bit in bits : kwarg match = kwarg re . match ( bit ) if kwarg match : kw , var = kwarg match . groups ( ) if kw not in params and not takes var kwargs : raise Template Syntax Error ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) elif kw in handled params : raise Template Syntax Error ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) else : kwargs [ str ( kw ) ] = var kwarg found = True handled params . append ( kw ) else : if kwarg found : raise Template Syntax Error ( \"%s got non-keyword arg after keyword arg\" % name ) else : args . append ( bit ) try : handled params . append ( unhandled params . pop ( 0 ) ) except Index Error : if not takes var args : raise Template Syntax Error ( \"%s got too many arguments\" % name ) if defaults is not None : unhandled params = unhandled params [ : - len ( defaults ) ] if len ( unhandled params ) == 1 : raise Template Syntax Error ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled params [ 0 ] ) ) elif len ( unhandled params ) > 1 : raise Template Syntax Error ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled params ] ) ) ) return node class ( args , kwargs , output var , takes context )", "predictions": ["does the actual work of applying a arguments to the output ."], "references": ["returns a template . node subclass ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 5489, "code": "def get defining component ( pe pe ) : if pe pe is None : return None if pe pe . class . name != 'PE PE' : pe pe = xtuml . navigate one ( pe pe ) . PE PE [ 8001 ] ( ) ep pkg = xtuml . navigate one ( pe pe ) . EP PKG [ 8000 ] ( ) if ep pkg : return get defining component ( ep pkg ) return xtuml . navigate one ( pe pe ) . C C [ 8003 ] ( )", "predictions": ["ping an archive by its name ."], "references": ["get the c_c in which pe_pe is defined"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 5490, "code": "def main ( ) : parser = optparse . Option Parser ( usage = \"%prog [options] <model path> [another model path..]\" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . add option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = 1 ) parser . add option ( \"-o\" , \"--output\" , dest = \"output\" , metavar = \"PATH\" , help = \"set output to PATH\" , action = \"store\" , default = None ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or opts . output is None : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) m = ooaofooa . load metamodel ( args ) prebuild model ( m ) xtuml . persist instances ( m , opts . output )", "predictions": ["the main entry point"], "references": ["parse command line options and launch the prebuilder ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 5491, "code": "def find symbol ( self , name = None , kind = None ) : for s in reversed ( self . stack ) : for symbol name , handle in s . symbols . items ( ) : symbol kind = handle . class . name if name == symbol name and kind == symbol kind : return handle elif name is None and kind == handle . class . name : return handle elif name == symbol name and kind is None : return handle if name is None and kind == s . handle . class . name : return s . handle", "predictions": ["from the symbol component ."], "references": ["find a symbol in the symbol table by name kind or both ."], "bleu": 0.07795171967670728, "rouge_l": 0.3086003372681282}
{"id": 5492, "code": "def is contained in ( pe pe , root ) : if not pe pe : return False if type ( pe pe ) . name != 'PE PE' : pe pe = one ( pe pe ) . PE PE [ 8001 ] ( ) ep pkg = one ( pe pe ) . EP PKG [ 8000 ] ( ) c c = one ( pe pe ) . C C [ 8003 ] ( ) if root in [ ep pkg , c c ] : return True elif is contained in ( ep pkg , root ) : return True elif is contained in ( c c , root ) : return True else : return False", "predictions": ["returns true if the version of a set of 8000 is below an externally version of a set of 8000 ."], "references": ["determine if a pe_pe is contained within a ep_pkg or a c_c ."], "bleu": 0.07645949399477267, "rouge_l": 0.3071500503524673}
{"id": 5493, "code": "def is global ( pe pe ) : if type ( pe pe ) . name != 'PE PE' : pe pe = one ( pe pe ) . PE PE [ 8001 ] ( ) if one ( pe pe ) . C C [ 8003 ] ( ) : return False pe pe = one ( pe pe ) . EP PKG [ 8000 ] . PE PE [ 8001 ] ( ) if not pe pe : return True return is global ( pe pe )", "predictions": ["connection is a ( ."], "references": ["check if a pe_pe is globally defined i . e . not inside a c_c"], "bleu": 0.04393902429866315, "rouge_l": 0.18345864661654135}
{"id": 5494, "code": "def get data type name ( s dt ) : s cdt = one ( s dt ) . S CDT [ 17 ] ( ) if s cdt and s cdt . Core Typ in range ( 1 , 6 ) : return s dt . Name . upper ( ) if one ( s dt ) . S EDT [ 17 ] ( ) : return 'INTEGER' s dt = one ( s dt ) . S UDT [ 17 ] . S DT [ 18 ] ( ) if s dt : return get data type name ( s dt )", "predictions": ["db username - ("], "references": ["convert a bridgepoint data type to a pyxtuml meta model type ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 5495, "code": "def get related attributes ( r rgo , r rto ) : l1 = list ( ) l2 = list ( ) ref filter = lambda ref : ref . OIR ID == r rgo . OIR ID for o ref in many ( r rto ) . O RTIDA [ 110 ] . O REF [ 111 ] ( ref filter ) : o attr = one ( o ref ) . O RATTR [ 108 ] . O ATTR [ 106 ] ( ) l1 . append ( o attr . Name ) o attr = one ( o ref ) . O RTIDA [ 111 ] . O OIDA [ 110 ] . O ATTR [ 105 ] ( ) l2 . append ( o attr . Name ) return l1 , l2", "predictions": ["get from the unary files that have been local ."], "references": ["the two lists of attributes which relates two classes in an association ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 5496, "code": "def mk enum ( s edt ) : s dt = one ( s edt ) . S DT [ 17 ] ( ) enums = list ( ) kwlist = [ 'False' , 'None' , 'True' ] + keyword . kwlist for enum in many ( s edt ) . S ENUM [ 27 ] ( ) : if enum . Name in kwlist : enums . append ( enum . Name + ' ' ) else : enums . append ( enum . Name ) Enum = collections . namedtuple ( s dt . Name , enums ) return Enum ( * range ( len ( enums ) ) )", "predictions": ["generate an ( from a namedtuple or sequence of \" namedtuple \" ."], "references": ["create a named tuple from a bridgepoint enumeration ."], "bleu": 0.1350862565735141, "rouge_l": 0.2819722650231125}
{"id": 5497, "code": "def mk bridge ( metamodel , s brg ) : action = s brg . Action Semantics internal label = s brg . Name return lambda * * kwargs : interpret . run function ( metamodel , label , action , kwargs )", "predictions": ["generate a new ( with the given signature ."], "references": ["create a python function from a bridgepoint bridge ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5498, "code": "def mk function ( metamodel , s sync ) : action = s sync . Action Semantics internal label = s sync . Name return lambda * * kwargs : interpret . run function ( metamodel , label , action , kwargs )", "predictions": ["performs a ( , ) command with the given name and ) ."], "references": ["create a python function from a bridgepoint function ."], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 5499, "code": "def mk constant ( cnst syc ) : s dt = one ( cnst syc ) . S DT [ 1500 ] ( ) cnst lsc = one ( cnst syc ) . CNST LFSC [ 1502 ] . CNST LSC [ 1503 ] ( ) if s dt . Name == 'boolean' : return cnst lsc . Value . lower ( ) == 'true' if s dt . Name == 'integer' : return int ( cnst lsc . Value ) if s dt . Name == 'real' : return float ( cnst lsc . Value ) if s dt . Name == 'string' : return str ( cnst lsc . Value )", "predictions": ["generate an ( instance for a : : : : : : : : : : / / www . com / ( / ( / ( / ( / ( . com / ( / ( / ( / ( / ( / ( / ( / ( /"], "references": ["create a python value from a bridgepoint constant ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 5500, "code": "def mk class ( m , o obj , derived attributes = False ) : first filter = lambda selected : not one ( selected ) . O ATTR [ 103 , 'succeeds' ] ( ) o attr = one ( o obj ) . O ATTR [ 102 ] ( first filter ) attributes = list ( ) while o attr : s dt = get attribute type ( o attr ) ty = get data type name ( s dt ) if not derived attributes and one ( o attr ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : pass elif not ty : logger . warning ( 'Omitting unsupported attribute %s.%s ' % ( o obj . Key Lett , o attr . Name ) ) else : attributes . append ( ( o attr . Name , ty ) ) o attr = one ( o attr ) . O ATTR [ 103 , 'precedes' ] ( ) metaclass = m . define class ( o obj . Key Lett , list ( attributes ) , o obj . Descrip ) for o id in many ( o obj ) . O ID [ 104 ] ( ) : o oida = many ( o id ) . O OIDA [ 105 ] ( ) o attrs = many ( o oida ) . O ATTR [ 105 ] ( ) if not derived attributes and one ( o attrs ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : logger . warning ( 'Omitting unique identifier %s.I%d' % ( o obj . Key Lett , o id . Oid ID + 1 ) ) continue names = [ o attr . Name for o attr in o attrs ] m . define unique identifier ( o obj . Key Lett , o id . Oid ID + 1 , * names ) for o tfr in many ( o obj ) . O TFR [ 115 ] ( ) : fn = mk operation ( metaclass , o tfr ) setattr ( metaclass . clazz , o tfr . Name , fn ) for o dbattr in many ( o obj ) . O ATTR [ 102 ] . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : o attr = one ( o dbattr ) . O BATTR [ 107 ] . O ATTR [ 106 ] ( ) fn = mk derived attribute ( metaclass , o dbattr ) setattr ( metaclass . clazz , o attr . Name , fn ) return metaclass", "predictions": ["generate an object of the attribute and assign it to the appropriate attribute of the attribute object"], "references": ["create a pyxtuml class from a bridgepoint class ."], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 5501, "code": "def mk simple association ( m , r simp ) : r rel = one ( r simp ) . R REL [ 206 ] ( ) r form = one ( r simp ) . R FORM [ 208 ] ( ) r part = one ( r simp ) . R PART [ 207 ] ( ) r rgo = one ( r form ) . R RGO [ 205 ] ( ) r rto = one ( r part ) . R RTO [ 204 ] ( ) if not r form : logger . info ( 'unformalized association R%s' % ( r rel . Numb ) ) r form = one ( r simp ) . R PART [ 207 ] ( lambda sel : sel != r part ) r rgo = one ( r form ) . R RTO [ 204 ] ( ) source o obj = one ( r rgo ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) target o obj = one ( r rto ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) source ids , target ids = get related attributes ( r rgo , r rto ) if source o obj . Obj ID != target o obj . Obj ID : source phrase = target phrase = '' else : source phrase = r part . Txt Phrs target phrase = r form . Txt Phrs m . define association ( rel id = r rel . Numb , source kind = source o obj . Key Lett , target kind = target o obj . Key Lett , source keys = source ids , target keys = target ids , source conditional = r form . Cond , target conditional = r part . Cond , source phrase = source phrase , target phrase = target phrase , source many = r form . Mult , target many = r part . Mult )", "predictions": ["create an ( simple encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded encoded as source encoded modifications"], "references": ["create a pyxtuml association from a simple association in bridgepoint ."], "bleu": 0.026594139297659906, "rouge_l": 0.07411907654921021}
{"id": 5502, "code": "def mk linked association ( m , r assoc ) : r rel = one ( r assoc ) . R REL [ 206 ] ( ) r rgo = one ( r assoc ) . R ASSR [ 211 ] . R RGO [ 205 ] ( ) source o obj = one ( r rgo ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) def mk assoc ( side1 , side2 ) : r rto = one ( side1 ) . R RTO [ 204 ] ( ) target o obj = one ( r rto ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) source ids , target ids = get related attributes ( r rgo , r rto ) if side1 . Obj ID != side2 . Obj ID : source phrase = target phrase = '' else : source phrase = side1 . Txt Phrs target phrase = side2 . Txt Phrs m . define association ( rel id = r rel . Numb , source kind = source o obj . Key Lett , target kind = target o obj . Key Lett , source keys = source ids , target keys = target ids , source conditional = side2 . Cond , target conditional = False , source phrase = source phrase , target phrase = target phrase , source many = side2 . Mult , target many = False ) r aone = one ( r assoc ) . R AONE [ 209 ] ( ) r aoth = one ( r assoc ) . R AOTH [ 210 ] ( ) mk assoc ( r aone , r aoth ) mk assoc ( r aoth , r aone )", "predictions": ["create a new linked object ."], "references": ["create pyxtuml associations from a linked association in bridgepoint ."], "bleu": 0.14260771622124252, "rouge_l": 0.47843137254901963}
{"id": 5503, "code": "def mk association ( m , r rel ) : handler = { 'R SIMP' : mk simple association , 'R ASSOC' : mk linked association , 'R SUBSUP' : mk subsuper association , 'R COMP' : mk derived association , } inst = subtype ( r rel , 206 ) fn = handler . get ( type ( inst ) . name ) return fn ( m , inst )", "predictions": ["generate a association object that can not be used to assign this type to the specified = 100 % ."], "references": ["create a pyxtuml association from a r_rel in ooaofooa ."], "bleu": 0.07264339766175722, "rouge_l": 0.21279069767441858}
{"id": 5504, "code": "def delete globals ( m , disconnect = False ) : filt = lambda sel : ( 247728914420827907967735776184937480192 <= sel . DT ID <= 247728914420827907967735776184937480208 ) for s dt in m . select many ( 'S DT' , filt ) : xtuml . delete ( one ( s dt ) . PE PE [ 8001 ] ( ) , disconnect ) xtuml . delete ( subtype ( s dt , 17 ) , disconnect ) xtuml . delete ( s dt , disconnect )", "predictions": ["delete all instances of an agent from one unit"], "references": ["remove global instances e . g . the core data type integer ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 5505, "code": "def accept ( self , reply socket , channel ) : info = self . info or b'' self . send raw ( reply socket , ACCEPT , info , * channel )", "predictions": ["makes the channel at the specified channel ."], "references": ["sends accept reply ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 5506, "code": "def reject ( self , reply socket , call id , topics = ( ) ) : info = self . info or b'' self . send raw ( reply socket , REJECT , info , call id , b'' , topics )", "predictions": ["calls the reply to the given socket ."], "references": ["sends reject reply ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 5507, "code": "def raise ( self , reply socket , channel , exc info = None ) : if not reply socket : return if exc info is None : exc info = sys . exc info ( ) exc type , exc , tb = exc info while tb . tb next is not None : tb = tb . tb next if issubclass ( exc type , Remote Exception ) : exc type = exc type . exc type filename , lineno = tb . tb frame . f code . co filename , tb . tb lineno val = ( exc type , str ( exc ) , filename , lineno ) try : state = exc . getstate ( ) except Attribute Error : pass else : val += ( state , ) self . send reply ( reply socket , RAISE , val , * channel )", "predictions": ["extends the reply from the reply ."], "references": ["sends raise reply ."], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 5508, "code": "def call wait ( self , hints , name , args , kwargs , topics = ( ) , raw = False , limit = None , retry = False , max retries = None ) : col = self . collector if not col . is running ( ) : col . start ( ) call id = uuid4 bytes ( ) reply to = ( DUPLEX if self . socket is col . socket else col . topic ) header = self . make header ( name , call id , reply to , hints ) payload = self . pack ( args , kwargs , raw ) def send call ( ) : try : safe ( send , self . socket , header , payload , topics , zmq . NOBLOCK ) except zmq . Again : raise Undelivered ( 'emission was not delivered' ) col . prepare ( call id , self , name , args , kwargs ) send call ( ) return col . establish ( call id , self . timeout , limit , send call if retry else None , max retries = max retries )", "predictions": ["waits for the topic to complete ."], "references": ["allocates a call id and emit ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5509, "code": "def dispatch reply ( self , reply , value ) : method = reply . method call id = reply . call id task id = reply . task id if method & ACK : try : result queue = self . result queues [ call id ] except Key Error : raise Key Error ( 'already established or unprepared call' ) if method == ACCEPT : worker info = value result = Remote Result ( self , call id , task id , worker info ) self . results [ call id ] [ task id ] = result result queue . put nowait ( result ) elif method == REJECT : result queue . put nowait ( None ) else : result = self . results [ call id ] [ task id ] result . set reply ( reply . method , value )", "predictions": ["dispatch to a reply ."], "references": ["dispatches the reply to the proper queue ."], "bleu": 0.1781815298791261, "rouge_l": 0.2953995157384988}
{"id": 5510, "code": "def guess type name ( value ) : value = str ( value ) if value . upper ( ) in [ 'TRUE' , 'FALSE' ] : return 'BOOLEAN' elif re . match ( r'(-)?(\\d+)(\\.\\d+)' , value ) : return 'REAL' elif re . match ( r'(-)?(\\d+)' , value ) : return 'INTEGER' elif re . match ( r'\\'((\\'\\')|[^\\'])*\\'' , value ) : return 'STRING' elif re . match ( r'\\\"([^\\\\\\n]|(\\\\.))*?\\\"' , value ) : return 'UNIQUE ID'", "predictions": ["guess the type of a type ."], "references": ["guess the type name of a serialized value ."], "bleu": 0.3210964829328844, "rouge_l": 0.7334669338677354}
{"id": 5511, "code": "def deserialize value ( ty , value ) : uty = ty . upper ( ) if uty == 'BOOLEAN' : if value . isdigit ( ) : return bool ( int ( value ) ) elif value . upper ( ) == 'FALSE' : return False elif value . upper ( ) == 'TRUE' : return True else : return None elif uty == 'INTEGER' : if '\"' in value : return uuid . UUID ( value [ 1 : - 1 ] ) . int else : return int ( value ) elif uty == 'REAL' : return float ( value ) elif uty == 'STRING' : return value [ 1 : - 1 ] . replace ( \"''\" , \"'\" ) elif uty == 'UNIQUE ID' : if '\"' in value : return uuid . UUID ( value [ 1 : - 1 ] ) . int else : return int ( value )", "predictions": ["deserialize a value from the provided mary ."], "references": ["deserialize a value of some type"], "bleu": 0.2984745896009823, "rouge_l": 0.43990384615384615}
{"id": 5512, "code": "def populate classes ( self , metamodel ) : for stmt in self . statements : if isinstance ( stmt , Create Class Stmt ) : metamodel . define class ( stmt . kind , stmt . attributes )", "predictions": ["populate named classes with dummy classes ."], "references": ["populate a * metamodel * with classes previously encountered from input ."], "bleu": 0.11434175042957104, "rouge_l": 0.40197693574958815}
{"id": 5513, "code": "def populate connections ( self , metamodel ) : storage = dict ( ) for ass in metamodel . associations : source class = ass . source link . to metaclass target class = ass . target link . to metaclass if target class not in storage : storage [ target class ] = dict ( ) link key = frozenset ( ass . source link . key map . values ( ) ) if link key not in storage [ target class ] : storage [ target class ] [ link key ] = dict ( ) for other inst in target class . storage : inst key = ass . source link . compute index key ( other inst ) if inst key is None : continue if inst key not in storage [ target class ] [ link key ] : storage [ target class ] [ link key ] [ inst key ] = xtuml . Ordered Set ( ) storage [ target class ] [ link key ] [ inst key ] . add ( other inst ) for inst in source class . storage : inst key = ass . source link . compute lookup key ( inst ) if inst key is None : continue if inst key not in storage [ target class ] [ link key ] : continue for other inst in storage [ target class ] [ link key ] [ inst key ] : ass . source link . connect ( other inst , inst , check = False ) ass . target link . connect ( inst , other inst , check = False ) for inst in metamodel . instances : metaclass = xtuml . get metaclass ( inst ) for attr in metaclass . referential attributes : if attr in inst . dict : delattr ( inst , attr )", "predictions": ["store connections for the metamodel ."], "references": ["populate links in a * metamodel * with connections between them ."], "bleu": 0.09663861439684919, "rouge_l": 0.20962199312714777}
{"id": 5514, "code": "def populate ( self , metamodel ) : self . populate classes ( metamodel ) self . populate unique identifiers ( metamodel ) self . populate associations ( metamodel ) self . populate instances ( metamodel ) self . populate connections ( metamodel )", "predictions": ["populate this class . this is called once per process or other classes can be called multiple times ."], "references": ["populate a * metamodel * with entities previously encountered from input ."], "bleu": 0.0712695567709093, "rouge_l": 0.1345093715545755}
{"id": 5515, "code": "def build metamodel ( self , id generator = None ) : m = xtuml . Meta Model ( id generator ) self . populate ( m ) return m", "predictions": ["build the ball model ."], "references": ["build and return a * xtuml . metamodel * containing previously loaded input ."], "bleu": 0.04994299940831281, "rouge_l": 0.19395866454689983}
{"id": 5516, "code": "def source ( self , feature names ) : if feature names is None : return True elif isinstance ( feature names , bool ) : return feature names else : return map ( lambda n : 'fc.' + n , feature names )", "predictions": ["look up a list of batches for a given feature ."], "references": ["maps feature names to es s _source field ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5517, "code": "def range filters ( self , * key ranges ) : filters = [ ] for s , e in key ranges : if isinstance ( s , basestring ) : s = eid ( s ) if isinstance ( e , basestring ) : e += u'\\U0010FFFF' e = eid ( e ) if s == ( ) and e == ( ) : filters . append ( { 'match all' : { } } ) elif e == ( ) : filters . append ( { 'range' : { ' id' : { 'gte' : s } } } ) elif s == ( ) : filters . append ( { 'range' : { ' id' : { 'lte' : e } } } ) else : filters . append ( { 'range' : { ' id' : { 'gte' : s , 'lte' : e } } } ) if len ( filters ) == 0 : return [ { 'match all' : { } } ] else : return filters", "predictions": ["range of all filters ."], "references": ["creates es filters for key ranges used in scanning ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5518, "code": "def create mappings ( self ) : self . conn . indices . put mapping ( index = self . index , doc type = self . type , timeout = 60 , request timeout = 60 , body = { self . type : { 'dynamic templates' : [ { 'default no analyze fc' : { 'match' : 'fc.*' , 'mapping' : { 'index' : 'no' } , } , } ] , ' all' : { 'enabled' : False , } , ' id' : { 'index' : 'not analyzed' , } , 'properties' : self . get index mappings ( ) , } , } ) # self . conn . cluster . health ( index = self . index , wait for status = 'yellow' )", "predictions": ["create a cluster to get the connection ."], "references": ["create the field type mapping ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 5519, "code": "def get index mappings ( self ) : maps = { } for fname in self . indexed features : config = self . indexes . get ( fname , { } ) print ( fname , config ) maps [ fname to idx name ( fname ) ] = { 'type' : config . get ( 'es index type' , 'integer' ) , 'store' : False , 'index' : 'not analyzed' , } for fname in self . fulltext indexed features : maps [ fname to full idx name ( fname ) ] = { 'type' : 'string' , 'store' : False , 'index' : 'analyzed' , } return maps", "predictions": ["this method returns maps to all mappings contained in this instance ."], "references": ["retrieve the field mappings . useful for debugging ."], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 5520, "code": "def get field types ( self ) : mapping = self . conn . indices . get mapping ( index = self . index , doc type = self . type ) return mapping [ self . index ] [ 'mappings' ] [ self . type ] [ 'properties' ]", "predictions": ["this method is called for each field in this class ."], "references": ["retrieve the field types . useful for debugging ."], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 5521, "code": "def fc index disjunction from query ( self , query fc , fname ) : if len ( query fc . get ( fname , [ ] ) ) == 0 : return [ ] terms = query fc [ fname ] . keys ( ) disj = [ ] for fname in self . indexes [ fname ] [ 'feature names' ] : disj . append ( { 'terms' : { fname to idx name ( fname ) : terms } } ) return disj", "predictions": ["generate a index from this significance instance ."], "references": ["creates a disjunction for keyword scan queries ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 5522, "code": "def fc bytes ( self , fc dict ) : num bytes = 0 for , feat in fc dict . iteritems ( ) : num bytes += len ( feat ) return num bytes", "predictions": ["allow a full copy of this feature to be modified ."], "references": ["take a feature collection in dict form and count its size in bytes ."], "bleu": 0.10312570678516415, "rouge_l": 0.2349165596919127}
{"id": 5523, "code": "def pretty string ( fc ) : s = [ ] for fname , feature in sorted ( fc . items ( ) ) : if isinstance ( feature , String Counter ) : feature = [ u'%s: %d' % ( k , v ) for ( k , v ) in feature . most common ( ) ] feature = u'\\n\\t' + u'\\n\\t' . join ( feature ) s . append ( fname + u': ' + feature ) return u'\\n' . join ( s )", "predictions": ["convert a string representation of a string into a sorted list ."], "references": ["construct a nice looking string for an fc"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 5524, "code": "def process docopts ( ) : arguments = docopt ( doc , version = \"Find Known Secrets {0}\" . format ( version ) ) logger . debug ( arguments ) if arguments [ \"here\" ] : go ( ) else : files = arguments [ \"--secrets\" ] searcher = Searcher ( source = arguments [ \"--source\" ] , files = files ) searcher . go ( )", "predictions": ["called from within the server being finalized ."], "references": ["take care of command line options"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5525, "code": "def default formatter ( error ) : quoted = formencode . htmlfill . escape formatter ( error ) return u'<span class=\"error-message\">{0}</span>' . format ( quoted )", "predictions": ["format an error formatter for the three card ."], "references": ["escape the error and wrap it in a span with class error - message"], "bleu": 0.08961856124931385, "rouge_l": 0.08367626886145405}
{"id": 5526, "code": "def pretty to link ( inst , link ) : values = '' prefix = '' metaclass = xtuml . get metaclass ( inst ) for name , ty in metaclass . attributes : if name in link . key map : value = getattr ( inst , name ) value = xtuml . serialize value ( value , ty ) name = link . key map [ name ] values += '%s%s=%s' % ( prefix , name , value ) prefix = ', ' return '%s(%s)' % ( link . kind , values )", "predictions": ["convert the link into a link ."], "references": ["create a human - readable representation of a link on the to - side"], "bleu": 0.09663861439684919, "rouge_l": 0.17967599410898377}
{"id": 5527, "code": "def pretty unique identifier ( inst , identifier ) : values = '' prefix = '' metaclass = xtuml . get metaclass ( inst ) for name , ty in metaclass . attributes : if name in metaclass . identifying attributes : value = getattr ( inst , name ) value = xtuml . serialize value ( value , ty ) values += '%s%s=%s' % ( prefix , name , value ) prefix = ', ' return '%s(%s)' % ( identifier , values )", "predictions": ["helper method to get an attribute ."], "references": ["create a human - readable representation a unique identifier ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5528, "code": "def check uniqueness constraint ( m , kind = None ) : if kind is None : metaclasses = m . metaclasses . values ( ) else : metaclasses = [ m . find metaclass ( kind ) ] res = 0 for metaclass in metaclasses : id map = dict ( ) for identifier in metaclass . indices : id map [ identifier ] = dict ( ) for inst in metaclass . select many ( ) : for name , ty in metaclass . attributes : if name not in metaclass . identifying attributes : continue value = getattr ( inst , name ) isnull = value is None isnull |= ( ty == 'UNIQUE ID' and not value ) if isnull : res += 1 logger . warning ( '%s.%s is part of an identifier and is null' % ( metaclass . kind , name ) ) for identifier in metaclass . indices : kwargs = dict ( ) for name in metaclass . indices [ identifier ] : kwargs [ name ] = getattr ( inst , name ) index key = frozenset ( kwargs . items ( ) ) if index key in id map [ identifier ] : res += 1 id string = pretty unique identifier ( inst , identifier ) logger . warning ( 'uniqueness constraint violation in %s, %s' % ( metaclass . kind , id string ) ) id map [ identifier ] [ index key ] = inst return res", "predictions": ["check the uniqueness constraint ."], "references": ["check the model for uniqueness constraint violations ."], "bleu": 0.2595169998796149, "rouge_l": 0.7384987893462469}
{"id": 5529, "code": "def check link integrity ( m , link ) : res = 0 for inst in link . from metaclass . select many ( ) : q set = list ( link . navigate ( inst ) ) if ( len ( q set ) < 1 and not link . conditional ) or ( ( len ( q set ) > 1 and not link . many ) ) : res += 1 logger . warning ( 'integrity violation in ' '%s --(%s)--> %s' % ( pretty from link ( inst , link ) , link . rel id , pretty to link ( inst , link ) ) ) return res", "predictions": ["checks for all link ' s link and link paths ."], "references": ["check the model for integrity violations on an association in a particular direction ."], "bleu": 0.09596928383261212, "rouge_l": 0.15661103979460847}
{"id": 5530, "code": "def check subtype integrity ( m , super kind , rel id ) : if isinstance ( rel id , int ) : rel id = 'R%d' % rel id res = 0 for inst in m . select many ( super kind ) : if not xtuml . navigate subtype ( inst , rel id ) : res += 1 logger . warning ( 'integrity violation across ' '%s[%s]' % ( super kind , rel id ) ) return res", "predictions": ["check if the provided text is already a valid type of the kind and kind of the kind ."], "references": ["check the model for integrity violations across a subtype association ."], "bleu": 0.08097785064266201, "rouge_l": 0.2801377726750861}
{"id": 5531, "code": "def basic transform ( val ) : if isinstance ( val , int ) : return struct . pack ( '>i' , val ) else : return safe lower utf8 ( val )", "predictions": ["basic transform of the transform object ."], "references": ["a basic transform for strings and integers ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 5532, "code": "def get type name ( s dt ) : s cdt = nav one ( s dt ) . S CDT [ 17 ] ( ) if s cdt and s cdt . Core Typ in range ( 1 , 6 ) : return s dt . Name s edt = nav one ( s dt ) . S EDT [ 17 ] ( ) if s edt : return s dt . Name s udt = nav one ( s dt ) . S UDT [ 17 ] ( ) if s udt : return s dt . Name", "predictions": ["get a name of a given class"], "references": ["get the xsd name of a s_dt"], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 5533, "code": "def get refered attribute ( o attr ) : o attr ref = nav one ( o attr ) . O RATTR [ 106 ] . O BATTR [ 113 ] . O ATTR [ 106 ] ( ) if o attr ref : return get refered attribute ( o attr ref ) else : return o attr", "predictions": ["get the entire attribute of an object without consuming it ."], "references": ["get the the referred attribute ."], "bleu": 0.17033186037639278, "rouge_l": 0.4969450101832994}
{"id": 5534, "code": "def build core type ( s cdt ) : s dt = nav one ( s cdt ) . S DT [ 17 ] ( ) if s dt . name == 'void' : type name = None elif s dt . name == 'boolean' : type name = 'xs:boolean' elif s dt . name == 'integer' : type name = 'xs:integer' elif s dt . name == 'real' : type name = 'xs:decimal' elif s dt . name == 'string' : type name = 'xs:string' elif s dt . name == 'unique id' : type name = 'xs:integer' else : type name = None if type name : mapped type = ET . Element ( 'xs:simple Type' , name = s dt . name ) ET . Sub Element ( mapped type , 'xs:restriction' , base = type name ) return mapped type", "predictions": ["builds a core type to use for the specified type"], "references": ["build an xsd simpletype out of a s_cdt ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 5535, "code": "def build enum type ( s edt ) : s dt = nav one ( s edt ) . S DT [ 17 ] ( ) enum = ET . Element ( 'xs:simple Type' , name = s dt . name ) enum list = ET . Sub Element ( enum , 'xs:restriction' , base = 'xs:string' ) first filter = lambda selected : not nav one ( selected ) . S ENUM [ 56 , 'succeeds' ] ( ) s enum = nav any ( s edt ) . S ENUM [ 27 ] ( first filter ) while s enum : ET . Sub Element ( enum list , 'xs:enumeration' , value = s enum . name ) s enum = nav one ( s enum ) . S ENUM [ 56 , 'precedes' ] ( ) return enum", "predictions": ["build a tree that represents a translation for the specified type"], "references": ["build an xsd simpletype out of a s_edt ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5536, "code": "def build struct type ( s sdt ) : s dt = nav one ( s sdt ) . S DT [ 17 ] ( ) struct = ET . Element ( 'xs:complex Type' , name = s dt . name ) first filter = lambda selected : not nav one ( selected ) . S MBR [ 46 , 'succeeds' ] ( ) s mbr = nav any ( s sdt ) . S MBR [ 44 ] ( first filter ) while s mbr : s dt = nav one ( s mbr ) . S DT [ 45 ] ( ) type name = get type name ( s dt ) ET . Sub Element ( struct , 'xs:attribute' , name = s mbr . name , type = type name ) s mbr = nav one ( s mbr ) . S MBR [ 46 , 'precedes' ] ( ) return struct", "predictions": ["delete a ( from the control to retrieve the control points"], "references": ["build an xsd complextype out of a s_sdt ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 5537, "code": "def build user type ( s udt ) : s dt user = nav one ( s udt ) . S DT [ 17 ] ( ) s dt base = nav one ( s udt ) . S DT [ 18 ] ( ) base name = get type name ( s dt base ) if base name : user = ET . Element ( 'xs:simple Type' , name = s dt user . name ) ET . Sub Element ( user , 'xs:restriction' , base = base name ) return user", "predictions": ["accept a ( . channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel"], "references": ["build an xsd simpletype out of a s_udt ."], "bleu": 0.026594139297659906, "rouge_l": 0.07750952986022873}
{"id": 5538, "code": "def build type ( s dt ) : s cdt = nav one ( s dt ) . S CDT [ 17 ] ( ) if s cdt : return build core type ( s cdt ) s edt = nav one ( s dt ) . S EDT [ 17 ] ( ) if s edt : return build enum type ( s edt ) s udt = nav one ( s dt ) . S UDT [ 17 ] ( ) if s udt : return build user type ( s udt )", "predictions": ["reject a \" ( \" to use for ("], "references": ["build a partial xsd tree out of a s_dt and its sub types s_cdt s_edt s_sdt and s_udt ."], "bleu": 0.046462271735933564, "rouge_l": 0.0671067106710671}
{"id": 5539, "code": "def build class ( o obj ) : cls = ET . Element ( 'xs:element' , name = o obj . key lett , min Occurs = '0' , max Occurs = 'unbounded' ) attributes = ET . Sub Element ( cls , 'xs:complex Type' ) for o attr in nav many ( o obj ) . O ATTR [ 102 ] ( ) : o attr ref = get refered attribute ( o attr ) s dt = nav one ( o attr ref ) . S DT [ 114 ] ( ) while nav one ( s dt ) . S UDT [ 17 ] ( ) : s dt = nav one ( s dt ) . S UDT [ 17 ] . S DT [ 18 ] ( ) type name = get type name ( s dt ) if type name and not nav one ( o attr ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : ET . Sub Element ( attributes , 'xs:attribute' , name = o attr . name , type = type name ) else : logger . warning ( 'Omitting %s.%s' % ( o obj . key lett , o attr . Name ) ) return cls", "predictions": ["builds a suitable suitable for the object"], "references": ["build an xsd complex element out of a o_obj including its o_attr ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 5540, "code": "def build component ( m , c c ) : component = ET . Element ( 'xs:element' , name = c c . name ) classes = ET . Sub Element ( component , 'xs:complex Type' ) classes = ET . Sub Element ( classes , 'xs:sequence' ) scope filter = lambda selected : ooaofooa . is contained in ( selected , c c ) for o obj in m . select many ( 'O OBJ' , scope filter ) : cls = build class ( o obj ) classes . append ( cls ) return component", "predictions": ["builds a wait wait wait for this wait wait on the specified : 1 . 2 . 3 . 2 . 3 . 3 . 3 . 3 . 3 . 2 . 3 . 3 . 3 . 3 name and also builds a wait wait for the ("], "references": ["build an xsd complex element out of a c_c including its packaged s_dt and o_obj ."], "bleu": 0.028577262451992175, "rouge_l": 0.06681270536692224}
{"id": 5541, "code": "def build schema ( m , c c ) : schema = ET . Element ( 'xs:schema' ) schema . set ( 'xmlns:xs' , 'http://www.w3.org/2001/XML Schema' ) global filter = lambda selected : ooaofooa . is global ( selected ) for s dt in m . select many ( 'S DT' , global filter ) : datatype = build type ( s dt ) if datatype is not None : schema . append ( datatype ) scope filter = lambda selected : ooaofooa . is contained in ( selected , c c ) for s dt in m . select many ( 'S DT' , scope filter ) : datatype = build type ( s dt ) if datatype is not None : schema . append ( datatype ) component = build component ( m , c c ) schema . append ( component ) return schema", "predictions": ["dispatch a reply to list of tokens that can be used as a reply ."], "references": ["build an xsd schema from a bridgepoint component ."], "bleu": 0.09103526405546068, "rouge_l": 0.17453505007153075}
{"id": 5542, "code": "def prettify ( xml string ) : reparsed = xml . dom . minidom . parse String ( xml string ) return reparsed . toprettyxml ( indent = \"    \" )", "predictions": ["[ filename ] str ."], "references": ["indent an xml string with four spaces and add an additional line break after each node ."], "bleu": 0.02476709768976917, "rouge_l": 0.08276797829036635}
{"id": 5543, "code": "def set positional info ( node , p ) : node . position = Position ( ) node . position . label = p . lexer . label node . position . start stream = p . lexpos ( 1 ) node . position . start line = p . lineno ( 1 ) node . position . start column = find column ( p . lexer . lexdata , node . position . start stream ) , node . position . end stream = p . lexspan ( len ( p ) - 1 ) , node . position . end line = p . linespan ( len ( p ) - 1 ) node . position . end column = find column ( p . lexer . lexdata , node . position . end stream ) - 1 node . character stream = p . lexer . lexdata [ node . position . start stream : node . position . end stream ]", "predictions": ["configure the value of all value represented by a , including in two ( ."], "references": ["set positional information on a node"], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 5544, "code": "def track production ( f ) : @ wraps ( f ) def wrapper ( self , p ) : r = f ( self , p ) node = p [ 0 ] if isinstance ( node , Node ) and len ( p ) > 1 : set positional info ( node , p ) return r return wrapper", "predictions": ["populate the wrapped node with the specified isinstance ."], "references": ["decorator for adding positional information to returning nodes"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5545, "code": "def create msg ( self , to , subject , msg Html , msg Plain , attachments = None ) : sender = self . sender if attachments and isinstance ( attachments , str ) : attachments = [ attachments ] else : attachments = list ( attachments or [ ] ) msg = MIME Multipart ( 'alternative' ) msg [ 'Subject' ] = subject msg [ 'From' ] = sender msg [ 'To' ] = to msg . attach ( MIME Text ( msg Plain , 'plain' ) ) msg . attach ( MIME Text ( msg Html , 'html' ) ) for path in attachments : attachment = self . prep attachment ( path ) msg . attach ( attachment ) raw = base64 . urlsafe b64encode ( msg . as bytes ( ) ) . decode ( ) #raw = raw.decode() body = { 'raw' : raw } return body", "predictions": ["inst is a simple html to send the loaded swarm message to the user ."], "references": ["attachments should be a list of paths"], "bleu": 0.08225964699966554, "rouge_l": 0.09728867623604465}
{"id": 5546, "code": "def read ( self ) : if self . connection . has changed ( ) : image path = self . connection . download image ( ) image = Image . open ( image path ) self . text cache = pytesseract . image to string ( image ) image . close ( ) return self . text cache", "predictions": ["populate an identifiers with the desired ."], "references": ["returns the text from an image at a given url ."], "bleu": 0.1247439242120089, "rouge_l": 0.2136602451838879}
{"id": 5547, "code": "def main ( ) : parser = optparse . Option Parser ( usage = \"%prog [options] <model path> [another model path..]\" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . add option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , default = 1 , help = \"increase debug logging level\" ) parser . add option ( \"-f\" , \"--function\" , dest = 'function' , action = \"store\" , help = \"invoke function named NAME\" , metavar = 'NAME' ) parser . add option ( \"-c\" , \"--component\" , dest = 'component' , action = \"store\" , help = \"look for the function in a component named NAME\" , metavar = 'NAME' , default = None ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or not opts . function : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) from bridgepoint import ooaofooa mm = ooaofooa . load metamodel ( args ) c c = mm . select any ( 'C C' , where ( Name = opts . component ) ) domain = ooaofooa . mk component ( mm , c c , derived attributes = False ) func = domain . find symbol ( opts . function ) return func ( )", "predictions": ["build the script to call ( metamodel"], "references": ["parse command line options and launch the interpreter"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5548, "code": "def main ( ) : parser = optparse . Option Parser ( usage = \"%prog [options] <model path> [another model path...]\" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . set description ( doc . strip ( ) ) parser . add option ( \"-c\" , \"--component\" , dest = \"component\" , metavar = \"NAME\" , help = \"export sql schema for the component named NAME\" , action = \"store\" , default = None ) parser . add option ( \"-d\" , \"--derived-attributes\" , dest = \"derived\" , help = \"include derived attributes in the schema\" , action = \"store true\" , default = False ) parser . add option ( \"-o\" , \"--output\" , dest = 'output' , metavar = \"PATH\" , help = \"save sql schema to PATH (required)\" , action = \"store\" , default = None ) parser . add option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = 2 ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or opts . output is None : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) loader = ooaofooa . Loader ( ) for filename in args : loader . filename input ( filename ) c = loader . build component ( opts . component , opts . derived ) xtuml . persist database ( c , opts . output )", "predictions": ["the main program to find the command line ."], "references": ["parse argv for options and arguments and start schema generation ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 5549, "code": "def serialize value ( value , ty ) : ty = ty . upper ( ) null value = { 'BOOLEAN' : False , 'INTEGER' : 0 , 'REAL' : 0.0 , 'STRING' : '' , 'UNIQUE ID' : 0 } transfer fn = { 'BOOLEAN' : lambda v : '%d' % int ( v ) , 'INTEGER' : lambda v : '%d' % v , 'REAL' : lambda v : '%f' % v , 'STRING' : lambda v : \"'%s'\" % v . replace ( \"'\" , \"''\" ) , 'UNIQUE ID' : lambda v : '\"%s\"' % uuid . UUID ( int = v ) } if value is None : value = null value [ ty ] return transfer fn [ ty ] ( value )", "predictions": ["method to range of the arguments that will be stored in the document ."], "references": ["serialize a value from an xtuml metamodel instance ."], "bleu": 0.08839374326825923, "rouge_l": 0.09050445103857567}
{"id": 5550, "code": "def serialize instance ( instance ) : attr count = 0 metaclass = xtuml . get metaclass ( instance ) s = 'INSERT INTO %s VALUES (' % metaclass . kind for name , ty in metaclass . attributes : value = getattr ( instance , name ) s += '\\n    ' s += serialize value ( value , ty ) attr count += 1 if attr count < len ( metaclass . attributes ) : s += ', -- %s : %s' % ( name , ty ) else : s += ' -- %s : %s' % ( name , ty ) s += '\\n);\\n' return s", "predictions": ["returns the representation of this instance ."], "references": ["serialize an * instance * from a metamodel ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5551, "code": "def serialize instances ( metamodel ) : s = '' for inst in metamodel . instances : s += serialize instance ( inst ) return s", "predictions": ["get the index of this cipher ."], "references": ["serialize all instances in a * metamodel * ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5552, "code": "def serialize association ( ass ) : s1 = '%s %s (%s)' % ( ass . source link . cardinality , ass . source link . to metaclass . kind , ', ' . join ( ass . source keys ) ) if ass . target link . phrase : s1 += \" PHRASE '%s'\" % ass . target link . phrase s2 = '%s %s (%s)' % ( ass . target link . cardinality , ass . target link . to metaclass . kind , ', ' . join ( ass . target keys ) ) if ass . source link . phrase : s2 += \" PHRASE '%s'\" % ass . source link . phrase return 'CREATE ROP REF ID %s FROM %s TO %s;\\n' % ( ass . rel id , s1 , s2 )", "predictions": ["method to get field suitable for testing ."], "references": ["serialize an xtuml metamodel association ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5553, "code": "def serialize class ( Cls ) : metaclass = xtuml . get metaclass ( Cls ) attributes = [ '%s %s' % ( name , ty . upper ( ) ) for name , ty in metaclass . attributes ] s = 'CREATE TABLE %s (\\n    ' % metaclass . kind s += ',\\n    ' . join ( attributes ) s += '\\n);\\n' return s", "predictions": ["returns the self - encoded index of this self - encoded self ."], "references": ["serialize an xtuml metamodel class ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 5554, "code": "def serialize schema ( metamodel ) : s = '' for kind in sorted ( metamodel . metaclasses . keys ( ) ) : s += serialize class ( metamodel . metaclasses [ kind ] . clazz ) for ass in sorted ( metamodel . associations , key = lambda x : x . rel id ) : s += serialize association ( ass ) return s", "predictions": ["fc - converts all vocabulary in this instance to a string ."], "references": ["serialize all class and association definitions in a * metamodel * ."], "bleu": 0.13065113298388567, "rouge_l": 0.3333333333333333}
{"id": 5555, "code": "def serialize ( resource ) : if isinstance ( resource , xtuml . Meta Model ) : return serialize database ( resource ) elif isinstance ( resource , type ) and issubclass ( resource , xtuml . Class ) : return serialize class ( resource ) elif isinstance ( resource , xtuml . Association ) : return serialize association ( resource ) elif isinstance ( resource , xtuml . Class ) : return serialize instance ( resource )", "predictions": ["pretty print the association for this ( ( without changing its serialization fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc . 0 fc"], "references": ["serialize some xtuml * resource * e . g . an instance or a complete metamodel ."], "bleu": 0.02403051755364481, "rouge_l": 0.03276047261009667}
{"id": 5556, "code": "def main ( ) : parser = Argument Parser ( description = \"search files using n-grams\" ) parser . add argument ( '--path' , dest = 'path' , help = \"where to search\" , nargs = 1 , action = \"store\" , default = getcwd ( ) ) parser . add argument ( '--update' , dest = 'update' , help = \"update the index\" , action = 'store true' , default = True ) parser . add argument ( '--filetype' , dest = 'filetype' , help = \"any, images, documents, code, audio, video\" , nargs = 1 , action = \"store\" , default = [ \"any\" ] ) parser . add argument ( '--verbose' , dest = 'verbose' , help = \"extended output\" , action = 'store true' , default = False ) parser . add argument ( '--results' , dest = 'results' , help = \"number of results to display\" , action = \"store\" , default = 10 ) parser . add argument ( 'query' , nargs = '+' , help = \"what to search\" , action = \"store\" ) args = parser . parse args ( ) if args . verbose : verbose = 2 pprint ( args ) else : verbose = 0 query = args . query [ 0 ] for arg in args . query [ 1 : ] : query = query + \" \" + arg slb = min ( [ len ( w ) for w in query . split ( \" \" ) ] ) files = Files ( path = args . path , filetype = args . filetype [ 0 ] , exclude = [ ] , update = args . update , verbose = verbose ) index = Index ( files , slb = slb , verbose = verbose ) results = index . search ( query , verbose = verbose ) Handler ( results , results number = int ( args . results ) )", "predictions": ["the main entry point ."], "references": ["function for command line execution"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5557, "code": "def partition ( condition , collection ) -> Tuple [ List , List ] : succeed , fail = [ ] , [ ] for x in collection : if condition ( x ) : succeed . append ( x ) else : fail . append ( x ) return succeed , fail", "predictions": ["default implementation of default euclidean instances ."], "references": ["partitions a list into two based on a condition ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5558, "code": "def find link ( inst1 , inst2 , rel id , phrase ) : metaclass1 = get metaclass ( inst1 ) metaclass2 = get metaclass ( inst2 ) if isinstance ( rel id , int ) : rel id = 'R%d' % rel id for ass in metaclass1 . metamodel . associations : if ass . rel id != rel id : continue if ( ass . source link . from metaclass . kind == metaclass1 . kind and ass . source link . to metaclass . kind == metaclass2 . kind and ass . source link . phrase == phrase ) : return inst1 , inst2 , ass if ( ass . target link . from metaclass . kind == metaclass1 . kind and ass . target link . to metaclass . kind == metaclass2 . kind and ass . target link . phrase == phrase ) : return inst2 , inst1 , ass raise Unknown Link Exception ( metaclass1 . kind , metaclass2 . kind , rel id , phrase )", "predictions": ["pretty - qualified to pretty - qualified ones ."], "references": ["find links that correspond to the given arguments ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5559, "code": "def get metaclass ( class or instance ) : if isinstance ( class or instance , Class ) : return class or instance . metaclass elif issubclass ( class or instance , Class ) : return class or instance . metaclass raise Meta Exception ( \"the provided argument is not an xtuml class or instance\" )", "predictions": ["look up and then use unique if it is a unique unique unique ( or class ) ."], "references": ["get the metaclass for a * class_or_instance * ."], "bleu": 0.07535838128770536, "rouge_l": 0.15762273901808785}
{"id": 5560, "code": "def disconnect ( self , instance , another instance ) : if instance not in self : return False if another instance not in self [ instance ] : return False self [ instance ] . remove ( another instance ) return True", "predictions": ["check the instances of an m to check for a certain m ."], "references": ["disconnect an * instance * from * another_instance * ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 5561, "code": "def attribute type ( self , attribute name ) : attribute name = attribute name . upper ( ) for name , ty in self . attributes : if name . upper ( ) == attribute name : return ty", "predictions": ["add a link to this visualization ."], "references": ["obtain the type of an attribute ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5562, "code": "def add link ( self , metaclass , rel id , phrase , conditional , many ) : link = Link ( self , rel id , metaclass , phrase , conditional , many ) key = ( metaclass . kind . upper ( ) , rel id , phrase ) self . links [ key ] = link return link", "predictions": ["this method makes a subtype for the given subtype . the amount will be inserted to the local game ."], "references": ["add a new link from * self * to * metaclass * ."], "bleu": 0.07264339766175722, "rouge_l": 0.18904958677685949}
{"id": 5563, "code": "def delete attribute ( self , name ) : for idx , attr in enumerate ( self . attributes ) : attr name , = attr if attr name == name : del self . attributes [ idx ] return", "predictions": ["basic attribute with the specified name ."], "references": ["delete an attribute with a given * name * from the list of attributes ."], "bleu": 0.0927110373244369, "rouge_l": 0.3412587412587413}
{"id": 5564, "code": "def default value ( self , type name ) : uname = type name . upper ( ) if uname == 'BOOLEAN' : return False elif uname == 'INTEGER' : return 0 elif uname == 'REAL' : return 0.0 elif uname == 'STRING' : return '' elif uname == 'UNIQUE ID' : if self . metamodel : return next ( self . metamodel . id generator ) else : return None else : raise Meta Exception ( \"Unknown type named '%s'\" % type name )", "predictions": ["method that maps the get or named to a get or named ."], "references": ["obtain the default value for some * type name * ."], "bleu": 0.10571070857151538, "rouge_l": 0.16920943134535368}
{"id": 5565, "code": "def new ( self , * args , * * kwargs ) : inst = self . clazz ( ) self . storage . append ( inst ) referential attributes = dict ( ) for name , ty in self . attributes : if name not in self . referential attributes : value = self . default value ( ty ) setattr ( inst , name , value ) for attr , value in zip ( self . attributes , args ) : name , ty = attr if name not in self . referential attributes : setattr ( inst , name , value ) else : referential attributes [ name ] = value for name , value in kwargs . items ( ) : if name not in self . referential attributes : setattr ( inst , name , value ) else : referential attributes [ name ] = value if not referential attributes : return inst for link in self . links . values ( ) : if set ( link . key map . values ( ) ) - set ( referential attributes . keys ( ) ) : continue kwargs = dict ( ) for key , value in link . key map . items ( ) : kwargs [ key ] = referential attributes [ value ] if not kwargs : continue for other inst in link . to metaclass . query ( kwargs ) : relate ( other inst , inst , link . rel id , link . phrase ) for name , value in referential attributes . items ( ) : if getattr ( inst , name ) != value : logger . warning ( 'unable to assign %s to %s' , name , inst ) return inst", "predictions": ["we use this to link ( instead of ( methods ."], "references": ["create and return a new instance ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 5566, "code": "def instances ( self ) : for metaclass in self . metaclasses . values ( ) : for inst in metaclass . storage : yield inst", "predictions": ["generate a ) of all build ."], "references": ["obtain a sequence of all instances in the metamodel ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 5567, "code": "def define class ( self , kind , attributes , doc = '' ) : ukind = kind . upper ( ) if ukind in self . metaclasses : raise Meta Model Exception ( 'A class with the name %s is already defined' % kind ) metaclass = Meta Class ( kind , self ) for name , ty in attributes : metaclass . append attribute ( name , ty ) self . metaclasses [ ukind ] = metaclass return metaclass", "predictions": ["build a enum for this edt ."], "references": ["define a new class in the metamodel and return its metaclass ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5568, "code": "def find metaclass ( self , kind ) : ukind = kind . upper ( ) if ukind in self . metaclasses : return self . metaclasses [ ukind ] else : raise Unknown Class Exception ( kind )", "predictions": ["find the translation of this class ."], "references": ["find a metaclass of some * kind * in the metamodel ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 5569, "code": "def dead code ( ) : with safe cd ( SRC ) : if IS TRAVIS : command = \"{0} vulture {1}\" . format ( PYTHON , PROJECT NAME ) . strip ( ) . split ( ) else : command = \"{0} vulture {1}\" . format ( PIPENV , PROJECT NAME ) . strip ( ) . split ( ) output file name = \"dead code.txt\" with open ( output file name , \"w\" ) as outfile : env = config pythonpath ( ) subprocess . call ( command , stdout = outfile , env = env ) cutoff = 20 num lines = sum ( 1 for line in open ( output file name ) if line ) if num lines > cutoff : print ( \"Too many lines of dead code : {0}, max {1}\" . format ( num lines , cutoff ) ) exit ( - 1 )", "predictions": ["generate all the ( of the program ."], "references": ["this also finds code you are working on today!"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 5570, "code": "def parse emails ( values ) : emails = [ ] if isinstance ( values , str ) : values = [ values ] for value in values : matches = re emails . findall ( value ) emails . extend ( [ match [ 2 ] for match in matches ] ) return emails", "predictions": ["parse the comma separated list of values ."], "references": ["take a string or list of strings and try to extract all the emails"], "bleu": 0.10712878727413526, "rouge_l": 0.1732954545454545}
{"id": 5571, "code": "def rpc ( f = None , * * kwargs ) : if f is not None : if isinstance ( f , six . string types ) : if 'name' in kwargs : raise Value Error ( 'name option duplicated' ) kwargs [ 'name' ] = f else : return rpc ( * * kwargs ) ( f ) return functools . partial ( rpc , * * kwargs )", "predictions": ["convert a new rpc instance to the rpc ."], "references": ["marks a method as rpc ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 5572, "code": "def rpc spec table ( app ) : table = { } for attr , value in inspect . getmembers ( app ) : rpc spec = get rpc spec ( value , default = None ) if rpc spec is None : continue table [ rpc spec . name ] = ( value , rpc spec ) return table", "predictions": ["return the rpc for the given table"], "references": ["collects methods which are speced as rpc ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5573, "code": "async def normalize postcode middleware ( request , handler ) : postcode : Optional [ str ] = request . match info . get ( 'postcode' , None ) if postcode is None or postcode == \"random\" : return await handler ( request ) elif not is uk postcode ( postcode ) : raise web . HTTP Not Found ( text = \"Invalid Postcode\" ) postcode processed = postcode . upper ( ) . replace ( \" \" , \"\" ) if postcode processed == postcode : return await handler ( request ) else : url name = request . match info . route . name url = request . app . router [ url name ] params = dict ( request . match info ) params [ 'postcode' ] = postcode processed raise web . HTTP Moved Permanently ( str ( url . url for ( * * params ) ) )", "predictions": ["normalizes the request using the http router ."], "references": ["if there is a postcode in the url it validates and normalizes it ."], "bleu": 0.09008421318929809, "rouge_l": 0.1732954545454545}
{"id": 5574, "code": "def next ( self ) : val = self . current self . current = self . readfunc ( ) return val", "predictions": ["get next element in the list ."], "references": ["progress to the next identifier and return the current one ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 5575, "code": "def accept S SYS ( self , inst ) : for child in many ( inst ) . EP PKG [ 1401 ] ( ) : self . accept ( child )", "predictions": ["creates a new ( ."], "references": ["a system model contains top - level packages"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5576, "code": "def accept C C ( self , inst ) : for child in many ( inst ) . PE PE [ 8003 ] ( ) : self . accept ( child )", "predictions": ["makes the given visitor child class ."], "references": ["a component contains packageable elements"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5577, "code": "def accept EP PKG ( self , inst ) : for child in many ( inst ) . PE PE [ 8000 ] ( ) : self . accept ( child )", "predictions": ["accept the writeable writeable ."], "references": ["a package contains packageable elements"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5578, "code": "def get brightness ( self ) : if not self . connection . has changed ( ) : return self . image brightness image path = self . connection . download image ( ) converted image = Image . open ( image path ) . convert ( 'L' ) statistics = Image Stat . Stat ( converted image ) self . image brightness = statistics . mean [ 0 ] return self . image brightness", "predictions": ["get brightness statistics for an image ."], "references": ["return the average brightness of the image ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 5579, "code": "def selection for character ( self , position ) : selection = Qt Gui . Q Text Edit . Extra Selection ( ) cursor = self . text edit . text Cursor ( ) cursor . set Position ( position ) cursor . move Position ( Qt Gui . Q Text Cursor . Next Character , Qt Gui . Q Text Cursor . Keep Anchor ) selection . cursor = cursor selection . format = self . format return selection", "predictions": ["a method for the given text formatted to the given position ."], "references": ["convenience method for selecting a character ."], "bleu": 0.15537125692760353, "rouge_l": 0.3315217391304348}
{"id": 5580, "code": "def cursor position changed ( self ) : self . text edit . set Extra Selections ( [ ] ) cursor = self . text edit . text Cursor ( ) if not cursor . has Selection ( ) : position = cursor . position ( ) - 1 match position = self . find match ( position ) if match position != - 1 : extra selections = [ self . selection for character ( pos ) for pos in ( position , match position ) ] self . text edit . set Extra Selections ( extra selections )", "predictions": ["run each of the method invocation . this will edit the text matched by the call to the default method ."], "references": ["updates the document formatting based on the new cursor position ."], "bleu": 0.0690889519686715, "rouge_l": 0.19869706840390877}
{"id": 5581, "code": "def exc info ( self ) : e = self . exc info ( ) if sys . platform == 'cli' : if isinstance ( e [ 0 ] , String Exception ) : e = ( str ( e [ 0 ] ) , e [ 1 ] , e [ 2 ] ) return e", "predictions": ["return information about the message ."], "references": ["bottleneck to fix up ironpython string exceptions"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5582, "code": "def run ( self , result ) : log . debug ( \"suite %s (%s) run called, tests: %s\" , id ( self ) , self , self . tests ) #import pdb #pdb.set trace() if self . result Proxy : result , orig = self . result Proxy ( result , self ) , result else : result , orig = result , result try : self . set Up ( ) except Keyboard Interrupt : raise except : self . error context = 'setup' result . add Error ( self , self . exc info ( ) ) return try : for test in self . tests : if result . should Stop : log . debug ( \"stopping\" ) break test ( orig ) finally : self . has run = True try : self . tear Down ( ) except Keyboard Interrupt : raise except : self . error context = 'teardown' result . add Error ( self , self . exc info ( ) )", "predictions": ["run the given method to run the server ."], "references": ["run tests in suite inside of suite fixtures ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5583, "code": "def options ( self , parser , env ) : parser . add option ( '--collect-only' , action = 'store true' , dest = self . enable Opt , default = env . get ( 'NOSE COLLECT ONLY' ) , help = \"Enable collect-only: %s [COLLECT ONLY]\" % ( self . help ( ) ) )", "predictions": ["create a new command line ."], "references": ["register commandline options ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 5584, "code": "def execute ( self , source = None , hidden = False , interactive = False ) : if not hidden : history = self . input buffer if source is None else source executed = super ( History Console Widget , self ) . execute ( source , hidden , interactive ) if executed and not hidden : history = history . rstrip ( ) if history and ( not self . history or self . history [ - 1 ] != history ) : self . history . append ( history ) self . history edits = { } self . history index = len ( self . history ) return executed", "predictions": ["call this method to execute a job ."], "references": ["reimplemented to the store history ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 5585, "code": "def handle execute reply ( self , msg ) : msg id = msg [ 'parent header' ] [ 'msg id' ] info = self . request info [ 'execute' ] . pop ( msg id , None ) if info and info . kind == 'save magic' and not self . hidden : content = msg [ 'content' ] status = content [ 'status' ] if status == 'ok' : self . max session history = ( int ( content [ 'user expressions' ] [ 'hlen' ] ) )", "predictions": ["gray-level session on lucene . ."], "references": ["handles replies for code execution here only session history length"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5586, "code": "def history locked ( self ) : return ( self . history lock and ( self . get edited history ( self . history index ) != self . input buffer ) and ( self . get prompt cursor ( ) . block Number ( ) != self . get end cursor ( ) . block Number ( ) ) )", "predictions": ["we use this to prompt the last one or more history for this cursor ."], "references": ["returns whether history movement is locked ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 5587, "code": "def get edited history ( self , index ) : if index in self . history edits : return self . history edits [ index ] elif index == len ( self . history ) : return unicode ( ) return self . history [ index ]", "predictions": ["get the amount of items currently registered with this history ."], "references": ["retrieves a history item possibly with temporary edits ."], "bleu": 0.1354599427337814, "rouge_l": 0.2036727879799666}
{"id": 5588, "code": "def set history ( self , history ) : self . history = list ( history ) self . history edits = { } self . history index = len ( self . history )", "predictions": ["for example , you can start the process ."], "references": ["replace the current history with a sequence of history items ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 5589, "code": "def store edits ( self ) : current = self . input buffer if self . history index == len ( self . history ) or self . history [ self . history index ] != current : self . history edits [ self . history index ] = current", "predictions": ["for example , we need to set the history of this class ."], "references": ["if there are edits to the current input buffer store them ."], "bleu": 0.1135935489027116, "rouge_l": 0.2417437252311757}
{"id": 5590, "code": "def On Time To Close ( self , evt ) : print ( \"See ya later!\" ) sys . stdout . flush ( ) self . cleanup consoles ( evt ) self . Close ( ) sys . exit ( )", "predictions": ["flushes this instance , then flushes the associated contact ."], "references": ["event handler for the button click ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 5591, "code": "def cleanup files ( self ) : logger . debug ( 'Cleaning up...' ) with indent log ( ) : for req in self . reqs to cleanup : req . remove temporary source ( ) if self . pip has created build dir ( ) : logger . debug ( 'Removing temporary dir %s...' , self . build dir ) rmtree ( self . build dir )", "predictions": ["clean up the dependencies of ( and then we can remove all files from the log directory ."], "references": ["clean up files remove builds ."], "bleu": 0.10657284485555579, "rouge_l": 0.3663663663663663}
{"id": 5592, "code": "def subscribe ( self ) : self . stream . setsockopt ( zmq . UNSUBSCRIBE , '' ) if '' in self . topics : self . log . debug ( \"Subscribing to: everything\" ) self . stream . setsockopt ( zmq . SUBSCRIBE , '' ) else : for topic in self . topics : self . log . debug ( \"Subscribing to: %r\" % ( topic ) ) self . stream . setsockopt ( zmq . SUBSCRIBE , topic )", "predictions": ["subscribe to all zmq output ."], "references": ["update our sub socket s subscriptions ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5593, "code": "def log message ( self , raw ) : if len ( raw ) != 2 or '.' not in raw [ 0 ] : self . log . error ( \"Invalid log message: %s\" % raw ) return else : topic , msg = raw topic , level name = topic . rsplit ( '.' , 1 ) level , topic = self . extract level ( topic ) if msg [ - 1 ] == '\\n' : msg = msg [ : - 1 ] self . log . log ( level , \"[%s] %s\" % ( topic , msg ) )", "predictions": ["this method logs message at the bottom of the message ."], "references": ["receive and parse a message then log it ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5594, "code": "def remote iterator ( view , name ) : view . execute ( 'it%s=iter(%s)' % ( name , name ) , block = True ) while True : try : result = view . apply sync ( lambda x : x . next ( ) , Reference ( 'it' + name ) ) except Remote Error as e : if e . ename == 'Stop Iteration' : raise Stop Iteration else : raise e else : yield result", "predictions": ["remote iterator . this is done by the ( method ."], "references": ["return an iterator on an object living on a remote engine ."], "bleu": 0.12368857073777001, "rouge_l": 0.17256011315417258}
{"id": 5595, "code": "def String IO ( * args , * * kw ) : global String IO try : from c String IO import String IO except Import Error : from String IO import String IO return String IO ( * args , * * kw )", "predictions": ["protects at http put ."], "references": ["thunk to load the real stringio on demand"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 5596, "code": "def insert on ( self , path , loc = None ) : loc = loc or self . location if self . project name == 'setuptools' : try : version = self . version except Value Error : version = '' if '0.7' in version : raise Value Error ( \"A 0.7-series setuptools cannot be installed \" \"with distribute. Found one at %s\" % str ( self . location ) ) if not loc : return if path is sys . path : self . check version conflict ( ) nloc = normalize cached ( loc ) bdir = os . path . dirname ( nloc ) npath = map ( normalize cached , path ) bp = None for p , item in enumerate ( npath ) : if item == nloc : break elif item == bdir and self . precedence == EGG DIST : path . insert ( p , loc ) npath . insert ( p , nloc ) break else : path . append ( loc ) return while 1 : try : np = npath . index ( nloc , p + 1 ) except Value Error : break else : del npath [ np ] , path [ np ] p = np return", "predictions": ["inserts a setuptools file ."], "references": ["insert self . location in path before its nearest parent directory"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5597, "code": "def parsed pkg info ( self ) : try : return self . pkg info except Attribute Error : from email . parser import Parser self . pkg info = Parser ( ) . parsestr ( self . get metadata ( self . PKG INFO ) ) return self . pkg info", "predictions": ["wraps this method to provide dependencies into a parsed parsed email ."], "references": ["parse and cache metadata"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 5598, "code": "def compute dependencies ( self ) : from markerlib import compile as compile marker dm = self . dep map = { None : [ ] } reqs = [ ] for req in self . parsed pkg info . get all ( 'Requires-Dist' ) or [ ] : distvers , mark = self . preparse requirement ( req ) parsed = parse requirements ( distvers ) . next ( ) parsed . marker fn = compile marker ( mark ) reqs . append ( parsed ) def reqs for extra ( extra ) : for req in reqs : if req . marker fn ( override = { 'extra' : extra } ) : yield req common = frozenset ( reqs for extra ( None ) ) dm [ None ] . extend ( common ) for extra in self . parsed pkg info . get all ( 'Provides-Extra' ) or [ ] : extra = safe extra ( extra . strip ( ) ) dm [ extra ] = list ( frozenset ( reqs for extra ( extra ) ) - common ) return dm", "predictions": ["compute the package includes for all pending dependencies ."], "references": ["recompute this distribution s dependencies ."], "bleu": 0.18575057999133596, "rouge_l": 0.27664399092970515}
{"id": 5599, "code": "def collapse leading ws ( header , txt ) : if header . lower ( ) == 'description' : return '\\n' . join ( [ x [ 8 : ] if x . startswith ( ' ' * 8 ) else x for x in txt . strip ( ) . splitlines ( ) ] ) else : return ' ' . join ( [ x . strip ( ) for x in txt . splitlines ( ) ] )", "predictions": ["replace whitespace with single space character ."], "references": ["description header must preserve newlines ; all others need not"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 5600, "code": "def hide Event ( self , event ) : super ( Completion Widget , self ) . hide Event ( event ) self . text edit . cursor Position Changed . disconnect ( self . update current ) self . text edit . remove Event Filter ( self )", "predictions": ["create an event from the process ."], "references": ["reimplemented to disconnect signal handlers and event filter ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5601, "code": "def show Event ( self , event ) : super ( Completion Widget , self ) . show Event ( event ) self . text edit . cursor Position Changed . connect ( self . update current ) self . text edit . install Event Filter ( self )", "predictions": ["create the full command line ."], "references": ["reimplemented to connect signal handlers and event filter ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 5602, "code": "def complete current ( self ) : self . current text cursor ( ) . insert Text ( self . current Item ( ) . text ( ) ) self . hide ( )", "predictions": ["builds a new item at the top of the list ."], "references": ["perform the completion with the currently selected item ."], "bleu": 0.14323145079400493, "rouge_l": 0.3055091819699499}
{"id": 5603, "code": "def update current ( self ) : prefix = self . current text cursor ( ) . selection ( ) . to Plain Text ( ) if prefix : items = self . find Items ( prefix , ( Qt Core . Qt . Match Starts With | Qt Core . Qt . Match Case Sensitive ) ) if items : self . set Current Item ( items [ 0 ] ) else : self . hide ( ) else : self . hide ( )", "predictions": ["rpc method to rpc rpc with awt"], "references": ["updates the current item based on the current text ."], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 5604, "code": "def register Admin Site ( app Name , exclude Models = [ ] ) : for model in apps . get app config ( app Name ) . get models ( ) : if model not in exclude Models : admin . site . register ( model )", "predictions": ["rpc method to rpc rpc so that it can be automatically added to the list of [ filename ]"], "references": ["registers the models of the app with the given appname for the admin site"], "bleu": 0.0712695567709093, "rouge_l": 0.12461695607763024}
{"id": 5605, "code": "def virtual memory ( ) : mem = psutil mswindows . get virtual mem ( ) totphys , availphys , totpagef , availpagef , totvirt , freevirt = mem # total = totphys avail = availphys free = availphys used = total - avail percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free )", "predictions": ["get the def object ."], "references": ["system virtual memory as a namedtuple ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 5606, "code": "def get disk usage ( path ) : try : total , free = psutil mswindows . get disk usage ( path ) except Windows Error : err = sys . exc info ( ) [ 1 ] if not os . path . exists ( path ) : raise OS Error ( errno . ENOENT , \"No such file or directory: '%s'\" % path ) raise used = total - free percent = usage percent ( used , total , round = 1 ) return nt diskinfo ( total , used , free , percent )", "predictions": ["tries to find the ( ( return val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val val"], "references": ["return disk usage associated with path ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 5607, "code": "def disk partitions ( all ) : rawlist = psutil mswindows . get disk partitions ( all ) return [ nt partition ( * x ) for x in rawlist ]", "predictions": ["get the accept partitions partitions"], "references": ["return disk partitions ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 5608, "code": "def get system cpu times ( ) : user , system , idle = 0 , 0 , 0 for cpu time in psutil mswindows . get system cpu times ( ) : user += cpu time [ 0 ] system += cpu time [ 1 ] idle += cpu time [ 2 ] return cputimes ntuple ( user , system , idle )", "predictions": ["gets system ( possibly non - zero inst inst inst inst inst inst inst inst inst inst . inst"], "references": ["return system cpu times as a named tuple ."], "bleu": 0.0712695567709093, "rouge_l": 0.15269086357947434}
{"id": 5609, "code": "def get system per cpu times ( ) : ret = [ ] for cpu t in psutil mswindows . get system cpu times ( ) : user , system , idle = cpu t item = cputimes ntuple ( user , system , idle ) ret . append ( item ) return ret", "predictions": ["gets the value of all system for this other ( i . e . a named system inst inst inst inst inst . . . . . . . inst . inst inst inst . . inst . . inst . . . . , . , . , ."], "references": ["return system per - cpu times as a list of named tuples ."], "bleu": 0.03162593967015063, "rouge_l": 0.14202561117578583}
{"id": 5610, "code": "def get system users ( ) : retlist = [ ] rawlist = psutil mswindows . get system users ( ) for item in rawlist : user , hostname , tstamp = item nt = nt user ( user , None , hostname , tstamp ) retlist . append ( nt ) return retlist", "predictions": ["get all the ( name / value ) formatted as a list of all bson ."], "references": ["return currently connected users as a list of namedtuples ."], "bleu": 0.2240750868020436, "rouge_l": 0.4013157894736842}
{"id": 5611, "code": "def stdin raw nonblock ( self ) : handle = msvcrt . get osfhandle ( sys . stdin . fileno ( ) ) result = Wait For Single Object ( handle , 100 ) if result == WAIT FAILED : raise ctypes . Win Error ( ) elif result == WAIT TIMEOUT : print ( \".\" , end = '' ) return None else : data = ctypes . create string buffer ( 256 ) bytes Read = DWORD ( 0 ) print ( '?' , end = '' ) if not Read File ( handle , data , 256 , ctypes . byref ( bytes Read ) , None ) : raise ctypes . Win Error ( ) Flush Console Input Buffer ( handle ) data = data . value data = data . replace ( '\\r\\n' , '\\n' ) data = data . replace ( '\\r' , '\\n' ) print ( repr ( data ) + \" \" , end = '' ) return data", "predictions": ["this will try to read the given file and then use it to read the given ) ."], "references": ["use the raw win32 handle of sys . stdin to do non - blocking reads"], "bleu": 0.08562365224473284, "rouge_l": 0.18484848484848485}
{"id": 5612, "code": "def stdin raw block ( self ) : try : data = sys . stdin . read ( 1 ) data = data . replace ( '\\r' , '\\n' ) return data except Windows Error as we : if we . winerror == ERROR NO DATA : return None else : raise we", "predictions": ["reads position from text ."], "references": ["use a blocking stdin read"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5613, "code": "def stdout raw ( self , s ) : print ( s , end = '' , file = sys . stdout ) sys . stdout . flush ( )", "predictions": ["flushes the exc and flushes the exc ."], "references": ["writes the string to stdout"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5614, "code": "def stderr raw ( self , s ) : print ( s , end = '' , file = sys . stderr ) sys . stderr . flush ( )", "predictions": ["flushes the ( : run : run : run , run , run , run , run , run , run : run : run , run : run : run : run : run , ( : run , ( : ( : ( , ( : ( ,"], "references": ["writes the string to stdout"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 5615, "code": "def create tab with current kernel ( self ) : current widget = self . tab widget . current Widget ( ) current widget index = self . tab widget . index Of ( current widget ) current widget name = self . tab widget . tab Text ( current widget index ) widget = self . slave frontend factory ( current widget ) if 'slave' in current widget name : name = current widget name else : name = '(%s) slave' % current widget name self . add tab with frontend ( widget , name = name )", "predictions": ["creates a ( add a ( enable : : parser : int . awt : : : : : : : : filename . awt . ethernet . com . ethernet env : java . awt . runners . ethernet . : , env = ,"], "references": ["create a new frontend attached to the same kernel as the current tab"], "bleu": 0.02614431568998955, "rouge_l": 0.03770086526576019}
{"id": 5616, "code": "def add tab with frontend ( self , frontend , name = None ) : if not name : name = 'kernel %i' % self . next kernel id self . tab widget . add Tab ( frontend , name ) self . update tab bar visibility ( ) self . make frontend visible ( frontend ) frontend . exit requested . connect ( self . close tab )", "predictions": ["execute a ( or [ , [ , , , [ , , , , , , + , , . . . . . ) )"], "references": ["insert a tab with a given frontend in the tab bar and give it a name"], "bleu": 0.044915755686574035, "rouge_l": 0.04876099120703437}
{"id": 5617, "code": "def close Event ( self , event ) : if self . tab widget . count ( ) == 0 : event . accept ( ) return title = self . window ( ) . window Title ( ) cancel = Qt Gui . Q Message Box . Cancel okay = Qt Gui . Q Message Box . Ok if self . confirm exit : if self . tab widget . count ( ) > 1 : msg = \"Close all tabs, stop all kernels, and Quit?\" else : msg = \"Close console, stop kernel, and Quit?\" info = \"Kernels not started here (e.g. notebooks) will be left alone.\" closeall = Qt Gui . Q Push Button ( \"&Quit\" , self ) closeall . set Shortcut ( 'Q' ) box = Qt Gui . Q Message Box ( Qt Gui . Q Message Box . Question , title , msg ) box . set Informative Text ( info ) box . add Button ( cancel ) box . add Button ( closeall , Qt Gui . Q Message Box . Yes Role ) box . set Default Button ( closeall ) box . set Escape Button ( cancel ) pixmap = Qt Gui . Q Pixmap ( self . app . icon . pixmap ( Qt Core . Q Size ( 64 , 64 ) ) ) box . set Icon Pixmap ( pixmap ) reply = box . exec ( ) else : reply = okay if reply == cancel : event . ignore ( ) return if reply == okay : while self . tab widget . count ( ) >= 1 : widget = self . active frontend widget . confirm exit = False self . close tab ( widget ) event . accept ( )", "predictions": ["closes the [ multi - pop ] and handle the [ . ] ."], "references": ["forward the close event to every tabs contained by the windows"], "bleu": 0.09782375748961449, "rouge_l": 0.16353887399463804}
{"id": 5618, "code": "def toggle boolean ( self , request ) : try : item id = int ( request . POST . get ( 'item id' , None ) ) attr = str ( request . POST . get ( 'attr' , None ) ) except : return Http Response Bad Request ( \"Malformed request\" ) if not request . user . is staff : logging . warning ( \"Denied AJAX request by non-staff %s to toggle boolean %s for object #%s\" , request . user , attr , item id ) return Http Response Forbidden ( \"You do not have permission to access this object\" ) self . collect editable booleans ( ) if not self . ajax editable booleans . has key ( attr ) : return Http Response Bad Request ( \"not a valid attribute %s\" % attr ) try : obj = self . model . default manager . get ( pk = item id ) except self . model . Does Not Exist : return Http Response Not Found ( \"Object does not exist\" ) can change = False if hasattr ( obj , \"user can\" ) and obj . user can ( request . user , change page = True ) : can change = True else : can change = self . has change permission ( request , obj = obj ) if not can change : logging . warning ( \"Denied AJAX request by %s to toggle boolean %s for object %s\" , request . user , attr , item id ) return Http Response Forbidden ( \"You do not have permission to access this object\" ) logging . info ( \"Processing request by %s to toggle %s on %s\" , request . user , attr , obj ) try : before data = self . ajax editable booleans [ attr ] ( self , obj ) setattr ( obj , attr , not getattr ( obj , attr ) ) obj . save ( ) self . refresh changelist caches ( ) data = self . ajax editable booleans [ attr ] ( self , obj ) except Exception : #, e: logging . exception ( \"Unhandled exception while toggling %s on %s\" , attr , obj ) return Http Response Server Error ( \"Unable to toggle %s on %s\" % ( attr , obj ) ) d = [ ] for a , b in zip ( before data , data ) : if a != b : d . append ( b ) return Http Response ( json . dumps ( d ) , mimetype = \"application/json\" )", "predictions": ["handle logic for setting \" and change \" ."], "references": ["handle an ajax toggle_boolean request"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 5619, "code": "def add children ( G , parent , level , n = 2 ) : if level == 0 : return for i in range ( n ) : child = parent + str ( i ) G . add node ( child ) G . add edge ( parent , child ) add children ( G , child , level - 1 , n )", "predictions": ["recursively get the children of this node"], "references": ["add children recursively to a binary tree ."], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 5620, "code": "def make bintree ( levels ) : G = nx . Di Graph ( ) root = '0' G . add node ( root ) add children ( G , root , levels , 2 ) return G", "predictions": ["makes a new graph of this graph ."], "references": ["make a symmetrical binary tree with"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5621, "code": "def submit jobs ( view , G , jobs ) : results = { } for node in nx . topological sort ( G ) : with view . temp flags ( after = [ results [ n ] for n in G . predecessors ( node ) ] ) : results [ node ] = view . apply ( jobs [ node ] ) return results", "predictions": ["store edits using provided self ."], "references": ["submit jobs via client where g describes the time dependencies ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 5622, "code": "def validate tree ( G , results ) : for node in G : started = results [ node ] . metadata . started for parent in G . predecessors ( node ) : finished = results [ parent ] . metadata . completed assert started > finished , \"%s should have happened after %s\" % ( node , parent )", "predictions": ["validates that the given self - qualified tree is a valid tree of the given self - exit tree ."], "references": ["validate that jobs executed after their dependencies ."], "bleu": 0.06760229884571738, "rouge_l": 0.1548223350253807}
{"id": 5623, "code": "def copy ( self , name = None ) : if name is None : name = self . name return Color Scheme ( name , self . colors . dict ( ) )", "predictions": ["copies the full name of this color to the specified color ."], "references": ["return a full copy of the object optionally renaming it ."], "bleu": 0.13065113298388567, "rouge_l": 0.3505747126436781}
{"id": 5624, "code": "def add scheme ( self , new scheme ) : if not isinstance ( new scheme , Color Scheme ) : raise Value Error , 'Color Scheme Table only accepts Color Scheme instances' self [ new scheme . name ] = new scheme", "predictions": ["subscribe to a profile and internally"], "references": ["add a new color scheme to the table ."], "bleu": 0.14827340167306757, "rouge_l": 0.12869198312236285}
{"id": 5625, "code": "def home lib ( home ) : if hasattr ( sys , 'pypy version info' ) : lib = 'site-packages' else : lib = os . path . join ( 'lib' , 'python' ) return os . path . join ( home , lib )", "predictions": ["return use this to get the mocked log file ."], "references": ["return the lib dir under the home installation scheme"], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 5626, "code": "def handle stdin request ( self , timeout = 0.1 ) : msg rep = self . km . stdin channel . get msg ( timeout = timeout ) self . handle iopub ( ) if self . session id == msg rep [ \"parent header\" ] . get ( \"session\" ) : real handler = signal . getsignal ( signal . SIGINT ) def double int ( sig , frame ) : real handler ( sig , frame ) raise Keyboard Interrupt signal . signal ( signal . SIGINT , double int ) try : raw data = raw input ( msg rep [ \"content\" ] [ \"prompt\" ] ) except EOF Error : raw data = '\\x04' except Keyboard Interrupt : sys . stdout . write ( '\\n' ) return finally : signal . signal ( signal . SIGINT , real handler ) if not ( self . km . stdin channel . msg ready ( ) or self . km . shell channel . msg ready ( ) ) : self . km . stdin channel . input ( raw data )", "predictions": ["remote remote as the client , or remote as as it will first remote endpoint . if the as the client is now then the as long as it will first remote messages will first remote endpoint will first remote method will first remote messages . < p > note"], "references": ["method to capture raw_input"], "bleu": 0.02403051755364481, "rouge_l": 0.04375896700143472}
{"id": 5627, "code": "def wait for kernel ( self , timeout = None ) : tic = time . time ( ) self . km . hb channel . unpause ( ) while True : self . run cell ( '1' , False ) if self . km . hb channel . is beating ( ) : break else : if timeout is not None and ( time . time ( ) - tic ) > timeout : return False return True", "predictions": ["waits for the next push operation to be provisioned or kw ."], "references": ["method to wait for a kernel to be ready"], "bleu": 0.14694106251955755, "rouge_l": 0.2932692307692307}
{"id": 5628, "code": "def interact ( self , display banner = None ) : if self . exit now : return if display banner is None : display banner = self . display banner if isinstance ( display banner , basestring ) : self . show banner ( display banner ) elif display banner : self . show banner ( ) more = False if not self . wait for kernel ( 3 ) : error ( \"Kernel did not respond\\n\" ) return if self . has readline : self . readline startup hook ( self . pre readline ) hlen b4 cell = self . readline . get current history length ( ) else : hlen b4 cell = 0 while not self . exit now : if not self . km . is alive : action = \"restart\" if self . km . has kernel else \"wait for restart\" ans = self . ask yes no ( \"kernel died, %s ([y]/n)?\" % action , default = 'y' ) if ans : if self . km . has kernel : self . km . restart kernel ( True ) self . wait for kernel ( 3 ) else : self . exit now = True continue try : self . hooks . pre prompt hook ( ) if more : try : prompt = self . prompt manager . render ( 'in2' ) except Exception : self . showtraceback ( ) if self . autoindent : self . rl do indent = True else : try : prompt = self . separate in + self . prompt manager . render ( 'in' ) except Exception : self . showtraceback ( ) line = self . raw input ( prompt ) if self . exit now : break if self . autoindent : self . rl do indent = False except Keyboard Interrupt : #double-guard against keyboardinterrupts during kbdint handling try : self . write ( '\\n Keyboard Interrupt\\n' ) source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) more = False except Keyboard Interrupt : pass except EOF Error : if self . autoindent : self . rl do indent = False if self . has readline : self . readline startup hook ( None ) self . write ( '\\n' ) self . exit ( ) except bdb . Bdb Quit : warn ( 'The Python debugger has exited with a Bdb Quit exception.\\n' 'Because of how pdb handles the stack, it is impossible\\n' 'for I Python to properly format this particular exception.\\n' 'I Python will resume normal operation.' ) except : self . showtraceback ( ) else : self . input splitter . push ( line ) more = self . input splitter . push accepts more ( ) if ( self . Syntax TB . last syntax error and self . autoedit syntax ) : self . edit syntax error ( ) if not more : source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) self . run cell ( source raw ) self . exit now = False", "predictions": ["we will call this to edit the insert operation on the ( ."], "references": ["closely emulate the interactive python console ."], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 5629, "code": "def set style ( self , style ) : if isinstance ( style , basestring ) : style = get style by name ( style ) self . style = style self . clear caches ( )", "predictions": ["create a pkg object ."], "references": ["sets the style to the specified pygments style ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5630, "code": "def get format ( self , token ) : if token in self . formats : return self . formats [ token ] if self . style is None : result = self . get format from document ( token , self . document ) else : result = self . get format from style ( token , self . style ) self . formats [ token ] = result return result", "predictions": ["compute the dependencies of this dependencies ."], "references": ["returns a qtextcharformat for token or none ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5631, "code": "def get format from document ( self , token , document ) : code , html = self . formatter . format lines ( [ ( token , u'dummy' ) ] ) . next ( ) self . document . set Html ( html ) return Qt Gui . Q Text Cursor ( self . document ) . char Format ( )", "predictions": ["this is the default leading leading zeros ."], "references": ["returns a qtextcharformat for token by"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5632, "code": "def get format from style ( self , token , style ) : result = Qt Gui . Q Text Char Format ( ) for key , value in style . style for token ( token ) . items ( ) : if value : if key == 'color' : result . set Foreground ( self . get brush ( value ) ) elif key == 'bgcolor' : result . set Background ( self . get brush ( value ) ) elif key == 'bold' : result . set Font Weight ( Qt Gui . Q Font . Bold ) elif key == 'italic' : result . set Font Italic ( True ) elif key == 'underline' : result . set Underline Style ( Qt Gui . Q Text Char Format . Single Underline ) elif key == 'sans' : result . set Font Style Hint ( Qt Gui . Q Font . Sans Serif ) elif key == 'roman' : result . set Font Style Hint ( Qt Gui . Q Font . Times ) elif key == 'mono' : result . set Font Style Hint ( Qt Gui . Q Font . Type Writer ) return result", "predictions": ["get the format for each token ."], "references": ["returns a qtextcharformat for token by reading a pygments style ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 5633, "code": "def find command ( cmd , paths = None , pathext = None ) : if paths is None : paths = os . environ . get ( 'PATH' , '' ) . split ( os . pathsep ) if isinstance ( paths , six . string types ) : paths = [ paths ] if pathext is None : pathext = get pathext ( ) pathext = [ ext for ext in pathext . lower ( ) . split ( os . pathsep ) if len ( ext ) ] if os . path . splitext ( cmd ) [ 1 ] . lower ( ) in pathext : pathext = [ '' ] for path in paths : cmd path = os . path . join ( path , cmd ) for ext in pathext : cmd path ext = cmd path + ext if os . path . isfile ( cmd path ext ) : return cmd path ext if os . path . isfile ( cmd path ) : return cmd path raise Bad Command ( 'Cannot find command %r' % cmd )", "predictions": ["searches for the command and returns its path ."], "references": ["searches the path for the given command and returns its path"], "bleu": 0.47948633265056306, "rouge_l": 0.7858293075684382}
{"id": 5634, "code": "def normalize path ( path ) : return os . path . normcase ( os . path . realpath ( os . path . expanduser ( path ) ) )", "predictions": ["normalizes a normalized path to an ' / ' ."], "references": ["convert a path to its canonical case - normalized absolute version ."], "bleu": 0.16153071659734697, "rouge_l": 0.3577712609970674}
{"id": 5635, "code": "def check nsp ( dist , attr , value ) : assert string list ( dist , attr , value ) for nsp in value : if not dist . has contents for ( nsp ) : raise Distutils Setup Error ( \"Distribution contains no modules or packages for \" + \"namespace package %r\" % nsp ) if '.' in nsp : parent = '.' . join ( nsp . split ( '.' ) [ : - 1 ] ) if parent not in value : distutils . log . warn ( \"%r is declared as a package namespace, but %r is not:\" \" please correct this in setup.py\" , nsp , parent )", "predictions": ["verify that namespace packages are valid ."], "references": ["verify that namespace packages are valid"], "bleu": 0.8408964152537145, "rouge_l": 0.9360613810741688}
{"id": 5636, "code": "def check extras ( dist , attr , value ) : try : for k , v in value . items ( ) : list ( pkg resources . parse requirements ( v ) ) except ( Type Error , Value Error , Attribute Error ) : raise Distutils Setup Error ( \"'extras require' must be a dictionary whose values are \" \"strings or lists of strings containing valid project/version \" \"requirement specifiers.\" )", "predictions": ["verify that all required attributes have been reached ."], "references": ["verify that extras_require mapping is valid"], "bleu": 0.18575057999133596, "rouge_l": 0.27664399092970515}
{"id": 5637, "code": "def check entry points ( dist , attr , value ) : try : pkg resources . Entry Point . parse map ( value ) except Value Error , e : raise Distutils Setup Error ( e )", "predictions": ["verify that all required entry values exist in this schema ."], "references": ["verify that entry_points map is parseable"], "bleu": 0.14991106946711685, "rouge_l": 0.2484725050916497}
{"id": 5638, "code": "def transform assign system ( line ) : m = assign system re . match ( line ) if m is not None : cmd = m . group ( 'cmd' ) lhs = m . group ( 'lhs' ) new line = '%s = get ipython().getoutput(%r)' % ( lhs , cmd ) return new line return line", "predictions": ["transforms system line using , and . ."], "references": ["handle the files = !ls syntax ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5639, "code": "def transform assign magic ( line ) : m = assign magic re . match ( line ) if m is not None : cmd = m . group ( 'cmd' ) lhs = m . group ( 'lhs' ) new line = '%s = get ipython().magic(%r)' % ( lhs , cmd ) return new line return line", "predictions": ["assign magic line to ( ."], "references": ["handle the a = %who syntax ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5640, "code": "def transform ipy prompt ( line ) : if not line or line . isspace ( ) : return line m = ipy prompt re . match ( line ) if m : return line [ len ( m . group ( 0 ) ) : ] else : return line", "predictions": ["transforms the line into something more than one line ."], "references": ["handle inputs that start classic ipython prompt syntax ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 5641, "code": "def reset ( self ) : self . indent spaces = 0 self . buffer [ : ] = [ ] self . source = '' self . code = None self . is complete = False self . full dedent = False", "predictions": ["resets the state of the indent to a full buffer ."], "references": ["reset the input buffer and associated state ."], "bleu": 0.14323145079400493, "rouge_l": 0.32504440497335696}
{"id": 5642, "code": "def reset ( self ) : super ( I Python Input Splitter , self ) . reset ( ) self . buffer raw [ : ] = [ ] self . source raw = '' self . cell magic parts = [ ] self . processing cell magic = False", "predictions": ["resets the state of the calls to its java . awt . processing ."], "references": ["reset the input buffer and associated state ."], "bleu": 0.10511846841633776, "rouge_l": 0.28683385579937304}
{"id": 5643, "code": "def source raw reset ( self ) : out = self . source out r = self . source raw self . reset ( ) return out , out r", "predictions": ["reset the source state to a raw stream and returns a raw array ."], "references": ["return input and raw source and perform a full reset ."], "bleu": 0.12090340630072073, "rouge_l": 0.3270777479892761}
{"id": 5644, "code": "def handle cell magic ( self , lines ) : self . processing cell magic = True first , , body = lines . partition ( '\\n' ) magic name , , line = first . partition ( ' ' ) magic name = magic name . lstrip ( ESC MAGIC ) self . cell magic parts = [ body ] tpl = 'get ipython(). run cached cell magic(%r, %r)' tlines = tpl % ( magic name , line ) self . store ( tlines ) self . store ( lines , self . buffer raw , 'source raw' ) #self. is complete = last two blanks(lines) self . is complete = last blank ( lines ) return self . is complete", "predictions": ["handle processing and run this cell ."], "references": ["process lines when they start with %% which marks cell magics ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5645, "code": "def line mode cell append ( self , lines ) : self . store ( lines , self . buffer raw , 'source raw' ) self . cell magic parts . append ( lines ) last block = self . cell magic parts [ - 1 ] self . is complete = last blank ( last block ) and lines . isspace ( ) return self . is complete", "predictions": ["appends a line to the tokens contained in this analyzed ."], "references": ["append new content for a cell magic in line mode ."], "bleu": 0.14323145079400493, "rouge_l": 0.2727272727272727}
{"id": 5646, "code": "def transform cell ( self , cell ) : self . reset ( ) self . push ( cell ) return self . source reset ( )", "predictions": ["adds the specific cell to this color ."], "references": ["process and translate a cell of input ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 5647, "code": "def observers for notification ( self , ntype , sender ) : keys = ( ( ntype , sender ) , ( ntype , None ) , ( None , sender ) , ( None , None ) ) obs = set ( ) for k in keys : obs . update ( self . observers . get ( k , set ( ) ) ) return obs", "predictions": ["observers for all possible parents ."], "references": ["find all registered observers that should recieve notification"], "bleu": 0.17516432701748888, "rouge_l": 0.13926940639269406}
{"id": 5648, "code": "def status ( self , verbose = 0 ) : self . update status ( ) self . group report ( self . running , 'Running' ) self . group report ( self . completed , 'Completed' ) self . group report ( self . dead , 'Dead' ) self . comp report [ : ] = [ ] self . dead report [ : ] = [ ]", "predictions": ["generate a status for the given method ."], "references": ["print a status of all jobs currently being managed ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 5649, "code": "def init ( self ) : for attr in [ 'call' , 'strform' ] : assert hasattr ( self , attr ) , \"Missing attribute <%s>\" % attr self . num = None self . status = Background Job Base . stat created self . stat code = Background Job Base . stat created c self . finished = False self . result = '<Background Job has not completed>' try : make tb = get ipython ( ) . Interactive TB . text except : make tb = Auto Formatted TB ( mode = 'Context' , color scheme = 'No Color' , tb offset = 1 ) . text self . make tb = lambda : make tb ( None , None , None ) self . tb = None threading . Thread . init ( self )", "predictions": ["initializes ( ."], "references": ["common initialization for all backgroundjob objects"], "bleu": 0.16620830006469264, "rouge_l": 0.0}
{"id": 5650, "code": "def energy ( self , state = None ) : state = self . state if state is None else state route = state e = 0 if self . distance matrix : for i in range ( len ( route ) ) : e += self . distance matrix [ \"{},{}\" . format ( route [ i - 1 ] , route [ i ] ) ] else : for i in range ( len ( route ) ) : e += distance ( self . cities [ route [ i - 1 ] ] , self . cities [ route [ i ] ] ) return e", "predictions": ["reorders this utterance at the specified state ."], "references": ["calculates the length of the route ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 5651, "code": "def defaults ( self , keys = None ) : d = { } keys = self . keys if keys is None else keys for key in keys : d [ key ] = None return d", "predictions": ["allow subclasses to access to a method on a class ."], "references": ["create an empty record"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5652, "code": "def init db ( self ) : sqlite3 . register adapter ( dict , adapt dict ) sqlite3 . register converter ( 'dict' , convert dict ) sqlite3 . register adapter ( list , adapt bufs ) sqlite3 . register converter ( 'bufs' , convert bufs ) dbfile = os . path . join ( self . location , self . filename ) self . db = sqlite3 . connect ( dbfile , detect types = sqlite3 . PARSE DECLTYPES , cached statements = 64 ) first table = previous table = self . table i = 0 while not self . check table ( ) : i += 1 self . table = first table + ' %i' % i self . log . warn ( \"Table %s exists and doesn't match db format, trying %s\" % ( previous table , self . table ) ) previous table = self . table self . db . execute ( % self . table ) self . db . commit ( )", "predictions": ["creates ( . this method is called after all the table creation ."], "references": ["connect to the database and get new session number ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 5653, "code": "def render expression ( self , check ) : expressions = [ ] args = [ ] skeys = set ( check . keys ( ) ) skeys . difference update ( set ( self . keys ) ) skeys . difference update ( set ( [ 'buffers' , 'result buffers' ] ) ) if skeys : raise Key Error ( \"Illegal testing key(s): %s\" % skeys ) for name , sub check in check . iteritems ( ) : if isinstance ( sub check , dict ) : for test , value in sub check . iteritems ( ) : try : op = operators [ test ] except Key Error : raise Key Error ( \"Unsupported operator: %r\" % test ) if isinstance ( op , tuple ) : op , join = op if value is None and op in null operators : expr = \"%s %s\" % ( name , null operators [ op ] ) else : expr = \"%s %s ?\" % ( name , op ) if isinstance ( value , ( tuple , list ) ) : if op in null operators and any ( [ v is None for v in value ] ) : raise Value Error ( \"Cannot use %r test with NULL values on SQ Lite backend\" % test ) expr = '( %s )' % ( join . join ( [ expr ] * len ( value ) ) ) args . extend ( value ) else : args . append ( value ) expressions . append ( expr ) else : if sub check is None : expressions . append ( \"%s IS NULL\" % name ) else : expressions . append ( \"%s = ?\" % name ) args . append ( sub check ) expr = \" AND \" . join ( expressions ) return expr , args", "predictions": ["render a expression using its parameters ."], "references": ["turn a mongodb - style search dict into an sql query ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5654, "code": "def add record ( self , msg id , rec ) : d = self . defaults ( ) d . update ( rec ) d [ 'msg id' ] = msg id line = self . dict to list ( d ) tups = '(%s)' % ( ',' . join ( [ '?' ] * len ( line ) ) ) self . db . execute ( \"INSERT INTO %s VALUES %s\" % ( self . table , tups ) , line )", "predictions": ["records a message with a desired id ."], "references": ["add a new task record by msg_id ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 5655, "code": "def get record ( self , msg id ) : cursor = self . db . execute ( \"\"\"SELECT * FROM %s WHERE msg id==?\"\"\" % self . table , ( msg id , ) ) line = cursor . fetchone ( ) if line is None : raise Key Error ( \"No such msg: %r\" % msg id ) return self . list to dict ( line )", "predictions": ["records the currently executing record for the provided id ."], "references": ["get a specific task record by msg_id ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 5656, "code": "def update record ( self , msg id , rec ) : query = \"UPDATE %s SET \" % self . table sets = [ ] keys = sorted ( rec . keys ( ) ) values = [ ] for key in keys : sets . append ( '%s = ?' % key ) values . append ( rec [ key ] ) query += ', ' . join ( sets ) query += ' WHERE msg id == ?' values . append ( msg id ) self . db . execute ( query , values )", "predictions": ["update a record with a new set of sets ."], "references": ["update the data in an existing record ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 5657, "code": "def drop matching records ( self , check ) : expr , args = self . render expression ( check ) query = \"DELETE FROM %s WHERE %s\" % ( self . table , expr ) self . db . execute ( query , args )", "predictions": ["drop database from the database ."], "references": ["remove a record from the db ."], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 5658, "code": "def get history ( self ) : query = \"\"\"SELECT msg id FROM %s ORDER by submitted ASC\"\"\" % self . table cursor = self . db . execute ( query ) return [ tup [ 0 ] for tup in cursor . fetchall ( ) ]", "predictions": ["get a history object as a nested query string ."], "references": ["get all msg_ids ordered by time submitted ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 5659, "code": "def table ( rows ) : output = '<table>' for row in rows : output += '<tr>' for column in row : output += '<td>{s}</td>' . format ( s = column ) output += '</tr>' output += '</table>' return output", "predictions": ["return a string representation of the given rows ."], "references": ["output a simple table with several columns ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5660, "code": "def link ( url , text = '' , classes = '' , target = '' , get = \"\" , * * kwargs ) : if not ( url . startswith ( 'http' ) or url . startswith ( '/' ) ) : urlargs = { } for arg , val in kwargs . items ( ) : if arg [ : 4 ] == \"url \" : urlargs [ arg [ 4 : ] ] = val url = reverse ( url , kwargs = urlargs ) if get : url += '?' + get return html . tag ( 'a' , text or url , { 'class' : classes , 'target' : target , 'href' : url } )", "predictions": ["link to the target argument ."], "references": ["output a link tag ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 5661, "code": "def jsfile ( url ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url return '<script type=\"text/javascript\" src=\"{src}\"></script>' . format ( src = url )", "predictions": ["jsfile : this is a backwards compatibility with respect to the url ."], "references": ["output a script tag to a js file ."], "bleu": 0.1135935489027116, "rouge_l": 0.2819722650231125}
{"id": 5662, "code": "def cssfile ( url ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url return '<link href=\"{src}\" rel=\"stylesheet\">' . format ( src = url )", "predictions": ["cssfile : this is a backwards compatibility with respect to the url ."], "references": ["output a link tag to a css stylesheet ."], "bleu": 0.1135935489027116, "rouge_l": 0.2819722650231125}
{"id": 5663, "code": "def img ( url , alt = '' , classes = '' , style = '' ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url attr = { 'class' : classes , 'alt' : alt , 'style' : style , 'src' : url } return html . tag ( 'img' , '' , attr )", "predictions": ["returns the url without a leading slash ."], "references": ["image tag helper ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 5664, "code": "def sub ( value , arg ) : try : return valid numeric ( value ) - valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value - arg except Exception : return ''", "predictions": [") an argument to produce the correct type ."], "references": ["subtract the arg from the value ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 5665, "code": "def mul ( value , arg ) : try : return valid numeric ( value ) * valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value * arg except Exception : return ''", "predictions": ["similar to the ( ( ( ( ( ( paths , paths , : paths , ( paths , paths , int paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths are the same as the ( paths paths paths paths"], "references": ["multiply the arg with the value ."], "bleu": 0.026594139297659906, "rouge_l": 0.08122503328894808}
{"id": 5666, "code": "def div ( value , arg ) : try : return valid numeric ( value ) / valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value / arg except Exception : return ''", "predictions": ["provide a new argument type shell ."], "references": ["divide the arg by the value ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5667, "code": "def mod ( value , arg ) : try : return valid numeric ( value ) % valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value % arg except Exception : return ''", "predictions": ["method to check if an argument is a valid variable or not ."], "references": ["return the modulo value ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 5668, "code": "def options ( self , parser , env ) : parser . add option ( \"--processes\" , action = \"store\" , default = env . get ( 'NOSE PROCESSES' , 0 ) , dest = \"multiprocess workers\" , metavar = \"NUM\" , help = \"Spread test run among this many processes. \" \"Set a number equal to the number of processors \" \"or cores in your machine for best results. \" \"[NOSE PROCESSES]\" ) parser . add option ( \"--process-timeout\" , action = \"store\" , default = env . get ( 'NOSE PROCESS TIMEOUT' , 10 ) , dest = \"multiprocess timeout\" , metavar = \"SECONDS\" , help = \"Set timeout for return of results from each \" \"test runner process. [NOSE PROCESS TIMEOUT]\" ) parser . add option ( \"--process-restartworker\" , action = \"store true\" , default = env . get ( 'NOSE PROCESS RESTARTWORKER' , False ) , dest = \"multiprocess restartworker\" , help = \"If set, will restart each worker process once\" \" their tests are done, this helps control memory \" \"leaks from killing the system. \" \"[NOSE PROCESS RESTARTWORKER]\" )", "predictions": ["check we have a pkg for the command execution ."], "references": ["register command - line options ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 5669, "code": "def run ( self , result ) : log . debug ( \"suite %s (%s) run called, tests: %s\" , id ( self ) , self , self . tests ) if self . result Proxy : result , orig = self . result Proxy ( result , self ) , result else : result , orig = result , result try : #log.debug('set Up for %s', id(self)); self . set Up ( ) except Keyboard Interrupt : raise except : self . error context = 'setup' result . add Error ( self , self . exc info ( ) ) return try : for test in self . tests : if ( isinstance ( test , nose . case . Test ) and self . arg is not None ) : test . test . arg = self . arg else : test . arg = self . arg test . test Queue = self . test Queue test . tasks = self . tasks if result . should Stop : log . debug ( \"stopping\" ) break #log.debug('running test %s in suite %s', test, self); try : test ( orig ) except Keyboard Interrupt , e : timeout = isinstance ( e , Timed Out Exception ) if timeout : msg = 'Timeout when running test %s in suite %s' else : msg = 'Keyboard Interrupt when running test %s in suite %s' log . debug ( msg , test , self ) err = ( Timed Out Exception , Timed Out Exception ( str ( test ) ) , sys . exc info ( ) [ 2 ] ) test . config . plugins . add Error ( test , err ) orig . add Error ( test , err ) if not timeout : raise finally : self . has run = True try : #log.debug('tear Down for %s', id(self)); self . tear Down ( ) except Keyboard Interrupt : raise except : self . error context = 'teardown' result . add Error ( self , self . exc info ( ) )", "predictions": ["check the execution of the queue ."], "references": ["run tests in suite inside of suite fixtures ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5670, "code": "def add builtin ( self , key , value ) : bdict = builtin . dict orig = bdict . get ( key , Builtin Undefined ) if value is Hide Builtin : if orig is not Builtin Undefined : #same as 'key in bdict' self . orig builtins [ key ] = orig del bdict [ key ] else : self . orig builtins [ key ] = orig bdict [ key ] = value", "predictions": ["we need to transform the usage information in our usage map to a form ."], "references": ["add a builtin and save the original ."], "bleu": 0.09782375748961449, "rouge_l": 0.18401206636500753}
{"id": 5671, "code": "def remove builtin ( self , key , orig ) : if orig is Builtin Undefined : del builtin . dict [ key ] else : builtin . dict [ key ] = orig", "predictions": ["transform a given key ."], "references": ["remove an added builtin and re - set the original ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5672, "code": "def activate ( self ) : add builtin = self . add builtin for name , func in self . auto builtins . iteritems ( ) : add builtin ( name , func )", "predictions": ["adds a new flat method to the class ."], "references": ["store ipython references in the __builtin__ namespace ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5673, "code": "def explicit rel links ( self , rels = ( 'homepage' , 'download' ) ) : rels = set ( rels ) for anchor in self . parsed . findall ( \".//a\" ) : if anchor . get ( \"rel\" ) and anchor . get ( \"href\" ) : found rels = set ( anchor . get ( \"rel\" ) . split ( ) ) if found rels & rels : href = anchor . get ( \"href\" ) url = self . clean link ( urllib parse . urljoin ( self . base url , href ) ) yield Link ( url , self , trusted = False )", "predictions": ["creates a link to all links ."], "references": ["yields all links with the given relations"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 5674, "code": "def unshell list ( s ) : if not s : return None if sys . platform == 'win32' : s = s . strip ( \"'\" ) return s . split ( ',' )", "predictions": ["reset the string on which this command is available ."], "references": ["turn a command - line argument into a list ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 5675, "code": "def add action ( self , dash , dashdash , action code ) : option = self . add option ( dash , dashdash , action = 'callback' , callback = self . append action ) option . action code = action code", "predictions": ["source will be added to a specific raw raw r ."], "references": ["add a specialized option that is the action to execute ."], "bleu": 0.1354599427337814, "rouge_l": 0.18181818181818182}
{"id": 5676, "code": "def append action ( self , option , opt unused , value unused , parser ) : parser . values . actions . append ( option . action code )", "predictions": ["appends the given cell to the set of body body ."], "references": ["callback for an option that adds to the actions list ."], "bleu": 0.16108992769687397, "rouge_l": 0.2727272727272727}
{"id": 5677, "code": "def help ( self , error = None , topic = None , parser = None ) : assert error or topic or parser if error : print ( error ) print ( \"Use 'coverage help' for help.\" ) elif parser : print ( parser . format help ( ) . strip ( ) ) else : help msg = HELP TOPICS . get ( topic , '' ) . strip ( ) if help msg : print ( help msg % self . covpkg . dict ) else : print ( \"Don't know topic %r\" % topic )", "predictions": ["prints the contents of two blocksnapshot objects ."], "references": ["display an error message or the named topic ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5678, "code": "def do execute ( self , options , args ) : old path0 = sys . path [ 0 ] self . coverage . start ( ) code ran = True try : try : if options . module : sys . path [ 0 ] = '' self . run python module ( args [ 0 ] , args ) else : filename = args [ 0 ] sys . path [ 0 ] = os . path . abspath ( os . path . dirname ( filename ) ) self . run python file ( filename , args ) except No Source : code ran = False raise finally : self . coverage . stop ( ) if code ran : self . coverage . save ( ) sys . path [ 0 ] = old path0", "predictions": ["executes this method . the dummy program will be called once the dummy method is installed ."], "references": ["implementation of coverage run ."], "bleu": 0.07223943354597204, "rouge_l": 0.10082644628099173}
{"id": 5679, "code": "def do debug ( self , args ) : if not args : self . help fn ( \"What information would you like: data, sys?\" ) return ERR for info in args : if info == 'sys' : print ( \"-- sys ----------------------------------------\" ) for line in info formatter ( self . coverage . sysinfo ( ) ) : print ( \" %s\" % line ) elif info == 'data' : print ( \"-- data ---------------------------------------\" ) self . coverage . load ( ) print ( \"path: %s\" % self . coverage . data . filename ) print ( \"has arcs: %r\" % self . coverage . data . has arcs ( ) ) summary = self . coverage . data . summary ( fullpath = True ) if summary : filenames = sorted ( summary . keys ( ) ) print ( \"\\n%d files:\" % len ( filenames ) ) for f in filenames : print ( \"%s: %d lines\" % ( f , summary [ f ] ) ) else : print ( \"No data collected\" ) else : self . help fn ( \"Don't know what you mean by %r\" % info ) return ERR return OK", "predictions": ["get the just like ( notification ."], "references": ["implementation of coverage debug ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5680, "code": "def unserialize object ( bufs ) : bufs = list ( bufs ) sobj = pickle . loads ( bufs . pop ( 0 ) ) if isinstance ( sobj , ( list , tuple ) ) : for s in sobj : if s . data is None : s . data = bufs . pop ( 0 ) return uncan Sequence ( map ( unserialize , sobj ) ) , bufs elif isinstance ( sobj , dict ) : newobj = { } for k in sorted ( sobj . iterkeys ( ) ) : s = sobj [ k ] if s . data is None : s . data = bufs . pop ( 0 ) newobj [ k ] = uncan ( unserialize ( s ) ) return newobj , bufs else : if sobj . data is None : sobj . data = bufs . pop ( 0 ) return uncan ( unserialize ( sobj ) ) , bufs", "predictions": ["performs a status of the given arguments ."], "references": ["reconstruct an object serialized by serialize_object from data buffers ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 5681, "code": "def set ( self ) : if sys . displayhook is not self . hook : self . old hook = sys . displayhook sys . displayhook = self . hook", "predictions": ["sets the jcombobox flag ."], "references": ["set the hook ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 5682, "code": "def is url ( url ) : if '://' not in url : return False proto , addr = url . split ( '://' , 1 ) if proto . lower ( ) not in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] : return False return True", "predictions": ["checks if the ( or a css , is a simple , which is a simple , which is a simple , so we can check for the ( e . g . if the ( https , , , , , , , , , , , , ,"], "references": ["boolean check for whether a string is a zmq url"], "bleu": 0.041622077335088555, "rouge_l": 0.11366459627329194}
{"id": 5683, "code": "def validate url ( url ) : if not isinstance ( url , basestring ) : raise Type Error ( \"url must be a string, not %r\" % type ( url ) ) url = url . lower ( ) proto addr = url . split ( '://' ) assert len ( proto addr ) == 2 , 'Invalid url: %r' % url proto , addr = proto addr assert proto in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] , \"Invalid protocol: %r\" % proto pat = re . compile ( r'^([\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?\\.)*[\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?$' ) if proto == 'tcp' : lis = addr . split ( ':' ) assert len ( lis ) == 2 , 'Invalid url: %r' % url addr , s port = lis try : port = int ( s port ) except Value Error : raise Assertion Error ( \"Invalid port %r in url: %r\" % ( port , url ) ) assert addr == '*' or pat . match ( addr ) is not None , 'Invalid url: %r' % url else : pass return True", "predictions": ["validates an email address ."], "references": ["validate a url for zeromq"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5684, "code": "def validate url container ( container ) : if isinstance ( container , basestring ) : url = container return validate url ( url ) elif isinstance ( container , dict ) : container = container . itervalues ( ) for element in container : validate url container ( element )", "predictions": ["init method for a given ( ( ( ( possibly non - null ) ."], "references": ["validate a potentially nested collection of urls ."], "bleu": 0.09103526405546068, "rouge_l": 0.18401206636500753}
{"id": 5685, "code": "def pull ( keys ) : user ns = globals ( ) if isinstance ( keys , ( list , tuple , set ) ) : for key in keys : if not user ns . has key ( key ) : raise Name Error ( \"name '%s' is not defined\" % key ) return map ( user ns . get , keys ) else : if not user ns . has key ( keys ) : raise Name Error ( \"name '%s' is not defined\" % keys ) return user ns . get ( keys )", "predictions": ["% % ( and return ("], "references": ["helper method for implementing client . pull via client . apply"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 5686, "code": "def select random ports ( n ) : ports = [ ] for i in xrange ( n ) : sock = socket . socket ( ) sock . bind ( ( '' , 0 ) ) while sock . getsockname ( ) [ 1 ] in random ports : sock . close ( ) sock = socket . socket ( ) sock . bind ( ( '' , 0 ) ) ports . append ( sock ) for i , sock in enumerate ( ports ) : port = sock . getsockname ( ) [ 1 ] sock . close ( ) ports [ i ] = port random ports . add ( port ) return ports", "predictions": ["add or remove record and join them ."], "references": ["selects and return n random ports that are available ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 5687, "code": "def get readline tail ( self , n = 10 ) : end = self . shell . readline . get current history length ( ) + 1 start = max ( end - n , 1 ) ghi = self . shell . readline . get history item return [ ghi ( x ) for x in range ( start , end ) ]", "predictions": ["get a list of record representing this execute ."], "references": ["get the last n items in readline history ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5688, "code": "def init logstart ( self ) : if self . logappend : self . magic ( 'logstart %s append' % self . logappend ) elif self . logfile : self . magic ( 'logstart %s' % self . logfile ) elif self . logstart : self . magic ( 'logstart' )", "predictions": ["initialize the style class ."], "references": ["initialize logging in case it was requested at the command line ."], "bleu": 0.08006212224540951, "rouge_l": 0.3285457809694794}
{"id": 5689, "code": "def restore sys module state ( self ) : try : for k , v in self . orig sys module state . iteritems ( ) : setattr ( sys , k , v ) except Attribute Error : pass if self . orig sys modules main mod is not None : sys . modules [ self . orig sys modules main name ] = self . orig sys modules main mod", "predictions": ["save a serializable object to a specific records ."], "references": ["restore the state of the sys module ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 5690, "code": "def register post execute ( self , func ) : if not callable ( func ) : raise Value Error ( 'argument %s must be callable' % func ) self . post execute [ func ] = True", "predictions": ["get job that is supported by the driver ."], "references": ["register a function for calling after code execution ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 5691, "code": "def new main mod ( self , ns = None ) : main mod = self . user main module init fakemod dict ( main mod , ns ) return main mod", "predictions": ["entry point for ansible module ."], "references": ["return a new main module object for user code execution ."], "bleu": 0.1141650334026257, "rouge_l": 0.2234432234432234}
{"id": 5692, "code": "def ofind property ( self , oname , info ) : if info . found : path = oname . split ( '.' ) root = '.' . join ( path [ : - 1 ] ) if info . parent is not None : try : target = getattr ( info . parent , ' class ' ) try : target = getattr ( target , path [ - 1 ] ) if isinstance ( target , property ) : oname = root + '. class .' + path [ - 1 ] info = Struct ( self . ofind ( oname ) ) except Attribute Error : pass except Attribute Error : pass return info", "predictions": ["create a link to the link ."], "references": ["second part of object finding to look for property details ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 5693, "code": "def object find ( self , oname , namespaces = None ) : inf = Struct ( self . ofind ( oname , namespaces ) ) return Struct ( self . ofind property ( oname , inf ) )", "predictions": ["this method is called to ( ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["find an object and return a struct with info about it ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 5694, "code": "def init history ( self ) : self . history manager = History Manager ( shell = self , config = self . config ) self . configurables . append ( self . history manager )", "predictions": ["initialize the reporter object ."], "references": ["sets up the command history and starts regular autosaves ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5695, "code": "def set completer frame ( self , frame = None ) : if frame : self . Completer . namespace = frame . f locals self . Completer . global namespace = frame . f globals else : self . Completer . namespace = self . user ns self . Completer . global namespace = self . user global ns", "predictions": ["sets this url to the latest ."], "references": ["set the frame of the completer ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5696, "code": "def ex ( self , cmd ) : with self . builtin trap : exec cmd in self . user global ns , self . user ns", "predictions": ["reports the user for admin admin ."], "references": ["execute a normal python statement in user namespace ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5697, "code": "def run cached cell magic ( self , magic name , line ) : cell = self . current cell magic body self . current cell magic body = None return self . run cell magic ( magic name , line , cell )", "predictions": ["this method is called via reflection from the database . this class is automatically added to the game ."], "references": ["special method to call a cell magic with the data stored in self ."], "bleu": 0.08097785064266201, "rouge_l": 0.24923391215526047}
{"id": 5698, "code": "def broadcast ( client , sender , msg name , dest name = None , block = None ) : dest name = msg name if dest name is None else dest name client [ sender ] . execute ( 'com.publish(%s)' % msg name , block = None ) targets = client . ids targets . remove ( sender ) return client [ targets ] . execute ( '%s=com.consume()' % dest name , block = None )", "predictions": ["pings an incoming message ."], "references": ["broadcast a message from one engine to all others ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5699, "code": "def send ( client , sender , targets , msg name , dest name = None , block = None ) : dest name = msg name if dest name is None else dest name def send ( targets , m name ) : msg = globals ( ) [ m name ] return com . send ( targets , msg ) client [ sender ] . apply async ( send , targets , msg name ) return client [ targets ] . execute ( '%s=com.recv()' % dest name , block = None )", "predictions": ["send a message to the remote node ."], "references": ["send a message from one to one - or - more engines ."], "bleu": 0.17680564514930266, "rouge_l": 0.45658682634730546}
{"id": 5700, "code": "def list profiles in ( path ) : files = os . listdir ( path ) profiles = [ ] for f in files : full path = os . path . join ( path , f ) if os . path . isdir ( full path ) and f . startswith ( 'profile ' ) : profiles . append ( f . split ( ' ' , 1 ) [ - 1 ] ) return profiles", "predictions": ["list all the profiles"], "references": ["list profiles in a given root directory"], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 5701, "code": "def list bundled profiles ( ) : path = os . path . join ( get ipython package dir ( ) , u'config' , u'profile' ) files = os . listdir ( path ) profiles = [ ] for profile in files : full path = os . path . join ( path , profile ) if os . path . isdir ( full path ) and profile != \" pycache \" : profiles . append ( profile ) return profiles", "predictions": ["list all bundled profiles"], "references": ["list profiles that are bundled with ipython ."], "bleu": 0.1571901051328651, "rouge_l": 0.31443298969072164}
{"id": 5702, "code": "def next ( self ) : result = self . readline ( ) if result == self . empty buffer : raise Stop Iteration return result", "predictions": ["returns the next item in the list ."], "references": ["this is to support iterators over a file - like object ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 5703, "code": "def prepare regex pattern ( self , p ) : if isinstance ( p . pattern , unicode ) : p = re . compile ( p . pattern . encode ( 'utf-8' ) , p . flags & ~ re . UNICODE ) return p", "predictions": ["prepare a regex pattern for . ."], "references": ["recompile unicode regexes as bytes regexes . overridden in subclass ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 5704, "code": "def prepare regex pattern ( self , p ) : if isinstance ( p . pattern , bytes ) : p = re . compile ( p . pattern . decode ( self . encoding ) , p . flags ) return p", "predictions": ["prepare prepare for data by parsing it before and adds it to the pattern ."], "references": ["recompile bytes regexes as unicode regexes ."], "bleu": 0.08225964699966554, "rouge_l": 0.09728867623604465}
{"id": 5705, "code": "def finish displayhook ( self ) : sys . stdout . flush ( ) sys . stderr . flush ( ) self . session . send ( self . pub socket , self . msg , ident = self . topic ) self . msg = None", "predictions": ["flushes the localname of this instance ."], "references": ["finish up all displayhook activities ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 5706, "code": "def log listener ( log : logging . Logger = None , level = logging . INFO ) : if log is None : log = logging . get Logger ( \"Progress Monitor\" ) def listen ( monitor ) : name = \"{}: \" . format ( monitor . name ) if monitor . name is not None else \"\" perc = int ( monitor . progress * 100 ) msg = \"[{name}{perc:3d}%] {monitor.message}\" . format ( * * locals ( ) ) log . log ( level , msg ) return listen", "predictions": ["logs logging into the log ."], "references": ["progress monitor listener that logs all updates to the given logger"], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 5707, "code": "def last error ( self ) : if not len ( self . log ) : raise Runtime Error ( 'Nothing executed' ) try : errs = [ l for l in self . log if l [ 1 ] != 0 ] return errs [ - 1 ] [ 2 ] except Index Error : #TODO return 'no last error'", "predictions": ["a method that returns the last error in a list ."], "references": ["get the output of the last command exevuted ."], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 5708, "code": "def check output ( self , cmd ) : ret , output = self . exec ( cmd ) if not ret == 0 : raise Command Error ( self ) return output", "predictions": ["check to see if this command has been requested to be retained ."], "references": ["wrapper for subprocess . check_output ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 5709, "code": "def arcs executed ( self ) : executed = self . coverage . data . executed arcs ( self . filename ) m2fl = self . parser . first line executed = [ ( m2fl ( l1 ) , m2fl ( l2 ) ) for ( l1 , l2 ) in executed ] return sorted ( executed )", "predictions": ["implementation of ( interface ."], "references": ["returns a sorted list of the arcs actually executed in the code ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 5710, "code": "def arcs missing ( self ) : possible = self . arc possibilities ( ) executed = self . arcs executed ( ) missing = [ p for p in possible if p not in executed and p [ 0 ] not in self . no branch ] return sorted ( missing )", "predictions": ["convenience method that returns a sorted list of possible layouts ."], "references": ["returns a sorted list of the arcs in the code not executed ."], "bleu": 0.36153103074039705, "rouge_l": 0.4925975773889637}
{"id": 5711, "code": "def arcs unpredicted ( self ) : possible = self . arc possibilities ( ) executed = self . arcs executed ( ) unpredicted = [ e for e in executed if e not in possible and e [ 0 ] != e [ 1 ] ] return sorted ( unpredicted )", "predictions": ["get the sorted list of ( objects ."], "references": ["returns a sorted list of the executed arcs missing from the code ."], "bleu": 0.17680564514930266, "rouge_l": 0.3652694610778443}
{"id": 5712, "code": "def branch lines ( self ) : exit counts = self . parser . exit counts ( ) return [ l1 for l1 , count in iitems ( exit counts ) if count > 1 ]", "predictions": ["this method can be called to get each time a space of the branch ."], "references": ["returns a list of line numbers that have more than one exit ."], "bleu": 0.09782375748961449, "rouge_l": 0.21708185053380782}
{"id": 5713, "code": "def total branches ( self ) : exit counts = self . parser . exit counts ( ) return sum ( [ count for count in exit counts . values ( ) if count > 1 ] )", "predictions": ["a total number of exit ."], "references": ["how many total branches are there?"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5714, "code": "def set precision ( cls , precision ) : assert 0 <= precision < 10 cls . precision = precision cls . near0 = 1.0 / 10 ** precision cls . near100 = 100.0 - cls . near0", "predictions": ["used to set the value of a precision ."], "references": ["set the number of decimal places used to report percentages ."], "bleu": 0.20344044854715337, "rouge_l": 0.3929146537842191}
{"id": 5715, "code": "def get pc covered ( self ) : if self . n statements > 0 : pc cov = ( 100.0 * ( self . n executed + self . n executed branches ) / ( self . n statements + self . n branches ) ) else : pc cov = 100.0 return pc cov", "predictions": ["this method is called to get derived from the specific neurons ."], "references": ["returns a single percentage value for coverage ."], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 5716, "code": "def highlight text ( needles , haystack , cls name = 'highlighted' , words = False , case = False ) : if not needles : return haystack if not haystack : return '' if words : pattern = r\"(%s)\" % \"|\" . join ( [ '\\\\b{}\\\\b' . format ( re . escape ( n ) ) for n in needles ] ) else : pattern = r\"(%s)\" % \"|\" . join ( [ re . escape ( n ) for n in needles ] ) if case : regex = re . compile ( pattern ) else : regex = re . compile ( pattern , re . I ) i , out = 0 , \"\" for m in regex . finditer ( haystack ) : out += \"\" . join ( [ haystack [ i : m . start ( ) ] , '<span class=\"%s\">' % cls name , haystack [ m . start ( ) : m . end ( ) ] , \"</span>\" ] ) i = m . end ( ) return mark safe ( out + haystack [ i : ] )", "predictions": ["highlight the text of a resumes ."], "references": ["applies cls_name to all needles found in haystack ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5717, "code": "def highlight ( string , keywords , cls name = 'highlighted' ) : if not keywords : return string if not string : return '' include , exclude = get text tokenizer ( keywords ) highlighted = highlight text ( include , string , cls name ) return highlighted", "predictions": ["given a specific string and a specific string ."], "references": ["given an list of words this function highlights the matched text in the given string ."], "bleu": 0.09170230785170237, "rouge_l": 0.22846441947565538}
{"id": 5718, "code": "def highlight words ( string , keywords , cls name = 'highlighted' ) : if not keywords : return string if not string : return '' include , exclude = get text tokenizer ( keywords ) highlighted = highlight text ( include , string , cls name , words = True ) return highlighted", "predictions": ["extract all words from the given string ."], "references": ["given an list of words this function highlights the matched words in the given string ."], "bleu": 0.1718504581966079, "rouge_l": 0.3930412371134021}
{"id": 5719, "code": "def run setup ( setup script , args ) : old dir = os . getcwd ( ) save argv = sys . argv [ : ] save path = sys . path [ : ] setup dir = os . path . abspath ( os . path . dirname ( setup script ) ) temp dir = os . path . join ( setup dir , 'temp' ) if not os . path . isdir ( temp dir ) : os . makedirs ( temp dir ) save tmp = tempfile . tempdir save modules = sys . modules . copy ( ) pr state = pkg resources . getstate ( ) try : tempfile . tempdir = temp dir os . chdir ( setup dir ) try : sys . argv [ : ] = [ setup script ] + list ( args ) sys . path . insert ( 0 , setup dir ) Directory Sandbox ( setup dir ) . run ( lambda : execfile ( \"setup.py\" , { ' file ' : setup script , ' name ' : ' main ' } ) ) except System Exit , v : if v . args and v . args [ 0 ] : raise finally : pkg resources . setstate ( pr state ) sys . modules . update ( save modules ) del modules = [ mod name for mod name in sys . modules if mod name not in save modules and not mod name . startswith ( 'encodings.' ) ] map ( sys . modules . delitem , del modules ) os . chdir ( old dir ) sys . path [ : ] = save path sys . argv [ : ] = save argv tempfile . tempdir = save tmp", "predictions": ["run all the setup scripts ."], "references": ["run a distutils setup script sandboxed in its directory"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 5720, "code": "def run ( self , func ) : try : self . copy ( self ) if file : builtin . file = self . file builtin . open = self . open self . active = True return func ( ) finally : self . active = False if file : builtin . file = file builtin . open = open self . copy ( os )", "predictions": ["this is done by the abstract method ."], "references": ["run func under os sandboxing"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5721, "code": "def unquote ends ( istr ) : if not istr : return istr if ( istr [ 0 ] == \"'\" and istr [ - 1 ] == \"'\" ) or ( istr [ 0 ] == '\"' and istr [ - 1 ] == '\"' ) : return istr [ 1 : - 1 ] else : return istr", "predictions": ["sorts the specified character . if a string is inside it ' s nor meaning it is filled ."], "references": ["remove a single pair of quotes from the endpoints of a string ."], "bleu": 0.09629943614188137, "rouge_l": 0.2587486744432662}
{"id": 5722, "code": "def find optimal ( rlist , separator size = 2 , displaywidth = 80 ) : for nrow in range ( 1 , len ( rlist ) + 1 ) : chk = map ( max , chunks ( rlist , nrow ) ) sumlength = sum ( chk ) ncols = len ( chk ) if sumlength + separator size * ( ncols - 1 ) <= displaywidth : break return { 'columns numbers' : ncols , 'optimal separator width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows numbers' : nrow , 'columns width' : chk }", "predictions": ["find the optimal product of the given list of curves ."], "references": ["calculate optimal info to columnize a list of string"], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 5723, "code": "def get or default ( mylist , i , default = None ) : if i >= len ( mylist ) : return default else : return mylist [ i ]", "predictions": ["return a or default number ."], "references": ["return list item number or default if don t exist"], "bleu": 0.16959011078459055, "rouge_l": 0.3588235294117647}
{"id": 5724, "code": "def build kernel argv ( self , argv = None ) : if argv is None : argv = sys . argv [ 1 : ] self . kernel argv = swallow argv ( argv , self . frontend aliases , self . frontend flags ) self . kernel argv . append ( \"--Kernel App.parent appname='%s'\" % self . name )", "predictions": ["create and initialize a kernel ."], "references": ["build argv to be passed to kernel subprocess"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5725, "code": "def init ssh ( self ) : if not self . sshserver and not self . sshkey : return if self . sshkey and not self . sshserver : self . sshserver = self . ip self . ip = LOCALHOST info = dict ( ip = self . ip , shell port = self . shell port , iopub port = self . iopub port , stdin port = self . stdin port , hb port = self . hb port ) self . log . info ( \"Forwarding connections to %s via %s\" % ( self . ip , self . sshserver ) ) self . ip = LOCALHOST try : newports = tunnel to kernel ( info , self . sshserver , self . sshkey ) except : self . log . error ( \"Could not setup tunnels\" , exc info = True ) self . exit ( 1 ) self . shell port , self . iopub port , self . stdin port , self . hb port = newports cf = self . connection file base , ext = os . path . splitext ( cf ) base = os . path . basename ( base ) self . connection file = os . path . basename ( base ) + '-ssh' + ext self . log . critical ( \"To connect another client via this tunnel, use:\" ) self . log . critical ( \"--existing %s\" % self . connection file )", "predictions": ["creates ( object for ( ."], "references": ["set up ssh tunnels if needed ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5726, "code": "def pretty ( obj , verbose = False , max width = 79 , newline = '\\n' ) : stream = String IO ( ) printer = Representation Printer ( stream , verbose , max width , newline ) printer . pretty ( obj ) printer . flush ( ) return stream . getvalue ( )", "predictions": ["print out the object as a formatted stream ."], "references": ["pretty print the object s representation ."], "bleu": 0.21105340631872635, "rouge_l": 0.5115303983228512}
{"id": 5727, "code": "def pprint ( obj , verbose = False , max width = 79 , newline = '\\n' ) : printer = Representation Printer ( sys . stdout , verbose , max width , newline ) printer . pretty ( obj ) printer . flush ( ) sys . stdout . write ( newline ) sys . stdout . flush ( )", "predictions": ["prints a list of objects ."], "references": ["like pretty but print to stdout ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5728, "code": "def super pprint ( obj , p , cycle ) : p . begin group ( 8 , '<super: ' ) p . pretty ( obj . self class ) p . text ( ',' ) p . breakable ( ) p . pretty ( obj . self ) p . end group ( 8 , '>' )", "predictions": ["the ( ( or optionally overwrites with the default with the fact with the default value with the fact with a ( with a ( with a ( with a ( with a ( with a ( with a ( with a ( with a ( with a ( with"], "references": ["the pprint for the super type ."], "bleu": 0.026594139297659906, "rouge_l": 0.08122503328894808}
{"id": 5729, "code": "def re pattern pprint ( obj , p , cycle ) : p . text ( 're.compile(' ) pattern = repr ( obj . pattern ) if pattern [ : 1 ] in 'u U' : pattern = pattern [ 1 : ] prefix = 'ur' else : prefix = 'r' pattern = prefix + pattern . replace ( '\\\\\\\\' , '\\\\' ) p . text ( pattern ) if obj . flags : p . text ( ',' ) p . breakable ( ) done one = False for flag in ( 'TEMPLATE' , 'IGNORECASE' , 'LOCALE' , 'MULTILINE' , 'DOTALL' , 'UNICODE' , 'VERBOSE' , 'DEBUG' ) : if obj . flags & getattr ( re , flag ) : if done one : p . text ( '|' ) p . text ( 're.' + flag ) done one = True p . text ( ')' )", "predictions": ["match all fields of type , given a cached cached cached cached cached values ."], "references": ["the pprint function for regular expression patterns ."], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 5730, "code": "def type pprint ( obj , p , cycle ) : if obj . module in ( ' builtin ' , 'exceptions' ) : name = obj . name else : name = obj . module + '.' + obj . name p . text ( name )", "predictions": ["the ( or optionally overwrites dest of the given object ."], "references": ["the pprint for classes and types ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 5731, "code": "def function pprint ( obj , p , cycle ) : if obj . module in ( ' builtin ' , 'exceptions' ) or not obj . module : name = obj . name else : name = obj . module + '.' + obj . name p . text ( '<function %s>' % name )", "predictions": ["the ( graphmodel order name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["base pprint for all functions and builtin functions ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 5732, "code": "def exception pprint ( obj , p , cycle ) : if obj . class . module in ( 'exceptions' , 'builtins' ) : name = obj . class . name else : name = '%s.%s' % ( obj . class . module , obj . class . name ) step = len ( name ) + 1 p . begin group ( step , name + '(' ) for idx , arg in enumerate ( getattr ( obj , 'args' , ( ) ) ) : if idx : p . text ( ',' ) p . breakable ( ) p . pretty ( arg ) p . end group ( step , ')' )", "predictions": ["the profiles that has the same attribute as the one ."], "references": ["base pprint for all exceptions ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 5733, "code": "def for type ( typ , func ) : oldfunc = type pprinters . get ( typ , None ) if func is not None : type pprinters [ typ ] = func return oldfunc", "predictions": ["retrieves all elements of an argument with the given bundled bundled bundled bundled ."], "references": ["add a pretty printer for a given type ."], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 5734, "code": "def text ( self , obj ) : width = len ( obj ) if self . buffer : text = self . buffer [ - 1 ] if not isinstance ( text , Text ) : text = Text ( ) self . buffer . append ( text ) text . add ( obj , width ) self . buffer width += width self . break outer groups ( ) else : self . output . write ( obj ) self . output width += width", "predictions": ["make a next object ."], "references": ["add literal text to the output ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 5735, "code": "def end group ( self , dedent = 0 , close = '' ) : self . indentation -= dedent group = self . group stack . pop ( ) if not group . breakables : self . group queue . remove ( group ) if close : self . text ( close )", "predictions": ["closes the prepare python regex ."], "references": ["end a group . see begin_group for more details ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5736, "code": "def flush ( self ) : for data in self . buffer : self . output width += data . output ( self . output , self . output width ) self . buffer . clear ( ) self . buffer width = 0", "predictions": ["prepare the output stream for this signature ."], "references": ["flush data that is left in the buffer ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5737, "code": "def pretty ( self , obj ) : obj id = id ( obj ) cycle = obj id in self . stack self . stack . append ( obj id ) self . begin group ( ) try : obj class = getattr ( obj , ' class ' , None ) or type ( obj ) try : printer = self . singleton pprinters [ obj id ] except ( Type Error , Key Error ) : pass else : return printer ( obj , self , cycle ) for cls in get mro ( obj class ) : if cls in self . type pprinters : return self . type pprinters [ cls ] ( obj , self , cycle ) else : printer = self . in deferred types ( cls ) if printer is not None : return printer ( obj , self , cycle ) else : if ' repr pretty ' in obj class . dict : meth = obj class . repr pretty if callable ( meth ) : return meth ( obj , self , cycle ) return default pprint ( obj , self , cycle ) finally : self . end group ( ) self . stack . pop ( )", "predictions": ["finish the object for a printer ."], "references": ["pretty print the given object ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 5738, "code": "def write row into ods ( ods , sheet no , row no , row ) : ods . content . get Sheet ( sheet no ) for j , col in enumerate ( row ) : cell = ods . content . get Cell ( j , row no + 1 ) cell . string Value ( escape apostrophe ( col ) ) if j % 2 == 1 : cell . set Cell Color ( settings . EVEN COLUMN BG COLOR ) else : cell . set Cell Color ( settings . ODD COLUMN BG COLOR )", "predictions": ["log a listener from a file ."], "references": ["write row with translations to ods file into specified sheet and row_no ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 5739, "code": "def osx clipboard get ( ) : p = subprocess . Popen ( [ 'pbpaste' , '-Prefer' , 'ascii' ] , stdout = subprocess . PIPE ) text , stderr = p . communicate ( ) text = text . replace ( '\\r' , '\\n' ) return text", "predictions": ["extract [ if ] from the given len ."], "references": ["get the clipboard s text on os x ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5740, "code": "def get build prefix ( ) : path = os . path . join ( tempfile . gettempdir ( ) , 'pip build %s' % get username ( ) . replace ( ' ' , ' ' ) ) if WINDOWS : \"\"\" on windows(tested on 7) temp dirs are isolated \"\"\" return path try : os . mkdir ( path ) write delete marker file ( path ) except OS Error : file uid = None try : file uid = get path uid ( path ) except OS Error : file uid = None if file uid != os . geteuid ( ) : msg = ( \"The temporary folder for building (%s) is either not owned by\" \" you, or is a symlink.\" % path ) print ( msg ) print ( \"pip will not work until the temporary folder is either \" \"deleted or is a real directory owned by your user account.\" ) raise exceptions . Installation Error ( msg ) return path", "predictions": ["find all output of this output ."], "references": ["returns a safe build_prefix"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5741, "code": "def prepare communication ( self ) : Rect Partitioner . prepare communication ( self ) if self . lower neighbors [ 0 ] >= 0 : self . in lower buffers = [ zeros ( 1 , float ) ] self . out lower buffers = [ zeros ( 1 , float ) ] if self . upper neighbors [ 0 ] >= 0 : self . in upper buffers = [ zeros ( 1 , float ) ] self . out upper buffers = [ zeros ( 1 , float ) ]", "predictions": ["arcs for shift , show edges for later retrieval ."], "references": ["prepare the buffers to be used for later communications"], "bleu": 0.16590387014219712, "rouge_l": 0.21254355400696867}
{"id": 5742, "code": "def prepare communication ( self ) : Rect Partitioner . prepare communication ( self ) self . in lower buffers = [ [ ] , [ ] ] self . out lower buffers = [ [ ] , [ ] ] self . in upper buffers = [ [ ] , [ ] ] self . out upper buffers = [ [ ] , [ ] ] size1 = self . subd hi ix [ 1 ] - self . subd lo ix [ 1 ] + 1 if self . lower neighbors [ 0 ] >= 0 : self . in lower buffers [ 0 ] = zeros ( size1 , float ) self . out lower buffers [ 0 ] = zeros ( size1 , float ) if self . upper neighbors [ 0 ] >= 0 : self . in upper buffers [ 0 ] = zeros ( size1 , float ) self . out upper buffers [ 0 ] = zeros ( size1 , float ) size0 = self . subd hi ix [ 0 ] - self . subd lo ix [ 0 ] + 1 if self . lower neighbors [ 1 ] >= 0 : self . in lower buffers [ 1 ] = zeros ( size0 , float ) self . out lower buffers [ 1 ] = zeros ( size0 , float ) if self . upper neighbors [ 1 ] >= 0 : self . in upper buffers [ 1 ] = zeros ( size0 , float ) self . out upper buffers [ 1 ] = zeros ( size0 , float )", "predictions": ["arcs for processing , show corners of treebank lines that are currently incremented before draw the datastream ."], "references": ["prepare the buffers to be used for later communications"], "bleu": 0.07535838128770536, "rouge_l": 0.07881136950904392}
{"id": 5743, "code": "def extract dates ( obj ) : if isinstance ( obj , dict ) : obj = dict ( obj ) for k , v in obj . iteritems ( ) : obj [ k ] = extract dates ( v ) elif isinstance ( obj , ( list , tuple ) ) : obj = [ extract dates ( o ) for o in obj ] elif isinstance ( obj , basestring ) : if ISO8601 PAT . match ( obj ) : obj = datetime . strptime ( obj , ISO8601 ) return obj", "predictions": ["arcs for all internal descriptions in the arc this utility is a specific key . this method should be used to arcs the instance of a specific \" arc \" ."], "references": ["extract iso8601 dates from unpacked json"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5744, "code": "def squash dates ( obj ) : if isinstance ( obj , dict ) : obj = dict ( obj ) for k , v in obj . iteritems ( ) : obj [ k ] = squash dates ( v ) elif isinstance ( obj , ( list , tuple ) ) : obj = [ squash dates ( o ) for o in obj ] elif isinstance ( obj , datetime ) : obj = obj . strftime ( ISO8601 ) return obj", "predictions": ["collect all fields of a given object ."], "references": ["squash datetime objects into iso8601 strings"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5745, "code": "def date default ( obj ) : if isinstance ( obj , datetime ) : return obj . strftime ( ISO8601 ) else : raise Type Error ( \"%r is not JSON serializable\" % obj )", "predictions": ["return the changes to the specified object ."], "references": ["default function for packing datetime objects in json ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5746, "code": "def check site dir ( self ) : instdir = normalize path ( self . install dir ) pth file = os . path . join ( instdir , 'easy-install.pth' ) is site dir = instdir in self . all site dirs if not is site dir and not self . multi version : is site dir = self . check pth processing ( ) else : testfile = self . pseudo tempname ( ) + '.write-test' test exists = os . path . exists ( testfile ) try : if test exists : os . unlink ( testfile ) open ( testfile , 'w' ) . close ( ) os . unlink ( testfile ) except ( OS Error , IO Error ) : self . cant write to target ( ) if not is site dir and not self . multi version : raise Distutils Error ( self . no default version msg ( ) ) if is site dir : if self . pth file is None : self . pth file = Pth Distributions ( pth file , self . all site dirs ) else : self . pth file = None PYTHONPATH = os . environ . get ( 'PYTHONPATH' , '' ) . split ( os . pathsep ) if instdir not in map ( normalize path , [ f for f in PYTHONPATH if f ] ) : self . sitepy installed = True elif self . multi version and not os . path . exists ( pth file ) : self . sitepy installed = True self . pth file = None self . install dir = instdir", "predictions": ["check if this precision is a precision precision ."], "references": ["verify that self . install_dir is . pth - capable dir if needed"], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 5747, "code": "def write script ( self , script name , contents , mode = \"t\" , * ignored ) : from setuptools . command . easy install import chmod , current umask log . info ( \"Installing %s script to %s\" , script name , self . install dir ) target = os . path . join ( self . install dir , script name ) self . outfiles . append ( target ) mask = current umask ( ) if not self . dry run : ensure directory ( target ) f = open ( target , \"w\" + mode ) f . write ( contents ) f . close ( ) chmod ( target , 0777 - mask )", "predictions": ["get a pc that is stored as a pc ."], "references": ["write an executable file to the scripts directory"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5748, "code": "def sleep here ( count , t ) : import time , sys print ( \"hi from engine %i\" % id ) sys . stdout . flush ( ) time . sleep ( t ) return count , t", "predictions": ["opens a ping sequence of tokens ."], "references": ["simple function that takes args prints a short message sleeps for a time and returns the same args"], "bleu": 0.038589346254072475, "rouge_l": 0.07411907654921021}
{"id": 5749, "code": "def convert pyx sources to c ( self ) : def pyx to c ( source ) : if source . endswith ( '.pyx' ) : source = source [ : - 4 ] + '.c' return source self . sources = map ( pyx to c , self . sources )", "predictions": ["creates a list of string from the planet style ."], "references": ["convert . pyx extensions to . c"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 5750, "code": "def main ( connection file ) : ctx = zmq . Context . instance ( ) with open ( connection file ) as f : cfg = json . loads ( f . read ( ) ) location = cfg [ 'location' ] reg url = cfg [ 'url' ] session = Session ( key = str to bytes ( cfg [ 'exec key' ] ) ) query = ctx . socket ( zmq . DEALER ) query . connect ( disambiguate url ( cfg [ 'url' ] , location ) ) session . send ( query , \"connection request\" ) idents , msg = session . recv ( query , mode = 0 ) c = msg [ 'content' ] iopub url = disambiguate url ( c [ 'iopub' ] , location ) sub = ctx . socket ( zmq . SUB ) sub . setsockopt ( zmq . SUBSCRIBE , b'' ) sub . connect ( iopub url ) while True : try : idents , msg = session . recv ( sub , mode = 0 ) except Keyboard Interrupt : return topic = idents [ 0 ] if msg [ 'msg type' ] == 'stream' : print ( \"%s: %s\" % ( topic , msg [ 'content' ] [ 'data' ] ) ) elif msg [ 'msg type' ] == 'pyerr' : c = msg [ 'content' ] print ( topic + ':' ) for line in c [ 'traceback' ] : print ( '    ' + line )", "predictions": ["retrieve a brief from the exclude ."], "references": ["watch iopub channel and print messages"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5751, "code": "def log level changed ( self , name , old , new ) : if isinstance ( new , basestring ) : new = getattr ( logging , new ) self . log level = new self . log . set Level ( new )", "predictions": ["logs how many messages have been logged ."], "references": ["adjust the log level when log_level is set ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5752, "code": "def flags changed ( self , name , old , new ) : for key , value in new . iteritems ( ) : assert len ( value ) == 2 , \"Bad flag: %r:%s\" % ( key , value ) assert isinstance ( value [ 0 ] , ( dict , Config ) ) , \"Bad flag: %r:%s\" % ( key , value ) assert isinstance ( value [ 1 ] , basestring ) , \"Bad flag: %r:%s\" % ( key , value )", "predictions": ["recursively set all callbacks to the first from each key ."], "references": ["ensure flags dict is valid"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5753, "code": "def print alias help ( self ) : if not self . aliases : return lines = [ ] classdict = { } for cls in self . classes : for c in cls . mro ( ) [ : - 3 ] : classdict [ c . name ] = c for alias , longname in self . aliases . iteritems ( ) : classname , traitname = longname . split ( '.' , 1 ) cls = classdict [ classname ] trait = cls . class traits ( config = True ) [ traitname ] help = cls . class get trait help ( trait ) . splitlines ( ) help [ 0 ] = help [ 0 ] . replace ( longname , alias ) + ' (%s)' % longname if len ( alias ) == 1 : help [ 0 ] = help [ 0 ] . replace ( '--%s=' % alias , '-%s ' % alias ) lines . extend ( help ) print os . linesep . join ( lines )", "predictions": ["get all the ( in implemented : can be found in java . lang . . : . . . : . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["print the alias part of the help ."], "bleu": 0.026594139297659906, "rouge_l": 0.07932379713914176}
{"id": 5754, "code": "def print flag help ( self ) : if not self . flags : return lines = [ ] for m , ( cfg , help ) in self . flags . iteritems ( ) : prefix = '--' if len ( m ) > 1 else '-' lines . append ( prefix + m ) lines . append ( indent ( dedent ( help . strip ( ) ) ) ) print os . linesep . join ( lines )", "predictions": ["prints all the : [ , , , , , , , , , , , , , , , , , , , , , , , , ] , as well as possible ."], "references": ["print the flag part of the help ."], "bleu": 0.03709091243806319, "rouge_l": 0.1026936026936027}
{"id": 5755, "code": "def print subcommands ( self ) : if not self . subcommands : return lines = [ \"Subcommands\" ] lines . append ( '-' * len ( lines [ 0 ] ) ) lines . append ( '' ) for p in wrap paragraphs ( self . subcommand description ) : lines . append ( p ) lines . append ( '' ) for subc , ( cls , help ) in self . subcommands . iteritems ( ) : lines . append ( subc ) if help : lines . append ( indent ( dedent ( help . strip ( ) ) ) ) lines . append ( '' ) print os . linesep . join ( lines )", "predictions": ["prints the ordereddict in a given section ."], "references": ["print the subcommand part of the help ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 5756, "code": "def update config ( self , config ) : newconfig = deepcopy ( self . config ) newconfig . merge ( config ) self . config = newconfig", "predictions": ["updates ( with default values ."], "references": ["fire the traits events when the config is updated ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5757, "code": "def initialize subcommand ( self , subc , argv = None ) : subapp , help = self . subcommands . get ( subc ) if isinstance ( subapp , basestring ) : subapp = import item ( subapp ) self . class . clear instance ( ) self . subapp = subapp . instance ( ) self . subapp . initialize ( argv )", "predictions": ["creates and initializes the subapp mechanism ."], "references": ["initialize a subcommand with argv ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 5758, "code": "def parse command line ( self , argv = None ) : argv = sys . argv [ 1 : ] if argv is None else argv if argv and argv [ 0 ] == 'help' : argv = argv [ 1 : ] + [ '-h' ] if self . subcommands and len ( argv ) > 0 : subc , subargv = argv [ 0 ] , argv [ 1 : ] if re . match ( r'^\\w(\\-?\\w)*$' , subc ) and subc in self . subcommands : return self . initialize subcommand ( subc , subargv ) if '-h' in argv or '--help' in argv or '--help-all' in argv : self . print description ( ) self . print help ( '--help-all' in argv ) self . print examples ( ) self . exit ( 0 ) if '--version' in argv or '-V' in argv : self . print version ( ) self . exit ( 0 ) flags , aliases = self . flatten flags ( ) loader = KV Arg Parse Config Loader ( argv = argv , aliases = aliases , flags = flags ) config = loader . load config ( ) self . update config ( config ) self . extra args = loader . extra args", "predictions": ["for testing we need to call this method to pretty print the appropriate error message for the underlying ( 79 79 79 79 ! 79 , exit 79 and . 79 79 79 79 ."], "references": ["parse the command line arguments ."], "bleu": 0.03816712639899379, "rouge_l": 0.11182401466544455}
{"id": 5759, "code": "def load config file ( self , filename , path = None ) : loader = Py File Config Loader ( filename , path = path ) try : config = loader . load config ( ) except Config File Not Found : raise except Exception : filename = loader . full filename or filename self . log . error ( \"Exception while loading config file %s\" , filename , exc info = True ) else : self . log . debug ( \"Loaded config file: %s\" , loader . full filename ) self . update config ( config )", "predictions": ["loads configuration from a obj ."], "references": ["load a . py based config file by filename and path ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 5760, "code": "def generate config file ( self ) : lines = [ % self . name ] lines . append ( '' ) lines . append ( 'c = get config()' ) lines . append ( '' ) for cls in self . classes : lines . append ( cls . class config section ( ) ) return '\\n' . join ( lines )", "predictions": ["generate a config file for each section ."], "references": ["generate default config file from configurables"], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 5761, "code": "def downsample ( array , k ) : length = array . shape [ 0 ] indices = random . sample ( xrange ( length ) , k ) return array [ indices ]", "predictions": ["returns a new array with the same content as this array but with the specified sub - values ."], "references": ["choose k random elements of array ."], "bleu": 0.0712695567709093, "rouge_l": 0.16781292984869325}
{"id": 5762, "code": "def write ( self , msg ) : if self . should ( 'pid' ) : msg = \"pid %5d: %s\" % ( os . getpid ( ) , msg ) self . output . write ( msg + \"\\n\" ) self . output . flush ( )", "predictions": ["write \"pid message to the stream ."], "references": ["write a line of debug output ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5763, "code": "def class config section ( cls ) : def c ( s ) : \"\"\"return a commented, wrapped block.\"\"\" s = '\\n\\n' . join ( wrap paragraphs ( s , 78 ) ) return + s . replace ( '\\n' , ) breaker = '#' + '-' * 78 s = % cls . name lines = [ breaker , s , breaker , '' ] desc = cls . class traits ( ) . get ( 'description' ) if desc : desc = desc . default value else : desc = getattr ( cls , ' doc ' , '' ) if desc : lines . append ( c ( desc ) ) lines . append ( '' ) parents = [ ] for parent in cls . mro ( ) : if parent is not cls and issubclass ( parent , Configurable ) and parent . class traits ( config = True ) : parents . append ( parent ) if parents : pstr = ', ' . join ( [ p . name for p in parents ] ) lines . append ( c ( '%s will inherit config from: %s' % ( cls . name , pstr ) ) ) lines . append ( '' ) for name , trait in cls . class traits ( config = True ) . iteritems ( ) : help = trait . get metadata ( 'help' ) or '' lines . append ( c ( help ) ) lines . append ( % ( cls . name , name , trait . get default value ( ) ) ) lines . append ( '' ) return '\\n' . join ( lines )", "predictions": ["help function for ( ( ) ."], "references": ["get the config class config section"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5764, "code": "def clear instance ( cls ) : if not cls . initialized ( ) : return for subclass in cls . walk mro ( ) : if isinstance ( subclass . instance , cls ) : subclass . instance = None", "predictions": ["clear all instances of the class ."], "references": ["unset _instance for this class and singleton parents ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5765, "code": "def format Failure ( self , test , err ) : ec , ev , tb = err tbinfo = inspect traceback ( tb ) test . tbinfo = tbinfo return ( ec , '\\n' . join ( [ str ( ev ) , tbinfo ] ) , tb )", "predictions": ["formats a new call to the console ."], "references": ["add detail from traceback inspection to error message of a failure ."], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 5766, "code": "def crash handler lite ( etype , evalue , tb ) : traceback . print exception ( etype , evalue , tb ) from I Python . core . interactiveshell import Interactive Shell if Interactive Shell . initialized ( ) : config = \"%config \" else : config = \"c.\" print >> sys . stderr , lite message template . format ( email = author email , config = config )", "predictions": ["prints the crash at the top of a crash ."], "references": ["a light excepthook adding a small message to the usual traceback"], "bleu": 0.12623203108004888, "rouge_l": 0.09442724458204334}
{"id": 5767, "code": "def make report ( self , traceback ) : sec sep = self . section sep report = [ '*' * 75 + '\\n\\n' + 'I Python post-mortem report\\n\\n' ] rpt add = report . append rpt add ( sys info ( ) ) try : config = pformat ( self . app . config ) rpt add ( sec sep ) rpt add ( 'Application name: %s\\n\\n' % self . app name ) rpt add ( 'Current user configuration structure:\\n\\n' ) rpt add ( config ) except : pass rpt add ( sec sep + 'Crash traceback:\\n\\n' + traceback ) return '' . join ( report )", "predictions": ["make a report for this deployment ."], "references": ["return a string containing a crash report ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 5768, "code": "def call handlers ( self , msg ) : self . message received . emit ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] signal = getattr ( self , msg type , None ) if signal : signal . emit ( msg ) if not self . handlers called : self . first reply . emit ( ) self . handlers called = True", "predictions": ["call this method with your arguments . once the method calls the method invocation , it will catch the method invocation ."], "references": ["reimplemented to emit signals instead of making callbacks ."], "bleu": 0.05538696232597745, "rouge_l": 0.06979405034324943}
{"id": 5769, "code": "def call handlers ( self , msg ) : self . message received . emit ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] signal = getattr ( self , msg type + ' received' , None ) if signal : signal . emit ( msg ) elif msg type in ( 'stdout' , 'stderr' ) : self . stream received . emit ( msg )", "predictions": ["call this method with your arguments until the class is decremented ."], "references": ["reimplemented to emit signals instead of making callbacks ."], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 5770, "code": "def flush ( self ) : super ( Qt Sub Socket Channel , self ) . flush ( ) Qt Core . Q Core Application . instance ( ) . process Events ( )", "predictions": ["flushes the underlying sockets ."], "references": ["reimplemented to ensure that signals are dispatched immediately ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5771, "code": "def call handlers ( self , msg ) : self . message received . emit ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] if msg type == 'input request' : self . input requested . emit ( msg )", "predictions": ["call this method with the arguments passed to the arguments ."], "references": ["reimplemented to emit signals instead of making callbacks ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5772, "code": "def start kernel ( self , * args , * * kw ) : if self . shell channel is not None : self . shell channel . reset first reply ( ) super ( Qt Kernel Manager , self ) . start kernel ( * args , * * kw ) self . started kernel . emit ( )", "predictions": ["begins the inner operation ."], "references": ["reimplemented for proper heartbeat management ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5773, "code": "def start channels ( self , * args , * * kw ) : super ( Qt Kernel Manager , self ) . start channels ( * args , * * kw ) self . started channels . emit ( )", "predictions": ["start the underlying method ."], "references": ["reimplemented to emit signal ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 5774, "code": "def shell channel ( self ) : if self . shell channel is None : self . shell channel = super ( Qt Kernel Manager , self ) . shell channel self . shell channel . first reply . connect ( self . first reply ) return self . shell channel", "predictions": ["analysis the user for this shell ."], "references": ["reimplemented for proper heartbeat management ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5775, "code": "def read ( self , fp , * * kwargs ) : nbs = fp . read ( ) if not py3compat . PY3 and not isinstance ( nbs , unicode ) : nbs = py3compat . str to unicode ( nbs ) return self . reads ( nbs , * * kwargs )", "predictions": ["read a single character from the specified reader ."], "references": ["read a notebook from a file like object"], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 5776, "code": "def write ( self , nb , fp , * * kwargs ) : nbs = self . writes ( nb , * * kwargs ) if not py3compat . PY3 and not isinstance ( nbs , unicode ) : nbs = py3compat . str to unicode ( nbs ) return fp . write ( nbs )", "predictions": ["write out the operation back to this class ."], "references": ["write a notebook to a file like object"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5777, "code": "def method magic marker ( magic kind ) : validate type ( magic kind ) def magic deco ( arg ) : call = lambda f , * a , * * k : f ( * a , * * k ) if callable ( arg ) : func = arg name = func . func name retval = decorator ( call , func ) record magic ( magics , magic kind , name , name ) elif isinstance ( arg , basestring ) : name = arg def mark ( func , * a , * * kw ) : record magic ( magics , magic kind , name , func . func name ) return decorator ( call , func ) retval = mark else : raise Type Error ( \"Decorator can only be called with \" \"string or function\" ) return retval magic deco . doc = docstring template . format ( 'method' , magic kind ) return magic deco", "predictions": ["mark the method as a method or a method ."], "references": ["decorator factory for methods in magics subclasses ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5778, "code": "def function magic marker ( magic kind ) : validate type ( magic kind ) def magic deco ( arg ) : call = lambda f , * a , * * k : f ( * a , * * k ) caller = sys . getframe ( 1 ) for ns in [ 'f locals' , 'f globals' , 'f builtins' ] : get ipython = getattr ( caller , ns ) . get ( 'get ipython' ) if get ipython is not None : break else : raise Name Error ( 'Decorator can only run in context where ' '`get ipython` exists' ) ip = get ipython ( ) if callable ( arg ) : func = arg name = func . func name ip . register magic function ( func , magic kind , name ) retval = decorator ( call , func ) elif isinstance ( arg , basestring ) : name = arg def mark ( func , * a , * * kw ) : ip . register magic function ( func , magic kind , name ) return decorator ( call , func ) retval = mark else : raise Type Error ( \"Decorator can only be called with \" \"string or function\" ) return retval ds = docstring template . format ( 'function' , magic kind ) ds += dedent ( ) magic deco . doc = ds return magic deco", "predictions": ["decorator to register a function with a magic ."], "references": ["decorator factory for standalone functions ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 5779, "code": "def format latex ( self , strng ) : escape re = re . compile ( r'(%| |\\$|#|&)' , re . MULTILINE ) cmd name re = re . compile ( r'^(%s.*?):' % ESC MAGIC , re . MULTILINE ) cmd re = re . compile ( r'(?P<cmd>%s.+?\\b)(?!\\}\\}:)' % ESC MAGIC , re . MULTILINE ) par re = re . compile ( r'\\\\$' , re . MULTILINE ) newline re = re . compile ( r'\\\\n' ) #strng = cmd name re.sub(r'\\n\\\\texttt{\\\\textsl{\\\\large \\1}}:',strng) strng = cmd name re . sub ( r'\\n\\\\bigskip\\n\\\\texttt{\\\\textbf{ \\1}}:' , strng ) strng = cmd re . sub ( r'\\\\texttt{\\g<cmd>}' , strng ) strng = par re . sub ( r'\\\\\\\\' , strng ) strng = escape re . sub ( r'\\\\\\1' , strng ) strng = newline re . sub ( r'\\\\textbackslash{}n' , strng ) return strng", "predictions": ["formats a latex with its trailing space ."], "references": ["format a string for latex inclusion ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 5780, "code": "def default option ( self , fn , optstr ) : if fn not in self . lsmagic ( ) : error ( \"%s is not a magic function\" % fn ) self . options table [ fn ] = optstr", "predictions": ["create a default option for this object ."], "references": ["make an entry in the options_table for fn with value optstr"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 5781, "code": "def page guiref ( arg s = None ) : from I Python . core import page page . page ( gui reference , auto html = True )", "predictions": ["instantiates a new page page ."], "references": ["show a basic reference about the gui console ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 5782, "code": "def task with callable ( the callable , label = None , schedule = DEFAULT SCHEDULE , userdata = None , pk override = None ) : task = Task ( ) if isinstance ( the callable , str ) : if pk override is not None : components = the callable . split ( '.' ) info = dict ( func type = 'instancemethod' , module name = '.' . join ( components [ : - 2 ] ) , class name = components [ - 2 ] , class path = '.' . join ( components [ : - 1 ] ) , model pk = pk override , func name = components [ - 1 ] , func path = the callable , ) task . funcinfo = info else : task . funcinfo = get func info ( func from string ( the callable ) ) else : task . funcinfo = get func info ( the callable ) if label is None : task . label = task . funcinfo [ 'func path' ] else : task . label = label task . schedule = schedule if not croniter . is valid ( task . schedule ) : raise Value Error ( f\"Cron schedule {task.schedule} is not valid\" ) if userdata is None : task . userdata = dict ( ) else : if isinstance ( userdata , dict ) : task . userdata = userdata else : raise Value Error ( \"Userdata must be a dictionary of JSON-serializable data\" ) return task", "predictions": ["task hook to schedule a task ."], "references": ["factory function to create a properly initialized task ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 5783, "code": "def func from info ( self ) : info = self . funcinfo functype = info [ 'func type' ] if functype in [ 'instancemethod' , 'classmethod' , 'staticmethod' ] : the modelclass = get module member by dottedpath ( info [ 'class path' ] ) if functype == 'instancemethod' : the modelobject = the modelclass . objects . get ( pk = info [ 'model pk' ] ) the callable = get member ( the modelobject , info [ 'func name' ] ) else : the callable = get member ( the modelclass , info [ 'func name' ] ) return the callable elif functype == 'function' : mod = import module ( info [ 'module name' ] ) the callable = get member ( mod , info [ 'func name' ] ) return the callable else : raise Value Error ( f\"Unknown functype '{functype} in task {self.pk} ({self.label})\" )", "predictions": ["convert a limited method name to a callable object ."], "references": ["find and return a callable object from a task info dictionary"], "bleu": 0.22447582175704436, "rouge_l": 0.28328173374613}
{"id": 5784, "code": "def calc next run ( self ) : base time = self . last run if self . last run == HAS NOT RUN : if self . wait for schedule is False : self . next run = timezone . now ( ) self . wait for schedule = False self . save ( ) return else : base time = timezone . now ( ) self . next run = croniter ( self . schedule , base time ) . get next ( datetime ) self . save ( )", "predictions": ["lists the next task ."], "references": ["calculate next run time of this task"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 5785, "code": "def run ( self , message ) : the callable = self . func from info ( ) try : task message = dict ( task = self , channel message = message , ) the callable ( task message ) finally : if self . end running < self . next run : self . enabled = False Channel ( KILL TASK CHANNEL ) . send ( { 'id' : self . pk } ) return if self . iterations == 0 : return else : self . iterations -= 1 if self . iterations == 0 : self . enabled = False Channel ( KILL TASK CHANNEL ) . send ( { 'id' : self . pk } ) self . save ( )", "predictions": ["run the task . this will be called multiple times ."], "references": ["internal instance method run by worker process to actually run the task callable ."], "bleu": 0.17066020143571542, "rouge_l": 0.31322207958921694}
{"id": 5786, "code": "def run asap ( self ) : now = timezone . now ( ) self . last run = now self . calc next run ( ) self . save ( ) self . submit ( now )", "predictions": ["the main loop . this method is called when the timezone is full ."], "references": ["instance method to run this task immediately ."], "bleu": 0.10511846841633776, "rouge_l": 0.19122257053291536}
{"id": 5787, "code": "def run iterations ( cls , the callable , iterations = 1 , label = None , schedule = '* * * * * *' , userdata = None , run immediately = False , delay until = None ) : task = task with callable ( the callable , label = label , schedule = schedule , userdata = userdata ) task . iterations = iterations if delay until is not None : if isinstance ( delay until , datetime ) : if delay until > timezone . now ( ) : task . start running = delay until else : raise Value Error ( \"Task cannot start running in the past\" ) else : raise Value Error ( \"delay until must be a datetime.datetime instance\" ) if run immediately : task . next run = timezone . now ( ) else : task . calc next run ( ) task . save ( )", "predictions": ["performs the actual work ."], "references": ["class method to run a callable with a specified number of iterations"], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 5788, "code": "def run once ( cls , the callable , userdata = None , delay until = None ) : cls . run iterations ( the callable , userdata = userdata , run immediately = True , delay until = delay until )", "predictions": ["starts a new builder ."], "references": ["class method to run a one - shot task immediately ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 5789, "code": "def bind kernel ( self , * * kwargs ) : if self . kernel app is not None : return self . log . info ( \"Opening ports for direct connections as an I Python kernel\" ) kernel = self . kernel kwargs . setdefault ( 'config' , self . config ) kwargs . setdefault ( 'log' , self . log ) kwargs . setdefault ( 'profile dir' , self . profile dir ) kwargs . setdefault ( 'session' , self . engine . session ) app = self . kernel app = IP Kernel App ( * * kwargs ) IP Kernel App . instance = app app . init connection file ( ) app . shell port = app . bind socket ( kernel . shell streams [ 0 ] , app . shell port ) app . log . debug ( \"shell ROUTER Channel on port: %i\" , app . shell port ) app . iopub port = app . bind socket ( kernel . iopub socket , app . iopub port ) app . log . debug ( \"iopub PUB Channel on port: %i\" , app . iopub port ) kernel . stdin socket = self . engine . context . socket ( zmq . ROUTER ) app . stdin port = app . bind socket ( kernel . stdin socket , app . stdin port ) app . log . debug ( \"stdin ROUTER Channel on port: %i\" , app . stdin port ) app . init heartbeat ( ) app . log connection info ( ) app . write connection file ( )", "predictions": ["starts a new running connection with the given kernel ."], "references": ["promote engine to listening kernel accessible to frontends ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 5790, "code": "def pid exists ( pid ) : if not isinstance ( pid , int ) : raise Type Error ( 'an integer is required' ) if pid < 0 : return False try : os . kill ( pid , 0 ) except OS Error : e = sys . exc info ( ) [ 1 ] return e . errno == errno . EPERM else : return True", "predictions": ["check if the pid exists ."], "references": ["check whether pid exists in the current process table ."], "bleu": 0.17749896924055253, "rouge_l": 0.47843137254901963}
{"id": 5791, "code": "def get disk usage ( path ) : st = os . statvfs ( path ) free = ( st . f bavail * st . f frsize ) total = ( st . f blocks * st . f frsize ) used = ( st . f blocks - st . f bfree ) * st . f frsize percent = usage percent ( used , total , round = 1 ) return nt diskinfo ( total , used , free , percent )", "predictions": ["get disk usage from path"], "references": ["return disk usage associated with path ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 5792, "code": "def run ( self ) : try : from winapi import WAIT OBJECT 0 , INFINITE except Import Error : from subprocess import WAIT OBJECT 0 , INFINITE handles = [ ] if self . interrupt handle : handles . append ( self . interrupt handle ) if self . parent handle : handles . append ( self . parent handle ) arch = platform . architecture ( ) [ 0 ] c int = ctypes . c int64 if arch . startswith ( '64' ) else ctypes . c int while True : result = ctypes . windll . kernel32 . Wait For Multiple Objects ( len ( handles ) , ( c int * len ( handles ) ) ( * handles ) , False , INFINITE ) if WAIT OBJECT 0 <= result < len ( handles ) : handle = handles [ result - WAIT OBJECT 0 ] if handle == self . interrupt handle : interrupt main ( ) elif handle == self . parent handle : os . exit ( 1 ) elif result < 0 : warn ( ) return", "predictions": ["runs this thread with the given arguments ."], "references": ["run the poll loop . this method never returns ."], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 5793, "code": "def filter ns ( ns , name pattern = \"*\" , type pattern = \"all\" , ignore case = True , show all = True ) : pattern = name pattern . replace ( \"*\" , \".*\" ) . replace ( \"?\" , \".\" ) if ignore case : reg = re . compile ( pattern + \"$\" , re . I ) else : reg = re . compile ( pattern + \"$\" ) return dict ( ( key , obj ) for key , obj in ns . iteritems ( ) if reg . match ( key ) and show hidden ( key , show all ) and is type ( obj , type pattern ) )", "predictions": ["downsample the given text of this handler ."], "references": ["filter a namespace dictionary by name pattern and item type ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 5794, "code": "def draw if interactive ( ) : fig = Gcf . get active ( ) . canvas . figure if not hasattr ( fig , 'show' ) : fig . show = lambda * a : send figure ( fig ) if not matplotlib . is interactive ( ) : return try : show . to draw . remove ( fig ) except Value Error : pass show . to draw . append ( fig ) show . draw called = True", "predictions": ["write the servers for a particular figure command ."], "references": ["is called after every pylab drawing command"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5795, "code": "def send figure ( fig ) : fmt = Inline Backend . instance ( ) . figure format data = print figure ( fig , fmt ) if data is None : return mimetypes = { 'png' : 'image/png' , 'svg' : 'image/svg+xml' } mime = mimetypes [ fmt ] sys . stdout . flush ( ) sys . stderr . flush ( ) publish display data ( 'I Python.zmq.pylab.backend inline.send figure' , { mime : data } )", "predictions": ["class used by the openweathermap to class to class in a specified config ."], "references": ["draw the given figure and send it as a png payload ."], "bleu": 0.10511846841633776, "rouge_l": 0.2340153452685422}
{"id": 5796, "code": "def handle sigint ( self , sig , frame ) : signal . signal ( signal . SIGINT , self . signal stop ) thread = threading . Thread ( target = self . confirm exit ) thread . daemon = True thread . start ( )", "predictions": ["clear the initialized daemon and exit"], "references": ["sigint handler spawns confirmation dialog"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5797, "code": "def render ( self , name , color = True , * * kwargs ) : if name == 'rewrite' : return self . render rewrite ( color = color ) if color : scheme = self . color scheme table . active colors if name == 'out' : colors = color lists [ 'normal' ] colors . number , colors . prompt , colors . normal = scheme . out number , scheme . out prompt , scheme . normal else : colors = color lists [ 'inp' ] colors . number , colors . prompt , colors . normal = scheme . in number , scheme . in prompt , scheme . in normal if name == 'in2' : colors . prompt = scheme . in prompt2 else : colors = color lists [ 'nocolor' ] colors . number , colors . prompt , colors . normal = '' , '' , '' count = self . shell . execution count fmtargs = dict ( color = colors , count = count , dots = \".\" * len ( str ( count ) ) , width = self . width , txtwidth = self . txtwidth ) fmtargs . update ( self . lazy evaluate fields ) fmtargs . update ( kwargs ) prompt = colors . prompt + self . templates [ name ] + colors . normal return self . formatter . format ( prompt , * * fmtargs )", "predictions": ["get a test templates ."], "references": ["render but don t justify or update the width or txtwidth attributes ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 5798, "code": "def mappable ( obj ) : if isinstance ( obj , ( tuple , list ) ) : return True for m in array Modules : if isinstance ( obj , m [ 'type' ] ) : return True return False", "predictions": ["helper that resolves all fields in object to their proper elements . this is intended to be used for debugging purposes ."], "references": ["return whether an object is mappable or not ."], "bleu": 0.06586656967644004, "rouge_l": 0.20938215102974828}
{"id": 5799, "code": "def get Partition ( self , seq , p , q ) : if p < 0 or p >= q : print \"No partition exists.\" return remainder = len ( seq ) % q basesize = len ( seq ) // q hi = [ ] lo = [ ] for n in range ( q ) : if n < remainder : lo . append ( n * ( basesize + 1 ) ) hi . append ( lo [ - 1 ] + basesize + 1 ) else : lo . append ( n * basesize + remainder ) hi . append ( lo [ - 1 ] + basesize ) try : result = seq [ lo [ p ] : hi [ p ] ] except Type Error : result = list ( islice ( seq , lo [ p ] , hi [ p ] ) ) return result", "predictions": ["returns all calculation in self - composed order ."], "references": ["returns the pth partition of q partitions of seq ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 5800, "code": "def main ( ) : parser = optparse . Option Parser ( usage = MAIN USAGE ) newopt = parser . add option newopt ( '--ipython' , action = 'store const' , dest = 'mode' , const = 'ipython' , help = 'I Python interactive runner (default).' ) newopt ( '--python' , action = 'store const' , dest = 'mode' , const = 'python' , help = 'Python interactive runner.' ) newopt ( '--sage' , action = 'store const' , dest = 'mode' , const = 'sage' , help = 'SAGE interactive runner.' ) opts , args = parser . parse args ( ) runners = dict ( ipython = I Python Runner , python = Python Runner , sage = SAGE Runner ) try : ext = os . path . splitext ( args [ 0 ] ) [ - 1 ] except Index Error : ext = '' modes = { '.ipy' : 'ipython' , '.py' : 'python' , '.sage' : 'sage' } mode = modes . get ( ext , \"ipython\" ) if opts . mode : mode = opts . mode runners [ mode ] ( ) . main ( args )", "predictions": ["the main entry point ."], "references": ["run as a command - line script ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5801, "code": "def main ( self , argv = None ) : parser = optparse . Option Parser ( usage = USAGE % self . class . name ) newopt = parser . add option newopt ( '-i' , '--interact' , action = 'store true' , default = False , help = 'Interact with the program after the script is run.' ) opts , args = parser . parse args ( argv ) if len ( args ) != 1 : print >> sys . stderr , \"You must supply exactly one file to run.\" sys . exit ( 1 ) self . run file ( args [ 0 ] , opts . interact )", "predictions": ["call this when the user presses the commands ."], "references": ["run as a command - line script ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 5802, "code": "def xml file ( self , cu , analysis ) : package name = rpartition ( cu . name , \".\" ) [ 0 ] class Name = cu . name package = self . packages . setdefault ( package name , [ { } , 0 , 0 , 0 , 0 ] ) xclass = self . xml out . create Element ( \"class\" ) xclass . append Child ( self . xml out . create Element ( \"methods\" ) ) xlines = self . xml out . create Element ( \"lines\" ) xclass . append Child ( xlines ) xclass . set Attribute ( \"name\" , class Name ) filename = cu . file locator . relative filename ( cu . filename ) xclass . set Attribute ( \"filename\" , filename . replace ( \"\\\\\" , \"/\" ) ) xclass . set Attribute ( \"complexity\" , \"0\" ) branch stats = analysis . branch stats ( ) for line in sorted ( analysis . statements ) : xline = self . xml out . create Element ( \"line\" ) xline . set Attribute ( \"number\" , str ( line ) ) xline . set Attribute ( \"hits\" , str ( int ( line not in analysis . missing ) ) ) if self . arcs : if line in branch stats : total , taken = branch stats [ line ] xline . set Attribute ( \"branch\" , \"true\" ) xline . set Attribute ( \"condition-coverage\" , \"%d%% (%d/%d)\" % ( 100 * taken / total , taken , total ) ) xlines . append Child ( xline ) class lines = len ( analysis . statements ) class hits = class lines - len ( analysis . missing ) if self . arcs : class branches = sum ( [ t for t , k in branch stats . values ( ) ] ) missing branches = sum ( [ t - k for t , k in branch stats . values ( ) ] ) class br hits = class branches - missing branches else : class branches = 0.0 class br hits = 0.0 xclass . set Attribute ( \"line-rate\" , rate ( class hits , class lines ) ) xclass . set Attribute ( \"branch-rate\" , rate ( class br hits , class branches ) ) package [ 0 ] [ class Name ] = xclass package [ 1 ] += class hits package [ 2 ] += class lines package [ 3 ] += class br hits package [ 4 ] += class branches", "predictions": ["the class has been copied ."], "references": ["add to the xml report for a single file ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 5803, "code": "def reduce freqs ( freqlist ) : allfreqs = np . zeros like ( freqlist [ 0 ] ) for f in freqlist : allfreqs += f return allfreqs", "predictions": ["compute a new array containing all ( of the given self - grams ."], "references": ["add up a list of freq counts to get the total counts ."], "bleu": 0.11114924776032006, "rouge_l": 0.2982885085574572}
{"id": 5804, "code": "def compute n digit freqs ( filename , n ) : d = txt file to digits ( filename ) freqs = n digit freqs ( d , n ) return freqs", "predictions": ["computes the background file for a given * * * * * * * * * * * * * * * * * * * kernel ."], "references": ["read digits of pi from a file and compute the n digit frequencies ."], "bleu": 0.05442133807846856, "rouge_l": 0.10132890365448505}
{"id": 5805, "code": "def txt file to digits ( filename , the type = str ) : with open ( filename , 'r' ) as f : for line in f . readlines ( ) : for c in line : if c != '\\n' and c != ' ' : yield the type ( c )", "predictions": ["iterates over the characters in the file ."], "references": ["yield the digits of pi read from a . txt file ."], "bleu": 0.13755608571892394, "rouge_l": 0.28955696202531644}
{"id": 5806, "code": "def one digit freqs ( digits , normalize = False ) : freqs = np . zeros ( 10 , dtype = 'i4' ) for d in digits : freqs [ int ( d ) ] += 1 if normalize : freqs = freqs / freqs . sum ( ) return freqs", "predictions": ["compute the ( of a vector of ) ) ) ."], "references": ["consume digits of pi and compute 1 digit freq . counts ."], "bleu": 0.12368857073777001, "rouge_l": 0.17256011315417258}
{"id": 5807, "code": "def two digit freqs ( digits , normalize = False ) : freqs = np . zeros ( 100 , dtype = 'i4' ) last = digits . next ( ) this = digits . next ( ) for d in digits : index = int ( last + this ) freqs [ index ] += 1 last = this this = d if normalize : freqs = freqs / freqs . sum ( ) return freqs", "predictions": ["compute the read of the input fp ."], "references": ["consume digits of pi and compute 2 digits freq . counts ."], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 5808, "code": "def plot two digit freqs ( f2 ) : f2 copy = f2 . copy ( ) f2 copy . shape = ( 10 , 10 ) ax = plt . matshow ( f2 copy ) plt . colorbar ( ) for i in range ( 10 ) : for j in range ( 10 ) : plt . text ( i - 0.2 , j + 0.2 , str ( j ) + str ( i ) ) plt . ylabel ( 'First digit' ) plt . xlabel ( 'Second digit' ) return ax", "predictions": ["write the ( to the end of all ( ."], "references": ["plot two digits frequency counts using matplotlib ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5809, "code": "def plot one digit freqs ( f1 ) : ax = plt . plot ( f1 , 'bo-' ) plt . title ( 'Single digit counts in pi' ) plt . xlabel ( 'Digit' ) plt . ylabel ( 'Count' ) return ax", "predictions": ["method that prepares the magic and returns a list of magic tuples ."], "references": ["plot one digit frequency counts using matplotlib ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 5810, "code": "def debug src ( src , pm = False , globs = None ) : testsrc = script from examples ( src ) debug script ( testsrc , pm , globs )", "predictions": ["create a new pirexception ."], "references": ["debug a single doctest docstring in argument src"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5811, "code": "def debug script ( src , pm = False , globs = None ) : import pdb srcfilename = tempfile . mktemp ( \".py\" , \"doctestdebug\" ) f = open ( srcfilename , 'w' ) f . write ( src ) f . close ( ) try : if globs : globs = globs . copy ( ) else : globs = { } if pm : try : execfile ( srcfilename , globs , globs ) except : print sys . exc info ( ) [ 1 ] pdb . post mortem ( sys . exc info ( ) [ 2 ] ) else : pdb . run ( \"execfile(%r)\" % srcfilename , globs , globs ) finally : os . remove ( srcfilename )", "predictions": ["format a latex latex so that it can be written to a pdb ."], "references": ["debug a test script . src is the script as a string ."], "bleu": 0.10511846841633776, "rouge_l": 0.2237163814180929}
{"id": 5812, "code": "def hdict ( self , hashroot ) : hfiles = self . keys ( hashroot + \"/*\" ) hfiles . sort ( ) last = len ( hfiles ) and hfiles [ - 1 ] or '' if last . endswith ( 'xx' ) : hfiles = [ last ] + hfiles [ : - 1 ] all = { } for f in hfiles : try : all . update ( self [ f ] ) except Key Error : print \"Corrupt\" , f , \"deleted - hset is not threadsafe!\" del self [ f ] self . uncache ( f ) return all", "predictions": ["each instance in the map , i . e . , the second operation is a representation of the map ."], "references": ["get all data contained in hashed category hashroot as dict"], "bleu": 0.05809665204409193, "rouge_l": 0.06892655367231638}
{"id": 5813, "code": "def keys ( self , globpat = None ) : if globpat is None : files = self . root . walkfiles ( ) else : files = [ Path ( p ) for p in glob . glob ( self . root / globpat ) ] return [ self . normalized ( p ) for p in files if p . isfile ( ) ]", "predictions": ["compute list of page page page page ."], "references": ["all keys in db or all keys matching a glob"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 5814, "code": "def allow ( self , record ) : if not self : return True return self . allow ( record ) and not self . deny ( record )", "predictions": [", operation of this lock ."], "references": ["returns whether this record should be printed"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5815, "code": "def options ( self , parser , env ) : parser . add option ( \"--nologcapture\" , action = \"store false\" , default = not env . get ( self . env opt ) , dest = \"logcapture\" , help = \"Disable logging capture plugin. \" \"Logging configurtion will be left intact.\" \" [NOSE NOLOGCAPTURE]\" ) parser . add option ( \"--logging-format\" , action = \"store\" , dest = \"logcapture format\" , default = env . get ( 'NOSE LOGFORMAT' ) or self . logformat , metavar = \"FORMAT\" , help = \"Specify custom format to print statements. \" \"Uses the same format as used by standard logging handlers.\" \" [NOSE LOGFORMAT]\" ) parser . add option ( \"--logging-datefmt\" , action = \"store\" , dest = \"logcapture datefmt\" , default = env . get ( 'NOSE LOGDATEFMT' ) or self . logdatefmt , metavar = \"FORMAT\" , help = \"Specify custom date/time format to print statements. \" \"Uses the same format as used by standard logging handlers.\" \" [NOSE LOGDATEFMT]\" ) parser . add option ( \"--logging-filter\" , action = \"store\" , dest = \"logcapture filters\" , default = env . get ( 'NOSE LOGFILTER' ) , metavar = \"FILTER\" , help = \"Specify which statements to filter in/out. \" \"By default, everything is captured. If the output is too\" \" verbose,\\nuse this option to filter out needless output.\\n\" \"Example: filter=foo will capture statements issued ONLY to\\n\" \" foo or foo.what.ever.sub but not foobar or other logger.\\n\" \"Specify multiple loggers with comma: filter=foo,bar,baz.\\n\" \"If any logger name is prefixed with a minus, eg filter=-foo,\\n\" \"it will be excluded rather than included. Default: \" \"exclude logging messages from nose itself (-nose).\" \" [NOSE LOGFILTER]\\n\" ) parser . add option ( \"--logging-clear-handlers\" , action = \"store true\" , default = False , dest = \"logcapture clear\" , help = \"Clear all other logging handlers\" ) parser . add option ( \"--logging-level\" , action = \"store\" , default = 'NOTSET' , dest = \"logcapture level\" , help = \"Set the log level to capture\" )", "predictions": ["func is the common method for handling func logic ."], "references": ["register commandline options ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 5816, "code": "def format Error ( self , test , err ) : test . captured Logging = records = self . format Log Records ( ) if not records : return err ec , ev , tb = err return ( ec , self . add Capture To Err ( ev , records ) , tb )", "predictions": ["create and return a new instance of this class ."], "references": ["add captured log messages to error output ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5817, "code": "def get new csv writers ( trans title , meta title , trans csv path , meta csv path ) : trans writer = Unicode Writer ( trans csv path ) trans writer . writerow ( trans title ) meta writer = Unicode Writer ( meta csv path ) meta writer . writerow ( meta title ) return trans writer , meta writer", "predictions": ["create and return a ( self . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["prepare new csv writers write title rows and return them ."], "bleu": 0.033984283835209204, "rouge_l": 0.11117861482381533}
{"id": 5818, "code": "def subscribe user ( self , user ) : url = self . root url + \"subscribe user\" values = { } values [ \"username\" ] = user return self . query ( url , values )", "predictions": ["run the resource at the end of the request ."], "references": ["method to subscribe a user to a service"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5819, "code": "def init parser ( ) : usage = parser = Option Parser ( usage , version = \"%prog \" + notifo . version ) parser . add option ( \"-u\" , \"--user\" , action = \"store\" , dest = \"user\" , help = \"your notifo username\" ) parser . add option ( \"-s\" , \"--secret\" , action = \"store\" , dest = \"secret\" , help = \"your notifo API secret\" ) parser . add option ( \"-n\" , \"--name\" , action = \"store\" , dest = \"name\" , help = \"recipient for the notification\" ) parser . add option ( \"-l\" , \"--label\" , action = \"store\" , dest = \"label\" , help = \"label for the notification\" ) parser . add option ( \"-t\" , \"--title\" , action = \"store\" , dest = \"title\" , help = \"title of the notification\" ) parser . add option ( \"-c\" , \"--callback\" , action = \"store\" , dest = \"callback\" , help = \"callback URL to call\" ) parser . add option ( \"-m\" , \"--message\" , action = \"store true\" , dest = \"message\" , default = False , help = \"send message instead of notification\" ) ( options , args ) = parser . parse args ( ) return ( parser , options , args )", "predictions": ["create and initialize the command line options ."], "references": ["function to init option parser"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5820, "code": "def make code from py ( filename ) : try : source file = open source ( filename ) except IO Error : raise No Source ( \"No file to run: %r\" % filename ) try : source = source file . read ( ) finally : source file . close ( ) if not source or source [ - 1 ] != '\\n' : source += '\\n' code = compile ( source , filename , \"exec\" ) return code", "predictions": ["run once from ( ."], "references": ["get source from filename and make a code object of it ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 5821, "code": "def make code from pyc ( filename ) : try : fpyc = open ( filename , \"rb\" ) except IO Error : raise No Code ( \"No file to run: %r\" % filename ) try : magic = fpyc . read ( 4 ) if magic != imp . get magic ( ) : raise No Code ( \"Bad magic number in .pyc file\" ) fpyc . read ( 4 ) if sys . version info >= ( 3 , 3 ) : fpyc . read ( 4 ) code = marshal . load ( fpyc ) finally : fpyc . close ( ) return code", "predictions": ["bind the kernel kernel code into the same file as the ( ."], "references": ["get a code object from a . pyc file ."], "bleu": 0.1135935489027116, "rouge_l": 0.2671532846715329}
{"id": 5822, "code": "def html tableify ( item matrix , select = None , header = None , footer = None ) : if not item matrix : return '' html cols = [ ] tds = lambda text : u'<td>' + text + u'  </td>' trs = lambda text : u'<tr>' + text + u'</tr>' tds items = [ map ( tds , row ) for row in item matrix ] if select : row , col = select tds items [ row ] [ col ] = u'<td class=\"inverted\">' + item matrix [ row ] [ col ] + u'  </td>' #select the right item html cols = map ( trs , ( u'' . join ( row ) for row in tds items ) ) head = '' foot = '' if header : head = ( u'<tr>' + '' . join ( ( u'<td>' + header + u'</td>' ) * len ( item matrix [ 0 ] ) ) + '</tr>' ) if footer : foot = ( u'<tr>' + '' . join ( ( u'<td>' + footer + u'</td>' ) * len ( item matrix [ 0 ] ) ) + '</tr>' ) html = ( u'<table class=\"completion\" style=\"white-space:pre\">' + head + ( u'' . join ( html cols ) ) + foot + u'</table>' ) return html", "predictions": ["all the items in the pid are found in the input array ."], "references": ["returnr a string for an html table"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 5823, "code": "def current ( self , value ) : current = min ( max ( self . min , value ) , self . max ) self . current = current if current > self . stop : self . stop = current self . start = current - self . width elif current < self . start : self . start = current self . stop = current + self . width if abs ( self . start - self . min ) <= self . sticky lenght : self . start = self . min if abs ( self . stop - self . max ) <= self . sticky lenght : self . stop = self . max", "predictions": ["a happy method to provide the health path for each object ."], "references": ["set current cursor position"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 5824, "code": "def update list ( self , hilight = True ) : self . sliding interval . current = self . index [ 0 ] head = None foot = None if self . sliding interval . start > 0 : head = '...' if self . sliding interval . stop < self . sliding interval . max : foot = '...' items m = self . justified items [ self . sliding interval . start : self . sliding interval . stop + 1 ] self . console widget . clear temporary buffer ( ) if ( hilight ) : sel = ( self . sliding interval . nth , self . index [ 1 ] ) else : sel = None strng = html tableify ( items m , select = sel , header = head , footer = foot ) self . console widget . fill temporary buffer ( self . old cursor , strng , html = True )", "predictions": ["updates the list of ( objects with this list of ( ."], "references": ["update the list of completion and hilight the currently selected completion"], "bleu": 0.19338531381761725, "rouge_l": 0.2629310344827586}
{"id": 5825, "code": "def complete current ( self ) : i = self . index item = self . items [ i [ 0 ] ] [ i [ 1 ] ] item = item . strip ( ) if item : self . current text cursor ( ) . insert Text ( item ) self . cancel completion ( )", "predictions": ["wrapper around complete that passes through the underlying method ."], "references": ["perform the completion with the currently selected item ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 5826, "code": "def wordfreq ( text , is filename = False ) : if is filename : with open ( text ) as f : text = f . read ( ) freqs = { } for word in text . split ( ) : lword = word . lower ( ) freqs [ lword ] = freqs . get ( lword , 0 ) + 1 return freqs", "predictions": ["wordfreq text . this is the same as the name but will generate a ( if it has a final word ."], "references": ["return a dictionary of words and word counts in a string ."], "bleu": 0.06964541799727335, "rouge_l": 0.18635437881873726}
{"id": 5827, "code": "def print wordfreq ( freqs , n = 10 ) : words , counts = freqs . keys ( ) , freqs . values ( ) items = zip ( counts , words ) items . sort ( reverse = True ) for ( count , word ) in items [ : n ] : print ( word , count )", "predictions": ["prints the items in the bloom filter ."], "references": ["print the n most common words and counts in the freqs dict ."], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 5828, "code": "def tostring ( self ) : root = self . as element ( ) indent ( root ) txt = ET . tostring ( root , encoding = \"utf-8\" ) txt = re . sub ( r' [A-Z] ' , '' , txt ) txt = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n' + txt return txt", "predictions": ["create a indent for this layout ."], "references": ["return the string representation of the job description xml ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5829, "code": "def write ( self , filename ) : txt = self . tostring ( ) with open ( filename , 'w' ) as f : f . write ( txt )", "predictions": ["write the given bytes to the file ."], "references": ["write the xml job description to a file ."], "bleu": 0.2451240194075422, "rouge_l": 0.5820610687022901}
{"id": 5830, "code": "def begin ( self , total : int , name = None , message = None ) : self . total = total message = message or name or \"Working...\" self . name = name or \"Progress Monitor\" self . update ( 0 , message )", "predictions": ["creates a new save object ."], "references": ["call before starting work on a monitor specifying name and amount of work"], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 5831, "code": "def task ( self , total : int , name = None , message = None ) : self . begin ( total , name , message ) try : yield self finally : self . done ( )", "predictions": ["a method to wrap the underlying method in a unit ."], "references": ["wrap code into a begin and end call on this monitor"], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 5832, "code": "def subtask ( self , units : int ) : sm = self . submonitor ( units ) try : yield sm finally : if sm . total is None : self . update ( units ) else : sm . done ( )", "predictions": ["a method to generate a subtask for each time the subtask is started by the subtask ."], "references": ["create a submonitor with the given units"], "bleu": 0.07994607499472013, "rouge_l": 0.18020679468242246}
{"id": 5833, "code": "def update ( self , units : int = 1 , message : str = None ) : if self . total is None : raise Exception ( \"Cannot call progressmonitor.update before calling begin\" ) self . worked = min ( self . total , self . worked + units ) if message : self . message = message for listener in self . listeners : listener ( self )", "predictions": ["y_j stats for testing ."], "references": ["increment the monitor with n units worked and an optional message"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 5834, "code": "def load config ( self ) : self . clear ( ) try : self . find file ( ) except IO Error as e : raise Config File Not Found ( str ( e ) ) self . read file as dict ( ) self . convert to config ( ) return self . config", "predictions": ["loads configuration from the given file ."], "references": ["load the config from a file and return it as a struct ."], "bleu": 0.09912033646614596, "rouge_l": 0.2846034214618974}
{"id": 5835, "code": "def read file as dict ( self ) : def load subconfig ( fname , profile = None ) : from I Python . core . profiledir import Profile Dir , Profile Dir Error if profile is not None : try : profile dir = Profile Dir . find profile dir by name ( get ipython dir ( ) , profile , ) except Profile Dir Error : return path = profile dir . location else : path = self . path loader = Py File Config Loader ( fname , path ) try : sub config = loader . load config ( ) except Config File Not Found : pass else : self . config . merge ( sub config ) def get config ( ) : return self . config namespace = dict ( load subconfig = load subconfig , get config = get config ) fs encoding = sys . getfilesystemencoding ( ) or 'ascii' conf filename = self . full filename . encode ( fs encoding ) py3compat . execfile ( conf filename , namespace )", "predictions": ["read the profile for the specified file ."], "references": ["load the config file into self . config with recursive loading ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 5836, "code": "def load flag ( self , cfg ) : if isinstance ( cfg , ( dict , Config ) ) : for sec , c in cfg . iteritems ( ) : self . config [ sec ] . update ( c ) else : raise Type Error ( \"Invalid flag: %r\" % cfg )", "predictions": ["load configuration from configuration"], "references": ["update self . config from a flag which can be a dict or config"], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 5837, "code": "def decode argv ( self , argv , enc = None ) : uargv = [ ] if enc is None : enc = DEFAULT ENCODING for arg in argv : if not isinstance ( arg , unicode ) : arg = arg . decode ( enc ) uargv . append ( arg ) return uargv", "predictions": ["decoding decoding with base64 formatting"], "references": ["decode argv if bytes using stin . encoding falling back on default enc"], "bleu": 0.04635036983311895, "rouge_l": 0.0}
{"id": 5838, "code": "def interrupt then kill ( self , delay = 2.0 ) : try : self . signal ( SIGINT ) except Exception : self . log . debug ( \"interrupt failed\" ) pass self . killer = ioloop . Delayed Callback ( lambda : self . signal ( SIGKILL ) , delay * 1000 , self . loop ) self . killer . start ( )", "predictions": ["gathers a request by the given delay ."], "references": ["send int wait a delay and then send kill ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 5839, "code": "def start ( self , n ) : dlist = [ ] for i in range ( n ) : if i > 0 : time . sleep ( self . delay ) el = self . launcher class ( work dir = self . work dir , config = self . config , log = self . log , profile dir = self . profile dir , cluster id = self . cluster id , ) el . engine cmd = copy . deepcopy ( self . engine cmd ) el . engine args = copy . deepcopy ( self . engine args ) el . on stop ( self . notice engine stopped ) d = el . start ( ) self . launchers [ i ] = el dlist . append ( d ) self . notify start ( dlist ) return dlist", "predictions": ["this method starts the start of all the launcher ."], "references": ["start n engines by profile or profile_dir ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 5840, "code": "def find args ( self ) : return self . mpi cmd + [ '-n' , str ( self . n ) ] + self . mpi args + self . program + self . program args", "predictions": ["a program for the program . note that the program will be returned in the parent problem ."], "references": ["build self . args using all the fields ."], "bleu": 0.08097785064266204, "rouge_l": 0.2364341085271318}
{"id": 5841, "code": "def start ( self , n ) : self . n = n return super ( MPI Launcher , self ) . start ( )", "predictions": ["start the first power of the queue ."], "references": ["start n instances of the program using mpiexec ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 5842, "code": "def start ( self , n ) : self . n = n return super ( MPI Engine Set Launcher , self ) . start ( n )", "predictions": ["begins the sender of this class ."], "references": ["start n engines by profile or profile_dir ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5843, "code": "def send file ( self , local , remote ) : remote = \"%s:%s\" % ( self . location , remote ) for i in range ( 10 ) : if not os . path . exists ( local ) : self . log . debug ( \"waiting for %s\" % local ) time . sleep ( 1 ) else : break self . log . info ( \"sending %s to %s\" , local , remote ) check output ( self . scp cmd + [ local , remote ] )", "predictions": ["send a new request to the server ."], "references": ["send a single file"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 5844, "code": "def fetch file ( self , remote , local ) : full remote = \"%s:%s\" % ( self . location , remote ) self . log . info ( \"fetching %s from %s\" , local , full remote ) for i in range ( 10 ) : check = check output ( self . ssh cmd + self . ssh args + [ self . location , 'test -e' , remote , \"&& echo 'yes' || echo 'no'\" ] ) check = check . strip ( ) if check == 'no' : time . sleep ( 1 ) elif check == 'yes' : break check output ( self . scp cmd + [ full remote , local ] )", "predictions": ["get data for this file ."], "references": ["fetch a single file"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 5845, "code": "def engine count ( self ) : count = 0 for n in self . engines . itervalues ( ) : if isinstance ( n , ( tuple , list ) ) : n , args = n count += n return count", "predictions": ["counts the training engines of this layout ."], "references": ["determine engine count from engines dict"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5846, "code": "def start ( self , n ) : self . write job file ( n ) args = [ 'submit' , '/jobfile:%s' % self . job file , '/scheduler:%s' % self . scheduler ] self . log . debug ( \"Starting Win HPC Job: %s\" % ( self . job cmd + ' ' + ' ' . join ( args ) , ) ) output = check output ( [ self . job cmd ] + args , env = os . environ , cwd = self . work dir , stderr = STDOUT ) job id = self . parse job id ( output ) self . notify start ( job id ) return job id", "predictions": ["creates a new job job to start the job ."], "references": ["start n copies of the process using the win hpc job scheduler ."], "bleu": 0.11742832364135733, "rouge_l": 0.33983286908078}
{"id": 5847, "code": "def parse job id ( self , output ) : m = self . job id regexp . search ( output ) if m is not None : job id = m . group ( ) else : raise Launcher Error ( \"Job id couldn't be determined: %s\" % output ) self . job id = job id self . log . info ( 'Job submitted with job id: %r' , job id ) return job id", "predictions": ["parses an output job by parsing it and passes it to the output ."], "references": ["take the output of the submit command and return the job id ."], "bleu": 0.13834368456410945, "rouge_l": 0.2982885085574572}
{"id": 5848, "code": "def write batch script ( self , n ) : self . n = n if self . batch template file and not self . batch template : with open ( self . batch template file ) as f : self . batch template = f . read ( ) if not self . batch template : self . batch template = self . default template if not self . job array regexp . search ( self . batch template ) : self . log . debug ( \"adding job array settings to batch script\" ) firstline , rest = self . batch template . split ( '\\n' , 1 ) self . batch template = u'\\n' . join ( [ firstline , self . job array template , rest ] ) if self . queue and not self . queue regexp . search ( self . batch template ) : self . log . debug ( \"adding PBS queue settings to batch script\" ) firstline , rest = self . batch template . split ( '\\n' , 1 ) self . batch template = u'\\n' . join ( [ firstline , self . queue template , rest ] ) script as string = self . formatter . format ( self . batch template , * * self . context ) self . log . debug ( 'Writing batch script: %s' , self . batch file ) with open ( self . batch file , 'w' ) as f : f . write ( script as string ) os . chmod ( self . batch file , stat . S IRUSR | stat . S IWUSR | stat . S IXUSR )", "predictions": ["writes the ( ( ( ) and writes it to the batch ."], "references": ["instantiate and write the batch script to the work_dir ."], "bleu": 0.17194656088289215, "rouge_l": 0.3562043795620438}
{"id": 5849, "code": "def start ( self , n ) : self . log . debug ( \"Starting %s: %r\" , self . class . name , self . args ) self . write batch script ( n ) output = check output ( self . args , env = os . environ ) job id = self . parse job id ( output ) self . notify start ( job id ) return job id", "predictions": ["starts a new job ."], "references": ["start n copies of the process using a batch system ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 5850, "code": "def context menu make ( self , pos ) : format = self . control . cursor For Position ( pos ) . char Format ( ) name = format . string Property ( Qt Gui . Q Text Format . Image Name ) if name : menu = Qt Gui . Q Menu ( ) menu . add Action ( 'Copy Image' , lambda : self . copy image ( name ) ) menu . add Action ( 'Save Image As...' , lambda : self . save image ( name ) ) menu . add Separator ( ) svg = self . name to svg map . get ( name , None ) if svg is not None : menu . add Separator ( ) menu . add Action ( 'Copy SVG' , lambda : svg to clipboard ( svg ) ) menu . add Action ( 'Save SVG As...' , lambda : save svg ( svg , self . control ) ) else : menu = super ( Rich I Python Widget , self ) . context menu make ( pos ) return menu", "predictions": ["make a context menu ."], "references": ["reimplemented to return a custom context menu for images ."], "bleu": 0.1501861529550426, "rouge_l": 0.5030927835051546}
{"id": 5851, "code": "def handle pyout ( self , msg ) : if not self . hidden and self . is from this session ( msg ) : content = msg [ 'content' ] prompt number = content . get ( 'execution count' , 0 ) data = content [ 'data' ] if data . has key ( 'image/svg+xml' ) : self . pre image append ( msg , prompt number ) self . append svg ( data [ 'image/svg+xml' ] , True ) self . append html ( self . output sep2 , True ) elif data . has key ( 'image/png' ) : self . pre image append ( msg , prompt number ) self . append png ( decodestring ( data [ 'image/png' ] . encode ( 'ascii' ) ) , True ) self . append html ( self . output sep2 , True ) elif data . has key ( 'image/jpeg' ) and self . jpg supported : self . pre image append ( msg , prompt number ) self . append jpg ( decodestring ( data [ 'image/jpeg' ] . encode ( 'ascii' ) ) , True ) self . append html ( self . output sep2 , True ) else : return super ( Rich I Python Widget , self ) . handle pyout ( msg )", "predictions": ["handle watching this sender without a notification ."], "references": ["overridden to handle rich data types like svg ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5852, "code": "def handle display data ( self , msg ) : if not self . hidden and self . is from this session ( msg ) : source = msg [ 'content' ] [ 'source' ] data = msg [ 'content' ] [ 'data' ] metadata = msg [ 'content' ] [ 'metadata' ] if data . has key ( 'image/svg+xml' ) : self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) svg = data [ 'image/svg+xml' ] self . append svg ( svg , True ) elif data . has key ( 'image/png' ) : self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) png = decodestring ( data [ 'image/png' ] . encode ( 'ascii' ) ) self . append png ( png , True ) elif data . has key ( 'image/jpeg' ) and self . jpg supported : self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) jpg = decodestring ( data [ 'image/jpeg' ] . encode ( 'ascii' ) ) self . append jpg ( jpg , True ) else : return super ( Rich I Python Widget , self ) . handle display data ( msg )", "predictions": ["displays a display message ."], "references": ["overridden to handle rich data types like svg ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5853, "code": "def append jpg ( self , jpg , before prompt = False ) : self . append custom ( self . insert jpg , jpg , before prompt )", "predictions": ["appends the specified charsequence to this writer ."], "references": ["append raw jpg data to the widget ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 5854, "code": "def append png ( self , png , before prompt = False ) : self . append custom ( self . insert png , png , before prompt )", "predictions": ["appends the text representation of this condition to the specified png ."], "references": ["append raw png data to the widget ."], "bleu": 0.15537125692760353, "rouge_l": 0.3112244897959184}
{"id": 5855, "code": "def append svg ( self , svg , before prompt = False ) : self . append custom ( self . insert svg , svg , before prompt )", "predictions": ["appends the svg svg at the end of the list ."], "references": ["append raw svg data to the widget ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 5856, "code": "def copy image ( self , name ) : image = self . get image ( name ) Qt Gui . Q Application . clipboard ( ) . set Image ( image )", "predictions": ["update an list of images to this file ."], "references": ["copies the imageresource with name to the clipboard ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5857, "code": "def get image ( self , name ) : document = self . control . document ( ) image = document . resource ( Qt Gui . Q Text Document . Image Resource , Qt Core . Q Url ( name ) ) return image", "predictions": ["this method returns a current current current current current current image to the ( ."], "references": ["returns the qimage stored as the imageresource with name ."], "bleu": 0.09782375748961449, "rouge_l": 0.2489795918367347}
{"id": 5858, "code": "def insert img ( self , cursor , img , fmt ) : try : image = Qt Gui . Q Image ( ) image . load From Data ( img , fmt . upper ( ) ) except Value Error : self . insert plain text ( cursor , 'Received invalid %s data.' % fmt ) else : format = self . add image ( image ) cursor . insert Block ( ) cursor . insert Image ( format ) cursor . insert Block ( )", "predictions": ["inserts the full pixel at the specified filename ."], "references": ["insert a raw image jpg or png"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5859, "code": "def insert svg ( self , cursor , svg ) : try : image = svg to image ( svg ) except Value Error : self . insert plain text ( cursor , 'Received invalid SVG data.' ) else : format = self . add image ( image ) self . name to svg map [ format . name ( ) ] = svg cursor . insert Block ( ) cursor . insert Image ( format ) cursor . insert Block ( )", "predictions": ["inserts the svg for the passed n - svg ."], "references": ["insert raw svg data into the widet ."], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 5860, "code": "def save image ( self , name , format = 'PNG' ) : dialog = Qt Gui . Q File Dialog ( self . control , 'Save Image' ) dialog . set Accept Mode ( Qt Gui . Q File Dialog . Accept Save ) dialog . set Default Suffix ( format . lower ( ) ) dialog . set Name Filter ( '%s file (*.%s)' % ( format , format . lower ( ) ) ) if dialog . exec ( ) : filename = dialog . selected Files ( ) [ 0 ] image = self . get image ( name ) image . save ( filename , format )", "predictions": ["tostring - saves the txt ( and insert it to the . element at the end of the tokens element ."], "references": ["shows a save dialog for the imageresource with name ."], "bleu": 0.06429451441231726, "rouge_l": 0.13785310734463277}
{"id": 5861, "code": "def exit now changed ( self , name , old , new ) : if new : loop = ioloop . IO Loop . instance ( ) loop . add timeout ( time . time ( ) + 0.1 , loop . stop )", "predictions": ["stops the timer with the specified : stop , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , ."], "references": ["stop eventloop when exit_now fires"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 5862, "code": "def init environment ( self ) : env = os . environ env [ 'TERM' ] = 'xterm-color' env [ 'CLICOLOR' ] = '1' env [ 'PAGER' ] = 'cat' env [ 'GIT PAGER' ] = 'cat' install payload page ( )", "predictions": ["initialize the style . the util method is called after the system ."], "references": ["configure the user s environment ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 5863, "code": "def ask exit ( self ) : self . exit now = True payload = dict ( source = 'I Python.zmq.zmqshell.ZMQ Interactive Shell.ask exit' , exit = True , keepkernel = self . keepkernel on exit , ) self . payload manager . write payload ( payload )", "predictions": ["this method calls the task ."], "references": ["engage the exit actions ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 5864, "code": "def read ( self , filename ) : kwargs = { } if sys . version info >= ( 3 , 2 ) : kwargs [ 'encoding' ] = \"utf-8\" return configparser . Raw Config Parser . read ( self , filename , * * kwargs )", "predictions": ["reads the configuration from the given file ."], "references": ["read a filename as utf - 8 configuration data ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 5865, "code": "def from environment ( self , env var ) : env = os . environ . get ( env var , '' ) if env : self . timid = ( '--timid' in env )", "predictions": ["create the environment from the given environment ."], "references": ["read configuration from the env_var environment variable ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 5866, "code": "def from args ( self , * * kwargs ) : for k , v in iitems ( kwargs ) : if v is not None : if k in self . MUST BE LIST and isinstance ( v , string class ) : v = [ v ] setattr ( self , k , v )", "predictions": ["convert keyword arguments from something to a resource ."], "references": ["read config values from kwargs ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 5867, "code": "def set attr from config option ( self , cp , attr , where , type = '' ) : section , option = where . split ( \":\" ) if cp . has option ( section , option ) : method = getattr ( cp , 'get' + type ) setattr ( self , attr , method ( section , option ) )", "predictions": ["create a new instance ."], "references": ["set an attribute on self if it exists in the configparser ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 5868, "code": "def delims ( self , delims ) : expr = '[' + '' . join ( '\\\\' + c for c in delims ) + ']' self . delim re = re . compile ( expr ) self . delims = delims self . delim expr = expr", "predictions": ["only adds this object to the end of the list ."], "references": ["set the delimiters for line splitting ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 5869, "code": "def split line ( self , line , cursor pos = None ) : l = line if cursor pos is None else line [ : cursor pos ] return self . delim re . split ( l ) [ - 1 ]", "predictions": ["splits a argv into multiple lines ."], "references": ["split a line of text with a cursor at the given position ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 5870, "code": "def greedy changed ( self , name , old , new ) : if new : self . splitter . delims = GREEDY DELIMS else : self . splitter . delims = DELIMS if self . readline : self . readline . set completer delims ( self . splitter . delims )", "predictions": ["description of the method"], "references": ["update the splitter and readline delims when greedy is changed"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 5871, "code": "def alias matches ( self , text ) : main text = self . text until cursor . lstrip ( ) if ' ' in main text and not main text . startswith ( 'sudo' ) : return [ ] text = os . path . expanduser ( text ) aliases = self . alias table . keys ( ) if text == '' : return aliases else : return [ a for a in aliases if a . startswith ( text ) ]", "predictions": ["this do a call to start a certain start of the top of the pattern ."], "references": ["match internal system aliases"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 5872, "code": "def python matches ( self , text ) : if \".\" in text : try : matches = self . attr matches ( text ) if text . endswith ( '.' ) and self . omit names : if self . omit names == 1 : no name = ( lambda txt : re . match ( r'.*\\. .*? ' , txt ) is None ) else : no name = ( lambda txt : re . match ( r'.*\\. .*?' , txt ) is None ) matches = filter ( no name , matches ) except Name Error : matches = [ ] else : matches = self . global matches ( text ) return matches", "predictions": ["this method searches for the given : : : / / www . com / foo / foo / hibernate / foo / foo / hibernate / 2004 / 2004 / foo / foo / foo / foo / ajax / foo / foo / hibernate / hibernate / hibernate"], "references": ["match attributes or global python names"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 5873, "code": "def match one ( self , rec , tests ) : for key , test in tests . iteritems ( ) : if not test ( rec . get ( key , None ) ) : return False return True", "predictions": ["searches for matching operation in the stream ."], "references": ["check if a specific record matches tests ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 5874, "code": "def match ( self , check ) : matches = [ ] tests = { } for k , v in check . iteritems ( ) : if isinstance ( v , dict ) : tests [ k ] = Composite Filter ( v ) else : tests [ k ] = lambda o : o == v for rec in self . records . itervalues ( ) : if self . match one ( rec , tests ) : matches . append ( copy ( rec ) ) return matches", "predictions": ["performs a hard start of the full set of tests on this builder ."], "references": ["find all the matches for a check dict ."], "bleu": 0.10511846841633776, "rouge_l": 0.18100890207715134}
{"id": 5875, "code": "def extract subdict ( self , rec , keys ) : d = { } d [ 'msg id' ] = rec [ 'msg id' ] for key in keys : d [ key ] = rec [ key ] return copy ( d )", "predictions": ["send a list of common key numbers ."], "references": ["extract subdict of keys"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 5876, "code": "def add record ( self , msg id , rec ) : if self . records . has key ( msg id ) : raise Key Error ( \"Already have msg id %r\" % ( msg id ) ) self . records [ msg id ] = rec", "predictions": ["records a message to a list of bytes ."], "references": ["add a new task record by msg_id ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5877, "code": "def get record ( self , msg id ) : if not msg id in self . records : raise Key Error ( \"No such msg id %r\" % ( msg id ) ) return copy ( self . records [ msg id ] )", "predictions": ["engine copy or other message"], "references": ["get a specific task record by msg_id ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 5878, "code": "def drop matching records ( self , check ) : matches = self . match ( check ) for m in matches : del self . records [ m [ 'msg id' ] ]", "predictions": ["start the instance of the database from the database ."], "references": ["remove a record from the db ."], "bleu": 0.17827531042796255, "rouge_l": 0.36454183266932266}
{"id": 5879, "code": "def get history ( self ) : msg ids = self . records . keys ( ) return sorted ( msg ids , key = lambda m : self . records [ m ] [ 'submitted' ] )", "predictions": ["parse this message object ."], "references": ["get all msg_ids ordered by time submitted ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5880, "code": "def quiet ( self ) : try : cell = self . shell . history manager . input hist parsed [ self . prompt count ] if cell . rstrip ( ) . endswith ( ';' ) : return True except Index Error : pass return False", "predictions": ["optimistically operation . this is a wrapper around the implementation of the = = ( ."], "references": ["should we silence the display hook because of ; ?"], "bleu": 0.08513012360883544, "rouge_l": 0.16052631578947368}
{"id": 5881, "code": "def update user ns ( self , result ) : if result is not self . shell . user ns [ ' oh' ] : if len ( self . shell . user ns [ ' oh' ] ) >= self . cache size and self . do full cache : warn ( 'Output cache limit (currently ' + `self.cache size` + ' entries) hit.\\n' 'Flushing cache and resetting history counter...\\n' 'The only history variables available will be  , ,  and  1\\n' 'with the current result.' ) self . flush ( ) if ' ' not in builtin . dict : self . = self . self . = self . self . = result self . shell . push ( { ' ' : self . , ' ' : self . , ' ' : self . } , interactive = False ) to main = { } if self . do full cache : new result = ' ' + `self.prompt count` to main [ new result ] = result self . shell . push ( to main , interactive = False ) self . shell . user ns [ ' oh' ] [ self . prompt count ] = result", "predictions": ["updates the division object for the ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["update user_ns with various things like _ __ _1 etc ."], "bleu": 0.02403051755364481, "rouge_l": 0.037059538274605106}
{"id": 5882, "code": "def log output ( self , format dict ) : if self . shell . logger . log output : self . shell . logger . log write ( format dict [ 'text/plain' ] , 'output' ) self . shell . history manager . output hist reprs [ self . prompt count ] = format dict [ 'text/plain' ]", "predictions": ["logs a user to the log ."], "references": ["log the output ."], "bleu": 0.22089591134157885, "rouge_l": 0.3824451410658307}
{"id": 5883, "code": "def finish displayhook ( self ) : io . stdout . write ( self . shell . separate out2 ) io . stdout . flush ( )", "predictions": ["flushes the internal buffer , flushing all the output streams ."], "references": ["finish up all displayhook activities ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 5884, "code": "def load ipython extension ( ip ) : global loaded if not loaded : plugin = Store Magic ( shell = ip , config = ip . config ) ip . plugin manager . register plugin ( 'storemagic' , plugin ) loaded = True", "predictions": ["called to handle display the self - in the self - defined ."], "references": ["load the extension in ipython ."], "bleu": 0.1135935489027116, "rouge_l": 0.3382624768946396}
{"id": 5885, "code": "def raise if freezed ( self ) : if self . is freezed : name = type ( self ) . name raise Invalid Operation Exception ( 'obj {name} is freezed.' . format ( name = name ) )", "predictions": ["append the operation to the underlying stream ."], "references": ["raise invalidoperationexception if is freezed ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5886, "code": "def mysql timestamp converter ( s ) : if s [ 4 ] == '-' : return Date Time or None ( s ) s = s + \"0\" * ( 14 - len ( s ) ) parts = map ( int , filter ( None , ( s [ : 4 ] , s [ 4 : 6 ] , s [ 6 : 8 ] , s [ 8 : 10 ] , s [ 10 : 12 ] , s [ 12 : 14 ] ) ) ) try : return Timestamp ( * parts ) except ( System Exit , Keyboard Interrupt ) : raise except : return None", "predictions": ["method to get a png png png date string"], "references": ["convert a mysql timestamp to a timestamp object ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5887, "code": "def eventloop changed ( self , name , old , new ) : loop = ioloop . IO Loop . instance ( ) loop . add timeout ( time . time ( ) + 0.1 , self . enter eventloop )", "predictions": ["create a new projection ."], "references": ["schedule call to eventloop from ioloop"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 5888, "code": "def start ( self ) : self . shell . exit now = False if self . control stream : self . control stream . on recv ( self . dispatch control , copy = False ) def make dispatcher ( stream ) : def dispatcher ( msg ) : return self . dispatch shell ( stream , msg ) return dispatcher for s in self . shell streams : s . on recv ( make dispatcher ( s ) , copy = False )", "predictions": ["start this exportobject . this method is called by the ui thread ."], "references": ["register dispatchers for streams"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 5889, "code": "def do one iteration ( self ) : if self . control stream : self . control stream . flush ( ) for stream in self . shell streams : stream . flush ( zmq . POLLIN , 1 ) stream . flush ( zmq . POLLOUT )", "predictions": ["get all the zmq zmq zmq ."], "references": ["step eventloop just once"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5890, "code": "def publish pyin ( self , code , parent , execution count ) : self . session . send ( self . iopub socket , u'pyin' , { u'code' : code , u'execution count' : execution count } , parent = parent , ident = self . topic ( 'pyin' ) )", "predictions": ["this method is called by the server when it wants to publish a topic ."], "references": ["publish the code request on the pyin stream ."], "bleu": 0.09782375748961449, "rouge_l": 0.17453505007153075}
{"id": 5891, "code": "def abort request ( self , stream , ident , parent ) : msg ids = parent [ 'content' ] . get ( 'msg ids' , None ) if isinstance ( msg ids , basestring ) : msg ids = [ msg ids ] if not msg ids : self . abort queues ( ) for mid in msg ids : self . aborted . add ( str ( mid ) ) content = dict ( status = 'ok' ) reply msg = self . session . send ( stream , 'abort reply' , content = content , parent = parent , ident = ident ) self . log . debug ( \"%s\" , reply msg )", "predictions": ["abort a reply at the bottom of this method ."], "references": ["abort a specifig msg by id"], "bleu": 0.16590387014219712, "rouge_l": 0.26180257510729615}
{"id": 5892, "code": "def clear request ( self , stream , idents , parent ) : self . shell . reset ( False ) msg = self . session . send ( stream , 'clear reply' , ident = idents , parent = parent , content = dict ( status = 'ok' ) )", "predictions": ["creates a token object from the specified stream ."], "references": ["clear our namespace ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 5893, "code": "def topic ( self , topic ) : if self . int id >= 0 : base = \"engine.%i\" % self . int id else : base = \"kernel.%s\" % self . ident return py3compat . cast bytes ( \"%s.%s\" % ( base , topic ) )", "predictions": ["get the topic to the topic ."], "references": ["prefixed topic for iopub messages"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5894, "code": "def at shutdown ( self ) : if self . shutdown message is not None : self . session . send ( self . iopub socket , self . shutdown message , ident = self . topic ( 'shutdown' ) ) self . log . debug ( \"%s\" , self . shutdown message ) [ s . flush ( zmq . POLLOUT ) for s in self . shell streams ]", "predictions": ["called when we receive a running zmq shutdown ."], "references": ["actions taken at shutdown by the kernel called by python s atexit ."], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 5895, "code": "def init gui pylab ( self ) : shell = self . shell showtraceback = shell . showtraceback try : def print tb ( etype , evalue , stb ) : print ( \"GUI event loop or pylab initialization failed\" , file = io . stderr ) print ( shell . Interactive TB . stb2text ( stb ) , file = io . stderr ) shell . showtraceback = print tb Interactive Shell App . init gui pylab ( self ) finally : shell . showtraceback = showtraceback", "predictions": ["for testing . this method is called when the user selects a specific file ."], "references": ["enable gui event loop integration taking pylab into account ."], "bleu": 0.08225964699966554, "rouge_l": 0.08299319727891155}
{"id": 5896, "code": "def before Context ( self ) : mods = sys . modules . copy ( ) self . mod stack . append ( mods )", "predictions": ["create a new copy of the given stack ."], "references": ["copy sys . modules onto my mod stack"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 5897, "code": "def virtual memory ( ) : total , active , inactive , wired , free = psutil osx . get virtual mem ( ) avail = inactive + free used = active + inactive + wired percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free , active , inactive , wired )", "predictions": ["system virtual memory as much as possible ."], "references": ["system virtual memory as a namedtuple ."], "bleu": 0.46713797772820004, "rouge_l": 0.6747787610619468}
{"id": 5898, "code": "def get system cpu times ( ) : user , nice , system , idle = psutil osx . get system cpu times ( ) return cputimes ntuple ( user , nice , system , idle )", "predictions": ["gets the cpu cpu times ."], "references": ["return system cpu times as a namedtuple ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 5899, "code": "def get system per cpu times ( ) : ret = [ ] for cpu t in psutil osx . get system per cpu times ( ) : user , nice , system , idle = cpu t item = cputimes ntuple ( user , nice , system , idle ) ret . append ( item ) return ret", "predictions": ["gets the value of the system per time per cpu time ."], "references": ["return system cpu times as a named tuple"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 5900, "code": "def get process cmdline ( self ) : if not pid exists ( self . pid ) : raise No Such Process ( self . pid , self . process name ) return psutil osx . get process cmdline ( self . pid )", "predictions": ["adds the process based on the pid of this process ."], "references": ["return process cmdline as a list of arguments ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 5901, "code": "def get memory info ( self ) : rss , vms = psutil osx . get process memory info ( self . pid ) [ : 2 ] return nt meminfo ( rss , vms )", "predictions": ["get information about the memory used by the memory usage ."], "references": ["return a tuple with the process rss and vms size ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 5902, "code": "def get ext memory info ( self ) : rss , vms , pfaults , pageins = psutil osx . get process memory info ( self . pid ) return self . nt ext mem ( rss , vms , pfaults * PAGESIZE , pageins * PAGESIZE )", "predictions": ["get information about this memory usage ."], "references": ["return a tuple with the process rss and vms size ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 5903, "code": "def get open files ( self ) : if self . pid == 0 : return [ ] files = [ ] rawlist = psutil osx . get process open files ( self . pid ) for path , fd in rawlist : if isfile strict ( path ) : ntuple = nt openfile ( path , fd ) files . append ( ntuple ) return files", "predictions": ["this is the method that attempts to get the open pid for the pid ."], "references": ["return files opened by process ."], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 5904, "code": "def usage percent ( used , total , round = None ) : try : ret = ( used / total ) * 100 except Zero Division Error : ret = 0 if round is not None : return round ( ret , round ) else : return ret", "predictions": ["display usage of the given period ."], "references": ["calculate percentage usage of used against total ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 5905, "code": "def memoize ( f ) : cache = { } def memf ( * x ) : if x not in cache : cache [ x ] = f ( * x ) return cache [ x ] return memf", "predictions": ["memoize the cache and return the cache ."], "references": ["a simple memoize decorator for functions ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 5906, "code": "def deprecated ( replacement = None ) : def outer ( fun ) : msg = \"psutil.%s is deprecated\" % fun . name if replacement is not None : msg += \"; use %s instead\" % replacement if fun . doc is None : fun . doc = msg @ wraps ( fun ) def inner ( * args , * * kwargs ) : warnings . warn ( msg , category = Deprecation Warning , stacklevel = 2 ) return fun ( * args , * * kwargs ) return inner return outer", "predictions": ["deprecated decorator for the function which is called after a function is executed ."], "references": ["a decorator which can be used to mark functions as deprecated ."], "bleu": 0.11633270842295028, "rouge_l": 0.2340153452685422}
{"id": 5907, "code": "def login ( self ) : try : self . gd client = gdata . docs . client . Docs Client ( ) self . gd client . Client Login ( self . email , self . password , self . source ) except Request Error as e : raise PO Docs Error ( e )", "predictions": ["login to server and log list ."], "references": ["login into google docs with user authentication info ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5908, "code": "def get gdocs key ( self ) : try : args = urlparse . parse qs ( urlparse . urlparse ( self . url ) . query ) self . key = args [ 'key' ] [ 0 ] except Key Error as e : raise PO Docs Error ( e )", "predictions": ["sets the gdocs on the current entity ."], "references": ["parse gdocs key from spreadsheet url ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 5909, "code": "def ensure temp path exists ( self ) : try : if not os . path . exists ( self . temp path ) : os . mkdir ( self . temp path ) except OS Error as e : raise PO Docs Error ( e )", "predictions": ["creates a named buffered directory . if the object already exists , it is created ."], "references": ["make sure temp directory exists and create one if it does not ."], "bleu": 0.10123734869668824, "rouge_l": 0.28110599078341014}
{"id": 5910, "code": "def download ( self ) : trans csv path = os . path . realpath ( os . path . join ( self . temp path , GDOCS TRANS CSV ) ) meta csv path = os . path . realpath ( os . path . join ( self . temp path , GDOCS META CSV ) ) self . download csv from gdocs ( trans csv path , meta csv path ) try : csv to po ( trans csv path , meta csv path , self . locale root , self . po files path , header = self . header ) except IO Error as e : raise PO Docs Error ( e ) self . clear temp ( )", "predictions": ["download all previously loaded ( ."], "references": ["download csv files from gdocs and convert them into po files structure ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 5911, "code": "def clear ( self ) : empty file path = os . path . join ( self . temp path , 'empty.csv' ) try : empty file = open ( empty file path , 'w' ) empty file . write ( ',' ) empty file . close ( ) except IO Error as e : raise PO Docs Error ( e ) self . upload file to gdoc ( empty file path , content type = 'text/csv' ) os . remove ( empty file path )", "predictions": ["clears and returns a file ."], "references": ["clear gdoc spreadsheet by sending empty csv file ."], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 5912, "code": "def new qt console ( self , evt = None ) : return connect qtconsole ( self . ipkernel . connection file , profile = self . ipkernel . profile )", "predictions": ["create a new console ."], "references": ["start a new qtconsole connected to our kernel"], "bleu": 0.1971902775417715, "rouge_l": 0.2953995157384988}
{"id": 5913, "code": "def url has contents ( url , contents , case sensitive = False , timeout = 10 ) : try : req = urllib2 . urlopen ( url , timeout = timeout ) except Exception , : False else : rep = req . read ( ) if ( not case sensitive and rep . lower ( ) . find ( contents . lower ( ) ) >= 0 ) or ( case sensitive and rep . find ( contents ) >= 0 ) : return True else : return False", "predictions": ["does a url have a timeout ."], "references": ["check whether the html page contains the content or not and return boolean"], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 5914, "code": "def get response code ( url , timeout = 10 ) : try : req = urllib2 . urlopen ( url , timeout = timeout ) except HTTP Error , e : return e . getcode ( ) except Exception , : fail ( \"Couldn't reach the URL '%s'\" % url ) else : return req . getcode ( )", "predictions": ["perform an http get request"], "references": ["visit the url and return the http response code in int"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5915, "code": "def clear output ( self , stdout = True , stderr = True , other = True ) : if stdout : print ( '\\033[2K\\r' , file = io . stdout , end = '' ) io . stdout . flush ( ) if stderr : print ( '\\033[2K\\r' , file = io . stderr , end = '' ) io . stderr . flush ( )", "predictions": ["clears the output of the output ."], "references": ["clear the output of the cell receiving output ."], "bleu": 0.44683107184405757, "rouge_l": 0.7334669338677354}
{"id": 5916, "code": "def source file ( self ) : if os . path . exists ( self . filename ) : return open source ( self . filename ) source = self . file locator . get zip data ( self . filename ) if source is not None : return String IO ( source ) raise Coverage Exception ( \"No source for code '%s'.\" % self . filename )", "predictions": ["generate a file for the source code ."], "references": ["return an open file for reading the source of the code unit ."], "bleu": 0.15451666492113134, "rouge_l": 0.5479041916167664}
{"id": 5917, "code": "def total seconds ( td ) : try : return td . total seconds ( ) except Attribute Error : return 1e-6 * ( td . microseconds + ( td . seconds + td . days * 24 * 3600 ) * 10 ** 6 )", "predictions": ["returns the total seconds of this instance ."], "references": ["timedelta . total_seconds was added in 2 . 7"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5918, "code": "def abort ( self ) : assert not self . ready ( ) , \"Can't abort, I am already done!\" return self . client . abort ( self . msg ids , targets = self . targets , block = True )", "predictions": ["abort this http resource at the specified account ."], "references": ["abort my tasks ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 5919, "code": "def elapsed ( self ) : if self . ready ( ) : return self . wall time now = submitted = datetime . now ( ) for msg id in self . msg ids : if msg id in self . client . metadata : stamp = self . client . metadata [ msg id ] [ 'submitted' ] if stamp and stamp < submitted : submitted = stamp return total seconds ( now - submitted )", "predictions": ["returns the elapsed message of this quaternion ."], "references": ["elapsed time since initial submission"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5920, "code": "def wait interactive ( self , interval = 1. , timeout = None ) : N = len ( self ) tic = time . time ( ) while not self . ready ( ) and ( timeout is None or time . time ( ) - tic <= timeout ) : self . wait ( interval ) clear output ( ) print ( \"%4i/%i tasks finished after %4i s\" % ( self . progress , N , self . elapsed ) , end = \"\" ) sys . stdout . flush ( ) print ( ) print ( \"done\" )", "predictions": ["start the shutdown with a now ."], "references": ["interactive wait printing progress at regular intervals"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5921, "code": "def republish displaypub ( self , content , eid ) : try : ip = get ipython ( ) except Name Error : return md = content [ 'metadata' ] or { } md [ 'engine' ] = eid ip . display pub . publish ( content [ 'source' ] , content [ 'data' ] , md )", "predictions": ["creates the do not filled with this ) ."], "references": ["republish individual displaypub content dicts"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5922, "code": "def wait for outputs ( self , timeout = - 1 ) : if not self . success : return tic = time . time ( ) while not all ( md [ 'outputs ready' ] for md in self . metadata ) : time . sleep ( 0.01 ) self . client . flush iopub ( self . client . iopub socket ) if timeout >= 0 and time . time ( ) > tic + timeout : break", "predictions": ["waits for the socket to be triggered ."], "references": ["wait for the status = idle message that indicates we have all outputs"], "bleu": 0.11296874775996037, "rouge_l": 0.18263473053892215}
{"id": 5923, "code": "def unordered iter ( self ) : try : rlist = self . get ( 0 ) except error . Timeout Error : pending = set ( self . msg ids ) while pending : try : self . client . wait ( pending , 1e-3 ) except error . Timeout Error : pass ready = pending . difference ( self . client . outstanding ) pending = pending . difference ( ready ) while ready : msg id = ready . pop ( ) ar = Async Result ( self . client , msg id , self . fname ) rlist = ar . get ( ) try : for r in rlist : yield r except Type Error : yield rlist else : for r in rlist : yield r", "predictions": ["abort operation that iterates all ."], "references": ["iterator for results * as they arrive * on fcfs basis ignoring submission order ."], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 5924, "code": "def wait ( self , timeout = - 1 ) : start = time . time ( ) if self . ready : return local ids = filter ( lambda msg id : msg id in self . client . outstanding , self . msg ids ) local ready = self . client . wait ( local ids , timeout ) if local ready : remote ids = filter ( lambda msg id : msg id not in self . client . results , self . msg ids ) if not remote ids : self . ready = True else : rdict = self . client . result status ( remote ids , status only = False ) pending = rdict [ 'pending' ] while pending and ( timeout < 0 or time . time ( ) < start + timeout ) : rdict = self . client . result status ( remote ids , status only = False ) pending = rdict [ 'pending' ] if pending : time . sleep ( 0.1 ) if not pending : self . ready = True if self . ready : try : results = map ( self . client . results . get , self . msg ids ) self . result = results if self . single result : r = results [ 0 ] if isinstance ( r , Exception ) : raise r else : results = error . collect exceptions ( results , self . fname ) self . result = self . reconstruct result ( results ) except Exception , e : self . exception = e self . success = False else : self . success = True finally : self . metadata = map ( self . client . metadata . get , self . msg ids )", "predictions": ["clear for a shutdown ."], "references": ["wait for result to complete ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 5925, "code": "def abs file ( filename ) : path = os . path . expandvars ( os . path . expanduser ( filename ) ) path = os . path . abspath ( os . path . realpath ( path ) ) path = actual path ( path ) return path", "predictions": ["parses the if the if the if the if the if the if the if the if it is absolute ."], "references": ["return the absolute normalized form of filename ."], "bleu": 0.0690889519686715, "rouge_l": 0.2250922509225092}
{"id": 5926, "code": "def sep ( s ) : sep match = re . search ( r\"[\\\\/]\" , s ) if sep match : the sep = sep match . group ( 0 ) else : the sep = os . sep return the sep", "predictions": ["= \" ( * self self - recursive \" self - testenvironment \" self - based self self - based self - n \" self - > \""], "references": ["find the path separator used in this string or os . sep if none ."], "bleu": 0.03639374222382004, "rouge_l": 0.0}
{"id": 5927, "code": "def match ( self , fpath ) : for d in self . dirs : if fpath . startswith ( d ) : if fpath == d : return True if fpath [ len ( d ) ] == os . sep : return True return False", "predictions": ["init for matching rule handlers ."], "references": ["does fpath indicate a file in one of our trees?"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 5928, "code": "def match ( self , fpath ) : for pat in self . pats : if fnmatch . fnmatch ( fpath , pat ) : return True return False", "predictions": ["compares two strings in the first matching location ."], "references": ["does fpath match one of our filename patterns?"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5929, "code": "def loop qt4 ( kernel ) : from I Python . external . qt for kernel import Qt Core from I Python . lib . guisupport import get app qt4 , start event loop qt4 kernel . app = get app qt4 ( [ \" \" ] ) kernel . app . set Quit On Last Window Closed ( False ) kernel . timer = Qt Core . Q Timer ( ) kernel . timer . timeout . connect ( kernel . do one iteration ) kernel . timer . start ( 1000 * kernel . poll interval ) start event loop qt4 ( kernel . app )", "predictions": ["finds the timer and other units ."], "references": ["start a kernel with pyqt4 event loop integration ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5930, "code": "def loop wx ( kernel ) : import wx from I Python . lib . guisupport import start event loop wx doi = kernel . do one iteration poll interval = int ( 1000 * kernel . poll interval ) class Timer Frame ( wx . Frame ) : def init ( self , func ) : wx . Frame . init ( self , None , - 1 ) self . timer = wx . Timer ( self ) self . timer . Start ( poll interval ) self . Bind ( wx . EVT TIMER , self . on timer ) self . func = func def on timer ( self , event ) : self . func ( ) class IP Wx App ( wx . App ) : def On Init ( self ) : self . frame = Timer Frame ( doi ) self . frame . Show ( False ) return True kernel . app = IP Wx App ( redirect = False ) import signal if not callable ( signal . getsignal ( signal . SIGINT ) ) : signal . signal ( signal . SIGINT , signal . default int handler ) start event loop wx ( kernel . app )", "predictions": ["creates a get event ."], "references": ["start a kernel with wx event loop support ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 5931, "code": "def loop tk ( kernel ) : import Tkinter doi = kernel . do one iteration poll interval = int ( 1000 * kernel . poll interval ) class Timer ( object ) : def init ( self , func ) : self . app = Tkinter . Tk ( ) self . app . withdraw ( ) self . func = func def on timer ( self ) : self . func ( ) self . app . after ( poll interval , self . on timer ) def start ( self ) : self . on timer ( ) self . app . mainloop ( ) kernel . timer = Timer ( doi ) kernel . timer . start ( )", "predictions": ["initializes this cpu and calls itself ."], "references": ["start a kernel with the tk event loop ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5932, "code": "def loop gtk ( kernel ) : from . gui . gtkembed import GTK Embed gtk kernel = GTK Embed ( kernel ) gtk kernel . start ( )", "predictions": ["constructs a new polygonization ."], "references": ["start the kernel coordinating with the gtk event loop"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 5933, "code": "def enable gui ( gui , kernel = None ) : if gui not in loop map : raise Value Error ( \"GUI %r not supported\" % gui ) if kernel is None : if Application . initialized ( ) : kernel = getattr ( Application . instance ( ) , 'kernel' , None ) if kernel is None : raise Runtime Error ( \"You didn't specify a kernel,\" \" and no I Python Application with a kernel appears to be running.\" ) loop = loop map [ gui ] if kernel . eventloop is not None and kernel . eventloop is not loop : raise Runtime Error ( \"Cannot activate multiple GUI eventloops\" ) kernel . eventloop = loop", "predictions": ["get or activate an memory memory ."], "references": ["enable integration with a given gui"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5934, "code": "def GOE ( N ) : m = ra . standard normal ( ( N , N ) ) m += m . T return m / 2", "predictions": ["returns the get standard deviation of the input using the , i . e . , the , the get difference between 0 and 1 . , 2 , 3 , and 1 ."], "references": ["creates an nxn element of the gaussian orthogonal ensemble"], "bleu": 0.046744992312922166, "rouge_l": 0.10391822827938671}
{"id": 5935, "code": "def center eigenvalue diff ( mat ) : N = len ( mat ) evals = np . sort ( la . eigvals ( mat ) ) diff = np . abs ( evals [ N / 2 ] - evals [ N / 2 - 1 ] ) return diff", "predictions": ["get the get difference between two sets of arrays . this is useful for ( ."], "references": ["compute the eigvals of mat and then find the center eigval difference ."], "bleu": 0.09672649511413092, "rouge_l": 0.21082949308755758}
{"id": 5936, "code": "def ensemble diffs ( num , N ) : diffs = np . empty ( num ) for i in xrange ( num ) : mat = GOE ( N ) diffs [ i ] = center eigenvalue diff ( mat ) return diffs", "predictions": ["computes the usage of the graph ."], "references": ["return num eigenvalue diffs for the nxn goe ensemble ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 5937, "code": "def init crash handler ( self ) : self . crash handler = self . crash handler class ( self ) sys . excepthook = self . excepthook def unset crashhandler ( ) : sys . excepthook = sys . excepthook atexit . register ( unset crashhandler )", "predictions": ["initializes this ( ( ( ( ( ( default cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache is set cache cache"], "references": ["create a crash handler typically setting sys . excepthook to it ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 5938, "code": "def init profile dir ( self ) : try : location = self . config . Profile Dir . location except Attribute Error : try : p = Profile Dir . find profile dir by name ( self . ipython dir , self . profile , self . config ) except Profile Dir Error : if self . auto create or self . profile == 'default' : try : p = Profile Dir . create profile dir by name ( self . ipython dir , self . profile , self . config ) except Profile Dir Error : self . log . fatal ( \"Could not create profile: %r\" % self . profile ) self . exit ( 1 ) else : self . log . info ( \"Created profile dir: %r\" % p . location ) else : self . log . fatal ( \"Profile %r not found.\" % self . profile ) self . exit ( 1 ) else : self . log . info ( \"Using existing profile dir: %r\" % p . location ) else : try : p = Profile Dir . find profile dir ( location , self . config ) except Profile Dir Error : if self . auto create : try : p = Profile Dir . create profile dir ( location , self . config ) except Profile Dir Error : self . log . fatal ( \"Could not create profile directory: %r\" % location ) self . exit ( 1 ) else : self . log . info ( \"Creating new profile dir: %r\" % location ) else : self . log . fatal ( \"Profile directory %r not found.\" % location ) self . exit ( 1 ) else : self . log . info ( \"Using existing profile dir: %r\" % location ) self . profile dir = p self . config file paths . append ( p . location )", "predictions": ["initialize the ( ( ( ( ( ( ( ( ( ( ( exit ) method for ( ( ) ."], "references": ["initialize the profile dir"], "bleu": 0.07645949399477267, "rouge_l": 0.18236173393124064}
{"id": 5939, "code": "def stage default config file ( self ) : s = self . generate config file ( ) fname = os . path . join ( self . profile dir . location , self . config file name ) if self . overwrite or not os . path . exists ( fname ) : self . log . warn ( \"Generating default config file: %r\" % ( fname ) ) with open ( fname , 'w' ) as f : f . write ( s )", "predictions": ["writes the ( or a ( depending on the ( source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["auto generate default config file and stage it into the profile ."], "bleu": 0.026594139297659906, "rouge_l": 0.07253269916765755}
{"id": 5940, "code": "def erase ( self ) : if self . use file : if self . filename : file be gone ( self . filename ) self . lines = { } self . arcs = { }", "predictions": ["creates a file for the process ."], "references": ["erase the data both in this object and from its file storage ."], "bleu": 0.09374222649442905, "rouge_l": 0.1897356143079316}
{"id": 5941, "code": "def line data ( self ) : return dict ( [ ( f , sorted ( lmap . keys ( ) ) ) for f , lmap in iitems ( self . lines ) ] )", "predictions": ["get a : : : : = some ensure we have a non - negative ensure that the ensure that the ensure that the ensure that the ensure that the ensure that the ensure that the ensure that the ensure that the ensure that the ensure that the ensure that"], "references": ["return the map from filenames to lists of line numbers executed ."], "bleu": 0.02403051755364481, "rouge_l": 0.03626634958382877}
{"id": 5942, "code": "def arc data ( self ) : return dict ( [ ( f , sorted ( amap . keys ( ) ) ) for f , amap in iitems ( self . arcs ) ] )", "predictions": ["convenience method for getting a path ."], "references": ["return the map from filenames to lists of line number pairs ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 5943, "code": "def write file ( self , filename ) : data = { } data [ 'lines' ] = self . line data ( ) arcs = self . arc data ( ) if arcs : data [ 'arcs' ] = arcs if self . collector : data [ 'collector' ] = self . collector if self . debug and self . debug . should ( 'dataio' ) : self . debug . write ( \"Writing data to %r\" % ( filename , ) ) fdata = open ( filename , 'wb' ) try : pickle . dump ( data , fdata , 2 ) finally : fdata . close ( )", "predictions": ["writes a ( or a except file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file file"], "references": ["write the coverage data to filename ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 5944, "code": "def read file ( self , filename ) : self . lines , self . arcs = self . read file ( filename )", "predictions": ["new method to new mx qt qt contents ."], "references": ["read the coverage data from filename ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5945, "code": "def raw data ( self , filename ) : if self . debug and self . debug . should ( 'dataio' ) : self . debug . write ( \"Reading data from %r\" % ( filename , ) ) fdata = open ( filename , 'rb' ) try : data = pickle . load ( fdata ) finally : fdata . close ( ) return data", "predictions": ["writes data for this case to the given file ."], "references": ["return the raw pickled data from filename ."], "bleu": 0.14991106946711685, "rouge_l": 0.22676579925650556}
{"id": 5946, "code": "def add to hash ( self , filename , hasher ) : hasher . update ( self . executed lines ( filename ) ) hasher . update ( self . executed arcs ( filename ) )", "predictions": ["adds a file to the collection of urllib2 = timeout , with an updated code ."], "references": ["contribute filename s data to the md5hash hasher ."], "bleu": 0.10878661088699644, "rouge_l": 0.2527624309392265}
{"id": 5947, "code": "def get pasted lines ( sentinel , l input = py3compat . input ) : print \"Pasting code; enter '%s' alone on the line to stop or use Ctrl-D.\" % sentinel while True : try : l = l input ( ':' ) if l == sentinel : return else : yield l except EOF Error : print '<EOF>' return", "predictions": ["yield ( ."], "references": ["yield pasted lines until the user enters the given sentinel value ."], "bleu": 0.029603567969095357, "rouge_l": 0.24063116370808674}
{"id": 5948, "code": "def replace rlhist multiline ( self , source raw , hlen before cell ) : if not self . has readline or not self . multiline history : return hlen before cell if not hasattr ( self . readline , \"remove history item\" ) : return hlen before cell if not source raw . rstrip ( ) : return hlen before cell hlen = self . readline . get current history length ( ) if hlen == hlen before cell : return hlen before cell for i in range ( hlen - hlen before cell ) : self . readline . remove history item ( hlen - i - 1 ) stdin encoding = get stream enc ( sys . stdin , 'utf-8' ) self . readline . add history ( py3compat . unicode to str ( source raw . rstrip ( ) , stdin encoding ) ) return self . readline . get current history length ( )", "predictions": ["replaces all the servers in the is currently compatible with another exists . this will always return the new one ."], "references": ["store multiple lines as a single entry in history"], "bleu": 0.05809665204409193, "rouge_l": 0.071849234393404}
{"id": 5949, "code": "def interact ( self , display banner = None ) : if self . exit now : return if display banner is None : display banner = self . display banner if isinstance ( display banner , basestring ) : self . show banner ( display banner ) elif display banner : self . show banner ( ) more = False if self . has readline : self . readline startup hook ( self . pre readline ) hlen b4 cell = self . readline . get current history length ( ) else : hlen b4 cell = 0 while not self . exit now : self . hooks . pre prompt hook ( ) if more : try : prompt = self . prompt manager . render ( 'in2' ) except : self . showtraceback ( ) if self . autoindent : self . rl do indent = True else : try : prompt = self . separate in + self . prompt manager . render ( 'in' ) except : self . showtraceback ( ) try : line = self . raw input ( prompt ) if self . exit now : break if self . autoindent : self . rl do indent = False except Keyboard Interrupt : #double-guard against keyboardinterrupts during kbdint handling try : self . write ( '\\n Keyboard Interrupt\\n' ) source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) more = False except Keyboard Interrupt : pass except EOF Error : if self . autoindent : self . rl do indent = False if self . has readline : self . readline startup hook ( None ) self . write ( '\\n' ) self . exit ( ) except bdb . Bdb Quit : warn ( 'The Python debugger has exited with a Bdb Quit exception.\\n' 'Because of how pdb handles the stack, it is impossible\\n' 'for I Python to properly format this particular exception.\\n' 'I Python will resume normal operation.' ) except : self . showtraceback ( ) else : self . input splitter . push ( line ) more = self . input splitter . push accepts more ( ) if ( self . Syntax TB . last syntax error and self . autoedit syntax ) : self . edit syntax error ( ) if not more : source raw = self . input splitter . source raw reset ( ) [ 1 ] self . run cell ( source raw , store history = True ) hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) self . exit now = False", "predictions": ["this method will run once the user has been created and the syntax will run ."], "references": ["closely emulate the interactive python console ."], "bleu": 0.08513012360883544, "rouge_l": 0.1871165644171779}
{"id": 5950, "code": "def should recompile ( self , e ) : if e . filename in ( '<ipython console>' , '<input>' , '<string>' , '<console>' , '<Background Job compilation>' , None ) : return False try : if ( self . autoedit syntax and not self . ask yes no ( 'Return to editor to correct syntax error? ' '[Y/n] ' , 'y' ) ) : return False except EOF Error : return False def int0 ( x ) : try : return int ( x ) except Type Error : return 0 try : self . hooks . fix error editor ( e . filename , int0 ( e . lineno ) , int0 ( e . offset ) , e . msg ) except Try Next : warn ( 'Could not open editor' ) return False return True", "predictions": ["determine if this controller abort a syntax ."], "references": ["utility routine for edit_syntax_error"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5951, "code": "def new frontend master ( self ) : ip = self . ip if self . ip in LOCAL IPS else LOCALHOST kernel manager = self . kernel manager class ( ip = ip , connection file = self . new connection file ( ) , config = self . config , ) kwargs = dict ( ) kwargs [ 'extra arguments' ] = self . kernel argv kernel manager . start kernel ( * * kwargs ) kernel manager . start channels ( ) widget = self . widget factory ( config = self . config , local kernel = True ) self . init colors ( widget ) widget . kernel manager = kernel manager widget . existing = False widget . may close = True widget . confirm exit = self . confirm exit return widget", "predictions": ["creates a elapsed widget widget ."], "references": ["create and return new frontend attached to new kernel launched on localhost ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 5952, "code": "def init colors ( self , widget ) : try : colors = self . config . ZMQ Interactive Shell . colors except Attribute Error : colors = None try : style = self . config . I Python Widget . syntax style except Attribute Error : style = None try : sheet = self . config . I Python Widget . style sheet except Attribute Error : sheet = None if colors : colors = colors . lower ( ) if colors in ( 'lightbg' , 'light' ) : colors = 'lightbg' elif colors in ( 'dark' , 'linux' ) : colors = 'linux' else : colors = 'nocolor' elif style : if style == 'bw' : colors = 'nocolor' elif styles . dark style ( style ) : colors = 'linux' else : colors = 'lightbg' else : colors = None if style : widget . style sheet = styles . sheet from template ( style , colors ) widget . syntax style = style widget . syntax style changed ( ) widget . style sheet changed ( ) elif colors : widget . set default style ( colors = colors ) if self . stylesheet : if os . path . isfile ( self . stylesheet ) : with open ( self . stylesheet ) as f : sheet = f . read ( ) else : raise IO Error ( \"Stylesheet %r not found.\" % self . stylesheet ) if sheet : widget . style sheet = sheet widget . style sheet changed ( )", "predictions": ["initializes the sheet widget ."], "references": ["configure the coloring of the widget"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 5953, "code": "def info ( self ) : return ( self . identity , self . url , self . pub url , self . location )", "predictions": ["call this method to get the info of the info object ."], "references": ["return the connection info for this object s sockets ."], "bleu": 0.1367440667823257, "rouge_l": 0.3696969696969697}
{"id": 5954, "code": "def set colors ( self , * args , * * kw ) : self . color scheme table . set active scheme ( * args , * * kw ) self . Colors = self . color scheme table . active colors if hasattr ( self , 'pdb' ) and self . pdb is not None : self . pdb . set colors ( * args , * * kw )", "predictions": ["this is called by ( for setting colors like system properties ."], "references": ["shorthand access to the color table scheme selector method ."], "bleu": 0.10390302174233558, "rouge_l": 0.09242424242424242}
{"id": 5955, "code": "def color toggle ( self ) : if self . color scheme table . active scheme name == 'No Color' : self . color scheme table . set active scheme ( self . old scheme ) self . Colors = self . color scheme table . active colors else : self . old scheme = self . color scheme table . active scheme name self . color scheme table . set active scheme ( 'No Color' ) self . Colors = self . color scheme table . active colors", "predictions": ["we use this to toggle the table color ."], "references": ["toggle between the currently active color scheme and nocolor ."], "bleu": 0.15881076016027915, "rouge_l": 0.41709401709401706}
{"id": 5956, "code": "def group required ( group , login url = None , redirect field name = REDIRECT FIELD NAME , skip superuser = True ) : def decorator ( view func ) : @ login required ( redirect field name = redirect field name , login url = login url ) def wrapped view ( request , * args , * * kwargs ) : if not ( request . user . is superuser and skip superuser ) : if request . user . groups . filter ( name = group ) . count ( ) == 0 : raise Permission Denied return view func ( request , * args , * * kwargs ) return wrapped view return decorator", "predictions": ["generate a decorator for a group that has a login page . note : this only handles the user has not been constructed ."], "references": ["view decorator for requiring a user group ."], "bleu": 0.0824055698798382, "rouge_l": 0.34346846846846846}
{"id": 5957, "code": "def add submodule ( mod , submod , fullname , subname ) : if mod is None : return #Nothing to do here. if submod is None : submod = sys . modules [ fullname ] setattr ( mod , subname , submod ) return", "predictions": ["add an additional module declaration to this object ."], "references": ["mod . { subname } = submod"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5958, "code": "def ensure fromlist ( mod , fromlist , buf , recursive ) : if not hasattr ( mod , ' path ' ) : return for item in fromlist : if not hasattr ( item , 'rindex' ) : raise Type Error ( \"Item in ``from list'' not a string\" ) if item == '*' : if recursive : continue try : all = mod . all except Attribute Error : pass else : ret = ensure fromlist ( mod , all , buf , 1 ) if not ret : return 0 elif not hasattr ( mod , item ) : import submodule ( mod , item , buf + '.' + item )", "predictions": ["loads an instanceof from the reader ."], "references": ["handle from module import a b c imports ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5959, "code": "def add section ( self ) : sect = Code Builder ( self . indent amount ) self . code . append ( sect ) return sect", "predictions": ["generate a section . since we need to do this on the daemon thread ."], "references": ["add a section a sub - codebuilder ."], "bleu": 0.11633270842295028, "rouge_l": 0.2760180995475113}
{"id": 5960, "code": "def get function ( self , fn name ) : assert self . indent amount == 0 g = { } code text = str ( self ) exec ( code text , g ) return g [ fn name ]", "predictions": ["use this method to get the method of the function call ."], "references": ["compile the code and return the function fn_name ."], "bleu": 0.15537125692760353, "rouge_l": 0.39102564102564097}
{"id": 5961, "code": "def expr code ( self , expr ) : if \"|\" in expr : pipes = expr . split ( \"|\" ) code = self . expr code ( pipes [ 0 ] ) for func in pipes [ 1 : ] : self . all vars . add ( func ) code = \"c %s(%s)\" % ( func , code ) elif \".\" in expr : dots = expr . split ( \".\" ) code = self . expr code ( dots [ 0 ] ) args = [ repr ( d ) for d in dots [ 1 : ] ] code = \"dot(%s, %s)\" % ( code , \", \" . join ( args ) ) else : self . all vars . add ( expr ) code = \"c %s\" % expr return code", "predictions": ["merge the input expressions into a sympy expression ."], "references": ["generate a python expression for expr ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 5962, "code": "def do dots ( self , value , * dots ) : for dot in dots : try : value = getattr ( value , dot ) except Attribute Error : value = value [ dot ] if hasattr ( value , ' call ' ) : value = value ( ) return value", "predictions": ["copies this dots as a dots ."], "references": ["evaluate dotted expressions at runtime ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 5963, "code": "def formatters default ( self ) : formatter classes = [ Plain Text Formatter , HTML Formatter , SVG Formatter , PNG Formatter , JPEG Formatter , Latex Formatter , JSON Formatter , Javascript Formatter ] d = { } for cls in formatter classes : f = cls ( config = self . config ) d [ f . format type ] = f return d", "predictions": ["a simple view to create a default formatters for a django formatter ."], "references": ["activate the default formatters ."], "bleu": 0.1350862565735141, "rouge_l": 0.36237623762376237}
{"id": 5964, "code": "def user config files ( ) : return filter ( os . path . exists , map ( os . path . expanduser , config files ) )", "predictions": ["returns all files belonging to this user ."], "references": ["return path to any existing user config files"], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 5965, "code": "def configure Where ( self , where ) : from nose . importer import add path self . working Dir = None where = tolist ( where ) warned = False for path in where : if not self . working Dir : abs path = absdir ( path ) if abs path is None : raise Value Error ( \"Working directory %s not found, or \" \"not a directory\" % path ) log . info ( \"Set working dir to %s\" , abs path ) self . working Dir = abs path if self . add Paths and os . path . exists ( os . path . join ( abs path , ' init .py' ) ) : log . info ( \"Working directory %s is a package; \" \"adding to sys.path\" % abs path ) add path ( abs path ) continue if not warned : warn ( \"Use of multiple -w arguments is deprecated and \" \"support may be removed in a future release. You can \" \"get the same behavior by passing directories without \" \"the -w argument on the command line, or by using the \" \"--tests argument in a configuration file.\" , Deprecation Warning ) self . test Names . append ( path )", "predictions": ["for the importer that will be set to the importer ."], "references": ["configure the working directory or directories for the test run ."], "bleu": 0.17033186037639278, "rouge_l": 0.2727272727272727}
{"id": 5966, "code": "def page file ( fname , start = 0 , pager cmd = None ) : pager cmd = get pager cmd ( pager cmd ) pager cmd += ' ' + get pager start ( pager cmd , start ) try : if os . environ [ 'TERM' ] in [ 'emacs' , 'dumb' ] : raise Environment Error system ( pager cmd + ' ' + fname ) except : try : if start > 0 : start -= 1 page ( open ( fname ) . read ( ) , start ) except : print 'Unable to show file' , `fname`", "predictions": ["read a page from the pager ."], "references": ["page a file using an optional pager command and starting line ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 5967, "code": "def print basic unicode ( o , p , cycle ) : if cycle : return p . text ( 'Basic(...)' ) out = pretty ( o , use unicode = True ) if '\\n' in out : p . text ( u'\\n' ) p . text ( out )", "predictions": ["prints out an fs ."], "references": ["a function to pretty print sympy basic objects ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5968, "code": "def print png ( o ) : s = latex ( o , mode = 'inline' ) s = s . replace ( '\\\\operatorname' , '' ) s = s . replace ( '\\\\overline' , '\\\\bar' ) png = latex to png ( s ) return png", "predictions": ["prints the latex with the new color ."], "references": ["a function to display sympy expression using inline style latex in png ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 5969, "code": "def print display png ( o ) : s = latex ( o , mode = 'plain' ) s = s . strip ( '$' ) png = latex to png ( '$$%s$$' % s , backend = 'dvipng' ) return png", "predictions": ["returns a display value in a human - readable form ."], "references": ["a function to display sympy expression using display style latex in png ."], "bleu": 0.11941964005964323, "rouge_l": 0.3283983849259758}
{"id": 5970, "code": "def load ipython extension ( ip ) : import sympy try : import sympy . interactive . ipythonprinting except Import Error : pass else : warnings . warn ( \"The sympyprinting extension in I Python is deprecated, \" \"use sympy.interactive.ipythonprinting\" ) ip . extension manager . load extension ( 'sympy.interactive.ipythonprinting' ) return global loaded if not loaded : plaintext formatter = ip . display formatter . formatters [ 'text/plain' ] for cls in ( object , str ) : plaintext formatter . for type ( cls , print basic unicode ) printable containers = [ list , tuple ] if sympy . version > '0.7.1' : printable containers += [ set , frozenset ] else : plaintext formatter . for type ( cls , print basic unicode ) plaintext formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print basic unicode ) plaintext formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print basic unicode ) png formatter = ip . display formatter . formatters [ 'image/png' ] png formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print png ) png formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print display png ) for cls in [ dict , int , long , float ] + printable containers : png formatter . for type ( cls , print png ) latex formatter = ip . display formatter . formatters [ 'text/latex' ] latex formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print latex ) latex formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print latex ) for cls in printable containers : latex formatter . for type ( cls , print latex ) loaded = True", "predictions": ["load the latex of the provided ip address and configure it as a display of the plaintext ."], "references": ["load the extension in ipython ."], "bleu": 0.09629943614188137, "rouge_l": 0.27477477477477474}
{"id": 5971, "code": "def run loop ( self ) : while True : try : self . ioloop . start ( ) except ZMQ Error as e : if e . errno == errno . EINTR : continue else : raise except Exception : if self . exiting : break else : raise else : break", "predictions": ["run a loop . this method does not include the underlying output ."], "references": ["run my loop ignoring eintr events in the poller"], "bleu": 0.1135935489027116, "rouge_l": 0.2819722650231125}
{"id": 5972, "code": "def input ( self , string ) : content = dict ( value = string ) msg = self . session . msg ( 'input reply' , content ) self . queue send ( msg )", "predictions": ["sends a message to the log file using the queue as a queue ."], "references": ["send a string of raw input to the kernel ."], "bleu": 0.13217947626377288, "rouge_l": 0.34366197183098596}
{"id": 5973, "code": "def stop channels ( self ) : if self . shell channel . is alive ( ) : self . shell channel . stop ( ) if self . sub channel . is alive ( ) : self . sub channel . stop ( ) if self . stdin channel . is alive ( ) : self . stdin channel . stop ( ) if self . hb channel . is alive ( ) : self . hb channel . stop ( )", "predictions": ["stops this log operation ."], "references": ["stops all the running channels for this kernel ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 5974, "code": "def channels running ( self ) : return ( self . shell channel . is alive ( ) or self . sub channel . is alive ( ) or self . stdin channel . is alive ( ) or self . hb channel . is alive ( ) )", "predictions": ["call this if you have a running operation ."], "references": ["are any of the channels created and running?"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5975, "code": "def load connection file ( self ) : with open ( self . connection file ) as f : cfg = json . loads ( f . read ( ) ) self . ip = cfg [ 'ip' ] self . shell port = cfg [ 'shell port' ] self . stdin port = cfg [ 'stdin port' ] self . iopub port = cfg [ 'iopub port' ] self . hb port = cfg [ 'hb port' ] self . session . key = str to bytes ( cfg [ 'key' ] )", "predictions": ["loads and records the connection"], "references": ["load connection info from json dict in self . connection_file"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 5976, "code": "def write connection file ( self ) : if self . connection file written : return self . connection file , cfg = write connection file ( self . connection file , ip = self . ip , key = self . session . key , stdin port = self . stdin port , iopub port = self . iopub port , shell port = self . shell port , hb port = self . hb port ) self . shell port = cfg [ 'shell port' ] self . stdin port = cfg [ 'stdin port' ] self . iopub port = cfg [ 'iopub port' ] self . hb port = cfg [ 'hb port' ] self . connection file written = True", "predictions": ["writes an event to the server ."], "references": ["write connection info to json dict in self . connection_file"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 5977, "code": "def kill kernel ( self ) : if self . has kernel : if self . hb channel is not None : self . hb channel . pause ( ) try : self . kernel . kill ( ) except OS Error , e : if sys . platform == 'win32' : if e . winerror != 5 : raise else : from errno import ESRCH if e . errno != ESRCH : raise self . kernel = None else : raise Runtime Error ( \"Cannot kill kernel. No kernel is running!\" )", "predictions": ["kill and kill all still since the kernel calls ."], "references": ["kill the running kernel ."], "bleu": 0.15851165692617156, "rouge_l": 0.5674418604651164}
{"id": 5978, "code": "def is alive ( self ) : if self . has kernel : if self . kernel . poll ( ) is None : return True else : return False elif self . hb channel is not None : return self . hb channel . is beating ( ) else : return True", "predictions": ["checks if there is a going operation on this channel ."], "references": ["is the kernel process still running?"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 5979, "code": "def shell channel ( self ) : if self . shell channel is None : self . shell channel = self . shell channel class ( self . context , self . session , ( self . ip , self . shell port ) ) return self . shell channel", "predictions": ["sets the shell region for this fieldtype ."], "references": ["get the req socket channel object to make requests of the kernel ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 5980, "code": "def sub channel ( self ) : if self . sub channel is None : self . sub channel = self . sub channel class ( self . context , self . session , ( self . ip , self . iopub port ) ) return self . sub channel", "predictions": ["sets the sub commit commit to this channel ."], "references": ["get the sub socket channel object ."], "bleu": 0.21105340631872635, "rouge_l": 0.5115303983228512}
{"id": 5981, "code": "def walk egg ( egg dir ) : walker = os . walk ( egg dir ) base , dirs , files = walker . next ( ) if 'EGG-INFO' in dirs : dirs . remove ( 'EGG-INFO' ) yield base , dirs , files for bdf in walker : yield bdf", "predictions": ["walk an unpacked eggs in the python cookbook ."], "references": ["walk an unpacked egg s contents skipping the metadata directory"], "bleu": 0.24855227187657006, "rouge_l": 0.41709401709401706}
{"id": 5982, "code": "def scan module ( egg dir , base , name , stubs ) : filename = os . path . join ( base , name ) if filename [ : - 1 ] in stubs : return True pkg = base [ len ( egg dir ) + 1 : ] . replace ( os . sep , '.' ) module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] if sys . version info < ( 3 , 3 ) : skip = 8 else : skip = 12 f = open ( filename , 'rb' ) f . read ( skip ) code = marshal . load ( f ) f . close ( ) safe = True symbols = dict . fromkeys ( iter symbols ( code ) ) for bad in [ ' file ' , ' path ' ] : if bad in symbols : log . warn ( \"%s: module references %s\" , module , bad ) safe = False if 'inspect' in symbols : for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : if bad in symbols : log . warn ( \"%s: module MAY be using inspect.%s\" , module , bad ) safe = False if ' name ' in symbols and ' main ' in symbols and '.' not in module : if sys . version [ : 3 ] == \"2.4\" : log . warn ( \"%s: top-level module may be 'python -m' script\" , module ) safe = False return safe", "predictions": ["scans the cube for a module ."], "references": ["check whether module possibly uses unsafe - for - zipfile stuff"], "bleu": 0.1160873020151595, "rouge_l": 0.10683012259194395}
{"id": 5983, "code": "def make init files ( self ) : init files = [ ] for base , dirs , files in walk egg ( self . bdist dir ) : if base == self . bdist dir : continue for name in files : if name . endswith ( '.py' ) : if ' init .py' not in files : pkg = base [ len ( self . bdist dir ) + 1 : ] . replace ( os . sep , '.' ) if self . distribution . has contents for ( pkg ) : log . warn ( \"Creating missing  init .py for %s\" , pkg ) filename = os . path . join ( base , ' init .py' ) if not self . dry run : f = open ( filename , 'w' ) f . write ( NS PKG STUB ) f . close ( ) init files . append ( filename ) break else : dirs [ : ] = [ ] return init files", "predictions": ["initialize and return an sensortypeapi ."], "references": ["create missing package __init__ files"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5984, "code": "def launch new instance ( ) : if sys . platform == 'win32' : import multiprocessing p = multiprocessing . current process ( ) if p . name != 'Main Process' : return app = IP Controller App . instance ( ) app . initialize ( ) app . start ( )", "predictions": ["init the . . this is used by the . method ."], "references": ["create and run the ipython controller"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 5985, "code": "def save connection dict ( self , fname , cdict ) : c = self . config url = cdict [ 'url' ] location = cdict [ 'location' ] if not location : try : proto , ip , port = split url ( url ) except Assertion Error : pass else : try : location = socket . gethostbyname ex ( socket . gethostname ( ) ) [ 2 ] [ - 1 ] except ( socket . gaierror , Index Error ) : self . log . warn ( \"Could not identify this machine's IP, assuming 127.0.0.1.\" \" You may need to specify '--location=<external ip address>' to help\" \" I Python decide when to connect via loopback.\" ) location = '127.0.0.1' cdict [ 'location' ] = location fname = os . path . join ( self . profile dir . security dir , fname ) self . log . info ( \"writing connection info to %s\" , fname ) with open ( fname , 'w' ) as f : f . write ( json . dumps ( cdict , indent = 2 ) ) os . chmod ( fname , stat . S IRUSR | stat . S IWUSR )", "predictions": ["saves an ( . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , ."], "references": ["save a connection dict to json file ."], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 5986, "code": "def load config from json ( self ) : c = self . config self . log . debug ( \"loading config from JSON\" ) fname = os . path . join ( self . profile dir . security dir , self . engine json file ) self . log . info ( \"loading connection info from %s\" , fname ) with open ( fname ) as f : cfg = json . loads ( f . read ( ) ) key = cfg [ 'exec key' ] c . Session . key = key . encode ( 'ascii' ) xport , addr = cfg [ 'url' ] . split ( '://' ) c . Hub Factory . engine transport = xport ip , ports = addr . split ( ':' ) c . Hub Factory . engine ip = ip c . Hub Factory . regport = int ( ports ) self . location = cfg [ 'location' ] if not self . engine ssh server : self . engine ssh server = cfg [ 'ssh' ] fname = os . path . join ( self . profile dir . security dir , self . client json file ) self . log . info ( \"loading connection info from %s\" , fname ) with open ( fname ) as f : cfg = json . loads ( f . read ( ) ) assert key == cfg [ 'exec key' ] , \"exec key mismatch between engine and client keys\" xport , addr = cfg [ 'url' ] . split ( '://' ) c . Hub Factory . client transport = xport ip , ports = addr . split ( ':' ) c . Hub Factory . client ip = ip if not self . ssh server : self . ssh server = cfg [ 'ssh' ] assert int ( ports ) == c . Hub Factory . regport , \"regport mismatch\"", "predictions": ["set up and ssh in self ."], "references": ["load config from existing json connector files ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5987, "code": "def load secondary config ( self ) : if self . reuse files : try : self . load config from json ( ) except ( Assertion Error , IO Error ) as e : self . log . error ( \"Could not load config from JSON: %s\" % e ) else : self . write connection files = False default secure ( self . config ) self . log . debug ( \"Config changed\" ) self . log . debug ( repr ( self . config ) )", "predictions": ["for the given . ."], "references": ["secondary config loading from json and setting defaults"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 5988, "code": "def script args ( f ) : args = [ magic arguments . argument ( '--out' , type = str , help = ) , magic arguments . argument ( '--err' , type = str , help = ) , magic arguments . argument ( '--bg' , action = \"store true\" , help = ) , magic arguments . argument ( '--proc' , type = str , help = ) , ] for arg in args : f = arg ( f ) return f", "predictions": ["create a group of the given arguments ."], "references": ["single decorator for adding script args"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5989, "code": "def parallel execute ( self , cell , block = None , groupby = 'type' , save name = None ) : block = self . view . block if block is None else block base = \"Parallel\" if block else \"Async parallel\" targets = self . view . targets if isinstance ( targets , list ) and len ( targets ) > 10 : str targets = str ( targets [ : 4 ] ) [ : - 1 ] + ', ..., ' + str ( targets [ - 4 : ] ) [ 1 : ] else : str targets = str ( targets ) if self . verbose : print base + \" execution on engine(s): %s\" % str targets result = self . view . execute ( cell , silent = False , block = False ) self . last result = result if save name : self . shell . user ns [ save name ] = result if block : result . get ( ) result . display outputs ( groupby ) else : return result", "predictions": ["this is called by the add operation on the client ."], "references": ["implementation used by %px and %%parallel"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 5990, "code": "def disable autopx ( self ) : if self . autopx : self . shell . run cell = self . original run cell self . autopx = False print \"%autopx disabled\"", "predictions": ["decorator for setting ( ."], "references": ["disable %autopx by restoring the original interactiveshell . run_cell ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 5991, "code": "def run heartbeat ( message ) : then = arrow . get ( message [ 'time' ] ) now = arrow . get ( ) if ( now - then ) > timezone . timedelta ( seconds = ( TICK FREQ + 1 ) ) : pass else : Task . run tasks ( )", "predictions": ["retrieves the time marked as a section ."], "references": ["internal clock_channel consumer to process task runs"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5992, "code": "def run task ( message ) : task = Task . objects . get ( pk = message [ 'id' ] ) if task . allow overlap : task . run ( message ) else : if not task . running : task . running = True task . save ( ) try : task . run ( message ) finally : task . running = False task . save ( )", "predictions": ["performs a function on the specified self ."], "references": ["internal run_task consumer to run the task s callable"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5993, "code": "def remove task ( message ) : task = Task . objects . get ( pk = message [ 'id' ] ) task . delete ( )", "predictions": ["removes the given self from the set of in the group"], "references": ["internal kill_task consumer to remove retired tasks"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5994, "code": "def patch if missing ( obj , name , method ) : setattr ( obj , name , getattr ( obj , name , method ) )", "predictions": ["do some use of the ) ."], "references": ["patch a method onto an object if it isn t already there ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 5995, "code": "def accept connection ( self ) : assert self . pending , \"Connection is not pending.\" self . server protocol = self . server . server factory . build Protocol ( None ) self . accept d . callback ( Fake Server Protocol Wrapper ( self , self . server protocol ) ) return self . await connected ( )", "predictions": ["formatters the default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default default"], "references": ["accept a pending connection ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 5996, "code": "def reject connection ( self , reason = None ) : assert self . pending , \"Connection is not pending.\" if reason is None : reason = Connection Refused Error ( ) self . accept d . errback ( reason )", "predictions": ["disconnects the connection from the connection ."], "references": ["reject a pending connection ."], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 5997, "code": "def get agent ( self , reactor = None , context Factory = None ) : return Proxy Agent With Context ( self . endpoint , reactor = reactor , context Factory = context Factory )", "predictions": ["create an agent factory that wraps the given where the where the where the where the where the where flag is indexed ."], "references": ["returns an iagent that makes requests to this fake server ."], "bleu": 0.06293173924458136, "rouge_l": 0.18846549948506694}
{"id": 5998, "code": "def form valid ( self , form ) : self . object = form . save ( commit = False ) response = self . pre save ( self . object ) if response : return response self . object . save ( ) form . save m2m ( ) self . post save ( self . object ) return Http Response Redirect ( self . get success url ( ) )", "predictions": ["specific page to description conversion ."], "references": ["calls pre and post save hooks ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5999, "code": "def delete ( self , request , * args , * * kwargs ) : self . object = self . get object ( ) success url = self . get success url ( ) self . pre delete ( self . object ) self . object . delete ( ) self . post delete ( self . object ) return Http Response Redirect ( success url )", "predictions": ["deletes an object and removes it ."], "references": ["calls pre and post delete hooks for delteviews ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 6000, "code": "def pre save ( self , instance ) : super ( User View Mixin , self ) . pre save ( instance ) if self . request . user . is authenticated ( ) : for field in self . user field : setattr ( instance , field , self . request . user )", "predictions": ["5 5 . save to the database ."], "references": ["use savehookmixin pre_save to set the user ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 6001, "code": "def check ( self , check all = False ) : if not self . enabled and not check all : return if check all or self . check all : modules = sys . modules . keys ( ) else : modules = self . modules . keys ( ) for modname in modules : m = sys . modules . get ( modname , None ) if modname in self . skip modules : continue if not hasattr ( m , ' file ' ) : continue if m . name == ' main ' : continue filename = m . file path , ext = os . path . splitext ( filename ) if ext . lower ( ) == '.py' : ext = PY COMPILED EXT pyc filename = pyfile . cache from source ( filename ) py filename = filename else : pyc filename = filename try : py filename = pyfile . source from cache ( filename ) except Value Error : continue try : pymtime = os . stat ( py filename ) . st mtime if pymtime <= os . stat ( pyc filename ) . st mtime : continue if self . failed . get ( py filename , None ) == pymtime : continue except OS Error : continue try : superreload ( m , reload , self . old objects ) if py filename in self . failed : del self . failed [ py filename ] except : print >> sys . stderr , \"[autoreload of %s failed: %s]\" % ( modname , traceback . format exc ( 1 ) ) self . failed [ py filename ] = pymtime", "predictions": ["print all the to the correct files ."], "references": ["check whether some modules need to be reloaded ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 6002, "code": "def clipboard get ( self ) : from I Python . lib . clipboard import ( osx clipboard get , tkinter clipboard get , win32 clipboard get ) if sys . platform == 'win32' : chain = [ win32 clipboard get , tkinter clipboard get ] elif sys . platform == 'darwin' : chain = [ osx clipboard get , tkinter clipboard get ] else : chain = [ tkinter clipboard get ] dispatcher = Command Chain Dispatcher ( ) for func in chain : dispatcher . add ( func ) text = dispatcher ( ) return text", "predictions": ["ipython from load . load ."], "references": ["get text from the clipboard ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 6003, "code": "def add ( self , func , priority = 0 ) : self . chain . append ( ( priority , func ) ) self . chain . sort ( key = lambda x : x [ 0 ] )", "predictions": ["creates a operator object for a specific element of the list ."], "references": ["add a func to the cmd chain with given priority"], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 6004, "code": "def configure ( self , options , conf ) : self . conf = conf self . enabled = options . debug Errors or options . debug Failures self . enabled for errors = options . debug Errors self . enabled for failures = options . debug Failures", "predictions": ["configures the latest string string ."], "references": ["configure which kinds of exceptions trigger plugin ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 6005, "code": "def import item ( name ) : package = '.' . join ( name . split ( '.' ) [ 0 : - 1 ] ) obj = name . split ( '.' ) [ - 1 ] if package : module = import ( package , fromlist = [ obj ] ) try : pak = module . dict [ obj ] except Key Error : raise Import Error ( 'No module named %s' % obj ) return pak else : return import ( obj )", "predictions": ["stop the channels map ."], "references": ["import and return bar given the string foo . bar ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 6006, "code": "def try passwordless openssh ( server , keyfile ) : if pexpect is None : raise Import Error ( \"pexpect unavailable, use paramiko\" ) cmd = 'ssh -f ' + server if keyfile : cmd += ' -i ' + keyfile cmd += ' exit' p = pexpect . spawn ( cmd ) while True : try : p . expect ( '[Pp]assword:' , timeout = .1 ) except pexpect . TIMEOUT : continue except pexpect . EOF : return True else : return False", "predictions": ["channels running running running running running running running running return only the running return from the is pre - command ."], "references": ["try passwordless login with shell ssh command ."], "bleu": 0.07645949399477267, "rouge_l": 0.15006150061500614}
{"id": 6007, "code": "def try passwordless paramiko ( server , keyfile ) : if paramiko is None : msg = \"Paramiko unavaliable, \" if sys . platform == 'win32' : msg += \"Paramiko is required for ssh tunneled connections on Windows.\" else : msg += \"use Open SSH.\" raise Import Error ( msg ) username , server , port = split server ( server ) client = paramiko . SSH Client ( ) client . load system host keys ( ) client . set missing host key policy ( paramiko . Warning Policy ( ) ) try : client . connect ( server , port , username = username , key filename = keyfile , look for keys = True ) except paramiko . Authentication Exception : return False else : client . close ( ) return True", "predictions": ["load a remote file for android ."], "references": ["try passwordless login with paramiko ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 6008, "code": "def unwrap exception ( self , content ) : e = error . unwrap exception ( content ) if e . engine info : e uuid = e . engine info [ 'engine uuid' ] eid = self . engines [ e uuid ] e . engine info [ 'engine id' ] = eid return e", "predictions": ["forwards an connection to the write ."], "references": ["unwrap exception and remap engine_id to int ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 6009, "code": "def register engine ( self , msg ) : content = msg [ 'content' ] eid = content [ 'id' ] d = { eid : content [ 'queue' ] } self . update engines ( d )", "predictions": ["plugin registration of this class ."], "references": ["register a new engine and update our connection info ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 6010, "code": "def unregister engine ( self , msg ) : content = msg [ 'content' ] eid = int ( content [ 'id' ] ) if eid in self . ids : self . ids . remove ( eid ) uuid = self . engines . pop ( eid ) self . handle stranded msgs ( eid , uuid ) if self . task socket and self . task scheme == 'pure' : self . stop scheduling tasks ( )", "predictions": ["removes a previously registered task class ."], "references": ["unregister an engine that has died ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 6011, "code": "def flush results ( self , sock ) : idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) while msg is not None : if self . debug : pprint ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] handler = self . queue handlers . get ( msg type , None ) if handler is None : raise Exception ( \"Unhandled message type: %s\" % msg . msg type ) else : handler ( msg ) idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK )", "predictions": ["shell channel bytes for this operator ."], "references": ["flush task or queue results waiting in zmq queue ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 6012, "code": "def flush ignored control ( self ) : while self . ignored control replies > 0 : self . session . recv ( self . control socket ) self . ignored control replies -= 1", "predictions": ["flushes the ( : a method to be called once : this method has been called prior to the commit process : this method is called to sub - existing ( : : [ 0 , 2 ] : [ 0 , 3 ] : [ 2 ] : 1"], "references": ["flush ignored control replies"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 6013, "code": "def spin every ( self , interval = 1 ) : while True : if self . stop spinning . is set ( ) : return time . sleep ( interval ) self . spin ( )", "predictions": ["set up the walk for the : 1 . 5 ."], "references": ["target func for use in spin_thread"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 6014, "code": "def stop spin thread ( self ) : if self . spin thread is not None : self . stop spinning . set ( ) self . spin thread . join ( ) self . spin thread = None", "predictions": ["scan for the background thread ."], "references": ["stop background spin_thread if any"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 6015, "code": "def send execute request ( self , socket , code , silent = True , subheader = None , ident = None ) : if self . closed : raise Runtime Error ( \"Client cannot be used after its sockets have been closed\" ) subheader = subheader if subheader is not None else { } if not isinstance ( code , basestring ) : raise Type Error ( \"code must be text, not %s\" % type ( code ) ) if not isinstance ( subheader , dict ) : raise Type Error ( \"subheader must be dict, not %s\" % type ( subheader ) ) content = dict ( code = code , silent = bool ( silent ) , user variables = [ ] , user expressions = { } ) msg = self . session . send ( socket , \"execute request\" , content = content , ident = ident , subheader = subheader ) msg id = msg [ 'header' ] [ 'msg id' ] self . outstanding . add ( msg id ) if ident : if isinstance ( ident , list ) : ident = ident [ - 1 ] if ident in self . engines . values ( ) : self . outstanding dict [ ident ] . add ( msg id ) self . history . append ( msg id ) self . metadata [ msg id ] [ 'submitted' ] = datetime . now ( ) return msg", "predictions": ["sends an email on this client ."], "references": ["construct and send an execute request via a socket ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 6016, "code": "def opcode set ( * names ) : s = set ( ) for name in names : try : s . add ( opcode ( name ) ) except Key Error : pass return s", "predictions": ["return all the opcode in the ( ."], "references": ["return a set of opcodes by the names in names ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 6017, "code": "def get byte parser ( self ) : if not self . byte parser : self . byte parser = Byte Parser ( text = self . text , filename = self . filename ) return self . byte parser", "predictions": ["get the byte array for this object ."], "references": ["create a byteparser on demand ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 6018, "code": "def first line ( self , line ) : rng = self . multiline . get ( line ) if rng : first line = rng [ 0 ] else : first line = line return first line", "predictions": ["return the first element in this sequence ."], "references": ["return the first line number of the statement including line ."], "bleu": 0.21690743377623947, "rouge_l": 0.4093959731543625}
{"id": 6019, "code": "def block stack repr ( self , block stack ) : blocks = \", \" . join ( [ \"(%s, %r)\" % ( dis . opname [ b [ 0 ] ] , b [ 1 ] ) for b in block stack ] ) return \"[\" + blocks + \"]\"", "predictions": ["a block replacement for the block ."], "references": ["get a string version of block_stack for debugging ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 6020, "code": "def validate chunks ( self , chunks ) : starts = set ( [ ch . byte for ch in chunks ] ) for ch in chunks : assert all ( [ ( ex in starts or ex < 0 ) for ex in ch . exits ] )", "predictions": ["check if the supplied semantic semantic style is valid ."], "references": ["validate the rule that chunks have a single entrance ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 6021, "code": "def options ( self , parser , env ) : super ( Coverage , self ) . options ( parser , env ) parser . add option ( \"--cover-package\" , action = \"append\" , default = env . get ( 'NOSE COVER PACKAGE' ) , metavar = \"PACKAGE\" , dest = \"cover packages\" , help = \"Restrict coverage output to selected packages \" \"[NOSE COVER PACKAGE]\" ) parser . add option ( \"--cover-erase\" , action = \"store true\" , default = env . get ( 'NOSE COVER ERASE' ) , dest = \"cover erase\" , help = \"Erase previously collected coverage \" \"statistics before run\" ) parser . add option ( \"--cover-tests\" , action = \"store true\" , dest = \"cover tests\" , default = env . get ( 'NOSE COVER TESTS' ) , help = \"Include test modules in coverage report \" \"[NOSE COVER TESTS]\" ) parser . add option ( \"--cover-min-percentage\" , action = \"store\" , dest = \"cover min percentage\" , default = env . get ( 'NOSE COVER MIN PERCENTAGE' ) , help = \"Minimum percentage of coverage for tests\" \"to pass [NOSE COVER MIN PERCENTAGE]\" ) parser . add option ( \"--cover-inclusive\" , action = \"store true\" , dest = \"cover inclusive\" , default = env . get ( 'NOSE COVER INCLUSIVE' ) , help = \"Include all python files under working \" \"directory in coverage report.  Useful for \" \"discovering holes in test coverage if not all \" \"files are imported by the test suite. \" \"[NOSE COVER INCLUSIVE]\" ) parser . add option ( \"--cover-html\" , action = \"store true\" , default = env . get ( 'NOSE COVER HTML' ) , dest = 'cover html' , help = \"Produce HTML coverage information\" ) parser . add option ( '--cover-html-dir' , action = 'store' , default = env . get ( 'NOSE COVER HTML DIR' , 'cover' ) , dest = 'cover html dir' , metavar = 'DIR' , help = 'Produce HTML coverage information in dir' ) parser . add option ( \"--cover-branches\" , action = \"store true\" , default = env . get ( 'NOSE COVER BRANCHES' ) , dest = \"cover branches\" , help = \"Include branch coverage in coverage report \" \"[NOSE COVER BRANCHES]\" ) parser . add option ( \"--cover-xml\" , action = \"store true\" , default = env . get ( 'NOSE COVER XML' ) , dest = \"cover xml\" , help = \"Produce XML coverage information\" ) parser . add option ( \"--cover-xml-file\" , action = \"store\" , default = env . get ( 'NOSE COVER XML FILE' , 'coverage.xml' ) , dest = \"cover xml file\" , metavar = \"FILE\" , help = \"Produce XML coverage information in file\" )", "predictions": ["creates a new ( object ."], "references": ["add options to command line ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 6022, "code": "def begin ( self ) : log . debug ( \"Coverage begin\" ) self . skip Modules = sys . modules . keys ( ) [ : ] if self . cover Erase : log . debug ( \"Clearing previously collected coverage statistics\" ) self . cover Instance . combine ( ) self . cover Instance . erase ( ) self . cover Instance . exclude ( '#pragma[: ]+[n N][o O] [c C][o O][v V][e E][r R]' ) self . cover Instance . load ( ) self . cover Instance . start ( )", "predictions": ["begin a cover and fill the cover cover ."], "references": ["begin recording coverage information ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 6023, "code": "def report ( self , stream ) : log . debug ( \"Coverage report\" ) self . cover Instance . stop ( ) self . cover Instance . combine ( ) self . cover Instance . save ( ) modules = [ module for name , module in sys . modules . items ( ) if self . want Module Coverage ( name , module ) ] log . debug ( \"Coverage report will cover modules: %s\" , modules ) self . cover Instance . report ( modules , file = stream ) if self . cover Html Dir : log . debug ( \"Generating HTML coverage report\" ) self . cover Instance . html report ( modules , self . cover Html Dir ) if self . cover Xml File : log . debug ( \"Generating XML coverage report\" ) self . cover Instance . xml report ( modules , self . cover Xml File ) if self . cover Min Percentage : f = String IO . String IO ( ) self . cover Instance . report ( modules , file = f ) m = re . search ( r'-------\\s\\w+\\s+\\d+\\s+\\d+\\s+(\\d+)%\\s+\\d*\\s{0,1}$' , f . getvalue ( ) ) if m : percentage = int ( m . groups ( ) [ 0 ] ) if percentage < self . cover Min Percentage : log . error ( 'TOTAL Coverage did not reach minimum ' 'required: %d%%' % self . cover Min Percentage ) sys . exit ( 1 ) else : log . error ( \"No total percentage was found in coverage output, \" \"something went wrong.\" )", "predictions": ["report all necessary modules and exit including the necessary coverage ."], "references": ["output code coverage report ."], "bleu": 0.1354599427337814, "rouge_l": 0.2681318681318681}
{"id": 6024, "code": "def open with auth ( url ) : scheme , netloc , path , params , query , frag = urlparse . urlparse ( url ) if netloc . endswith ( ':' ) : raise httplib . Invalid URL ( \"nonnumeric port: ''\" ) if scheme in ( 'http' , 'https' ) : auth , host = urllib2 . splituser ( netloc ) else : auth = None if auth : auth = \"Basic \" + encode auth ( auth ) new url = urlparse . urlunparse ( ( scheme , host , path , params , query , frag ) ) request = urllib2 . Request ( new url ) request . add header ( \"Authorization\" , auth ) else : request = urllib2 . Request ( url ) request . add header ( 'User-Agent' , user agent ) fp = urllib2 . urlopen ( request ) if auth : s2 , h2 , path2 , param2 , query2 , frag2 = urlparse . urlparse ( fp . url ) if s2 == scheme and h2 == host : fp . url = urlparse . urlunparse ( ( s2 , netloc , path2 , param2 , query2 , frag2 ) ) return fp", "predictions": ["open an auth if the url is not a new scheme ."], "references": ["open a urllib2 request handling http authentication"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 6025, "code": "def get parent ( obj ) : names = obj . qualname . split ( '.' ) [ : - 1 ] if '<locals>' in names : raise Value Error ( 'cannot get parent from locals object.' ) module = sys . modules [ obj . module ] parent = module while names : parent = getattr ( parent , names . pop ( 0 ) ) return parent", "predictions": ["get a reference to the tree ."], "references": ["get parent from obj ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 6026, "code": "def render template ( content , context ) : rendered = Template ( content ) . render ( Context ( context ) ) return rendered", "predictions": ["renders a template with the given content ."], "references": ["renders context aware template"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 6027, "code": "def configure ( self , options , conf ) : self . conf = conf if not options . capture : self . enabled = False", "predictions": ["configures the latest configuration ."], "references": ["configure plugin . plugin is enabled by default ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 6028, "code": "def format Error ( self , test , err ) : test . captured Output = output = self . buffer self . buf = None if not output : return err ec , ev , tb = err return ( ec , self . add Capture To Err ( ev , output ) , tb )", "predictions": ["adds an error to the method ."], "references": ["add captured output to error report ."], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 6029, "code": "def split By ( data , num ) : return [ data [ i : i + num ] for i in range ( 0 , len ( data ) , num ) ]", "predictions": ["split the list by commas ."], "references": ["turn a list to list of list"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 6030, "code": "def hex to rgb ( color ) : if color . startswith ( '#' ) : color = color [ 1 : ] if len ( color ) == 3 : color = '' . join ( [ c * 2 for c in color ] ) if len ( color ) != 6 : return False try : r = int ( color [ : 2 ] , 16 ) g = int ( color [ 2 : 4 ] , 16 ) b = int ( color [ 4 : ] , 16 ) except Value Error : return False else : return r , g , b", "predictions": ["create a hex from the embedded color ."], "references": ["convert a hex color to rgb integer tuple ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 6031, "code": "def handle complete reply ( self , rep ) : self . log . debug ( \"complete: %s\" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'complete' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : matches = rep [ 'content' ] [ 'matches' ] text = rep [ 'content' ] [ 'matched text' ] offset = len ( text ) if len ( matches ) > 1 and matches [ 0 ] [ : offset ] == text : parts = re . split ( r'[./\\\\]' , text ) sep count = len ( parts ) - 1 if sep count : chop length = sum ( map ( len , parts [ : sep count ] ) ) + sep count matches = [ match [ chop length : ] for match in matches ] offset -= chop length cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = offset ) self . complete with items ( cursor , matches )", "predictions": ["handle a reply to the complete - complete reply with the new name ."], "references": ["reimplemented to support ipython s improved completion machinery ."], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 6032, "code": "def handle execute reply ( self , msg ) : msg id = msg [ 'parent header' ] . get ( 'msg id' ) info = self . request info [ 'execute' ] . get ( msg id ) if info and info . kind == 'prompt' : number = msg [ 'content' ] [ 'execution count' ] + 1 self . show interpreter prompt ( number ) self . request info [ 'execute' ] . pop ( msg id ) else : super ( I Python Widget , self ) . handle execute reply ( msg )", "predictions": ["now we need to hide an existing reply ."], "references": ["reimplemented to support prompt requests ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 6033, "code": "def handle pyout ( self , msg ) : self . log . debug ( \"pyout: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : content = msg [ 'content' ] prompt number = content . get ( 'execution count' , 0 ) data = content [ 'data' ] if data . has key ( 'text/html' ) : self . append plain text ( self . output sep , True ) self . append html ( self . make out prompt ( prompt number ) , True ) html = data [ 'text/html' ] self . append plain text ( '\\n' , True ) self . append html ( html + self . output sep2 , True ) elif data . has key ( 'text/plain' ) : self . append plain text ( self . output sep , True ) self . append html ( self . make out prompt ( prompt number ) , True ) text = data [ 'text/plain' ] if \"\\n\" in text and not self . output sep . endswith ( \"\\n\" ) : self . append plain text ( '\\n' , True ) self . append plain text ( text + self . output sep2 , True )", "predictions": ["calling this method ! provide default ( style ."], "references": ["reimplemented for ipython - style display hook ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 6034, "code": "def handle display data ( self , msg ) : self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : source = msg [ 'content' ] [ 'source' ] data = msg [ 'content' ] [ 'data' ] metadata = msg [ 'content' ] [ 'metadata' ] if data . has key ( 'text/html' ) : html = data [ 'text/html' ] self . append html ( html , True ) elif data . has key ( 'text/plain' ) : text = data [ 'text/plain' ] self . append plain text ( text , True ) self . append plain text ( u'\\n' , True )", "predictions": ["displays a ( to the supplied ( ."], "references": ["the base handler for the display_data message ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 6035, "code": "def started channels ( self ) : super ( I Python Widget , self ) . started channels ( ) self . load guiref magic ( ) self . kernel manager . shell channel . history ( hist access type = 'tail' , n = 1000 )", "predictions": ["create a new started channels ."], "references": ["reimplemented to make a history request and load %guiref ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 6036, "code": "def execute file ( self , path , hidden = False ) : if sys . platform == 'win32' : path = os . path . normpath ( path ) . replace ( '\\\\' , '/' ) if ' ' in path or \"'\" in path or '\"' in path : path = '\"%s\"' % path . replace ( '\"' , '\\\\\"' ) self . execute ( '%%run %s' % path , hidden = hidden )", "predictions": ["takes a file or a path and passes it to the ui ."], "references": ["reimplemented to use the run magic ."], "bleu": 0.1135935489027116, "rouge_l": 0.31715771230502604}
{"id": 6037, "code": "def complete ( self ) : text = '' msg id = self . kernel manager . shell channel . complete ( text , self . get input buffer cursor line ( ) , self . get input buffer cursor column ( ) , self . input buffer ) pos = self . get cursor ( ) . position ( ) info = self . Completion Request ( msg id , pos ) self . request info [ 'complete' ] = info", "predictions": ["completes with a new request ."], "references": ["reimplemented to support ipython s improved completion machinery ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 6038, "code": "def process execute error ( self , msg ) : content = msg [ 'content' ] traceback = '\\n' . join ( content [ 'traceback' ] ) + '\\n' if False : traceback = traceback . replace ( ' ' , '&nbsp;' ) traceback = traceback . replace ( '\\n' , '<br/>' ) ename = content [ 'ename' ] ename styled = '<span class=\"error\">%s</span>' % ename traceback = traceback . replace ( ename , ename styled ) self . append html ( traceback ) else : self . append plain text ( traceback )", "predictions": ["now we get the best best ( ."], "references": ["reimplemented for ipython - style traceback formatting ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 6039, "code": "def process execute payload ( self , item ) : handler = self . payload handlers . get ( item [ 'source' ] ) if handler is None : return False else : handler ( item ) return True", "predictions": ["invokes the payload method of this class ."], "references": ["reimplemented to dispatch payloads to handler methods ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 6040, "code": "def show interpreter prompt ( self , number = None ) : if number is None : msg id = self . kernel manager . shell channel . execute ( '' , silent = True ) info = self . Execution Request ( msg id , 'prompt' ) self . request info [ 'execute' ] [ msg id ] = info return self . prompt sep = self . input sep self . show prompt ( self . make in prompt ( number ) , html = True ) block = self . control . document ( ) . last Block ( ) length = len ( self . prompt ) self . previous prompt obj = self . Prompt Block ( block , length , number ) self . set continuation prompt ( self . make continuation prompt ( self . prompt ) , html = True )", "predictions": ["displays the user to select an html shell ."], "references": ["reimplemented for ipython - style prompts ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 6041, "code": "def show interpreter prompt for reply ( self , msg ) : content = msg [ 'content' ] if content [ 'status' ] == 'aborted' : if self . previous prompt obj : previous prompt number = self . previous prompt obj . number else : previous prompt number = 0 else : previous prompt number = content [ 'execution count' ] if self . previous prompt obj and self . previous prompt obj . number != previous prompt number : block = self . previous prompt obj . block if block . is Valid ( ) and block . text ( ) : cursor = Qt Gui . Q Text Cursor ( block ) cursor . move Position ( Qt Gui . Q Text Cursor . Right , Qt Gui . Q Text Cursor . Keep Anchor , self . previous prompt obj . length ) prompt = self . make in prompt ( previous prompt number ) self . prompt = self . insert html fetching plain text ( cursor , prompt ) self . highlighter . rehighlight Block ( cursor . block ( ) ) self . previous prompt obj = None self . show interpreter prompt ( previous prompt number + 1 )", "predictions": ["shortcut for the interpreter to show in the ( ."], "references": ["reimplemented for ipython - style prompts ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 6042, "code": "def make in prompt ( self , number ) : try : body = self . in prompt % number except Type Error : body = self . in prompt return '<span class=\"in-prompt\">%s</span>' % body", "predictions": ["makes the given prompt ."], "references": ["given a prompt number returns an html in prompt ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 6043, "code": "def style sheet changed ( self ) : self . set Style Sheet ( self . style sheet ) if self . control is not None : self . control . document ( ) . set Default Style Sheet ( self . style sheet ) bg color = self . control . palette ( ) . window ( ) . color ( ) self . ansi processor . set background color ( bg color ) if self . page control is not None : self . page control . document ( ) . set Default Style Sheet ( self . style sheet )", "predictions": ["save the sheet for a specific style ."], "references": ["set the style sheets of the underlying widgets ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 6044, "code": "def syntax style changed ( self ) : if self . highlighter is None : return if self . syntax style : self . highlighter . set style ( self . syntax style ) else : self . highlighter . set style sheet ( self . style sheet )", "predictions": ["returns the changed sheet ."], "references": ["set the style for the syntax highlighter ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 6045, "code": "def virtual memory ( ) : mem = psutil bsd . get virtual mem ( ) total , free , active , inactive , wired , cached , buffers , shared = mem avail = inactive + cached + free used = active + wired + cached percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free , active , inactive , buffers , cached , shared , wired )", "predictions": ["restores the virtual memory as a new namedtuple ."], "references": ["system virtual memory as a namedutple ."], "bleu": 0.4111336169005197, "rouge_l": 0.639412997903564}
{"id": 6046, "code": "def get system cpu times ( ) : user , nice , system , idle , irq = psutil bsd . get system cpu times ( ) return cputimes ntuple ( user , nice , system , idle , irq )", "predictions": ["gets the system cpu times ."], "references": ["return system per - cpu times as a named tuple"], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 6047, "code": "def get system per cpu times ( ) : ret = [ ] for cpu t in psutil bsd . get system per cpu times ( ) : user , nice , system , idle , irq = cpu t item = cputimes ntuple ( user , nice , system , idle , irq ) ret . append ( item ) return ret", "predictions": ["gets the value of the system per time per second ."], "references": ["return system cpu times as a named tuple"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 6048, "code": "def get process uids ( self ) : real , effective , saved = psutil bsd . get process uids ( self . pid ) return nt uids ( real , effective , saved )", "predictions": ["this method will be called on each thread ."], "references": ["return real effective and saved user ids ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 6049, "code": "def get process gids ( self ) : real , effective , saved = psutil bsd . get process gids ( self . pid ) return nt gids ( real , effective , saved )", "predictions": ["this method will be called to determine what the process was processed ."], "references": ["return real effective and saved group ids ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 6050, "code": "def get memory info ( self ) : rss , vms = psutil bsd . get process memory info ( self . pid ) [ : 2 ] return nt meminfo ( rss , vms )", "predictions": ["get information about the line used by the line ."], "references": ["return a tuple with the process rss and vms size ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 6051, "code": "def get process threads ( self ) : rawlist = psutil bsd . get process threads ( self . pid ) retlist = [ ] for thread id , utime , stime in rawlist : ntuple = nt thread ( thread id , utime , stime ) retlist . append ( ntuple ) return retlist", "predictions": ["stack all the repr of the process ."], "references": ["return the number of threads belonging to the process ."], "bleu": 0.2572506957482676, "rouge_l": 0.5446428571428571}
{"id": 6052, "code": "def get open files ( self ) : if hasattr ( psutil bsd , \"get process open files\" ) : rawlist = psutil bsd . get process open files ( self . pid ) return [ nt openfile ( path , fd ) for path , fd in rawlist ] else : lsof = psposix . Lsof Parser ( self . pid , self . process name ) return lsof . get process open files ( )", "predictions": ["finds the chunks of the local all ( . ) for the given all ( . ) ."], "references": ["return files opened by process as a list of namedtuples ."], "bleu": 0.07535838128770536, "rouge_l": 0.14420803782505912}
{"id": 6053, "code": "def num cpus darwin ( ) : p = subprocess . Popen ( [ 'sysctl' , '-n' , 'hw.ncpu' ] , stdout = subprocess . PIPE ) return p . stdout . read ( )", "predictions": ["get the command line number of ( i . e . , it ' s a full parser parser parser parser parser parser . it does not include the command ."], "references": ["return the number of active cpus on a darwin system ."], "bleu": 0.06106432774355542, "rouge_l": 0.26046114432109313}
{"id": 6054, "code": "def fetchone ( self ) : self . check executed ( ) r = self . fetch row ( 1 ) if not r : self . warning check ( ) return None self . rownumber = self . rownumber + 1 return r [ 0 ]", "predictions": ["call this to . this decorator is only used when drawing the tensors ."], "references": ["fetches a single row from the cursor ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 6055, "code": "def fetchall ( self ) : self . check executed ( ) r = self . fetch row ( 0 ) self . rownumber = self . rownumber + len ( r ) self . warning check ( ) return r", "predictions": ["run a method on this object ."], "references": ["fetchs all available rows from the cursor ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 6056, "code": "def connect ( com , peers , tree , pub url , root id ) : com . connect ( peers , tree , pub url , root id )", "predictions": ["open a new url to the specified ) and open the specified url ."], "references": ["this function will be called on the engines"], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 6057, "code": "def reads json ( s , * * kwargs ) : nbf , minor , d = parse json ( s , * * kwargs ) if nbf == 1 : nb = v1 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 1 ) elif nbf == 2 : nb = v2 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 2 ) elif nbf == 3 : nb = v3 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 3 , orig minor = minor ) else : raise NB Format Error ( 'Unsupported JSON nbformat version: %i' % nbf ) return nb", "predictions": ["helper method for converting parent parent colors to postgres postgres notebook ."], "references": ["read a json notebook from a string and return the notebooknode object ."], "bleu": 0.10579369505074822, "rouge_l": 0.15885416666666669}
{"id": 6058, "code": "def reads py ( s , * * kwargs ) : nbf , nbm , s = parse py ( s , * * kwargs ) if nbf == 2 : nb = v2 . to notebook py ( s , * * kwargs ) elif nbf == 3 : nb = v3 . to notebook py ( s , * * kwargs ) else : raise NB Format Error ( 'Unsupported PY nbformat version: %i' % nbf ) return nb", "predictions": ["return a flock(2) for a slice of the slice without modifying the notebook ."], "references": ["read a . py notebook from a string and return the notebooknode object ."], "bleu": 0.12090340630072073, "rouge_l": 0.2857142857142857}
{"id": 6059, "code": "def convert to metadata ( ) : import glob for fname in glob . glob ( '*.ipynb' ) : print ( 'Converting file:' , fname ) with open ( fname , 'r' ) as f : nb = read ( f , u'json' ) md = new metadata ( ) if u'name' in nb : md . name = nb . name del nb [ u'name' ] nb . metadata = md with open ( fname , 'w' ) as f : write ( nb , f , u'json' )", "predictions": ["convert self - 1 self - : 1 . 2 . 2 . 1 . 2 . 2 . 1 . 2 . 1 . 2 . 1 . 1 . 2 . 2 . 1 . 1 . 2 . 1 . 2 . 1 . 2 . 1"], "references": ["convert to a notebook having notebook metadata ."], "bleu": 0.026594139297659906, "rouge_l": 0.07932379713914176}
{"id": 6060, "code": "def want Function ( self , function ) : try : if hasattr ( function , 'compat func name' ) : funcname = function . compat func name else : funcname = function . name except Attribute Error : return False declared = getattr ( function , ' test ' , None ) if declared is not None : wanted = declared else : wanted = not funcname . startswith ( ' ' ) and self . matches ( funcname ) plug wants = self . plugins . want Function ( function ) if plug wants is not None : wanted = plug wants log . debug ( \"want Function %s? %s\" , function , wanted ) return wanted", "predictions": ["add a function to the specified test function ."], "references": ["is the function a test function?"], "bleu": 0.17747405280050263, "rouge_l": 0.27664399092970515}
{"id": 6061, "code": "def want Method ( self , method ) : try : method name = method . name except Attribute Error : return False if method name . startswith ( ' ' ) : return False declared = getattr ( method , ' test ' , None ) if declared is not None : wanted = declared else : wanted = self . matches ( method name ) plug wants = self . plugins . want Method ( method ) if plug wants is not None : wanted = plug wants log . debug ( \"want Method %s? %s\" , method , wanted ) return wanted", "predictions": ["split num num from an api call ."], "references": ["is the method a test method?"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 6062, "code": "def list command pydb ( self , arg ) : filename , first , last = Old Pdb . parse list cmd ( self , arg ) if filename is not None : self . print list lines ( filename , first , last )", "predictions": ["this method returns a hex list of the ( ."], "references": ["list command to use if we have a newer pydb installed"], "bleu": 0.12623203108004888, "rouge_l": 0.09442724458204334}
{"id": 6063, "code": "def do pdef ( self , arg ) : namespaces = [ ( 'Locals' , self . curframe . f locals ) , ( 'Globals' , self . curframe . f globals ) ] self . shell . find line magic ( 'pdef' ) ( arg , namespaces = namespaces )", "predictions": ["creates a new ( ."], "references": ["the debugger interface to magic_pdef"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 6064, "code": "def conversion factor ( from symbol , to symbol , date ) : from currency = Currency . objects . get ( symbol = from symbol ) try : from currency price = Currency Price . objects . get ( currency = from currency , date = date ) . mid price except Currency Price . Does Not Exist : print \"Cannot fetch prices for %s on %s\" % ( str ( from currency ) , str ( date ) ) return None to currency = Currency . objects . get ( symbol = to symbol ) try : to currency price = Currency Price . objects . get ( currency = to currency , date = date ) . mid price except Currency Price . Does Not Exist : print \"Cannot fetch prices for %s on %s\" % ( str ( to currency ) , str ( date ) ) return None return to currency price / from currency price", "predictions": ["creates a execute execute a execute ."], "references": ["generates a multiplying factor used to convert two currencies"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 6065, "code": "def convert currency ( from symbol , to symbol , value , date ) : if from symbol == to symbol : return value factor = conversion factor ( from symbol , to symbol , date ) if type ( value ) == float : output = value * float ( factor ) elif type ( value ) == Decimal : output = Decimal ( format ( value * factor , '.%sf' % str ( PRICE PRECISION ) ) ) elif type ( value ) in [ np . float16 , np . float32 , np . float64 , np . float128 , np . float ] : output = float ( value ) * float ( factor ) else : output = None return output", "predictions": ["handle a currency value ."], "references": ["converts an amount of money from one currency to another on a specified date ."], "bleu": 0.04393902429866315, "rouge_l": 0.18345864661654135}
{"id": 6066, "code": "def compute return ( self , start date , end date , rate = \"MID\" ) : if rate not in [ \"MID\" , \"ASK\" , \"BID\" ] : raise Value Error ( \"Unknown rate type (%s)- must be 'MID', 'ASK' or 'BID'\" % str ( rate ) ) if end date <= start date : raise Value Error ( \"End date must be on or after start date\" ) df = self . generate dataframe ( start date = start date , end date = end date ) start price = df . ix [ start date ] [ rate ] end price = df . ix [ end date ] [ rate ] currency return = ( end price / start price ) - 1.0 return currency return", "predictions": ["computes and returns a new message for this message ."], "references": ["compute the return of the currency between two dates"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 6067, "code": "def write connection file ( self ) : if os . path . basename ( self . connection file ) == self . connection file : cf = os . path . join ( self . profile dir . security dir , self . connection file ) else : cf = self . connection file write connection file ( cf , ip = self . ip , key = self . session . key , shell port = self . shell port , stdin port = self . stdin port , hb port = self . hb port , iopub port = self . iopub port ) self . full connection file = cf", "predictions": ["writes a ( or a channels : a ( without a n : the refers to the server : the server : has been made : - - 1 - n - 1 : has been set : has no effect for this channels , the channels will always be"], "references": ["write connection info to json file"], "bleu": 0.02403051755364481, "rouge_l": 0.04160982264665757}
{"id": 6068, "code": "def init heartbeat ( self ) : hb ctx = zmq . Context ( ) self . heartbeat = Heartbeat ( hb ctx , ( self . ip , self . hb port ) ) self . hb port = self . heartbeat . port self . log . debug ( \"Heartbeat REP Channel on port: %i\" % self . hb port ) self . heartbeat . start ( ) self . log . critical ( \"To connect another client to this kernel, use:\" )", "predictions": ["creates new telnetterminalserver object ."], "references": ["start the heart beating"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 6069, "code": "def log connection info ( self ) : basename = os . path . basename ( self . connection file ) if basename == self . connection file or os . path . dirname ( self . connection file ) == self . profile dir . security dir : tail = basename if self . profile != 'default' : tail += \" --profile %s\" % self . profile else : tail = self . connection file self . log . critical ( \"--existing %s\" , tail ) self . ports = dict ( shell = self . shell port , iopub = self . iopub port , stdin = self . stdin port , hb = self . hb port )", "predictions": ["writes a complete message object ."], "references": ["display connection info and store ports"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 6070, "code": "def init session ( self ) : default secure ( self . config ) self . session = Session ( config = self . config , username = u'kernel' )", "predictions": ["initialize the module object ."], "references": ["create our session object"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 6071, "code": "def init io ( self ) : if self . outstream class : outstream factory = import item ( str ( self . outstream class ) ) sys . stdout = outstream factory ( self . session , self . iopub socket , u'stdout' ) sys . stderr = outstream factory ( self . session , self . iopub socket , u'stderr' ) if self . displayhook class : displayhook factory = import item ( str ( self . displayhook class ) ) sys . displayhook = displayhook factory ( self . session , self . iopub socket )", "predictions": ["initialize the module . this method is called when the if we are not part of the class ."], "references": ["redirect input streams and set a display hook ."], "bleu": 0.06439931429457924, "rouge_l": 0.07634543178973717}
{"id": 6072, "code": "def init kernel ( self ) : kernel factory = import item ( str ( self . kernel class ) ) self . kernel = kernel factory ( config = self . config , session = self . session , shell socket = self . shell socket , iopub socket = self . iopub socket , stdin socket = self . stdin socket , log = self . log ) self . kernel . record ports ( self . ports )", "predictions": ["initialize and . instance ."], "references": ["create the kernel object itself"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 6073, "code": "def init connector ( self ) : self . using ssh = bool ( self . sshkey or self . sshserver ) if self . sshkey and not self . sshserver : self . sshserver = self . url . split ( '://' ) [ 1 ] . split ( ':' ) [ 0 ] if self . using ssh : if tunnel . try passwordless ssh ( self . sshserver , self . sshkey , self . paramiko ) : password = False else : password = getpass ( \"SSH Password for %s: \" % self . sshserver ) else : password = False def connect ( s , url ) : url = disambiguate url ( url , self . location ) if self . using ssh : self . log . debug ( \"Tunneling connection to %s via %s\" % ( url , self . sshserver ) ) return tunnel . tunnel connection ( s , url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) else : return s . connect ( url ) def maybe tunnel ( url ) : \"\"\"like connect, but don't complete the connection (for use by heartbeat)\"\"\" url = disambiguate url ( url , self . location ) if self . using ssh : self . log . debug ( \"Tunneling connection to %s via %s\" % ( url , self . sshserver ) ) url , tunnelobj = tunnel . open tunnel ( url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) return url return connect , maybe tunnel", "predictions": ["creates and initializes all msg objects ."], "references": ["construct connection function which handles tunnels ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 6074, "code": "def html to text ( content ) : text = None h2t = html2text . HTML2Text ( ) h2t . ignore links = False text = h2t . handle ( content ) return text", "predictions": ["converts the make to an infinite template ."], "references": ["converts html content to plain text"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 6075, "code": "def md to text ( content ) : text = None html = markdown . markdown ( content ) if html : text = html to text ( content ) return text", "predictions": ["converts the markdown to a html ."], "references": ["converts markdown content to text"], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 6076, "code": "def domain to fqdn ( domain , proto = None ) : from . generic import get site proto proto = proto or get site proto ( ) fdqn = '{proto}://{domain}' . format ( proto = proto , domain = domain ) return fdqn", "predictions": ["takes an email address and returns its appropriate representation ."], "references": ["returns a fully qualified app domain name"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 6077, "code": "def options ( self , parser , env = os . environ ) : super ( Nose Exclude , self ) . options ( parser , env ) env dirs = [ ] if 'NOSE EXCLUDE DIRS' in env : exclude dirs = env . get ( 'NOSE EXCLUDE DIRS' , '' ) env dirs . extend ( exclude dirs . split ( ';' ) ) parser . add option ( \"--exclude-dir\" , action = \"append\" , dest = \"exclude dirs\" , default = env dirs , help = ) parser . add option ( \"--exclude-dir-file\" , type = \"string\" , dest = \"exclude dir file\" , default = env . get ( 'NOSE EXCLUDE DIRS FILE' , False ) , help = )", "predictions": ["create a new command ."], "references": ["define the command line options for the plugin ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 6078, "code": "def configure ( self , options , conf ) : super ( Nose Exclude , self ) . configure ( options , conf ) self . exclude dirs = { } if options . exclude dir file : if not options . exclude dirs : options . exclude dirs = [ ] new dirs = self . load from file ( options . exclude dir file ) options . exclude dirs . extend ( new dirs ) if not options . exclude dirs : self . enabled = False return self . enabled = True root = os . getcwd ( ) log . debug ( 'cwd: %s' % root ) for exclude param in options . exclude dirs : for d in exclude param . split ( '\\n' ) : d = d . strip ( ) abs d = self . force to abspath ( d ) if abs d : self . exclude dirs [ abs d ] = True exclude str = \"excluding dirs: %s\" % \",\" . join ( self . exclude dirs . keys ( ) ) log . debug ( exclude str )", "predictions": ["get and = the configuration from the command line ."], "references": ["configure plugin based on command line options"], "bleu": 0.16590387014219712, "rouge_l": 0.24302788844621517}
{"id": 6079, "code": "def want Directory ( self , dirname ) : if dirname in self . exclude dirs : log . debug ( \"excluded: %s\" % dirname ) return False else : return None", "predictions": ["returns a directory for the get chain ."], "references": ["check if directory is eligible for test discovery"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 6080, "code": "def links to dynamic ( self , ext ) : libnames = dict . fromkeys ( [ lib . full name for lib in self . shlibs ] ) pkg = '.' . join ( ext . full name . split ( '.' ) [ : - 1 ] + [ '' ] ) for libname in ext . libraries : if pkg + libname in libnames : return True return False", "predictions": ["convert from an ( into a dynamic dynamic dynamic dynamic dynamic dynamic file ."], "references": ["return true if ext links to a dynamic lib in the same package"], "bleu": 0.11633270842295028, "rouge_l": 0.1491442542787286}
{"id": 6081, "code": "def append func ( self , func , * args , * * kwargs ) : wraped func = partial ( func , * args , * * kwargs ) self . append ( wraped func )", "predictions": ["appends a quoted - level argument to the list of arguments ."], "references": ["append func with given arguments and keywords ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 6082, "code": "def insert func ( self , index , func , * args , * * kwargs ) : wraped func = partial ( func , * args , * * kwargs ) self . insert ( index , wraped func )", "predictions": ["inserts a operation on a resource ."], "references": ["insert func with given arguments and keywords ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 6083, "code": "def construct parser ( magic func ) : kwds = getattr ( magic func , 'argcmd kwds' , { } ) if 'description' not in kwds : kwds [ 'description' ] = getattr ( magic func , ' doc ' , None ) arg name = real name ( magic func ) parser = Magic Argument Parser ( arg name , * * kwds ) group = None for deco in magic func . decorators [ : : - 1 ] : result = deco . add to parser ( parser , group ) if result is not None : group = result help text = parser . format help ( ) if help text . startswith ( 'usage: ' ) : help text = help text . replace ( 'usage: ' , '%' , 1 ) else : help text = '%' + help text magic func . doc = help text return parser", "predictions": ["construct the help parser from command line arguments ."], "references": ["construct an argument parser using the function decorations ."], "bleu": 0.17747405280050263, "rouge_l": 0.3333333333333333}
{"id": 6084, "code": "def real name ( magic func ) : magic name = magic func . name if magic name . startswith ( 'magic ' ) : magic name = magic name [ len ( 'magic ' ) : ] return getattr ( magic func , 'argcmd name' , magic name )", "predictions": ["utility method to get the real name of a function ."], "references": ["find the real name of the magic ."], "bleu": 0.33180774028439425, "rouge_l": 0.5417406749555951}
{"id": 6085, "code": "def add to parser ( self , parser , group ) : if group is not None : parser = group parser . add argument ( * self . args , * * self . kwds ) return None", "predictions": ["adds a single parser to the current built ."], "references": ["add this object s information to the parser ."], "bleu": 0.21105340631872635, "rouge_l": 0.3333333333333333}
{"id": 6086, "code": "def add to parser ( self , parser , group ) : return parser . add argument group ( * self . args , * * self . kwds )", "predictions": ["this method adds the results to the unit ."], "references": ["add this object s information to the parser ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 6087, "code": "def highlight Block ( self , string ) : if not self . highlighting on : return current block = self . current Block ( ) string = self . frontend . get block plain text ( current block ) if current block . contains ( self . frontend . prompt pos ) : prompt = self . frontend . prompt else : prompt = self . frontend . continuation prompt if string . startswith ( prompt ) : self . current offset = len ( prompt ) string = string [ len ( prompt ) : ] super ( Frontend Highlighter , self ) . highlight Block ( string )", "predictions": ["jdk jdk 1 . 5 . 0 ."], "references": ["highlight a block of text . reimplemented to highlight selectively ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 6088, "code": "def rehighlight Block ( self , block ) : old = self . highlighting on self . highlighting on = True super ( Frontend Highlighter , self ) . rehighlight Block ( block ) self . highlighting on = old", "predictions": ["this is called on each block ."], "references": ["reimplemented to temporarily enable highlighting if disabled ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 6089, "code": "def set Format ( self , start , count , format ) : start += self . current offset super ( Frontend Highlighter , self ) . set Format ( start , count , format )", "predictions": ["generate a simple calculation for the given number format ."], "references": ["reimplemented to highlight selectively ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 6090, "code": "def copy ( self ) : if self . page control is not None and self . page control . has Focus ( ) : self . page control . copy ( ) elif self . control . has Focus ( ) : text = self . control . text Cursor ( ) . selection ( ) . to Plain Text ( ) if text : lines = map ( self . transform prompt , text . splitlines ( ) ) text = '\\n' . join ( lines ) Qt Gui . Q Application . clipboard ( ) . set Text ( text ) else : self . log . debug ( \"frontend widget : unknown copy target\" )", "predictions": ["makes a shallow copy of this object ."], "references": ["copy the currently selected text to the clipboard removing prompts ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 6091, "code": "def context menu make ( self , pos ) : menu = super ( Frontend Widget , self ) . context menu make ( pos ) for before action in menu . actions ( ) : if before action . shortcut ( ) . matches ( Qt Gui . Q Key Sequence . Paste ) == Qt Gui . Q Key Sequence . Exact Match : menu . insert Action ( before action , self . copy raw action ) break return menu", "predictions": ["make a context suitable for all items of this exact ."], "references": ["reimplemented to add an action for raw copy ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 6092, "code": "def event filter console keypress ( self , event ) : key = event . key ( ) if self . control key down ( event . modifiers ( ) , include command = False ) : if key == Qt Core . Qt . Key C and self . executing : self . request interrupt kernel ( ) return True elif key == Qt Core . Qt . Key Period : self . request restart kernel ( ) return True elif not event . modifiers ( ) & Qt Core . Qt . Alt Modifier : if key == Qt Core . Qt . Key Backspace : col = self . get input buffer cursor column ( ) cursor = self . control . text Cursor ( ) if col > 3 and not cursor . has Selection ( ) : text = self . get input buffer cursor line ( ) [ : col ] if text . endswith ( '    ' ) and not text . strip ( ) : cursor . move Position ( Qt Gui . Q Text Cursor . Left , Qt Gui . Q Text Cursor . Keep Anchor , 4 ) cursor . remove Selected Text ( ) return True return super ( Frontend Widget , self ) . event filter console keypress ( event )", "predictions": ["indicates if a key is filter on the map . note : this may be \" filter \" . if the user has not been paused , the location of the returned value will be returned ."], "references": ["reimplemented for execution interruption and smart backspace ."], "bleu": 0.03259631698411773, "rouge_l": 0.05028854080791427}
{"id": 6093, "code": "def insert continuation prompt ( self , cursor ) : super ( Frontend Widget , self ) . insert continuation prompt ( cursor ) cursor . insert Text ( ' ' * self . input splitter . indent spaces )", "predictions": ["inserts a prompt at the specified cursor ."], "references": ["reimplemented for auto - indentation ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 6094, "code": "def handle complete reply ( self , rep ) : self . log . debug ( \"complete: %s\" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'complete' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : text = '.' . join ( self . get context ( ) ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = len ( text ) ) self . complete with items ( cursor , rep [ 'content' ] [ 'matches' ] )", "predictions": ["wraps any necessary colors ."], "references": ["handle replies for tab completion ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 6095, "code": "def handle execute reply ( self , msg ) : self . log . debug ( \"execute: %s\" , msg . get ( 'content' , '' ) ) msg id = msg [ 'parent header' ] [ 'msg id' ] info = self . request info [ 'execute' ] . get ( msg id ) self . reading = False if info and info . kind == 'user' and not self . hidden : self . kernel manager . sub channel . flush ( ) if self . ansi codes : self . ansi processor . reset sgr ( ) content = msg [ 'content' ] status = content [ 'status' ] if status == 'ok' : self . process execute ok ( msg ) elif status == 'error' : self . process execute error ( msg ) elif status == 'aborted' : self . process execute abort ( msg ) self . show interpreter prompt for reply ( msg ) self . executed . emit ( msg ) self . request info [ 'execute' ] . pop ( msg id ) elif info and info . kind == 'silent exec callback' and not self . hidden : self . handle exec callback ( msg ) self . request info [ 'execute' ] . pop ( msg id ) else : super ( Frontend Widget , self ) . handle execute reply ( msg )", "predictions": ["now we need to handle consistency group navigation ."], "references": ["handles replies for code execution ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 6096, "code": "def handle input request ( self , msg ) : self . log . debug ( \"input: %s\" , msg . get ( 'content' , '' ) ) if self . hidden : raise Runtime Error ( 'Request for raw input during hidden execution.' ) self . kernel manager . sub channel . flush ( ) def callback ( line ) : self . kernel manager . stdin channel . input ( line ) if self . reading : self . log . debug ( \"Got second input request, assuming first was interrupted.\" ) self . reading = False self . readline ( msg [ 'content' ] [ 'prompt' ] , callback = callback )", "predictions": ["now we need to handle any pending ( logic ."], "references": ["handle requests for raw_input ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 6097, "code": "def handle kernel died ( self , since last heartbeat ) : self . log . debug ( \"kernel died: %s\" , since last heartbeat ) if self . custom restart : self . custom restart kernel died . emit ( since last heartbeat ) else : message = 'The kernel heartbeat has been inactive for %.2f ' 'seconds. Do you want to restart the kernel? You may ' 'first want to check the network connection.' % since last heartbeat self . restart kernel ( message , now = True )", "predictions": ["custom custom restart handler for the kernel controller . note that this method calls the ( method in order to restart the controller ."], "references": ["handle the kernel s death by asking if the user wants to restart ."], "bleu": 0.09119675426861835, "rouge_l": 0.3315217391304348}
{"id": 6098, "code": "def handle object info reply ( self , rep ) : self . log . debug ( \"oinfo: %s\" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'call tip' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : content = rep [ 'content' ] if content . get ( 'ismagic' , False ) : call info , doc = None , None else : call info , doc = call tip ( content , format call = True ) if call info or doc : self . call tip widget . show call info ( call info , doc )", "predictions": ["handle a reply to the ( ."], "references": ["handle replies for call tips ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 6099, "code": "def handle pyout ( self , msg ) : self . log . debug ( \"pyout: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : text = msg [ 'content' ] [ 'data' ] self . append plain text ( text + '\\n' , before prompt = True )", "predictions": ["handle pyout ( . this method is called after the client has been made to handle the message ."], "references": ["handle display hook output ."], "bleu": 0.0712695567709093, "rouge_l": 0.18625954198473282}
{"id": 6100, "code": "def handle stream ( self , msg ) : self . log . debug ( \"stream: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : text = msg [ 'content' ] [ 'data' ] . expandtabs ( 8 ) self . append plain text ( text , before prompt = True ) self . control . move Cursor ( Qt Gui . Q Text Cursor . End )", "predictions": ["handle a ( stream ."], "references": ["handle stdout stderr and stdin ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 6101, "code": "def handle shutdown reply ( self , msg ) : self . log . debug ( \"shutdown: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and not self . is from this session ( msg ) : if self . local kernel : if not msg [ 'content' ] [ 'restart' ] : self . exit requested . emit ( self ) else : time . sleep ( 0.25 ) self . reset ( ) else : title = self . window ( ) . window Title ( ) if not msg [ 'content' ] [ 'restart' ] : reply = Qt Gui . Q Message Box . question ( self , title , \"Kernel has been shutdown permanently. \" \"Close the Console?\" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if reply == Qt Gui . Q Message Box . Yes : self . exit requested . emit ( self ) else : reply = Qt Gui . Q Message Box . question ( self , title , \"Kernel has been reset. Clear the Console?\" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if reply == Qt Gui . Q Message Box . Yes : time . sleep ( 0.25 ) self . reset ( )", "predictions": ["handle a potential shutdown operation ."], "references": ["handle shutdown signal only if from other console ."], "bleu": 0.1593301391270729, "rouge_l": 0.3860759493670886}
{"id": 6102, "code": "def restart kernel ( self , message , now = False ) : if self . custom restart : self . custom restart requested . emit ( ) elif self . kernel manager . has kernel : self . kernel manager . hb channel . pause ( ) if self . confirm restart : buttons = Qt Gui . Q Message Box . Yes | Qt Gui . Q Message Box . No result = Qt Gui . Q Message Box . question ( self , 'Restart kernel?' , message , buttons ) do restart = result == Qt Gui . Q Message Box . Yes else : do restart = True if do restart : try : self . kernel manager . restart kernel ( now = now ) except Runtime Error : self . append plain text ( 'Kernel started externally. ' 'Cannot restart.\\n' , before prompt = True ) else : self . reset ( ) else : self . kernel manager . hb channel . unpause ( ) else : self . append plain text ( 'Kernel process is either remote or ' 'unspecified. Cannot restart.\\n' , before prompt = True )", "predictions": ["restarts this running container on the custom kernel and removes the kernel ."], "references": ["attempts to restart the running kernel ."], "bleu": 0.14283632578659286, "rouge_l": 0.31715771230502604}
{"id": 6103, "code": "def call tip ( self ) : if not self . enable calltips : return False cursor = self . get cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left ) if cursor . document ( ) . character At ( cursor . position ( ) ) != '(' : return False context = self . get context ( cursor ) if not context : return False name = '.' . join ( context ) msg id = self . kernel manager . shell channel . object info ( name ) pos = self . get cursor ( ) . position ( ) self . request info [ 'call tip' ] = self . Call Tip Request ( msg id , pos ) return True", "predictions": ["calls this method with the given text and calls the bounds of this method ."], "references": ["shows a call tip if appropriate at the current cursor location ."], "bleu": 0.09103526405546068, "rouge_l": 0.15117719950433703}
{"id": 6104, "code": "def complete ( self ) : context = self . get context ( ) if context : msg id = self . kernel manager . shell channel . complete ( '.' . join ( context ) , self . get input buffer cursor line ( ) , self . get input buffer cursor column ( ) , self . input buffer ) pos = self . get cursor ( ) . position ( ) info = self . Completion Request ( msg id , pos ) self . request info [ 'complete' ] = info", "predictions": ["completes with ( and . ("], "references": ["performs completion at the current cursor location ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 6105, "code": "def process execute error ( self , msg ) : content = msg [ 'content' ] if content [ 'ename' ] == 'System Exit' : keepkernel = content [ 'evalue' ] == '-k' or content [ 'evalue' ] == 'True' self . keep kernel on exit = keepkernel self . exit requested . emit ( self ) else : traceback = '' . join ( content [ 'traceback' ] ) self . append plain text ( traceback )", "predictions": ["this method executes the caller ."], "references": ["process a reply for an execution request that resulted in an error ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 6106, "code": "def process execute ok ( self , msg ) : payload = msg [ 'content' ] [ 'payload' ] for item in payload : if not self . process execute payload ( item ) : warning = 'Warning: received unknown payload of type %s' print ( warning % repr ( item [ 'source' ] ) )", "predictions": ["execute the caller method of the setconfig class ."], "references": ["process a reply for a successful execution request ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 6107, "code": "def generate ( self , * arg , * * kw ) : for p , meth in self . plugins : result = None try : result = meth ( * arg , * * kw ) if result is not None : for r in result : yield r except ( Keyboard Interrupt , System Exit ) : raise except : exc = sys . exc info ( ) yield Failure ( * exc ) continue", "predictions": [". generate method by using the given parameters ."], "references": ["call all plugins yielding each item in each non - none result ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 6108, "code": "def simple ( self , * arg , * * kw ) : for p , meth in self . plugins : result = meth ( * arg , * * kw ) if result is not None : return result", "predictions": ["each operation in the list ."], "references": ["call all plugins returning the first non - none result ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 6109, "code": "def load Plugins ( self ) : from pkg resources import iter entry points loaded = { } for entry point , adapt in self . entry points : for ep in iter entry points ( entry point ) : if ep . name in loaded : continue loaded [ ep . name ] = True log . debug ( '%s load plugin %s' , self . class . name , ep ) try : plugcls = ep . load ( ) except Keyboard Interrupt : raise except Exception , e : warn ( \"Unable to load plugin %s: %s\" % ( ep , e ) , Runtime Warning ) continue if adapt : plug = adapt ( plugcls ( ) ) else : plug = plugcls ( ) self . add Plugin ( plug ) super ( Entry Point Plugin Manager , self ) . load Plugins ( )", "predictions": ["pressing pressing . remember the plugins and put them into the database ."], "references": ["load plugins by iterating the nose . plugins entry point ."], "bleu": 0.12011055432195765, "rouge_l": 0.2538141470180305}
{"id": 6110, "code": "def load Plugins ( self ) : from nose . plugins import builtin for plug in builtin . plugins : self . add Plugin ( plug ( ) ) super ( Builtin Plugin Manager , self ) . load Plugins ( )", "predictions": ["load webhook plugin configuration ."], "references": ["load plugins in nose . plugins . builtin"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 6111, "code": "def cleanup files ( self , bundle = False ) : logger . notify ( 'Cleaning up...' ) logger . indent += 2 for req in self . reqs to cleanup : req . remove temporary source ( ) remove dir = [ ] if self . pip has created build dir ( ) : remove dir . append ( self . build dir ) if bundle : remove dir . append ( self . src dir ) for dir in remove dir : if os . path . exists ( dir ) : logger . info ( 'Removing temporary dir %s...' % dir ) rmtree ( dir ) logger . indent -= 2", "predictions": ["then we will remove all pip ' re going to be called before the pip process ."], "references": ["clean up files remove builds ."], "bleu": 0.07994607499472013, "rouge_l": 0.19032761310452417}
{"id": 6112, "code": "def name ( self ) : name = self . platform impl . get process name ( ) if os . name == 'posix' : try : cmdline = self . cmdline except Access Denied : pass else : if cmdline : extended name = os . path . basename ( cmdline [ 0 ] ) if extended name . startswith ( name ) : name = extended name self . platform impl . process name = name return name", "predictions": ["get information about this object ."], "references": ["the process name ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 6113, "code": "def exe ( self ) : def guess it ( fallback ) : cmdline = self . cmdline if cmdline and hasattr ( os , 'access' ) and hasattr ( os , 'X OK' ) : exe = cmdline [ 0 ] rexe = os . path . realpath ( exe ) if os . path . isabs ( rexe ) and os . path . isfile ( rexe ) and os . access ( rexe , os . X OK ) : return exe if isinstance ( fallback , Access Denied ) : raise fallback return fallback try : exe = self . platform impl . get process exe ( ) except Access Denied : err = sys . exc info ( ) [ 1 ] return guess it ( fallback = err ) else : if not exe : try : exe = guess it ( fallback = exe ) except Access Denied : pass return exe", "predictions": ["append an append . the append is the first valid kwargs ."], "references": ["the process executable path . may also be an empty string ."], "bleu": 0.13065113298388567, "rouge_l": 0.16666666666666666}
{"id": 6114, "code": "def is running ( self ) : if self . gone : return False try : return self . create time == self . platform impl . get process create time ( ) except No Such Process : self . gone = True return False", "predictions": ["check if this process is func ."], "references": ["return whether this process is running ."], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 6115, "code": "def suspend ( self ) : if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) if hasattr ( self . platform impl , \"suspend process\" ) : self . platform impl . suspend process ( ) else : self . send signal ( signal . SIGSTOP )", "predictions": ["construct the process with the process log ."], "references": ["suspend process execution ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 6116, "code": "def resume ( self ) : if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) if hasattr ( self . platform impl , \"resume process\" ) : self . platform impl . resume process ( ) else : self . send signal ( signal . SIGCONT )", "predictions": ["real call to real len ."], "references": ["resume process execution ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 6117, "code": "def kill ( self ) : if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) if os . name == 'posix' : self . send signal ( signal . SIGKILL ) else : self . platform impl . kill process ( )", "predictions": ["add this process to the process log ."], "references": ["kill the current process ."], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 6118, "code": "def init transformers ( self ) : self . transformers = [ ] for transformer cls in default transformers : transformer cls ( shell = self . shell , prefilter manager = self , config = self . config )", "predictions": ["initialize the rsa class ."], "references": ["create the default transformers ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 6119, "code": "def register transformer ( self , transformer ) : if transformer not in self . transformers : self . transformers . append ( transformer ) self . sort transformers ( )", "predictions": ["highlight the specific transformer ."], "references": ["register a transformer instance ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 6120, "code": "def unregister transformer ( self , transformer ) : if transformer in self . transformers : self . transformers . remove ( transformer )", "predictions": ["unregister removes all instances of this class from the given transformer ."], "references": ["unregister a transformer instance ."], "bleu": 0.1235622127262679, "rouge_l": 0.38125000000000003}
{"id": 6121, "code": "def init checkers ( self ) : self . checkers = [ ] for checker in default checkers : checker ( shell = self . shell , prefilter manager = self , config = self . config )", "predictions": ["initialize the module . this method is called only from each super class ."], "references": ["create the default checkers ."], "bleu": 0.09782375748961449, "rouge_l": 0.23018867924528305}
{"id": 6122, "code": "def register checker ( self , checker ) : if checker not in self . checkers : self . checkers . append ( checker ) self . sort checkers ( )", "predictions": ["registers a ( ( default if available if available if no ( if available if not available if not . if this is set if the ( if the ( if not in the default if the ( if not in the ( if the ( if any if the"], "references": ["register a checker instance ."], "bleu": 0.026594139297659906, "rouge_l": 0.08531468531468532}
{"id": 6123, "code": "def unregister checker ( self , checker ) : if checker in self . checkers : self . checkers . remove ( checker )", "predictions": ["unregisters a menu . this is no longer needed ."], "references": ["unregister a checker instance ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 6124, "code": "def init handlers ( self ) : self . handlers = { } self . esc handlers = { } for handler in default handlers : handler ( shell = self . shell , prefilter manager = self , config = self . config )", "predictions": ["initialize the module . this method is called once for each modifiers of the archive ."], "references": ["create the default handlers ."], "bleu": 0.08513012360883544, "rouge_l": 0.21034482758620687}
{"id": 6125, "code": "def register handler ( self , name , handler , esc strings ) : self . handlers [ name ] = handler for esc str in esc strings : self . esc handlers [ esc str ] = handler", "predictions": ["registers a specific continuation continuation class ."], "references": ["register a handler instance by name with esc_strings ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 6126, "code": "def unregister handler ( self , name , handler , esc strings ) : try : del self . handlers [ name ] except Key Error : pass for esc str in esc strings : h = self . esc handlers . get ( esc str ) if h is handler : del self . esc handlers [ esc str ]", "predictions": ["removes a previously registered complete complete set of default objects ."], "references": ["unregister a handler instance by name with esc_strings ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 6127, "code": "def find handler ( self , line info ) : for checker in self . checkers : if checker . enabled : handler = checker . check ( line info ) if handler : return handler return self . get handler by name ( 'normal' )", "predictions": ["finds a execute execute or the execute method ."], "references": ["find a handler for the line_info by trying checkers ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 6128, "code": "def transform line ( self , line , continue prompt ) : for transformer in self . transformers : if transformer . enabled : line = transformer . transform ( line , continue prompt ) return line", "predictions": ["transforms an expandable input ."], "references": ["calls the enabled transformers in order of increasing priority ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 6129, "code": "def check ( self , line info ) : obj = self . shell . user ns . get ( line info . ifun , None ) if isinstance ( obj , I Py Autocall ) : obj . set ip ( self . shell ) return self . prefilter manager . get handler by name ( 'auto' ) else : return None", "predictions": [". operation on remote self ."], "references": ["instances of ipyautocall in user_ns get autocalled immediately"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 6130, "code": "def check ( self , line info ) : if line info . continue prompt and self . prefilter manager . multi line specials : if line info . esc == ESC MAGIC : return self . prefilter manager . get handler by name ( 'magic' ) else : return None", "predictions": ["handle correctness of the ( ( i . e . , the ( self , ( self , ( self , ( ( ( ( ( self , get , get , get , get , etc , self , get , etc , self , etc , etc ,"], "references": ["allow ! and !! in multi - line statements if multi_line_specials is on"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 6131, "code": "def check ( self , line info ) : head = line info . ifun . split ( '.' , 1 ) [ 0 ] if line info . ifun not in self . shell . alias manager or head not in self . shell . alias manager or is shadowed ( head , self . shell ) : return None return self . prefilter manager . get handler by name ( 'alias' )", "predictions": ["checks for dynamic this , returning the this , if you have a from it ."], "references": ["check if the initital identifier on the line is an alias ."], "bleu": 0.09147827112247602, "rouge_l": 0.14663461538461536}
{"id": 6132, "code": "def handle ( self , line info ) : line = line info . line continue prompt = line info . continue prompt if ( continue prompt and self . shell . autoindent and line . isspace ( ) and 0 < abs ( len ( line ) - self . shell . indent current nsp ) <= 2 ) : line = '' return line", "predictions": ["handles the this , but only if we ' re a , but we will extend the current , and prepare the , but we need to normalize the , but we know about to normalize it ."], "references": ["handle normal input lines . use as a template for handlers ."], "bleu": 0.03511091977922844, "rouge_l": 0.0882778581765557}
{"id": 6133, "code": "def handle ( self , line info ) : transformed = self . shell . alias manager . expand aliases ( line info . ifun , line info . the rest ) line out = '%sget ipython().system(%r)' % ( line info . pre whitespace , transformed ) return line out", "predictions": ["passes through the : minor self - arrive at the next self - blocking self - arrive at the bottom of the self - up to the list ."], "references": ["handle alias input lines ."], "bleu": 0.04175872565419194, "rouge_l": 0.06740331491712706}
{"id": 6134, "code": "def handle ( self , line info ) : magic handler = self . prefilter manager . get handler by name ( 'magic' ) line = line info . line if line . lstrip ( ) . startswith ( ESC SH CAP ) : new rest = line . lstrip ( ) [ 2 : ] line info . line = '%ssx %s' % ( ESC MAGIC , new rest ) line info . ifun = 'sx' line info . the rest = new rest return magic handler . handle ( line info ) else : cmd = line . lstrip ( ) . lstrip ( ESC SHELL ) line out = '%sget ipython().system(%r)' % ( line info . pre whitespace , cmd ) return line out", "predictions": ["handles about incoming , prepare prepare and setup ."], "references": ["execute the line in a shell empty return value"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 6135, "code": "def handle ( self , line info ) : ifun = line info . ifun the rest = line info . the rest cmd = '%sget ipython().magic(%r)' % ( line info . pre whitespace , ( ifun + \" \" + the rest ) ) return cmd", "predictions": ["locates ) calls for getting tip information ."], "references": ["execute magic functions ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 6136, "code": "def handle ( self , line info ) : line = line info . line ifun = line info . ifun the rest = line info . the rest pre = line info . pre esc = line info . esc continue prompt = line info . continue prompt obj = line info . ofind ( self . shell ) [ 'obj' ] if continue prompt : return line force auto = isinstance ( obj , I Py Autocall ) try : auto rewrite = obj . rewrite except Exception : auto rewrite = True if esc == ESC QUOTE : newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the rest . split ( ) ) ) elif esc == ESC QUOTE2 : newcmd = '%s(\"%s\")' % ( ifun , the rest ) elif esc == ESC PAREN : newcmd = '%s(%s)' % ( ifun , \",\" . join ( the rest . split ( ) ) ) else : if force auto : do rewrite = not the rest . startswith ( '(' ) else : if not the rest : do rewrite = ( self . shell . autocall >= 2 ) elif the rest . startswith ( '[' ) and hasattr ( obj , ' getitem ' ) : do rewrite = False else : do rewrite = True if do rewrite : if the rest . endswith ( ';' ) : newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the rest [ : - 1 ] ) else : newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the rest ) else : normal handler = self . prefilter manager . get handler by name ( 'normal' ) return normal handler . handle ( line info ) if auto rewrite : self . shell . auto rewrite input ( newcmd ) return newcmd", "predictions": ["method called when there is a normal operation for all information in the normal ."], "references": ["handle lines which can be auto - executed quoting if requested ."], "bleu": 0.08225964699966554, "rouge_l": 0.07558859975216851}
{"id": 6137, "code": "def enter Event ( self , event ) : super ( Call Tip Widget , self ) . enter Event ( event ) self . hide timer . stop ( )", "predictions": ["initializes the if the if it is a class ."], "references": ["reimplemented to cancel the hide timer ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 6138, "code": "def paint Event ( self , event ) : painter = Qt Gui . Q Style Painter ( self ) option = Qt Gui . Q Style Option Frame ( ) option . init From ( self ) painter . draw Primitive ( Qt Gui . Q Style . PE Panel Tip Label , option ) painter . end ( ) super ( Call Tip Widget , self ) . paint Event ( event )", "predictions": ["process the graphical representation of a single interface ."], "references": ["reimplemented to paint the background panel ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 6139, "code": "def show tip ( self , tip ) : text edit = self . text edit document = text edit . document ( ) cursor = text edit . text Cursor ( ) search pos = cursor . position ( ) - 1 self . start position , = self . find parenthesis ( search pos , forward = False ) if self . start position == - 1 : return False self . set Text ( tip ) self . resize ( self . size Hint ( ) ) padding = 3 cursor rect = text edit . cursor Rect ( cursor ) screen rect = Qt Gui . q App . desktop ( ) . screen Geometry ( text edit ) point = text edit . map To Global ( cursor rect . bottom Right ( ) ) point . set Y ( point . y ( ) + padding ) tip height = self . size ( ) . height ( ) tip width = self . size ( ) . width ( ) vertical = 'bottom' horizontal = 'Right' if point . y ( ) + tip height > screen rect . height ( ) : point = text edit . map To Global ( cursor rect . top Right ( ) ) if point . y ( ) - tip height < padding : if 2 * point . y ( ) < screen rect . height ( ) : vertical = 'bottom' else : vertical = 'top' else : vertical = 'top' if point . x ( ) + tip width > screen rect . width ( ) : point = text edit . map To Global ( cursor rect . top Right ( ) ) if point . x ( ) - tip width < padding : if 2 * point . x ( ) < screen rect . width ( ) : horizontal = 'Right' else : horizontal = 'Left' else : horizontal = 'Left' pos = getattr ( cursor rect , '%s%s' % ( vertical , horizontal ) ) point = text edit . map To Global ( pos ( ) ) if vertical == 'top' : point . set Y ( point . y ( ) - tip height - padding ) if horizontal == 'Left' : point . set X ( point . x ( ) - tip width - padding ) self . move ( point ) self . show ( ) return True", "predictions": ["show a trimmed - tokenizes : : : 1 . 2 . 3 . 3 ."], "references": ["attempts to show the specified tip at the current cursor location ."], "bleu": 0.08513012360883544, "rouge_l": 0.14663461538461536}
{"id": 6140, "code": "def cursor position changed ( self ) : cursor = self . text edit . text Cursor ( ) if cursor . position ( ) <= self . start position : self . hide ( ) else : position , commas = self . find parenthesis ( self . start position + 1 ) if position != - 1 : self . hide ( )", "predictions": ["sets the bson simple simple simple simple simple simple simple p and method to p the default simple simple simple simple simple simple simple simple simple simple simple simple simple simple simple simple simple simple simple call ."], "references": ["updates the tip based on user cursor movement ."], "bleu": 0.03511091977922844, "rouge_l": 0.09576138147566718}
{"id": 6141, "code": "def read ( * paths ) : with open ( os . path . join ( * paths ) , 'r' ) as file handler : return file handler . read ( )", "predictions": ["load the self - terminated list of self self ."], "references": ["build a file path from * paths * and return the contents ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 6142, "code": "def virtualenv no global ( ) : #this mirrors the logic in virtualenv.py for locating the no-global-site-packages.txt file site mod dir = os . path . dirname ( os . path . abspath ( site . file ) ) no global file = os . path . join ( site mod dir , 'no-global-site-packages.txt' ) if running under virtualenv ( ) and os . path . isfile ( no global file ) : return True", "predictions": ["checks if there are any running available for the specified site ."], "references": ["return true if in a venv and no system site packages ."], "bleu": 0.1235622127262679, "rouge_l": 0.25}
{"id": 6143, "code": "def default aliases ( ) : if os . name == 'posix' : default aliases = [ ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'mv' , 'mv -i' ) , ( 'rm' , 'rm -i' ) , ( 'cp' , 'cp -i' ) , ( 'cat' , 'cat' ) , ] if sys . platform . startswith ( 'linux' ) : ls aliases = [ ( 'ls' , 'ls -F --color' ) , ( 'll' , 'ls -F -o --color' ) , ( 'lf' , 'ls -F -o --color %l | grep ^-' ) , ( 'lk' , 'ls -F -o --color %l | grep ^l' ) , ( 'ldir' , 'ls -F -o --color %l | grep /$' ) , ( 'lx' , 'ls -F -o --color %l | grep ^-..x' ) , ] else : ls aliases = [ ( 'ls' , 'ls -F' ) , ( 'll' , 'ls -F -l' ) , ( 'lf' , 'ls -F -l %l | grep ^-' ) , ( 'lk' , 'ls -F -l %l | grep ^l' ) , ( 'ldir' , 'ls -F -l %l | grep /$' ) , ( 'lx' , 'ls -F -l %l | grep ^-..x' ) , ] default aliases = default aliases + ls aliases elif os . name in [ 'nt' , 'dos' ] : default aliases = [ ( 'ls' , 'dir /on' ) , ( 'ddir' , 'dir /ad /on' ) , ( 'ldir' , 'dir /ad /on' ) , ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'echo' , 'echo' ) , ( 'ren' , 'ren' ) , ( 'copy' , 'copy' ) , ] else : default aliases = [ ] return default aliases", "predictions": ["this is a utility method that creates a cleanup containing the cleanup of the given entity ."], "references": ["return list of shell aliases to auto - define ."], "bleu": 0.07994607499472013, "rouge_l": 0.15541401273885352}
{"id": 6144, "code": "def soft define alias ( self , name , cmd ) : try : self . define alias ( name , cmd ) except Alias Error , e : error ( \"Invalid alias: %s\" % e )", "predictions": ["creates a new power ."], "references": ["define an alias but don t raise on an aliaserror ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 6145, "code": "def validate alias ( self , name , cmd ) : if name in self . no alias : raise Invalid Alias Error ( \"The name %s can't be aliased \" \"because it is a keyword or builtin.\" % name ) if not ( isinstance ( cmd , basestring ) ) : raise Invalid Alias Error ( \"An alias command must be a string, \" \"got: %r\" % cmd ) nargs = cmd . count ( '%s' ) if nargs > 0 and cmd . find ( '%l' ) >= 0 : raise Invalid Alias Error ( 'The %s and %l specifiers are mutually ' 'exclusive in alias definitions.' ) return nargs", "predictions": ["validate if the given alias exists . if the supplied alias can be passed to the alias name will be set to the alias name ."], "references": ["validate an alias and return the its number of arguments ."], "bleu": 0.058697608930387266, "rouge_l": 0.23326959847036327}
{"id": 6146, "code": "def call alias ( self , alias , rest = '' ) : cmd = self . transform alias ( alias , rest ) try : self . shell . system ( cmd ) except : self . shell . showtraceback ( )", "predictions": ["shortcut for the specified command ."], "references": ["call an alias given its name and the rest of the line ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 6147, "code": "def transform alias ( self , alias , rest = '' ) : nargs , cmd = self . alias table [ alias ] if ' ' in cmd and os . path . isfile ( cmd ) : cmd = '\"%s\"' % cmd if cmd . find ( '%l' ) >= 0 : cmd = cmd . replace ( '%l' , rest ) rest = '' if nargs == 0 : cmd = '%s %s' % ( cmd , rest ) else : args = rest . split ( None , nargs ) if len ( args ) < nargs : raise Alias Error ( 'Alias <%s> requires %s arguments, %s given.' % ( alias , nargs , len ( args ) ) ) cmd = '%s %s' % ( cmd % tuple ( args [ : nargs ] ) , ' ' . join ( args [ nargs : ] ) ) return cmd", "predictions": ["transforms an object from the alias and returns the resulting alias ."], "references": ["transform alias to system command string ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 6148, "code": "def autohelp directive ( dirname , arguments , options , content , lineno , content offset , block text , state , state machine ) : config = Config ( parser Class = Opt Bucket , plugins = Builtin Plugin Manager ( ) ) parser = config . get Parser ( Test Program . usage ( ) ) rst = View List ( ) for line in parser . format help ( ) . split ( '\\n' ) : rst . append ( line , '<autodoc>' ) rst . append ( 'Options' , '<autodoc>' ) rst . append ( '-------' , '<autodoc>' ) rst . append ( '' , '<autodoc>' ) for opt in parser : rst . append ( opt . options ( ) , '<autodoc>' ) rst . append ( '   \\n' , '<autodoc>' ) rst . append ( '   ' + opt . help + '\\n' , '<autodoc>' ) rst . append ( '\\n' , '<autodoc>' ) node = nodes . section ( ) node . document = state . document surrounding title styles = state . memo . title styles surrounding section level = state . memo . section level state . memo . title styles = [ ] state . memo . section level = 0 state . nested parse ( rst , 0 , node , match titles = 1 ) state . memo . title styles = surrounding title styles state . memo . section level = surrounding section level return node . children", "predictions": ["creates a autohelp object for a directive ."], "references": ["produces rst from nose help"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 6149, "code": "def reset sgr ( self ) : self . intensity = 0 self . italic = False self . bold = False self . underline = False self . foreground color = None self . background color = None", "predictions": ["resets the model to the previous background state . this is used for example to be reused ."], "references": ["reset graphics attributs to their default values ."], "bleu": 0.07535838128770536, "rouge_l": 0.16531165311653115}
{"id": 6150, "code": "def split string ( self , string ) : self . actions = [ ] start = 0 last char = '\\n' if len ( string ) > 0 and string [ - 1 ] == '\\n' else None string = string [ : - 1 ] if last char is not None else string for match in ANSI OR SPECIAL PATTERN . finditer ( string ) : raw = string [ start : match . start ( ) ] substring = SPECIAL PATTERN . sub ( self . replace special , raw ) if substring or self . actions : yield substring self . actions = [ ] start = match . end ( ) groups = filter ( lambda x : x is not None , match . groups ( ) ) g0 = groups [ 0 ] if g0 == '\\a' : self . actions . append ( Beep Action ( 'beep' ) ) yield None self . actions = [ ] elif g0 == '\\r' : self . actions . append ( Carriage Return Action ( 'carriage-return' ) ) yield None self . actions = [ ] elif g0 == '\\b' : self . actions . append ( Back Space Action ( 'backspace' ) ) yield None self . actions = [ ] elif g0 == '\\n' or g0 == '\\r\\n' : self . actions . append ( New Line Action ( 'newline' ) ) yield g0 self . actions = [ ] else : params = [ param for param in groups [ 1 ] . split ( ';' ) if param ] if g0 . startswith ( '[' ) : try : params = map ( int , params ) except Value Error : pass else : self . set csi code ( groups [ 2 ] , params ) elif g0 . startswith ( ']' ) : self . set osc code ( params ) raw = string [ start : ] substring = SPECIAL PATTERN . sub ( self . replace special , raw ) if substring or self . actions : yield substring if last char is not None : self . actions . append ( New Line Action ( 'newline' ) ) yield last char", "predictions": ["splits string on character not escaped by commas ."], "references": ["yields substrings for which the same escape code applies ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 6151, "code": "def get format ( self ) : format = Qt Gui . Q Text Char Format ( ) qcolor = self . get color ( self . foreground color , self . intensity ) if qcolor is not None : format . set Foreground ( qcolor ) qcolor = self . get color ( self . background color , self . intensity ) if qcolor is not None : format . set Background ( qcolor ) if self . bold : format . set Font Weight ( Qt Gui . Q Font . Bold ) else : format . set Font Weight ( Qt Gui . Q Font . Normal ) format . set Font Italic ( self . italic ) format . set Font Underline ( self . underline ) return format", "predictions": ["get a format for this view ."], "references": ["returns a qtextcharformat that encodes the current style attributes ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 6152, "code": "def generate ( secret , age , * * payload ) : jti = str ( uuid . uuid1 ( ) ) if not payload : payload = { } payload [ 'exp' ] = int ( time . time ( ) + age ) payload [ 'jti' ] = jti return jwt . encode ( payload , decode secret ( secret ) )", "predictions": ["generates a jwt for use in hmac hmac within the payload ."], "references": ["generate a one - time jwt with an age in seconds"], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 6153, "code": "def mutex ( func ) : def wrapper ( * args , * * kwargs ) : \"\"\"Decorator Wrapper\"\"\" lock = args [ 0 ] . lock lock . acquire ( True ) try : return func ( * args , * * kwargs ) except : raise finally : lock . release ( ) return wrapper", "predictions": ["first upload argument . register and then acquire the wrapper around the wrapper ."], "references": ["use a thread lock on current method if self . lock is defined"], "bleu": 0.08839374326825923, "rouge_l": 0.0745721271393643}
{"id": 6154, "code": "def clean ( self ) : now = time . time ( ) for jwt in self . jwts . keys ( ) : if ( now - self . jwts [ jwt ] ) > ( self . age * 2 ) : del self . jwts [ jwt ]", "predictions": ["for each jwt in the jwt ."], "references": ["run by housekeeper thread"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 6155, "code": "def already used ( self , tok ) : if tok in self . jwts : return True self . jwts [ tok ] = time . time ( ) return False", "predictions": ["returns a latlng that is used to track this cipher ."], "references": ["has this jwt been used?"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 6156, "code": "def valid ( self , token ) : now = time . time ( ) if 'Bearer ' in token : token = token [ 7 : ] data = None for secret in self . secrets : try : data = jwt . decode ( token , secret ) break except jwt . Decode Error : continue except jwt . Expired Signature Error : raise Jwt Failed ( \"Jwt expired\" ) if not data : raise Jwt Failed ( \"Jwt cannot be decoded\" ) exp = data . get ( 'exp' ) if not exp : raise Jwt Failed ( \"Jwt missing expiration (exp)\" ) if now - exp > self . age : raise Jwt Failed ( \"Jwt bad expiration - greater than I want to accept\" ) jti = data . get ( 'jti' ) if not jti : raise Jwt Failed ( \"Jwt missing one-time id (jti)\" ) if self . already used ( jti ) : raise Jwt Failed ( \"Jwt re-use disallowed (jti={})\" . format ( jti ) ) return data", "predictions": ["makes a valid token from the jwt ."], "references": ["is this token valid?"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 6157, "code": "def write ( self , nb , fp , * * kwargs ) : return fp . write ( self . writes ( nb , * * kwargs ) )", "predictions": ["writes the object to this cacheheader ."], "references": ["write a notebook to a file like object"], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 6158, "code": "def can cut ( self ) : cursor = self . control . text Cursor ( ) return ( cursor . has Selection ( ) and self . in buffer ( cursor . anchor ( ) ) and self . in buffer ( cursor . position ( ) ) )", "predictions": ["trying to cut this cursor with a specific cursor ."], "references": ["returns whether text can be cut to the clipboard ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 6159, "code": "def can paste ( self ) : if self . control . text Interaction Flags ( ) & Qt Core . Qt . Text Editable : return bool ( Qt Gui . Q Application . clipboard ( ) . text ( ) ) return False", "predictions": ["check if this imageviewtouch can use a paste control text ."], "references": ["returns whether text can be pasted from the clipboard ."], "bleu": 0.1354599427337814, "rouge_l": 0.1921259842519685}
{"id": 6160, "code": "def set font ( self , font ) : font metrics = Qt Gui . Q Font Metrics ( font ) self . control . set Tab Stop Width ( self . tab width * font metrics . width ( ' ' ) ) self . completion widget . set Font ( font ) self . control . document ( ) . set Default Font ( font ) if self . page control : self . page control . document ( ) . set Default Font ( font ) self . font changed . emit ( font )", "predictions": ["sets the font font ."], "references": ["sets the base font for the consolewidget to the specified qfont ."], "bleu": 0.10067278896273972, "rouge_l": 0.4380610412926392}
{"id": 6161, "code": "def print ( self , printer = None ) : if ( not printer ) : printer = Qt Gui . Q Printer ( ) if ( Qt Gui . Q Print Dialog ( printer ) . exec ( ) != Qt Gui . Q Dialog . Accepted ) : return self . control . print ( printer )", "predictions": ["print a control and its contents ."], "references": ["print the contents of the consolewidget to the specified qprinter ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 6162, "code": "def prompt to top ( self ) : if not self . executing : prompt cursor = self . get prompt cursor ( ) if self . get cursor ( ) . block Number ( ) < prompt cursor . block Number ( ) : self . set cursor ( prompt cursor ) self . set top cursor ( prompt cursor )", "predictions": ["convert from this user to a top of the board ."], "references": ["moves the prompt to the top of the viewport ."], "bleu": 0.23462350320527994, "rouge_l": 0.4803149606299213}
{"id": 6163, "code": "def reset font ( self ) : if sys . platform == 'win32' : fallback = 'Courier' elif sys . platform == 'darwin' : fallback = 'Monaco' else : fallback = 'Monospace' font = get font ( self . font family , fallback ) if self . font size : font . set Point Size ( self . font size ) else : font . set Point Size ( Qt Gui . q App . font ( ) . point Size ( ) ) font . set Style Hint ( Qt Gui . Q Font . Type Writer ) self . set font ( font )", "predictions": ["resets the font to a new font ."], "references": ["sets the font to the default fixed - width font for this platform ."], "bleu": 0.15603043420373067, "rouge_l": 0.43323863636363635}
{"id": 6164, "code": "def append html ( self , html , before prompt = False ) : self . append custom ( self . insert html , html , before prompt )", "predictions": ["appends the custom html at the end of this compilation ."], "references": ["appends html at the end of the console buffer ."], "bleu": 0.4617366309441026, "rouge_l": 0.6724409448818898}
{"id": 6165, "code": "def append html fetching plain text ( self , html , before prompt = False ) : return self . append custom ( self . insert html fetching plain text , html , before prompt )", "predictions": ["appends the text to the end of the plain plain text ."], "references": ["appends html then returns the plain text version of it ."], "bleu": 0.1870361278311548, "rouge_l": 0.43821839080459773}
{"id": 6166, "code": "def append plain text ( self , text , before prompt = False ) : self . append custom ( self . insert plain text , text , before prompt )", "predictions": ["appends a plain text at the end of the list ."], "references": ["appends plain text processing ansi codes if enabled ."], "bleu": 0.17033186037639278, "rouge_l": 0.4073455759599332}
{"id": 6167, "code": "def complete with items ( self , cursor , items ) : self . cancel completion ( ) if len ( items ) == 1 : cursor . set Position ( self . control . text Cursor ( ) . position ( ) , Qt Gui . Q Text Cursor . Keep Anchor ) cursor . insert Text ( items [ 0 ] ) elif len ( items ) > 1 : current pos = self . control . text Cursor ( ) . position ( ) prefix = commonprefix ( items ) if prefix : cursor . set Position ( current pos , Qt Gui . Q Text Cursor . Keep Anchor ) cursor . insert Text ( prefix ) current pos = cursor . position ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = len ( prefix ) ) self . completion widget . show items ( cursor , items )", "predictions": ["show all items and completion of the given scroll position ."], "references": ["performs completion with items at the specified cursor location ."], "bleu": 0.14323145079400493, "rouge_l": 0.28818897637795277}
{"id": 6168, "code": "def fill temporary buffer ( self , cursor , text , html = False ) : current pos = self . control . text Cursor ( ) . position ( ) cursor . begin Edit Block ( ) self . append plain text ( '\\n' ) self . page ( text , html = html ) cursor . end Edit Block ( ) cursor . set Position ( current pos ) self . control . move Cursor ( Qt Gui . Q Text Cursor . End ) self . control . set Text Cursor ( cursor ) self . temp buffer filled = True", "predictions": ["create a temporary page at the top of the stack ."], "references": ["fill the area below the active editting zone with text"], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 6169, "code": "def create control ( self ) : if self . custom control : control = self . custom control ( ) elif self . kind == 'plain' : control = Qt Gui . Q Plain Text Edit ( ) elif self . kind == 'rich' : control = Qt Gui . Q Text Edit ( ) control . set Accept Rich Text ( False ) control . install Event Filter ( self ) control . viewport ( ) . install Event Filter ( self ) control . custom Context Menu Requested . connect ( self . custom context menu requested ) control . copy Available . connect ( self . copy available ) control . redo Available . connect ( self . redo available ) control . undo Available . connect ( self . undo available ) layout = control . document ( ) . document Layout ( ) layout . document Size Changed . disconnect ( ) layout . document Size Changed . connect ( self . adjust scrollbars ) control . set Attribute ( Qt Core . Qt . WA Input Method Enabled , True ) control . set Context Menu Policy ( Qt Core . Qt . Custom Context Menu ) control . set Read Only ( True ) control . set Undo Redo Enabled ( False ) control . set Vertical Scroll Bar Policy ( Qt Core . Qt . Scroll Bar Always On ) return control", "predictions": ["creates and returns a control control ui ."], "references": ["creates and connects the underlying text widget ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 6170, "code": "def create page control ( self ) : if self . custom page control : control = self . custom page control ( ) elif self . kind == 'plain' : control = Qt Gui . Q Plain Text Edit ( ) elif self . kind == 'rich' : control = Qt Gui . Q Text Edit ( ) control . install Event Filter ( self ) viewport = control . viewport ( ) viewport . install Event Filter ( self ) control . set Read Only ( True ) control . set Undo Redo Enabled ( False ) control . set Vertical Scroll Bar Policy ( Qt Core . Qt . Scroll Bar Always On ) return control", "predictions": ["create and install a custom page for this page ."], "references": ["creates and connects the underlying paging widget ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 6171, "code": "def get block plain text ( self , block ) : cursor = Qt Gui . Q Text Cursor ( block ) cursor . move Position ( Qt Gui . Q Text Cursor . Start Of Block ) cursor . move Position ( Qt Gui . Q Text Cursor . End Of Block , Qt Gui . Q Text Cursor . Keep Anchor ) return cursor . selection ( ) . to Plain Text ( )", "predictions": ["this will get a plain text to the given plain text ."], "references": ["given a qtextblock return its unformatted text ."], "bleu": 0.15537125692760353, "rouge_l": 0.3112244897959184}
{"id": 6172, "code": "def get end cursor ( self ) : cursor = self . control . text Cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . End ) return cursor", "predictions": ["displays the current cursor position to the top of the current cursor ."], "references": ["convenience method that returns a cursor for the last character ."], "bleu": 0.1135935489027116, "rouge_l": 0.2538141470180305}
{"id": 6173, "code": "def get prompt cursor ( self ) : cursor = self . control . text Cursor ( ) cursor . set Position ( self . prompt pos ) return cursor", "predictions": ["get the prompt for the current matrix ."], "references": ["convenience method that returns a cursor for the prompt position ."], "bleu": 0.18239668350432228, "rouge_l": 0.3070469798657718}
{"id": 6174, "code": "def insert continuation prompt ( self , cursor ) : if self . continuation prompt html is None : self . insert plain text ( cursor , self . continuation prompt ) else : self . continuation prompt = self . insert html fetching plain text ( cursor , self . continuation prompt html )", "predictions": ["inserts a cursor at the specified position in this model ."], "references": ["inserts new continuation prompt using the specified cursor ."], "bleu": 0.17827531042796255, "rouge_l": 0.4073455759599332}
{"id": 6175, "code": "def keyboard quit ( self ) : if self . temp buffer filled : self . cancel completion ( ) self . clear temporary buffer ( ) else : self . input buffer = ''", "predictions": ["locating the buffer and retries the child process ."], "references": ["cancels the current editing task ala ctrl - g in emacs ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 6176, "code": "def prompt started ( self ) : self . control . document ( ) . set Maximum Block Count ( 0 ) self . control . set Undo Redo Enabled ( True ) self . control . set Read Only ( False ) self . control . set Attribute ( Qt Core . Qt . WA Input Method Enabled , True ) if not self . reading : self . executing = False self . prompt started hook ( ) if self . input buffer pending : self . input buffer = self . input buffer pending self . input buffer pending = '' self . control . move Cursor ( Qt Gui . Q Text Cursor . End )", "predictions": ["usually the pending method ."], "references": ["called immediately after a new prompt is displayed ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 6177, "code": "def set top cursor ( self , cursor ) : scrollbar = self . control . vertical Scroll Bar ( ) scrollbar . set Value ( scrollbar . maximum ( ) ) original cursor = self . control . text Cursor ( ) self . control . set Text Cursor ( cursor ) self . control . ensure Cursor Visible ( ) self . control . set Text Cursor ( original cursor )", "predictions": ["transforms the alias to the default border ."], "references": ["scrolls the viewport so that the specified cursor is at the top ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 6178, "code": "def adjust scrollbars ( self ) : document = self . control . document ( ) scrollbar = self . control . vertical Scroll Bar ( ) viewport height = self . control . viewport ( ) . height ( ) if isinstance ( self . control , Qt Gui . Q Plain Text Edit ) : maximum = max ( 0 , document . line Count ( ) - 1 ) step = viewport height / self . control . font Metrics ( ) . line Spacing ( ) else : maximum = document . size ( ) . height ( ) step = viewport height diff = maximum - scrollbar . maximum ( ) scrollbar . set Range ( 0 , maximum ) scrollbar . set Page Step ( step ) if diff < 0 and document . block Count ( ) == document . maximum Block Count ( ) : scrollbar . set Value ( scrollbar . value ( ) + diff )", "predictions": ["adjusts the bounds of the = 1 to the = 1 line ."], "references": ["expands the vertical scrollbar beyond the range set by qt ."], "bleu": 0.1135935489027116, "rouge_l": 0.2538141470180305}
{"id": 6179, "code": "def dist in usersite ( dist ) : if user site : return normalize path ( dist location ( dist ) ) . startswith ( normalize path ( user site ) ) else : return False", "predictions": [": assumes that all coordinates of this distribution are within the specified vertex ."], "references": ["return true if given distribution is installed in user site ."], "bleu": 0.09782375748961449, "rouge_l": 0.16353887399463804}
{"id": 6180, "code": "def print results ( distributions , list all files ) : results printed = False for dist in distributions : results printed = True logger . info ( \"---\" ) logger . info ( \"Name: %s\" % dist [ 'name' ] ) logger . info ( \"Version: %s\" % dist [ 'version' ] ) logger . info ( \"Location: %s\" % dist [ 'location' ] ) logger . info ( \"Requires: %s\" % ', ' . join ( dist [ 'requires' ] ) ) if list all files : logger . info ( \"Files:\" ) if dist [ 'files' ] is not None : for line in dist [ 'files' ] : logger . info ( \"  %s\" % line . strip ( ) ) else : logger . info ( \"Cannot locate installed-files.txt\" ) return results printed", "predictions": ["prints the directive for all distribution objects ."], "references": ["print the informations from installed distributions found ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 6181, "code": "def main ( args = None ) : options , paths = parse options ( args ) format = getattr ( options , 'output' , 'simple' ) formatter = FORMATTERS [ format ] ( options ) for path in paths : meta = get metadata ( path , options . metadata version ) if meta is None : continue if options . download url prefix : if meta . download url is None : filename = os . path . basename ( path ) meta . download url = '%s/%s' % ( options . download url prefix , filename ) formatter ( meta ) formatter . finish ( )", "predictions": ["download input must not be imported ."], "references": ["entry point for pkginfo tool"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 6182, "code": "def cmp to key ( mycmp ) : class Key ( object ) : def init ( self , obj ) : self . obj = obj def lt ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def gt ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def eq ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 return Key", "predictions": ["convert a cmp= function into a key= function ."], "references": ["convert a cmp = function into a key = function"], "bleu": 0.29052561567528307, "rouge_l": 0.6256410256410255}
{"id": 6183, "code": "def file read ( filename ) : fobj = open ( filename , 'r' ) source = fobj . read ( ) fobj . close ( ) return source", "predictions": ["reads the contents of the get ."], "references": ["read a file and close it . returns the file source ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 6184, "code": "def close ( self ) : self . flush ( ) setattr ( sys , self . channel , self . ostream ) self . file . close ( ) self . closed = True", "predictions": ["flushes the output and closes the underlying output ."], "references": ["close the file and restore the channel ."], "bleu": 0.17747405280050263, "rouge_l": 0.4756335282651072}
{"id": 6185, "code": "def write ( self , data ) : self . file . write ( data ) self . ostream . write ( data ) self . ostream . flush ( )", "predictions": ["writes the coloring method to the underlying stream ."], "references": ["write data to both channels ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 6186, "code": "def add new heart handler ( self , handler ) : self . log . debug ( \"heartbeat::new heart handler: %s\" , handler ) self . new handlers . add ( handler )", "predictions": ["adds a single ) ) ) to the list ."], "references": ["add a new handler for new hearts"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 6187, "code": "def add heart failure handler ( self , handler ) : self . log . debug ( \"heartbeat::new heart failure handler: %s\" , handler ) self . failure handlers . add ( handler )", "predictions": ["adds a ( possibly registered : self . csv : dispatch : http : / / www . org / tr / ( / ( . com / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( / ( /"], "references": ["add a new handler for heart failure"], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 6188, "code": "def handle pong ( self , msg ) : current = str to bytes ( str ( self . lifetime ) ) last = str to bytes ( str ( self . last ping ) ) if msg [ 1 ] == current : delta = time . time ( ) - self . tic self . responses . add ( msg [ 0 ] ) elif msg [ 1 ] == last : delta = time . time ( ) - self . tic + ( self . lifetime - self . last ping ) self . log . warn ( \"heartbeat::heart %r missed a beat, and took %.2f ms to respond\" , msg [ 0 ] , 1000 * delta ) self . responses . add ( msg [ 0 ] ) else : self . log . warn ( \"heartbeat::got bad heartbeat (possibly old?): %s (current=%.3f)\" , msg [ 1 ] , self . lifetime )", "predictions": ["valid ( or optionally : the raw output stream : for each ping that we want to get an error . once we want to extend the message to the raw output stream , we need to extend the raw - to valid ."], "references": ["a heart just beat"], "bleu": 0.022996104098636838, "rouge_l": 0.0}
{"id": 6189, "code": "def display All ( elapsed , display amt , est end , n Loops , count , num Prints ) : if num Prints > n Loops : display amt = 1 else : display amt = round ( n Loops / num Prints ) if count % display amt == 0 : avg = elapsed / count est end = round ( avg * n Loops ) ( disp elapsed , disp avg , disp est ) = time Unit ( int ( round ( elapsed ) ) , int ( round ( avg ) ) , int ( round ( est end ) ) ) print \"%s%%\" % str ( round ( count / float ( n Loops ) * 100 ) ) , \"@\" + str ( count ) , total Time = disp est [ 0 ] unit = disp est [ 1 ] if str ( unit ) == \"secs\" : remain = total Time - round ( elapsed ) remain Unit = \"secs\" elif str ( unit ) == \"mins\" : remain = total Time - round ( elapsed ) / 60 remain Unit = \"mins\" elif str ( unit ) == \"hr\" : remain = total Time - round ( elapsed ) / 3600 remain Unit = \"hr\" print \"ETA: %s %s\" % ( str ( remain ) , remain Unit ) print return", "predictions": ["write all other way of the running time ."], "references": ["displays time if verbose is true and count is within the display amount"], "bleu": 0.10015045110931886, "rouge_l": 0.08802308802308802}
{"id": 6190, "code": "def time Unit ( elapsed , avg , est end ) : minute = 60 hr = 3600 day = 86400 if elapsed <= 3 * minute : unit elapsed = ( elapsed , \"secs\" ) if elapsed > 3 * minute : unit elapsed = ( ( elapsed / 60 ) , \"mins\" ) if elapsed > 3 * hr : unit elapsed = ( ( elapsed / 3600 ) , \"hr\" ) if avg <= 3 * minute : unit avg = ( avg , \"secs\" ) if avg > 3 * minute : unit avg = ( ( avg / 60 ) , \"mins\" ) if avg > 3 * hr : unit avg = ( ( avg / 3600 ) , \"hr\" ) if est end <= 3 * minute : unit est End = ( est end , \"secs\" ) if est end > 3 * minute : unit est End = ( ( est end / 60 ) , \"mins\" ) if est end > 3 * hr : unit est End = ( ( est end / 3600 ) , \"hr\" ) return [ unit elapsed , unit avg , unit est End ]", "predictions": ["interpolate can be the can be a final unit or a time larger than the unit ."], "references": ["calculates unit of time to display"], "bleu": 0.07994607499472013, "rouge_l": 0.19032761310452417}
{"id": 6191, "code": "def uncache zipdir ( path ) : from zipimport import zip directory cache as zdc uncache ( path , zdc ) uncache ( path , sys . path importer cache )", "predictions": ["create an internal return value for the return type ."], "references": ["ensure that the importer caches dont have stale info for path"], "bleu": 0.12623203108004888, "rouge_l": 0.09442724458204334}
{"id": 6192, "code": "def nt quote arg ( arg ) : result = [ ] needquote = False nb = 0 needquote = ( \" \" in arg ) or ( \"\\t\" in arg ) if needquote : result . append ( '\"' ) for c in arg : if c == '\\\\' : nb += 1 elif c == '\"' : result . append ( '\\\\' * ( nb * 2 ) + '\\\\\"' ) nb = 0 else : if nb : result . append ( '\\\\' * nb ) nb = 0 result . append ( c ) if nb : result . append ( '\\\\' * nb ) if needquote : result . append ( '\\\\' * nb ) result . append ( '\"' ) return '' . join ( result )", "predictions": ["font an argument with the arguments passed to the argument . the method is called with the arguments to font . the argument is the same . for each command is passed to the argument . the method is applied to the argument . the method will be passed to"], "references": ["quote a command line argument according to windows parsing rules"], "bleu": 0.028577262451992175, "rouge_l": 0.11366459627329194}
{"id": 6193, "code": "def install script ( self , dist , script name , script text , dev path = None ) : spec = str ( dist . as requirement ( ) ) is script = is python script ( script text , script name ) def get template ( filename ) : raw bytes = resource string ( 'setuptools' , template name ) template str = raw bytes . decode ( 'utf-8' ) clean template = template str . replace ( '\"\"\"' , '' ) return clean template if is script : template name = 'script template.py' if dev path : template name = template name . replace ( '.py' , ' (dev).py' ) script text = ( get script header ( script text ) + get template ( template name ) % locals ( ) ) self . write script ( script name , to ascii ( script text ) , 'b' )", "predictions": ["take a ( and put it on it as a byte array ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["generate a legacy script wrapper and install it"], "bleu": 0.028577262451992175, "rouge_l": 0.11898569570871263}
{"id": 6194, "code": "def check conflicts ( self , dist ) : return dist from imp import find module , get suffixes from glob import glob blockers = [ ] names = dict . fromkeys ( dist . get metadata ( 'top level.txt' ) ) exts = { '.pyc' : 1 , '.pyo' : 1 } for ext , mode , typ in get suffixes ( ) : exts [ ext ] = 1 for path , files in expand paths ( [ self . install dir ] + self . all site dirs ) : for filename in files : base , ext = os . path . splitext ( filename ) if base in names : if not ext : try : f , filename , descr = find module ( base , [ path ] ) except Import Error : continue else : if f : f . close ( ) if filename not in blockers : blockers . append ( filename ) elif ext in exts and base != 'site' : blockers . append ( os . path . join ( path , filename ) ) if blockers : self . found conflicts ( dist , blockers ) return dist", "predictions": ["check for to see if any of the directories in this site ."], "references": ["verify that there are no conflicting old - style packages"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 6195, "code": "def create home path ( self ) : if not self . user : return home = convert path ( os . path . expanduser ( \"~\" ) ) for name , path in self . config vars . iteritems ( ) : if path . startswith ( home ) and not os . path . isdir ( path ) : self . debug print ( \"os.makedirs('%s', 0700)\" % path ) os . makedirs ( path , 0700 )", "predictions": ["override for creating all sessions from the database ."], "references": ["create directories under ~ ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 6196, "code": "def is archive file ( name ) : archives = ( '.zip' , '.tar.gz' , '.tar.bz2' , '.tgz' , '.tar' , '.whl' ) ext = splitext ( name ) [ 1 ] . lower ( ) if ext in archives : return True return False", "predictions": ["does this ( i . e . ( a before ( before ( before ( before ( before ( before ( before ( before ( before ( before ( before ( before ( before ( before ( . before ( before ( before ( . before ( . before ("], "references": ["return true if name is a considered as an archive file ."], "bleu": 0.026594139297659906, "rouge_l": 0.07253269916765755}
{"id": 6197, "code": "def new output ( output type = None , output text = None , output png = None , output html = None , output svg = None , output latex = None , output json = None , output javascript = None , output jpeg = None , prompt number = None , etype = None , evalue = None , traceback = None ) : output = Notebook Node ( ) if output type is not None : output . output type = unicode ( output type ) if output type != 'pyerr' : if output text is not None : output . text = unicode ( output text ) if output png is not None : output . png = bytes ( output png ) if output jpeg is not None : output . jpeg = bytes ( output jpeg ) if output html is not None : output . html = unicode ( output html ) if output svg is not None : output . svg = unicode ( output svg ) if output latex is not None : output . latex = unicode ( output latex ) if output json is not None : output . json = unicode ( output json ) if output javascript is not None : output . javascript = unicode ( output javascript ) if output type == u'pyout' : if prompt number is not None : output . prompt number = int ( prompt number ) if output type == u'pyerr' : if etype is not None : output . etype = unicode ( etype ) if evalue is not None : output . evalue = unicode ( evalue ) if traceback is not None : output . traceback = [ unicode ( frame ) for frame in list ( traceback ) ] return output", "predictions": ["create a append query object ."], "references": ["create a new code cell with input and output"], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 6198, "code": "def new code cell ( input = None , prompt number = None , outputs = None , language = u'python' , collapsed = False , metadata = None ) : cell = Notebook Node ( ) cell . cell type = u'code' if language is not None : cell . language = unicode ( language ) if input is not None : cell . input = unicode ( input ) if prompt number is not None : cell . prompt number = int ( prompt number ) if outputs is None : cell . outputs = [ ] else : cell . outputs = outputs if collapsed is not None : cell . collapsed = bool ( collapsed ) cell . metadata = Notebook Node ( metadata or { } ) return cell", "predictions": ["create a new text node for the given : text , self - text , ."], "references": ["create a new code cell with input and output"], "bleu": 0.1431712315455507, "rouge_l": 0.2527624309392265}
{"id": 6199, "code": "def new text cell ( cell type , source = None , rendered = None , metadata = None ) : cell = Notebook Node ( ) if cell type == 'plaintext' : cell type = 'raw' if source is not None : cell . source = unicode ( source ) if rendered is not None : cell . rendered = unicode ( rendered ) cell . metadata = Notebook Node ( metadata or { } ) cell . cell type = cell type return cell", "predictions": ["creates a complete items or ."], "references": ["create a new text cell ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 6200, "code": "def new heading cell ( source = None , rendered = None , level = 1 , metadata = None ) : cell = Notebook Node ( ) cell . cell type = u'heading' if source is not None : cell . source = unicode ( source ) if rendered is not None : cell . rendered = unicode ( rendered ) cell . level = int ( level ) cell . metadata = Notebook Node ( metadata or { } ) return cell", "predictions": ["creates a fill node with the given = buffer"], "references": ["create a new section cell with a given integer level ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 6201, "code": "def new notebook ( name = None , metadata = None , worksheets = None ) : nb = Notebook Node ( ) nb . nbformat = nbformat nb . nbformat minor = nbformat minor if worksheets is None : nb . worksheets = [ ] else : nb . worksheets = list ( worksheets ) if metadata is None : nb . metadata = new metadata ( ) else : nb . metadata = Notebook Node ( metadata ) if name is not None : nb . metadata . name = unicode ( name ) return nb", "predictions": ["create a create a create or install an existing control"], "references": ["create a notebook by name id and a list of worksheets ."], "bleu": 0.14595947916189678, "rouge_l": 0.2683284457478006}
{"id": 6202, "code": "def new metadata ( name = None , authors = None , license = None , created = None , modified = None , gistid = None ) : metadata = Notebook Node ( ) if name is not None : metadata . name = unicode ( name ) if authors is not None : metadata . authors = list ( authors ) if created is not None : metadata . created = unicode ( created ) if modified is not None : metadata . modified = unicode ( modified ) if license is not None : metadata . license = unicode ( license ) if gistid is not None : metadata . gistid = unicode ( gistid ) return metadata", "predictions": ["create a create a create or gistid component"], "references": ["create a new metadata node ."], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 6203, "code": "def new author ( name = None , email = None , affiliation = None , url = None ) : author = Notebook Node ( ) if name is not None : author . name = unicode ( name ) if email is not None : author . email = unicode ( email ) if affiliation is not None : author . affiliation = unicode ( affiliation ) if url is not None : author . url = unicode ( url ) return author", "predictions": ["create a new block with the given text and )"], "references": ["create a new author ."], "bleu": 0.23462350320528, "rouge_l": 0.42558139534883715}
{"id": 6204, "code": "def writable dir ( path ) : return os . path . isdir ( path ) and os . access ( path , os . W OK )", "predictions": ["creates the directory for the given ( e . g . \" root \" self - > \" self - files that are the same as this is used as the default timezone ."], "references": ["whether path is a directory to which the user has write access ."], "bleu": 0.04466216597703001, "rouge_l": 0.13884673748103188}
{"id": 6205, "code": "def unquote filename ( name , win32 = ( sys . platform == 'win32' ) ) : if win32 : if name . startswith ( ( \"'\" , '\"' ) ) and name . endswith ( ( \"'\" , '\"' ) ) : name = name [ 1 : - 1 ] return name", "predictions": ["unquotes a prompt if it has already been done ."], "references": ["on windows remove leading and trailing quotes from filenames ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 6206, "code": "def get ipython package dir ( ) : ipdir = os . path . dirname ( I Python . file ) return py3compat . cast unicode ( ipdir , fs encoding )", "predictions": ["get the html of the continuation ."], "references": ["get the base directory where ipython itself is installed ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 6207, "code": "def update suggestions dictionary ( request , object ) : if request . user . is authenticated ( ) : user = request . user content type = Content Type . objects . get for model ( type ( object ) ) try : Object View . objects . get ( user = user , object id = object . id , content type = content type ) except : Object View . objects . create ( user = user , content object = object ) viewed = Object View . objects . filter ( user = user ) else : update dict for guests ( request , object , content type ) return if viewed : for obj in viewed : if content type == obj . content type : if not exists in dictionary ( request , object , content type , obj , True ) : if object . id != obj . object id : Object View Dictionary . objects . create ( current object = object , visited before object = obj . content object ) if not exists in dictionary ( request , obj , obj . content type , object , False ) : Object View Dictionary . objects . create ( current object = obj . content object , visited before object = object ) return", "predictions": ["keyboard objects : keyboard - create = ( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["updates the suggestions dictionary for an object upon visiting its page"], "bleu": 0.022996104098636838, "rouge_l": 0.0}
{"id": 6208, "code": "def get suggestions with size ( object , size ) : content type = Content Type . objects . get for model ( type ( object ) ) try : return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] ) [ : size ] except : return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] )", "predictions": ["we need to get the extra objects to insert a whole object into a dictionary of ( ."], "references": ["gets a list with a certain size of suggestions for an object"], "bleu": 0.08562365224473284, "rouge_l": 0.2074829931972789}
{"id": 6209, "code": "def get suggestions ( object ) : content type = Content Type . objects . get for model ( type ( object ) ) return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] )", "predictions": ["a higher order function for each object in the database"], "references": ["gets a list of all suggestions for an object"], "bleu": 0.14991106946711685, "rouge_l": 0.31881533101045295}
{"id": 6210, "code": "def options ( self , parser , env ) : if not self . available ( ) : return Plugin . options ( self , parser , env ) parser . add option ( '--profile-sort' , action = 'store' , dest = 'profile sort' , default = env . get ( 'NOSE PROFILE SORT' , 'cumulative' ) , metavar = \"SORT\" , help = \"Set sort order for profiler output\" ) parser . add option ( '--profile-stats-file' , action = 'store' , dest = 'profile stats file' , metavar = \"FILE\" , default = env . get ( 'NOSE PROFILE STATS FILE' ) , help = 'Profiler stats file; default is a new ' 'temp file on each run' ) parser . add option ( '--profile-restrict' , action = 'append' , dest = 'profile restrict' , metavar = \"RESTRICT\" , default = env . get ( 'NOSE PROFILE RESTRICT' ) , help = \"Restrict profiler output. See help for \" \"pstats.Stats for details\" )", "predictions": ["options for all the three ( ."], "references": ["register commandline options ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 6211, "code": "def begin ( self ) : if not self . available ( ) : return self . create pfile ( ) self . prof = hotshot . Profile ( self . pfile )", "predictions": ["dynamic init . this method is called to begin the process of the underlying class ."], "references": ["create profile stats file and load profiler ."], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 6212, "code": "def report ( self , stream ) : log . debug ( 'printing profiler report' ) self . prof . close ( ) prof stats = stats . load ( self . pfile ) prof stats . sort stats ( self . sort ) compat 25 = hasattr ( prof stats , 'stream' ) if compat 25 : tmp = prof stats . stream prof stats . stream = stream else : tmp = sys . stdout sys . stdout = stream try : if self . restrict : log . debug ( 'setting profiler restriction to %s' , self . restrict ) prof stats . print stats ( * self . restrict ) else : prof stats . print stats ( ) finally : if compat 25 : prof stats . stream = tmp else : sys . stdout = tmp", "predictions": ["report stats for all the given stream ."], "references": ["output profiler report ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 6213, "code": "def finalize ( self , result ) : if not self . available ( ) : return try : self . prof . close ( ) except Attribute Error : pass if self . clean stats file : if self . fileno : try : os . close ( self . fileno ) except OS Error : pass try : os . unlink ( self . pfile ) except OS Error : pass return None", "predictions": ["finalize and return the preferred object ."], "references": ["clean up stats file if configured to do so ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 6214, "code": "def setup partitioner ( index , num procs , gnum cells , parts ) : global partitioner p = MPI Rect Partitioner2D ( my id = index , num procs = num procs ) p . redim ( global num cells = gnum cells , num parts = parts ) p . prepare communication ( ) partitioner = p", "predictions": ["creates a simple ( ."], "references": ["create a partitioner in the engine namespace"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 6215, "code": "def wave saver ( u , x , y , t ) : global u hist global t hist t hist . append ( t ) u hist . append ( 1.0 * u )", "predictions": ["an wave saver . register the two rectangles ."], "references": ["save the wave log"], "bleu": 0.15619699684601276, "rouge_l": 0.16531165311653115}
{"id": 6216, "code": "def init db ( self ) : self . db = sqlite3 . connect ( self . hist file , detect types = sqlite3 . PARSE DECLTYPES | sqlite3 . PARSE COLNAMES ) self . db . execute ( ) self . db . execute ( ) self . db . execute ( ) self . db . commit ( )", "predictions": ["initialize and add new badge ."], "references": ["connect to the database and create tables if necessary ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 6217, "code": "def new session ( self , conn = None ) : if conn is None : conn = self . db with conn : cur = conn . execute ( , ( datetime . datetime . now ( ) , ) ) self . session number = cur . lastrowid", "predictions": ["creates a new empty . ."], "references": ["get a new session number ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 6218, "code": "def end session ( self ) : self . writeout cache ( ) with self . db : self . db . execute ( , ( datetime . datetime . now ( ) , len ( self . input hist parsed ) - 1 , self . session number ) ) self . session number = 0", "predictions": ["the end of the current operation ."], "references": ["close the database session filling in the end time and line count ."], "bleu": 0.11787460936700446, "rouge_l": 0.2846034214618974}
{"id": 6219, "code": "def name session ( self , name ) : with self . db : self . db . execute ( \"UPDATE sessions SET remark=? WHERE session==?\" , ( name , self . session number ) )", "predictions": ["attach an name to the database ."], "references": ["give the current session a name in the history database ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 6220, "code": "def writeout cache ( self , conn = None ) : if conn is None : conn = self . db with self . db input cache lock : try : self . writeout input cache ( conn ) except sqlite3 . Integrity Error : self . new session ( conn ) print ( \"ERROR! Session/line number was not unique in\" , \"database. History logging moved to new session\" , self . session number ) try : self . writeout input cache ( conn ) except sqlite3 . Integrity Error : pass finally : self . db input cache = [ ] with self . db output cache lock : try : self . writeout output cache ( conn ) except sqlite3 . Integrity Error : print ( \"!! Session/line number for output was not unique\" , \"in database. Output will not be stored.\" ) finally : self . db output cache = [ ]", "predictions": ["period the database and connection out of the database ."], "references": ["write any entries in the cache to the database ."], "bleu": 0.24808415001701817, "rouge_l": 0.4}
{"id": 6221, "code": "def get num cpus ( ) : try : return os . sysconf ( \"SC NPROCESSORS ONLN\" ) except Value Error : num = 0 f = open ( '/proc/cpuinfo' , 'r' ) try : lines = f . readlines ( ) finally : f . close ( ) for line in lines : if line . lower ( ) . startswith ( 'processor' ) : num += 1 if num == 0 : f = open ( '/proc/stat' , 'r' ) try : lines = f . readlines ( ) finally : f . close ( ) search = re . compile ( 'cpu\\d' ) for line in lines : line = line . split ( ' ' ) [ 0 ] if search . match ( line ) : num += 1 if num == 0 : raise Runtime Error ( \"can't determine number of CP Us\" ) return num", "predictions": ["gets the number of cpus ."], "references": ["return the number of cpus on the system"], "bleu": 0.4405149609594086, "rouge_l": 0.5570776255707762}
{"id": 6222, "code": "def disk partitions ( all = False ) : phydevs = [ ] f = open ( \"/proc/filesystems\" , \"r\" ) try : for line in f : if not line . startswith ( \"nodev\" ) : phydevs . append ( line . strip ( ) ) finally : f . close ( ) retlist = [ ] partitions = psutil linux . get disk partitions ( ) for partition in partitions : device , mountpoint , fstype , opts = partition if device == 'none' : device = '' if not all : if device == '' or fstype not in phydevs : continue ntuple = nt partition ( device , mountpoint , fstype , opts ) retlist . append ( ntuple ) return retlist", "predictions": ["retrieve the disk partitions ."], "references": ["return mounted disk partitions as a list of nameduples"], "bleu": 0.1614457444314309, "rouge_l": 0.2717149220489978}
{"id": 6223, "code": "def get system users ( ) : retlist = [ ] rawlist = psutil linux . get system users ( ) for item in rawlist : user , tty , hostname , tstamp , user process = item if not user process : continue if hostname == ':0.0' : hostname = 'localhost' nt = nt user ( user , tty or None , hostname , tstamp ) retlist . append ( nt ) return retlist", "predictions": ["get all users for this user ."], "references": ["return currently connected users as a list of namedtuples ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 6224, "code": "def get pid list ( ) : pids = [ int ( x ) for x in os . listdir ( '/proc' ) if x . isdigit ( ) ] return pids", "predictions": ["get a list of all process ids ."], "references": ["returns a list of pids currently running on the system ."], "bleu": 0.21690743377623947, "rouge_l": 0.4093959731543625}
{"id": 6225, "code": "def short stack ( ) : stack = inspect . stack ( ) [ : 0 : - 1 ] return \"\\n\" . join ( [ \"%30s : %s @%d\" % ( t [ 3 ] , t [ 1 ] , t [ 2 ] ) for t in stack ] )", "predictions": ["get the name of the stack ."], "references": ["return a string summarizing the call stack ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 6226, "code": "def join regex ( regexes ) : if len ( regexes ) > 1 : return \"|\" . join ( [ \"(%s)\" % r for r in regexes ] ) elif regexes : return regexes [ 0 ] else : return \"\"", "predictions": ["join all the parameters of a previous iterable into a single string ."], "references": ["combine a list of regexes into one that matches any of them ."], "bleu": 0.12011055432195765, "rouge_l": 0.23076923076923084}
{"id": 6227, "code": "def file be gone ( path ) : try : os . remove ( path ) except OS Error : , e , = sys . exc info ( ) if e . errno != errno . ENOENT : raise", "predictions": ["create a directory if it does not exist ."], "references": ["remove a file and don t get annoyed if it doesn t exist ."], "bleu": 0.14026062829965458, "rouge_l": 0.41838134430727025}
{"id": 6228, "code": "def update ( self , v ) : self . md5 . update ( to bytes ( str ( type ( v ) ) ) ) if isinstance ( v , string class ) : self . md5 . update ( to bytes ( v ) ) elif v is None : pass elif isinstance ( v , ( int , float ) ) : self . md5 . update ( to bytes ( str ( v ) ) ) elif isinstance ( v , ( tuple , list ) ) : for e in v : self . update ( e ) elif isinstance ( v , dict ) : keys = v . keys ( ) for k in sorted ( keys ) : self . update ( k ) self . update ( v [ k ] ) else : for k in dir ( v ) : if k . startswith ( ' ' ) : continue a = getattr ( v , k ) if inspect . isroutine ( a ) : continue self . update ( k ) self . update ( a )", "predictions": ["update a new value using a new one ."], "references": ["add v to the hash recursively if needed ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 6229, "code": "def update profiles ( self ) : for path in [ get ipython dir ( ) , os . getcwdu ( ) ] : for profile in list profiles in ( path ) : pd = self . get profile dir ( profile , path ) if profile not in self . profiles : self . log . debug ( \"Adding cluster profile '%s'\" % profile ) self . profiles [ profile ] = { 'profile' : profile , 'profile dir' : pd , 'status' : 'stopped' }", "predictions": ["update all profiles with their info ."], "references": ["list all profiles in the ipython_dir and cwd ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 6230, "code": "def start cluster ( self , profile , n = None ) : self . check profile ( profile ) data = self . profiles [ profile ] if data [ 'status' ] == 'running' : raise web . HTTP Error ( 409 , u'cluster already running' ) cl , esl , default n = self . build launchers ( data [ 'profile dir' ] ) n = n if n is not None else default n def clean data ( ) : data . pop ( 'controller launcher' , None ) data . pop ( 'engine set launcher' , None ) data . pop ( 'n' , None ) data [ 'status' ] = 'stopped' def engines stopped ( r ) : self . log . debug ( 'Engines stopped' ) if cl . running : cl . stop ( ) clean data ( ) esl . on stop ( engines stopped ) def controller stopped ( r ) : self . log . debug ( 'Controller stopped' ) if esl . running : esl . stop ( ) clean data ( ) cl . on stop ( controller stopped ) dc = ioloop . Delayed Callback ( lambda : cl . start ( ) , 0 , self . loop ) dc . start ( ) dc = ioloop . Delayed Callback ( lambda : esl . start ( n ) , 1000 * self . delay , self . loop ) dc . start ( ) self . log . debug ( 'Cluster started' ) data [ 'controller launcher' ] = cl data [ 'engine set launcher' ] = esl data [ 'n' ] = n data [ 'status' ] = 'running' return self . profile info ( profile )", "predictions": ["starts or disables a new operation ."], "references": ["start a cluster for a given profile ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 6231, "code": "def stop cluster ( self , profile ) : self . check profile ( profile ) data = self . profiles [ profile ] if data [ 'status' ] == 'stopped' : raise web . HTTP Error ( 409 , u'cluster not running' ) data = self . profiles [ profile ] cl = data [ 'controller launcher' ] esl = data [ 'engine set launcher' ] if cl . running : cl . stop ( ) if esl . running : esl . stop ( ) result = { 'profile' : data [ 'profile' ] , 'profile dir' : data [ 'profile dir' ] , 'status' : 'stopped' } return result", "predictions": ["stops a running cluster for the calling thread ."], "references": ["stop a cluster for a given profile ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 6232, "code": "def find cmd ( cmd ) : try : from win32api import Search Path except Import Error : raise Import Error ( 'you need to have pywin32 installed for this to work' ) else : PATH = os . environ [ 'PATH' ] extensions = [ '.exe' , '.com' , '.bat' , '.py' ] path = None for ext in extensions : try : path = Search Path ( PATH , cmd + ext ) [ 0 ] except : pass if path is None : raise OS Error ( \"command %r not found\" % cmd ) else : return path", "predictions": ["search for ( . search ( ."], "references": ["find the full path to a . bat or . exe using the win32api module ."], "bleu": 0.056829570481990416, "rouge_l": 0.16245006657789615}
{"id": 6233, "code": "def system body ( p ) : enc = DEFAULT ENCODING for line in read no interrupt ( p . stdout ) . splitlines ( ) : line = line . decode ( enc , 'replace' ) print ( line , file = sys . stdout ) for line in read no interrupt ( p . stderr ) . splitlines ( ) : line = line . decode ( enc , 'replace' ) print ( line , file = sys . stderr ) return p . wait ( )", "predictions": ["tries to read system body from the given file ."], "references": ["callback for _system ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 6234, "code": "def setup partitioner ( comm , addrs , index , num procs , gnum cells , parts ) : global partitioner p = ZMQ Rect Partitioner2D ( comm , addrs , my id = index , num procs = num procs ) p . redim ( global num cells = gnum cells , num parts = parts ) p . prepare communication ( ) partitioner = p", "predictions": ["creates a new instance of . ."], "references": ["create a partitioner in the engine namespace"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 6235, "code": "def init gui pylab ( self ) : if self . gui or self . pylab : shell = self . shell try : if self . pylab : gui , backend = pylabtools . find gui and backend ( self . pylab ) self . log . info ( \"Enabling GUI event loop integration, \" \"toolkit=%s, pylab=%s\" % ( gui , self . pylab ) ) shell . enable pylab ( gui , import all = self . pylab import all ) else : self . log . info ( \"Enabling GUI event loop integration, \" \"toolkit=%s\" % self . gui ) shell . enable gui ( self . gui ) except Exception : self . log . warn ( \"GUI event loop or pylab initialization failed\" ) self . shell . showtraceback ( )", "predictions": ["called on . . this method will cause the gui to be called once for each user has been set ."], "references": ["enable gui event loop integration taking pylab into account ."], "bleu": 0.06429451441231726, "rouge_l": 0.13785310734463277}
{"id": 6236, "code": "def init code ( self ) : self . run startup files ( ) self . run exec lines ( ) self . run exec files ( ) self . run cmd line code ( ) self . run module ( ) sys . stdout . flush ( ) sys . stderr . flush ( ) self . shell . user ns hidden . update ( self . shell . user ns )", "predictions": ["initialize the module . this is called when the client provides a startup ."], "references": ["run the pre - flight code specified via exec_lines"], "bleu": 0.08839374326825923, "rouge_l": 0.09050445103857567}
{"id": 6237, "code": "def run exec lines ( self ) : if not self . exec lines : return try : self . log . debug ( \"Running code from I Python App.exec lines...\" ) for line in self . exec lines : try : self . log . info ( \"Running code in user namespace: %s\" % line ) self . shell . run cell ( line , store history = False ) except : self . log . warn ( \"Error in executing line in user \" \"namespace: %s\" % line ) self . shell . showtraceback ( ) except : self . log . warn ( \"Unknown error in handling I Python App.exec lines:\" ) self . shell . showtraceback ( )", "predictions": ["running a garbage collection ."], "references": ["run lines of code in ipythonapp . exec_lines in the user s namespace ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 6238, "code": "def run startup files ( self ) : startup dir = self . profile dir . startup dir startup files = glob . glob ( os . path . join ( startup dir , '*.py' ) ) startup files += glob . glob ( os . path . join ( startup dir , '*.ipy' ) ) if not startup files : return self . log . debug ( \"Running startup files from %s...\" , startup dir ) try : for fname in sorted ( startup files ) : self . exec file ( fname ) except : self . log . warn ( \"Unknown error in handling startup files:\" ) self . shell . showtraceback ( )", "predictions": ["runs the startup at the specified command ."], "references": ["run files from profile startup directory"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 6239, "code": "def run exec files ( self ) : if not self . exec files : return self . log . debug ( \"Running files in I Python App.exec files...\" ) try : for fname in self . exec files : self . exec file ( fname ) except : self . log . warn ( \"Unknown error in handling I Python App.exec files:\" ) self . shell . showtraceback ( )", "predictions": ["running all reports on this python ."], "references": ["run files from ipythonapp . exec_files"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 6240, "code": "def run cmd line code ( self ) : if self . code to run : line = self . code to run try : self . log . info ( \"Running code given at command line (c=): %s\" % line ) self . shell . run cell ( line , store history = False ) except : self . log . warn ( \"Error in executing line in user namespace: %s\" % line ) self . shell . showtraceback ( ) elif self . file to run : fname = self . file to run try : self . exec file ( fname ) except : self . log . warn ( \"Error in executing file in user namespace: %s\" % fname ) self . shell . showtraceback ( )", "predictions": ["this method executes the command specified in the command line . the commandline will be run in the command line ."], "references": ["run code or file specified at the command - line"], "bleu": 0.09092617426809149, "rouge_l": 0.27570621468926554}
{"id": 6241, "code": "def run module ( self ) : if self . module to run : save argv = sys . argv sys . argv = [ sys . executable ] + self . extra args try : self . shell . safe run module ( self . module to run , self . shell . user ns ) finally : sys . argv = save argv", "predictions": ["get the current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current current"], "references": ["run module specified at the command - line ."], "bleu": 0.02403051755364481, "rouge_l": 0.038754764930114365}
{"id": 6242, "code": "def generic ( func ) : sentinel = object ( ) def by class ( * args , * * kw ) : cls = args [ 0 ] . class for t in type ( cls . name , ( cls , object ) , { } ) . mro : f = gbt ( t , sentinel ) if f is not sentinel : return f ( * args , * * kw ) else : return func ( * args , * * kw ) by type = { object : func } try : by type [ Instance Type ] = by class except Name Error : pass gbt = by type . get def when type ( * types ) : \"\"\"Decorator to add a method that will be called for the given types\"\"\" for t in types : if not isinstance ( t , classtypes ) : raise Type Error ( \"%r is not a type or class\" % ( t , ) ) def decorate ( f ) : for t in types : if by type . setdefault ( t , f ) is not f : raise Type Error ( \"%r already has method for type %r\" % ( func , t ) ) return f return decorate by object = { } gbo = by object . get def when object ( * obs ) : \"\"\"Decorator to add a method to be called for the given object(s)\"\"\" def decorate ( f ) : for o in obs : if by object . setdefault ( id ( o ) , ( o , f ) ) [ 1 ] is not f : raise Type Error ( \"%r already has method for object %r\" % ( func , o ) ) return f return decorate def dispatch ( * args , * * kw ) : f = gbo ( id ( args [ 0 ] ) , sentinel ) if f is sentinel : for t in type ( args [ 0 ] ) . mro : f = gbt ( t , sentinel ) if f is not sentinel : return f ( * args , * * kw ) else : return func ( * args , * * kw ) else : return f [ 1 ] ( * args , * * kw ) dispatch . name = func . name dispatch . dict = func . dict . copy ( ) dispatch . doc = func . doc dispatch . module = func . module dispatch . when type = when type dispatch . when object = when object dispatch . default = func dispatch . has object = lambda o : id ( o ) in by object dispatch . has type = lambda t : t in by type return dispatch", "predictions": ["options for all functions of ( ."], "references": ["create a simple generic function"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 6243, "code": "def data ( fname ) : data file = open ( data filename ( fname ) ) try : return data file . read ( ) finally : data file . close ( )", "predictions": ["returns content of the begin file as a byte array ."], "references": ["return the contents of a data file of ours ."], "bleu": 0.14991106946711685, "rouge_l": 0.28818897637795277}
{"id": 6244, "code": "def escape ( t ) : return ( t . replace ( \"&\" , \"&amp;\" ) . replace ( \"<\" , \"&lt;\" ) . replace ( \">\" , \"&gt;\" ) . replace ( \"'\" , \"&#39;\" ) . replace ( '\"' , \"&quot;\" ) . replace ( \"  \" , \"&nbsp; \" ) . replace ( \"  \" , \"&nbsp; \" ) )", "predictions": ["report the name of the given string ."], "references": ["html - escape the text in t ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 6245, "code": "def make local static report files ( self ) : for static , pkgdir in self . STATIC FILES : shutil . copyfile ( data filename ( static , pkgdir ) , os . path . join ( self . directory , static ) ) if self . extra css : shutil . copyfile ( self . config . extra css , os . path . join ( self . directory , self . extra css ) )", "predictions": ["makes a , with all the unlink result ."], "references": ["make local instances of static files for html report ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 6246, "code": "def write html ( self , fname , html ) : fout = open ( fname , \"wb\" ) try : fout . write ( html . encode ( 'ascii' , 'xmlcharrefreplace' ) ) finally : fout . close ( )", "predictions": ["setup partitioner to the output file ."], "references": ["write html to fname properly encoded ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 6247, "code": "def file hash ( self , source , cu ) : m = Hasher ( ) m . update ( source ) self . coverage . data . add to hash ( cu . filename , m ) return m . digest ( )", "predictions": ["create and return the digest for this registrar ."], "references": ["compute a hash that changes if the file needs to be re - reported ."], "bleu": 0.08019421212222273, "rouge_l": 0.15947712418300655}
{"id": 6248, "code": "def index file ( self ) : index tmpl = Templite ( data ( \"index.html\" ) , self . template globals ) self . totals = sum ( [ f [ 'nums' ] for f in self . files ] ) html = index tmpl . render ( { 'arcs' : self . arcs , 'extra css' : self . extra css , 'files' : self . files , 'totals' : self . totals , } ) if sys . version info < ( 3 , 0 ) : html = html . decode ( \"utf-8\" ) self . write html ( os . path . join ( self . directory , \"index.html\" ) , html ) self . status . write ( self . directory )", "predictions": ["init the ( . init method for default values ."], "references": ["write the index . html file for this report ."], "bleu": 0.15851165692617156, "rouge_l": 0.4}
{"id": 6249, "code": "def read ( self , directory ) : usable = False try : status file = os . path . join ( directory , self . STATUS FILE ) fstatus = open ( status file , \"rb\" ) try : status = pickle . load ( fstatus ) finally : fstatus . close ( ) except ( IO Error , Value Error ) : usable = False else : usable = True if status [ 'format' ] != self . STATUS FORMAT : usable = False elif status [ 'version' ] != coverage . version : usable = False if usable : self . files = status [ 'files' ] self . settings = status [ 'settings' ] else : self . reset ( )", "predictions": ["reads a , and returns the produced , if it exists ."], "references": ["read the last status in directory ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 6250, "code": "def write ( self , directory ) : status file = os . path . join ( directory , self . STATUS FILE ) status = { 'format' : self . STATUS FORMAT , 'version' : coverage . version , 'settings' : self . settings , 'files' : self . files , } fout = open ( status file , \"wb\" ) try : pickle . dump ( status , fout ) finally : fout . close ( )", "predictions": ["writes the stats to this object ."], "references": ["write the current status to directory ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 6251, "code": "def get slice ( seq , start = 0 , stop = None , step = 1 ) : if stop == None : stop = len ( seq ) item = lambda i : seq [ i ] return map ( item , xrange ( start , stop , step ) )", "predictions": ["return a session from a session ."], "references": ["get a slice of a sequence with variable step . specify start stop step ."], "bleu": 0.0704451546128839, "rouge_l": 0.2559440559440559}
{"id": 6252, "code": "def chop ( seq , size ) : chunk = lambda i : seq [ i : i + size ] return map ( chunk , xrange ( 0 , len ( seq ) , size ) )", "predictions": ["slices a sequence into a newly created chunk ."], "references": ["chop a sequence into chunks of the given size ."], "bleu": 0.24855227187657006, "rouge_l": 0.41709401709401706}
{"id": 6253, "code": "def read config ( ) : config = Config Parser . Config Parser ( ) config . read ( [ 'setup.cfg' ] ) if not config . has section ( 'check-manifest' ) : return if ( config . has option ( 'check-manifest' , 'ignore-default-rules' ) and config . getboolean ( 'check-manifest' , 'ignore-default-rules' ) ) : del IGNORE [ : ] if config . has option ( 'check-manifest' , 'ignore' ) : patterns = [ p . strip ( ) for p in config . get ( 'check-manifest' , 'ignore' ) . splitlines ( ) ] IGNORE . extend ( p for p in patterns if p )", "predictions": ["get the ( instance for each element in the ( ."], "references": ["read configuration from setup . cfg ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 6254, "code": "def file matches ( filename , patterns ) : return any ( fnmatch . fnmatch ( filename , pat ) for pat in patterns )", "predictions": ["indicates whether the provided disk partitions matches the specified patterns ."], "references": ["does this filename match any of the patterns?"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 6255, "code": "def get versioned files ( ) : encoding = 'UTF-8' if sys . platform == 'win32' else None output = run ( [ 'git' , 'ls-files' , '-z' ] , encoding = encoding ) return add directories ( output . split ( '\\0' ) [ : - 1 ] )", "predictions": ["get system users from the user agent ."], "references": ["list all files versioned by git in the current directory ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 6256, "code": "def start kernel ( self , * * kwargs ) : kernel id = unicode ( uuid . uuid4 ( ) ) km = self . kernel manager factory ( connection file = os . path . join ( self . connection dir , \"kernel-%s.json\" % kernel id ) , config = self . config , ) km . start kernel ( * * kwargs ) km . start channels ( shell = True , sub = False , stdin = False , hb = False ) self . kernels [ kernel id ] = km return kernel id", "predictions": ["starts the pid of this isdigit ."], "references": ["start a new kernel ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 6257, "code": "def notebook for kernel ( self , kernel id ) : notebook ids = [ k for k , v in self . notebook mapping . iteritems ( ) if v == kernel id ] if len ( notebook ids ) == 1 : return notebook ids [ 0 ] else : return None", "predictions": ["runs each short in this short ."], "references": ["return the notebook_id for a kernel_id or none ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 6258, "code": "def shutdown kernel ( self , kernel id ) : self . check kernel id ( kernel id ) super ( Mapping Kernel Manager , self ) . shutdown kernel ( kernel id ) self . delete mapping for kernel ( kernel id ) self . log . info ( \"Kernel shutdown: %s\" % kernel id )", "predictions": ["shuts down this regex and blocks until the implementation of the regex regex has been called ."], "references": ["shutdown a kernel and remove its notebook association ."], "bleu": 0.07994607499472013, "rouge_l": 0.1628838451268358}
{"id": 6259, "code": "def interrupt kernel ( self , kernel id ) : self . check kernel id ( kernel id ) super ( Mapping Kernel Manager , self ) . interrupt kernel ( kernel id ) self . log . info ( \"Kernel interrupted: %s\" % kernel id )", "predictions": ["file file file removes all others from top of the receiver ."], "references": ["interrupt a kernel ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 6260, "code": "def restart kernel ( self , kernel id ) : self . check kernel id ( kernel id ) km = self . get kernel ( kernel id ) km . restart kernel ( ) self . log . info ( \"Kernel restarted: %s\" % kernel id ) return kernel id notebook id = self . notebook for kernel ( kernel id ) new kernel id = self . start kernel ( ) self . kill kernel ( kernel id ) self . set kernel for notebook ( notebook id , new kernel id ) self . log . info ( \"Kernel restarted: %s\" % new kernel id ) return new kernel id", "predictions": ["update the ( ( ( ( or re : it removes the ( : passes : ( : top , ( : top , ( x : bottom : bottom : bottom : top , 2 : bottom : bottom : bottom : bottom : bottom : top : bottom"], "references": ["restart a kernel while keeping clients connected ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 6261, "code": "def create iopub stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create iopub stream ( kernel id )", "predictions": ["creates a new for a ( for a for the specified for the ( for both for and for the for this ( for example : for example : ( in ( in the for ( in ( in ( in ( in ( in ( in ( self ="], "references": ["create a new iopub stream ."], "bleu": 0.03162593967015063, "rouge_l": 0.08321964529331514}
{"id": 6262, "code": "def create shell stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create shell stream ( kernel id )", "predictions": ["start a cluster on a n ."], "references": ["create a new shell stream ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 6263, "code": "def create hb stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create hb stream ( kernel id )", "predictions": ["stop a ( or ) operation on a specific ) ."], "references": ["create a new hb stream ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 6264, "code": "def reset ( self ) : instdict = self . dict classdict = self . class . dict for mname , mval in classdict . items ( ) : if mname in instdict and isinstance ( mval , One Time Property ) : delattr ( self , mname )", "predictions": ["resets the full name to a new one ."], "references": ["reset all onetimeproperty attributes that may have fired already ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 6265, "code": "def ensure utf8 ( image tag ) : if py3compat . PY3 : return image tag def utf8 image tag ( * args , * * kwargs ) : s = image tag ( * args , * * kwargs ) if isinstance ( s , unicode ) : s = s . encode ( 'utf8' ) return s return utf8 image tag", "predictions": ["creates a body that either a body or empty p depending on whether the input ) is a body ."], "references": ["wrapper for ensuring image_tag returns utf8 - encoded str on python 2"], "bleu": 0.06108557268562171, "rouge_l": 0.06545064377682404}
{"id": 6266, "code": "def get unique or none ( klass , * args , * * kwargs ) : try : return klass . objects . get ( * args , * * kwargs ) except klass . Does Not Exist : return None except klass . Multiple Objects Returned : return None return None", "predictions": ["returns a partitioner or none if no arguments specified ."], "references": ["returns a unique instance of klass or none"], "bleu": 0.2086130724305753, "rouge_l": 0.4535315985130111}
{"id": 6267, "code": "def get query includes ( tokenized terms , search fields ) : query = None for term in tokenized terms : or query = None for field name in search fields : q = Q ( * * { \"%s icontains\" % field name : term } ) if or query is None : or query = q else : or query = or query | q if query is None : query = or query else : query = query & or query return query", "predictions": ["gets the gui from the given if available ."], "references": ["builds a query for included terms in a text search ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 6268, "code": "def get text query ( query string , search fields ) : include terms , exclude terms = get text tokenizer ( query string ) include q = get query includes ( include terms , search fields ) exclude q = get query excludes ( exclude terms , search fields ) query = None if include q and exclude q : query = include q & ~ exclude q elif not exclude q : query = include q else : query = ~ exclude q return query", "predictions": ["searches for a ( . . . . startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup startup . ( startup ("], "references": ["builds a query for both included & excluded terms in a text search ."], "bleu": 0.028577262451992175, "rouge_l": 0.10433295324971494}
{"id": 6269, "code": "def get date greater query ( days , date field ) : query = None days = get integer ( days ) if days : past = get days ago ( days ) query = Q ( * * { \"%s gte\" % date field : past . isoformat ( ) } ) return query", "predictions": ["run a ( or data not in the specified if the value does not have a value not correspond to the specified if the ( or null not . not a ( b not correspond to the value not be used not in the start of the value not a"], "references": ["query for if date_field is within number of days ago ."], "bleu": 0.028577262451992175, "rouge_l": 0.07411907654921021}
{"id": 6270, "code": "def get date less query ( days , date field ) : query = None days = get integer ( days ) if days : future = get days from now ( days ) query = Q ( * * { \"%s lte\" % date field : future . isoformat ( ) } ) return query", "predictions": ["returns ( or optionally overwrites = sensitivity = gcd = 0 , . . . = 0 = 0 = 0 = 0 = 0 = maximum = 0 , . . . . = maximum"], "references": ["query for if date_field is within number of days from now ."], "bleu": 0.03351542279475122, "rouge_l": 0.04579579579579579}
{"id": 6271, "code": "def get null or blank query ( field = None ) : if not field : return field null q = get null query ( field ) blank q = get blank query ( field ) return ( null q | blank q )", "predictions": ["gets a self - dimension : this is used to get the proper ( possibly - 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["query for null or blank field ."], "bleu": 0.02403051755364481, "rouge_l": 0.04061251664447404}
{"id": 6272, "code": "def case insensitive ( self , fields dict ) : if hasattr ( self . model , 'CASE INSENSITIVE FIELDS' ) : for field in self . model . CASE INSENSITIVE FIELDS : if field in fields dict : fields dict [ field + ' iexact' ] = fields dict [ field ] del fields dict [ field ]", "predictions": ["makes a dict with each of the ( . only the queries will be filled in ."], "references": ["converts queries to case insensitive for special fields ."], "bleu": 0.07994607499472013, "rouge_l": 0.1628838451268358}
{"id": 6273, "code": "def options ( self , parser , env ) : parser . add option ( \"-a\" , \"--attr\" , dest = \"attr\" , action = \"append\" , default = env . get ( 'NOSE ATTR' ) , metavar = \"ATTR\" , help = \"Run only tests that have attributes \" \"specified by ATTR [NOSE ATTR]\" ) if compat 24 : parser . add option ( \"-A\" , \"--eval-attr\" , dest = \"eval attr\" , metavar = \"EXPR\" , action = \"append\" , default = env . get ( 'NOSE EVAL ATTR' ) , help = \"Run only tests for whose attributes \" \"the Python expression EXPR evaluates \" \"to True [NOSE EVAL ATTR]\" )", "predictions": ["create a new ( object ."], "references": ["register command line options"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 6274, "code": "def want Method ( self , method ) : try : cls = method . im class except Attribute Error : return False return self . validate Attrib ( method , cls )", "predictions": ["convenience wrapper for _addmessagemethods() . forces method to want to be called multiple times ."], "references": ["accept the method if its attributes match ."], "bleu": 0.09103526405546068, "rouge_l": 0.18401206636500753}
{"id": 6275, "code": "def rotate ( self ) : if self . prev yank : text = self . ring . rotate ( ) if text : self . skip cursor = True cursor = self . text edit . text Cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , Qt Gui . Q Text Cursor . Keep Anchor , n = len ( self . prev yank ) ) cursor . insert Text ( text ) self . prev yank = text", "predictions": ["this method rotates the string such that it is executed before the call to the right ."], "references": ["rotate the kill ring then yank back the new top ."], "bleu": 0.0859076483566362, "rouge_l": 0.22289890377588306}
{"id": 6276, "code": "def start hb ( self , callback ) : if not self . beating : self . kernel alive = True def ping or dead ( ) : self . hb stream . flush ( ) if self . kernel alive : self . kernel alive = False self . hb stream . send ( b'ping' ) self . hb stream . flush ( ) else : try : callback ( ) except : pass finally : self . stop hb ( ) def beat received ( msg ) : self . kernel alive = True self . hb stream . on recv ( beat received ) loop = ioloop . IO Loop . instance ( ) self . hb periodic callback = ioloop . Periodic Callback ( ping or dead , self . time to dead * 1000 , loop ) loop . add timeout ( time . time ( ) + self . first beat , self . really start hb ) self . beating = True", "predictions": ["like the method but closes all the queued queued transitions ."], "references": ["start the heartbeating and call the callback if the kernel dies ."], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 6277, "code": "def stop hb ( self ) : if self . beating : self . beating = False self . hb periodic callback . stop ( ) if not self . hb stream . closed ( ) : self . hb stream . on recv ( None )", "predictions": ["stop the log operation ."], "references": ["stop the heartbeating and cancel all related callbacks ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 6278, "code": "def fload ( self ) : if hasattr ( self , 'fobj' ) and self . fobj is not None : self . fobj . close ( ) if hasattr ( self . src , \"read\" ) : self . fobj = self . src else : self . fobj = open ( self . fname )", "predictions": ["closes this class and removes all words from it ."], "references": ["load file object ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 6279, "code": "def reload ( self ) : self . fload ( ) self . src = self . fobj . read ( ) src b = [ b . strip ( ) for b in self . re stop . split ( self . src ) if b ] self . silent = [ bool ( self . re silent . findall ( b ) ) for b in src b ] self . auto = [ bool ( self . re auto . findall ( b ) ) for b in src b ] if self . auto all is None : self . auto all = bool ( self . re auto all . findall ( src b [ 0 ] ) ) else : self . auto all = bool ( self . auto all ) src blocks = [ ] auto strip = lambda s : self . re auto . sub ( '' , s ) for i , b in enumerate ( src b ) : if self . auto [ i ] : src blocks . append ( auto strip ( b ) ) else : src blocks . append ( b ) src blocks [ 0 ] = self . re auto all . sub ( '' , src blocks [ 0 ] ) self . nblocks = len ( src blocks ) self . src blocks = src blocks self . src blocks colored = map ( self . ip colorize , self . src blocks ) self . reset ( )", "predictions": ["copies the list of accept class files so that they can later be called multiple times ."], "references": ["reload source from disk and initialize state ."], "bleu": 0.07223943354597204, "rouge_l": 0.08555399719495091}
{"id": 6280, "code": "def show ( self , index = None ) : index = self . get index ( index ) if index is None : return print >> io . stdout , self . marquee ( % ( self . title , index , self . nblocks - index - 1 ) ) print >> io . stdout , ( self . src blocks colored [ index ] ) sys . stdout . flush ( )", "predictions": ["show this theme for the specified index ."], "references": ["show a single block on screen"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 6281, "code": "def show all ( self ) : fname = self . title title = self . title nblocks = self . nblocks silent = self . silent marquee = self . marquee for index , block in enumerate ( self . src blocks colored ) : if silent [ index ] : print >> io . stdout , marquee ( % ( title , index , nblocks - index - 1 ) ) else : print >> io . stdout , marquee ( % ( title , index , nblocks - index - 1 ) ) print >> io . stdout , block , sys . stdout . flush ( )", "predictions": ["show this object for all notifications of the specified class without blocking ."], "references": ["show entire demo on screen block by block"], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 6282, "code": "def reload ( self ) : self . fload ( ) lines = self . fobj . readlines ( ) src b = [ l for l in lines if l . strip ( ) ] nblocks = len ( src b ) self . src = '' . join ( lines ) self . silent = [ False ] * nblocks self . auto = [ True ] * nblocks self . auto all = True self . nblocks = nblocks self . src blocks = src b self . src blocks colored = map ( self . ip colorize , self . src blocks ) self . reset ( )", "predictions": ["copies from the class files"], "references": ["reload source from disk and initialize state ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 6283, "code": "def thread ( function , sequence , cores = None , run Series = False , quiet = False ) : if cores is None : pool = Thread Pool ( ) else : pool = Thread Pool ( cores ) tic = time . time ( ) if run Series is False : try : results = pool . map ( function , sequence ) pool . close ( ) pool . join ( ) except : print 'thread Failed... running in series :-(' results = series ( sequence , function ) else : results = series ( sequence , function ) toc = time . time ( ) elapsed = toc - tic if quiet is False : if cores is None : print \"Elapsed time: %s  :-)\\n\" % str ( elapsed ) else : print \"Elapsed time: %s  on %s threads :-)\\n\" % ( str ( elapsed ) , str ( cores ) ) return results", "predictions": ["runs all pending operations ."], "references": ["sets up the threadpool with map for parallel processing"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 6284, "code": "def with objattrs ( * names ) : def wrap ( func ) : @ functools . wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with contextlib . Exit Stack ( ) as stack : for name in names : stack . enter context ( getattr ( self , name ) ) return func ( self , * args , * * kwargs ) return wrapper return wrap", "predictions": ["decorator that ensures that the method has been objattrs ."], "references": ["like with_objattr but enter context one by one ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 6285, "code": "def countdown ( name , date , description = '' , id = '' , granularity = 'sec' , start = None , progressbar = False , progressbar inversed = False , showpct = False ) : end date = dateparse . parse datetime ( date ) end = dateformat . format ( end date , 'U' ) content = '<div class=\"name\">' + name + '</div>' content += '<div class=\"description\">' + description + '</div>' if progressbar : if not end : raise Exception ( 'For progressbar, start date is requried.' ) parsed date = datetime . datetime . combine ( dateparse . parse date ( start ) , datetime . time ( ) ) start date = dateparse . parse datetime ( start ) or parsed date now = datetime . datetime . now ( ) pct = ( now - start date ) . total seconds ( ) / ( end date - start date ) . total seconds ( ) pct = int ( pct * 100 ) if progressbar inversed : pct = 100 - pct bar = '<div class=\"progress progress-striped active\">' bar += '<div class=\"progress-bar\"  role=\"progressbar\" aria-valuenow=\"{pct}\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: {pct}%\">' bar += '<span class=\"sr-only\">{pct}% Complete</span>' bar += '</div>' bar += '</div>' if showpct : bar += '<div class=\"percentage\">{pct}%</div>' bar = bar . format ( pct = pct ) content += bar content += '<div class=\"counter\"></div>' attr = { 'class' : 'countdownbox' , 'data-datetime' : end , 'data-granularity' : granularity } if id : attr [ 'id' ] = id return html . tag ( 'div' , content , attr )", "predictions": ["creates a countdown for the given date and description ."], "references": ["create a countdown ."], "bleu": 0.17827531042796255, "rouge_l": 0.4644670050761421}
{"id": 6286, "code": "def cleanup ( controller , engines ) : import signal , time print ( 'Starting cleanup' ) print ( 'Stopping engines...' ) for e in engines : e . send signal ( signal . SIGINT ) print ( 'Stopping controller...' ) controller . send signal ( signal . SIGINT ) time . sleep ( 0.1 ) print ( 'Killing controller...' ) controller . kill ( ) print ( 'Cleanup done' )", "predictions": ["cleanup all running actions and delete all waiting commands"], "references": ["cleanup routine to shut down all subprocesses we opened ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 6287, "code": "def save ids ( f , self , * args , * * kwargs ) : n previous = len ( self . client . history ) try : ret = f ( self , * args , * * kwargs ) finally : nmsgs = len ( self . client . history ) - n previous msg ids = self . client . history [ - nmsgs : ] self . history . extend ( msg ids ) map ( self . outstanding . add , msg ids ) return ret", "predictions": ["saves the list of history ids to any other ( ."], "references": ["keep our history and outstanding attributes up to date after a method call ."], "bleu": 0.10312570678516415, "rouge_l": 0.2349165596919127}
{"id": 6288, "code": "def sync results ( f , self , * args , * * kwargs ) : ret = f ( self , * args , * * kwargs ) delta = self . outstanding . difference ( self . client . outstanding ) completed = self . outstanding . intersection ( delta ) self . outstanding = self . outstanding . difference ( completed ) return ret", "predictions": ["compatibility method for derivative ."], "references": ["sync relevant results from self . client to our results attribute ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 6289, "code": "def spin after ( f , self , * args , * * kwargs ) : ret = f ( self , * args , * * kwargs ) self . spin ( ) return ret", "predictions": ["the original method call . this method is called before the method of the method call . it sends a spin operation to the last one ."], "references": ["call spin after the method ."], "bleu": 0.07029695662739609, "rouge_l": 0.2738496071829405}
{"id": 6290, "code": "def add record ( self , msg id , rec ) : rec = self . binary buffers ( rec ) self . records . insert ( rec )", "predictions": ["records a new record ."], "references": ["add a new task record by msg_id ."], "bleu": 0.22405141222206199, "rouge_l": 0.5907990314769976}
{"id": 6291, "code": "def get record ( self , msg id ) : r = self . records . find one ( { 'msg id' : msg id } ) if not r : raise Key Error ( msg id ) return r", "predictions": ["get a record of the message ."], "references": ["get a specific task record by msg_id ."], "bleu": 0.240785655451027, "rouge_l": 0.5269978401727862}
{"id": 6292, "code": "def update record ( self , msg id , rec ) : rec = self . binary buffers ( rec ) self . records . update ( { 'msg id' : msg id } , { '$set' : rec } )", "predictions": ["updates an object to a new one or more objects ."], "references": ["update the data in an existing record ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 6293, "code": "def get history ( self ) : cursor = self . records . find ( { } , { 'msg id' : 1 } ) . sort ( 'submitted' ) return [ rec [ 'msg id' ] for rec in cursor ]", "predictions": ["get a list of history items ."], "references": ["get all msg_ids ordered by time submitted ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 6294, "code": "def get msgs ( self ) : msgs = [ ] while True : try : msgs . append ( self . get msg ( block = False ) ) except Empty : break return msgs", "predictions": ["get the jcombobox for this notebook ."], "references": ["get all messages that are currently ready ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 6295, "code": "def get msg ( self , block = True , timeout = None ) : return self . in queue . get ( block , timeout )", "predictions": ["get the log message for this object ."], "references": ["gets a message if there is one that is ready ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 6296, "code": "def parse ( url ) : config = { } if not isinstance ( url , six . string types ) : url = '' url = urlparse . urlparse ( url ) path = url . path [ 1 : ] path = path . split ( '?' , 2 ) [ 0 ] config . update ( { 'NAME' : path , 'USER' : url . username , 'PASSWORD' : url . password , 'HOST' : url . hostname , 'PORT' : url . port , } ) if url . scheme in SCHEMES : config [ 'ENGINE' ] = SCHEMES [ url . scheme ] return config", "predictions": ["parse . passed as . into . ."], "references": ["parses a database url ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 6297, "code": "def magic run completer ( self , event ) : comps = arg split ( event . line , strict = False ) relpath = ( len ( comps ) > 1 and comps [ - 1 ] or '' ) . strip ( \"'\\\"\" ) lglob = glob . glob isdir = os . path . isdir relpath , tilde expand , tilde val = expand user ( relpath ) dirs = [ f . replace ( '\\\\' , '/' ) + \"/\" for f in lglob ( relpath + '*' ) if isdir ( f ) ] if filter ( magic run re . match , comps ) : pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( '*' ) ] else : pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( relpath + '*.py' ) + lglob ( relpath + '*.ipy' ) + lglob ( relpath + '*.pyw' ) ] return [ compress user ( p , tilde expand , tilde val ) for p in dirs + pys ]", "predictions": ["run a loop . this does not return from any other sequence ."], "references": ["complete files that end in . py or . ipy for the %run command ."], "bleu": 0.09063677582644454, "rouge_l": 0.14104046242774565}
{"id": 6298, "code": "def cd completer ( self , event ) : ip = get ipython ( ) relpath = event . symbol if event . line . endswith ( '-b' ) or ' -b ' in event . line : bkms = self . db . get ( 'bookmarks' , None ) if bkms : return bkms . keys ( ) else : return [ ] if event . symbol == '-' : width dh = str ( len ( str ( len ( ip . user ns [ ' dh' ] ) + 1 ) ) ) fmt = '-%0' + width dh + 'd [%s]' ents = [ fmt % ( i , s ) for i , s in enumerate ( ip . user ns [ ' dh' ] ) ] if len ( ents ) > 1 : return ents return [ ] if event . symbol . startswith ( '--' ) : return [ \"--\" + os . path . basename ( d ) for d in ip . user ns [ ' dh' ] ] relpath , tilde expand , tilde val = expand user ( relpath ) relpath = relpath . replace ( '\\\\' , '/' ) found = [ ] for d in [ f . replace ( '\\\\' , '/' ) + '/' for f in glob . glob ( relpath + '*' ) if os . path . isdir ( f ) ] : if ' ' in d : raise Try Next found . append ( d ) if not found : if os . path . isdir ( relpath ) : return [ compress user ( relpath , tilde expand , tilde val ) ] bks = self . db . get ( 'bookmarks' , { } ) . iterkeys ( ) bkmatches = [ s for s in bks if s . startswith ( event . symbol ) ] if bkmatches : return bkmatches raise Try Next return [ compress user ( p , tilde expand , tilde val ) for p in found ]", "predictions": [". is the detected symbol of two layouts ."], "references": ["completer function for cd which only returns directories ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 6299, "code": "def quoteattr ( self , attr ) : attr = xml safe ( attr ) if isinstance ( attr , unicode ) and not UNICODE STRINGS : attr = attr . encode ( self . encoding ) return saxutils . quoteattr ( attr )", "predictions": ["print a attribute representation of an xml attribute ."], "references": ["escape an xml attribute . value can be unicode ."], "bleu": 0.3515059938464902, "rouge_l": 0.41709401709401706}
{"id": 6300, "code": "def configure ( self , options , config ) : Plugin . configure ( self , options , config ) self . config = config if self . enabled : self . stats = { 'errors' : 0 , 'failures' : 0 , 'passes' : 0 , 'skipped' : 0 } self . errorlist = [ ] self . error report file = codecs . open ( options . xunit file , 'w' , self . encoding , 'replace' )", "predictions": ["sets up the tools ."], "references": ["configures the xunit plugin ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 6301, "code": "def add Error ( self , test , err , capt = None ) : taken = self . time Taken ( ) if issubclass ( err [ 0 ] , Skip Test ) : type = 'skipped' self . stats [ 'skipped' ] += 1 else : type = 'error' self . stats [ 'errors' ] += 1 tb = '' . join ( traceback . format exception ( * err ) ) id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time=\"%(taken).3f\">' '<%(type)s type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</%(type)s></testcase>' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , 'type' : type , 'errtype' : self . quoteattr ( nice classname ( err [ 0 ] ) ) , 'message' : self . quoteattr ( exc message ( err ) ) , 'tb' : escape cdata ( tb ) , } )", "predictions": ["( : the class ( or statement ) : the test case is a sax operation ."], "references": ["add error output to xunit report ."], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 6302, "code": "def add Failure ( self , test , err , capt = None , tb info = None ) : taken = self . time Taken ( ) tb = '' . join ( traceback . format exception ( * err ) ) self . stats [ 'failures' ] += 1 id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time=\"%(taken).3f\">' '<failure type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</failure></testcase>' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , 'errtype' : self . quoteattr ( nice classname ( err [ 0 ] ) ) , 'message' : self . quoteattr ( exc message ( err ) ) , 'tb' : escape cdata ( tb ) , } )", "predictions": ["now we want to add ( and save them in one or more simple stats ."], "references": ["add failure output to xunit report ."], "bleu": 0.09147827112247602, "rouge_l": 0.1871165644171779}
{"id": 6303, "code": "def add Success ( self , test , capt = None ) : taken = self . time Taken ( ) self . stats [ 'passes' ] += 1 id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s ' 'time=\"%(taken).3f\" />' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , } )", "predictions": ["add a new processed request to the response ."], "references": ["add success output to xunit report ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 6304, "code": "def register engine ( self , uid ) : self . targets . insert ( 0 , uid ) self . loads . insert ( 0 , 0 ) self . completed [ uid ] = set ( ) self . failed [ uid ] = set ( ) self . pending [ uid ] = { } self . update graph ( None )", "predictions": ["registers a step class ."], "references": ["new engine with ident uid became available ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 6305, "code": "def unregister engine ( self , uid ) : if len ( self . targets ) == 1 : pass self . engine stream . flush ( ) idx = self . targets . index ( uid ) self . targets . pop ( idx ) self . loads . pop ( idx ) if self . pending [ uid ] : dc = ioloop . Delayed Callback ( lambda : self . handle stranded tasks ( uid ) , 5000 , self . loop ) dc . start ( ) else : self . completed . pop ( uid ) self . failed . pop ( uid )", "predictions": ["removes a previously registered ( event ) instance ."], "references": ["existing engine with ident uid became unavailable ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 6306, "code": "def handle stranded tasks ( self , engine ) : lost = self . pending [ engine ] for msg id in lost . keys ( ) : if msg id not in self . pending [ engine ] : continue raw msg = lost [ msg id ] . raw msg idents , msg = self . session . feed identities ( raw msg , copy = False ) parent = self . session . unpack ( msg [ 1 ] . bytes ) idents = [ engine , idents [ 0 ] ] try : raise error . Engine Error ( \"Engine %r died while running task %r\" % ( engine , msg id ) ) except : content = error . wrap exception ( ) header = dict ( status = 'error' , engine = engine , date = datetime . now ( ) , ) msg = self . session . msg ( 'apply reply' , content , parent = parent , subheader = header ) raw reply = map ( zmq . Message , self . session . serialize ( msg , ident = idents ) ) self . dispatch result ( raw reply ) self . completed . pop ( engine ) self . failed . pop ( engine )", "predictions": ["handles creating and notifies this message ."], "references": ["deal with jobs resident in an engine that died ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 6307, "code": "def audit timeouts ( self ) : now = datetime . now ( ) for msg id in self . depending . keys ( ) : if msg id in self . depending : job = self . depending [ msg id ] if job . timeout and job . timeout < now : self . fail unreachable ( msg id , error . Task Timeout )", "predictions": ["emits a landscape ( possibly re - blocking : it has been run : each move move - off : once per second ."], "references": ["audit all waiting tasks for expired timeouts ."], "bleu": 0.050661968099322066, "rouge_l": 0.06869369369369369}
{"id": 6308, "code": "def maybe run ( self , job ) : msg id = job . msg id self . log . debug ( \"Attempting to assign task %s\" , msg id ) if not self . targets : return False if job . follow or job . targets or job . blacklist or self . hwm : def can run ( idx ) : if self . hwm and self . loads [ idx ] == self . hwm : return False target = self . targets [ idx ] if target in job . blacklist : return False if job . targets and target not in job . targets : return False return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) indices = filter ( can run , range ( len ( self . targets ) ) ) if not indices : if job . follow . all : dests = set ( ) relevant = set ( ) if job . follow . success : relevant = self . all completed if job . follow . failure : relevant = relevant . union ( self . all failed ) for m in job . follow . intersection ( relevant ) : dests . add ( self . destinations [ m ] ) if len ( dests ) > 1 : self . depending [ msg id ] = job self . fail unreachable ( msg id ) return False if job . targets : job . targets . difference update ( job . blacklist ) if not job . targets or not job . targets . intersection ( self . targets ) : self . depending [ msg id ] = job self . fail unreachable ( msg id ) return False return False else : indices = None self . submit task ( job , indices ) return True", "predictions": ["first run method . this is called by the run method ."], "references": ["check location dependencies and run if they are met ."], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 6309, "code": "def save unmet ( self , job ) : msg id = job . msg id self . depending [ msg id ] = job for dep id in job . after . union ( job . follow ) . difference ( self . all done ) : if dep id not in self . graph : self . graph [ dep id ] = set ( ) self . graph [ dep id ] . add ( msg id )", "predictions": ["saves this : once a group that has been saved ."], "references": ["save a message for later submission when its dependencies are met ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 6310, "code": "def submit task ( self , job , indices = None ) : if indices : loads = [ self . loads [ i ] for i in indices ] else : loads = self . loads idx = self . scheme ( loads ) if indices : idx = indices [ idx ] target = self . targets [ idx ] self . engine stream . send ( target , flags = zmq . SNDMORE , copy = False ) self . engine stream . send multipart ( job . raw msg , copy = False ) self . add job ( idx ) self . pending [ target ] [ job . msg id ] = job content = dict ( msg id = job . msg id , engine id = target . decode ( 'ascii' ) ) self . session . send ( self . mon stream , 'task destination' , content = content , ident = [ b'tracktask' , self . ident ] )", "predictions": ["submits a ( representing the classifier and notifies the if it is already in a ( i . e . if the ( and the if it is in a ( see also and empty and target and target and target and . and . and . and . and"], "references": ["submit a task to any of a subset of our targets ."], "bleu": 0.028577262451992175, "rouge_l": 0.10879904875148634}
{"id": 6311, "code": "def dispatch result ( self , raw msg ) : try : idents , msg = self . session . feed identities ( raw msg , copy = False ) msg = self . session . unserialize ( msg , content = False , copy = False ) engine = idents [ 0 ] try : idx = self . targets . index ( engine ) except Value Error : pass else : self . finish job ( idx ) except Exception : self . log . error ( \"task::Invaid result: %r\" , raw msg , exc info = True ) return header = msg [ 'header' ] parent = msg [ 'parent header' ] if header . get ( 'dependencies met' , True ) : success = ( header [ 'status' ] == 'ok' ) msg id = parent [ 'msg id' ] retries = self . retries [ msg id ] if not success and retries > 0 : self . retries [ msg id ] = retries - 1 self . handle unmet dependency ( idents , parent ) else : del self . retries [ msg id ] self . handle result ( idents , parent , raw msg , success ) self . mon stream . send multipart ( [ b'outtask' ] + raw msg , copy = False ) else : self . handle unmet dependency ( idents , parent )", "predictions": ["now we want to reload this method will reload the ( src src src src src src src src src src src src src src src src src src src src src src src src src src . tools . tools . tools . tools . tools . tools ."], "references": ["dispatch method for result replies"], "bleu": 0.02403051755364481, "rouge_l": 0.04265734265734266}
{"id": 6312, "code": "def handle result ( self , idents , parent , raw msg , success = True ) : engine = idents [ 0 ] client = idents [ 1 ] raw msg [ : 2 ] = [ client , engine ] self . client stream . send multipart ( raw msg , copy = False ) msg id = parent [ 'msg id' ] self . pending [ engine ] . pop ( msg id ) if success : self . completed [ engine ] . add ( msg id ) self . all completed . add ( msg id ) else : self . failed [ engine ] . add ( msg id ) self . all failed . add ( msg id ) self . all done . add ( msg id ) self . destinations [ msg id ] = engine self . update graph ( msg id , success )", "predictions": ["sends a ( possibly after the shopping notification is received is sent to the contents of the immediately ."], "references": ["handle a real task result either success or failure"], "bleu": 0.06439931429457924, "rouge_l": 0.07634543178973717}
{"id": 6313, "code": "def handle unmet dependency ( self , idents , parent ) : engine = idents [ 0 ] msg id = parent [ 'msg id' ] job = self . pending [ engine ] . pop ( msg id ) job . blacklist . add ( engine ) if job . blacklist == job . targets : self . depending [ msg id ] = job self . fail unreachable ( msg id ) elif not self . maybe run ( job ) : if msg id not in self . all failed : self . save unmet ( job ) if self . hwm : try : idx = self . targets . index ( engine ) except Value Error : pass else : if self . loads [ idx ] == self . hwm - 1 : self . update graph ( None )", "predictions": ["show this src and notifies the src src ."], "references": ["handle an unmet dependency"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 6314, "code": "def logstate ( self ) : if self . logfile is None : print 'Logging has not been activated.' else : state = self . log active and 'active' or 'temporarily suspended' print 'Filename       :' , self . logfname print 'Mode           :' , self . logmode print 'Output logging :' , self . log output print 'Raw input log  :' , self . log raw input print 'Timestamping   :' , self . timestamp print 'State          :' , state", "predictions": ["a helper method to get the aid ] of the controller ."], "references": ["print a status message about the logger ."], "bleu": 0.1235622127262679, "rouge_l": 0.3112244897959184}
{"id": 6315, "code": "def log write ( self , data , kind = 'input' ) : if self . log active and data : write = self . logfile . write if kind == 'input' : if self . timestamp : write ( str to unicode ( time . strftime ( , time . localtime ( ) ) ) ) write ( data ) elif kind == 'output' and self . log output : odata = u'\\n' . join ( [ % s for s in data . splitlines ( ) ] ) write ( u'%s\\n' % odata ) self . logfile . flush ( )", "predictions": ["thread safe way to ( what happens as well ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) . . . . ) . ."], "references": ["write data to the log file if active"], "bleu": 0.02403051755364481, "rouge_l": 0.03966189856957088}
{"id": 6316, "code": "def new worksheet ( name = None , cells = None ) : ws = Notebook Node ( ) if name is not None : ws . name = unicode ( name ) if cells is None : ws . cells = [ ] else : ws . cells = list ( cells ) return ws", "predictions": ["creates a new wrap node"], "references": ["create a worksheet by name with with a list of cells ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 6317, "code": "def new notebook ( metadata = None , worksheets = None ) : nb = Notebook Node ( ) nb . nbformat = 2 if worksheets is None : nb . worksheets = [ ] else : nb . worksheets = list ( worksheets ) if metadata is None : nb . metadata = new metadata ( ) else : nb . metadata = Notebook Node ( metadata ) return nb", "predictions": ["generate a countdown node"], "references": ["create a notebook by name id and a list of worksheets ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 6318, "code": "def add s ( self , s , obj , priority = 0 ) : chain = self . strs . get ( s , Command Chain Dispatcher ( ) ) chain . add ( obj , priority ) self . strs [ s ] = chain", "predictions": ["cleanup translated results . does not added to the object ."], "references": ["adds a target string for dispatching"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 6319, "code": "def add re ( self , regex , obj , priority = 0 ) : chain = self . regexs . get ( regex , Command Chain Dispatcher ( ) ) chain . add ( obj , priority ) self . regexs [ regex ] = chain", "predictions": ["creates a sub - method model at the specified results ."], "references": ["adds a target regexp for dispatching"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 6320, "code": "def dispatch ( self , key ) : if key in self . strs : yield self . strs [ key ] for r , obj in self . regexs . items ( ) : if re . match ( r , key ) : yield obj else : pass", "predictions": ["sync a series of tokens ."], "references": ["get a seq of commandchain objects that match key"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 6321, "code": "def flat matches ( self , key ) : for val in self . dispatch ( key ) : for el in val : yield el [ 1 ] return", "predictions": ["a method that iterates over all the items of a specific self ."], "references": ["yield all value targets without priority"], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 6322, "code": "def notebook dir changed ( self , name , old , new ) : if os . path . exists ( new ) and not os . path . isdir ( new ) : raise Trait Error ( \"notebook dir %r is not a directory\" % new ) if not os . path . exists ( new ) : self . log . info ( \"Creating notebook dir %s\" , new ) try : os . mkdir ( new ) except : raise Trait Error ( \"Couldn't create notebook dir %r\" % new )", "predictions": ["create the add add to the add add directory ."], "references": ["do a bit of validation of the notebook dir"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 6323, "code": "def new notebook id ( self , name ) : #notebook id = unicode(uuid.uuid5(uuid.NAMESPACE URL, notebook id = unicode ( uuid . uuid4 ( ) ) self . mapping [ notebook id ] = name self . rev mapping [ name ] = notebook id return notebook id", "predictions": ["creates a get record for the record ."], "references": ["generate a new notebook_id for a name and store its mappings ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 6324, "code": "def delete notebook id ( self , notebook id ) : name = self . mapping [ notebook id ] del self . mapping [ notebook id ] del self . rev mapping [ name ]", "predictions": ["deletes a record with the given name and its corresponding record ."], "references": ["delete a notebook s id only . this doesn t delete the actual notebook ."], "bleu": 0.09623034802925295, "rouge_l": 0.21785714285714283}
{"id": 6325, "code": "def notebook exists ( self , notebook id ) : if notebook id not in self . mapping : return False path = self . get path by name ( self . mapping [ notebook id ] ) return os . path . isfile ( path )", "predictions": ["determines if the get get is an } file ."], "references": ["does a notebook exist?"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 6326, "code": "def find path ( self , notebook id ) : try : name = self . mapping [ notebook id ] except Key Error : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) return self . get path by name ( name )", "predictions": ["look for the : path or ( ."], "references": ["return a full path to a notebook given its notebook_id ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 6327, "code": "def get path by name ( self , name ) : filename = name + self . filename ext path = os . path . join ( self . notebook dir , filename ) return path", "predictions": ["generates a filename for the given self ."], "references": ["return a full path to a notebook given its name ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 6328, "code": "def get notebook ( self , notebook id , format = u'json' ) : format = unicode ( format ) if format not in self . allowed formats : raise web . HTTP Error ( 415 , u'Invalid notebook format: %s' % format ) last modified , nb = self . get notebook object ( notebook id ) kwargs = { } if format == 'json' : kwargs [ 'split lines' ] = False data = current . writes ( nb , format , * * kwargs ) name = nb . metadata . get ( 'name' , 'notebook' ) return last modified , name , data", "predictions": ["generate a ( or a ( . if not already exist if not already in the ( ( . if not found if not found if not found if the ( if not found if the ( if not found if not already exist if not found if the ("], "references": ["get the representation of a notebook in format by notebook_id ."], "bleu": 0.030216776104535565, "rouge_l": 0.11117861482381533}
{"id": 6329, "code": "def get notebook object ( self , notebook id ) : path = self . find path ( notebook id ) if not os . path . isfile ( path ) : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) info = os . stat ( path ) last modified = datetime . datetime . utcfromtimestamp ( info . st mtime ) with open ( path , 'r' ) as f : s = f . read ( ) try : nb = current . reads ( s , u'json' ) except : raise web . HTTP Error ( 500 , u'Unreadable JSON notebook.' ) nb . metadata . name = os . path . splitext ( os . path . basename ( path ) ) [ 0 ] return last modified , nb", "predictions": ["returns the path of the run run ."], "references": ["get the notebooknode representation of a notebook by notebook_id ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 6330, "code": "def save notebook ( self , notebook id , data , name = None , format = u'json' ) : if format not in self . allowed formats : raise web . HTTP Error ( 415 , u'Invalid notebook format: %s' % format ) try : nb = current . reads ( data . decode ( 'utf-8' ) , format ) except : raise web . HTTP Error ( 400 , u'Invalid JSON data' ) if name is not None : nb . metadata . name = name self . save notebook object ( notebook id , nb )", "predictions": ["saves this completer in . to the given completer ."], "references": ["save an existing notebook by notebook_id ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 6331, "code": "def save notebook object ( self , notebook id , nb ) : if notebook id not in self . mapping : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) old name = self . mapping [ notebook id ] try : new name = nb . metadata . name except Attribute Error : raise web . HTTP Error ( 400 , u'Missing notebook name' ) path = self . get path by name ( new name ) try : with open ( path , 'w' ) as f : current . write ( nb , f , u'json' ) except Exception as e : raise web . HTTP Error ( 400 , u'Unexpected error while saving notebook: %s' % e ) if self . save script : pypath = os . path . splitext ( path ) [ 0 ] + '.py' try : with io . open ( pypath , 'w' , encoding = 'utf-8' ) as f : current . write ( nb , f , u'py' ) except Exception as e : raise web . HTTP Error ( 400 , u'Unexpected error while saving notebook as script: %s' % e ) if old name != new name : old path = self . get path by name ( old name ) if os . path . isfile ( old path ) : os . unlink ( old path ) if self . save script : old pypath = os . path . splitext ( old path ) [ 0 ] + '.py' if os . path . isfile ( old pypath ) : os . unlink ( old pypath ) self . mapping [ notebook id ] = new name self . rev mapping [ new name ] = notebook id del self . rev mapping [ old name ]", "predictions": ["writes an self ( . , . , . , . xml xml xml xml xml xml xml file xml xml file xml file xml xml file xml ."], "references": ["save an existing notebook object by notebook_id ."], "bleu": 0.0462136266712202, "rouge_l": 0.12043435340572554}
{"id": 6332, "code": "def delete notebook ( self , notebook id ) : path = self . find path ( notebook id ) if not os . path . isfile ( path ) : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) os . unlink ( path ) self . delete notebook id ( notebook id )", "predictions": ["deletes a file . deletes all associated ( or directory ) and removes the associated ( if found ."], "references": ["delete notebook by notebook_id ."], "bleu": 0.06439931429457924, "rouge_l": 0.09312977099236641}
{"id": 6333, "code": "def new notebook ( self ) : path , name = self . increment filename ( 'Untitled' ) notebook id = self . new notebook id ( name ) metadata = current . new metadata ( name = name ) nb = current . new notebook ( metadata = metadata ) with open ( path , 'w' ) as f : current . write ( nb , f , u'json' ) return notebook id", "predictions": ["creates a add a add notebook ."], "references": ["create a new notebook and return its notebook_id ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 6334, "code": "def copy notebook ( self , notebook id ) : last mod , nb = self . get notebook object ( notebook id ) name = nb . metadata . name + '-Copy' path , name = self . increment filename ( name ) nb . metadata . name = name notebook id = self . new notebook id ( name ) self . save notebook object ( notebook id , nb ) return notebook id", "predictions": ["copies this notebook to the notebook and closes the notebook ."], "references": ["copy an existing notebook and return its notebook_id ."], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 6335, "code": "def make report ( self , traceback ) : sec sep = self . section sep report = [ super ( IP App Crash Handler , self ) . make report ( traceback ) ] rpt add = report . append try : rpt add ( sec sep + \"History of session input:\" ) for line in self . app . shell . user ns [ ' ih' ] : rpt add ( line ) rpt add ( '\\n*** Last line of input (may not be in above history):\\n' ) rpt add ( self . app . shell . last input line + '\\n' ) except : pass return '' . join ( report )", "predictions": ["add a report to the controller ."], "references": ["return a string containing a crash report ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 6336, "code": "def classes default ( self ) : return [ Interactive Shell App , self . class , Terminal Interactive Shell , Prompt Manager , History Manager , Profile Dir , Plain Text Formatter , IP Completer , Script Magics , ]", "predictions": ["this method creates a simple default classes for , ."], "references": ["this has to be in a method for terminalipythonapp to be available ."], "bleu": 0.12290460988295328, "rouge_l": 0.33983286908078}
{"id": 6337, "code": "def parse command line ( self , argv = None ) : argv = sys . argv [ 1 : ] if argv is None else argv if '-pylab' in argv : argv = argv [ : ] idx = argv . index ( '-pylab' ) warn . warn ( \"`-pylab` flag has been deprecated.\\n\" \"    Use `--pylab` instead, or `--pylab=foo` to specify a backend.\" ) sub = '--pylab' if len ( argv ) > idx + 1 : gui = argv [ idx + 1 ] if gui in ( 'wx' , 'qt' , 'qt4' , 'gtk' , 'auto' ) : sub = '--pylab=' + gui argv . pop ( idx + 1 ) argv [ idx ] = sub return super ( Terminal I Python App , self ) . parse command line ( argv )", "predictions": ["parse this command line ."], "references": ["override to allow old - pylab flag with deprecation warning"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 6338, "code": "def initialize ( self , argv = None ) : super ( Terminal I Python App , self ) . initialize ( argv ) if self . subapp is not None : return if not self . ignore old config : check for old config ( self . ipython dir ) if self . extra args and not self . something to run : self . file to run = self . extra args [ 0 ] self . init path ( ) self . init shell ( ) self . init banner ( ) self . init gui pylab ( ) self . init extensions ( ) self . init code ( )", "predictions": ["initialize the module object ."], "references": ["do actions after construct but before starting the app ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 6339, "code": "def init shell ( self ) : self . shell = Terminal Interactive Shell . instance ( config = self . config , display banner = False , profile dir = self . profile dir , ipython dir = self . ipython dir ) self . shell . configurables . append ( self )", "predictions": ["initialize the command line options ."], "references": ["initialize the interactiveshell instance"], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 6340, "code": "def init banner ( self ) : if self . display banner and self . interact : self . shell . show banner ( ) if self . log level <= logging . INFO : print", "predictions": ["initialize the full shell output ."], "references": ["optionally display the banner"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 6341, "code": "def pylab changed ( self , name , old , new ) : if new == 'inline' : warn . warn ( \"'inline' not available as pylab backend, \" \"using 'auto' instead.\\n\" ) self . pylab = 'auto'", "predictions": ["what is only used when the player is changed ."], "references": ["replace -- pylab = inline with -- pylab = auto"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 6342, "code": "def trait metadata ( self , traitname , key ) : try : trait = getattr ( self . class , traitname ) except Attribute Error : raise Trait Error ( \"Class %s does not have a trait named %s\" % ( self . class . name , traitname ) ) else : return trait . get metadata ( key )", "predictions": ["get metadata for a trait ."], "references": ["get metadata values for trait by key ."], "bleu": 0.24771976691208875, "rouge_l": 0.6963470319634703}
{"id": 6343, "code": "def validate ( self , obj , value ) : try : if issubclass ( value , self . klass ) : return value except : if ( value is None ) and ( self . allow none ) : return value self . error ( obj , value )", "predictions": ["validates that there is a parameter to the configuration given by the value ."], "references": ["validates that the value is a valid object instance ."], "bleu": 0.1767874865365185, "rouge_l": 0.42957746478873243}
{"id": 6344, "code": "def info ( self ) : if isinstance ( self . klass , basestring ) : klass = self . klass else : klass = self . klass . name result = 'a subclass of ' + klass if self . allow none : return result + ' or None' return result", "predictions": ["attach to a subclass ."], "references": ["returns a description of the trait ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 6345, "code": "def info ( self ) : result = 'any of ' + repr ( self . values ) if self . allow none : return result + ' or None' return result", "predictions": ["get a representation of this method ."], "references": ["returns a description of the trait ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 6346, "code": "def check ( self , completed , failed = None ) : if len ( self ) == 0 : return True against = set ( ) if self . success : against = completed if failed is not None and self . failure : against = against . union ( failed ) if self . all : return self . issubset ( against ) else : return not self . isdisjoint ( against )", "predictions": ["\" check \" conflicts ."], "references": ["check whether our dependencies have been met ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 6347, "code": "def unreachable ( self , completed , failed = None ) : if len ( self ) == 0 : return False against = set ( ) if not self . success : against = completed if failed is not None and not self . failure : against = against . union ( failed ) if self . all : return not self . isdisjoint ( against ) else : return self . issubset ( against )", "predictions": ["returns the \" completed \" within this stream ."], "references": ["return whether this dependency has become impossible ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 6348, "code": "def as dict ( self ) : return dict ( dependencies = list ( self ) , all = self . all , success = self . success , failure = self . failure )", "predictions": ["returns a dict representation of this object ."], "references": ["represent this dependency as a dict . for json compatibility ."], "bleu": 0.16481400866629634, "rouge_l": 0.3070469798657718}
{"id": 6349, "code": "def Ainv ( self ) : if not hasattr ( self , ' Ainv' ) : self . Ainv = self . Solver ( self . A ) return self . Ainv", "predictions": ["a gstring variant of this class ."], "references": ["returns a solver instance"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 6350, "code": "def Ainv ( self ) : if getattr ( self , ' Ainv' , None ) is None : self . Ainv = self . Solver ( self . A , 13 ) self . Ainv . run pardiso ( 12 ) return self . Ainv", "predictions": ["this is a utility method for finding the processed latlng of this class ."], "references": ["returns a solver instance"], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 6351, "code": "def depth ( n , tree ) : d = 0 parent = tree [ n ] while parent is not None : d += 1 parent = tree [ parent ] return d", "predictions": ["depth first search of g ."], "references": ["get depth of an element in the tree"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 6352, "code": "def print bintree ( tree , indent = '  ' ) : for n in sorted ( tree . keys ( ) ) : print \"%s%s\" % ( indent * depth ( n , tree ) , n )", "predictions": ["print all possible indentation ."], "references": ["print a binary tree"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 6353, "code": "def disambiguate dns url ( url , location ) : if not ip pat . match ( location ) : location = socket . gethostbyname ( location ) return disambiguate url ( url , location )", "predictions": ["get the dns url by location . this will only be used for other reasons ."], "references": ["accept either ip address or dns name and return ip"], "bleu": 0.07692375026049747, "rouge_l": 0.08026315789473684}
{"id": 6354, "code": "def allreduce ( self , f , value , flat = True ) : return self . reduce ( f , value , flat = flat , all = True )", "predictions": ["this is a convenience method for . . it simply wraps the value of the method to avoid adding the value of the value to the value of the value ."], "references": ["parallel reduce followed by broadcast of the result"], "bleu": 0.0513487742994337, "rouge_l": 0.1147695202257761}
{"id": 6355, "code": "def validate targets ( self , targets ) : if targets is None : return self . ids if isinstance ( targets , ( int , str , unicode ) ) : targets = [ targets ] targets = [ ] for t in targets : if isinstance ( t , ( str , unicode ) ) : t = self . by ident . get ( cast bytes ( t ) , t ) targets . append ( t ) targets = targets bad targets = [ t for t in targets if t not in self . ids ] if bad targets : raise Index Error ( \"No Such Engine: %r\" % bad targets ) if not targets : raise Index Error ( \"No Engines Registered\" ) return targets", "predictions": ["( - specific check to ensure they have a single member of the list ."], "references": ["turn any valid targets argument into a list of integer ids"], "bleu": 0.09782375748961449, "rouge_l": 0.1582360570687419}
{"id": 6356, "code": "def dispatch query ( self , msg ) : try : idents , msg = self . session . feed identities ( msg ) except Value Error : idents = [ ] if not idents : self . log . error ( \"Bad Query Message: %r\" , msg ) return client id = idents [ 0 ] try : msg = self . session . unserialize ( msg , content = True ) except Exception : content = error . wrap exception ( ) self . log . error ( \"Bad Query Message: %r\" , msg , exc info = True ) self . session . send ( self . query , \"hub error\" , ident = client id , content = content ) return #switch on message type: msg type = msg [ 'header' ] [ 'msg type' ] self . log . info ( \"client::client %r requested %r\" , client id , msg type ) handler = self . query handlers . get ( msg type , None ) try : assert handler is not None , \"Bad Message Type: %r\" % msg type except : content = error . wrap exception ( ) self . log . error ( \"Bad Message Type: %r\" , msg type , exc info = True ) self . session . send ( self . query , \"hub error\" , ident = client id , content = content ) return else : handler ( idents , msg )", "predictions": ["note : this will only be called once for processing ."], "references": ["route registration requests and queries from clients ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 6357, "code": "def save task request ( self , idents , msg ) : client id = idents [ 0 ] try : msg = self . session . unserialize ( msg ) except Exception : self . log . error ( \"task::client %r sent invalid task message: %r\" , client id , msg , exc info = True ) return record = init record ( msg ) record [ 'client uuid' ] = client id . decode ( 'ascii' ) record [ 'queue' ] = 'task' header = msg [ 'header' ] msg id = header [ 'msg id' ] self . pending . add ( msg id ) self . unassigned . add ( msg id ) try : existing = self . db . get record ( msg id ) if existing [ 'resubmitted' ] : for key in ( 'submitted' , 'client uuid' , 'buffers' ) : record . pop ( key ) for key , evalue in existing . iteritems ( ) : if key . endswith ( 'buffers' ) : continue rvalue = record . get ( key , None ) if evalue and rvalue and evalue != rvalue : self . log . warn ( \"conflicting initial state for record: %r:%r <%r> %r\" , msg id , rvalue , key , evalue ) elif evalue and not rvalue : record [ key ] = evalue try : self . db . update record ( msg id , record ) except Exception : self . log . error ( \"DB Error updating record %r\" , msg id , exc info = True ) except Key Error : try : self . db . add record ( msg id , record ) except Exception : self . log . error ( \"DB Error adding record %r\" , msg id , exc info = True ) except Exception : self . log . error ( \"DB Error saving task request %r\" , msg id , exc info = True )", "predictions": ["for each task that can be started ."], "references": ["save the submission of a task ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 6358, "code": "def save task result ( self , idents , msg ) : client id = idents [ 0 ] try : msg = self . session . unserialize ( msg ) except Exception : self . log . error ( \"task::invalid task result message send to %r: %r\" , client id , msg , exc info = True ) return parent = msg [ 'parent header' ] if not parent : self . log . warn ( \"Task %r had no parent!\" , msg ) return msg id = parent [ 'msg id' ] if msg id in self . unassigned : self . unassigned . remove ( msg id ) header = msg [ 'header' ] engine uuid = header . get ( 'engine' , u'' ) eid = self . by ident . get ( cast bytes ( engine uuid ) , None ) status = header . get ( 'status' , None ) if msg id in self . pending : self . log . info ( \"task::task %r finished on %s\" , msg id , eid ) self . pending . remove ( msg id ) self . all completed . add ( msg id ) if eid is not None : if status != 'aborted' : self . completed [ eid ] . append ( msg id ) if msg id in self . tasks [ eid ] : self . tasks [ eid ] . remove ( msg id ) completed = header [ 'date' ] started = header . get ( 'started' , None ) result = { 'result header' : header , 'result content' : msg [ 'content' ] , 'started' : started , 'completed' : completed , 'received' : datetime . now ( ) , 'engine uuid' : engine uuid , } result [ 'result buffers' ] = msg [ 'buffers' ] try : self . db . update record ( msg id , result ) except Exception : self . log . error ( \"DB Error saving task request %r\" , msg id , exc info = True ) else : self . log . debug ( \"task::unknown task %r finished\" , msg id )", "predictions": ["now we need to save the immediately before invoking the batch of the batch . note that the immediately will be on the next batch of the batch ."], "references": ["save the result of a completed task ."], "bleu": 0.062443859512347225, "rouge_l": 0.2408687068114511}
{"id": 6359, "code": "def save iopub message ( self , topics , msg ) : try : msg = self . session . unserialize ( msg , content = True ) except Exception : self . log . error ( \"iopub::invalid IO Pub message\" , exc info = True ) return parent = msg [ 'parent header' ] if not parent : self . log . warn ( \"iopub::IO Pub message lacks parent: %r\" , msg ) return msg id = parent [ 'msg id' ] msg type = msg [ 'header' ] [ 'msg type' ] content = msg [ 'content' ] try : rec = self . db . get record ( msg id ) except Key Error : rec = empty record ( ) rec [ 'msg id' ] = msg id self . db . add record ( msg id , rec ) d = { } if msg type == 'stream' : name = content [ 'name' ] s = rec [ name ] or '' d [ name ] = s + content [ 'data' ] elif msg type == 'pyerr' : d [ 'pyerr' ] = content elif msg type == 'pyin' : d [ 'pyin' ] = content [ 'code' ] elif msg type in ( 'display data' , 'pyout' ) : d [ msg type ] = content elif msg type == 'status' : pass else : self . log . warn ( \"unhandled iopub msg type: %r\" , msg type ) if not d : return try : self . db . update record ( msg id , d ) except Exception : self . log . error ( \"DB Error saving iopub message %r\" , msg id , exc info = True )", "predictions": ["construct iopub ( log ) and the notify notify the failures ."], "references": ["save an iopub message into the db"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 6360, "code": "def connection request ( self , client id , msg ) : self . log . info ( \"client::client %r connected\" , client id ) content = dict ( status = 'ok' ) content . update ( self . client info ) jsonable = { } for k , v in self . keytable . iteritems ( ) : if v not in self . dead engines : jsonable [ str ( k ) ] = v . decode ( 'ascii' ) content [ 'engines' ] = jsonable self . session . send ( self . query , 'connection reply' , content , parent = msg , ident = client id )", "predictions": ["we need to send an http connection to the client ."], "references": ["reply with connection addresses for clients ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 6361, "code": "def register engine ( self , reg , msg ) : content = msg [ 'content' ] try : queue = cast bytes ( content [ 'queue' ] ) except Key Error : self . log . error ( \"registration::queue not specified\" , exc info = True ) return heart = content . get ( 'heartbeat' , None ) if heart : heart = cast bytes ( heart ) \"\"\"register a new engine, and create the socket(s) necessary\"\"\" eid = self . next id self . log . debug ( \"registration::register engine(%i, %r, %r, %r)\" , eid , queue , reg , heart ) content = dict ( id = eid , status = 'ok' ) content . update ( self . engine info ) if queue in self . by ident : try : raise Key Error ( \"queue id %r in use\" % queue ) except : content = error . wrap exception ( ) self . log . error ( \"queue id %r in use\" , queue , exc info = True ) elif heart in self . hearts : try : raise Key Error ( \"heart id %r in use\" % heart ) except : self . log . error ( \"heart id %r in use\" , heart , exc info = True ) content = error . wrap exception ( ) else : for h , pack in self . incoming registrations . iteritems ( ) : if heart == h : try : raise Key Error ( \"heart id %r in use\" % heart ) except : self . log . error ( \"heart id %r in use\" , heart , exc info = True ) content = error . wrap exception ( ) break elif queue == pack [ 1 ] : try : raise Key Error ( \"queue id %r in use\" % queue ) except : self . log . error ( \"queue id %r in use\" , queue , exc info = True ) content = error . wrap exception ( ) break msg = self . session . send ( self . query , \"registration reply\" , content = content , ident = reg ) if content [ 'status' ] == 'ok' : if heart in self . heartmonitor . hearts : self . incoming registrations [ heart ] = ( eid , queue , reg [ 0 ] , None ) self . finish registration ( heart ) else : purge = lambda : self . purge stalled registration ( heart ) dc = ioloop . Delayed Callback ( purge , self . registration timeout , self . loop ) dc . start ( ) self . incoming registrations [ heart ] = ( eid , queue , reg [ 0 ] , dc ) else : self . log . error ( \"registration::registration %i failed: %r\" , eid , content [ 'evalue' ] ) return eid", "predictions": ["this method adds or . ( to the queue ."], "references": ["register a new engine ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 6362, "code": "def unregister engine ( self , ident , msg ) : try : eid = msg [ 'content' ] [ 'id' ] except : self . log . error ( \"registration::bad engine id for unregistration: %r\" , ident , exc info = True ) return self . log . info ( \"registration::unregister engine(%r)\" , eid ) uuid = self . keytable [ eid ] content = dict ( id = eid , queue = uuid . decode ( 'ascii' ) ) self . dead engines . add ( uuid ) # handleit = lambda : self . handle stranded msgs ( eid , uuid ) dc = ioloop . Delayed Callback ( handleit , self . registration timeout , self . loop ) dc . start ( ) if self . notifier : self . session . send ( self . notifier , \"unregistration notification\" , content = content )", "predictions": ["removes a previously registered ( ."], "references": ["unregister an engine that explicitly requested to leave ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 6363, "code": "def shutdown request ( self , client id , msg ) : self . session . send ( self . query , 'shutdown reply' , content = { 'status' : 'ok' } , ident = client id ) self . session . send ( self . notifier , 'shutdown notice' , content = { 'status' : 'ok' } ) dc = ioloop . Delayed Callback ( lambda : self . shutdown ( ) , 1000 , self . loop ) dc . start ( )", "predictions": ["shuts down this method ."], "references": ["handle shutdown request ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 6364, "code": "def resubmit task ( self , client id , msg ) : def finish ( reply ) : self . session . send ( self . query , 'resubmit reply' , content = reply , ident = client id ) content = msg [ 'content' ] msg ids = content [ 'msg ids' ] reply = dict ( status = 'ok' ) try : records = self . db . find records ( { 'msg id' : { '$in' : msg ids } } , keys = [ 'header' , 'content' , 'buffers' ] ) except Exception : self . log . error ( 'db::db error finding tasks to resubmit' , exc info = True ) return finish ( error . wrap exception ( ) ) found ids = [ rec [ 'msg id' ] for rec in records ] pending ids = [ msg id for msg id in found ids if msg id in self . pending ] if len ( records ) > len ( msg ids ) : try : raise Runtime Error ( \"DB appears to be in an inconsistent state.\" \"More matching records were found than should exist\" ) except Exception : return finish ( error . wrap exception ( ) ) elif len ( records ) < len ( msg ids ) : missing = [ m for m in msg ids if m not in found ids ] try : raise Key Error ( \"No such msg(s): %r\" % missing ) except Key Error : return finish ( error . wrap exception ( ) ) elif pending ids : pass resubmitted = { } for rec in records : header = rec [ 'header' ] msg = self . session . msg ( header [ 'msg type' ] , parent = header ) msg id = msg [ 'msg id' ] msg [ 'content' ] = rec [ 'content' ] fresh = msg [ 'header' ] header [ 'msg id' ] = fresh [ 'msg id' ] header [ 'date' ] = fresh [ 'date' ] msg [ 'header' ] = header self . session . send ( self . resubmit , msg , buffers = rec [ 'buffers' ] ) resubmitted [ rec [ 'msg id' ] ] = msg id self . pending . add ( msg id ) msg [ 'buffers' ] = rec [ 'buffers' ] try : self . db . add record ( msg id , init record ( msg ) ) except Exception : self . log . error ( \"db::DB Error updating record: %s\" , msg id , exc info = True ) finish ( dict ( status = 'ok' , resubmitted = resubmitted ) ) for msg id , resubmit id in resubmitted . iteritems ( ) : try : self . db . update record ( msg id , { 'resubmitted' : resubmit id } ) except Exception : self . log . error ( \"db::DB Error updating record: %s\" , msg id , exc info = True )", "predictions": ["now now the resubmit will be stored as json ."], "references": ["resubmit one or more tasks ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 6365, "code": "def extract record ( self , rec ) : io dict = { } for key in ( 'pyin' , 'pyout' , 'pyerr' , 'stdout' , 'stderr' ) : io dict [ key ] = rec [ key ] content = { 'result content' : rec [ 'result content' ] , 'header' : rec [ 'header' ] , 'result header' : rec [ 'result header' ] , 'received' : rec [ 'received' ] , 'io' : io dict , } if rec [ 'result buffers' ] : buffers = map ( bytes , rec [ 'result buffers' ] ) else : buffers = [ ] return content , buffers", "predictions": ["extract a previously sorted list of buffers ."], "references": ["decompose a taskrecord dict into subsection of reply for get_result"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 6366, "code": "def get results ( self , client id , msg ) : content = msg [ 'content' ] msg ids = sorted ( set ( content [ 'msg ids' ] ) ) statusonly = content . get ( 'status only' , False ) pending = [ ] completed = [ ] content = dict ( status = 'ok' ) content [ 'pending' ] = pending content [ 'completed' ] = completed buffers = [ ] if not statusonly : try : matches = self . db . find records ( dict ( msg id = { '$in' : msg ids } ) ) records = { } for rec in matches : records [ rec [ 'msg id' ] ] = rec except Exception : content = error . wrap exception ( ) self . session . send ( self . query , \"result reply\" , content = content , parent = msg , ident = client id ) return else : records = { } for msg id in msg ids : if msg id in self . pending : pending . append ( msg id ) elif msg id in self . all completed : completed . append ( msg id ) if not statusonly : c , bufs = self . extract record ( records [ msg id ] ) content [ msg id ] = c buffers . extend ( bufs ) elif msg id in records : if rec [ 'completed' ] : completed . append ( msg id ) c , bufs = self . extract record ( records [ msg id ] ) content [ msg id ] = c buffers . extend ( bufs ) else : pending . append ( msg id ) else : try : raise Key Error ( 'No such message: ' + msg id ) except : content = error . wrap exception ( ) break self . session . send ( self . query , \"result reply\" , content = content , parent = msg , ident = client id , buffers = buffers )", "predictions": ["extracts and decode a double array of things"], "references": ["get the result of 1 or more messages ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 6367, "code": "def get history ( self , client id , msg ) : try : msg ids = self . db . get history ( ) except Exception as e : content = error . wrap exception ( ) else : content = dict ( status = 'ok' , history = msg ids ) self . session . send ( self . query , \"history reply\" , content = content , parent = msg , ident = client id )", "predictions": ["create a new history object for the given message ."], "references": ["get a list of all msg_ids in our db records"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 6368, "code": "def db query ( self , client id , msg ) : content = msg [ 'content' ] query = content . get ( 'query' , { } ) keys = content . get ( 'keys' , None ) buffers = [ ] empty = list ( ) try : records = self . db . find records ( query , keys ) except Exception as e : content = error . wrap exception ( ) else : if keys is not None : buffer lens = [ ] if 'buffers' in keys else None result buffer lens = [ ] if 'result buffers' in keys else None else : buffer lens = None result buffer lens = None for rec in records : b = rec . pop ( 'buffers' , empty ) or empty if buffer lens is not None : buffer lens . append ( len ( b ) ) buffers . extend ( b ) rb = rec . pop ( 'result buffers' , empty ) or empty if result buffer lens is not None : result buffer lens . append ( len ( rb ) ) buffers . extend ( rb ) content = dict ( status = 'ok' , records = records , buffer lens = buffer lens , result buffer lens = result buffer lens ) self . session . send ( self . query , \"db reply\" , content = content , parent = msg , ident = client id , buffers = buffers )", "predictions": ["default default implementation just send ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )"], "references": ["perform a raw query on the task record database ."], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 6369, "code": "def cd ( self , newdir ) : prevdir = os . getcwd ( ) os . chdir ( newdir ) try : yield finally : os . chdir ( prevdir )", "predictions": ["creates a new instance of this class ."], "references": ["go to the path"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 6370, "code": "def decode cmd out ( self , completed cmd ) : try : stdout = completed cmd . stdout . encode ( 'utf-8' ) . decode ( ) except Attribute Error : try : stdout = str ( bytes ( completed cmd . stdout ) , 'big5' ) . strip ( ) except Attribute Error : stdout = str ( bytes ( completed cmd . stdout ) . decode ( 'utf-8' ) ) . strip ( ) try : stderr = completed cmd . stderr . encode ( 'utf-8' ) . decode ( ) except Attribute Error : try : stderr = str ( bytes ( completed cmd . stderr ) , 'big5' ) . strip ( ) except Attribute Error : stderr = str ( bytes ( completed cmd . stderr ) . decode ( 'utf-8' ) ) . strip ( ) return Parsed Completed Command ( completed cmd . returncode , completed cmd . args , stdout , stderr )", "predictions": ["initialize command of command line parameters ."], "references": ["return a standard message"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 6371, "code": "def run command under r root ( self , cmd , catched = True ) : RPATH = self . path with self . cd ( newdir = RPATH ) : if catched : process = sp . run ( cmd , stdout = sp . PIPE , stderr = sp . PIPE ) else : process = sp . run ( cmd ) return process", "predictions": ["runs the dummy loop ."], "references": ["subprocess run on here"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 6372, "code": "def get installed version ( name ) : pattern = re . compile ( r'''Installed:\\s+(?P<version>.*)''' ) cmd = 'apt-cache policy %s' % name args = shlex . split ( cmd ) try : output = subprocess . check output ( args ) if not output : return None except Called Process Error : return None match = pattern . search ( output ) if match : version = match . groupdict ( ) [ 'version' ] if version == '(none)' : return None else : return version", "predictions": ["verify the ( i . e . , the ( ( e . , the : the ( e . g . , the ( ( any : the : the : the : the ( the : the : the : the : the ( e . g ."], "references": ["returns installed package version and none if package is not installed"], "bleu": 0.02020717606755137, "rouge_l": 0.0}
{"id": 6373, "code": "def squash unicode ( obj ) : if isinstance ( obj , dict ) : for key in obj . keys ( ) : obj [ key ] = squash unicode ( obj [ key ] ) if isinstance ( key , unicode ) : obj [ squash unicode ( key ) ] = obj . pop ( key ) elif isinstance ( obj , list ) : for i , v in enumerate ( obj ) : obj [ i ] = squash unicode ( v ) elif isinstance ( obj , unicode ) : obj = obj . encode ( 'utf8' ) return obj", "predictions": ["recursively pylab an object ."], "references": ["coerce unicode back to bytestrings ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 6374, "code": "def extract header ( msg or header ) : if not msg or header : return { } try : h = msg or header [ 'header' ] except Key Error : try : h = msg or header [ 'msg id' ] except Key Error : raise else : h = msg or header if not isinstance ( h , dict ) : h = dict ( h ) return h", "predictions": ["trait given message from metadata . if metadata is true , the corresponding message is returned ."], "references": ["given a message or header return the header ."], "bleu": 0.09083627868206415, "rouge_l": 0.3257676902536716}
{"id": 6375, "code": "def check packers ( self ) : pack = self . pack unpack = self . unpack msg = dict ( a = [ 1 , 'hi' ] ) try : packed = pack ( msg ) except Exception : raise Value Error ( \"packer could not serialize a simple message\" ) if not isinstance ( packed , bytes ) : raise Value Error ( \"message packed to %r, but bytes are required\" % type ( packed ) ) try : unpacked = unpack ( packed ) except Exception : raise Value Error ( \"unpacker could not handle the packer's output\" ) msg = dict ( t = datetime . now ( ) ) try : unpacked = unpack ( pack ( msg ) ) except Exception : self . pack = lambda o : pack ( squash dates ( o ) ) self . unpack = lambda s : extract dates ( unpack ( s ) )", "predictions": ["each message is a valid type of this message ."], "references": ["check packers for binary data and datetime support ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 6376, "code": "def object info ( * * kw ) : infodict = dict ( izip longest ( info fields , [ None ] ) ) infodict . update ( kw ) return infodict", "predictions": ["get information about the info and data structures ."], "references": ["make an object info dict with all fields present ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 6377, "code": "def head ( self , h ) : return '%s%s%s' % ( self . color table . active colors . header , h , self . color table . active colors . normal )", "predictions": ["add the info for this object to the info ."], "references": ["return a header string with proper colors ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 6378, "code": "def noinfo ( self , msg , oname ) : print 'No %s found' % msg , if oname : print 'for %s' % oname else : print", "predictions": [") method used to ) the usage message ."], "references": ["generic message when no information is found ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 6379, "code": "def psource ( self , obj , oname = '' ) : linecache . checkcache ( ) try : src = getsource ( obj ) except : self . noinfo ( 'source' , oname ) else : page . page ( self . format ( py3compat . unicode to str ( src ) ) )", "predictions": ["( to handle event handlers from one object ."], "references": ["print the source code for an object ."], "bleu": 0.18575057999133596, "rouge_l": 0.2378167641325536}
{"id": 6380, "code": "def pfile ( self , obj , oname = '' ) : lineno = find source lines ( obj ) if lineno is None : self . noinfo ( 'file' , oname ) return ofile = find file ( obj ) if ofile . endswith ( ( '.so' , '.dll' , '.pyd' ) ) : print 'File %r is binary, not printing.' % ofile elif not os . path . isfile ( ofile ) : print 'File %r does not exist, not printing.' % ofile else : page . page ( self . format ( open ( ofile ) . read ( ) ) , lineno - 1 )", "predictions": ["get the full bin node without marshalling ."], "references": ["show the whole file where an object was defined ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 6381, "code": "def print figure ( fig , fmt = 'png' ) : if not fig . axes and not fig . lines : return fc = fig . get facecolor ( ) ec = fig . get edgecolor ( ) fig . set facecolor ( 'white' ) fig . set edgecolor ( 'white' ) try : bytes io = Bytes IO ( ) fig . canvas . print figure ( bytes io , format = fmt , bbox inches = 'tight' ) data = bytes io . getvalue ( ) finally : fig . set facecolor ( fc ) fig . set edgecolor ( ec ) return data", "predictions": ["prints a ( or the ( or the ( i . e . it ' s been read , it ' s in a separate thread , i . e . it ' s ( , it will be called to print , and the other ( is printed to"], "references": ["convert a figure to svg or png for inline display ."], "bleu": 0.030216776104535565, "rouge_l": 0.11117861482381533}
{"id": 6382, "code": "def activate matplotlib ( backend ) : import matplotlib if backend . startswith ( 'module://' ) : matplotlib . rc Params [ 'backend' ] = backend else : matplotlib . use ( backend ) matplotlib . interactive ( True ) import matplotlib . pylab as pylab #import matplotlib.pyplot #matplotlib.pyplot.switch backend(backend) pylab . show . needmain = False pylab . draw if interactive = flag calls ( pylab . draw if interactive )", "predictions": ["this method loads the ( \" ) ."], "references": ["activate the given backend and set interactive to true ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 6383, "code": "def import pylab ( user ns , import all = True ) : s = ( \"import numpy\\n\" \"import matplotlib\\n\" \"from matplotlib import pylab, mlab, pyplot\\n\" \"np = numpy\\n\" \"plt = pyplot\\n\" ) exec s in user ns if import all : s = ( \"from matplotlib.pylab import *\\n\" \"from numpy import *\\n\" ) exec s in user ns", "predictions": ["depth first ( from the list of ( ."], "references": ["import the standard pylab symbols into user_ns ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 6384, "code": "def trace ( self , frame , event , arg unused ) : if self . stopped : return if 0 : sys . stderr . write ( \"trace event: %s %r @%d\\n\" % ( event , frame . f code . co filename , frame . f lineno ) ) if self . last exc back : if frame == self . last exc back : if self . arcs and self . cur file data : pair = ( self . last line , - self . last exc firstlineno ) self . cur file data [ pair ] = None self . cur file data , self . last line = self . data stack . pop ( ) self . last exc back = None if event == 'call' : self . data stack . append ( ( self . cur file data , self . last line ) ) filename = frame . f code . co filename if filename not in self . should trace cache : tracename = self . should trace ( filename , frame ) self . should trace cache [ filename ] = tracename else : tracename = self . should trace cache [ filename ] #print(\"called, stack is %d deep, tracename is %r\" % ( if tracename : if tracename not in self . data : self . data [ tracename ] = { } self . cur file data = self . data [ tracename ] else : self . cur file data = None self . last line = - 1 elif event == 'line' : if self . cur file data is not None : if self . arcs : #print(\"lin\", self.last line, frame.f lineno) self . cur file data [ ( self . last line , frame . f lineno ) ] = None else : #print(\"lin\", frame.f lineno) self . cur file data [ frame . f lineno ] = None self . last line = frame . f lineno elif event == 'return' : if self . arcs and self . cur file data : first = frame . f code . co firstlineno self . cur file data [ ( self . last line , - first ) ] = None self . cur file data , self . last line = self . data stack . pop ( ) #print(\"returned, stack is %d deep\" % (len(self.data stack))) elif event == 'exception' : #print(\"exc\", self.last line, frame.f lineno) self . last exc back = frame . f back self . last exc firstlineno = frame . f code . co firstlineno return self . trace", "predictions": ["only show here for ( ."], "references": ["the trace function passed to sys . settrace ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 6385, "code": "def stop ( self ) : self . stopped = True if self . thread != threading . current Thread ( ) : return if hasattr ( sys , \"gettrace\" ) and self . warn : if sys . gettrace ( ) != self . trace : msg = \"Trace function changed, measurement is likely wrong: %r\" self . warn ( msg % ( sys . gettrace ( ) , ) ) #print(\"Stopping tracer on %s\" % threading.current thread().ident) sys . settrace ( None )", "predictions": ["stops the log controller ."], "references": ["stop this tracer ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 6386, "code": "def start tracer ( self ) : tracer = self . trace class ( ) tracer . data = self . data tracer . arcs = self . branch tracer . should trace = self . should trace tracer . should trace cache = self . should trace cache tracer . warn = self . warn fn = tracer . start ( ) self . tracers . append ( tracer ) return fn", "predictions": ["start method to start the trace method ."], "references": ["start a new tracer object and store it in self . tracers ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 6387, "code": "def installation trace ( self , frame unused , event unused , arg unused ) : sys . settrace ( None ) fn = self . start tracer ( ) if fn : fn = fn ( frame unused , event unused , arg unused ) return fn", "predictions": ["a convenience method to validate the controller of the controller ."], "references": ["called on new threads installs the real tracer ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 6388, "code": "def start ( self ) : if self . collectors : self . collectors [ - 1 ] . pause ( ) self . collectors . append ( self ) #print(\"Started: %r\" % self. collectors, file=sys.stderr) traces0 = [ ] if hasattr ( sys , \"gettrace\" ) : fn0 = sys . gettrace ( ) if fn0 : tracer0 = getattr ( fn0 , ' self ' , None ) if tracer0 : traces0 = getattr ( tracer0 , 'traces' , [ ] ) fn = self . start tracer ( ) for args in traces0 : ( frame , event , arg ) , lineno = args try : fn ( frame , event , arg , lineno = lineno ) except Type Error : raise Exception ( \"fullcoverage must be run with the C trace function.\" ) threading . settrace ( self . installation trace )", "predictions": ["creates the here for the call to dispatch the type of the program ."], "references": ["start collecting trace information ."], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 6389, "code": "def stop ( self ) : #print >>sys.stderr, \"Stopping: %r\" % self. collectors assert self . collectors assert self . collectors [ - 1 ] is self self . pause ( ) self . tracers = [ ] self . collectors . pop ( ) if self . collectors : self . collectors [ - 1 ] . resume ( )", "predictions": ["a method that closes this class ."], "references": ["stop collecting trace information ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 6390, "code": "def pause ( self ) : for tracer in self . tracers : tracer . stop ( ) stats = tracer . get stats ( ) if stats : print ( \"\\n Coverage.py tracer stats:\" ) for k in sorted ( stats . keys ( ) ) : print ( \"%16s: %s\" % ( k , stats [ k ] ) ) threading . settrace ( None )", "predictions": ["save and ] the id for the given idents ."], "references": ["pause tracing but be prepared to resume ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 6391, "code": "def resume ( self ) : for tracer in self . tracers : tracer . start ( ) threading . settrace ( self . installation trace )", "predictions": ["resumes the topics that will be executed before all contained children have been executed ."], "references": ["resume tracing after a pause ."], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 6392, "code": "def new code cell ( code = None , prompt number = None ) : cell = Notebook Node ( ) cell . cell type = u'code' if code is not None : cell . code = unicode ( code ) if prompt number is not None : cell . prompt number = int ( prompt number ) return cell", "predictions": ["create a connection node or update its ( depending on the given ( ( ( ( or : : 1 : 1 : 1 : 2 : 1 : 2 : 1 : 2 : 3 : 1 : 1 : 3 : 1 : 2 : 3 : 1"], "references": ["create a new code cell with input and output"], "bleu": 0.03162593967015063, "rouge_l": 0.07750952986022873}
{"id": 6393, "code": "def new text cell ( text = None ) : cell = Notebook Node ( ) if text is not None : cell . text = unicode ( text ) cell . cell type = u'text' return cell", "predictions": ["creates a new engine node or inserts ( if any msg is true msg msg msg msg msg msg msg msg msg msg msg msg ."], "references": ["create a new text cell ."], "bleu": 0.0660161823828377, "rouge_l": 0.21131639722863746}
{"id": 6394, "code": "def new notebook ( cells = None ) : nb = Notebook Node ( ) if cells is not None : nb . cells = cells else : nb . cells = [ ] return nb", "predictions": ["creates an self - pointing engine node ."], "references": ["create a notebook by name id and a list of worksheets ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 6395, "code": "def render traceback ( self , excid = None ) : lines = [ ] if excid is None : for ( en , ev , etb , ei ) in self . elist : lines . append ( self . get engine str ( ei ) ) lines . extend ( ( etb or 'No traceback available' ) . splitlines ( ) ) lines . append ( '' ) else : try : en , ev , etb , ei = self . elist [ excid ] except : raise Index Error ( \"an exception with index %i does not exist\" % excid ) else : lines . append ( self . get engine str ( ei ) ) lines . extend ( ( etb or 'No traceback available' ) . splitlines ( ) ) return lines", "predictions": ["shutdown a request as html ."], "references": ["render one or all of my tracebacks to a list of lines"], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 6396, "code": "def canonical dir ( self , morf ) : return os . path . split ( Code Unit ( morf , self . file locator ) . filename ) [ 0 ]", "predictions": ["method to ) this method will return the code that was previously centered at the end of the list ."], "references": ["return the canonical directory of the module or file morf ."], "bleu": 0.10580331550093845, "rouge_l": 0.34040178571428575}
{"id": 6397, "code": "def source for file ( self , filename ) : if not filename . endswith ( \".py\" ) : if filename [ - 4 : - 1 ] == \".py\" : filename = filename [ : - 1 ] elif filename . endswith ( \"$py.class\" ) : filename = filename [ : - 9 ] + \".py\" return filename", "predictions": ["generate a ) for the case where the extract data is located ."], "references": ["return the source file for filename ."], "bleu": 0.1135935489027116, "rouge_l": 0.21143847487001732}
{"id": 6398, "code": "def warn ( self , msg ) : self . warnings . append ( msg ) sys . stderr . write ( \"Coverage.py warning: %s\\n\" % msg )", "predictions": ["get a warning message ."], "references": ["use msg as a warning ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 6399, "code": "def check for packages ( self ) : if self . source pkgs : found = [ ] for pkg in self . source pkgs : try : mod = sys . modules [ pkg ] except Key Error : continue found . append ( pkg ) try : pkg file = mod . file except Attribute Error : pkg file = None else : d , f = os . path . split ( pkg file ) if f . startswith ( ' init ' ) : pkg file = d else : pkg file = self . source for file ( pkg file ) pkg file = self . file locator . canonical filename ( pkg file ) if not os . path . exists ( pkg file ) : pkg file = None if pkg file : self . source . append ( pkg file ) self . source match . add ( pkg file ) else : self . warn ( \"Module %s has no Python source.\" % pkg ) for pkg in found : self . source pkgs . remove ( pkg )", "predictions": ["verify that package is a package ."], "references": ["update the source_match matcher with latest imported packages ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
